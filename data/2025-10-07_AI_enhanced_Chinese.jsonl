{"id": "2510.03557", "categories": ["cs.DC", "astro-ph.CO", "astro-ph.IM", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.03557", "abs": "https://arxiv.org/abs/2510.03557", "authors": ["Nicholas Frontiere", "J. D. Emberson", "Michael Buehlmann", "Esteban M. Rangel", "Salman Habib", "Katrin Heitmann", "Patricia Larsen", "Vitali Morozov", "Adrian Pope", "Claude-Andr\u00e9 Faucher-Gigu\u00e8re", "Antigoni Georgiadou", "Damien Lebrun-Grandi\u00e9", "Andrey Prokopenko"], "title": "Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability", "comment": null, "summary": "Resolving the most fundamental questions in cosmology requires simulations\nthat match the scale, fidelity, and physical complexity demanded by\nnext-generation sky surveys. To achieve the realism needed for this critical\nscientific partnership, detailed gas dynamics, along with a host of\nastrophysical effects, must be treated self-consistently with gravity for\nend-to-end modeling of structure formation. As an important step on this\nroadmap, exascale computing enables simulations that span survey-scale volumes\nwhile incorporating key subgrid processes that shape complex cosmic structures.\nWe present results from CRK-HACC, a cosmological hydrodynamics code built for\nthe extreme scalability requirements set by modern cosmological surveys. Using\nseparation-of-scale techniques, GPU-resident tree solvers, in situ analysis\npipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion\nparticle full-sky simulation, over an order of magnitude larger than previous\nefforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6\nbillion particles per second and writing more than 100 PB of data in just over\none week of runtime.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.03872", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03872", "abs": "https://arxiv.org/abs/2510.03872", "authors": ["Sreedhar Narayanaswamy", "Pratikkumar Dilipkumar Patel", "Ian Karlin", "Apoorv Gupta", "Sudhir Saripalli", "Janey Guo"], "title": "Datacenter Energy Optimized Power Profiles", "comment": null, "summary": "This paper presents datacenter power profiles, a new NVIDIA software feature\nreleased with Blackwell B200, aimed at improving energy efficiency and/or\nperformance. The initial feature provides coarse-grain user control for HPC and\nAI workloads leveraging hardware and software innovations for intelligent power\nmanagement and domain knowledge of HPC and AI workloads. The resulting\nworkload-aware optimization recipes maximize computational throughput while\noperating within strict facility power constraints. The phase-1 Blackwell\nimplementation achieves up to 15% energy savings while maintaining performance\nlevels above 97% for critical applications, enabling an overall throughput\nincrease of up to 13% in a power-constrained facility.\n  KEYWORDS GPU power management, energy efficiency, power profile, HPC\noptimization, Max-Q, Blackwell architecture", "AI": {"tldr": "NVIDIA Blackwell B200\u7684datacenter power profiles\u529f\u80fd\u901a\u8fc7\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u4f18\u5316HPC\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u4e8615%\u7684\u80fd\u6e90\u8282\u7701\u548c13%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u5728\u4e25\u683c\u7535\u529b\u7ea6\u675f\u4e0b\u7684\u80fd\u6e90\u6548\u7387\u548c/\u6216\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\u521b\u65b0\uff0c\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u7528\u6237\u63a7\u5236\uff0c\u9488\u5bf9HPC\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u3002", "result": "\u521d\u6b65\u5b9e\u73b0\u9636\u6bb5\u8fbe\u5230\u4e86\u9ad8\u8fbe15%\u7684\u80fd\u6e90\u8282\u7701\uff0c\u540c\u65f6\u5173\u952e\u5e94\u7528\u6027\u80fd\u4fdd\u6301\u572897%\u4ee5\u4e0a\uff0c\u6574\u4f53\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe13%\u3002", "conclusion": "Blackwell B200\u7684datacenter power profiles\u529f\u80fd\u901a\u8fc7\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u548c\u9886\u57df\u77e5\u8bc6\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u5173\u952e\u5e94\u7528\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe15%\u7684\u80fd\u6e90\u8282\u7701\u548c13%\u7684\u6574\u4f53\u541e\u5410\u91cf\u63d0\u5347\u3002"}}
{"id": "2510.03891", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.03891", "abs": "https://arxiv.org/abs/2510.03891", "authors": ["Shawn Shuoshuo Chen", "Daiyaan Arfeen", "Minlan Yu", "Peter Steenkiste", "Srinivasan Seshan"], "title": "Toward Co-adapting Machine Learning Job Shape and Cluster Topology", "comment": null, "summary": "Allocating resources to distributed machine learning jobs in multi-tenant\ntorus-topology clusters must meet each job's specific placement and\ncommunication requirements, which are typically described using shapes. There\nis an inherent tension between minimizing network contention and maximizing\ncluster utilization when placing various-shaped jobs. While existing schedulers\ntypically optimize for one objective at the expense of the other, we\ndemonstrate that both can be achieved simultaneously.\n  Our proposed approach, RFold, adapts both job shapes and the underlying\ncluster topology at runtime. This is accomplished by combining two techniques:\n(1) identifying homomorphic job shapes that support the jobs communication\nneeds, and (2) reconfiguring the optical circuit switch-enabled topology to\nsupport more diverse job shapes. Preliminary evaluation performed on a\n4096-node torus cluster simulator indicates that RFold can improve absolute\ncluster utilization by 57% and reduce job completion time by up to 11x relative\nto existing methods", "AI": {"tldr": "RFold \u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\uff0c\u540c\u65f6\u4f18\u5316\u7f51\u7edc\u4e89\u7528\u548c\u96c6\u7fa4\u5229\u7528\u7387\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u79df\u6237\u73af\u9762\u62d3\u6251\u96c6\u7fa4\u4e2d\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4f5c\u4e1a\u5206\u914d\u8d44\u6e90\u9700\u8981\u6ee1\u8db3\u6bcf\u4e2a\u4f5c\u4e1a\u7684\u7279\u5b9a\u653e\u7f6e\u548c\u901a\u4fe1\u9700\u6c42\uff0c\u800c\u73b0\u6709\u8c03\u5ea6\u5668\u901a\u5e38\u53ea\u80fd\u4f18\u5316\u4e00\u4e2a\u76ee\u6807\uff08\u5982\u6700\u5c0f\u5316\u7f51\u7edc\u4e89\u7528\u6216\u6700\u5927\u5316\u96c6\u7fa4\u5229\u7528\u7387\uff09\u800c\u727a\u7272\u53e6\u4e00\u4e2a\u3002", "method": "RFold \u7ed3\u5408\u4e86\u4e24\u79cd\u6280\u672f\uff1a(1) \u8bc6\u522b\u652f\u6301\u4f5c\u4e1a\u901a\u4fe1\u9700\u6c42\u7684\u540c\u6784\u4f5c\u4e1a\u5f62\u72b6\uff0c(2) \u91cd\u65b0\u914d\u7f6e\u57fa\u4e8e\u5149\u8def\u4ea4\u6362\u7684\u62d3\u6251\u7ed3\u6784\u4ee5\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u4f5c\u4e1a\u5f62\u72b6\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0cRFold \u5728 4096 \u8282\u70b9\u73af\u9762\u96c6\u7fa4\u6a21\u62df\u5668\u4e2d\uff0c\u53ef\u5c06\u7edd\u5bf9\u96c6\u7fa4\u5229\u7528\u7387\u63d0\u9ad8 57%\uff0c\u5e76\u5c06\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u81f3\u591a 11 \u500d\u3002", "conclusion": "RFold \u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\u7ed3\u6784\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u540c\u65f6\u6700\u5c0f\u5316\u7f51\u7edc\u4e89\u7528\u548c\u6700\u5927\u5316\u96c6\u7fa4\u5229\u7528\u7387\u7684\u76ee\u6807\u3002"}}
{"id": "2510.03970", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03970", "abs": "https://arxiv.org/abs/2510.03970", "authors": ["Zainab Saad", "Jialin Yang", "Henry Leung", "Steve Drew"], "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning", "comment": "Accepted to 2025 IEEE Smart World Congress (SWC 2025)", "summary": "The growing reliance on large-scale data centers to run resource-intensive\nworkloads has significantly increased the global carbon footprint, underscoring\nthe need for sustainable computing solutions. While container orchestration\nplatforms like Kubernetes help optimize workload scheduling to reduce carbon\nemissions, existing methods often depend on centralized machine learning models\nthat raise privacy concerns and struggle to generalize across diverse\nenvironments. In this paper, we propose a federated learning approach for\nenergy consumption prediction that preserves data privacy by keeping sensitive\noperational data within individual enterprises. By extending the Kubernetes\nEfficient Power Level Exporter (Kepler), our framework trains XGBoost models\ncollaboratively across distributed clients using Flower's FedXgbBagging\naggregation using a bagging strategy, eliminating the need for centralized data\nsharing. Experimental results on the SPECPower benchmark dataset show that our\nFL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a\ncentralized baseline. This work addresses the unresolved trade-off between data\nprivacy and energy prediction efficiency in prior systems such as Kepler and\nCASPER and offers enterprises a viable pathway toward sustainable cloud\ncomputing without compromising operational privacy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u534f\u4f5c\u8bad\u7ec3XGBoost\u6a21\u578b\u4f18\u5316\u80fd\u6e90\u6d88\u8017\u9884\u6d4b\uff0c\u4fdd\u62a4\u9690\u79c1\u4e14\u964d\u4f4e\u8bef\u5dee\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u5fc3\u7684\u8d44\u6e90\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u589e\u52a0\u4e86\u5168\u7403\u78b3\u8db3\u8ff9\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5b58\u5728\u9690\u79c1\u95ee\u9898\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u6269\u5c55Kubernetes\u9ad8\u6548\u529f\u7387\u7ea7\u522b\u5bfc\u51fa\u5668\uff08Kepler\uff09\uff0c\u91c7\u7528Flower\u7684FedXgbBagging\u805a\u5408\u7b56\u7565\uff0c\u5728\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u4e0a\u534f\u4f5c\u8bad\u7ec3XGBoost\u6a21\u578b\u3002", "result": "\u5728SPECPower\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\u6bd4\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e\u4e8611.7%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u6e90\u6d88\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u4e0a\u534f\u4f5c\u8bad\u7ec3XGBoost\u6a21\u578b\uff0c\u65e2\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1\uff0c\u53c8\u63d0\u9ad8\u4e86\u9884\u6d4b\u6548\u7387\uff0c\u4e3a\u53ef\u6301\u7eed\u4e91\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.03308", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03308", "abs": "https://arxiv.org/abs/2510.03308", "authors": ["Jiong Lin", "Jialong Ning", "Judah Goldfeder", "Hod Lipson"], "title": "Creative synthesis of kinematic mechanisms", "comment": "6pages, 6 figures", "summary": "In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework.", "AI": {"tldr": "\u901a\u8fc7\u56fe\u50cf\u751f\u6210\u6846\u67b6\u63a2\u7d22\u5e73\u9762\u8fde\u6746\u673a\u6784\u7684\u52a8\u529b\u5b66\u5408\u6210\uff0c\u4f7f\u7528VAE\u6a21\u578b\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u8868\u793a\u548c\u5408\u6210\u591a\u79cd\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u590d\u6742\u5ea6\u9012\u589e\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5e73\u9762\u8fde\u6746\u673a\u6784\u52a8\u529b\u5b66\u5408\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u7edf\u4e00\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\u8868\u793a\u548c\u5408\u6210\u5305\u62ec\u590d\u6742\u591a\u73af\u673a\u6784\u5728\u5185\u7684\u591a\u79cd\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u6f5c\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u63a2\u7d22\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u672a\u89c1\u8fd0\u52a8\u66f2\u7ebf\u548c\u6a21\u62df\u65b0\u8fd0\u52a8\u5b66\u4e2d\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u5c06\u8f68\u8ff9\u70b9\u7684\u7ed8\u5236\u901f\u5ea6\u7f16\u7801\u4e3a\u989c\u8272\u68af\u5ea6\uff0c\u652f\u6301\u57fa\u4e8e\u8f68\u8ff9\u5f62\u72b6\u548c\u901f\u5ea6\u5206\u5e03\u7684\u52a8\u529b\u5b66\u5408\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u6807\u51c6\u56db\u8fde\u6746\u673a\u6784\u96c6\u3001\u6df7\u5408\u56db\u8fde\u6746\u548c\u66f2\u67c4\u6ed1\u5757\u673a\u6784\u96c6\uff0c\u4ee5\u53ca\u5305\u542b\u591a\u73af\u673a\u6784\u7684\u590d\u6742\u96c6\u3002\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u56fe\u50cf\u7684\u8868\u793a\u5728\u751f\u6210\u673a\u68b0\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u56fe\u50cf\u751f\u6210\u6846\u67b6\u5728\u673a\u68b0\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5305\u62ec\u65cb\u8f6c\u548c\u6ed1\u52a8\u5173\u8282\u5728\u5185\u7684\u673a\u5236\u53ef\u4ee5\u5728\u7edf\u4e00\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e2d\u8868\u793a\u548c\u5408\u6210\u3002"}}
{"id": "2510.03427", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.03427", "abs": "https://arxiv.org/abs/2510.03427", "authors": ["Hossein Gholizadeh", "Yonggang Jiang"], "title": "A Subquadratic Two-Party Communication Protocol for Minimum Cost Flow", "comment": null, "summary": "In this paper, we discuss the maximum flow problem in the two-party\ncommunication model, where two parties, each holding a subset of edges on a\ncommon vertex set, aim to compute the maximum flow of the union graph with\nminimal communication. We show that this can be solved with\n$\\tilde{O}(n^{1.5})$ bits of communication, improving upon the trivial\n$\\tilde{O}(n^2)$ bound.\n  To achieve this, we derive two additional, more general results:\n  1. We present a randomized algorithm for linear programs with two-sided\nconstraints that requires $\\tilde{O}(n^{1.5}k)$ bits of communication when each\nconstraint has at most $k$ non-zeros. This result improves upon the prior work\nby [Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24], which\nachieves a complexity of $\\tilde{O}(n^2)$ bits for LPs with one-sided\nconstraints. Upon more precise analysis, their algorithm can reach a bit\ncomplexity of $\\tilde{O}(n^{1.5} + nk)$ for one-sided constraint LPs.\nNevertheless, for sparse matrices, our approach matches this complexity while\nextending the scope to two-sided constraints.\n  2. Leveraging this result, we demonstrate that the minimum cost flow problem,\nas a special case of solving linear programs with two-sided constraints and as\na general case of maximum flow problem, can also be solved with a communication\ncomplexity of $\\tilde{O}(n^{1.5})$ bits.\n  These results are achieved by adapting an interior-point method (IPM)-based\nalgorithm for solving LPs with two-sided constraints in the sequential setting\nby [van den Brand, Lee, Liu, Saranurak, Sidford, Song, Wang, STOC'21] to the\ntwo-party communication model. This adaptation utilizes techniques developed by\n[Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24] for distributed\nconvex optimization.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u4e24\u65b9\u901a\u4fe1\u6a21\u578b\u4e2d\u6700\u5927\u6d41\u95ee\u9898\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u81f3$\tilde{O}(n^{1.5})$\u6bd4\u7279\uff0c\u5e76\u63a8\u5e7f\u81f3\u7ebf\u6027\u89c4\u5212\u548c\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u4e24\u65b9\u901a\u4fe1\u6a21\u578b\u4e0b\u6700\u5927\u6d41\u95ee\u9898\u7684\u901a\u4fe1\u6548\u7387\uff0c\u65e8\u5728\u51cf\u5c11\u8ba1\u7b97\u6700\u5927\u6d41\u6240\u9700\u7684\u901a\u4fe1\u91cf\uff0c\u540c\u65f6\u63a8\u5e7f\u81f3\u66f4\u4e00\u822c\u7684\u7ebf\u6027\u89c4\u5212\u548c\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u57fa\u4e8e\u5185\u70b9\u6cd5\uff08IPM\uff09\u7684\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u9002\u5e94\u4e8e\u4e24\u65b9\u901a\u4fe1\u6a21\u578b\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u51f8\u4f18\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u590d\u6742\u5ea6\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u73b0\u4e86$\tilde{O}(n^{1.5})$\u6bd4\u7279\u7684\u901a\u4fe1\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u5148\u524d\u7684$\tilde{O}(n^2)$\u7ed3\u679c\uff0c\u5e76\u63a8\u5e7f\u81f3\u7a00\u758f\u77e9\u9635\u548c\u53cc\u8fb9\u7ea6\u675f\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6539\u8fdb\u901a\u4fe1\u590d\u6742\u5ea6\u81f3$\tilde{O}(n^{1.5})$\u6bd4\u7279\uff0c\u89e3\u51b3\u4e86\u4e24\u65b9\u901a\u4fe1\u6a21\u578b\u4e2d\u7684\u6700\u5927\u6d41\u95ee\u9898\uff0c\u5e76\u63a8\u5e7f\u81f3\u7ebf\u6027\u89c4\u5212\u4e0e\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\u3002"}}
{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6539\u8fdb\u8d44\u6e90\u6cc4\u6f0f\u4fee\u590d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8d44\u6e90\u5305\u88c5\u5668\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u4fee\u590d\u786c\u7f16\u7801\u5e93\u8d44\u6e90\u7c7b\u578b\u7684\u6cc4\u6f0f\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4ee3\u7801\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u8d44\u6e90\u5305\u88c5\u5668\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u56db\u79cd\u5173\u952e\u8d21\u732e\uff1a(1) \u5c06\u8d44\u6e90\u7ba1\u7406\u89c4\u8303\u7684\u63a8\u65ad\u96c6\u6210\u5230\u4fee\u590d\u6d41\u7a0b\u4e2d\uff1b(2) \u5c06\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u66f4\u6613\u4e8e\u5206\u6790\u7684\u53d8\u4f53\uff1b(3) \u5f15\u5165\u65b0\u9896\u7684\u5b57\u6bb5\u5305\u542b\u5206\u6790\uff1b(4) \u5f15\u5165\u65b0\u7684\u4fee\u590d\u6a21\u5f0f\u3002", "result": "\u5728NJR\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0c\u672c\u6587\u7684\u5b9e\u73b0Arodnap\u5c06\u8d44\u6e90\u6cc4\u6f0f\u8b66\u544a\u7684\u4fee\u590d\u7387\u4ece41%\u63d0\u5347\u81f368%\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u96c6\u6210\u8d44\u6e90\u7ba1\u7406\u89c4\u8303\u7684\u63a8\u65ad\u3001\u7a0b\u5e8f\u8f6c\u6362\u3001\u65b0\u9896\u7684\u5b57\u6bb5\u5305\u542b\u5206\u6790\u4ee5\u53ca\u5f15\u5165\u65b0\u7684\u4fee\u590d\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u6cc4\u6f0f\u4fee\u590d\u7684\u6210\u529f\u7387\uff0c\u4ece41%\u63d0\u5347\u81f368%\u3002"}}
{"id": "2510.04360", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2510.04360", "abs": "https://arxiv.org/abs/2510.04360", "authors": ["Yutong Huang", "Zhiyuan Guo", "Yiying Zhang"], "title": "An Early Exploration of Deep-Learning-Driven Prefetching for Far Memory", "comment": null, "summary": "Far-memory systems, where applications store less-active data in more\nenergy-efficient memory media, are increasingly adopted by data centers.\nHowever, applications are bottlenecked by on-demand data fetching from far- to\nlocal-memory. We present Memix, a far-memory system that embodies a\ndeep-learning-system co-design for efficient and accurate prefetching,\nminimizing on-demand far-memory accesses. One key observation is that memory\naccesses are shaped by both application semantics and runtime context,\nproviding an opportunity to optimize each independently. Preliminary evaluation\nof Memix on data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 42%.", "AI": {"tldr": "Memix\u662f\u4e00\u79cd\u8fdc\u5185\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u9884\u53d6\uff0c\u51cf\u5c11\u6309\u9700\u8bbf\u95ee\uff0c\u6027\u80fd\u63d0\u534742%\u3002", "motivation": "\u8fdc\u5185\u5b58\u7cfb\u7edf\u4e2d\uff0c\u6309\u9700\u4ece\u8fdc\u5185\u5b58\u83b7\u53d6\u6570\u636e\u6210\u4e3a\u5e94\u7528\u74f6\u9888\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u80fd\u6548\u3002", "method": "Memix\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5e94\u7528\u8bed\u4e49\u548c\u8fd0\u884c\u65f6\u4e0a\u4e0b\u6587\uff0c\u72ec\u7acb\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u5728\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0cMemix\u6bd4\u73b0\u6709\u6700\u4f18\u8fdc\u5185\u5b58\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe42%\u3002", "conclusion": "Memix\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fdc\u5185\u5b58\u7684\u6309\u9700\u8bbf\u95ee\uff0c\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u8fdc\u5185\u5b58\u7cfb\u7edf\u6027\u80fd\u9ad8\u51fa42%\u3002"}}
{"id": "2510.03438", "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03438", "abs": "https://arxiv.org/abs/2510.03438", "authors": ["Grace Ra Kim", "Duncan Eddy", "Vedant Srinivas", "Mykel J. Kochenderfer"], "title": "Scalable Ground Station Selection for Large LEO Constellations", "comment": "14 pages, 7 tables, 10 figures, submitted to IEEE Aeroconf 2026", "summary": "Effective ground station selection is critical for low Earth orbiting (LEO)\nsatellite constellations to minimize operational costs, maximize data downlink\nvolume, and reduce communication gaps between access windows. Traditional\nground station selection typically begins by choosing from a fixed set of\nlocations offered by Ground Station-as-a-Service (GSaaS) providers, which helps\nreduce the problem scope to optimizing locations over existing infrastructure.\nHowever, finding a globally optimal solution for stations using existing\nmixed-integer programming methods quickly becomes intractable at scale,\nespecially when considering multiple providers and large satellite\nconstellations. To address this issue, we introduce a scalable, hierarchical\nframework that decomposes the global selection problem into single-satellite,\nshort time-window subproblems. Optimal station choices from each subproblem are\nclustered to identify consistently high-value locations across all decomposed\ncases. Cluster-level sets are then matched back to the closest GSaaS candidate\nsites to produce a globally feasible solution. This approach enables scalable\ncoordination while maintaining near-optimal performance. We evaluate our\nmethod's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10\nstations), achieving solutions within 95% of the global IP optimum for all test\ncases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and\nPlanet's Flock (96) show that while exact IP solutions fail to scale, our\nframework continues to deliver high-quality site selections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u805a\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u536b\u661f\u661f\u5ea7\u5730\u9762\u7ad9\u9009\u62e9\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u5c40\u6700\u4f18\u3002", "motivation": "\u4f20\u7edf\u5730\u9762\u7ad9\u9009\u62e9\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u536b\u661f\u661f\u5ea7\u548c\u591a\u63d0\u4f9b\u5546\u60c5\u51b5\u4e0b\u96be\u4ee5\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u5168\u7403\u9009\u62e9\u95ee\u9898\u5206\u89e3\u4e3a\u5355\u536b\u661f\u3001\u77ed\u65f6\u95f4\u7a97\u53e3\u5b50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u548c\u5339\u914dGSaaS\u5019\u9009\u7ad9\u70b9\u6765\u751f\u6210\u5168\u5c40\u53ef\u884c\u89e3\u3002", "result": "\u5728\u5408\u6210\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u5168\u5c40IP\u6700\u4f18\u89e3\u768495%\uff0c\u5e76\u5728\u5b9e\u9645\u536b\u661f\u661f\u5ea7\uff08\u5982Capella Space\u3001ICEYE\u548cPlanet's Flock\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5168\u7403\u5730\u9762\u7ad9\u9009\u62e9\u95ee\u9898\u5206\u89e3\u4e3a\u5355\u536b\u661f\u77ed\u65f6\u95f4\u7a97\u53e3\u5b50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u534f\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03342", "abs": "https://arxiv.org/abs/2510.03342", "authors": ["Abbas Abdolmaleki", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Ashwin Balakrishna", "Nathan Batchelor", "Alex Bewley", "Jeff Bingham", "Michael Bloesch", "Konstantinos Bousmalis", "Philemon Brakel", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Christine Chan", "Oscar Chang", "London Chappellet-Volpini", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "Adrian Collister", "David B. D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Meet Kirankumar Dave", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Carl Doersch", "Adil Dostmohamed", "Yilun Du", "Debidatta Dwibedi", "Sathish Thoppay Egambaram", "Michael Elabd", "Tom Erez", "Xiaolin Fang", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Ruiqi Gao", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Oliver Groth", "Agrim Gupta", "Roland Hafner", "Steven Hansen", "Leonard Hasenclever", "Sam Haves", "Nicolas Heess", "Brandon Hernaez", "Alex Hofer", "Jasmine Hsu", "Lu Huang", "Sandy H. Huang", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Sally Jesmonth", "Abhishek Jindal", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Matija Kecman", "J. Chase Kew", "Donnie Kim", "Frank Kim", "Junkyung Kim", "Thomas Kipf", "Sean Kirmani", "Ksenia Konyushkova", "Li Yang Ku", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Tuan Anh Le", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Guy Lever", "Jacky Liang", "Li-Heng Lin", "Fangchen Liu", "Shangbang Long", "Caden Lu", "Sharath Maddineni", "Anirudha Majumdar", "Kevis-Kokitsi Maninis", "Andrew Marmon", "Sergio Martinez", "Assaf Hurwitz Michaely", "Niko Milonopoulos", "Joss Moore", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Joy Ortiz", "Kenneth Oslund", "Carolina Parada", "Emilio Parisotto", "Amaris Paryag", "Acorn Pooley", "Thomas Power", "Alessio Quaglino", "Haroon Qureshi", "Rajkumar Vasudeva Raju", "Helen Ran", "Dushyant Rao", "Kanishka Rao", "Isaac Reid", "David Rendleman", "Krista Reymann", "Miguel Rivas", "Francesco Romano", "Yulia Rubanova", "Peter Pastor Sampedro", "Pannag R Sanketi", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Mohit Shridhar", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Rachel Sterneck", "Ian Storz", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Saran Tunyasuvunakool", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Maria Bauza Villalonga", "Oriol Vinyals", "Ren\u00e9 Wagner", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Chengda Wu", "Markus Wulfmeier", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Jimmy Yan", "Sherry Yang", "Skye Yang", "Yuxiang Yang", "Hiu Hong Yu", "Wenhao Yu", "Wentao Yuan", "Yuan Yuan", "Jingwei Zhang", "Tingnan Zhang", "Zhiyuan Zhang", "Allan Zhou", "Guangyao Zhou", "Yuxiang Zhou"], "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer", "comment": null, "summary": "General-purpose robots need a deep understanding of the physical world,\nadvanced reasoning, and general and dexterous control. This report introduces\nthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,\na multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER\n1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together\nthree major innovations. First, Gemini Robotics 1.5 features a novel\narchitecture and a Motion Transfer (MT) mechanism, which enables it to learn\nfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.\nSecond, Gemini Robotics 1.5 interleaves actions with a multi-level internal\nreasoning process in natural language. This enables the robot to \"think before\nacting\" and notably improves its ability to decompose and execute complex,\nmulti-step tasks, and also makes the robot's behavior more interpretable to the\nuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for\nembodied reasoning, i.e., for reasoning capabilities that are critical for\nrobots, such as visual and spatial understanding, task planning, and progress\nestimation. Together, this family of models takes us a step towards an era of\nphysical agents-enabling robots to perceive, think and then act so they can\nsolve complex multi-step tasks.", "AI": {"tldr": "Gemini Robotics 1.5 \u548c Gemini Robotics-ER 1.5 \u901a\u8fc7\u65b0\u9896\u67b6\u6784\u3001\u8fd0\u52a8\u8f6c\u79fb\u673a\u5236\u548c\u591a\u7ea7\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u901a\u7528\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u8fc8\u5411\u7269\u7406\u4ee3\u7406\u65f6\u4ee3\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u9700\u8981\u5bf9\u7269\u7406\u4e16\u754c\u7684\u6df1\u523b\u7406\u89e3\u3001\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u4ee5\u53ca\u901a\u7528\u4e14\u7075\u5de7\u7684\u63a7\u5236\u3002", "method": "Gemini Robotics 1.5 \u91c7\u7528\u4e86\u65b0\u9896\u7684\u67b6\u6784\u548c\u8fd0\u52a8\u8f6c\u79fb\uff08MT\uff09\u673a\u5236\uff0c\u80fd\u591f\u4ece\u5f02\u6784\u3001\u591a\u4f53\u73b0\u673a\u5668\u4eba\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u4f7fVLA\u66f4\u901a\u7528\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u4ea4\u7ec7\u52a8\u4f5c\u4e0e\u591a\u7ea7\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "Gemini Robotics 1.5 \u548c Gemini Robotics-ER 1.5 \u5728\u4f53\u73b0\u63a8\u7406\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5305\u62ec\u89c6\u89c9\u548c\u7a7a\u95f4\u7406\u89e3\u3001\u4efb\u52a1\u89c4\u5212\u548c\u8fdb\u5ea6\u4f30\u8ba1\u7b49\u5173\u952e\u80fd\u529b\u3002", "conclusion": "Gemini Robotics 1.5 \u548c Gemini Robotics-ER 1.5 \u6a21\u578b\u5bb6\u65cf\u6807\u5fd7\u7740\u5411\u7269\u7406\u4ee3\u7406\u65f6\u4ee3\u8fc8\u8fdb\u4e86\u4e00\u6b65\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u3001\u601d\u8003\u5e76\u884c\u52a8\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\u3002"}}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents.", "AI": {"tldr": "WAREX\u662f\u4e00\u79cd\u8bc4\u4f30\u6d4f\u89c8\u5668\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u4ee3\u7406\u5728\u6a21\u62df\u4e0d\u7a33\u5b9a\u73af\u5883\u65f6\u4efb\u52a1\u6210\u529f\u7387\u5927\u5e45\u4e0b\u964d\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\uff08\u5982\u7f51\u7edc\u95ee\u9898\u3001\u7f51\u7ad9\u653b\u51fb\u7b49\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u8d34\u8fd1\u5b9e\u9645\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u73b0\u6709\u57fa\u51c6\uff08WebArena\u3001WebVoyager\u548cREAL\uff09\u4e2d\u5f15\u5165WAREX\uff0c\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u7684\u4e0d\u7a33\u5b9a\u6027\uff08\u5982\u7f51\u7edc\u95ee\u9898\u3001HTTPS\u8fde\u63a5\u95ee\u9898\u548c\u7f51\u7ad9\u653b\u51fb\uff09\u6765\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u5f15\u5165WAREX\u540e\uff0c\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u73b0\u6709\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u6709\u9650\u3002", "conclusion": "WAREX\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709\u6d4f\u89c8\u5668\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\u3002"}}
{"id": "2510.04186", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04186", "abs": "https://arxiv.org/abs/2510.04186", "authors": ["Xuan Jiang", "Xuanyu Zhou", "Yibo Zhao", "Shangqing Cao", "Jinhua Zhao", "Mark Hansen", "Raja Sengupta"], "title": "From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility", "comment": null, "summary": "Urban Air Mobility (UAM) presents a transformative vision for metropolitan\ntransportation, but its practical implementation is hindered by substantial\ninfrastructure costs and operational complexities. We address these challenges\nby modeling a UAM network that leverages existing regional airports and\noperates with an optimized, heterogeneous fleet of aircraft. We introduce\nLPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU\ncomputing to co-optimize UAM demand, fleet operations, and ground\ntransportation interactions simultaneously. Our equilibrium search algorithm is\nextended to accurately forecast demand and determine the most efficient fleet\ncomposition. Applied to a case study of the San Francisco Bay Area, our results\ndemonstrate that this UAM model can yield over 20 minutes' travel time savings\nfor 230,000 selected trips. However, the analysis also reveals that system-wide\nsuccess is critically dependent on seamless integration with ground access and\ndynamic scheduling.", "AI": {"tldr": "\u901a\u8fc7LPSim\u6846\u67b6\u4f18\u5316UAM\u7f51\u7edc\uff0c\u65e7\u91d1\u5c71\u6e7e\u533a\u6848\u4f8b\u663e\u793a\u663e\u8457\u65c5\u884c\u65f6\u95f4\u8282\u7701\uff0c\u4f46\u9700\u4e0e\u5730\u9762\u4ea4\u901a\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u5b9e\u65bd\u4e2d\u7684\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u548c\u64cd\u4f5c\u590d\u6742\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165LPSim\u6846\u67b6\uff0c\u5229\u7528\u591aGPU\u8ba1\u7b97\u8fdb\u884c\u5927\u89c4\u6a21\u5e76\u884c\u6a21\u62df\uff0c\u6269\u5c55\u5747\u8861\u641c\u7d22\u7b97\u6cd5\u4ee5\u9884\u6d4b\u9700\u6c42\u5e76\u4f18\u5316\u673a\u961f\u7ec4\u6210\u3002", "result": "\u5728\u65e7\u91d1\u5c71\u6e7e\u533a\u6848\u4f8b\u4e2d\uff0cUAM\u6a21\u578b\u4e3a23\u4e07\u6b21\u9009\u5b9a\u884c\u7a0b\u8282\u7701\u4e86\u8d85\u8fc720\u5206\u949f\u7684\u65c5\u884c\u65f6\u95f4\u3002", "conclusion": "UAM\u6a21\u578b\u7684\u6210\u529f\u5173\u952e\u5728\u4e8e\u4e0e\u5730\u9762\u4ea4\u901a\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u52a8\u6001\u8c03\u5ea6\uff0c\u5c3d\u7ba1\u5728\u65e7\u91d1\u5c71\u6e7e\u533a\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u65c5\u884c\u65f6\u95f4\u8282\u7701\u3002"}}
{"id": "2510.03312", "categories": ["cs.GR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03312", "abs": "https://arxiv.org/abs/2510.03312", "authors": ["Rong Liu", "Zhongpai Gao", "Benjamin Planche", "Meida Chen", "Van Nguyen Nguyen", "Meng Zheng", "Anwesa Choudhuri", "Terrence Chen", "Yue Wang", "Andrew Feng", "Ziyan Wu"], "title": "Universal Beta Splatting", "comment": null, "summary": "We introduce Universal Beta Splatting (UBS), a unified framework that\ngeneralizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for\nexplicit radiance field rendering. Unlike fixed Gaussian primitives, Beta\nkernels enable controllable dependency modeling across spatial, angular, and\ntemporal dimensions within a single representation. Our unified approach\ncaptures complex light transport effects, handles anisotropic view-dependent\nappearance, and models scene dynamics without requiring auxiliary networks or\nspecific color encodings. UBS maintains backward compatibility by approximating\nto Gaussian Splatting as a special case, guaranteeing plug-in usability and\nlower performance bounds. The learned Beta parameters naturally decompose scene\nproperties into interpretable without explicit supervision: spatial (surface\nvs. texture), angular (diffuse vs. specular), and temporal (static vs.\ndynamic). Our CUDA-accelerated implementation achieves real-time rendering\nwhile consistently outperforming existing methods across static,\nview-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable\nuniversal primitive for radiance field rendering. Our project website is\navailable at https://rongliu-leo.github.io/universal-beta-splatting/.", "AI": {"tldr": "UBS\u901a\u8fc7Beta\u6838\u7edf\u4e00\u6846\u67b6\u6539\u8fdb\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5904\u7406\u5404\u5411\u5f02\u6027\u89c6\u56fe\u4f9d\u8d56\u5916\u89c2\u548c\u573a\u666f\u52a8\u6001\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002UBS\u65e8\u5728\u901a\u8fc7Beta\u6838\u7684\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u6e32\u67d3\u80fd\u529b\u3002", "method": "UBS\u6846\u67b6\u5229\u7528\u5404\u5411\u5f02\u6027Beta\u6838\u8fdb\u884c\u663e\u5f0f\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u65e0\u9700\u8f85\u52a9\u7f51\u7edc\u6216\u7279\u5b9a\u989c\u8272\u7f16\u7801\u3002Beta\u6838\u7684\u53c2\u6570\u81ea\u7136\u5206\u89e3\u573a\u666f\u5c5e\u6027\u4e3a\u53ef\u89e3\u91ca\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u5982\u7a7a\u95f4\uff08\u8868\u9762\u4e0e\u7eb9\u7406\uff09\u3001\u89d2\u5ea6\uff08\u6f2b\u53cd\u5c04\u4e0e\u955c\u9762\u53cd\u5c04\uff09\u548c\u65f6\u95f4\uff08\u9759\u6001\u4e0e\u52a8\u6001\uff09\u3002", "result": "UBS\u5728\u9759\u6001\u3001\u89c6\u56fe\u4f9d\u8d56\u548c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7CUDA\u52a0\u901f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\u3002Beta\u6838\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u901a\u7528\u57fa\u5143\uff0c\u9002\u7528\u4e8e\u8f90\u5c04\u573a\u6e32\u67d3\u3002", "conclusion": "Universal Beta Splatting\uff08UBS\uff09\u4f5c\u4e3a\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u5e7f3D\u9ad8\u65af\u6cfc\u6e85\u5230N\u7ef4\u5404\u5411\u5f02\u6027Beta\u6838\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u8f90\u5c04\u573a\u6e32\u67d3\u3002Beta\u6838\u7684\u53ef\u63a7\u4f9d\u8d56\u6027\u5efa\u6a21\u80fd\u529b\u4f7f\u5176\u5728\u7a7a\u95f4\u3001\u89d2\u5ea6\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7684\u5411\u540e\u517c\u5bb9\u6027\u3002"}}
{"id": "2510.04050", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.04050", "abs": "https://arxiv.org/abs/2510.04050", "authors": ["Sukanya Samanta", "Manohar Reddy"], "title": "A Dynamic Programming Approach to Evader Pathfinding in Static Pursuit Scenarios", "comment": null, "summary": "The interdiction of escaping adversaries in urban networks is a critical\nsecurity challenge. State-of-the-art game-theoretic models, such as the Escape\nInterdiction Game (EIG), provide comprehensive frameworks but assume a highly\ndynamic interaction and entail significant computational complexity, which can\nbe prohibitive for real-time applications. This paper investigates a crucial\nsub-problem: an evader's optimal pathfinding calculus when faced with a static\nor pre-determined defender deployment. We propose the Dynamic Programming for\nEvader Route Optimization (DPERO) algorithm, which models the environment as a\ngraph with probabilistic risks at various nodes. By transforming the\nmultiplicative survival objective into an additive cost function using\nlogarithms, we frame the task as a shortest path problem solvable with value\niteration. This approach allows for the efficient computation of a path that\noptimally balances safety and distance. Experimental results on simulated grid\nnetworks demonstrate that DPERO identifies routes with significantly higher\nsurvival probabilities compared to naive shortest-path baselines, validating\nits efficacy as a practical tool for vulnerability analysis and strategic\nplanning.", "AI": {"tldr": "DPERO\u7b97\u6cd5\u901a\u8fc7\u56fe\u5efa\u6a21\u548c\u5bf9\u6570\u8f6c\u6362\uff0c\u9ad8\u6548\u8ba1\u7b97\u9003\u9038\u8005\u5728\u9759\u6001\u9632\u5fa1\u4e0b\u7684\u6700\u4f18\u8def\u5f84\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9003\u9038\u62e6\u622a\u6e38\u620f\uff08EIG\uff09\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\uff0c\u56e0\u6b64\u7814\u7a76\u5728\u9759\u6001\u9632\u5fa1\u90e8\u7f72\u4e0b\u9003\u9038\u8005\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "method": "\u63d0\u51faDPERO\u7b97\u6cd5\uff0c\u5c06\u73af\u5883\u5efa\u6a21\u4e3a\u5e26\u6709\u6982\u7387\u98ce\u9669\u7684\u56fe\uff0c\u901a\u8fc7\u5bf9\u6570\u8f6c\u6362\u5c06\u751f\u5b58\u76ee\u6807\u8f6c\u5316\u4e3a\u52a0\u6027\u6210\u672c\u51fd\u6570\uff0c\u5e76\u5229\u7528\u503c\u8fed\u4ee3\u6c42\u89e3\u6700\u77ed\u8def\u5f84\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u7f51\u683c\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPERO\u7b97\u6cd5\u6bd4\u57fa\u7ebf\u6700\u77ed\u8def\u5f84\u65b9\u6cd5\u80fd\u8bc6\u522b\u51fa\u751f\u5b58\u6982\u7387\u663e\u8457\u66f4\u9ad8\u7684\u8def\u5f84\u3002", "conclusion": "DPERO\u7b97\u6cd5\u901a\u8fc7\u5c06\u751f\u5b58\u6982\u7387\u8f6c\u5316\u4e3a\u5bf9\u6570\u5f62\u5f0f\u7684\u52a0\u6027\u6210\u672c\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u9759\u6001\u9632\u5fa1\u90e8\u7f72\u4e0b\u9003\u9038\u8005\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u6f0f\u6d1e\u5206\u6790\u548c\u6218\u7565\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature.", "AI": {"tldr": "ALMAS\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u8f6f\u4ef6\u5de5\u7a0b\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u5f0f\u96c6\u6210\u5230\u654f\u6377\u5f00\u53d1\u56e2\u961f\u4e2d\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4efb\u52a1\u81ea\u52a8\u5316\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u662f\u591a\u65b9\u9762\u7684\uff0c\u73b0\u6709LLM\u7cfb\u7edf\u9700\u8003\u8651\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u591a\u4e2a\u9636\u6bb5\u3002", "method": "\u63d0\u51fa\u4e86ALMAS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u667a\u80fd\u4f53\u4e0e\u654f\u6377\u89d2\u8272\u5bf9\u9f50\uff0c\u5e76\u4ee5\u6a21\u5757\u5316\u65b9\u5f0f\u96c6\u6210\u5230\u5f00\u53d1\u73af\u5883\u4e2d\u3002", "result": "ALMAS\u80fd\u591f\u65e0\u7f1d\u751f\u6210\u5e94\u7528\u7a0b\u5e8f\u5e76\u6dfb\u52a0\u65b0\u529f\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "ALMAS\u6846\u67b6\u5c55\u793a\u4e86LLM\u591a\u667a\u80fd\u4f53\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u654f\u6377\u5f00\u53d1\u56e2\u961f\u4e2d\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4efb\u52a1\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.04607", "categories": ["cs.OS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04607", "abs": "https://arxiv.org/abs/2510.04607", "authors": ["Yuan Wang", "Mingyu Li", "Haibo Chen"], "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents", "comment": null, "summary": "Computer-use agents (CUAs) powered by large language models (LLMs) have\nemerged as a promising approach to automating computer tasks, yet they struggle\nwith graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to\ndecompose high-level goals into lengthy, error-prone sequences of fine-grained\nactions, resulting in low success rates and an excessive number of LLM calls.\n  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms\nexisting GUIs into three declarative primitives: access, state, and\nobservation, which are better suited for LLMs. Our key idea is policy-mechanism\nseparation: LLMs focus on high-level semantic planning (policy) while GOI\nhandles low-level navigation and interaction (mechanism). GOI does not require\nmodifying the application source code or relying on application programming\ninterfaces (APIs).\n  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on\nWindows. Compared to a leading GUI-based agent baseline, GOI improves task\nsuccess rates by 67% and reduces interaction steps by 43.5%. Notably, GOI\ncompletes over 61% of successful tasks with a single LLM call.", "AI": {"tldr": "GOI\u901a\u8fc7\u7b56\u7565\u4e0e\u673a\u5236\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728GUI\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684GUI\u81ea\u52a8\u5316\u65b9\u6cd5\u56e0\u9700\u8981\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5e8f\u5217\u800c\u6548\u7387\u4f4e\u4e0b\uff0cGOI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GOI\u5c06\u73b0\u6709GUI\u8f6c\u6362\u4e3a\u4e09\u4e2a\u58f0\u660e\u6027\u539f\u8bed\uff1aaccess\u3001state\u548cobservation\uff0c\u5b9e\u73b0\u4e86\u7b56\u7565\u4e0e\u673a\u5236\u7684\u5206\u79bb\u3002", "result": "GOI\u5728Microsoft Office Suite\u4e0a\u7684\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8667%\uff0c\u4ea4\u4e92\u6b65\u9aa4\u51cf\u5c11\u4e8643.5%\u3002", "conclusion": "GOI\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u4ea4\u4e92\u6b65\u9aa4\uff0c\u8bc1\u660e\u4e86\u5176\u5728GUI\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03491", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03491", "abs": "https://arxiv.org/abs/2510.03491", "authors": ["Sarah-Michelle Hammer", "Stefan Schmid", "Rachee Singh", "Vamsi Addanki"], "title": "Short-circuiting Rings for Low-Latency AllReduce", "comment": null, "summary": "Efficient collective communication is critical for many distributed ML and\nHPC applications. In this context, it is widely believed that the Ring\nalgorithm for the AllReduce collective communication operation is optimal only\nfor large messages, while Recursive Doubling is preferable for small ones due\nto its logarithmic number of steps compared to the linear number for Ring. In\nthis paper, we challenge this long-held assumption and show that the Ring\nalgorithm can remain optimal even for short messages in ring-based GPU-to-GPU\ntopologies, once realistic propagation delays and link capacity constraints are\naccounted for. We find that the total propagation delay for both Ring and\nRecursive Doubling essentially sums to the same value, but the latter incurs\nsignificantly higher congestion due to longer hop counts, leading to increased\ncompletion times. This surprising result motivates our case for in-collective\nadaptive topologies, particularly in the context of emerging photonic\ninterconnects, which can break through the limitations of static topology\ndesigns at the collective communication granularity. We design a \\emph{simple\nand fast} heuristic for circuit-switching that enables Recursive Doubling to\nexploit dynamically reconfigurable photonic paths, carefully balancing\nreconfiguration delays, propagation latencies, and link congestion to minimize\noverall completion time. Our preliminary evaluations, using realistic\nreconfiguration delays, show that our circuit-switching schedules enable faster\ncompletion times for Recursive Doubling, even compared to Ring AllReduce on\nstatic ring topologies. We conclude by highlighting key challenges and future\nresearch directions for realizing practical, in-collective photonic switching.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86Ring\u7b97\u6cd5\u4ec5\u9002\u7528\u4e8e\u5927\u6d88\u606f\u7684\u4f20\u7edf\u8ba4\u77e5\uff0c\u63d0\u51fa\u5728\u8003\u8651\u5b9e\u9645\u5ef6\u8fdf\u548c\u94fe\u8def\u9650\u5236\u540e\uff0cRing\u7b97\u6cd5\u5bf9\u5c0f\u6d88\u606f\u4e5f\u6709\u6548\u3002\u901a\u8fc7\u52a8\u6001\u5149\u8def\u5f84\u91cd\u6784\uff0c\u9012\u5f52\u52a0\u500d\u7b97\u6cd5\u6027\u80fd\u53ef\u8d85\u8d8aRing\u7b97\u6cd5\uff0c\u4e3a\u5149\u4e92\u8fde\u4ea4\u6362\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u6311\u6218\u4e86\u957f\u671f\u4ee5\u6765\u8ba4\u4e3aRing\u7b97\u6cd5\u4ec5\u5728\u5927\u6d88\u606f\u4f20\u8f93\u4e2d\u4f18\u4e8e\u9012\u5f52\u52a0\u500d\u7b97\u6cd5\u7684\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u5728\u8003\u8651\u5b9e\u9645\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u5bb9\u91cf\u9650\u5236\u540e\uff0cRing\u7b97\u6cd5\u5728\u5c0f\u6d88\u606f\u4f20\u8f93\u4e2d\u540c\u6837\u53ef\u80fd\u66f4\u4f18\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u5feb\u901f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u8def\u4ea4\u6362\uff0c\u4f7f\u9012\u5f52\u52a0\u500d\u7b97\u6cd5\u80fd\u591f\u5229\u7528\u52a8\u6001\u53ef\u91cd\u6784\u7684\u5149\u8def\u5f84\uff0c\u5e73\u8861\u91cd\u6784\u5ef6\u8fdf\u3001\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u62e5\u585e\uff0c\u4ee5\u6700\u5c0f\u5316\u603b\u4f53\u5b8c\u6210\u65f6\u95f4\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u8003\u8651\u5b9e\u9645\u91cd\u6784\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u8bbe\u8ba1\u7684\u7535\u8def\u4ea4\u6362\u8c03\u5ea6\u65b9\u6848\u4f7f\u9012\u5f52\u52a0\u500d\u7b97\u6cd5\u7684\u5b8c\u6210\u65f6\u95f4\u5feb\u4e8e\u9759\u6001\u73af\u5f62\u62d3\u6251\u4e0a\u7684Ring AllReduce\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u96c6\u4f53\u901a\u4fe1\u4e2d\u91c7\u7528\u81ea\u9002\u5e94\u62d3\u6251\u7ed3\u6784\u7684\u5fc5\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u65b0\u5174\u7684\u5149\u4e92\u8fde\u6280\u672f\u80cc\u666f\u4e0b\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u6784\u5149\u8def\u5f84\u6765\u4f18\u5316\u901a\u4fe1\u6548\u7387\u3002\u540c\u65f6\uff0c\u6307\u51fa\u4e86\u5b9e\u73b0\u5b9e\u7528\u5316\u5149\u4e92\u8fde\u4ea4\u6362\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.03457", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.03457", "abs": "https://arxiv.org/abs/2510.03457", "authors": ["Jianfeng Lin", "Tianyu Wang", "Baxi Chong", "Matthew Fernandez", "Zhaochen Xu", "Daniel I. Goldman"], "title": "Optimal swimming with body compliance in an overdamped medium", "comment": null, "summary": "Elongate animals and robots use undulatory body waves to locomote through\ndiverse environments. Geometric mechanics provides a framework to model and\noptimize such systems in highly damped environments, connecting a prescribed\nshape change pattern (gait) with locomotion displacement. However, existing\napproaches assume precise execution of prescribed gaits, whereas in practice\nenvironmental interactions with compliant bodies of animals or robots\nfrequently perturb the realized trajectories. In this work, we extend geometric\nmechanics to predict locomotor performance and search for optimal swimming\nstrategy of compliant undulators. We introduce a compliant extension of\nPurcell's three-link swimmer by incorporating series-connected springs at the\njoints. Body dynamics are derived with resistive force theory. Geometric\nmechanics is incorporated into movement prediction and into an optimization\nframework that identifies strategies for controlling compliant swimmers to\nachieve maximal displacement. We validate our framework on a physical\ncable-driven three-link limbless robot, and demonstrate accurate prediction and\noptimization of locomotor performance under varied programmed, state-dependent\ncompliance in a granular medium. Our results establish a systematic\nphysics-based approach for modeling and controlling compliant swimming\nlocomotion, highlighting compliance as a design feature that can be exploited\nfor robust movement in homogeneous and heterogeneous environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u51e0\u4f55\u529b\u5b66\u65b9\u6cd5\uff0c\u4ee5\u5efa\u6a21\u548c\u4f18\u5316\u67d4\u6027\u6ce2\u52a8\u5668\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u67d4\u6027\u5728\u73af\u5883\u9002\u5e94\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u51e0\u4f55\u529b\u5b66\u65b9\u6cd5\u5047\u8bbe\u7cbe\u786e\u6267\u884c\u89c4\u5b9a\u7684\u6b65\u6001\uff0c\u800c\u5728\u5b9e\u8df5\u4e2d\uff0c\u52a8\u7269\u6216\u673a\u5668\u4eba\u7684\u67d4\u6027\u4f53\u4e0e\u73af\u5883\u7684\u76f8\u4e92\u4f5c\u7528\u7ecf\u5e38\u6270\u52a8\u5b9e\u9645\u8f68\u8ff9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6269\u5c55\u51e0\u4f55\u529b\u5b66\u4ee5\u9884\u6d4b\u67d4\u6027\u6ce2\u52a8\u5668\u7684\u8fd0\u52a8\u6027\u80fd\u5e76\u5bfb\u627e\u6700\u4f18\u6e38\u6cf3\u7b56\u7565\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u67d4\u6027\u6269\u5c55\u7684Purcell\u4e09\u8fde\u6746\u6e38\u6cf3\u5668\uff0c\u901a\u8fc7\u5728\u5173\u8282\u5904\u4e32\u8054\u5f39\u7c27\u6765\u5b9e\u73b0\u3002\u5229\u7528\u963b\u529b\u7406\u8bba\u63a8\u5bfc\u4e86\u8eab\u4f53\u52a8\u529b\u5b66\uff0c\u5e76\u5c06\u51e0\u4f55\u529b\u5b66\u7eb3\u5165\u8fd0\u52a8\u9884\u6d4b\u548c\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u4ee5\u8bc6\u522b\u63a7\u5236\u67d4\u6027\u6e38\u6cf3\u5668\u5b9e\u73b0\u6700\u5927\u4f4d\u79fb\u7684\u7b56\u7565\u3002", "result": "\u7814\u7a76\u5728\u7269\u7406\u7535\u7f06\u9a71\u52a8\u7684\u4e09\u8fde\u6746\u65e0\u80a2\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5e76\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u5c55\u793a\u4e86\u5728\u53d8\u5316\u7684\u7f16\u7a0b\u3001\u72b6\u6001\u4f9d\u8d56\u6027\u67d4\u6027\u4e0b\u5bf9\u8fd0\u52a8\u6027\u80fd\u7684\u51c6\u786e\u9884\u6d4b\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u63a7\u5236\u67d4\u6027\u6e38\u6cf3\u8fd0\u52a8\uff0c\u5f3a\u8c03\u4e86\u67d4\u6027\u4f5c\u4e3a\u4e00\u79cd\u8bbe\u8ba1\u7279\u5f81\uff0c\u53ef\u4ee5\u5728\u540c\u8d28\u548c\u5f02\u8d28\u73af\u5883\u4e2d\u88ab\u5229\u7528\u4ee5\u5b9e\u73b0\u7a33\u5065\u8fd0\u52a8\u3002"}}
{"id": "2510.03377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u548cRIPG\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u5236\u9020\u4e1a\u4e2d\u7684\u80fd\u6e90\u6548\u7387\u8c03\u5ea6\u95ee\u9898\u3002", "motivation": "\u4e0d\u53ef\u518d\u751f\u80fd\u6e90\u7a00\u7f3a\u3001\u4f9b\u5e94\u94fe\u5730\u7f18\u653f\u6cbb\u95ee\u9898\u3001\u4ef7\u683c\u4e0a\u6da8\u53ca\u6c14\u5019\u53d8\u5316\u8feb\u4f7f\u5236\u9020\u4e1a\u5bfb\u6c42\u66f4\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u589e\u5f3a\u7684epsilon\u7ea6\u675f\u65b9\u6cd5\u548c\u591a\u76ee\u6807\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5RIPG\u6765\u89e3\u51b3BHFS\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5c0f\u3001\u4e2d\u3001\u5927\u89c4\u6a21\u5b9e\u4f8b\u9a8c\u8bc1\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5e73\u8861\u5b8c\u5de5\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684\u591a\u76ee\u6807\u6df7\u5408\u6574\u6570\u89c4\u5212\u6a21\u578b\u548cRIPG\u7b97\u6cd5\u5728\u89e3\u51b3BHFS\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u5b8c\u5de5\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u3002"}}
{"id": "2510.04310", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04310", "abs": "https://arxiv.org/abs/2510.04310", "authors": ["Hagit Attiya", "Itay Flam", "Jennifer L. Welch"], "title": "Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience", "comment": "31 pages, 4 figures, 1 table, 5 algorithms", "summary": "We study communication abstractions for asynchronous Byzantine fault\ntolerance with optimal failure resilience, where $n > 3f$. Two classic patterns\n-- canonical asynchronous rounds and communication-closed layers -- have long\nbeen considered as general frameworks for designing distributed algorithms,\nmaking asynchronous executions appear synchronous and enabling modular\nreasoning.\n  We show that these patterns are inherently limited in the critical resilience\nregime $3f < n \\le 5f$. Several key tasks -- such as approximate and crusader\nagreement, reliable broadcast and gather -- cannot be solved by bounded-round\ncanonical-round algorithms, and are unsolvable if communication closure is\nimposed. These results explain the historical difficulty of achieving\noptimal-resilience algorithms within round-based frameworks.\n  On the positive side, we show that the gather abstraction admits\nconstant-time solutions with optimal resilience ($n > 3f$), and supports\nmodular reductions. Specifically, we present the first optimally-resilient\nalgorithm for connected consensus by reducing it to gather.\n  Our results demonstrate that while round-based abstractions are analytically\nconvenient, they obscure the true complexity of Byzantine fault-tolerant\nalgorithms. Richer communication patterns such as gather provide a better\nfoundation for modular, optimal-resilience design.", "AI": {"tldr": "\u7ecf\u5178\u5f02\u6b65\u901a\u4fe1\u6a21\u5f0f\u5728\u6700\u4f18\u5bb9\u9519\u8303\u56f4\u5185\u5b58\u5728\u5c40\u9650\uff0c\u4f46gather\u62bd\u8c61\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u63a2\u7d22\u5728\u5f02\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u8bbe\u8ba1\u5177\u6709\u6700\u4f18\u5bb9\u9519\u80fd\u529b\uff08n > 3f\uff09\u7684\u901a\u4fe1\u62bd\u8c61\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e24\u79cd\u7ecf\u5178\u6a21\u5f0f\uff08\u89c4\u8303\u5f02\u6b65\u8f6e\u6b21\u548c\u901a\u4fe1\u5c01\u95ed\u5c42\uff09\u5728\u5173\u952e\u5bb9\u9519\u8303\u56f4\uff083f < n \u2264 5f\uff09\u5185\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86gather\u62bd\u8c61\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u53d1\u73b0\u7ecf\u5178\u6a21\u5f0f\u5728\u5173\u952e\u5bb9\u9519\u8303\u56f4\u5185\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46gather\u62bd\u8c61\u652f\u6301\u6052\u5b9a\u65f6\u95f4\u89e3\u51b3\u65b9\u6848\u548c\u6a21\u5757\u5316\u5f52\u7ea6\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u8fde\u63a5\u5171\u8bc6\u7684\u6700\u4f18\u5bb9\u9519\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u8f6e\u6b21\u7684\u62bd\u8c61\u867d\u7136\u5728\u5206\u6790\u4e0a\u65b9\u4fbf\uff0c\u4f46\u63a9\u76d6\u4e86\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u7684\u771f\u6b63\u590d\u6742\u6027\u3002\u66f4\u4e30\u5bcc\u7684\u901a\u4fe1\u6a21\u5f0f\uff08\u5982gather\uff09\u4e3a\u6a21\u5757\u5316\u3001\u6700\u4f18\u5bb9\u9519\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840\u3002"}}
{"id": "2510.03433", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03433", "abs": "https://arxiv.org/abs/2510.03433", "authors": ["\u00c1ron Samuel Kov\u00e1cs", "Pedro Hermosilla", "Renata G. Raidou"], "title": "Style Brush: Guided Style Transfer for 3D Objects", "comment": null, "summary": "We introduce Style Brush, a novel style transfer method for textured meshes\ndesigned to empower artists with fine-grained control over the stylization\nprocess. Our approach extends traditional 3D style transfer methods by\nintroducing a novel loss function that captures style directionality, supports\nmultiple style images or portions thereof, and enables smooth transitions\nbetween styles in the synthesized texture. The use of easily generated guiding\ntextures streamlines user interaction, making our approach accessible to a\nbroad audience. Extensive evaluations with various meshes, style images, and\ncontour shapes demonstrate the flexibility of our method and showcase the\nvisual appeal of the generated textures.", "AI": {"tldr": "Style Brush\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u5f15\u5bfc\u7eb9\u7406\uff0c\u63d0\u4f9b\u7cbe\u7ec6\u63a7\u5236\u5e76\u7b80\u5316\u7528\u6237\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u827a\u672f\u5bb6\u7fa4\u4f53\u3002", "motivation": "\u65e8\u5728\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u5bf9\u98ce\u683c\u5316\u8fc7\u7a0b\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u540c\u65f6\u7b80\u5316\u7528\u6237\u4ea4\u4e92\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u53d7\u4f17\u80fd\u591f\u4f7f\u7528\u8be5\u6280\u672f\u3002", "method": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u4f20\u7edf\u76843D\u98ce\u683c\u8fc1\u79fb\u6280\u672f\uff0c\u5f15\u5165\u4e86\u80fd\u591f\u6355\u6349\u98ce\u683c\u65b9\u5411\u6027\u7684\u65b0\u9896\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u591a\u79cd\u98ce\u683c\u56fe\u50cf\u6216\u5176\u90e8\u5206\uff0c\u5e76\u5b9e\u73b0\u5408\u6210\u7eb9\u7406\u4e2d\u98ce\u683c\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7f51\u683c\u3001\u98ce\u683c\u56fe\u50cf\u548c\u8f6e\u5ed3\u5f62\u72b6\u4e0a\u5177\u6709\u7075\u6d3b\u6027\uff0c\u751f\u6210\u7684\u7eb9\u7406\u5177\u6709\u89c6\u89c9\u5438\u5f15\u529b\u3002", "conclusion": "Style Brush\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u548c\u6613\u4e8e\u751f\u6210\u7684\u5f15\u5bfc\u7eb9\u7406\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u4e86\u7cbe\u7ec6\u7684\u98ce\u683c\u5316\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u7f51\u683c\u548c\u98ce\u683c\u56fe\u50cf\u4e0a\u7684\u7075\u6d3b\u6027\u548c\u89c6\u89c9\u5438\u5f15\u529b\u3002"}}
{"id": "2510.04435", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.04435", "abs": "https://arxiv.org/abs/2510.04435", "authors": ["Shaofeng H. -C. Jiang", "Pan Peng", "Haoze Wang"], "title": "Streaming Max-Cut in General Metrics", "comment": null, "summary": "Max-Cut is a fundamental combinatorial optimization problem that has been\nstudied in various computational settings. In this work, we initiate the study\nof its streaming complexity in general metric spaces with access to distance\noracles. We give a $(1 + \\epsilon)$-approximation algorithm for estimating the\nMax-Cut value sliding-window streams using only poly-logarithmic space. This is\nthe first sliding-window algorithm for Max-Cut even in Euclidean spaces, and it\nachieves a similar error-space tradeoff as the state-of-the-art insertion-only\nalgorithms in Euclidean settings [Chen, Jiang, Krauthgamer, STOC'23], but\nwithout relying on Euclidean structures. In sharp contrast, we prove a\npolynomial-space lower bound for any $\\mathrm{poly}(n)$-approximation in the\ndynamic streaming setting. This yields a separation from the Euclidean case,\nwhere the polylogarithmic-space $(1+\\epsilon)$-approximation extends to dynamic\nstreams.\n  On the technical side, our sliding-window algorithm builds on the smooth\nhistogram framework of [Braverman and Ostrovsky, SICOMP'10]. To make this\nframework applicable, we establish the first smoothness bound for metric\nMax-Cut. Moreover, we develop a streaming algorithm for metric Max-Cut in\ninsertion-only streams, whose key ingredient is a new metric reservoir sampling\ntechnique.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u6ed1\u52a8\u7a97\u53e3\u6d41\u4e2d\u5b9e\u73b0\u4e86Max-Cut\u7684\u591a\u5bf9\u6570\u7a7a\u95f4$(1 + \\epsilon)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u52a8\u6001\u6d41\u4e2d\u7684\u591a\u9879\u5f0f\u7a7a\u95f4\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76Max-Cut\u5728\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u6d41\u5f0f\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u6ed1\u52a8\u7a97\u53e3\u6d41\u548c\u52a8\u6001\u6d41\u4e2d\u7684\u8fd1\u4f3c\u7b97\u6cd5\u53ca\u5176\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "method": "\u57fa\u4e8eBraverman\u548cOstrovsky\u7684\u5e73\u6ed1\u76f4\u65b9\u56fe\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u5ea6\u91cfMax-Cut\u7684\u7b2c\u4e00\u4e2a\u5e73\u6ed1\u6027\u8fb9\u754c\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6c34\u5e93\u91c7\u6837\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u6ed1\u52a8\u7a97\u53e3\u6d41\u4e2d\u7684$(1 + \\epsilon)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4f7f\u7528\u591a\u5bf9\u6570\u7a7a\u95f4\uff0c\u5e76\u8bc1\u660e\u4e86\u52a8\u6001\u6d41\u4e2d\u7684\u591a\u9879\u5f0f\u7a7a\u95f4\u4e0b\u754c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u6ed1\u52a8\u7a97\u53e3\u6d41\u4e2d\u4f7f\u7528\u591a\u5bf9\u6570\u7a7a\u95f4\u4f30\u8ba1Max-Cut\u503c\u7684$(1 + \\epsilon)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u52a8\u6001\u6d41\u8bbe\u7f6e\u4e2d\u7684\u591a\u9879\u5f0f\u7a7a\u95f4\u4e0b\u754c\uff0c\u4ece\u800c\u4e0e\u6b27\u51e0\u91cc\u5f97\u60c5\u51b5\u5f62\u6210\u5bf9\u6bd4\u3002"}}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u9884\u6d4b\u4ee3\u7801\u7247\u6bb5\u7684\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u800c\u975e\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u6765\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u76f8\u5bf9\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u7edd\u5bf9\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u6307\u6807\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u65f6\u51c6\u786e\u6027\u6709\u9650\uff0c\u8fd9\u6e90\u4e8e\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u6570\u636e\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u3002", "method": "\u4f7f\u7528\u5305\u542b150\u4e2aJava\u4ee3\u7801\u7247\u6bb5\u548c12.5k\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u6d4b\u91cf\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6bd4\u8f83\u7edd\u5bf9\u548c\u76f8\u5bf9\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u9884\u6d4b\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u6a21\u578b\u6700\u591a\u6bd4\u57fa\u7ebf\u63d0\u9ad833.4%\uff0c\u800c\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u6a21\u578b\u7684\u5e73\u5747\u63d0\u5347\u5206\u522b\u8fbe\u5230137.8%\uff08\u7247\u6bb5\u7ea7\uff09\u548c74.7%\uff08\u5f00\u53d1\u8005\u7ea7\uff09\u3002", "conclusion": "\u76f8\u5bf9\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u9884\u6d4b\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u7edd\u5bf9\u9884\u6d4b\u6a21\u578b\uff0c\u652f\u6301\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.03524", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.03524", "abs": "https://arxiv.org/abs/2510.03524", "authors": ["Mohammad Reza Akbari", "Hamid Barati", "Ali Barati"], "title": "A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem", "comment": null, "summary": "Fog computing integrates cloud and edge resources. According to an\nintelligent and decentralized method, this technology processes data generated\nby IoT sensors to seamlessly integrate physical and cyber environments.\nInternet of Things uses wireless and smart objects. They communicate with each\nother, monitor the environment, collect information, and respond to user\nrequests. These objects have limited energy resources since they use batteries\nto supply energy. Also, they cannot replace their batteries. As a result, the\nnetwork lifetime is limited and short. Thus, reducing energy consumption and\naccelerating the data transmission process are very important challenges in IoT\nnetworks to reduce the response time. In the data transmission process,\nselecting an appropriate cluster head node is very important because it can\nreduce the delay when sending data to the fog. In this paper, cluster head\nnodes are selected based on several important criteria such as distance,\nresidual energy, received signal strength, and link expiration time. Then,\nobjects send the processed data to the server hierarchically through a balanced\ntree. The simulation results show that the proposed method outperforms the\nenergy-efficient centroid-based routing protocol (EECRP) and the Emergency\nResponse IoT based on Global Information Decision (ERGID) in terms of packet\ndelivery rate, delay, response time, and network lifetime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6807\u51c6\u9009\u62e9\u7c07\u5934\u8282\u70b9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6811\u4f20\u8f93\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u8054\u7f51\u7f51\u7edc\u7684\u6027\u80fd\u548c\u5bff\u547d\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u91cf\u6709\u9650\u4e14\u65e0\u6cd5\u66f4\u6362\u7535\u6c60\uff0c\u5bfc\u81f4\u7f51\u7edc\u5bff\u547d\u77ed\uff0c\u56e0\u6b64\u964d\u4f4e\u80fd\u8017\u548c\u52a0\u901f\u6570\u636e\u4f20\u8f93\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u8ddd\u79bb\u3001\u5269\u4f59\u80fd\u91cf\u3001\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u548c\u94fe\u8def\u8fc7\u671f\u65f6\u95f4\u7b49\u6807\u51c6\u9009\u62e9\u7c07\u5934\u8282\u70b9\uff0c\u5e76\u901a\u8fc7\u5e73\u8861\u6811\u5c42\u6b21\u5316\u4f20\u8f93\u6570\u636e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u534f\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u5305\u4f20\u9012\u7387\u3001\u5ef6\u8fdf\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u7f51\u7edc\u5bff\u547d\u65b9\u9762\u4f18\u4e8eEECRP\u548cERGID\u534f\u8bae\u3002"}}
{"id": "2510.03460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03460", "abs": "https://arxiv.org/abs/2510.03460", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching", "comment": null, "summary": "Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)\nsystems, as robots need to respond to dynamic environments in real time by\ncontinuously observing their surroundings and replanning their motions to\nensure both safe interactions and efficient task execution. Current\nsampling-based motion planners face challenges in scaling to high-dimensional\nconfiguration spaces and often require post-processing to interpolate and\nsmooth the generated paths, resulting in time inefficiency in complex\nenvironments. Optimization-based planners, on the other hand, can incorporate\nmultiple constraints and generate smooth trajectories directly, making them\npotentially more time-efficient. However, optimization-based planners are\nsensitive to initialization and may get stuck in local minima. In this work, we\npresent a novel learning-based method that utilizes a Flow Matching model\nconditioned on a single-view point cloud to learn near-optimal solutions for\noptimization initialization. Our method does not require prior knowledge of the\nenvironment, such as obstacle locations and geometries, and can generate\nfeasible trajectories directly from single-view depth camera input. Simulation\nstudies on a UR5e robotic manipulator in cluttered workspaces demonstrate that\nthe proposed generative initializer achieves a high success rate on its own,\nsignificantly improves the success rate of trajectory optimization compared\nwith traditional and learning-based benchmark initializers, requires fewer\noptimization iterations, and exhibits strong generalization to unseen\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFlow Matching\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u4f18\u5316\u521d\u59cb\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u4f18\u5316\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u91c7\u6837\u548c\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u6269\u5c55\u6027\u5dee\u3001\u65f6\u95f4\u6548\u7387\u4f4e\u4ee5\u53ca\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u57fa\u4e8eFlow Matching\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u5355\u89c6\u89d2\u70b9\u4e91\u6570\u636e\uff0c\u5b66\u4e60\u751f\u6210\u4f18\u5316\u7684\u521d\u59cb\u89e3\uff0c\u65e0\u9700\u73af\u5883\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6742\u4e71\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u51c6\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4e14\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u66f4\u5c11\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u8fc7Flow Matching\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u751f\u6210\u4f18\u5316\u7684\u521d\u59cb\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u4f18\u5316\u7684\u6210\u529f\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u77e5\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03399", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u5c11\u6570\u6a21\u578b\u80fd\u6b63\u786e\u8bc6\u522b\u81ea\u8eab\u751f\u6210\u6587\u672c\uff0c\u4e14\u5b58\u5728\u5bf9GPT\u548cClaude\u5bb6\u65cf\u7684\u062d\u0644\u0629\u504f\u89c1\u3002\u8fd9\u5bf9AI\u5b89\u5168\u6027\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u53d7\u5230\u6a21\u578b\u662f\u5426\u5177\u5907\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u7684\u77db\u76fe strong \u89e3\u91ca\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u53ef\u8f7b\u677e\u5e94\u7528\u548c\u66f4\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u91cf\u4e8610\u4e2a\u0c02\u0c26\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u4e8c\u5143\u81ea\u6211\u8bc6\u522b\u548c\u7cbe\u786e\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u81ea\u6211\u8bc6\u522b\u4e0a\u8868\u73b0 consistently \u5931\u8d25\uff0c\u4ec54/10\u6a21\u578b\u80fd\u6b63\u786e\u9884\u6d4b\u81ea\u8eab\u751f\u6210\u6587\u672c\uff0c\u4e14\u6027\u80fd rarely \u9ad8\u4e8e\u968f\u673a chance\u3002\u6a21\u578b\u5bf9GPT\u548cClaude\u5bb6\u65cf\u8868\u73b0\u51fa strong \u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u6027\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5f00\u53d1\u9002\u5f53AI\u81ea\u6211\u610f\u8bc6\u7684\u65b9\u5411\u3002"}}
{"id": "2510.04404", "categories": ["cs.DC", "cs.PF", "68M14, 68T05, 90C59", "C.2.4; D.4.4; D.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.04404", "abs": "https://arxiv.org/abs/2510.04404", "authors": ["Jahidul Arafat", "Fariha Tasmin", "Sanjaya Poudel", "Ahsan Habib Tareq"], "title": "Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks", "comment": "45 pages, 8 tables, 1 figure. Comprehensive evaluation of 12\n  messaging frameworks with AI-enhanced orchestration system", "summary": "Modern distributed systems demand low-latency, fault-tolerant event\nprocessing that exceeds traditional messaging architecture limits. While\nframeworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and\nserverless event buses have matured significantly, no unified comparative study\nevaluates them holistically under standardized conditions. This paper presents\nthe first comprehensive benchmarking framework evaluating 12 messaging systems\nacross three representative workloads: e-commerce transactions, IoT telemetry\ningestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event\nOrchestration), employing machine learning-driven predictive scaling,\nreinforcement learning for dynamic resource allocation, and multi-objective\noptimization. Our evaluation reveals fundamental trade-offs: Apache Kafka\nachieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires\nsubstantial operational expertise; Apache Pulsar provides balanced performance\n(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions\noffer elastic scaling for variable workloads despite higher baseline latency\n(80-120ms p95). AIEO demonstrates 34\\% average latency reduction, 28\\% resource\nutilization improvement, and 42% cost optimization across all platforms. We\ncontribute standardized benchmarking methodologies, open-source intelligent\norchestration, and evidence-based decision guidelines. The evaluation\nencompasses 2,400+ experimental configurations with rigorous statistical\nanalysis, providing comprehensive performance characterization and establishing\nfoundations for next-generation distributed system design.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf912\u79cd\u6d88\u606f\u7cfb\u7edf\u8fdb\u884c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faAIEO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u5bb9\u9519\u4e8b\u4ef6\u5904\u7406\u7684\u9700\u6c42\u8d85\u51fa\u4e86\u4f20\u7edf\u6d88\u606f\u67b6\u6784\u7684\u9650\u5236\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u7edf\u4e00\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u5f15\u5165AIEO\u6846\u67b6\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9884\u6d4b\u6269\u5c55\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5bf912\u79cd\u6d88\u606f\u7cfb\u7edf\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8fdb\u884c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "AIEO\u5c55\u793a\u4e8634%\u7684\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e\u300128%\u7684\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u548c42%\u7684\u6210\u672c\u4f18\u5316\u3002Apache Kafka\u5728\u5cf0\u503c\u541e\u5410\u91cf\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cApache Pulsar\u5728\u591a\u79df\u6237\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u65e0\u670d\u52a1\u5668\u89e3\u51b3\u65b9\u6848\u5728\u5f39\u6027\u6269\u5c55\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86AIEO\uff08AI\u589e\u5f3a\u4e8b\u4ef6\u7f16\u6392\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9884\u6d4b\u6269\u5c55\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5904\u7406\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03434", "categories": ["cs.GR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03434", "abs": "https://arxiv.org/abs/2510.03434", "authors": ["Zhiying Jiang", "Raihan Seraj", "Marcos Villagra", "Bidhan Roy"], "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model", "comment": null, "summary": "We present Paris, the first publicly released diffusion model pre-trained\nentirely through decentralized computation. Paris demonstrates that\nhigh-quality text-to-image generation can be achieved without centrally\ncoordinated infrastructure. Paris is open for research and commercial use.\nParis required implementing our Distributed Diffusion Training framework from\nscratch. The model consists of 8 expert diffusion models (129M-605M parameters\neach) trained in complete isolation with no gradient, parameter, or\nintermediate activation synchronization. Rather than requiring synchronized\ngradient updates across thousands of GPUs, we partition data into semantically\ncoherent clusters where each expert independently optimizes its subset while\ncollectively approximating the full distribution. A lightweight transformer\nrouter dynamically selects appropriate experts at inference, achieving\ngeneration quality comparable to centrally coordinated baselines. Eliminating\nsynchronization enables training on heterogeneous hardware without specialized\ninterconnects. Empirical validation confirms that Paris's decentralized\ntraining maintains generation quality while removing the dedicated GPU cluster\nrequirement for large-scale diffusion models. Paris achieves this using\n14$\\times$ less training data and 16$\\times$ less compute than the prior\ndecentralized baseline.", "AI": {"tldr": "Paris\u662f\u9996\u4e2a\u5b8c\u5168\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u4e2d\u5fc3\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\uff0c\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "motivation": "\u63a2\u7d22\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u5728\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u6d88\u9664\u5bf9\u4e13\u7528GPU\u96c6\u7fa4\u548c\u540c\u6b65\u68af\u5ea6\u66f4\u65b0\u7684\u4f9d\u8d56\u3002", "method": "Paris\u75318\u4e2a\u4e13\u5bb6\u6269\u6563\u6a21\u578b\u7ec4\u6210\uff0c\u6bcf\u4e2a\u6a21\u578b\u5728\u5b8c\u5168\u9694\u79bb\u7684\u73af\u5883\u4e0b\u8bad\u7ec3\uff0c\u65e0\u9700\u68af\u5ea6\u3001\u53c2\u6570\u6216\u4e2d\u95f4\u6fc0\u6d3b\u540c\u6b65\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7Transformer\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4e2d\u5fc3\u534f\u8c03\u57fa\u7ebf\u76f8\u5f53\u7684\u751f\u6210\u8d28\u91cf\u3002", "result": "Paris\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e8614\u500d\u8bad\u7ec3\u6570\u636e\u548c16\u500d\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\uff0c\u4e14\u53ef\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u8bad\u7ec3\u3002", "conclusion": "Paris\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u53ef\u4ee5\u5728\u6ca1\u6709\u4e2d\u5fc3\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6269\u6563\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002"}}
{"id": "2510.04737", "categories": ["cs.DS", "math.OC", "F.2; G.2"], "pdf": "https://arxiv.org/pdf/2510.04737", "abs": "https://arxiv.org/abs/2510.04737", "authors": ["Yusuf Amidu", "Khaled Elbassioni", "Adriana F. Gabor"], "title": "Online Multiple Resource Allocation Problems with Departures via the Primal-Dual Approach", "comment": null, "summary": "In this paper we propose primal-dual algorithms for different variants of the\nonline resource allocation problem with departures. In the basic variant,\nrequests (items) arrive over time to a set of resources (knapsacks) and upon\narrival, the duration of time a request may occupy a resource, the demand and\nreward if the request can be granted, become known. %We assume that the\nduration of stay of a request may depend on the resource. %and that resources\nmay have different capacity sizes. The goal of the algorithm is to decide\nwhether to accept/reject a request upon arrival and to which resource to\nallocate it such that the reward obtained over time is maximized. Under some\nmild assumptions, we show that the proposed primal-dual algorithm achieves a\ncompetitive ratio of $O\\big(\\log(\\bar\\theta^{\\max}\\cdot\\bar d^{\\max})\\big)$,\nwhere $\\bar \\theta^{\\max}$ is the maximum value density fluctuation ratio and\n$\\bar d^{\\max}$ is the maximum duration fluctuation ratio. We prove similar\nresults for two other variants, namely, one with an additional load balancing\nconstraint, and the multi-dimensional variant where an admitted request\nconsumes capacity on multiple resources. Our results show that the primal-dual\napproach offers a simple, unified framework for obtaining competitive ratios\ncomparable to those previously obtained via threshold policies known for these\nproblems. Additionally, we show that this framework allows us to incorporate\nadditional constraints, such as load-balancing constraints, without sacrificing\nthe competitive ratio.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\u5904\u7406\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ade\u4e89\u6bd4\u5e76\u652f\u6301\u989d\u5916\u7ea6\u675f\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e2d\u8bf7\u6c42\u7684\u52a8\u6001\u5230\u8fbe\u548c\u8d44\u6e90\u5206\u914d\u7684\u590d\u6742\u6027\uff0c\u76ee\u6807\u662f\u6700\u5927\u5316\u968f\u65f6\u95f4\u83b7\u5f97\u7684\u5956\u52b1\u3002", "method": "\u4f7f\u7528\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\u5904\u7406\u4e0d\u540c\u53d8\u4f53\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5305\u62ec\u57fa\u672c\u53d8\u4f53\u3001\u5e26\u8d1f\u8f7d\u5e73\u8861\u7ea6\u675f\u7684\u53d8\u4f53\u548c\u591a\u7ef4\u53d8\u4f53\u3002", "result": "\u7b97\u6cd5\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6bd4\u4e3a$O\\big(\\log(\\bar\\theta^{\\max}\\cdot\\bar d^{\\max})\\big)$\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u53d8\u4f53\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\u4e3a\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u7ade\u4e89\u6bd4\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u989d\u5916\u7ea6\u675f\uff08\u5982\u8d1f\u8f7d\u5e73\u8861\u7ea6\u675f\uff09\u3002"}}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316Java\u4ee3\u7801\u5e93\u4e2d\u7684\u5e93\u66f4\u65b0\uff0c\u51cf\u5c11\u4e86\u7ef4\u62a4\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u517c\u5bb9\u6027\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u5e93\u7684\u6269\u5c55\uff0c\u5e93\u4f9d\u8d56\u53ef\u80fd\u8fc7\u65f6\uff0c\u624b\u52a8\u66f4\u65b0\u8017\u65f6\u4e14\u6613\u5f15\u5165\u7834\u574f\u6027\u53d8\u66f4\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u7531\u591a\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a\u6458\u8981\u4ee3\u7406\u3001\u63a7\u5236\u4ee3\u7406\u548c\u4ee3\u7801\u4ee3\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u5b9a\u4f4d\u548c\u4fee\u590d\u4ee3\u7801\u4e2d\u7684\u5e93\u66f4\u65b0\u95ee\u9898\u3002", "result": "\u5728\u5de5\u4e1a\u7528\u4f8b\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u4f7f\u7528\u7684token\u66f4\u5c11\uff0c\u4e14\u8fbe\u5230\u4e8671.4%\u7684\u7cbe\u786e\u5ea6\uff0c\u5c55\u73b0\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u4ee3\u7406\u548c\u8fc1\u79fb\u6587\u6863\uff0c\u6709\u6548\u5b9e\u73b0\u4e86Java\u4ee3\u7801\u5e93\u4e2d\u5e93\u66f4\u65b0\u7684\u81ea\u52a8\u63a8\u8350\u548c\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ef4\u62a4\u6548\u7387\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2510.03533", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.03533", "abs": "https://arxiv.org/abs/2510.03533", "authors": ["Mohammad Reza Akbari", "Hamid Barati", "Ali Barati"], "title": "An efficient grey theory-driven path selection for energy efficiency control in the Internet of Things using fog and cloud computing", "comment": null, "summary": "Due to the big data exchange on the Internet of Things, proper routing and\nselecting the best routes for fast data transmission improve network\nperformance. There are major challenges, like high delay, when cloud computing\nis used. Therefore, one solution is to use other schemes, such as fog\ncomputing. In fog computing, all data is not sent to the cloud and the fog\nnodes close to objects are used for data processing. This reduces the network\ndelay. In this paper, we propose an overlapping clustering method called\nMFCT-IoT to select the best cluster head nodes to guarantee the fast data\ntransfer from objects to fog nodes. The selected cluster head nodes are\nresponsible for sending the collected data to the closest fog nodes in the\nnetwork edge. Upon receiving the data, the fog nodes process it, and if a\nresponse is ready, they respond immediately to the object. Otherwise, they\nmerge and transmit the data to the cloud servers, which are considered as the\nroot node of the proposed hierarchical tree. After processing, the merged data\nis sent to the object. We compare the proposed scheme with two schemes,\nincluding ERGID and EECRP. These schemes are evaluated based on various\ncriteria, including the response time, packet delivery ratio, end-to-end delay,\nnetwork lifetime, and energy consumption. The results indicate that the\nproposed method outperforms others in terms of all criteria.", "AI": {"tldr": "\u63d0\u51faMFCT-IoT\u91cd\u53e0\u805a\u7c7b\u65b9\u6cd5\u4f18\u5316\u7269\u8054\u7f51\u6570\u636e\u8def\u7531\uff0c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u7269\u8054\u7f51\u5927\u6570\u636e\u4ea4\u6362\u4e2d\u4e91\u8ba1\u7b97\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u96fe\u8ba1\u7b97\u901a\u8fc7\u5c31\u8fd1\u5904\u7406\u6570\u636e\u51cf\u5c11\u7f51\u7edc\u5ef6\u8fdf\u3002", "method": "\u91c7\u7528\u91cd\u53e0\u805a\u7c7b\u65b9\u6cd5MFCT-IoT\uff0c\u9009\u62e9\u6700\u4f73\u7c07\u5934\u8282\u70b9\uff0c\u786e\u4fdd\u6570\u636e\u4ece\u5bf9\u8c61\u5feb\u901f\u4f20\u8f93\u81f3\u96fe\u8282\u70b9\u3002", "result": "MFCT-IoT\u5728\u5404\u9879\u8bc4\u4f30\u6807\u51c6\u4e2d\u8868\u73b0\u4f18\u4e8eERGID\u548cEECRP\u3002", "conclusion": "\u63d0\u51fa\u7684MFCT-IoT\u65b9\u6cd5\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u6570\u636e\u5305\u4f20\u9012\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u7f51\u7edc\u751f\u547d\u5468\u671f\u548c\u80fd\u8017\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6848ERGID\u548cEECRP\u3002"}}
{"id": "2510.03471", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03471", "abs": "https://arxiv.org/abs/2510.03471", "authors": ["Dingqi Zhang", "Ran Tao", "Sheng Cheng", "Naira Hovakimyan", "Mark W. Mueller"], "title": "A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control", "comment": null, "summary": "Robust adaptive control methods are essential for maintaining quadcopter\nperformance under external disturbances and model uncertainties. However,\nfragmented evaluations across tasks, simulators, and implementations hinder\nsystematic comparison of these methods. This paper introduces an\neasy-to-deploy, modular simulation testbed for quadcopter control, built on\nRotorPy, that enables evaluation under a wide range of disturbances such as\nwind, payload shifts, rotor faults, and control latency. The framework includes\na library of representative adaptive and non-adaptive controllers and provides\ntask-relevant metrics to assess tracking accuracy and robustness. The unified\nmodular environment enables reproducible evaluation across control methods and\neliminates redundant reimplementation of components such as disturbance models,\ntrajectory generators, and analysis tools. We illustrate the testbed's\nversatility through examples spanning multiple disturbance scenarios and\ntrajectory types, including automated stress testing, to demonstrate its\nutility for systematic analysis. Code is available at\nhttps://github.com/Dz298/AdaptiveQuadBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7edf\u4e00\u8bc4\u4f30\u56db\u65cb\u7ffc\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u5206\u6563\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u56db\u65cb\u7ffc\u63a7\u5236\u65b9\u6cd5\u5728\u4efb\u52a1\u3001\u6a21\u62df\u5668\u548c\u5b9e\u73b0\u4e0a\u7684\u5206\u6563\u8bc4\u4f30\uff0c\u96be\u4ee5\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6613\u4e8e\u90e8\u7f72\u7684\u6a21\u5757\u5316\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u591a\u79cd\u4ee3\u8868\u6027\u7684\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u4efb\u52a1\u76f8\u5173\u6307\u6807\u6765\u8bc4\u4f30\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u5e73\u53f0\u652f\u6301\u591a\u79cd\u5e72\u6270\u573a\u666f\u548c\u8f68\u8ff9\u7c7b\u578b\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u81ea\u52a8\u538b\u529b\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u7cfb\u7edf\u6027\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eRotorPy\u7684\u6a21\u5757\u5316\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u7edf\u4e00\u8bc4\u4f30\u56db\u65cb\u7ffc\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u8bc4\u4f30\u5206\u6563\u7684\u95ee\u9898\u3002"}}
{"id": "2510.03418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ContraGen\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f01\u4e1a\u6587\u6863\u4e2d\u7684\u77db\u76fe\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u4eba\u5de5\u9a8c\u8bc1\u63d0\u5347RAG\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u77db\u76fe\u68c0\u6d4b\u57fa\u51c6\u4ec5\u9650\u4e8e\u53e5\u5b50\u7ea7\u5206\u6790\uff0c\u65e0\u6cd5\u6355\u6349\u4f01\u4e1a\u6587\u6863\uff08\u5982\u5408\u540c\u3001\u8d22\u52a1\u6587\u4ef6\u3001\u5408\u89c4\u62a5\u544a\u6216\u653f\u7b56\u624b\u518c\uff09\u7684\u590d\u6742\u6027\uff0c\u8fd9\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u5408\u89c4\u6027\u3001\u6cbb\u7406\u548c\u95ee\u8d23\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86ContraGen\uff0c\u4e00\u4e2a\u9488\u5bf9\u4f01\u4e1a\u9886\u57df\u7684\u77db\u76fe\u611f\u77e5\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5e26\u6709\u5d4c\u5165\u77db\u76fe\u7684\u5408\u6210\u4f01\u4e1a\u98ce\u683c\u6587\u6863\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u77db\u76fe\u6316\u6398\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u6587\u6863\u5185\u548c\u8de8\u6587\u6863\u7684\u4e00\u81f4\u6027\u3002", "result": "ContraGen\u6846\u67b6\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u4f01\u4e1a\u6587\u6863\uff0c\u5efa\u6a21\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\u5e38\u89c1\u7684\u77db\u76fe\u7c7b\u578b\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u77db\u76fe\u611f\u77e5\u7684\u68c0\u7d22\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7ed3\u5408\u4eba\u5de5\u76d1\u7763\u4ee5\u53cd\u6620\u7279\u5b9a\u9886\u57df\u7684\u5224\u65ad\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u4f01\u4e1a\u4fe1\u606f\u68c0\u7d22\u5e94\u7528\u4e2d\u5efa\u7acb\u66f4\u53ef\u4fe1\u548c\u8d1f\u8d23\u4efb\u7684RAG\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5176\u4e2d\u68c0\u6d4b\u548c\u89e3\u51b3\u77db\u76fe\u5bf9\u4e8e\u964d\u4f4e\u98ce\u9669\u548c\u786e\u4fdd\u5408\u89c4\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04644", "abs": "https://arxiv.org/abs/2510.04644", "authors": ["Hirotsugu Kakugawa", "Sayaka Kamei", "Masahiro Shibata", "Fukuhito Ooshita"], "title": "The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms", "comment": null, "summary": "Self-stabilization is a versatile methodology in the design of fault-tolerant\ndistributed algorithms for transient faults. A self-stabilizing system\nautomatically recovers from any kind and any finite number of transient faults.\nThis property is specifically useful in modern distributed systems with a large\nnumber of components. In this paper, we propose a new communication and\nexecution model named the R(1)W(1) model in which each process can read and\nwrite its own and neighbors' local variables in a single step. We propose\nself-stabilizing distributed algorithms in the R(1)W(1) model for the problems\nof maximal matching, minimal k-dominating set and maximal k-dependent set.\nFinally, we propose an example transformer, based on randomized distance-two\nlocal mutual exclusion, to simulate algorithms designed for the R(1)W(1) model\nin the synchronous message passing model with synchronized clocks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR(1)W(1)\u6a21\u578b\u53ca\u81ea\u7a33\u5b9a\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6700\u5927\u5339\u914d\u7b49\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6a21\u578b\u95f4\u6a21\u62df\u3002", "motivation": "\u81ea\u7a33\u5b9a\u662f\u8bbe\u8ba1\u5bb9\u9519\u5206\u5e03\u5f0f\u7b97\u6cd5\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u73b0\u4ee3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002R(1)W(1)\u6a21\u578b\u7684\u63d0\u51fa\u65e8\u5728\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u901a\u4fe1\u548c\u6267\u884c\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86R(1)W(1)\u6a21\u578b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8fdb\u7a0b\u53ef\u4ee5\u5728\u5355\u4e00\u6b65\u9aa4\u4e2d\u8bfb\u5199\u81ea\u5df1\u548c\u90bb\u5c45\u7684\u5c40\u90e8\u53d8\u91cf\u3002\u57fa\u4e8e\u8be5\u6a21\u578b\u8bbe\u8ba1\u4e86\u81ea\u7a33\u5b9a\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u793a\u4f8b\u8f6c\u6362\u5668\u7528\u4e8e\u6a21\u578b\u95f4\u6a21\u62df\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u4e86R(1)W(1)\u6a21\u578b\u4e0b\u7684\u81ea\u7a33\u5b9a\u7b97\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6a21\u578b\u95f4\u7684\u7b97\u6cd5\u6a21\u62df\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u901a\u4fe1\u548c\u6267\u884c\u6a21\u578bR(1)W(1)\uff0c\u5e76\u57fa\u4e8e\u8be5\u6a21\u578b\u8bbe\u8ba1\u4e86\u81ea\u7a33\u5b9a\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5927\u5339\u914d\u3001\u6700\u5c0fk-\u652f\u914d\u96c6\u548c\u6700\u5927k-\u4f9d\u8d56\u96c6\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u8ddd\u79bb\u4e8c\u5c40\u90e8\u4e92\u65a5\u7684\u793a\u4f8b\u8f6c\u6362\u5668\uff0c\u7528\u4e8e\u5728\u540c\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u4e2d\u6a21\u62dfR(1)W(1)\u6a21\u578b\u8bbe\u8ba1\u7684\u7b97\u6cd5\u3002"}}
{"id": "2510.03597", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03597", "abs": "https://arxiv.org/abs/2510.03597", "authors": ["Sina Alemohammad", "Zhangyang Wang", "Richard G. Baraniuk"], "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "comment": null, "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/SinaAlemohammad/Neon", "AI": {"tldr": "Neon\u662f\u4e00\u79cd\u901a\u8fc7\u8d1f\u5411\u5916\u63a8\u89e3\u51b3\u81ea\u8bad\u7ec3\u6a21\u578b\u5d29\u6e83\u7684\u65b0\u65b9\u6cd5\uff0c\u7b80\u5355\u9ad8\u6548\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u751f\u6210AI\u6a21\u578b\u7684\u6269\u5c55\uff0c\u800c\u81ea\u8bad\u7ec3\u5bfc\u81f4\u7684\u6a21\u578b\u5d29\u6e83\uff08MAD\uff09\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Neon\u9996\u5148\u5728\u81ea\u5408\u6210\u7684\u6570\u636e\u4e0a\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u7136\u540e\u53cd\u5411\u68af\u5ea6\u66f4\u65b0\u4ee5\u8fdc\u79bb\u9000\u5316\u7684\u6743\u91cd\u3002", "result": "Neon\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5c06xAR-L\u6a21\u578b\u5728ImageNet 256x256\u4e0a\u7684FID\u63d0\u5347\u81f31.02\uff0c\u4ec5\u97000.36%\u7684\u989d\u5916\u8bad\u7ec3\u8ba1\u7b97\u3002", "conclusion": "Neon\u901a\u8fc7\u8d1f\u5411\u5916\u63a8\u6280\u672f\u6709\u6548\u5730\u89e3\u51b3\u4e86\u81ea\u8bad\u7ec3\u5bfc\u81f4\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5b9e\u73b0\u7b80\u5355\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u3002"}}
{"id": "2510.04918", "categories": ["cs.DS", "cs.CC", "cs.CG"], "pdf": "https://arxiv.org/pdf/2510.04918", "abs": "https://arxiv.org/abs/2510.04918", "authors": ["Sanjeev Khanna", "Ashwin Padaki", "Krish Singal", "Erik Waingarten"], "title": "A Polynomial Space Lower Bound for Diameter Estimation in Dynamic Streams", "comment": "FOCS 2025", "summary": "We study the space complexity of estimating the diameter of a subset of\npoints in an arbitrary metric space in the dynamic (turnstile) streaming model.\nThe input is given as a stream of updates to a frequency vector $x \\in\n\\mathbb{Z}_{\\geq 0}^n$, where the support of $x$ defines a multiset of points\nin a fixed metric space $M = ([n], \\mathsf{d})$. The goal is to estimate the\ndiameter of this multiset, defined as $\\max\\{\\mathsf{d}(i,j) : x_i, x_j > 0\\}$,\nto a specified approximation factor while using as little space as possible.\n  In insertion-only streams, a simple $O(\\log n)$-space algorithm achieves a\n2-approximation. In sharp contrast to this, we show that in the dynamic\nstreaming model, any algorithm achieving a constant-factor approximation to\ndiameter requires polynomial space. Specifically, we prove that a\n$c$-approximation to the diameter requires $n^{\\Omega(1/c)}$ space. Our lower\nbound relies on two conceptual contributions: (1) a new connection between\ndynamic streaming algorithms and linear sketches for {\\em scale-invariant}\nfunctions, a class that includes diameter estimation, and (2) a connection\nbetween linear sketches for diameter and the {\\em minrank} of graphs, a notion\npreviously studied in index coding. We complement our lower bound with a nearly\nmatching upper bound, which gives a $c$-approximation to the diameter in\ngeneral metrics using $n^{O(1/c)}$ space.", "AI": {"tldr": "\u52a8\u6001\u6d41\u6a21\u578b\u4e2d\uff0c\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u76f4\u5f84\u9700\u8981\u591a\u9879\u5f0f\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u8fd1\u4e4e\u5339\u914d\u7684\u4e0a\u754c\u3002", "motivation": "\u7814\u7a76\u5728\u52a8\u6001\u6d41\u6a21\u578b\u4e0b\u4f30\u8ba1\u70b9\u96c6\u76f4\u5f84\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u4ee5\u7406\u89e3\u5176\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5c06\u52a8\u6001\u6d41\u7b97\u6cd5\u4e0e\u5c3a\u5ea6\u4e0d\u53d8\u51fd\u6570\u7684\u7ebf\u6027\u8349\u56fe\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5229\u7528\u56fe\u7684minrank\u6982\u5ff5\uff0c\u5efa\u7acb\u4e86\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u52a8\u6001\u6d41\u6a21\u578b\u4e2d\uff0cc-\u8fd1\u4f3c\u76f4\u5f84\u9700\u8981n^\u03a9(1/c)\u7a7a\u95f4\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528n^O(1/c)\u7a7a\u95f4\u7684c-\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u52a8\u6001\u6d41\u6a21\u578b\u4e2d\uff0c\u4efb\u4f55\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u76f4\u5f84\u7684\u7b97\u6cd5\u90fd\u9700\u8981\u591a\u9879\u5f0f\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fd1\u4e4e\u5339\u914d\u7684\u4e0a\u754c\u3002"}}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin L\u00e4ufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries.", "AI": {"tldr": "\u63d0\u51faAgentHub\u7814\u7a76\u8bae\u7a0b\uff0c\u89e3\u51b3LLM-based\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u5171\u4eab\u3002", "motivation": "\u5f53\u524dLLM-based\u667a\u80fd\u4f53\u7684\u57fa\u7840\u8bbe\u65bd\u5728\u53d1\u73b0\u3001\u8bc4\u4f30\u548c\u7ba1\u7406\u65b9\u9762\u4ecd\u7136\u5206\u6563\uff0c\u4e0e\u6210\u719f\u7684\u751f\u6001\u7cfb\u7edf\uff08\u5982npm\u548cHugging Face\uff09\u76f8\u6bd4\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faAgentHub\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u660e\u786e\u5176\u5173\u952e\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86AgentHub\uff0c\u4e00\u4e2a\u65e8\u5728\u6539\u8fdb\u5f00\u6e90\u5206\u53d1\u548c\u4fc3\u8fdb\u91cd\u7528\u7684\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "AgentHub\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u51b3\u80fd\u529b\u6e05\u6670\u6027\u3001\u751f\u547d\u5468\u671f\u900f\u660e\u5ea6\u3001\u4e92\u64cd\u4f5c\u6027\u3001\u6cbb\u7406\u3001\u5b89\u5168\u548c\u5de5\u4f5c\u6d41\u96c6\u6210\u7b49\u5173\u952e\u6311\u6218\uff0c\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.03714", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.03714", "abs": "https://arxiv.org/abs/2510.03714", "authors": ["Nalith Udugampola", "Xiaoyu Ai", "Binghao Li", "Henry Gong", "Aruna Seneviratne"], "title": "A Position- and Energy-Aware Routing Strategy for Subterranean LoRa Mesh Networks", "comment": null, "summary": "Although LoRa is predominantly employed with the single-hop LoRaWAN protocol,\nrecent advancements have extended its application to multi-hop mesh topologies.\nDesigning efficient routing for LoRa mesh networks remains challenging due to\nLoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional\nprotocols for low-traffic, aboveground networks with strict duty cycle\nconstraints or uses flooding-based methods in subterranean environments.\nHowever, these approaches inefficiently utilize the limited available network\nbandwidth in these low-data-rate networks due to excessive control overhead,\nacknowledgments, and redundant retransmissions. In this paper, we introduce a\nnovel position- and energy-aware routing strategy tailored for subterranean\nLoRa mesh networks aimed at enhancing maximum throughput and power efficiency\nwhile also maintaining high packet delivery ratios. Our mechanism begins with a\nlightweight position learning phase, during which LoRa repeaters ascertain\ntheir relative positions and gather routing information. Afterwards, the\nnetwork becomes fully operational with adaptive routing, leveraging standby\nLoRa repeaters for recovery from packet collisions and losses, and energy-aware\nroute switching to balance battery depletion across repeaters. The simulation\nresults on a representative subterranean network demonstrate a 185% increase in\nmaximum throughput and a 75% reduction in energy consumption compared to a\npreviously optimized flooding-based approach for high traffic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5730\u4e0bLoRa\u7f51\u72b6\u7f51\u7edc\u7684\u65b0\u578b\u8def\u7531\u7b56\u7565\uff0c\u901a\u8fc7\u4f4d\u7f6e\u548c\u80fd\u91cf\u611f\u77e5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u534f\u8bae\u5728\u4f4e\u6570\u636e\u7387\u7f51\u7edc\u4e2d\u56e0\u63a7\u5236\u5f00\u9500\u3001\u786e\u8ba4\u548c\u5197\u4f59\u91cd\u4f20\u800c\u4f4e\u6548\u5229\u7528\u6709\u9650\u5e26\u5bbd\uff0c\u5c24\u5176\u662f\u5728\u5730\u4e0b\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u4f4d\u7f6e\u5b66\u4e60\u9636\u6bb5\u548c\u81ea\u9002\u5e94\u8def\u7531\uff0c\u5229\u7528\u5907\u7528LoRa\u4e2d\u7ee7\u5668\u6062\u590d\u6570\u636e\u5305\u51b2\u7a81\u548c\u4e22\u5931\uff0c\u5e76\u901a\u8fc7\u80fd\u91cf\u611f\u77e5\u8def\u7531\u5207\u6362\u5e73\u8861\u4e2d\u7ee7\u5668\u7684\u7535\u6c60\u6d88\u8017\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4e4b\u524d\u4f18\u5316\u7684\u57fa\u4e8e\u6d2a\u6c34\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6700\u5927\u541e\u5410\u91cf\u63d0\u9ad8\u4e86185%\uff0c\u80fd\u8017\u964d\u4f4e\u4e8675%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5730\u4e0bLoRa\u7f51\u72b6\u7f51\u7edc\u7684\u65b0\u578b\u4f4d\u7f6e\u548c\u80fd\u91cf\u611f\u77e5\u8def\u7531\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u5927\u541e\u5410\u91cf\u548c\u529f\u7387\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6570\u636e\u5305\u4ea4\u4ed8\u7387\u3002"}}
{"id": "2510.03472", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03472", "abs": "https://arxiv.org/abs/2510.03472", "authors": ["Yulun Zhang", "Alexandre O. G. Barbosa", "Federico Pecora", "Jiaoyang Li"], "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems", "comment": "Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS) 2025", "summary": "We study optimizing a destination-to-chutes task mapping to improve\nthroughput in Robotic Sorting Systems (RSS), where a team of robots sort\npackages on a sortation floor by transporting them from induct workstations to\neject chutes based on their shipping destinations (e.g. Los Angeles or\nPittsburgh). The destination-to-chutes task mapping is used to determine which\nchutes a robot can drop its package. Finding a high-quality task mapping is\nchallenging because of the complexity of a real-world RSS. First, optimizing\ntask mapping is interdependent with robot target assignment and path planning.\nSecond, chutes will be CLOSED for a period of time once they receive sufficient\npackages to allow for downstream processing. Third, task mapping quality\ndirectly impacts the downstream processing, as scattered chutes for the same\ndestination increase package handling time. In this paper, we first formally\ndefine task mappings and the problem of Task Mapping Optimization (TMO). We\nthen present a simulator of RSS to evaluate task mappings. We then present a\nsimple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear\nProgramming, demonstrating the advantage of our optimized task mappings over\nthe greedily generated ones in various RSS setups with different map sizes,\nnumbers of chutes, and destinations. Finally, we use Quality Diversity\nalgorithms to analyze the throughput of a diverse set of task mappings. Our\ncode is available online at https://github.com/lunjohnzhang/tmo_public.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u4efb\u52a1\u6620\u5c04\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u5728\u591a\u79cd\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u4f18\u5316\u76ee\u7684\u5730\u5230\u6ed1\u69fd\u7684\u4efb\u52a1\u6620\u5c04\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff0c\u89e3\u51b3\u5b9e\u9645RSS\u4e2d\u7684\u590d\u6742\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u8fdb\u884c\u4efb\u52a1\u6620\u5c04\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u5206\u6790\u541e\u5410\u91cf\u3002", "result": "\u4f18\u5316\u7684\u4efb\u52a1\u6620\u5c04\u5728\u5404\u79cdRSS\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u8d2a\u5a6a\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8fdb\u5316\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\u4f18\u5316\u7684\u4efb\u52a1\u6620\u5c04\u5728\u591a\u79cdRSS\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u8d2a\u5a6a\u751f\u6210\u7684\u6620\u5c04\uff0c\u4e14\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u6709\u52a9\u4e8e\u5206\u6790\u4e0d\u540c\u4efb\u52a1\u6620\u5c04\u7684\u541e\u5410\u91cf\u3002"}}
{"id": "2510.03453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9a\u6027\u6bd4\u8f83\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u795e\u7ecf\u67b6\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7406\u8bba\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u795e\u7ecf\u67b6\u6784\u7684\u7406\u8bba\u5177\u6709\u6311\u6218\u6027\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u5bbd\u6cdb\u7684\u7406\u8bba\u8bc4\u4f30\u89c6\u89d2\uff0c\u5bf9\u9762\u5411\u5168\u8111\u7684\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u67b6\u6784\u53ca\u5176\u5b8c\u6574\u7cfb\u7edf\u8fdb\u884c\u4e86\u5b9a\u6027\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bbd\u6cdb\u7684\u5b9a\u6027\u6bd4\u8f83\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u5fc3\u7406\u67b6\u6784\u53ca\u5176\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u6bd4\u8f83\uff0c\u672c\u6587\u5f3a\u8c03\u4e86\u8bc4\u4f30\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u795e\u7ecf\u67b6\u6784\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u89c6\u89d2\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2510.03813", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03813", "abs": "https://arxiv.org/abs/2510.03813", "authors": ["Byungjun Kim", "Soobin Um", "Jong Chul Ye"], "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization", "comment": null, "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices.", "AI": {"tldr": "A new method, Contrastive Noise Optimization, enhances diversity in text-to-image generation by optimizing initial noise with a contrastive loss, achieving better results without sensitivity to hyperparameters.", "motivation": "Existing text-to-image diffusion models often suffer from limited diversity in outputs due to strong text guidance, with current methods offering only modest improvements or being sensitive to hyperparameter tuning.", "method": "The approach involves shaping the initial noise using a contrastive loss defined in the Tweedie data space, optimizing a batch of noise latents to repel instances within the batch for diversity while anchoring them to a reference sample for fidelity.", "result": "The method achieves a superior quality-diversity Pareto frontier and remains robust to hyperparameter choices, as validated by extensive experiments.", "conclusion": "Contrastive Noise Optimization provides a robust and effective method to enhance the diversity of text-to-image generation without compromising fidelity, demonstrating superior performance across various T2I backbones."}}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "AI": {"tldr": "Refine\u662f\u4e00\u4e2a\u65b0\u578b\u8865\u4e01\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u6b67\u3001\u591a\u6837\u5316\u8865\u4e01\u548cLLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\uff0c\u663e\u8457\u63d0\u5347LLM-based APR\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684APR\u6280\u672f\u7531\u4e8e\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u4e0d\u5b8c\u6574\u7684\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5e38\u751f\u6210\u90e8\u5206\u6b63\u786e\u7684\u8865\u4e01\uff08Draft Patches\uff09\uff0c\u65e0\u6cd5\u5b8c\u5168\u4fee\u590d\u9519\u8bef\u6216\u8fc7\u5ea6\u62df\u5408\u6d4b\u8bd5\u7528\u4f8b\u3002", "method": "Refine\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u5b9e\u73b0\u8865\u4e01\u4f18\u5316\uff1a\u6a21\u7cca\u95ee\u9898\u548c\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u6d88\u6b67\u3001\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u591a\u6837\u5316\u8865\u4e01\u5019\u9009\u3001\u4ee5\u53ca\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u805a\u5408\u90e8\u5206\u4fee\u590d\u3002", "result": "\u5728SWE-Bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRefine\u5c06AutoCodeRover\u7684\u6027\u80fd\u63d0\u5347\u4e8614.67%\uff0c\u8fbe\u523051.67%\u7684\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u57fa\u7ebf\u3002\u5728SWE-Bench Verified\u4e0a\uff0c\u5206\u8fa8\u7387\u63d0\u9ad8\u4e8612.2%\uff0c\u96c6\u6210\u5230\u591a\u4e2aAPR\u7cfb\u7edf\u540e\u5e73\u5747\u63d0\u534714%\u3002", "conclusion": "Refine\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u5730\u5c06\u90e8\u5206\u6b63\u786e\u7684\u8865\u4e01\u8f6c\u5316\u4e3a\u6b63\u786e\u8865\u4e01\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u63a5\u8fd1\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2510.03807", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03807", "abs": "https://arxiv.org/abs/2510.03807", "authors": ["Vaskar Chakma", "Wooyeol Choi"], "title": "6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection", "comment": null, "summary": "Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)\ntechnology face critical limitations in achieving real-time performance for\nmission-critical industrial applications. Existing 5G-enabled systems suffer\nfrom latencies exceeding 10ms, which are inadequate for applications requiring\nsub-millisecond response times, such as autonomous industrial control and\npredictive maintenance. This research aims to develop and validate a 6G-enabled\nDigital Twin framework that achieves ultra-low latency communication and\nreal-time synchronization between physical industrial assets and their digital\ncounterparts, specifically targeting bearing fault detection as a critical\nindustrial use case. The proposed framework integrates terahertz communications\n(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence\nwithin a five-layer architecture. Experimental validation was conducted using\nthe Case Western Reserve University (CWRU) bearing dataset, implementing\ncomprehensive feature extraction (15 time and frequency domain features) and\nRandom Forest classification algorithms. The system performance was evaluated\nagainst traditional WiFi-6 and 5G networks across multiple metrics, including\nclassification accuracy, end-to-end latency, and scalability. It achieved 97.7%\nfault classification accuracy with 0.8ms end-to-end latency, representing a\n15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)\nnetworks. The system demonstrated superior scalability with sub-linear\nprocessing time growth and maintained consistent performance across four\nbearing fault categories (normal, inner race, outer race, and ball faults) with\nmacro-averaged F1-scores exceeding 97%.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a6G\u652f\u6301\u7684\u6570\u5b57\u5316\u5b6a\u751f\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u8f74\u627f\u6545\u969c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u5f53\u524d\u96c6\u6210\u4e86\u6570\u5b57\u5316\u5b6a\u751f\u6280\u672f\u7684CPS\u5728\u5b9e\u73b0\u5173\u952e\u4efb\u52a1\u5de5\u4e1a\u5e94\u7528\u7684\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c5G\u7cfb\u7edf\u7684\u5ef6\u8fdf\u8d85\u8fc710ms\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e9a\u6beb\u79d2\u7ea7\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u592a\u8d6b\u5179\u901a\u4fe1\u3001\u667a\u80fd\u53cd\u5c04\u9762\u548c\u8fb9\u7f18\u4eba\u5de5\u667a\u80fd\uff0c\u91c7\u7528\u4e94\u5c42\u67b6\u6784\uff0c\u5e76\u5728CWRU\u8f74\u627f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u4e8615\u4e2a\u65f6\u9891\u57df\u7279\u5f81\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u7b97\u6cd5\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e8697.7%\u7684\u6545\u969c\u5206\u7c7b\u51c6\u786e\u7387\u548c0.8ms\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u76f8\u6bd4WiFi-6\u548c5G\u7f51\u7edc\u5206\u522b\u63d0\u5347\u4e8615.6\u500d\u548c5.25\u500d\uff0c\u540c\u65f6\u5728\u53ef\u6269\u5c55\u6027\u548c\u591a\u7c7b\u522b\u6545\u969c\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a6G\u652f\u6301\u7684\u6570\u5b57\u5316\u5b6a\u751f\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8d85\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8f74\u627f\u6545\u969c\u68c0\u6d4b\u65b9\u9762\u3002"}}
{"id": "2510.03481", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03481", "abs": "https://arxiv.org/abs/2510.03481", "authors": ["Khang Vo Huynh", "David Parker", "Lu Feng"], "title": "Robust Permissive Controller Synthesis for Interval MDPs", "comment": null, "summary": "We address the problem of robust permissive controller synthesis for robots\noperating under uncertain dynamics, modeled as Interval Markov Decision\nProcesses (IMDPs). IMDPs generalize standard MDPs by allowing transition\nprobabilities to vary within intervals, capturing epistemic uncertainty from\nsensing noise, actuation imprecision, and coarse system abstractions-common in\nrobotics. Traditional controller synthesis typically yields a single\ndeterministic strategy, limiting adaptability. In contrast, permissive\ncontrollers (multi-strategies) allow multiple actions per state, enabling\nruntime flexibility and resilience. However, prior work on permissive\ncontroller synthesis generally assumes exact transition probabilities, which is\nunrealistic in many robotic applications. We present the first framework for\nrobust permissive controller synthesis on IMDPs, guaranteeing that all\nstrategies compliant with the synthesized multi-strategy satisfy reachability\nor reward-based specifications under all admissible transitions. We formulate\nthe problem as mixed-integer linear programs (MILPs) and propose two encodings:\na baseline vertex-enumeration method and a scalable duality-based method that\navoids explicit enumeration. Experiments on four benchmark domains show that\nboth methods synthesize robust, maximally permissive controllers and scale to\nlarge IMDPs with up to hundreds of thousands of states.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728IMDPs\u4e0a\u5408\u6210\u9c81\u68d2\u6027\u8bb8\u53ef\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7MILP\u65b9\u6cd5\u786e\u4fdd\u7b56\u7565\u6ee1\u8db3\u89c4\u8303\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\uff08\u5efa\u6a21\u4e3aIMDPs\uff09\uff0c\u4f20\u7edf\u63a7\u5236\u5668\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u5355\u4e00\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002\u800c\u8bb8\u53ef\u63a7\u5236\u5668\u5141\u8bb8\u591a\u4e2a\u52a8\u4f5c\u6bcf\u72b6\u6001\uff0c\u589e\u5f3a\u4e86\u8fd0\u884c\u65f6\u7075\u6d3b\u6027\u548c\u5f39\u6027\u3002\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5047\u8bbe\u7cbe\u786e\u8f6c\u79fb\u6982\u7387\uff0c\u8fd9\u5728\u8bb8\u591a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4e0d\u73b0\u5b9e\u3002", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILPs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7f16\u7801\u65b9\u6cd5\uff1a\u57fa\u7ebf\u9876\u70b9\u679a\u4e3e\u65b9\u6cd5\u548c\u907f\u514d\u663e\u5f0f\u679a\u4e3e\u7684\u53ef\u6269\u5c55\u5bf9\u5076\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5408\u6210\u9c81\u68d2\u4e14\u6700\u5927\u8bb8\u53ef\u7684\u63a7\u5236\u5668\uff0c\u5e76\u6269\u5c55\u5230\u5177\u6709\u6570\u5341\u4e07\u72b6\u6001\u7684\u5927\u578bIMDPs\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728IMDPs\u4e0a\u5408\u6210\u9c81\u68d2\u6027\u8bb8\u53ef\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u786e\u4fdd\u6240\u6709\u7b26\u5408\u5408\u6210\u591a\u7b56\u7565\u7684\u7b56\u7565\u5728\u6240\u6709\u5141\u8bb8\u7684\u8f6c\u79fb\u4e0b\u6ee1\u8db3\u53ef\u8fbe\u6027\u6216\u57fa\u4e8e\u5956\u52b1\u7684\u89c4\u8303\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u9c81\u68d2\u4e14\u6700\u5927\u8bb8\u53ef\u7684\u63a7\u5236\u5668\uff0c\u5e76\u6269\u5c55\u5230\u5177\u6709\u6570\u5341\u4e07\u72b6\u6001\u7684\u5927\u578bIMDPs\u3002"}}
{"id": "2510.03469", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7LLMs\u5c06\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u8f6c\u6362\u4e3aKripke\u7ed3\u6784\u548cLTL\u8fdb\u884c\u6a21\u578b\u68c0\u67e5\uff0c\u5b9e\u9a8c\u663e\u793aGPT-5\u5728\u5206\u7c7b\u548c\u5f62\u5f0f\u5316\u8868\u793a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bed\u4e49\u5b8c\u7f8e\u6a21\u578b\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u4e0e\u5176\u9884\u671f\u884c\u4e3a\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u9a8c\u8bc1\u8ba1\u5212\u7684\u6b63\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u8f6c\u6362\u4e3aKripke\u7ed3\u6784\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6a21\u578b\u68c0\u67e5\u3002", "result": "GPT-5\u5728PlanBench\u8ba1\u5212\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5206\u7c7b\u6027\u80fdF1\u5f97\u5206\u4e3a96.3%\uff0c\u4e14\u51e0\u4e4e\u603b\u80fd\u751f\u6210\u8bed\u6cd5\u5b8c\u7f8e\u7684\u5f62\u5f0f\u5316\u8868\u793a\u3002", "conclusion": "\u867d\u7136GPT-5\u5728\u5206\u7c7b\u6027\u80fd\u548c\u751f\u6210\u5f62\u5f0f\u5316\u8868\u793a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bed\u4e49\u5b8c\u7f8e\u5f62\u5f0f\u5316\u6a21\u578b\u7684\u5408\u6210\u4ecd\u9700\u672a\u6765\u63a2\u7d22\u3002"}}
{"id": "2510.03837", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03837", "abs": "https://arxiv.org/abs/2510.03837", "authors": ["Shen Fan", "Przemyslaw Musialski"], "title": "Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models", "comment": null, "summary": "We propose a simple, data-efficient pipeline that augments an implicit\nreconstruction network based on neural SDF-based CAD parts with a\npart-segmentation head trained under PartField-generated supervision. Unlike\nmethods tied to fixed taxonomies, our model accepts meshes with any number of\nparts and produces coherent, geometry-aligned labels in a single pass. We\nevaluate on randomly sampled CAD meshes from the ABC dataset with intentionally\nvaried part cardinalities, including over-segmented shapes, and report strong\nperformance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation\n(mIoU, Accuracy), together with a new Segmentation Consistency metric that\ncaptures local label smoothness. We attach a lightweight segmentation head to\nthe Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction\nwhile providing accurate part labels for meshes with any number of parts. Even\nunder degraded reconstructions on thin or intricate geometries, segmentation\nremains accurate and label-coherent, often preserving the correct part count.\nOur approach therefore offers a practical route to semantically structured CAD\nmeshes without requiring curated taxonomies or exact palette matches. We\ndiscuss limitations in boundary precision, partly due to per-face supervision,\nand outline paths toward boundary-aware training and higher resolution labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecfSDF\u91cd\u5efa\u548c\u90e8\u4ef6\u5206\u5272\u7684\u7075\u6d3b\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u90e8\u4ef6\u6570\u91cf\u7684CAD\u7f51\u683c\uff0c\u5e76\u5728\u91cd\u5efa\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c3d\u7ba1\u8fb9\u754c\u7cbe\u5ea6\u6709\u5f85\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u5177\u6709\u4e0d\u540c\u90e8\u4ef6\u6570\u91cf\u7684CAD\u7f51\u683c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u52a0\u7075\u6d3b\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u795e\u7ecfSDF\u7684\u9690\u5f0f\u91cd\u5efa\u7f51\u7edc\u548cPartField\u751f\u6210\u7684\u76d1\u7763\u8bad\u7ec3\u7684\u90e8\u4ef6\u5206\u5272\u5934\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u5904\u7406\u6d41\u7a0b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u6570\u91cf\u90e8\u4ef6\u7684\u7f51\u683c\uff0c\u5e76\u5728\u5355\u6b21\u5904\u7406\u4e2d\u751f\u6210\u51e0\u4f55\u5bf9\u9f50\u7684\u6807\u7b7e\u3002", "result": "\u5728ABC\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\uff08CDL1/CDL2, F1-micro, NC\uff09\u548c\u5206\u5272\uff08mIoU, Accuracy\uff09\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u5272\u4e00\u81f4\u6027\u6307\u6807\u3002\u5373\u4f7f\u5728\u8584\u6216\u590d\u6742\u51e0\u4f55\u7684\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u65f6\uff0c\u5206\u5272\u4ecd\u4fdd\u6301\u51c6\u786e\u548c\u6807\u7b7e\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aCAD\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u56fa\u5b9a\u5206\u7c7b\u6216\u7cbe\u786e\u8c03\u8272\u677f\u5339\u914d\u7684\u8bed\u4e49\u7ed3\u6784\u751f\u6210\u9014\u5f84\uff0c\u5c3d\u7ba1\u5728\u8fb9\u754c\u7cbe\u5ea6\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4f46\u4e3a\u672a\u6765\u7684\u8fb9\u754c\u611f\u77e5\u8bad\u7ec3\u548c\u9ad8\u5206\u8fa8\u7387\u6807\u7b7e\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700RAG\u7684\u63d0\u793a\u65b9\u6cd5\uff08GHL\uff09\u6765\u81ea\u52a8\u751f\u6210\u9ad8\u7ea7\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728Bluetooth\u548cMozilla\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u4ece\u9700\u6c42\u6587\u6863\u624b\u52a8\u751f\u6210\u9ad8\u7ea7\u6d4b\u8bd5\u7528\u4f8b\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5982RAG\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u5e94\u7528\u5b9a\u5236\uff0c\u52b3\u52a8\u5bc6\u96c6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5c06\u9700\u6c42\u6587\u6863\u8f93\u5165LLM\u4ee5\u751f\u6210\u76f8\u5e94\u7684\u6d4b\u8bd5\u8bbe\u8ba1\u6280\u672f\uff0c\u7136\u540e\u4e3a\u6bcf\u79cd\u751f\u6210\u7684\u6d4b\u8bd5\u8bbe\u8ba1\u6280\u672f\u751f\u6210\u9ad8\u7ea7\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u9a8c\u8bc1\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728Bluetooth\u548cMozilla\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b8f\u53ec\u56de\u7387\u5206\u522b\u4e3a0.81\u548c0.37\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u901a\u8fc7\u63d0\u793a\u800c\u4e0d\u9700\u8981\u521b\u5efaRAG\u7684\u65b9\u6cd5\uff08GHL\uff09\uff0c\u7528\u4e8e\u4ece\u9700\u6c42\u6587\u6863\u4e2d\u81ea\u52a8\u751f\u6210\u9ad8\u7ea7\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03829", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03829", "abs": "https://arxiv.org/abs/2510.03829", "authors": ["Andr\u00e9 Coelho", "Pedro Ribeiro", "Helder Fontes", "Rui Campos"], "title": "A4FN: an Agentic AI Architecture for Autonomous Flying Networks", "comment": "This paper has been accepted for presentation in the Auto ML for\n  Zero-Touch Network Management Workshop (WS04-01) at the IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "This position paper presents A4FN, an Agentic Artificial Intelligence (AI)\narchitecture for intent-driven automation in Flying Networks (FNs) using\nUnmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI\nand Large Language Models (LLMs) to enable real-time, context-aware network\ncontrol via a distributed agentic system. It comprises two components: the\nPerception Agent (PA), which semantically interprets multimodal input --\nincluding imagery, audio, and telemetry data -- from UAV-mounted sensors to\nderive Service Level Specifications (SLSs); and the Decision-and-Action Agent\n(DAA), which reconfigures the network based on inferred intents. A4FN embodies\nkey properties of Agentic AI, including autonomy, goal-driven reasoning, and\ncontinuous perception-action cycles. Designed for mission-critical,\ninfrastructure-limited scenarios such as disaster response, it supports\nadaptive reconfiguration, dynamic resource management, and interoperability\nwith emerging wireless technologies. The paper details the A4FN architecture,\nits core innovations, and open research challenges in multi-agent coordination\nand Agentic AI integration in next-generation FNs.", "AI": {"tldr": "A4FN\u662f\u4e00\u79cd\u57fa\u4e8eAgentic AI\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u548cLLMs\u5b9e\u73b0\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\u7b49\u5173\u952e\u4efb\u52a1\u573a\u666f\u3002", "motivation": "\u9488\u5bf9\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u573a\u666f\uff08\u5982\u707e\u5bb3\u54cd\u5e94\uff09\uff0c\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u91cd\u6784\u3001\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u3002", "method": "A4FN\u91c7\u7528\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u4e86\u611f\u77e5\u4ee3\u7406\uff08PA\uff09\u548c\u51b3\u7b56\u4e0e\u884c\u52a8\u4ee3\u7406\uff08DAA\uff09\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7f51\u7edc\u63a7\u5236\u3002", "result": "A4FN\u5c55\u793a\u4e86\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u9a71\u52a8\u63a8\u7406\u548c\u8fde\u7eed\u611f\u77e5-\u884c\u52a8\u5faa\u73af\u7b49Agentic AI\u7279\u6027\uff0c\u652f\u6301\u4e0e\u65b0\u5174\u65e0\u7ebf\u6280\u672f\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "A4FN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAgentic AI\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u610f\u56fe\u9a71\u52a8\u81ea\u52a8\u5316\uff0c\u65e8\u5728\u89e3\u51b3\u5173\u952e\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u7f51\u7edc\u63a7\u5236\u6311\u6218\u3002"}}
{"id": "2510.03496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03496", "abs": "https://arxiv.org/abs/2510.03496", "authors": ["Vadivelan Murugesan", "Rajasundaram Mathiazhagan", "Sanjana Joshi", "Aliasghar Arab"], "title": "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*", "comment": null, "summary": "Human-robot collaboration requires precise prediction of human motion over\nextended horizons to enable proactive collision avoidance. Unlike existing\nplanners that rely solely on kinodynamic models, we present a prediction-driven\nsafe planning framework that leverages granular, joint-by-joint human motion\nforecasting validated in a physics-based digital twin. A capsule-based\nartificial potential field (APF) converts these granular predictions into\ncollision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when\nthresholds are exceeded. The depth camera is used to extract 3D skeletal poses\nand a convolutional neural network-bidirectional long short-term memory\n(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A\ndigital twin model integrates real-time human posture prediction placed in\nfront of a simulated robot to evaluate motions and physical contacts. The\nproposed method enables validation of planned trajectories ahead of time and\nbridging potential latency gaps in updating planned trajectories in real-time.\nIn 50 trials, our method achieved 100% proactive avoidance with > 250 mm\nclearance and sub-2 s replanning, demonstrating superior precision and\nreliability compared to existing kinematic-only planners through the\nintegration of predictive human modeling with digital twin validation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u6d4b\u6027\u4eba\u7c7b\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u7684\u5b89\u5168\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u907f\u969c\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4eba\u673a\u534f\u4f5c\u9700\u8981\u7cbe\u786e\u9884\u6d4b\u4eba\u7c7b\u52a8\u4f5c\u4ee5\u652f\u6301\u4e3b\u52a8\u907f\u969c\u3002\u73b0\u6709\u89c4\u5212\u5668\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u9a71\u52a8\u7684\u5b89\u5168\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u80f6\u56ca\u5f0f\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u5c06\u7ec6\u7c92\u5ea6\u9884\u6d4b\u8f6c\u5316\u4e3a\u78b0\u649e\u98ce\u9669\u6307\u6807\uff0c\u5e76\u5728\u8d85\u8fc7\u9608\u503c\u65f6\u89e6\u53d1\u81ea\u9002\u5e94RRT*\uff08A-RRT*\uff09\u89c4\u5212\u5668\u3002\u901a\u8fc7\u6df1\u5ea6\u6444\u50cf\u5934\u63d0\u53d63D\u9aa8\u9abc\u59ff\u6001\uff0c\u5e76\u4f7f\u7528CNN-BiLSTM\u6a21\u578b\u63d0\u524d\u9884\u6d4b\u5355\u4e2a\u5173\u8282\u8f68\u8ff9\u3002\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u6574\u5408\u5b9e\u65f6\u4eba\u7c7b\u59ff\u52bf\u9884\u6d4b\uff0c\u653e\u7f6e\u5728\u6a21\u62df\u673a\u5668\u4eba\u524d\u65b9\u4ee5\u8bc4\u4f30\u52a8\u4f5c\u548c\u7269\u7406\u63a5\u89e6\u3002", "result": "\u572850\u6b21\u8bd5\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u4e3b\u52a8\u907f\u969c\uff0c\u8ddd\u79bb\u5927\u4e8e250\u6beb\u7c73\uff0c\u91cd\u65b0\u89c4\u5212\u65f6\u95f4\u4f4e\u4e8e2\u79d2\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u9884\u6d4b\u6027\u4eba\u7c7b\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u572850\u6b21\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u4e3b\u52a8\u907f\u969c\uff0c\u4e14\u8ddd\u79bb\u5927\u4e8e250\u6beb\u7c73\uff0c\u91cd\u65b0\u89c4\u5212\u65f6\u95f4\u4f4e\u4e8e2\u79d2\uff0c\u5c55\u73b0\u4e86\u6bd4\u73b0\u6709\u4ec5\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u89c4\u5212\u5668\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.03485", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faPolicyGuardBench\u57fa\u51c6\u548cPolicyGuard-4B\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f51\u7edc\u4ee3\u7406\u8f68\u8ff9\u4e2d\u7684\u653f\u7b56\u8fdd\u89c4\uff0c\u5c55\u793a\u4e86\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u9ad8\u6548\u62a4\u680f\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u76ee\u524d\u5f88\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u7f51\u7edc\u4ee3\u7406\u751f\u6210\u7684\u957f\u671f\u8f68\u8ff9\u662f\u5426\u7b26\u5408\u5916\u90e8\u6216\u4eba\u4e3a\u6307\u5b9a\u7684\u653f\u7b56\uff0c\u4ee5\u53ca\u653f\u7b56\u8fdd\u89c4\u662f\u5426\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\uff08\u5982\u9886\u57df\u548c\u5b50\u57df\uff09\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "method": "\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u4ee3\u7406\u8fd0\u884c\u4e2d\u751f\u6210\u5e7f\u6cdb\u7684\u653f\u7b56\u96c6\uff0c\u5e76\u521b\u5efa\u5e26\u6709\u8fdd\u89c4\u6807\u7b7e\u7684\u5b50\u57df\u5185\u548c\u8de8\u5b50\u57df\u914d\u5bf9\uff0c\u6784\u5efa\u4e86PolicyGuardBench\u3002\u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86PolicyGuard-4B\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u62a4\u680f\u6a21\u578b\u3002", "result": "PolicyGuard-4B\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5728\u672a\u89c1\u8fc7\u7684\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "PolicyGuardBench\u548cPolicyGuard-4B\u4e3a\u7814\u7a76\u7f51\u7edc\u4ee3\u7406\u8f68\u8ff9\u4e2d\u7684\u653f\u7b56\u5408\u89c4\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u6846\u67b6\uff0c\u5e76\u8868\u660e\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u63a8\u5e7f\u7684\u62a4\u680f\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2510.03915", "categories": ["cs.CV", "cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03915", "abs": "https://arxiv.org/abs/2510.03915", "authors": ["Sagar Bharadwaj", "Harrison Williams", "Luke Wang", "Michael Liang", "Tao Jin", "Srinivasan Seshan", "Anthony Rowe"], "title": "OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications", "comment": null, "summary": "World-scale augmented reality (AR) applications need a ubiquitous 6DoF\nlocalization backend to anchor content to the real world consistently across\ndevices. Large organizations such as Google and Niantic are 3D scanning outdoor\npublic spaces in order to build their own Visual Positioning Systems (VPS).\nThese centralized VPS solutions fail to meet the needs of many future AR\napplications -- they do not cover private indoor spaces because of privacy\nconcerns, regulations, and the labor bottleneck of updating and maintaining 3D\nscans. In this paper, we present OpenFLAME, a federated VPS backend that allows\nindependent organizations to 3D scan and maintain a separate VPS service for\ntheir own spaces. This enables access control of indoor 3D scans, distributed\nmaintenance of the VPS backend, and encourages larger coverage. Sharding of VPS\nservices introduces several unique challenges -- coherency of localization\nresults across spaces, quality control of VPS services, selection of the right\nVPS service for a location, and many others. We introduce the concept of\nfederated image-based localization and provide reference solutions for managing\nand merging data across maps without sharing private data.", "AI": {"tldr": "OpenFLAME\u662f\u4e00\u4e2a\u8054\u90a6\u5316\u7684VPS\u540e\u7aef\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0fVPS\u5728\u79c1\u6709\u5ba4\u5185\u7a7a\u95f4\u8986\u76d6\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u72ec\u7acb\u7ec4\u7ec7\u7ef4\u62a4\u5404\u81ea\u7684VPS\u670d\u52a1\u3002", "motivation": "\u96c6\u4e2d\u5f0fVPS\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6ee1\u8db3\u672a\u6765AR\u5e94\u7528\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u79c1\u6709\u5ba4\u5185\u7a7a\u95f4\u7684\u8986\u76d6\u3001\u9690\u79c1\u4fdd\u62a4\u4ee5\u53ca3D\u626b\u63cf\u7684\u66f4\u65b0\u548c\u7ef4\u62a4\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86OpenFLAME\uff0c\u4e00\u4e2a\u8054\u90a6\u5316\u7684VPS\u540e\u7aef\uff0c\u5141\u8bb8\u72ec\u7acb\u7ec4\u7ec7\u4e3a\u5176\u81ea\u6709\u7a7a\u95f4\u8fdb\u884c3D\u626b\u63cf\u548c\u7ef4\u62a4\u72ec\u7acb\u7684VPS\u670d\u52a1\u3002\u901a\u8fc7\u8054\u90a6\u56fe\u50cf\u5b9a\u4f4d\u7684\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u8de8\u7a7a\u95f4\u5b9a\u4f4d\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3001VPS\u670d\u52a1\u8d28\u91cf\u63a7\u5236\u7b49\u6311\u6218\u3002", "result": "OpenFLAME\u5b9e\u73b0\u4e86\u5bf9\u79c1\u6709\u5ba4\u5185\u7a7a\u95f4\u7684\u53ef\u63a7\u8bbf\u95ee\u3001VPS\u540e\u7aef\u7684\u5206\u5e03\u5f0f\u7ef4\u62a4\uff0c\u5e76\u4fc3\u8fdb\u4e86\u66f4\u5927\u8303\u56f4\u7684\u8986\u76d6\u3002", "conclusion": "OpenFLAME\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5316\u7684\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\uff08VPS\uff09\u540e\u7aef\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0fVPS\u5728\u8986\u76d6\u79c1\u6709\u5ba4\u5185\u7a7a\u95f4\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u7ef4\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765AR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03964", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03964", "abs": "https://arxiv.org/abs/2510.03964", "authors": ["Ville Cantory", "Darya Biparva", "Haoyu Tan", "Tongyu Nie", "John Schroeder", "Ruofei Du", "Victoria Interrante", "Piotr Didyk"], "title": "Enhancing Foveated Rendering with Weighted Reservoir Sampling", "comment": "To appear in The 18th ACM SIGGRAPH Conference on Motion, Interaction,\n  and Games (MIG '25), December 03-05, 2025, Zurich, Switzerland", "summary": "Spatiotemporal sensitivity to high frequency information declines with\nincreased peripheral eccentricity. Foveated rendering exploits this by\ndecreasing the spatial resolution of rendered images in peripheral vision,\nreducing the rendering cost by omitting high frequency details. As foveation\nlevels increase, the rendering quality is reduced, and traditional foveated\nrendering systems tend not to preserve samples that were previously rendered at\nhigh spatial resolution in previous frames. Additionally, prior research has\nshown that saccade landing positions are distributed around a target location\nrather than landing at a single point, and that even during fixations, eyes\nperform small microsaccades around a fixation point. This creates an\nopportunity for sampling from temporally neighbouring frames with differing\nfoveal locations to reduce the required rendered size of the foveal region\nwhile achieving a higher perceived image quality. We further observe that the\ntemporal presentation of pixels frame-to-frame can be viewed as a data stream,\npresenting a random sampling problem. Following this intuition, we propose a\nWeighted Reservoir Sampling technique to efficiently maintain a reservoir of\nthe perceptually relevant high quality pixel samples from previous frames and\nincorporate them into the computation of the current frame. This allows the\nrenderer to render a smaller region of foveal pixels per frame by temporally\nreusing pixel samples that are still relevant to reconstruct a higher perceived\nimage quality, while allowing for higher levels of foveation. Our method\noperates on the output of foveated rendering, and runs in under 1\\,ms at 4K\nresolution, making it highly efficient and integrable with real-time VR and AR\nfoveated rendering systems.", "AI": {"tldr": "\u901a\u8fc7\u52a0\u6743\u50a8\u5c42\u91c7\u6837\u6280\u672f\uff0c\u5229\u7528\u5148\u524d\u5e27\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u51cf\u5c11\u6bcf\u5e27\u4e2d\u592e\u51f9\u533a\u57df\u7684\u6e32\u67d3\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6VR/AR\u7cfb\u7edf\u3002", "motivation": "\u5229\u7528\u4eba\u773c\u5bf9\u9ad8\u9891\u4fe1\u606f\u7684\u65f6\u7a7a\u654f\u611f\u6027\u968f\u504f\u5fc3\u8ddd\u589e\u52a0\u800c\u4e0b\u964d\u7684\u7279\u6027\uff0c\u4ee5\u53ca\u773c\u52a8\uff08\u5305\u62ec\u5fae\u773c\u8df3\uff09\u7684\u5206\u5e03\u7279\u6027\uff0c\u4ee5\u51cf\u5c11\u6e32\u67d3\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u50a8\u5c42\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u4ece\u5148\u524d\u5e27\u4e2d\u9ad8\u6548\u7ef4\u62a4\u548c\u6574\u5408\u611f\u77e5\u76f8\u5173\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u57284K\u5206\u8fa8\u7387\u4e0b\u8fd0\u884c\u65f6\u95f4\u4e0d\u52301\u6beb\u79d2\uff0c\u80fd\u591f\u4e0e\u5b9e\u65f6VR\u548cAR\u7cfb\u7edf\u96c6\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u52a0\u6743\u50a8\u5c42\u91c7\u6837\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u6bcf\u5e27\u9700\u8981\u6e32\u67d3\u7684\u4e2d\u592e\u51f9\u533a\u57df\u5927\u5c0f\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6VR\u548cAR\u7cfb\u7edf\u3002"}}
{"id": "2510.03712", "categories": ["cs.SE", "68M15, 90B25, 68T05, 90C29", "C.4; C.2.4; D.2.5; D.4.5"], "pdf": "https://arxiv.org/pdf/2510.03712", "abs": "https://arxiv.org/abs/2510.03712", "authors": ["Jahidul Arafat", "Kh. M. Moniruzzaman", "Shamim Hossain", "Fariha Tasmin", "Kamrujjaman", "Ahsan Habib Tareq"], "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems", "comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios", "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u68c0\u6d4b\u548c\u9884\u9632\u5206\u5e03\u5f0f\u7cfb\u7edf\u6f5c\u5728\u98ce\u9669\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u7cfb\u7edf\uff08HYDRA\u3001RAVEN\u3001APEX\uff09\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6fc0\u8fdb\u4f18\u5316\u7b56\u7565\u53ef\u80fd\u9690\u85cf\u6f5c\u5728\u98ce\u9669\uff0c\u5f53\u524d\u53ef\u9760\u6027\u5de5\u7a0b\u4e3b\u8981\u5173\u6ce8\u88ab\u52a8\u4e8b\u4ef6\u54cd\u5e94\uff0c\u800c\u975e\u4e3b\u52a8\u68c0\u6d4b\u4f18\u5316\u5f15\u53d1\u7684\u6f0f\u6d1e\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u7cfb\u7edf\uff1aHYDRA\uff08\u91c7\u7528\u516d\u79cd\u4f18\u5316\u611f\u77e5\u6270\u52a8\u7b56\u7565\uff09\u3001RAVEN\uff08\u63d0\u4f9b\u8fde\u7eed\u751f\u4ea7\u76d1\u63a7\uff09\u548cAPEX\uff08\u5b9e\u73b0\u98ce\u9669\u611f\u77e5\u4f18\u5316\uff09\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u3001\u667a\u80fd\u6270\u52a8\u6d4b\u8bd5\u548c\u98ce\u9669\u611f\u77e5\u6027\u80fd\u4f18\u5316\u6765\u68c0\u6d4b\u548c\u9884\u9632\u6f5c\u5728\u98ce\u9669\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u6d4b\u8bd5\u73af\u5883\u4e2d\u5177\u6709\u5f3a\u7edf\u8ba1\u9a8c\u8bc1\uff08Cohen d>2.0\uff09\u548c\u9ad8\u53ef\u91cd\u590d\u6027\uff08r>0.92\uff09\u3002\u751f\u4ea7\u90e8\u7f7224\u5468\u540e\uff0c\u5e73\u5747\u6062\u590d\u65f6\u95f4\u51cf\u5c1169.1%\uff0c\u4e8b\u4ef6\u4e25\u91cd\u6027\u51cf\u5c1178.6%\uff0c\u9884\u9632\u4e8681\u8d77\u4e8b\u4ef6\uff0c\u5e74\u5747\u6536\u76ca144\u4e07\u7f8e\u5143\uff0c\u6295\u8d44\u56de\u62a5\u671f\u4e3a3.2\u4e2a\u6708\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5c06\u53ef\u9760\u6027\u5de5\u7a0b\u4ece\u88ab\u52a8\u4e8b\u4ef6\u7ba1\u7406\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u7684\u98ce\u9669\u611f\u77e5\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6062\u590d\u65f6\u95f4\u3001\u4e8b\u4ef6\u4e25\u91cd\u6027\u548c\u6f5c\u5728\u4e8b\u4ef6\u6570\u91cf\uff0c\u5177\u6709\u663e\u8457\u7684\u7ecf\u6d4e\u6548\u76ca\u548c\u9ad8\u6295\u8d44\u56de\u62a5\u7387\u3002"}}
{"id": "2510.04035", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04035", "abs": "https://arxiv.org/abs/2510.04035", "authors": ["Almamoon Alauthman", "Abeer Al-Hyari"], "title": "Analysis of LTE/5G Network Performance Parameters in Smartphone Use Cases: A Study of Packet Loss, Delay, and Slice Types", "comment": null, "summary": "The paper addresses optimizing two of the most important performance\nparameters, packet loss, and delay, in the critical path optimization of LTE\nand 5G networks using metaheuristic algorithms to play a vital role in the\nsmartphone user experience. In this context, nine metaheuristic algorithms,\nsuch as WOA, PSO, and ABC, have been studied for their effectiveness in various\nslices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that\nWOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO\nfollowed closely with a 30% packet loss reduction with a decrease of 6.1 ms in\ndelay. In most scenarios, ABC accomplished good results with a packet loss\nreduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results\nemphasize how selecting appropriate algorithms based on the intended network\nslice is crucial for optimizing resource utilization and network efficiency. It\nprovides a quantitative framework for assessing and improving the reliability\nand responsiveness of an LTE/5G network. It encourages more research in hybrid\noptimization techniques and real-time adaptation mechanisms for further\nimprovements", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316LTE/5G\u7f51\u7edc\u6027\u80fd\uff0cWOA\u8868\u73b0\u6700\u4f73\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u7f51\u7edc\u5207\u7247\u9009\u62e9\u7b97\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u6df7\u5408\u4f18\u5316\u6280\u672f\u3002", "motivation": "\u4f18\u5316LTE\u548c5G\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u6027\u80fd\u53c2\u6570\uff08\u5982\u4e22\u5305\u7387\u548c\u5ef6\u8fdf\uff09\uff0c\u4ee5\u63d0\u5347\u667a\u80fd\u624b\u673a\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u7814\u7a76\u4e86\u4e5d\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u5982WOA\u3001PSO\u3001ABC\uff09\u5728eMBB\u3001URLLC\u548cmMTC\u7f51\u7edc\u5207\u7247\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "WOA\u8868\u73b0\u6700\u4f73\uff0c\u4e22\u5305\u7387\u51cf\u5c1131%\uff0c\u5ef6\u8fdf\u964d\u4f4e6.3\u6beb\u79d2\uff1bPSO\u7d27\u968f\u5176\u540e\uff0c\u4e22\u5305\u7387\u51cf\u5c1130%\uff0c\u5ef6\u8fdf\u964d\u4f4e6.1\u6beb\u79d2\uff1bABC\u5728mMTC\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e22\u5305\u7387\u51cf\u5c1129%\uff0c\u5ef6\u8fdf\u964d\u4f4e6\u6beb\u79d2\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u57fa\u4e8e\u76ee\u6807\u7f51\u7edc\u5207\u7247\u5bf9\u4e8e\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u7f51\u7edc\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u6df7\u5408\u4f18\u5316\u6280\u672f\u548c\u5b9e\u65f6\u9002\u5e94\u673a\u5236\u4ee5\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2510.03504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03504", "abs": "https://arxiv.org/abs/2510.03504", "authors": ["Yutong Wang", "Yichun Qu", "Tengxiang Wang", "Lishuo Pan", "Nora Ayanian"], "title": "Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning", "comment": null, "summary": "Maintaining connectivity is crucial in many multi-robot applications, yet\nfragile to obstacles and visual occlusions. We present a real-time distributed\nframework for multi-robot navigation certified by high-order control barrier\nfunctions (HOCBFs) that controls inter-robot proximity to maintain connectivity\nwhile avoiding collisions. We incorporate control Lyapunov functions to enable\nconnectivity recovery from initial disconnected configurations and temporary\nlosses, providing robust connectivity during navigation in obstacle-rich\nenvironments. Our trajectory generation framework concurrently produces\nplanning and control through a Bezier-parameterized trajectory, which naturally\nprovides smooth curves with arbitrary degree of derivatives. The main\ncontribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory\ngeneration and control method for connectivity maintenance and recovery of\nmulti-robot systems. We validate the framework through extensive simulations\nand a physical experiment with 4 Crazyflie nano-quadrotors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408HOCBFs\u548cCLFs\uff0c\u5b9e\u73b0\u8fde\u63a5\u6027\u7ef4\u62a4\u548c\u6062\u590d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u8fde\u63a5\u6027\u7ef4\u62a4\u5bf9\u969c\u788d\u7269\u548c\u89c6\u89c9\u906e\u6321\u975e\u5e38\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u4fdd\u6301\u548c\u6062\u590d\u8fde\u63a5\u6027\u7684\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\u3002", "method": "\u91c7\u7528\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08HOCBFs\uff09\u548c\u63a7\u5236Lyapunov\u51fd\u6570\uff08CLFs\uff09\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Bezier\u53c2\u6570\u5316\u8f68\u8ff9\u751f\u6210\u5e73\u6ed1\u66f2\u7ebf\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fde\u63a5\u6027\u7ef4\u62a4\u548c\u6062\u590d\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c4\u4e2aCrazyflie\u7eb3\u7c73\u56db\u65cb\u7ffc\u7684\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08HOCBFs\uff09\u7684\u5b9e\u65f6\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u63a7\u5236\u673a\u5668\u4eba\u95f4\u7684\u8ddd\u79bb\u6765\u4fdd\u6301\u8fde\u63a5\u6027\u5e76\u907f\u514d\u78b0\u649e\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u6a21\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation.", "AI": {"tldr": "OneFlow \u662f\u9996\u4e2a\u652f\u6301\u5e76\u53d1\u6df7\u5408\u6a21\u6001\u751f\u6210\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bad\u7ec3\u66f4\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u5728\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u4e4b\u95f4\u5f3a\u5236\u4e25\u683c\u7684\u56e0\u679c\u987a\u5e8f\uff0c\u9650\u5236\u4e86\u751f\u6210\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002OneFlow \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5e76\u53d1\u751f\u6210\u3002", "method": "OneFlow \u7ed3\u5408\u4e86\u57fa\u4e8e\u63d2\u5165\u7684 Edit Flow\uff08\u7528\u4e8e\u79bb\u6563\u6587\u672c\u6807\u8bb0\uff09\u548c Flow Matching\uff08\u7528\u4e8e\u56fe\u50cf\u6f5c\u5728\u8868\u793a\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u91c7\u6837\u5b9e\u73b0\u5e76\u53d1\u6587\u672c\u56fe\u50cf\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOneFlow \u5728 1B \u5230 8B \u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u4f18\u4e8e\u81ea\u56de\u5f52\u57fa\u7ebf\uff0c\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u8868\u73b0\u66f4\u4f73\uff0c\u8bad\u7ec3 FLOPs \u51cf\u5c11\u9ad8\u8fbe 50%\u3002", "conclusion": "OneFlow \u662f\u4e00\u79cd\u521b\u65b0\u7684\u975e\u81ea\u56de\u5f52\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u53d8\u957f\u548c\u5e76\u53d1\u6df7\u5408\u6a21\u6001\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u8ba1\u7b97\u91cf\u3002"}}
{"id": "2510.04371", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u63a8\u6d4b\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u6267\u884c\u9884\u6d4b\u52a8\u4f5c\u663e\u8457\u964d\u4f4eAI\u4ee3\u7406\u7684\u5ef6\u8fdf\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "AI\u4ee3\u7406\u5728\u73af\u5883\u4e2d\u7684\u6267\u884c\u901f\u5ea6\u8f83\u6162\uff0c\u963b\u788d\u4e86\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u3002\u4f8b\u5982\uff0c\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u56fd\u9645\u8c61\u68cb\u4ee3\u7406\u4e4b\u95f4\u7684\u5bf9\u5f08\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u63a8\u6d4b\u52a8\u4f5c\u6846\u67b6\uff0c\u5229\u7528\u5feb\u901f\u6a21\u578b\u9884\u6d4b\u53ef\u80fd\u7684\u52a8\u4f5c\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u591a\u4e2a\u6b65\u9aa4\u3002", "result": "\u5728\u6e38\u620f\u3001\u7535\u5b50\u5546\u52a1\u3001\u7f51\u9875\u641c\u7d22\u7b49\u73af\u5883\u4e2d\uff0c\u63a8\u6d4b\u52a8\u4f5c\u5b9e\u73b0\u4e86\u9ad8\u8fbe55%\u7684\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u63a8\u6d4b\u52a8\u4f5c\u6846\u67b6\u5728\u591a\u4e2a\u4ee3\u7406\u73af\u5883\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u5e76\u901a\u8fc7\u66f4\u5f3a\u7684\u731c\u6d4b\u6a21\u578b\u3001\u591a\u6b65\u63a8\u6d4b\u7b49\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4f4e\u5ef6\u8fdf\u4ee3\u7406\u7cfb\u7edf\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.04536", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04536", "abs": "https://arxiv.org/abs/2510.04536", "authors": ["Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Satoshi Ohshima", "Takahiro Katagiri"], "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG", "comment": null, "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.", "AI": {"tldr": "3Dify\u662f\u4e00\u4e2a\u5229\u7528LLMs\u751f\u62103D-CG\u5185\u5bb9\u7684\u6846\u67b6\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u7528\u6237\u53cd\u9988\u548c\u672c\u5730LLM\u96c6\u6210\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\u548c\u6210\u672c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7b80\u53163D-CG\u5185\u5bb9\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\u548c\u6210\u672c\u3002", "method": "3Dify\u57fa\u4e8eDify\u5e73\u53f0\uff0c\u7ed3\u5408MCP\u548cRAG\u7b49LLM\u6280\u672f\uff0c\u81ea\u52a8\u5316\u64cd\u4f5cDCC\u5de5\u5177\u6216\u901a\u8fc7CUA\u65b9\u6cd5\u81ea\u52a8\u5316GUI\u64cd\u4f5c\uff0c\u652f\u6301\u7528\u6237\u53cd\u9988\u4ee5\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u9ad8\u8d28\u91cf3D-CG\u5185\u5bb9\u7684\u6846\u67b6\uff0c\u652f\u6301\u7528\u6237\u53cd\u9988\u548c\u672c\u5730LLM\u96c6\u6210\u3002", "conclusion": "3Dify\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLMs\u548cDCC\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u62103D-CG\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u5e76\u652f\u6301\u7528\u6237\u53cd\u9988\u548c\u672c\u5730LLM\u96c6\u6210\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u7ecf\u6d4e\u6210\u672c\u3002"}}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs .", "AI": {"tldr": "APIDA-Chat\u901a\u8fc7\u5f00\u6e90\u7ba1\u9053\u751f\u6210\u4f4e\u6210\u672c\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u5728API\u641c\u7d22\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u91ca\u5c0f\u4f17\u6216\u4e13\u6709API\u65f6\u56e0\u7f3a\u4e4f\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "APIDA-Chat\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u7ed3\u5408\u4f20\u7edf\u5bf9\u8bdd\u89c4\u5212\u5668\u4e0e\u9ad8\u6027\u80fd\u6559\u5e08LLM\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6570\u636e\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u5fae\u8c03\u540e\u7684\u5b66\u751f\u6a21\u578b\u5b9e\u73b0\u4f4e\u6210\u672c\u5bf9\u8bdd\u751f\u6210\u3002", "result": "\u5fae\u8c03\u540e\u7684\u5b66\u751f\u6a21\u578b\u5728BLEU\u548cBERTScore\u4e0a\u5206\u522b\u4ece0.38\u63d0\u5347\u81f30.50\u548c0.88\u63d0\u5347\u81f30.91\u3002", "conclusion": "APIDA-Chat\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u7ba1\u9053\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u751f\u6210\u4f4e\u6210\u672c\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u5728API\u641c\u7d22\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u4e14\u5b8c\u5168\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002"}}
{"id": "2510.04052", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04052", "abs": "https://arxiv.org/abs/2510.04052", "authors": ["Behrooz Farkiani", "Fan Liu", "Patrick Crowley"], "title": "The Door to Policy Portability might be an IP Overlay", "comment": null, "summary": "Portable service mesh implementations enable layer 4 to layer 7 policy\nenforcement across diverse infrastructures, but they remain tied to\ninfrastructure-specific layer 3 network policies. Network policies enable\ncontrol over IP traffic flow regardless of whether traffic is authorized at the\napplication level. However, not all infrastructure supports enforcing them, and\nachieving consistent enforcement across heterogeneous environments is\nchallenging. For example, studies have shown that the majority of Kubernetes\nclusters do not enforce any network policies. We propose integrating network\npolicy enforcement with service meshes to protect data-plane traffic in a\nportable, infrastructure-agnostic way. This enables developers to define\nintegrated layer 3 to layer 7 policies and ensure they are enforced across any\ninfrastructure. Additionally, due to its portability, our approach can be used\noutside the service environment to enforce policies on end-user traffic and\nprovide an end-to-end secure extended overlay. Our solution builds an overlay\nlayer 3 network and enforces layer 3 policies by routing traffic through\nspecific policy enforcement points and utilizing authorization keys. We\nprototyped our idea using Kubernetes and Istio, and show that while it adds\nless than 1ms latency, it can implement complex policies comparable to\nKubernetes native network policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7f51\u7edc\u7b56\u7565\u6267\u884c\u4e0e\u670d\u52a1\u7f51\u683c\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u4ee5\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u4fbf\u643a\u5f0f\u7b56\u7565\u6267\u884c\uff0c\u539f\u578b\u9a8c\u8bc1\u4e86\u5176\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4fbf\u643a\u5f0f\u670d\u52a1\u7f51\u683c\u5b9e\u73b0\u867d\u7136\u652f\u6301\u7b2c4\u5c42\u5230\u7b2c7\u5c42\u7684\u7b56\u7565\u6267\u884c\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u57fa\u7840\u8bbe\u65bd\u7279\u5b9a\u7684\u7b2c3\u5c42\u7f51\u7edc\u7b56\u7565\uff0c\u4e14\u8de8\u5f02\u6784\u73af\u5883\u7684\u4e00\u81f4\u6267\u884c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8986\u76d6\u5c42\u7b2c3\u5c42\u7f51\u7edc\uff0c\u5e76\u5c06\u6d41\u91cf\u8def\u7531\u81f3\u7279\u5b9a\u7684\u7b56\u7565\u6267\u884c\u70b9\uff0c\u540c\u65f6\u5229\u7528\u6388\u6743\u5bc6\u94a5\u6765\u6267\u884c\u7b2c3\u5c42\u7b56\u7565\u3002", "result": "\u539f\u578b\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Kubernetes\u548cIstio\u4e0a\u7684\u5b9e\u73b0\u589e\u52a0\u4e86\u4e0d\u52301\u6beb\u79d2\u7684\u5ef6\u8fdf\uff0c\u4f46\u80fd\u591f\u6267\u884c\u4e0eKubernetes\u539f\u751f\u7f51\u7edc\u7b56\u7565\u76f8\u5f53\u7684\u590d\u6742\u7b56\u7565\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7f51\u7edc\u7b56\u7565\u6267\u884c\u4e0e\u670d\u52a1\u7f51\u683c\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u7684\u4fbf\u643a\u5f0f\u3001\u57fa\u7840\u8bbe\u65bd\u65e0\u5173\u7684\u7b56\u7565\u6267\u884c\uff0c\u4ece\u800c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4ece\u7b2c3\u5c42\u5230\u7b2c7\u5c42\u7684\u7efc\u5408\u7b56\u7565\u63a7\u5236\u3002"}}
{"id": "2510.03529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03529", "abs": "https://arxiv.org/abs/2510.03529", "authors": ["Zekai Liang", "Xiao Liang", "Soofiyan Atar", "Sreyan Das", "Zoe Chiu", "Peihan Zhang", "Florian Richter", "Shanglei Liu", "Michael C. Yip"], "title": "LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy", "comment": null, "summary": "Robotic laparoscopic surgery has gained increasing attention in recent years\nfor its potential to deliver more efficient and precise minimally invasive\nprocedures. However, adoption of surgical robotic platforms remains largely\nconfined to high-resource medical centers, exacerbating healthcare disparities\nin rural and low-resource regions. To close this gap, a range of solutions has\nbeen explored, from remote mentorship to fully remote telesurgery. Yet, the\npractical deployment of surgical robotic systems to underserved communities\nremains an unsolved challenge. Humanoid systems offer a promising path toward\ndeployability, as they can directly operate in environments designed for humans\nwithout extensive infrastructure modifications -- including operating rooms. In\nthis work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic\nteleoperation framework. The system leverages an inverse-mapping strategy for\nmanual-wristed laparoscopic instruments that abides to remote center-of-motion\nconstraints, enabling precise hand-to-tool control of off-the-shelf surgical\nlaparoscopic tools without additional setup requirements. A control console\nequipped with a stereo vision system provides real-time visual feedback.\nFinally, a comprehensive user study across platforms demonstrates the\neffectiveness of the proposed framework and provides initial evidence for the\nfeasibility of deploying humanoid robots in laparoscopic procedures.", "AI": {"tldr": "LapSurgie\u662f\u9996\u4e2a\u57fa\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8179\u8154\u955c\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u6620\u5c04\u7b56\u7565\u548c\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u90e8\u7f72\u7684\u96be\u9898\uff0c\u7f29\u5c0f\u533b\u7597\u8d44\u6e90\u5dee\u8ddd\u3002\u4eba\u5f62\u673a\u5668\u4eba\u56e0\u5176\u65e0\u9700\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u6539\u9020\u7684\u4f18\u52bf\uff0c\u6210\u4e3a\u53ef\u884c\u8def\u5f84\u3002", "method": "\u91c7\u7528\u9006\u6620\u5c04\u7b56\u7565\u63a7\u5236\u624b\u52a8\u8155\u5f0f\u8179\u8154\u955c\u5668\u68b0\uff0c\u9075\u5b88\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u7ea6\u675f\uff0c\u65e0\u9700\u989d\u5916\u8bbe\u7f6e\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684\u624b\u5230\u5de5\u5177\u63a7\u5236\u3002\u63a7\u5236\u53f0\u914d\u5907\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u901a\u8fc7\u8de8\u5e73\u53f0\u7684\u5168\u9762\u7528\u6237\u7814\u7a76\uff0c\u8bc1\u660e\u4e86LapSurgie\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8179\u8154\u955c\u624b\u672f\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "LapSurgie\u6846\u67b6\u901a\u8fc7\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u8179\u8154\u955c\u624b\u672f\u7684\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8179\u8154\u955c\u624b\u672f\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03605", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures.", "AI": {"tldr": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9700\u8bad\u7ec3\u6570\u636e\u591a\u6837\u4e14\u4efb\u52a1\u96be\u5ea6\u9002\u4e2d\uff0c\u5426\u5219\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002", "motivation": "\u63a2\u7a76\u6d4b\u8bd5\u65f6\u6269\u5c55\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u957f\u94fe\u5f0f\u601d\u8003\uff08CoTs\uff09\u51fa\u73b0\u53ca\u5176\u6027\u80fd\u6539\u8fdb\u7684\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5728\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u8bad\u7ec3\u7684\u53d8\u6362\u6a21\u578b\uff0c\u7814\u7a76\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\u3002", "result": "\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u51cf\u5c11\u8bad\u7ec3\u63d0\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\uff1b\u82e5\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u6240\u9700\u7684\u6280\u80fd\uff0c\u589e\u52a0\u8ba1\u7b97\u4f1a\u635f\u5bb3\u6027\u80fd\uff1b\u4efb\u52a1\u96be\u5ea6\u901a\u8fc7\u7279\u5f81\u534f\u65b9\u5dee\u77e9\u9635\u7684\u6700\u5c0f\u7279\u5f81\u503c\u8868\u5f81\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u4efb\u52a1\u96be\u5ea6\u5bf9\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u7279\u5f81\u534f\u65b9\u5dee\u77e9\u9635\u7684\u6700\u5c0f\u7279\u5f81\u503c\u8f83\u5927\u65f6\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2510.04952", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04952", "abs": "https://arxiv.org/abs/2510.04952", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits", "comment": "22 pages, 2 figures", "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5408\u89c4\u4ee3\u7406\u7684\u7b97\u6cd5\u4ea4\u6613\u7cfb\u7edf\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5408\u89c4\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u7b97\u6cd5\u4ea4\u6613\u4e2d\u6267\u884c\u8d28\u91cf\u4e0e\u5408\u89c4\u6267\u884c\u7684\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u5e02\u573a\u73af\u5883\u4e0b\uff0c\u786e\u4fdd\u4ea4\u6613\u884c\u4e3a\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002", "method": "\u7cfb\u7edf\u67b6\u6784\u5305\u62ec\u9ad8\u7ea7\u89c4\u5212\u5668\u3001\u5f3a\u5316\u5b66\u4e60\u6267\u884c\u4ee3\u7406\u548c\u72ec\u7acb\u5408\u89c4\u4ee3\u7406\u3002\u6267\u884c\u4ee3\u7406\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd0\u884c\u65f6\u52a8\u4f5c\u5c4f\u853d\u786e\u4fdd\u52a8\u4f5c\u5b89\u5168\u3002\u5408\u89c4\u5ba1\u8ba1\u5c42\u91c7\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u6280\u672f\uff0c\u786e\u4fdd\u5408\u89c4\u6027\u9a8c\u8bc1\u4e0d\u6cc4\u9732\u4e13\u6709\u4fe1\u53f7\u3002", "result": "\u5728ABIDES\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\uff0c\u5b66\u4e60\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u6267\u884c\u843d\u5dee\u548c\u65b9\u5dee\uff0c\u4e14\u5728\u5404\u79cd\u538b\u529b\u6d4b\u8bd5\u4e2d\u672a\u51fa\u73b0\u7ea6\u675f\u8fdd\u89c4\u3002\u7ed3\u679c\u572895%\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5e02\u573a\u7b97\u6cd5\u4ea4\u6613\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6267\u884c\u4ee3\u7406\u548c\u72ec\u7acb\u5408\u89c4\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u6267\u884c\u8d28\u91cf\u4e0e\u4e25\u683c\u5408\u89c4\u6267\u884c\u7684\u5e73\u8861\u3002\u7cfb\u7edf\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u6267\u884c\u843d\u5dee\u548c\u65b9\u5dee\uff0c\u4e14\u672a\u89c2\u5bdf\u5230\u7ea6\u675f\u8fdd\u89c4\u3002"}}
{"id": "2510.04539", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04539", "abs": "https://arxiv.org/abs/2510.04539", "authors": ["Zeng Tao", "Zheng Ding", "Zeyuan Chen", "Xiang Zhang", "Leizhi Li", "Zhuowen Tu"], "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing", "comment": null, "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.", "AI": {"tldr": "C3Editor\u662f\u4e00\u4e2a\u53ef\u63a7\u4e14\u4e00\u81f4\u76842D-lifting-based 3D\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5efa\u7acb\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4e0a\u7684\u6311\u6218\uff0c\u5e76\u5728\u7f16\u8f91\u6548\u679c\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76842D-lifting-based 3D\u7f16\u8f91\u65b9\u6cd5\u5e38\u56e0\u7f3a\u4e4f\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\u548c\u96be\u4ee5\u786e\u4fdd\u591a\u89c6\u56fe\u7f16\u8f91\u4e00\u81f4\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u9009\u62e9GT\u89c6\u56fe\u53ca\u5176\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u7136\u540e\u5728GT\u89c6\u56fe\u548c\u591a\u4e2a\u89c6\u56fe\u4e2d\u5fae\u8c032D\u7f16\u8f91\u6a21\u578b\uff0c\u540c\u65f6\u5f15\u5165\u5355\u72ec\u7684LoRA\u6a21\u5757\u4ee5\u6ee1\u8db3GT\u89c6\u56fe\u62df\u5408\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u7684\u4e0d\u540c\u9700\u6c42\u3002", "result": "C3Editor\u57282D\u548c3D\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4e00\u81f4\u548c\u53ef\u63a7\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "C3Editor\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u5efa\u7acb\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u67092D-lifting-based\u65b9\u6cd5\u66f4\u4e00\u81f4\u548c\u53ef\u63a7\u76842D\u53ca3D\u7f16\u8f91\u6548\u679c\uff0c\u5e76\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me.", "AI": {"tldr": "\u5f00\u6e90\u4ee3\u7801\u8865\u5168\u63d2\u4ef6Code4MeV2\u4e3a\u5b66\u672f\u754c\u63d0\u4f9b\u900f\u660e\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u6027\u80fd\u5ab2\u7f8e\u5546\u4e1a\u5de5\u5177\uff0c\u83b7\u7528\u6237\u79ef\u6781\u8bc4\u4ef7\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u8865\u5168\u5de5\u5177\u7684\u7528\u6237\u4ea4\u4e92\u6570\u636e\u591a\u4e3a\u5927\u578b\u4f01\u4e1a\u79c1\u6709\uff0c\u963b\u788d\u4e86\u5b66\u672f\u754c\u7684\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u3002", "method": "\u91c7\u7528\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u8bbe\u8ba1\u652f\u6301\u5185\u8054\u4ee3\u7801\u8865\u5168\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u804a\u5929\u52a9\u624b\uff0c\u5e76\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\u3002\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u548c8\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5de5\u5177\u6027\u80fd\u3002", "result": "Code4MeV2\u5728\u4ee3\u7801\u8865\u5168\u6027\u80fd\u4e0a\u8fbe\u5230\u884c\u4e1a\u6c34\u5e73\uff08\u5e73\u5747\u5ef6\u8fdf200\u6beb\u79d2\uff09\uff0c\u4e13\u5bb6\u548c\u7528\u6237\u53cd\u9988\u663e\u793a\u5176\u4fe1\u606f\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Code4MeV2\u662f\u4e00\u6b3e\u5f00\u6e90\u3001\u6a21\u5757\u5316\u7684\u4ee3\u7801\u8865\u5168\u63d2\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u5b66\u672f\u754c\u5728AI\u4ee3\u7801\u8865\u5168\u5de5\u5177\u7814\u7a76\u4e2d\u9762\u4e34\u7684\u6570\u636e\u8bbf\u95ee\u96be\u9898\u3002\u901a\u8fc7\u5176\u900f\u660e\u6570\u636e\u6536\u96c6\u6846\u67b6\u548c\u884c\u4e1a\u7ea7\u6027\u80fd\u8868\u73b0\uff0c\u8be5\u5de5\u5177\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04183", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04183", "abs": "https://arxiv.org/abs/2510.04183", "authors": ["Lucas Pacheco", "Torsten Braun", "Kaushik Chowdhury", "Denis Ros\u00e1rio", "Batool Salehi", "Eduardo Cerqueira"], "title": "Dynamic Adaptive Federated Learning for mmWave Sector Selection", "comment": null, "summary": "Beamforming techniques use massive antenna arrays to formulate narrow\nLine-of-Sight signal sectors to address the increased signal attenuation in\nmillimeter Wave (mmWave). However, traditional sector selection schemes involve\nextensive searches for the highest signal-strength sector, introducing extra\nlatency and communication overhead. This paper introduces a dynamic layer-wise\nand clustering-based federated learning (FL) algorithm for beam sector\nselection in autonomous vehicle networks called enhanced Dynamic Adaptive FL\n(eDAFL). The algorithm detects and selects the most important layers of a\nmachine learning model for aggregation in the FL process, significantly\nreducing network overhead and failure risks. eDAFL also considers intra-cluster\nand inter-cluster approaches to reduce overfitting and increase the abstraction\nlevel. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating\nimproved model accuracy by approximately 6.76% compared to existing methods,\nwhile reducing inference time by 84.04% and model size by up to 52.20%.", "AI": {"tldr": "eDAFL\u662f\u4e00\u79cd\u52a8\u6001\u5206\u5c42\u548c\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7f51\u7edc\u7684\u6ce2\u675f\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u6ce2\u675f\u9009\u62e9\u65b9\u6848\u6d89\u53ca\u5e7f\u6cdb\u7684\u4fe1\u53f7\u5f3a\u5ea6\u641c\u7d22\uff0c\u5bfc\u81f4\u989d\u5916\u7684\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6ce2\u675f\u9009\u62e9\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5206\u5c42\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u9009\u62e9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u6700\u5173\u952e\u7684\u5c42\u8fdb\u884c\u805a\u5408\uff0c\u51cf\u5c11\u7f51\u7edc\u5f00\u9500\u3002\u540c\u65f6\u8003\u8651\u96c6\u7fa4\u5185\u548c\u96c6\u7fa4\u95f4\u7684\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u9ad8\u62bd\u8c61\u7ea7\u522b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0ceDAFL\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea66.76%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8684.04%\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e8652.20%\u3002", "conclusion": "eDAFL\u7b97\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7f51\u7edc\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6ce2\u675f\u9009\u62e9\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u7f51\u7edc\u5f00\u9500\u548c\u6545\u969c\u98ce\u9669\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.03532", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03532", "abs": "https://arxiv.org/abs/2510.03532", "authors": ["Zekai Liang", "Kazuya Miyata", "Xiao Liang", "Florian Richter", "Michael C. Yip"], "title": "Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection", "comment": null, "summary": "Accurate camera-to-robot calibration is essential for any vision-based\nrobotic control system and especially critical in minimally invasive surgical\nrobots, where instruments conduct precise micro-manipulations. However, MIS\nrobots have long kinematic chains and partial visibility of their degrees of\nfreedom in the camera, which introduces challenges for conventional\ncamera-to-robot calibration methods that assume stiff robots with good\nvisibility. Previous works have investigated both keypoint-based and\nrendering-based approaches to address this challenge in real-world conditions;\nhowever, they often struggle with consistent feature detection or have long\ninference times, neither of which are ideal for online robot control. In this\nwork, we propose a novel framework that unifies the detection of geometric\nprimitives (keypoints and shaft edges) through a shared encoding, enabling\nefficient pose estimation via projection geometry. This architecture detects\nboth keypoints and edges in a single inference and is trained on large-scale\nsynthetic data with projective labeling. This method is evaluated across both\nfeature detection and pose estimation, with qualitative and quantitative\nresults demonstrating fast performance and state-of-the-art accuracy in\nchallenging surgical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u68c0\u6d4b\u51e0\u4f55\u57fa\u5143\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u548c\u6295\u5f71\u51e0\u4f55\u5b9e\u73b0\u9ad8\u6548\u59ff\u6001\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u673a\u5668\u4eba\u76f8\u673a-\u673a\u5668\u4eba\u6821\u51c6\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u5fae\u521b\u624b\u672f\u673a\u5668\u4eba\u56e0\u5176\u957f\u8fd0\u52a8\u94fe\u548c\u90e8\u5206\u81ea\u7531\u5ea6\u53ef\u89c1\u6027\uff0c\u7ed9\u4f20\u7edf\u76f8\u673a-\u673a\u5668\u4eba\u6821\u51c6\u65b9\u6cd5\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u7edf\u4e00\u68c0\u6d4b\u51e0\u4f55\u57fa\u5143\uff08\u5173\u952e\u70b9\u548c\u8f74\u8fb9\u7f18\uff09\uff0c\u5e76\u5229\u7528\u6295\u5f71\u51e0\u4f55\u8fdb\u884c\u9ad8\u6548\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u7279\u5f81\u68c0\u6d4b\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u5747\u8868\u73b0\u51fa\u5feb\u901f\u6027\u80fd\u548c\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u624b\u672f\u73af\u5883\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u5728\u624b\u672f\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a-\u673a\u5668\u4eba\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc\uff08CPS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u548c\u6587\u672c\u4fee\u6539\u6765\u64cd\u7eb5\u4ee3\u7406\u51b3\u7b56\uff0c\u5728\u73b0\u5b9e\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4fdd\u6301\u9ad8\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5047\u8bbe\u5f3a\u5927\u7684\u767d\u76d2\u8bbf\u95ee\u6743\u9650\uff0c\u8981\u4e48\u4f7f\u7528\u4e0d\u5207\u5b9e\u9645\u7684\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5728\u73b0\u5b9e\u7684\u9ed1\u76d2\u5a01\u80c1\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\u6765\u64cd\u7eb5\u4ee3\u7406\u504f\u597d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc\uff08CPS\uff09\u7684\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4e0d\u53ef\u5bdf\u89c9\u4fee\u6539\uff0c\u5229\u7528CLIP\u53ef\u8f6c\u79fb\u56fe\u50cf\u6270\u52a8\u548cRLHF\u5f15\u8d77\u7684\u8bed\u8a00\u504f\u89c1\u6765\u5f15\u5bfc\u4ee3\u7406\u51b3\u7b56\u3002", "result": "CPS\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u630170%\u66f4\u4f4e\u7684\u68c0\u6d4b\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u968f\u7740\u4ee3\u7406\u7cfb\u7edf\u5728\u793e\u4f1a\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2510.04637", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04637", "abs": "https://arxiv.org/abs/2510.04637", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "comment": "SIGGRAPH ASIA 2025 (Conference Track); Project page:\n  https://pku-mocca.github.io/Social-Agent-Page/", "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors.", "AI": {"tldr": "Social Agent\u6846\u67b6\u901a\u8fc7LLM\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u534f\u8c03\u7684\u975e\u8a00\u8bed\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u4ea4\u4e92\u8d28\u91cf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5bf9\u8bdd\u4e2d\u975e\u8a00\u8bed\u884c\u4e3a\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u4e0a\u4e0b\u6587\u9002\u5f53\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u6307\u5bfc\u5bf9\u8bdd\u6d41\u7a0b\u548c\u786e\u5b9a\u4e92\u52a8\u884c\u4e3a\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u7684\u53cc\u4eba\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u4ece\u8bed\u97f3\u4fe1\u53f7\u5408\u6210\u534f\u8c03\u52a8\u4f5c\u3002", "result": "\u7528\u6237\u7814\u7a76\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u53cc\u4eba\u4ea4\u4e92\u7684\u8d28\u91cf\uff0c\u751f\u6210\u4e86\u81ea\u7136\u4e14\u540c\u6b65\u7684\u975e\u8a00\u8bed\u884c\u4e3a\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684Social Agent\u6846\u67b6\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53cc\u4eba\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u4e2d\u975e\u8a00\u8bed\u884c\u4e3a\u7684\u81ea\u7136\u5ea6\u548c\u540c\u6b65\u6027\u3002"}}
{"id": "2510.03802", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03802", "abs": "https://arxiv.org/abs/2510.03802", "authors": ["Gilberto Recupito", "Vincenzo De Martino", "Dario Di Nucci", "Fabio Palomba"], "title": "A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt", "comment": "Accepted at the International Workshop of Software Quality Assurance\n  for Artificial Intelligence 2025 (SQA4AI), Montr\\'eal, Canada", "summary": "The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized\nsoftware development, driving innovation across various domains. However, these\nsystems also introduce unique challenges, particularly in maintaining software\nquality and performance. Among these challenges, Self-Admitted Technical Debt\n(SATD) has emerged as a growing concern, significantly impacting the\nmaintainability and overall quality of ML and DL-enabled systems. Despite its\ncritical implications, the lifecycle of DL-specific SATD, how developers\nintroduce, acknowledge, and address it over time-remains underexplored. This\nstudy presents a preliminary analysis of the persistence and lifecycle of\nDL-specific SATD in DL-enabled systems. The purpose of this project is to\nuncover the patterns of SATD introduction, recognition, and durability during\nthe development life cycle, providing information on how to manage these\nissues. Using mining software repository techniques, we examined 40 ML\nprojects, focusing on 185 DL-specific SATD instances. The analysis tracked the\nintroduction and persistence of SATD instances through project commit histories\nto assess their lifecycle and developer actions. The findings indicate that\nDL-specific SATD is predominantly introduced during the early and middle stages\nof project development. Training and Hardware phases showed the longest SATD\ndurations, highlighting critical areas where debt accumulates and persists.\nAdditionally, developers introduce DL-specific SATD more frequently during\nfeature implementation and bug fixes. This study emphasizes the need for\ntargeted DL-specific SATD management strategies in DL-enabled systems to\nmitigate its impact. By understanding the temporal characteristics and\nevolution of DL-specific SATD, developers can prioritize interventions at\ncritical stages to improve the maintainability and quality of the system.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6280\u672f\u503a\u52a1\uff08SATD\uff09\u7814\u7a76\u63ed\u793a\u5176\u5728\u5f00\u53d1\u65e9\u671f\u7684\u5f15\u5165\u6a21\u5f0f\u548c\u6301\u4e45\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u548c\u786c\u4ef6\u9636\u6bb5\uff0c\u9700\u9488\u5bf9\u6027\u7ba1\u7406\u7b56\u7565\u4ee5\u63d0\u5347\u7cfb\u7edf\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u5feb\u901f\u91c7\u7528\u5e26\u6765\u4e86\u8f6f\u4ef6\u8d28\u91cf\u548c\u6027\u80fd\u7ef4\u62a4\u7684\u72ec\u7279\u6311\u6218\uff0c\u5c24\u5176\u662f\u81ea\u6211\u627f\u8ba4\u7684\u6280\u672f\u503a\u52a1\uff08SATD\uff09\u5bf9\u7cfb\u7edf\u53ef\u7ef4\u62a4\u6027\u548c\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\u6280\u672f\uff0c\u5206\u6790\u4e8640\u4e2a\u673a\u5668\u5b66\u4e60\u9879\u76ee\u4e2d\u7684185\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7279\u5b9aSATD\u5b9e\u4f8b\uff0c\u901a\u8fc7\u9879\u76ee\u63d0\u4ea4\u5386\u53f2\u8ffd\u8e2aSATD\u7684\u5f15\u5165\u548c\u6301\u4e45\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u7279\u5b9aSATD\u4e3b\u8981\u5728\u9879\u76ee\u5f00\u53d1\u7684\u65e9\u671f\u548c\u4e2d\u671f\u9636\u6bb5\u5f15\u5165\uff0c\u8bad\u7ec3\u548c\u786c\u4ef6\u9636\u6bb5\u663e\u793a\u51fa\u6700\u957f\u7684SATD\u6301\u7eed\u65f6\u95f4\uff0c\u5f00\u53d1\u8005\u66f4\u5e38\u5728\u529f\u80fd\u5b9e\u73b0\u548c\u9519\u8bef\u4fee\u590d\u65f6\u5f15\u5165SATD\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u9488\u5bf9\u7279\u5b9a\u6280\u672f\u503a\u52a1\uff08SATD\uff09\u7ba1\u7406\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u7406\u89e3\u5176\u65f6\u95f4\u7279\u6027\u548c\u6f14\u53d8\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u5728\u5173\u952e\u9636\u6bb5\u4f18\u5148\u5e72\u9884\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2510.04346", "categories": ["cs.NI", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.04346", "abs": "https://arxiv.org/abs/2510.04346", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins", "comment": "Code: https://github.com/nahshonmokua/LoRaWAN-Indoor-PL-parametrics", "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u73af\u5883\u611f\u77e5\u7684\u8def\u5f84\u635f\u8017\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u4f18\u5316\u5ba4\u5185LoRaWAN\u4f20\u64ad\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5ba4\u5185LoRaWAN\u4f20\u64ad\u53d7\u7ed3\u6784\u548c\u65f6\u95f4\u53d8\u5316\u7684\u4e0a\u4e0b\u6587\u56e0\u7d20\u5f71\u54cd\uff0c\u6311\u6218\u4e86\u5bf9\u6570\u8ddd\u79bb\u6a21\u578b\u548c\u5bf9\u6570\u6b63\u6001\u9634\u5f71\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u589e\u5f3a\u5bf9\u6570\u8ddd\u79bb\u591a\u5899\u5747\u503c\u4e0e\u73af\u5883\u534f\u53d8\u91cf\uff08\u5982\u76f8\u5bf9\u6e7f\u5ea6\u3001\u6e29\u5ea6\u3001\u4e8c\u6c27\u5316\u78b3\u3001\u9897\u7c92\u7269\u548c\u6c14\u538b\uff09\u4ee5\u53ca\u4fe1\u566a\u6bd4\uff0c\u6bd4\u8f83\u4e86\u591a\u5143\u7ebf\u6027\u56de\u5f52\u4e0e\u6b63\u5219\u5316\u53d8\u4f53\u3001\u8d1d\u53f6\u65af\u7ebf\u6027\u56de\u5f52\u548c\u9009\u62e9\u6027\u4e8c\u9636\u591a\u9879\u5f0f\u5e94\u7528\u4e8e\u8fde\u7eed\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u591a\u9879\u5f0f\u5747\u503c\u5c06\u4ea4\u53c9\u9a8c\u8bc1\u7684RMSE\u4ece8.07\u964d\u4f4e\u52307.09 dB\uff0c\u5e76\u5c06R^2\u4ece0.81\u63d0\u9ad8\u52300.86\u3002\u572899%\u7684\u6570\u636e\u5305\u4f20\u9012\u6bd4\u7387\u4e0b\uff0c\u73af\u5883\u611f\u77e5\u591a\u9879\u5f0f\u9700\u898125.7 dB\uff0c\u800c\u7ebf\u6027\u57fa\u7ebf\u9700\u898127.7\u81f327.9 dB\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u90e8\u7f72\u5c31\u7eea\u3001\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5177\u6709\u6821\u51c6\u7684\u53ef\u9760\u6027\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u7269\u8054\u7f51\u89c4\u5212\uff0c\u5e76\u4e0e6G\u76ee\u6807\u5bf9\u9f50\u3002"}}
{"id": "2510.03547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03547", "abs": "https://arxiv.org/abs/2510.03547", "authors": ["Carina Veil", "Moritz Flaschel", "Ellen Kuhl"], "title": "Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots", "comment": null, "summary": "Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary\nflexibility to bend, twist, and elongate in ways that rigid robots cannot.\nHowever, their motion planning remains a challenge, especially in cluttered\nenvironments with obstacles, due to their highly nonlinear and\ninfinite-dimensional kinematics. Here, we present a graph-based path planning\ntool for an elephant-trunk-inspired soft robotic arm designed with three\nartificial muscle fibers that allow for multimodal continuous deformation\nthrough contraction. Using a biomechanical model inspired by morphoelasticity\nand active filament theory, we precompute a shape library and construct a\n$k$-nearest neighbor graph in \\emph{shape space}, ensuring that each node\ncorresponds to a mechanically accurate and physically valid robot shape. For\nthe graph, we use signed distance functions to prune nodes and edges colliding\nwith obstacles, and define multi-objective edge costs based on geometric\ndistance and actuation effort, enabling energy-efficient planning with\ncollision avoidance. We demonstrate that our algorithm reliably avoids\nobstacles and generates feasible paths within milliseconds from precomputed\ngraphs using Dijkstra's algorithm. We show that including energy costs can\ndrastically reduce the actuation effort compared to geometry-only planning, at\nthe expense of longer tip trajectories. Our results highlight the potential of\nshape-space graph search for fast and reliable path planning in the field of\nsoft robotics, paving the way for real-time applications in surgical,\nindustrial, and assistive settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u72b6\u7a7a\u95f4\u7684\u56fe\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u8c61\u9f3b\u542f\u53d1\u7684\u8f6f\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u53ef\u884c\u8def\u5f84\u5e76\u4f18\u5316\u80fd\u91cf\u6d88\u8017\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\uff08\u5982\u8c61\u9f3b\u6216\u7ae0\u9c7c\u81c2\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u5728\u6742\u4e71\u73af\u5883\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u5176\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u65e0\u9650\u7ef4\u8fd0\u52a8\u5b66\u7279\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8def\u5f84\u89c4\u5212\u5de5\u5177\uff0c\u5229\u7528\u751f\u7269\u529b\u5b66\u6a21\u578b\u9884\u8ba1\u7b97\u5f62\u72b6\u5e93\uff0c\u5e76\u5728\u5f62\u72b6\u7a7a\u95f4\u4e2d\u6784\u5efak\u8fd1\u90bb\u56fe\uff0c\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u4fee\u526a\u78b0\u649e\u8282\u70b9\u548c\u8fb9\uff0c\u5b9a\u4e49\u57fa\u4e8e\u51e0\u4f55\u8ddd\u79bb\u548c\u9a71\u52a8\u529b\u7684\u591a\u76ee\u6807\u8fb9\u6210\u672c\u3002", "result": "\u7b97\u6cd5\u80fd\u5728\u6beb\u79d2\u7ea7\u65f6\u95f4\u5185\u53ef\u9760\u907f\u5f00\u969c\u788d\u7269\u5e76\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u4e14\u5305\u542b\u80fd\u91cf\u6210\u672c\u53ef\u663e\u8457\u51cf\u5c11\u9a71\u52a8\u529b\uff0c\u5c3d\u7ba1\u8f68\u8ff9\u957f\u5ea6\u589e\u52a0\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u5f62\u72b6\u7a7a\u95f4\u7684\u56fe\u641c\u7d22\u5728\u8f6f\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\uff08\u5982\u624b\u672f\u3001\u5de5\u4e1a\u548c\u8f85\u52a9\u573a\u666f\uff09\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.", "AI": {"tldr": "MITS\u662f\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u65b0\u578b\u6811\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7PMI\u8bc4\u5206\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u9ad8\u6548\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6811\u641c\u7d22\u65b9\u6cd5\u5728\u5373\u65f6\u53ef\u9760\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8d28\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "MITS\u6846\u67b6\u5f15\u5165\u4e86\u57fa\u4e8e\u70b9\u4e92\u4fe1\u606f\uff08PMI\uff09\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u901a\u8fc7\u675f\u641c\u7d22\u6269\u5c55\u641c\u7d22\u6811\uff0c\u65e0\u9700\u6602\u8d35\u7684\u8d85\u524d\u6a21\u62df\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMITS\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MITS\u901a\u8fc7\u4fe1\u606f\u8bba\u539f\u5219\u6307\u5bfc\u63a8\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e92\u4fe1\u606f\u7684\u6709\u6548\u8bc4\u5206\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u8def\u5f84\u7684\u9010\u6b65\u8bc4\u4f30\u548c\u641c\u7d22\u6811\u7684\u9ad8\u6548\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.04999", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04999", "abs": "https://arxiv.org/abs/2510.04999", "authors": ["Nilay Kumar", "Priyansh Bhandari", "G. Maragatham"], "title": "Bridging Text and Video Generation: A Survey", "comment": null, "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86T2V\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\uff0c\u6db5\u76d6\u4e86\u4eceGANs\u5230DiT\u67b6\u6784\u7684\u6f14\u53d8\u3001\u8bad\u7ec3\u914d\u7f6e\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u6027\u80fd\u5206\u6790\u3002", "motivation": "T2V\u6280\u672f\u5728\u6559\u80b2\u3001\u8425\u9500\u3001\u5a31\u4e50\u7b49\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5bf9\u9f50\u3001\u957f\u671f\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7b49\u6311\u6218\uff0c\u9700\u5168\u9762\u68b3\u7406\u5176\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8c03\u67e5\uff0c\u4ece\u65e9\u671fGANs\u548cVAEs\u5230\u6df7\u5408Diffusion-Transformer\uff08DiT\uff09\u67b6\u6784\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u3001\u6539\u8fdb\u4e4b\u5904\u53ca\u65b0\u67b6\u6784\u7684\u5fc5\u8981\u6027\u3002", "result": "\u7efc\u8ff0\u4e86T2V\u6a21\u578b\u7684\u8bad\u7ec3\u914d\u7f6e\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u8f6c\u5411\u66f4\u5168\u9762\u3001\u611f\u77e5\u5bf9\u9f50\u7b56\u7565\u7684\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u6280\u672f\u7684\u5f53\u524d\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u89c6\u89d2\u3002"}}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "Jos\u00e9 Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Fr\u00f6mmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide.", "AI": {"tldr": "Smart Paste\u662f\u4e00\u4e2aIDE\u529f\u80fd\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u7c98\u8d34\u4ee3\u7801\u540e\u7684\u7f16\u8f91\u9700\u6c42\uff0c\u5728Google\u5185\u90e8\u83b7\u5f9745%\u7684\u63a5\u53d7\u7387\uff0c\u5360\u516c\u53f8\u4ee3\u7801\u7f16\u5199\u76841%\u3002", "motivation": "\u624b\u52a8\u7f16\u8f91\u7c98\u8d34\u7684\u4ee3\u7801\u662f\u5f00\u53d1\u8005\u7684\u75db\u70b9\uff0cGoogle\u5185\u90e8\u6570\u636e\u663e\u793a\u4ee3\u7801\u7c98\u8d34\u9891\u7387\u662f\u624b\u52a8\u8f93\u5165\u76844\u500d\uff0c\u4e14\u7ecf\u5e38\u9700\u8981\u540e\u7eed\u7f16\u8f91\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u5f00\u53d1\u548c\u89c4\u6a21\u5316\uff0c\u5c06Smart Paste\u96c6\u6210\u5230IDE\u4e2d\uff0c\u63d0\u4f9b\u7c98\u8d34\u540e\u7684\u7f16\u8f91\u5efa\u8bae\u3002", "result": "Smart Paste\u7684\u63a5\u53d7\u7387\u4e3a45%\uff0c\u5360Google\u516c\u53f8\u8303\u56f4\u5185\u6240\u6709\u7f16\u5199\u4ee3\u7801\u76841%\u4ee5\u4e0a\u3002", "conclusion": "Smart Paste\u5728Google\u7684\u5f00\u53d1\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u5e76\u83b7\u5f97\u4e86\u79ef\u6781\u7684\u53cd\u9988\uff0c45%\u7684\u63a5\u53d7\u7387\u8868\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.04516", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04516", "abs": "https://arxiv.org/abs/2510.04516", "authors": ["Behrooz Farkiani", "Fan Liu", "Patrick Crowley"], "title": "Rethinking HTTP API Rate Limiting: A Client-Side Approach", "comment": null, "summary": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faATB\u548cAATB\u4e24\u79cd\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11HTTP 429\u9519\u8bef\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6570\u9000\u907f\u7b56\u7565\u3002", "motivation": "\u5728\u5171\u4eab\u914d\u989d\u4e0b\uff0c\u670d\u52a1\u5668\u7aef\u63a7\u5236\u6548\u7387\u4f4e\u4e0b\uff0c\u5ba2\u6237\u7aef\u7f3a\u4e4f\u5176\u4ed6\u8d1f\u8f7d\u7684\u53ef\u89c1\u6027\uff0c\u5bfc\u81f4\u91cd\u8bd5\u5931\u8d25\u3002\u73b0\u6709\u7b80\u5355\u7b56\u7565\uff08\u5982\u6307\u6570\u9000\u907f\uff09\u5bfc\u81f4\u8fc7\u591a\u91cd\u8bd5\u548c\u6210\u672c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65e0\u9700\u4e2d\u592e\u63a7\u5236\u7684\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u7b97\u6cd5\uff1aATB\uff08\u79bb\u7ebf\u65b9\u6cd5\uff09\u548cAATB\uff08\u5229\u7528\u805a\u5408\u9065\u6d4b\u6570\u636e\u589e\u5f3a\u91cd\u8bd5\u884c\u4e3a\uff09\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u6a21\u62df\uff0c\u7b97\u6cd5\u5c06HTTP 429\u9519\u8bef\u51cf\u5c11\u9ad8\u8fbe97.3%\uff0c\u4f18\u4e8e\u6307\u6570\u9000\u907f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u673a\u5236\uff08ATB\u548cAATB\uff09\u663e\u8457\u51cf\u5c11\u4e86HTTP 429\u9519\u8bef\uff0c\u5c3d\u7ba1\u7565\u5fae\u589e\u52a0\u4e86\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u5176\u4f18\u52bf\u8fdc\u8d85\u8fc7\u8fd9\u4e00\u7f3a\u70b9\u3002"}}
{"id": "2510.03599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03599", "abs": "https://arxiv.org/abs/2510.03599", "authors": ["Shafeef Omar", "Majid Khadiv"], "title": "Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning", "comment": null, "summary": "We present a unified framework for multi-task locomotion and manipulation\npolicy learning grounded in a contact-explicit representation. Instead of\ndesigning different policies for different tasks, our approach unifies the\ndefinition of a task through a sequence of contact goals-desired contact\npositions, timings, and active end-effectors. This enables leveraging the\nshared structure across diverse contact-rich tasks, leading to a single policy\nthat can perform a wide range of tasks. In particular, we train a\ngoal-conditioned reinforcement learning (RL) policy to realise given contact\nplans. We validate our framework on multiple robotic embodiments and tasks: a\nquadruped performing multiple gaits, a humanoid performing multiple biped and\nquadrupedal gaits, and a humanoid executing different bimanual object\nmanipulation tasks. Each of these scenarios is controlled by a single policy\ntrained to execute different tasks grounded in contacts, demonstrating\nversatile and robust behaviours across morphologically distinct systems. Our\nresults show that explicit contact reasoning significantly improves\ngeneralisation to unseen scenarios, positioning contact-explicit policy\nlearning as a promising foundation for scalable loco-manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u663e\u5f0f\u8868\u793a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u6267\u884c\u591a\u79cd\u79fb\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528\u4e0d\u540c\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e4b\u95f4\u7684\u5171\u4eab\u7ed3\u6784\uff0c\u907f\u514d\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u540c\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u5355\u4e00\u7b56\u7565\u6267\u884c\u591a\u79cd\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u663e\u5f0f\u8868\u793a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u63a5\u89e6\u76ee\u6807\uff08\u671f\u671b\u7684\u63a5\u89e6\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u6d3b\u52a8\u672b\u7aef\u6548\u5e94\u5668\uff09\u6765\u5b9a\u4e49\u4efb\u52a1\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u76ee\u6807\u6761\u4ef6\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u5b9e\u73b0\u7ed9\u5b9a\u7684\u63a5\u89e6\u8ba1\u5212\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u5b9e\u4f53\u548c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u56db\u8db3\u52a8\u7269\u7684\u591a\u79cd\u6b65\u6001\u3001\u4eba\u5f62\u673a\u5668\u4eba\u7684\u591a\u79cd\u53cc\u8db3\u548c\u56db\u8db3\u6b65\u6001\uff0c\u4ee5\u53ca\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u4e0d\u540c\u7684\u53cc\u624b\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u663e\u5f0f\u63a5\u89e6\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u57fa\u4e8e\u63a5\u89e6\u7684\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\u6210\u4e3a\u53ef\u6269\u5c55\u7684\u79fb\u52a8\u64cd\u4f5c\u7684\u6709\u524d\u666f\u57fa\u7840\u3002"}}
{"id": "2510.03680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding.", "AI": {"tldr": "Rainbow Padding \u901a\u8fc7\u66ff\u6362\u91cd\u590d <eos> \u4e3a\u4e0d\u540c\u586b\u5145\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684 <eos> \u6ea2\u51fa\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6307\u4ee4\u8c03\u4f18\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728 <eos> \u6ea2\u51fa\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u54cd\u5e94\u957f\u5ea6\u968f\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u7f29\u77ed\uff0c\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u5f15\u5165 Rainbow Padding\uff0c\u7528\u4e0d\u540c\u586b\u5145\u6807\u8bb0\u7684\u5faa\u73af\u66ff\u6362\u91cd\u590d\u7684 <eos> \u5360\u4f4d\u7b26\uff0c\u5e76\u901a\u8fc7 LoRA \u5fae\u8c03\u5728\u73b0\u6709\u6a21\u578b\u4e0a\u9ad8\u6548\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRainbow Padding \u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u4ec5\u9700\u4e03\u4e2a\u586b\u5145\u6807\u8bb0\u5373\u53ef\u9632\u6b62\u63d0\u524d\u7ec8\u6b62\u3002", "conclusion": "Rainbow Padding \u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u66ff\u6362\u91cd\u590d\u7684 <eos> \u5360\u4f4d\u7b26\u4e3a\u4e0d\u540c\u586b\u5145\u6807\u8bb0\u7684\u5faa\u73af\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2510.05081", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05081", "abs": "https://arxiv.org/abs/2510.05081", "authors": ["Ronen Kamenetsky", "Sara Dorfman", "Daniel Garibi", "Roni Paiss", "Or Patashnik", "Daniel Cohen-Or"], "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder", "comment": "Project page at: https://ronen94.github.io/SAEdit/", "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u6807\u8bb0\u7ea7\u64cd\u4f5c\u5b9e\u73b0\u89e3\u8026\u548c\u8fde\u7eed\u63a7\u5236\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u8bed\u4e49\u9694\u79bb\u65b9\u5411\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u7f16\u8f91\u63a7\u5236\uff0c\u5c24\u5176\u662f\u89e3\u8026\u548c\u8fde\u7eed\u63a7\u5236\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u7a00\u758f\u6f5c\u5728\u7a7a\u95f4\u6765\u8bc6\u522b\u8bed\u4e49\u9694\u79bb\u7684\u7ef4\u5ea6\uff0c\u901a\u8fc7\u6cbf\u7279\u5b9a\u65b9\u5411\u8c03\u6574\u6587\u672c\u5d4c\u5165\u5f3a\u5ea6\u6765\u63a7\u5236\u7f16\u8f91\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u6837\u5c5e\u6027\u548c\u9886\u57df\u4e2d\u5b9e\u73b0\u76f4\u89c2\u9ad8\u6548\u7684\u8fde\u7eed\u63a7\u5236\u7f16\u8f91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u7684\u6807\u8bb0\u7ea7\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u7f16\u8f91\u7684\u89e3\u8026\u548c\u8fde\u7eed\u63a7\u5236\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6269\u6563\u8fc7\u7a0b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30LLM\u4ee3\u7801\u751f\u6210\u7684\u7406\u8bba\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u53ef\u6bd4\u6027\u548c\u53ef\u91cd\u590d\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u7684\u5b9e\u8bc1\u8bc4\u4f30\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u7814\u7a76\u5728\u76ee\u6807\u3001\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u9650\u5236\u4e86\u53ef\u6bd4\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u6846\u67b6\u57fa\u4e8e\u4f5c\u8005\u5148\u524d\u8fdb\u884c\u6b64\u7c7b\u5b9e\u9a8c\u7684\u7ecf\u9a8c\u4ee5\u53ca\u5bf9\u8fd1\u671f\u7814\u7a76\u5173\u952e\u5f02\u540c\u70b9\u7684\u6bd4\u8f83\u5206\u6790\u3002\u5b83\u56f4\u7ed5\u95ee\u9898\u6765\u6e90\u3001\u8d28\u91cf\u5c5e\u6027\u548c\u6307\u6807\u7b49\u6838\u5fc3\u7ec4\u4ef6\u7ec4\u7ec7\u8bc4\u4f30\uff0c\u652f\u6301\u7ed3\u6784\u5316\u548c\u7cfb\u7edf\u5316\u7684\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u6848\u4f8b\u6620\u5c04\u5c55\u793a\u4e86\u6846\u67b6\u7684\u9002\u7528\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u6539\u8fdb\u7684\u673a\u4f1a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u62a5\u544a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7801\u751f\u6210\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u8ba1\u5212\u5c06\u8be5\u6846\u67b6\u53d1\u5c55\u4e3a\u66f4\u6210\u719f\u7684\u5de5\u5177\uff0c\u4ee5\u6807\u51c6\u5316LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u3002"}}
{"id": "2510.04620", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04620", "abs": "https://arxiv.org/abs/2510.04620", "authors": ["Siu Kei Chung", "Francisco Carpio", "Andrei Navoichyk", "Siarhei Valasovich", "Jordan Moore", "Slobodan Sudaric-Hefner", "Daniel Baker", "Thomas Demoor", "Maurizio Binello", "Christian Kaul", "Kai Wawrzinek"], "title": "Impossible Cloud Network: A Decentralized Internet Infrastructure Layer", "comment": null, "summary": "The internet faces a sovereignty crisis due to power concentration and data\ngrowth among a few hyperscalers, leading to centralization and loss of user\ncontrol. This consolidation risks censorship and creates single points of\nfailure. While Web3 offers decentralized solutions, they often sacrifice either\nscalability, decentralization, or security, which are key elements in the\nblockchain trilemma. These solutions also struggle with limited access to\nenterprise-grade hardware and frequently rely on centralized infrastructure.\nThe Impossible Cloud Network (ICN) addresses these issues by creating a\nmulti-tiered, decentralized infrastructure layer. ICN offers a composable\nservice layer, an enterprise-grade hardware resource layer, and a transparent,\npermissionless HyperNode network for performance enforcement. By strategically\ndecoupling and decentralizing each layer, ICN aims to provide an open,\nextensively scalable infrastructure that ensures digital sovereignty,\neliminates single points of trust, enables service programmability, and offers\na decoupled architecture for limitless possibilities in the future internet.", "AI": {"tldr": "ICN\u901a\u8fc7\u591a\u5c42\u6b21\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\u89e3\u51b3\u4e92\u8054\u7f51\u4e3b\u6743\u5371\u673a\uff0c\u63d0\u4f9b\u5f00\u653e\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6848\u3002", "motivation": "\u4e92\u8054\u7f51\u9762\u4e34\u7684\u4e3b\u6743\u5371\u673a\uff0c\u96c6\u4e2d\u5316\u548c\u7528\u6237\u63a7\u5236\u6743\u7684\u4e27\u5931\uff0c\u4ee5\u53caWeb3\u89e3\u51b3\u65b9\u6848\u5728\u533a\u5757\u94fe\u4e09\u96be\u6743\u8861\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "ICN\u901a\u8fc7\u6784\u5efa\u53ef\u7ec4\u5408\u7684\u670d\u52a1\u5c42\u3001\u4f01\u4e1a\u7ea7\u786c\u4ef6\u8d44\u6e90\u5c42\u548c\u900f\u660e\u7684HyperNode\u7f51\u7edc\uff0c\u5b9e\u73b0\u6027\u80fd\u6267\u884c\u3002", "result": "ICN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u6d88\u9664\u4e86\u5355\u4e00\u4fe1\u4efb\u70b9\uff0c\u5b9e\u73b0\u4e86\u670d\u52a1\u53ef\u7f16\u7a0b\u6027\u3002", "conclusion": "ICN\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u7684\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u4e92\u8054\u7f51\u7684\u4e3b\u6743\u5371\u673a\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u53bb\u4e2d\u5fc3\u5316\u5404\u5c42\uff0c\u63d0\u4f9b\u5f00\u653e\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u786e\u4fdd\u6570\u5b57\u4e3b\u6743\u3002"}}
{"id": "2510.03640", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03640", "abs": "https://arxiv.org/abs/2510.03640", "authors": ["Mostafa Emam", "Matthias Gerdts"], "title": "Safety-Oriented Dynamic Path Planning for Automated Vehicles", "comment": "Published in 2025 IEEE 101st Vehicular Technology Conference\n  (VTC2025-Spring), Oslo, Norway, June 17-20, 2025. Received Best Conference\n  Paper Award", "summary": "Ensuring safety in autonomous vehicles necessitates advanced path planning\nand obstacle avoidance capabilities, particularly in dynamic environments. This\npaper introduces a bi-level control framework that efficiently augments road\nboundaries by incorporating time-dependent grid projections of obstacle\nmovements, thus enabling precise and adaptive path planning. The main control\nloop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path\noptimization, wherein homotopy-based constraint relaxation is employed to\nimprove the solvability of the optimal control problem (OCP). Furthermore, an\nindependent backup loop runs concurrently to provide safe fallback trajectories\nwhen an optimal trajectory cannot be computed by the main loop within a\ncritical time frame, thus enhancing safety and real-time performance. Our\nevaluation showcases the benefits of the proposed methods in various driving\nscenarios, highlighting the real-time applicability and robustness of our\napproach. Overall, the framework represents a significant step towards safer\nand more reliable autonomous driving in complex and dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408NMPC\u548c\u5907\u4efd\u73af\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u907f\u969c\u80fd\u529b\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u4e3b\u63a7\u5236\u73af\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8fdb\u884c\u5b9e\u65f6\u8def\u5f84\u4f18\u5316\uff0c\u8f85\u4ee5\u72ec\u7acb\u7684\u5907\u4efd\u73af\u63d0\u4f9b\u5b89\u5168\u56de\u9000\u8f68\u8ff9\u3002", "result": "\u8bc4\u4f30\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u4e3a\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u76ee\u6807\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165GSR\u548cRCOF\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08LLMs\u5b9e\u73b0\u53ef\u89e3\u91ca\u8bc4\u4f30\uff0c\u4f01\u4e1a\u5e94\u7528\u663e\u793aGSR\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u8bc4\u4f30\u591a\u8f6e\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u8d28\u91cf\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5728\u8f6e\u6b21\u5c42\u9762\u8bc4\u4f30\uff0c\u800c\u5ffd\u7565\u4e86\u7528\u6237\u6574\u4f53\u76ee\u6807\u662f\u5426\u8fbe\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7ed3\u5408\u6559\u5e08LLMs\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u76ee\u6807\u5e76\u8bbe\u5b9a\u8d28\u91cf\u6807\u51c6\uff0cLLMs\u4f7f\u7528\u201c\u601d\u8003\u6807\u8bb0\u201d\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "result": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u8be5\u6846\u67b6\u8bc4\u4f30AIDA\uff0c\u89c2\u5bdf\u5230GSR\u4ece63%\u63d0\u5347\u81f379%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u591a\u667a\u80fd\u4f53\u804a\u5929\u673a\u5668\u4eba\u6545\u969c\u70b9\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u8bca\u65ad\u6574\u4f53\u6210\u529f\u60c5\u51b5\uff0c\u8bc6\u522b\u5173\u952e\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u4e3a\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.05097", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05097", "abs": "https://arxiv.org/abs/2510.05097", "authors": ["Robin Courant", "Xi Wang", "David Loiseaux", "Marc Christie", "Vicky Kalogeiton"], "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation", "comment": "Project page:\n  https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/", "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u751f\u6210\u4eba\u7c7b\u52a8\u4f5c\u548c\u76f8\u673a\u8f68\u8ff9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c4f\u5e55\u6846\u67b6\u4f5c\u4e3a\u6865\u6881\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e00\u81f4\u6027\u548c\u6587\u672c\u5bf9\u9f50\u6548\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4eba\u7c7b\u52a8\u4f5c\u548c\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u5206\u5f00\u5904\u7406\uff0c\u5ffd\u89c6\u4e86\u7535\u5f71\u6444\u5f71\u4e2d\u6f14\u5458\u8868\u6f14\u548c\u76f8\u673a\u5de5\u4f5c\u7684\u7d27\u5bc6\u4e92\u52a8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u7684\u8054\u5408\u751f\u6210\uff0c\u4fdd\u6301\u4e00\u81f4\u7684\u5c4f\u5e55\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8054\u5408\u81ea\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u53d8\u6362\u5c06\u4eba\u7c7b\u548c\u76f8\u673a\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u5230\u6846\u67b6\u6f5c\u5728\u7a7a\u95f4\u3002\u5f15\u5165\u8f85\u52a9\u91c7\u6837\u65b9\u6cd5\uff0c\u5229\u7528\u7ebf\u6027\u53d8\u6362\u5f15\u5bfc\u751f\u6210\u671d\u5411\u4e00\u81f4\u7684\u6846\u67b6\u6a21\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5e27\u5185\u4e00\u81f4\u7684\u4eba\u7c7b-\u76f8\u673a\u8fd0\u52a8\u65b9\u9762\u5177\u6709\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u540c\u65f6\u5728\u4e24\u79cd\u6a21\u6001\u7684\u6587\u672c\u5bf9\u9f50\u4e0a\u53d6\u5f97\u4e86\u63d0\u5347\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5c4f\u5e55\u6846\u67b6\u4f5c\u4e3a\u8f85\u52a9\u6a21\u6001\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u7c7b\u52a8\u4f5c\u548c\u76f8\u673a\u8f68\u8ff9\u7684\u8054\u5408\u751f\u6210\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e00\u81f4\u6027\u548c\u6587\u672c\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches.", "AI": {"tldr": "ACToR\u662f\u4e00\u79cd\u5bf9\u6297\u6027C\u5230Rust\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u6210\u529f\u7ffb\u8bd1\u4e2d\u7b49\u89c4\u6a21C\u4ee3\u7801\uff0c\u6d4b\u8bd5\u901a\u8fc7\u7387\u9ad8\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u73b0\u6709C\u5230Rust\u7684\u7ffb\u8bd1\u65b9\u6cd5\uff08\u5305\u62ecLLM\u8f85\u52a9\uff09\u65e0\u6cd5\u6cdb\u5316\u5230\u8f83\u5927\uff08>500\u884c\u4ee3\u7801\uff09\u7684C\u4ee3\u7801\u5e93\uff0c\u56e0\u5176\u4f9d\u8d56\u590d\u6742\u4e14\u6613\u5931\u6548\u7684\u7a0b\u5e8f\u5206\u6790\u3002", "method": "ACToR\u91c7\u7528\u5bf9\u6297\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u4ee3\u7406\u7684\u534f\u4f5c\u8fed\u4ee3\u751f\u6210Rust\u7ffb\u8bd1\uff1a\u751f\u6210\u5668\u5408\u6210\u5e76\u4f18\u5316\u7ffb\u8bd1\u4ee5\u901a\u8fc7\u6d4b\u8bd5\uff0c\u5224\u522b\u5668\u5219\u53d1\u73b0\u65b0\u7684\u5931\u8d25\u6d4b\u8bd5\u3002", "result": "ACToR\u6210\u529f\u7ffb\u8bd1\u4e8663\u4e2a\u5e73\u5747485\u884c\u4ee3\u7801\u7684\u771f\u5b9e\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u6d4b\u8bd5\u901a\u8fc7\u7387\u8d85\u8fc790%\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "ACToR \u662f\u9996\u4e2a\u80fd\u591f\u53ef\u9760\u5730\u5c06\u4e2d\u7b49\u89c4\u6a21\uff08\u5e73\u5747485\u884c\u4ee3\u7801\uff09\u7684C\u7a0b\u5e8f\u7ffb\u8bd1\u4e3aRust\u7684\u7cfb\u7edf\uff0c\u4e14\u5728\u96f6\u4eba\u5de5\u5e72\u9884\u4e0b\u8fbe\u523090%\u4ee5\u4e0a\u7684\u6d4b\u8bd5\u901a\u8fc7\u7387\uff0c\u76f8\u6bd4\u975e\u5bf9\u6297\u6027\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8618.9%\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2510.04651", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04651", "abs": "https://arxiv.org/abs/2510.04651", "authors": ["Adnan Aijaz", "Peizheng Li", "Sajida Gufran"], "title": "Satellite Direct-to-Device from Low Earth Orbit: Techno-Economic Analysis of a Global Non-Terrestrial Network", "comment": "8 pages, 13 figures. This paper has been accepted for presentation at\n  the IEEE/IFIP Wireless and Mobile Networking Conference (WMNC) 2025", "summary": "Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D)\ntechnology are at the heart of the next-generation global connectivity which\npromises direct access to space-based broadband services for unmodified\n3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to\nevaluate the feasibility, cost-effectiveness, and profitability of these\nservices. By assessing the technological aspects as well as economic\nimplications, stakeholders can make informed decisions about investment,\ndevelopment, and deployment strategies. This paper presents a comprehensive\ntechno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D\nsystems. The framework integrates a global satellite constellation model, radio\npropagation aspects including atmospheric and rainfall attenuation models\ncompliant with ITU-R recommendations, 3GPP-compliant capacity calculations,\nrealistic global population data, and an all-encompassing cost model accounting\nfor both capital and operational expenses associated with space and ground\nsegments. Further, the framework evaluates three different architectural\noptions for realizing a global non-terrestrial network (NTN) for satellite D2D\nservices. With an emphasis on reproducibility, the framework has been\nimplemented through significant enhancements to an open-source tool. The\neconomic assessment reveals that global satellite D2D services can be provided\nat a monthly cost per subscriber which is comparable to terrestrial services\nwhile achieving a positive return on investment (ROI). Moreover, the results\nshow the potential of Open RAN technology for realizing cost-effective\nsatellite D2D services.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6280\u672f\u7ecf\u6d4e\u5206\u6790\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86LEO\u536b\u661fD2D\u670d\u52a1\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6210\u672c\u4e0e\u5730\u9762\u670d\u52a1\u76f8\u5f53\u4e14\u80fd\u5b9e\u73b0\u6b63ROI\uff0cOpen RAN\u6280\u672f\u4e5f\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u548c\u536b\u661f\u76f4\u63a5\u5230\u8bbe\u5907\uff08D2D\uff09\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u76c8\u5229\u80fd\u529b\u5bf9\u5229\u76ca\u76f8\u5173\u8005\u5728\u6295\u8d44\u3001\u5f00\u53d1\u548c\u90e8\u7f72\u7b56\u7565\u4e0a\u505a\u51fa\u660e\u667a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6280\u672f\u7ecf\u6d4e\u5206\u6790\uff08TEA\uff09\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5168\u7403\u536b\u661f\u661f\u5ea7\u6a21\u578b\u3001\u7b26\u5408ITU-R\u5efa\u8bae\u7684\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u578b\u30013GPP\u517c\u5bb9\u7684\u5bb9\u91cf\u8ba1\u7b97\u3001\u5168\u7403\u4eba\u53e3\u6570\u636e\u4ee5\u53ca\u6db5\u76d6\u7a7a\u95f4\u548c\u5730\u9762\u6bb5\u8d44\u672c\u548c\u8fd0\u8425\u652f\u51fa\u7684\u6210\u672c\u6a21\u578b\u3002\u6846\u67b6\u8fd8\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5168\u7403\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u67b6\u6784\u9009\u9879\u3002", "result": "\u7ecf\u6d4e\u8bc4\u4f30\u663e\u793a\uff0c\u5168\u7403\u536b\u661fD2D\u670d\u52a1\u7684\u6bcf\u6708\u7528\u6237\u6210\u672c\u4e0e\u5730\u9762\u670d\u52a1\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u5b9e\u73b0\u6b63\u7684\u6295\u8d44\u56de\u62a5\u7387\uff08ROI\uff09\u3002\u6b64\u5916\uff0cOpen RAN\u6280\u672f\u5728\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u536b\u661fD2D\u670d\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u5168\u7403\u536b\u661f\u76f4\u63a5\u5230\u8bbe\u5907\uff08D2D\uff09\u670d\u52a1\u5728\u6210\u672c\u4e0a\u53ef\u4e0e\u5730\u9762\u670d\u52a1\u5ab2\u7f8e\uff0c\u4e14\u80fd\u5b9e\u73b0\u6b63\u7684\u6295\u8d44\u56de\u62a5\u7387\uff08ROI\uff09\uff0c\u540c\u65f6Open RAN\u6280\u672f\u5728\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u536b\u661fD2D\u670d\u52a1\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2510.03644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03644", "abs": "https://arxiv.org/abs/2510.03644", "authors": ["Mohammadjavad Javadi", "Robin Chhabra"], "title": "Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing", "comment": null, "summary": "Cosserat rod theory is the popular approach to modeling ferromagnetic soft\nrobots as 1-Dimensional (1D) slender structures in most applications, such as\nbiomedical. However, recent soft robots designed for locomotion and\nmanipulation often exhibit a large width-to-length ratio that categorizes them\nas 2D shells. For analysis and shape-morphing control purposes, we develop an\nefficient coordinate-free static model of hard-magnetic shells found in soft\nmagnetic grippers and walking soft robots. The approach is based on a novel\nformulation of Cosserat shell theory on the Special Euclidean group\n($\\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points\nwith six degrees of freedom (position & rotation) suitable for capturing the\nbehavior of a uniformly distributed array of spheroidal hard magnetic particles\nembedded in the rheological elastomer. The shell's configuration manifold is\nthe space of all smooth embeddings $\\mathbb{R}^2\\rightarrow\\mathbf{SE}(3)$.\nAccording to a novel definition of local deformation gradient based on the Lie\ngroup structure of $\\mathbf{SE}(3)$, we derive the strong and weak forms of\nequilibrium equations, following the principle of virtual work. We extract the\nlinearized version of the weak form for numerical implementations. The\nresulting finite element approach can avoid well-known challenges such as\nsingularity and locking phenomenon in modeling shell structures. The proposed\nmodel is analytically and experimentally validated through a series of test\ncases that demonstrate its superior efficacy, particularly when the shell\nundergoes severe rotations and displacements.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u58f3\u7406\u8bba\u7684\u786c\u78c1\u58f3\u9759\u6001\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5bbd\u957f\u6bd4\u5927\u7684\u8f6f\u673a\u5668\u4eba\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e25\u91cd\u53d8\u5f62\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684Cosserat\u6746\u7406\u8bba\u9002\u7528\u4e8e1D\u7ec6\u957f\u7ed3\u6784\uff0c\u4f46\u8fd1\u671f\u8bbe\u8ba1\u7684\u8f6f\u673a\u5668\u4eba\uff08\u5982\u6293\u53d6\u5668\u548c\u884c\u8d70\u673a\u5668\u4eba\uff09\u5177\u6709\u8f83\u5927\u7684\u5bbd\u957f\u6bd4\uff0c\u5c5e\u4e8e2D\u58f3\u7ed3\u6784\uff0c\u9700\u8981\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7279\u6b8a\u6b27\u51e0\u91cc\u5f97\u7fa4\uff08SE(3)\uff09\u7684Cosserat\u58f3\u7406\u8bba\u65b0\u516c\u5f0f\uff0c\u63a8\u5bfc\u4e86\u5e73\u8861\u65b9\u7a0b\u7684\u5f3a\u5f31\u5f62\u5f0f\uff0c\u5e76\u63d0\u53d6\u7ebf\u6027\u5316\u7248\u672c\u7528\u4e8e\u6570\u503c\u5b9e\u73b0\u3002", "result": "\u63d0\u51fa\u7684\u6709\u9650\u5143\u65b9\u6cd5\u907f\u514d\u4e86\u58f3\u7ed3\u6784\u5efa\u6a21\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff08\u5982\u5947\u5f02\u6027\u548c\u9501\u5b9a\u73b0\u8c61\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u58f3\u7406\u8bba\u7684\u9ad8\u6548\u9759\u6001\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u786c\u78c1\u58f3\u7684\u5206\u6790\u548c\u5f62\u72b6\u53d8\u5f62\u63a7\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e25\u91cd\u65cb\u8f6c\u548c\u4f4d\u79fb\u60c5\u51b5\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.03700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed.", "AI": {"tldr": "H-DDx\u662f\u4e00\u79cd\u5c42\u6b21\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u8bc6\u522b\u5e7f\u6cdb\u4e34\u5e8a\u80cc\u666f\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLMs\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6241\u5e73\u6307\u6807\uff08\u5982Top-k\u51c6\u786e\u7387\uff09\uff0c\u65e0\u6cd5\u533a\u5206\u4e34\u5e8a\u76f8\u5173\u7684\u8fd1\u8bef\u548c\u8bca\u65ad\u6027\u8fdc\u8bef\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165H-DDx\uff0c\u4e00\u79cd\u5229\u7528\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u7ba1\u9053\u5c06\u81ea\u7531\u6587\u672c\u8bca\u65ad\u6620\u5c04\u5230ICD-10\u4ee3\u7801\u7684\u5c42\u6b21\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u572822\u4e2a\u9886\u5148\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f20\u7edf\u6241\u5e73\u6307\u6807\u4f4e\u4f30\u4e86\u6027\u80fd\uff0c\u800cH-DDx\u7a81\u51fa\u4e86\u9886\u57df\u4e13\u7528\u5f00\u6e90\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5e76\u589e\u5f3a\u4e86\u9519\u8bef\u6a21\u5f0f\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "H-DDx\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u8bc4\u4f30\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4e86\u4e34\u5e8a\u76f8\u5173\u6027\uff0c\u63ed\u793a\u4e86LLMs\u5728\u8bc6\u522b\u5e7f\u6cdb\u4e34\u5e8a\u80cc\u666f\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5373\u4f7f\u672a\u7cbe\u786e\u8bca\u65ad\u3002"}}
{"id": "2411.18625", "categories": ["cs.CV", "cs.AI", "cs.GR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2411.18625", "abs": "https://arxiv.org/abs/2411.18625", "authors": ["Brian Chao", "Hung-Yu Tseng", "Lorenzo Porzi", "Chen Gao", "Tuotuo Li", "Qinbo Li", "Ayush Saraf", "Jia-Bin Huang", "Johannes Kopf", "Gordon Wetzstein", "Changil Kim"], "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling", "comment": "Will be presented at CVPR 2025. Project website:\n  https://textured-gaussians.github.io/", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D\nreconstruction and rendering technique due to its high-quality results and fast\ntraining and rendering time. However, pixels covered by the same Gaussian are\nalways shaded in the same color up to a Gaussian falloff scaling factor.\nFurthermore, the finest geometric detail any individual Gaussian can represent\nis a simple ellipsoid. These properties of 3DGS greatly limit the expressivity\nof individual Gaussian primitives. To address these issues, we draw inspiration\nfrom texture and alpha mapping in traditional graphics and integrate it with\n3DGS. Specifically, we propose a new generalized Gaussian appearance\nrepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texture\nmaps to model spatially varying color and opacity across the extent of each\nGaussian. As such, each Gaussian can represent a richer set of texture patterns\nand geometric structures, instead of just a single color and ellipsoid as in\nnaive Gaussian Splatting. Surprisingly, we found that the expressivity of\nGaussians can be greatly improved by using alpha-only texture maps, and further\naugmenting Gaussians with RGB texture maps achieves the highest expressivity.\nWe validate our method on a wide variety of standard benchmark datasets and our\nown custom captures at both the object and scene levels. We demonstrate image\nquality improvements over existing methods while using a similar or lower\nnumber of Gaussians.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u7eb9\u7406\u548c\u900f\u660e\u5ea6\u6620\u5c04\u6280\u672f\uff0c3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u5355\u4e2a\u9ad8\u65af\u57fa\u5143\u8868\u8fbe\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u7ed3\u6784\u8868\u793a\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u56e0\u5176\u9ad8\u8d28\u91cf\u7ed3\u679c\u548c\u5feb\u901f\u8bad\u7ec3\u6e32\u67d3\u65f6\u95f4\u6210\u4e3a\u5148\u8fdb\u76843D\u91cd\u5efa\u548c\u6e32\u67d3\u6280\u672f\uff0c\u4f46\u5176\u5355\u4e2a\u9ad8\u65af\u57fa\u5143\u7684\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u4ec5\u80fd\u8868\u793a\u5355\u4e00\u989c\u8272\u548c\u7b80\u5355\u692d\u7403\u4f53\uff0c\u9650\u5236\u4e86\u7eb9\u7406\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e7f\u4e49\u9ad8\u65af\u5916\u89c2\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\u6dfb\u52a0alpha\uff08A\uff09\u3001RGB\u6216RGBA\u7eb9\u7406\u6620\u5c04\uff0c\u4ee5\u6a21\u62df\u6bcf\u4e2a\u9ad8\u65af\u8303\u56f4\u5185\u7a7a\u95f4\u53d8\u5316\u7684\u989c\u8272\u548c\u900f\u660e\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u5355\u4e2a\u9ad8\u65af\u80fd\u591f\u8868\u793a\u66f4\u590d\u6742\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u4e00\u989c\u8272\u548c\u7b80\u5355\u692d\u7403\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528alpha\u7eb9\u7406\u6620\u5c04\u5373\u53ef\u663e\u8457\u63d0\u5347\u9ad8\u65af\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u800c\u7ed3\u5408RGB\u7eb9\u7406\u6620\u5c04\u5219\u8fbe\u5230\u6700\u9ad8\u8868\u8fbe\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u6355\u6349\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u4f7f\u7528\u76f8\u4f3c\u6216\u66f4\u5c11\u7684\u9ad8\u65af\u6570\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7eb9\u7406\u548c\u900f\u660e\u5ea6\u6620\u5c04\u6280\u672f\u96c6\u6210\u52303D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\uff0c\u63d0\u51fa\u7684\u5e7f\u4e49\u9ad8\u65af\u5916\u89c2\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u5355\u4e2a\u9ad8\u65af\u57fa\u5143\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u7684\u7eb9\u7406\u6a21\u5f0f\u548c\u51e0\u4f55\u7ed3\u6784\u8868\u793a\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u65af\u6570\u91cf\u7684\u6548\u7387\u3002"}}
{"id": "2510.03890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03890", "abs": "https://arxiv.org/abs/2510.03890", "authors": ["Jose Garcia-Alonso", "Enrique Moguel", "Jaime Alvarado-Valiente", "Javier Romero-Alvarez", "\u00c1lvaro M. Aparicio-Morales", "Juan M. Murillo", "Francisco Javier Cavero", "Adri\u00e1n Romero-Flores", "Alfonso E. Marquez-Chamorro", "Jos\u00e9 Antonio Parejo", "Antonio Ruiz-Cort\u00e9s", "Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Rethinking Services in the Quantum Age: The SOQ Paradigm", "comment": "39 pages, 5 figures, 6 tables", "summary": "Quantum computing is rapidly progressing from theoretical promise to\npractical implementation, offering significant computational advantages for\ntasks in optimization, simulation, cryptography, and machine learning. However,\nits integration into real-world software systems remains constrained by\nhardware fragility, platform heterogeneity, and the absence of robust software\nengineering practices. This paper introduces Service-Oriented Quantum (SOQ), a\nnovel paradigm that reimagines quantum software systems through the lens of\nclassical service-oriented computing. Unlike prior approaches such as Quantum\nService-Oriented Computing (QSOC), which treat quantum capabilities as\nauxiliary components within classical systems, SOQ positions quantum services\nas autonomous, composable, and interoperable entities. We define the\nfoundational principles of SOQ, propose a layered technology stack to support\nits realization, and identify the key research and engineering challenges that\nmust be addressed, including interoperability, hybridity, pricing models,\nservice abstractions, and workforce development. This approach is of vital\nimportance for the advancement of quantum technology because it enables the\nscalable, modular, and interoperable integration of quantum computing into\nreal-world software systems independently and without relying on a dedicated\nclassical environment to manage quantum processing.", "AI": {"tldr": "SOQ\u662f\u4e00\u79cd\u65b0\u7684\u670d\u52a1\u5bfc\u5411\u91cf\u5b50\u8ba1\u7b97\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u3001\u53ef\u7ec4\u5408\u7684\u91cf\u5b50\u670d\u52a1\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5728\u4f18\u5316\u3001\u6a21\u62df\u3001\u5bc6\u7801\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u53d7\u5230\u786c\u4ef6\u8106\u5f31\u6027\u3001\u5e73\u53f0\u5f02\u6784\u6027\u548c\u7f3a\u4e4f\u7a33\u5065\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u7684\u9650\u5236\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86SOQ\u7684\u57fa\u672c\u539f\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u6280\u672f\u6808\u6765\u652f\u6301\u5176\u5b9e\u73b0\uff0c\u5e76\u8bc6\u522b\u4e86\u5305\u62ec\u4e92\u64cd\u4f5c\u6027\u3001\u6df7\u5408\u6027\u3001\u5b9a\u4ef7\u6a21\u578b\u3001\u670d\u52a1\u62bd\u8c61\u548c\u4eba\u624d\u57f9\u517b\u5728\u5185\u7684\u5173\u952e\u7814\u7a76\u4e0e\u5de5\u7a0b\u6311\u6218\u3002", "result": "SOQ\u8303\u5f0f\u4e3a\u91cf\u5b50\u6280\u672f\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u5b9e\u73b0\u91cf\u5b50\u8ba1\u7b97\u7684\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u548c\u53ef\u4e92\u64cd\u4f5c\u96c6\u6210\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e13\u7528\u7684\u7ecf\u5178\u73af\u5883\u6765\u7ba1\u7406\u91cf\u5b50\u5904\u7406\u3002", "conclusion": "SOQ\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u670d\u52a1\u5bfc\u5411\u91cf\u5b50\u8ba1\u7b97\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u91cf\u5b50\u670d\u52a1\u5b9a\u4f4d\u4e3a\u81ea\u4e3b\u3001\u53ef\u7ec4\u5408\u4e14\u53ef\u4e92\u64cd\u4f5c\u7684\u5b9e\u4f53\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u73b0\u5b9e\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.04731", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04731", "abs": "https://arxiv.org/abs/2510.04731", "authors": ["Douglas Dziedzorm Agbeve", "Andrey Belogaev", "Chris Blondia", "Jeroen Famaey"], "title": "Evaluating UORA-Based Polling Mechanism for Latency-Sensitive Uplink Traffic in Wi-Fi Networks", "comment": null, "summary": "IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple\nAccess (OFDMA), which enables simultaneous transmissions through centralized\nresource allocation. However, effective uplink scheduling requires the Access\nPoint (AP) to identify which stations (STAs) have data to transmit. This\ntypically necessitates polling for buffer status reports, a process that\nbecomes increasingly inefficient and unscalable with growing device density. In\nthis paper, we study how the Uplink OFDMA-based Random Access (UORA) feature\nimproves the scalability and delay experienced by latency-sensitive data\nstreams. We show that UORA enables efficient uplink scheduling while\nopportunistically identifying buffered traffic from unscheduled STAs, striking\na balance between coordination and scalability. Performance evaluation of\ndifferent polling strategies is done by means of simulation in ns-3. The\nresults indicate that UORA-based polling outperforms alternative schemes in\ndensely deployed network environments with heterogeneous uplink traffic\npatterns. Furthermore, under highly sparse and sporadic traffic conditions,\nUORA-based polling yields over 40% delay reduction compared to Scheduled Access\n(SA) OFDMA.", "AI": {"tldr": "UORA-based polling\u901a\u8fc7\u9ad8\u6548\u8bc6\u522b\u672a\u8c03\u5ea6STA\u7684\u7f13\u51b2\u6d41\u91cf\uff0c\u53e5 \u0627\u0644\u0635\u062d\u0627\u0641\u0629\u4e0a\u884c\u5267\u672c\u7684\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u8868\u73b0\uff0c\u5c24\u5176\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u6d41\u91cf\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u8f6e\u8be2\u7b56\u7565\u3002", "motivation": "IEEE 802.11ax (Wi-Fi 6)\u5f15\u5165\u7684OFDMA\u867d\u7136\u652f\u6301\u96c6\u4e2d\u5f0f\u8d44\u6e90\u5206\u914d\uff0c\u4f46\u4e0a\u884c\u8c03\u5ea6\u9700\u8981AP\u8bc6\u522b\u54ea\u4e9bSTA\u6709\u6570\u636e\u4f20\u8f93\uff0c\u4f20\u7edf\u8f6e\u8be2\u65b9\u5f0f\u5728\u8bbe\u5907\u5bc6\u5ea6\u589e\u52a0\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u901a\u8fc7ns-3\u6a21\u62df\u8bc4\u4f30\u4e0d\u540c\u8f6e\u8be2\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "UORA-based polling\u5728\u5f02\u6784\u4e0a\u884c\u6d41\u91cf\u6a21\u5f0f\u7684\u5bc6\u96c6\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u7a00\u758f\u548c\u96f6\u661f\u6d41\u91cf\u6761\u4ef6\u4e0b\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "UORA-based polling\u5728\u5bc6\u96c6\u90e8\u7f72\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u9ad8\u5ea6\u7a00\u758f\u548c\u96f6\u661f\u6d41\u91cf\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4Scheduled Access (SA) OFDMA\uff0c\u5ef6\u8fdf\u51cf\u5c11\u4e8640%\u4ee5\u4e0a\u3002"}}
{"id": "2510.03660", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03660", "abs": "https://arxiv.org/abs/2510.03660", "authors": ["Mohammadjavad Javadi", "Charlie Wadds", "Robin Chhabra"], "title": "An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion", "comment": null, "summary": "Untethered soft robots are essential for advancing the real-world deployment\nof soft robotic systems in diverse and multitasking environments. Inspired by\nsoft-bodied inchworm, we present a fully untethered soft robot with a curved,\nflexible structure actuated by magnetic forces. The robot has a total mass of\n102.63 g and demonstrates multimodal locomotion, achieving a maximum walking\nspeed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight\nonboard control circuit enables wireless command transmission, while an\nintegrated camera provides environmental perception. Through structural\noptimization and system-level integration, the robot successfully performs\nwalking, steering, swimming, and payload transport without reliance on external\ninfrastructure. The robot's dynamic performance and locomotion capabilities are\nsystematically validated through experimental characterization.", "AI": {"tldr": "\u53d7\u5c3a\u8816\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u675f\u7f1a\u78c1\u9a71\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u5177\u5907\u591a\u6a21\u6001\u8fd0\u52a8\u548c\u65e0\u7ebf\u63a7\u5236\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u73af\u5883\u3002", "motivation": "\u53d7\u8f6f\u4f53\u5c3a\u8816\u542f\u53d1\uff0c\u5f00\u53d1\u5b8c\u5168\u65e0\u675f\u7f1a\u7684\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u4ee5\u63a8\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u591a\u6837\u5316\u3001\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7531\u78c1\u529b\u9a71\u52a8\u7684\u5f2f\u66f2\u67d4\u6027\u7ed3\u6784\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u914d\u5907\u7d27\u51d1\u8f7b\u91cf\u5316\u7684\u8f66\u8f7d\u63a7\u5236\u7535\u8def\u548c\u96c6\u6210\u6444\u50cf\u5934\uff0c\u5b9e\u73b0\u4e86\u65e0\u7ebf\u547d\u4ee4\u4f20\u8f93\u548c\u73af\u5883\u611f\u77e5\u3002", "result": "\u673a\u5668\u4eba\u603b\u8d28\u91cf\u4e3a102.63\u514b\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u8fd0\u52a8\u80fd\u529b\uff0c\u6700\u5927\u884c\u8d70\u901f\u5ea6\u4e3a3.74\u5398\u7c73/\u79d2\uff0c\u6e38\u6cf3\u901f\u5ea6\u4e3a0.82\u5398\u7c73/\u79d2\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u7cfb\u7edf\u7ea7\u96c6\u6210\uff0c\u8be5\u8f6f\u4f53\u673a\u5668\u4eba\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u7684\u884c\u8d70\u3001\u8f6c\u5411\u3001\u6e38\u6cf3\u548c\u8d1f\u8f7d\u8fd0\u8f93\uff0c\u5176\u52a8\u6001\u6027\u80fd\u548c\u8fd0\u52a8\u80fd\u529b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2510.03727", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u589e\u5f3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u586b\u8865\u4e86\u5176\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u53ef\u63a7\u76844D\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MFMs\uff09\u7f3a\u4e4f\u4f5c\u4e3a\u6709\u6548\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\uff0c\u5982\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u52a8\u6001\u6a21\u62df\u3001\u65f6\u7a7a\u4fe1\u606f\u7406\u89e3\u548c\u53ef\u63a7\u751f\u6210\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u9996\u5148\u901a\u8fc7\u5224\u522b\u4efb\u52a1\u589e\u5f3aMFMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u5316\u63a8\u7406\u6280\u80fd\uff08\u5982\u56e0\u679c\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u601d\u7ef4\u548c\u65f6\u7a7a\u63a8\u7406\uff09\u3002\u968f\u540e\u63a2\u7d22\u4e86MFMs\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u6001\u4e2d\u7684\u751f\u6210\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7ed3\u6784\u5316\u53ef\u63a7\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u573a\u666f\u56fe\u3001\u591a\u6a21\u6001\u6761\u4ef6\u5316\u548c\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MFMs\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u53ef\u63a7\u76844D\u751f\u6210\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u3001\u53ef\u7f16\u8f91\u548c\u53ef\u53d8\u5f62\u5bf9\u8c61\u7684\u65f6\u7a7a\u5408\u6210\u3002", "conclusion": "\u901a\u8fc7\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MFMs\uff09\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u672c\u7814\u7a76\u6210\u529f\u7f29\u5c0f\u4e86\u5176\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u56e0\u679c\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u601d\u7ef4\u548c\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u3002"}}
{"id": "2510.03894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03894", "abs": "https://arxiv.org/abs/2510.03894", "authors": ["Antonios Saravanos"], "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "comment": null, "summary": "The waterfall model, one of the earliest software development methodologies,\nhas played a foundational role in shaping contemporary software engineering\npractices. This paper provides a historical and critical overview of the model,\ntracing its conceptual origins in software engineering, its formalization by\nRoyce, and its evolution through decades of industry adoption and critique.\nAlthough often criticized for its rigidity, shortcomings, and high failure\nrates, the waterfall model persists in specific domains. Its principles\ncontinue to influence contemporary hybrid development frameworks that combine\ntraditional and agile methods. Drawing on a range of scholarly sources, this\nstudy synthesizes key developments in the perception and application of the\nwaterfall model. The analysis highlights how the model has shifted from a\nstandalone framework to a component within modern hybrid methodologies. By\nrevisiting its origins, assessing its present utility, and examining its role\nin contemporary development practices, this paper argues that the waterfall\nmodel remains relevant, not as a relic of the past but as part of context-aware\ndevelopment strategies. The paper contends that the model's enduring relevance\nlies in its adaptability. By recognizing both its limitations and its\nstrengths, and by understanding its integration within hybrid approaches,\npractitioners can make more informed decisions about methodology selection and\nprocess design in diverse development environments.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u5e76\u5206\u6790\u4e86\u7011\u5e03\u6a21\u578b\u7684\u5386\u53f2\u3001\u6279\u8bc4\u53ca\u73b0\u4ee3\u9002\u5e94\u6027\uff0c\u8ba4\u4e3a\u5176\u5728\u6df7\u5408\u5f00\u53d1\u6846\u67b6\u4e2d\u4ecd\u5177\u4ef7\u503c\u3002", "motivation": "\u63a2\u8ba8\u7011\u5e03\u6a21\u578b\u5728\u5f53\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6301\u7eed\u5f71\u54cd\u548c\u9002\u5e94\u6027\uff0c\u65e8\u5728\u91cd\u65b0\u8bc4\u4f30\u5176\u4ef7\u503c\u548c\u5728\u73b0\u4ee3\u6df7\u5408\u65b9\u6cd5\u8bba\u4e2d\u7684\u89d2\u8272\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u6279\u5224\u6027\u5206\u6790\uff0c\u7efc\u5408\u4e86\u7011\u5e03\u6a21\u578b\u7684\u6982\u5ff5\u8d77\u6e90\u3001Royce\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\u4ee5\u53ca\u5176\u5728\u884c\u4e1a\u4e2d\u7684\u6f14\u53d8\u548c\u6279\u8bc4\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u7011\u5e03\u6a21\u578b\u5df2\u4ece\u72ec\u7acb\u6846\u67b6\u8f6c\u53d8\u4e3a\u73b0\u4ee3\u6df7\u5408\u65b9\u6cd5\u8bba\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u539f\u5219\u4ecd\u5f71\u54cd\u5f53\u524d\u7684\u5f00\u53d1\u5b9e\u8df5\u3002", "conclusion": "\u7011\u5e03\u6a21\u578b\u867d\u7136\u5e38\u88ab\u6279\u8bc4\u5176\u50f5\u5316\u6027\u548c\u9ad8\u5931\u8d25\u7387\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u4ecd\u7136\u9002\u7528\uff0c\u5e76\u901a\u8fc7\u878d\u5165\u73b0\u4ee3\u6df7\u5408\u5f00\u53d1\u6846\u67b6\u4fdd\u6301\u5176\u76f8\u5173\u6027\u3002\u5176\u6301\u4e45\u4ef7\u503c\u5728\u4e8e\u9002\u5e94\u6027\u548c\u4f5c\u4e3a\u60c5\u5883\u611f\u77e5\u5f00\u53d1\u7b56\u7565\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2510.03677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03677", "abs": "https://arxiv.org/abs/2510.03677", "authors": ["Salim Rezvani", "Ammar Jaleel Mahmood", "Robin Chhabra"], "title": "Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments", "comment": null, "summary": "Robots with internal visual self-models promise unprecedented adaptability,\nyet existing autonomous modeling pipelines remain fragile under realistic\nsensing conditions such as noisy imagery and cluttered backgrounds. This paper\npresents the first systematic study quantifying how visual\ndegradations--including blur, salt-and-pepper noise, and Gaussian noise--affect\nrobotic self-modeling. Through both simulation and physical experiments, we\ndemonstrate their impact on morphology prediction, trajectory planning, and\ndamage recovery in state-of-the-art pipelines. To overcome these challenges, we\nintroduce a task-aware denoising framework that couples classical restoration\nwith morphology-preserving constraints, ensuring retention of structural cues\ncritical for self-modeling. In addition, we integrate semantic segmentation to\nrobustly isolate robots from cluttered and colorful scenes. Extensive\nexperiments show that our approach restores near-baseline performance across\nsimulated and physical platforms, while existing pipelines degrade\nsignificantly. These contributions advance the robustness of visual\nself-modeling and establish practical foundations for deploying self-aware\nrobots in unpredictable real-world environments.", "AI": {"tldr": "\u7814\u7a76\u91cf\u5316\u4e86\u89c6\u89c9\u9000\u5316\u5bf9\u673a\u5668\u4eba\u81ea\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\u548c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5efa\u6a21\u7ba1\u9053\u5728\u771f\u5b9e\u611f\u77e5\u6761\u4ef6\u4e0b\uff08\u5982\u566a\u58f0\u56fe\u50cf\u548c\u6742\u4e71\u80cc\u666f\uff09\u8868\u73b0\u8106\u5f31\uff0c\u91cf\u5316\u89c6\u89c9\u9000\u5316\u5bf9\u673a\u5668\u4eba\u81ea\u5efa\u6a21\u7684\u5f71\u54cd\u5e76\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u6062\u590d\u65b9\u6cd5\u548c\u5f62\u6001\u4fdd\u6301\u7ea6\u675f\u7684\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\uff0c\u4ee5\u53ca\u7528\u4e8e\u9c81\u68d2\u9694\u79bb\u673a\u5668\u4eba\u7684\u8bed\u4e49\u5206\u5272\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u7269\u7406\u5e73\u53f0\u4e0a\u6062\u590d\u4e86\u63a5\u8fd1\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u7ba1\u9053\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\u548c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u81ea\u5efa\u6a21\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u81ea\u611f\u77e5\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2510.03771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline.", "AI": {"tldr": "OptAgent\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u5199\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4e3b\u89c2\u4efb\u52a1\uff08\u5982\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u5199\uff09\u7f3a\u4e4f\u5355\u4e00\u6b63\u786e\u7b54\u6848\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u9700\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6a21\u62df\uff08\u6a21\u62df\u8d2d\u7269\u987e\u5ba2\uff09\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u52a8\u6001\u4f18\u5316\u67e5\u8be2\u91cd\u5199\u3002", "result": "\u57281000\u4e2a\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u4e0a\u6d4b\u8bd5\uff0cOptAgent\u6bd4\u539f\u59cb\u67e5\u8be2\u5e73\u5747\u63d0\u534721.98%\uff0c\u6bd4Best-of-N\u57fa\u7ebf\u63d0\u53473.36%\u3002", "conclusion": "OptAgent\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u5199\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e3b\u89c2\u4efb\u52a1\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively.", "AI": {"tldr": "MACOG\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u534f\u4f5c\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u3001\u5408\u89c4\u4e14\u53ef\u6269\u5c55\u7684IaC\u914d\u7f6e\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5728\u751f\u6210IaC\u65f6\u5b58\u5728\u5355\u6b21\u751f\u6210\u5bfc\u81f4\u7684\u8bed\u6cd5\u9519\u8bef\u3001\u7b56\u7565\u8fdd\u89c4\u548c\u8bbe\u8ba1\u4e0d\u53ef\u6269\u5c55\u7b49\u95ee\u9898\uff0c\u9700\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMACOG\u67b6\u6784\uff0c\u5c06IaC\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u6a21\u5757\u5316\u5b50\u4efb\u52a1\uff0c\u7531\u4e0d\u540c\u4e13\u4e1a\u667a\u80fd\u4f53\uff08\u5982Architect\u3001Provider Harmonizer\u7b49\uff09\u534f\u4f5c\u5b8c\u6210\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u9ed1\u677f\u548c\u72b6\u6001\u534f\u8c03\u5668\u4ea4\u4e92\u3002\u7ed3\u5408Terraform Plan\u548cOPA\u786e\u4fdd\u751f\u6210\u7684\u914d\u7f6e\u6b63\u786e\u4e14\u5408\u89c4\u3002", "result": "\u5728IaC-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMACOG\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff08\u5982GPT-5\u4ece54.90\u63d0\u5347\u81f374.02\uff09\uff0c\u5e76\u5728BLEU\u3001CodeBERTScore\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MACOG\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86IaC\u751f\u6210\u7684\u51c6\u786e\u6027\u3001\u5408\u89c4\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u4e91\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03706", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03706", "abs": "https://arxiv.org/abs/2510.03706", "authors": ["Eadom Dessalene", "Pavan Mantripragada", "Michael Maynord", "Yiannis Aloimonos"], "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning", "comment": "Video link:\n  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing", "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption.", "AI": {"tldr": "EmbodiSwap\u901a\u8fc7\u5408\u6210\u673a\u5668\u4eba\u8986\u76d6\u5c42\u548cV-JEPA\u89c6\u89c9\u9aa8\u5e72\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\uff0c\u6210\u529f\u7387\u8fbe82%\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u89c6\u9891\u4e0e\u76ee\u6807\u673a\u5668\u4eba\u4f53\u73b0\u4e4b\u95f4\u7684\u2018\u4f53\u73b0\u5dee\u8ddd\u2019\uff0c\u5b9e\u73b0\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u3002", "method": "\u91c7\u7528EmbodiSwap\u65b9\u6cd5\u751f\u6210\u5408\u6210\u673a\u5668\u4eba\u8986\u76d6\u5c42\uff0c\u5e76\u7ed3\u5408V-JEPA\u89c6\u89c9\u9aa8\u5e72\u8fdb\u884c\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8bad\u7ec3\u7684V-JEPA\u6a21\u578b\u8fbe\u523082%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5c11\u91cf\u6837\u672c\u8bad\u7ec3\u7684\u03c0\u2080\u7f51\u7edc\u3002", "conclusion": "EmbodiSwap\u6210\u529f\u5730\u5c06\u4eba\u7c7b\u89c6\u9891\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u5408\u6210\u673a\u5668\u4eba\u8986\u76d6\u5c42\uff0c\u5e76\u901a\u8fc7V-JEPA\u89c6\u89c9\u9aa8\u5e72\u5728\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u6210\u529f\u7387\u8fbe\u523082%\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2510.03777", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS.", "AI": {"tldr": "GuidedSampling \u901a\u8fc7\u89e3\u8026\u63a2\u7d22\u548c\u751f\u6210\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Repeated Sampling \u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u751f\u6210\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e38\u4f9d\u8d56\u76f8\u540c\u7684\u5e95\u5c42\u65b9\u6cd5\u5bfc\u81f4\u5197\u4f59\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86 GuidedSampling \u7b97\u6cd5\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u89e3\u8026\u4e3a\u63a2\u7d22\u548c\u751f\u6210\u4e24\u4e2a\u9636\u6bb5\uff1a\u63a2\u7d22\u9636\u6bb5\u8bc6\u522b\u89e3\u51b3\u95ee\u9898\u7684\u591a\u4e2a\u6982\u5ff5\uff0c\u751f\u6210\u9636\u6bb5\u5e94\u7528\u7279\u5b9a\u6982\u5ff5\u751f\u6210\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002", "result": "GuidedSampling \u5728 pass@50 \u4e0a\u5e73\u5747\u63d0\u5347 21.6%\uff0c\u5728 pass@5 \u4e0a\u5e73\u5747\u63d0\u5347 9.7%\uff0c\u4e14\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5e73\u5747\u6982\u5ff5\u6570\u4ece 1.67 \u589e\u52a0\u5230 3.03\u3002", "conclusion": "GuidedSampling \u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u65b9\u9762\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684 Repeated Sampling \u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u4eba\u7c7b\u6700\u4f73\u5b9e\u8df5\u7684\u6307\u4ee4\u7b56\u7565\u662f\u5426\u80fd\u589e\u5f3aLLMs\u6267\u884c\u591a\u6837\u5316\u91cd\u6784\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u57fa\u4e8eFowler\u6307\u5357\u7684\u6307\u4ee4\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u91cd\u6784\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u4eba\u5458\u5e38\u56e0\u91cd\u6784\u6240\u9700\u7684\u65f6\u95f4\u3001\u7cbe\u529b\u548c\u8d44\u6e90\u4ee5\u53ca\u7f3a\u4e4f\u5373\u65f6\u529f\u80fd\u6027\u56de\u62a5\u800c\u5ffd\u89c6\u91cd\u6784\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u91cd\u6784\u5de5\u5177\u652f\u6301\u7684\u91cd\u6784\u7c7b\u578b\u6709\u9650\u3002", "method": "\u7814\u7a76\u5229\u7528GPT-mini\u548cDeepSeek-V3\u7b49\u5148\u8fdbLLMs\u7684\u6307\u4ee4\u8ddf\u968f\u548c\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u591a\u79cd\u6307\u4ee4\u7b56\u7565\uff0c\u6db5\u76d661\u79cd\u5e38\u89c1\u91cd\u6784\u7c7b\u578b\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u548cGitHub\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eFowler\u6307\u5357\u7684\u6307\u4ee4\u8bbe\u8ba1\u4f7fLLMs\u80fd\u591f\u6210\u529f\u6267\u884c\u6240\u6709\u57fa\u51c6\u91cd\u6784\u7c7b\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4fdd\u6301\u7a0b\u5e8f\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u89c4\u5219\u578b\u6307\u4ee4\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u5173\u6ce8\u91cd\u6784\u6574\u4f53\u76ee\u6807\u800c\u975e\u56fa\u5b9a\u53d8\u6362\u7c7b\u578b\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8eMartin Fowler\u91cd\u6784\u6307\u5357\u7684\u6307\u4ee4\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u63d0\u5347LLMs\u5728\u591a\u79cd\u91cd\u6784\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u4fdd\u6301\u7a0b\u5e8f\u8bed\u4e49\u548c\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u3002"}}
{"id": "2510.03768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03768", "abs": "https://arxiv.org/abs/2510.03768", "authors": ["Aydin Ahmadi", "Baris Akgun"], "title": "Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics", "comment": null, "summary": "Data-driven planar pushing methods have recently gained attention as they\nreduce manual engineering effort and improve generalization compared to\nanalytical approaches. However, most prior work targets narrow capabilities\n(e.g., side switching, precision, or single-task training), limiting broader\napplicability. We present a model-based framework for non-prehensile tabletop\npushing that uses a single learned model to address multiple tasks without\nretraining. Our approach employs a recurrent GRU-based architecture with\nadditional non-linear layers to capture object-environment dynamics while\nensuring stability. A tailored state-action representation enables the model to\ngeneralize across uncertain dynamics, variable push lengths, and diverse tasks.\nFor control, we integrate the learned dynamics with a sampling-based Model\nPredictive Path Integral (MPPI) controller, which generates adaptive,\ntask-oriented actions. This framework supports side switching, variable-length\npushes, and objectives such as precise positioning, trajectory following, and\nobstacle avoidance. Training is performed in simulation with domain\nrandomization to support sim-to-real transfer. We first evaluate the\narchitecture through ablation studies, showing improved prediction accuracy and\nstable rollouts. We then validate the full system in simulation and real-world\nexperiments using a Franka Panda robot with markerless tracking. Results\ndemonstrate high success rates in precise positioning under strict thresholds\nand strong performance in trajectory tracking and obstacle avoidance. Moreover,\nmultiple tasks are solved simply by changing the controller's objective\nfunction, without retraining. While our current focus is on a single object\ntype, we extend the framework by training on wider push lengths and designing a\nbalanced controller that reduces the number of steps for longer-horizon goals.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u975e\u6293\u53d6\u684c\u9762\u63a8\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u5b66\u4e60\u6a21\u578b\u89e3\u51b3\u591a\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u9ad8\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51cf\u5c11\u624b\u52a8\u5de5\u7a0b\u5de5\u4f5c\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5e7f\u6cdb\u9002\u7528\u6027\u4e0a\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGRU\u7684\u5faa\u73af\u67b6\u6784\u548c\u975e\u7ebf\u6027\u5c42\u6765\u6355\u6349\u7269\u4f53-\u73af\u5883\u52a8\u6001\uff0c\u7ed3\u5408MPPI\u63a7\u5236\u5668\u751f\u6210\u81ea\u9002\u5e94\u4efb\u52a1\u5bfc\u5411\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u6210\u529f\u7387\u7684\u7cbe\u786e\u5b9a\u4f4d\u3001\u5f3a\u6027\u80fd\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5355\u4e00\u5b66\u4e60\u6a21\u578b\u89e3\u51b3\u4e86\u591a\u79cd\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5728\u7cbe\u786e\u5b9a\u4f4d\u3001\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u65b9\u9762\u7684\u9ad8\u6210\u529f\u7387\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002"}}
{"id": "2510.03845", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u9690\u85cf\u6e38\u620f\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u9057\u61be\u6700\u5c0f\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u9690\u85cf\u5b50\u6e38\u620f\u4e2d\u5feb\u901f\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u7406\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u7b56\u7565\u7a7a\u95f4\u6e38\u620f\u4e2d\u7684\u9690\u85cf\u7ed3\u6784\u95ee\u9898\uff0c\u7279\u522b\u662fAI\u5bf9\u9f50\u548c\u8bed\u8a00\u6e38\u620f\u4e2d\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u61be\u6700\u5c0f\u5316\u6280\u672f\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u4ee5\u8fbe\u5230\u6700\u4f18\u7684\u5916\u90e8\u9057\u61be\u548c\u4ea4\u6362\u9057\u61be\u8fb9\u754c\u3002", "result": "\u5f00\u53d1\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u53d1\u73b0\u5e76\u5229\u7528\u9690\u85cf\u7ed3\u6784\uff0c\u786e\u4fdd\u5728\u9690\u85cf\u5b50\u6e38\u620f\u4e2d\u5feb\u901f\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u7ed3\u5408\u9057\u61be\u6700\u5c0f\u5316\u6280\u672f\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9690\u85cf\u6e38\u620f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u9690\u85cf\u5b50\u6e38\u620f\u4e2d\u5feb\u901f\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u7406\u6027\u3002"}}
{"id": "2510.03920", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.03920", "abs": "https://arxiv.org/abs/2510.03920", "authors": ["Ravi Kalluri"], "title": "Why Does the Engineering Manager Still Exist in Agile Software Development?", "comment": "12 pages, 3 figures, 2 tables", "summary": "Although Agile methodologies emphasize decentralized decision-making and team\nautonomy, engineering managers continue to be employed in Agile software\norganizations. This apparent paradox suggests that traditional managerial\nfunctions persist despite the theoretical displacement of managerial hierarchy\nin Agile. This paper explores the persistence of engineering managers through a\nmultidimensional framework encompassing historical context, theoretical\ntensions, organizational realities, empirical evidence, evolving managerial\nroles, and practical implications. A systematic literature review underpins our\nmultifaceted analysis, supplemented by illustrative case studies. We conclude\nby proposing a conceptual model that reconciles Agile principles with\nmanagerial necessity, offering guidance for practitioners, researchers, and\ntool designers. Implications for leadership development, tool integration, and\nfuture research are discussed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u654f\u6377\u7ec4\u7ec7\u4e2d\u5de5\u7a0b\u7ecf\u7406\u6301\u7eed\u5b58\u5728\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u6765\u8c03\u548c\u654f\u6377\u539f\u5219\u4e0e\u7ba1\u7406\u9700\u6c42\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u8df5\u548c\u7814\u7a76\u7684\u610f\u4e49\u3002", "motivation": "\u5c3d\u7ba1\u654f\u6377\u65b9\u6cd5\u8bba\u5f3a\u8c03\u5206\u6563\u51b3\u7b56\u548c\u56e2\u961f\u81ea\u4e3b\u6027\uff0c\u4f46\u5de5\u7a0b\u7ecf\u7406\u5728\u654f\u6377\u8f6f\u4ef6\u7ec4\u7ec7\u4e2d\u4ecd\u7136\u5b58\u5728\uff0c\u8fd9\u79cd\u77db\u76fe\u73b0\u8c61\u4fc3\u4f7f\u7814\u7a76\u63a2\u7d22\u5176\u539f\u56e0\u548c\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7684\u6587\u732e\u7efc\u8ff0\u652f\u6301\u7684\u591a\u9762\u5206\u6790\uff0c\u5e76\u8f85\u4ee5\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5de5\u7a0b\u7ecf\u7406\u5728\u654f\u6377\u73af\u5883\u4e2d\u7684\u6301\u7eed\u5b58\u5728\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5c06\u654f\u6377\u539f\u5219\u4e0e\u7ba1\u7406\u5fc5\u8981\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u4ece\u4e1a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u5de5\u5177\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.03776", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03776", "abs": "https://arxiv.org/abs/2510.03776", "authors": ["Tiago Rodrigues de Almeida", "Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Johannes A. Stork", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets", "comment": "This paper has been accepted to the IEEE Robotics and Automation\n  Letters journal and presented at the 40th Anniversary of the IEEE\n  International Conference on Robotics and Automation, which was held in\n  Rotterdam, Netherlands on 23-26 September, 2024", "summary": "Robots and other intelligent systems navigating in complex dynamic\nenvironments should predict future actions and intentions of surrounding agents\nto reach their goals efficiently and avoid collisions. The dynamics of those\nagents strongly depends on their tasks, roles, or observable labels.\nClass-conditioned motion prediction is thus an appealing way to reduce forecast\nuncertainty and get more accurate predictions for heterogeneous agents.\nHowever, this is hardly explored in the prior art, especially for mobile robots\nand in limited data applications. In this paper, we analyse different\nclass-conditioned trajectory prediction methods on two datasets. We propose a\nset of conditional pattern-based and efficient deep learning-based baselines,\nand evaluate their performance on robotics and outdoors datasets (TH\\\"OR-MAGNI\nand Stanford Drone Dataset). Our experiments show that all methods improve\naccuracy in most of the settings when considering class labels. More\nimportantly, we observe that there are significant differences when learning\nfrom imbalanced datasets, or in new environments where sufficient data is not\navailable. In particular, we find that deep learning methods perform better on\nbalanced datasets, but in applications with limited data, e.g., cold start of a\nrobot in a new environment, or imbalanced classes, pattern-based methods may be\npreferable.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7c7b\u522b\u6761\u4ef6\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u5728\u5e73\u8861\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u6570\u636e\u6709\u9650\u6216\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\uff0c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u66f4\u4f18\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u9884\u6d4b\u5468\u56f4\u4ee3\u7406\u7684\u672a\u6765\u884c\u4e3a\u548c\u610f\u56fe\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u907f\u514d\u78b0\u649e\uff0c\u800c\u4ee3\u7406\u7684\u52a8\u6001\u6027\u4e0e\u5176\u4efb\u52a1\u3001\u89d2\u8272\u6216\u53ef\u89c2\u5bdf\u6807\u7b7e\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u5206\u6790\u4e86\u51e0\u79cd\u7c7b\u522b\u6761\u4ef6\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6761\u4ef6\u6a21\u5f0f\u7684\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8003\u8651\u7c7b\u522b\u6807\u7b7e\u65f6\uff0c\u6240\u6709\u65b9\u6cd5\u5728\u591a\u6570\u8bbe\u7f6e\u4e0b\u5747\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u6570\u636e\u6709\u9650\u6216\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\uff0c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u5728\u6709\u9650\u6570\u636e\u6216\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u53ef\u80fd\u6bd4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u66f4\u4f18\u3002"}}
{"id": "2510.03847", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference", "AI": {"tldr": "SLMs\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6210\u672c\u4f4e\u3001\u6548\u7387\u9ad8\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u3002\u901a\u8fc7\u8bbe\u8ba1\u6a21\u5f0f\u548c\u4e0d\u786e\u5b9a\u6027\u8def\u7531\uff0cSLMs\u53ef\u4f5c\u4e3a\u9ed8\u8ba4\u9009\u62e9\uff0cLLMs\u4f5c\u4e3a\u540e\u5907\u3002", "motivation": "\u63a2\u8ba8\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\uff0cSLMs\u662f\u5426\u80fd\u591f\u66ff\u4ee3LLMs\uff0c\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u9ad8\u7684\u6548\u7387\u5b9e\u73b0\u76f8\u540c\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "method": "\u7efc\u5408\u5206\u6790\u4e86\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709\u7684SLMs\uff08\u5982Phi-4-Mini\u3001Qwen-2.5-7B\u7b49\uff09\uff0c\u5e76\u7ed3\u5408\u73b0\u4ee3\u8bc4\u4f30\u5de5\u5177\uff08\u5982BFCL v3/v4\uff09\u548c\u670d\u52a1\u5806\u6808\uff08\u5982vLLM\u3001SGLang\uff09\u3002\u63d0\u51fa\u4e86SLM-default\u3001LLM-fallback\u7cfb\u7edf\uff0c\u5e76\u5f15\u5165\u4e86\u5de5\u7a0b\u6307\u6807\uff08\u5982CPS\u3001schema\u6709\u6548\u6027\u7387\u7b49\uff09\u3002", "result": "SLMs\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548cRAG\u7b49\u4efb\u52a1\u4e2d\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8aLLMs\uff0c\u4e14\u6210\u672c\u964d\u4f4e10x-100x\uff0c\u5ef6\u8fdf\u548c\u80fd\u8017\u663e\u8457\u6539\u5584\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9SLMs\u4f18\u5316\u7684\u4ee3\u7406\u5806\u6808\u8bbe\u8ba1\u6a21\u5f0f\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u9ad8\u7684\u6548\u7387\u5339\u914d\u6216\u8d85\u8d8a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5c24\u5176\u662f\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548cRAG\u65b9\u9762\u3002\u901a\u8fc7\u8bbe\u8ba1\u6a21\u5f0f\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8def\u7531\uff0cSLMs\u53ef\u4ee5\u4f5c\u4e3a\u9ed8\u8ba4\u9009\u62e9\uff0c\u540c\u65f6\u4fdd\u7559LLMs\u4f5c\u4e3a\u540e\u5907\u4ee5\u5e94\u5bf9\u5f00\u653e\u9886\u57df\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u3002"}}
{"id": "2510.04078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04078", "abs": "https://arxiv.org/abs/2510.04078", "authors": ["Han Hu", "Wei Minn", "Yonghui Liu", "Jiakun Liu", "Ferdian Thung", "Terry Yue Zhuo", "Lwin Khin Shar", "Debin Gao", "David Lo"], "title": "Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework", "comment": null, "summary": "The permission mechanism in the Android Framework is integral to safeguarding\nthe privacy of users by managing users' and processes' access to sensitive\nresources and operations. As such, developers need to be equipped with an\nin-depth understanding of API permissions to build robust Android apps.\nUnfortunately, the official API documentation by Android chronically suffers\nfrom imprecision and incompleteness, causing developers to spend significant\neffort to accurately discern necessary permissions. This potentially leads to\nincorrect permission declarations in Android app development, potentially\nresulting in security violations and app failures. Recent efforts in improving\npermission specification primarily leverage static and dynamic code analyses to\nuncover API-permission mappings within the Android framework. Yet, these\nmethodologies encounter substantial shortcomings, including poor adaptability\nto Android SDK and Framework updates, restricted code coverage, and a\npropensity to overlook essential API-permission mappings in intricate\ncodebases. This paper introduces a pioneering approach utilizing large language\nmodels (LLMs) for a systematic examination of API-permission mappings. In\naddition to employing LLMs, we integrate a dual-role prompting strategy and an\nAPI-driven code generation approach into our mapping discovery pipeline,\nresulting in the development of the corresponding tool, \\tool{}. We formulate\nthree research questions to evaluate the efficacy of \\tool{} against\nstate-of-the-art baselines, assess the completeness of official SDK\ndocumentation, and analyze the evolution of permission-required APIs across\ndifferent SDK releases. Our experimental results reveal that \\tool{} identifies\n2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and\n10 respectively, substantially outprforming existing baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLMs\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbAndroid API\u6743\u9650\u6620\u5c04\u7684\u53d1\u73b0\uff0c\u5f00\u53d1\u4e86\u5de5\u5177\\tool{}\uff0c\u5e76\u5728\u591a\u4e2aAndroid\u7248\u672c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "Android\u5b98\u65b9API\u6587\u6863\u5728\u6743\u9650\u63cf\u8ff0\u4e0a\u5b58\u5728\u4e0d\u7cbe\u786e\u548c\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u5728\u58f0\u660e\u6743\u9650\u65f6\u5bb9\u6613\u51fa\u9519\uff0c\u8fdb\u800c\u5f15\u53d1\u5b89\u5168\u98ce\u9669\u548c\u5e94\u7528\u7a0b\u5e8f\u6545\u969c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u53cc\u89d2\u8272\u63d0\u793a\u7b56\u7565\u548cAPI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u540d\u4e3a\\tool{}\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u5206\u6790API-\u6743\u9650\u6620\u5c04\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\\tool{}\u5728Android\u7248\u672c6\u30017\u548c10\u4e2d\u5206\u522b\u8bc6\u522b\u4e862,234\u30013,552\u548c4,576\u4e2aAPI-\u6743\u9650\u6620\u5c04\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u521b\u65b0\u7684\u53cc\u89d2\u8272\u63d0\u793a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86API\u6743\u9650\u6620\u5c04\u7684\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\uff0c\u4e3aAndroid\u5f00\u53d1\u8005\u548c\u5b89\u5168\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6743\u9650\u7ba1\u7406\u5de5\u5177\u3002"}}
{"id": "2510.03875", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03875", "abs": "https://arxiv.org/abs/2510.03875", "authors": ["Niranjan Kumar Ilampooranan", "Constantinos Chamzas"], "title": "COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments", "comment": null, "summary": "Having the ability to answer motion-planning queries within a fixed time\nbudget is critical for the widespread deployment of robotic systems.\nSemi-static environments, where most obstacles remain static but a limited set\ncan vary across queries, exhibit structured variability that can be\nsystematically exploited to provide stronger guarantees than in general\nmotion-planning problems. However, prior approaches in this setting either lack\nformal guarantees or rely on restrictive discretizations of obstacle\nconfigurations, limiting their applicability in realistic domains. This paper\nintroduces COVER, a novel framework that incrementally constructs a\ncoverage-verified roadmap in semi-static environments. By partitioning the\nobstacle configuration space and solving for feasible paths within each\npartition, COVER systematically verifies feasibility of the roadmap in each\npartition and guarantees fixed-time motion planning queries within the verified\nregions. We validate COVER with a 7-DOF simulated Panda robot performing table\nand shelf tasks, demonstrating that COVER achieves broader coverage with higher\nquery success rates than prior works.", "AI": {"tldr": "COVER\u662f\u4e00\u4e2a\u534a\u9759\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u533a\u9a8c\u8bc1\u8def\u7ebf\u56fe\uff0c\u4fdd\u8bc1\u56fa\u5b9a\u65f6\u95f4\u67e5\u8be2\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u534a\u9759\u6001\u73af\u5883\u4e2d\u5b58\u5728\u7ed3\u6784\u5316\u53ef\u53d8\u6027\uff0c\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u5229\u7528\u4ee5\u63d0\u4f9b\u6bd4\u4e00\u822c\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u66f4\u5f3a\u7684\u4fdd\u8bc1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6b63\u5f0f\u4fdd\u8bc1\u6216\u4f9d\u8d56\u9650\u5236\u6027\u79bb\u6563\u5316\u3002", "method": "COVER\u901a\u8fc7\u5206\u533a\u969c\u788d\u7269\u914d\u7f6e\u7a7a\u95f4\u5e76\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u6c42\u89e3\u53ef\u884c\u8def\u5f84\uff0c\u9010\u6b65\u6784\u5efa\u8986\u76d6\u9a8c\u8bc1\u7684\u8def\u7ebf\u56fe\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u6a21\u62dfPanda\u673a\u5668\u4eba\u6267\u884c\u684c\u9762\u548c\u8d27\u67b6\u4efb\u52a1\u4e2d\uff0cCOVER\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5e7f\u7684\u8986\u76d6\u8303\u56f4\u548c\u66f4\u9ad8\u7684\u67e5\u8be2\u6210\u529f\u7387\u3002", "conclusion": "COVER\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5730\u9a8c\u8bc1\u534a\u9759\u6001\u73af\u5883\u4e2d\u7684\u8def\u7ebf\u56fe\u5206\u533a\uff0c\u4e3a\u56fa\u5b9a\u65f6\u95f4\u8fd0\u52a8\u89c4\u5212\u67e5\u8be2\u63d0\u4f9b\u4e86\u4fdd\u8bc1\uff0c\u5e76\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u8986\u76d6\u7387\u548c\u67e5\u8be2\u6210\u529f\u7387\u3002"}}
{"id": "2510.03851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).", "AI": {"tldr": "MetaMuse\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u539f\u5219\u514b\u670dLLMs\u5728\u7b97\u6cd5\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f13\u5b58\u66ff\u6362\u548c\u5728\u7ebf\u88c5\u7bb1\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u591f\u5b9e\u9645\u9a71\u52a8\u7b97\u6cd5\u751f\u6210\uff0c\u53d1\u73b0\u5176\u503e\u5411\u4e8e\u5df2\u77e5\u7684\u901a\u7528\u8bbe\u8ba1\u800c\u975e\u521b\u9020\u6027\u7a81\u7834\uff0c\u56e0\u6b64\u9700\u8981\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "MetaMuse\u6846\u67b6\u57fa\u4e8e\u4e09\u4e2a\u81ea\u6211\u53cd\u601d\u539f\u5219\uff1a(1)\u5728\u53ef\u8861\u91cf\u7684\u6027\u80fd\u7a7a\u95f4\u4e2d\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\uff0c(2)\u901a\u8fc7\u5916\u90e8\u523a\u6fc0\u800c\u975e\u5185\u90e8\u968f\u673a\u6027\u5f15\u5bfc\u6784\u601d\uff0c(3)\u4f7f\u7528\u8def\u6807\u63a8\u7406\u800c\u975e\u81ea\u7531\u5f62\u5f0f\u7684\u94fe\u5f0f\u601d\u7ef4\u6784\u5efa\u53ef\u6267\u884c\u89e3\u51b3\u65b9\u6848\u3002", "result": "MetaMuse\u5728\u7f13\u5b58\u66ff\u6362\uff08\u51cf\u5c11\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u9ad8\u8fbe35.76%\uff09\u548c\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\uff08\u51cf\u5c11\u5bb9\u5668\u4f7f\u7528\u9ad8\u8fbe30.93%\uff09\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MetaMuse\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u53cd\u601d\u539f\u5219\uff0c\u6210\u529f\u514b\u670d\u4e86LLMs\u5728\u7b97\u6cd5\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u4e3a\u5168\u7403\u4e91\u63d0\u4f9b\u5546\u7684\u5173\u952e\u95ee\u9898\u751f\u6210\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment.", "AI": {"tldr": "GA4GC\u6846\u67b6\u4f18\u5316\u7f16\u7801\u4ee3\u7406\u7684\u53ef\u6301\u7eed\u6027\u548c\u6027\u80fd\uff0c\u51cf\u5c11\u8fd0\u884c\u65f637.7%\u5e76\u63d0\u9ad8\u6b63\u786e\u6027\uff0c\u6e29\u5ea6\u662f\u6700\u5173\u952e\u7684\u8d85\u53c2\u6570\u3002", "motivation": "\u89e3\u51b3LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u53ef\u6301\u7eed\u6027\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5982\u5355\u6b21\u8fd0\u884c\u6d88\u8017\u8d85\u8fc7100k tokens\u548c\u9ad8\u73af\u5883\u6210\u672c\u3002", "method": "\u5f15\u5165GA4GC\u6846\u67b6\uff0c\u7cfb\u7edf\u4f18\u5316\u7f16\u7801\u4ee3\u7406\u8fd0\u884c\u65f6\uff08\u66f4\u73af\u4fdd\u7684\u4ee3\u7406\uff09\u548c\u4ee3\u7801\u6027\u80fd\uff08\u66f4\u73af\u4fdd\u7684\u4ee3\u7801\uff09\u7684\u6743\u8861\uff0c\u901a\u8fc7\u53d1\u73b0Pareto\u6700\u4f18\u7684\u4ee3\u7406\u8d85\u53c2\u6570\u548c\u63d0\u793a\u6a21\u677f\u3002", "result": "\u5728SWE-Perf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGA4GC\u5b9e\u73b0\u4e86\u9ad8\u8fbe135\u500d\u7684\u8d85\u4f53\u79ef\u6539\u8fdb\uff0c\u51cf\u5c11\u4e8637.7%\u7684\u4ee3\u7406\u8fd0\u884c\u65f6\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6b63\u786e\u6027\u3002", "conclusion": "GA4GC\u6846\u67b6\u901a\u8fc7\u53d1\u73b0Pareto\u6700\u4f18\u7684\u4ee3\u7406\u8d85\u53c2\u6570\u548c\u63d0\u793a\u6a21\u677f\uff0c\u6709\u6548\u5e73\u8861\u4e86\u7f16\u7801\u4ee3\u7406\u8fd0\u884c\u65f6\u4e0e\u4ee3\u7801\u6027\u80fd\u7684\u6743\u8861\uff0c\u4e3a\u5de5\u4e1a\u90e8\u7f72\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u548c\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2510.03885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03885", "abs": "https://arxiv.org/abs/2510.03885", "authors": ["Sunghwan Kim", "Woojeh Chung", "Zhirui Dai", "Dwait Bhatt", "Arth Shukla", "Hao Su", "Yulun Tian", "Nikolay Atanasov"], "title": "Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning", "comment": "Project website can be found at\n  https://existentialrobotics.org/sbp_page/", "summary": "In this paper, we demonstrate that mobile manipulation policies utilizing a\n3D latent map achieve stronger spatial and temporal reasoning than policies\nrelying solely on images. We introduce Seeing the Bigger Picture (SBP), an\nend-to-end policy learning approach that operates directly on a 3D map of\nlatent features. In SBP, the map extends perception beyond the robot's current\nfield of view and aggregates observations over long horizons. Our mapping\napproach incrementally fuses multiview observations into a grid of\nscene-specific latent features. A pre-trained, scene-agnostic decoder\nreconstructs target embeddings from these features and enables online\noptimization of the map features during task execution. A policy, trainable\nwith behavior cloning or reinforcement learning, treats the latent map as a\nstate variable and uses global context from the map obtained via a 3D feature\naggregator. We evaluate SBP on scene-level mobile manipulation and sequential\ntabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons\nglobally over the scene, (ii) leverages the map as long-horizon memory, and\n(iii) outperforms image-based policies in both in-distribution and novel\nscenes, e.g., improving the success rate by 25% for the sequential manipulation\ntask.", "AI": {"tldr": "SBP\u662f\u4e00\u79cd\u57fa\u4e8e3D\u6f5c\u5728\u5730\u56fe\u7684\u7aef\u5230\u7aef\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u63a8\u7406\u548c\u957f\u671f\u8bb0\u5fc6\u63d0\u5347\u79fb\u52a8\u64cd\u7eb5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79fb\u52a8\u64cd\u7eb5\u7b56\u7565\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8d85\u51fa\u673a\u5668\u4eba\u5f53\u524d\u89c6\u91ce\u548c\u957f\u671f\u89c2\u5bdf\u805a\u5408\u7684\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86Seeing the Bigger Picture (SBP)\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f4\u63a5\u64cd\u4f5c3D\u6f5c\u5728\u7279\u5f81\u5730\u56fe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u89c6\u89d2\u89c2\u5bdf\u9010\u6b65\u878d\u5408\u5230\u573a\u666f\u7279\u5b9a\u7684\u6f5c\u5728\u7279\u5f81\u7f51\u683c\u4e2d\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684\u3001\u573a\u666f\u65e0\u5173\u7684\u89e3\u7801\u5668\u8fdb\u884c\u5728\u7ebf\u4f18\u5316\u3002", "result": "SBP\u5728\u573a\u666f\u7ea7\u79fb\u52a8\u64cd\u7eb5\u548c\u987a\u5e8f\u684c\u9762\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5168\u5c40\u63a8\u7406\u573a\u666f\u3001\u5229\u7528\u5730\u56fe\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\uff0c\u5e76\u5728\u5206\u5e03\u5185\u548c\u5168\u65b0\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u7b56\u7565\uff0c\u4f8b\u5982\u987a\u5e8f\u64cd\u7eb5\u4efb\u52a1\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8625%\u3002", "conclusion": "SBP\u901a\u8fc73D\u6f5c\u5728\u5730\u56fe\u5b9e\u73b0\u4e86\u6bd4\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684\u7b56\u7565\u66f4\u5f3a\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2510.03859", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548cXAI\u7684\u667a\u80fd\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728IoT\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u52a8\u6001\u3001\u9ad8\u7ef4\u4e14\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u4e0d\u65ad\u6f14\u53d8\u7684IoT\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u81ea\u9002\u5e94\u3001\u667a\u80fd\u5316\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528LLM\u652f\u6301\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5\u548cXAI\u4ee3\u7406\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u907f\u514d\u5904\u7406\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ec6\u8282\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u7535\u7f51\u548c\u533b\u7597\u573a\u666f\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u7ed3\u679c\u6e05\u6670\u5ea6\u548c\u54cd\u5e94\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684LLM\u589e\u5f3a\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u672a\u6765IoT\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2510.04143", "categories": ["cs.SE", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.04143", "abs": "https://arxiv.org/abs/2510.04143", "authors": ["Konstantinos Kitsios", "Francesco Sovrano", "Earl T. Barr", "Alberto Bacchelli"], "title": "Detecting Semantic Clones of Unseen Functionality", "comment": "13 pages, 3 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Semantic code clone detection is the task of detecting whether two snippets\nof code implement the same functionality (e.g., Sort Array). Recently, many\nneural models achieved near-perfect performance on this task. These models seek\nto make inferences based on their training data. Consequently, they better\ndetect clones similar to those they have seen during training and may struggle\nto detect those they have not. Developers seeking clones are, of course,\ninterested in both types of clones. We confirm this claim through a literature\nreview, identifying three practical clone detection tasks in which the model's\ngoal is to detect clones of a functionality even if it was trained on clones of\ndifferent functionalities. In light of this finding, we re-evaluate six\nstate-of-the-art models, including both task-specific models and generative\nLLMs, on the task of detecting clones of unseen functionality. Our experiments\nreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs\nperform on par with task-specific models without explicit training for clone\ndetection, but generalize better to unseen functionalities, where F1 drops up\nto 5% (average 3%) instead. We propose and evaluate the use of contrastive\nlearning to improve the performance of existing models on clones of unseen\nfunctionality. We draw inspiration from the computer vision and natural\nlanguage processing fields where contrastive learning excels at measuring\nsimilarity between two objects, even if they come from classes unseen during\ntraining. We replace the final classifier of the task-specific models with a\ncontrastive classifier, while for the generative LLMs we propose contrastive\nin-context learning, guiding the LLMs to focus on the differences between\nclones and non-clones. The F1 on clones of unseen functionality is improved by\nup to 26% (average 9%) for task-specific models and up to 5% (average 3%) for\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u529f\u80fd\u514b\u9686\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548cLLMs\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u8005\u5bf9\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u7684\u9700\u6c42\u4e0d\u4ec5\u9650\u4e8e\u6a21\u578b\u8bad\u7ec3\u65f6\u89c1\u8fc7\u7684\u529f\u80fd\u514b\u9686\uff0c\u8fd8\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u529f\u80fd\u514b\u9686\u3002\u73b0\u6709\u7684\u795e\u7ecf\u6a21\u578b\u5728\u68c0\u6d4b\u672a\u89c1\u529f\u80fd\u514b\u9686\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cF1\u5206\u6570\u663e\u8457\u4e0b\u964d\u3002", "method": "\u8bba\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86\u516d\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u751f\u6210\u5f0fLLMs\uff09\u5728\u672a\u89c1\u529f\u80fd\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e9b\u6a21\u578b\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u4f5c\u8005\u66ff\u6362\u4e86\u6700\u7ec8\u5206\u7c7b\u5668\u4e3a\u5bf9\u6bd4\u5206\u7c7b\u5668\uff1b\u5bf9\u4e8e\u751f\u6210\u5f0fLLMs\uff0c\u63d0\u51fa\u4e86\u5bf9\u6bd4\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u5728\u672a\u89c1\u529f\u80fd\u514b\u9686\u4e0a\u7684F1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe48%\uff08\u5e73\u574731%\uff09\uff0c\u800cLLMs\u8868\u73b0\u4e0e\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u76f8\u5f53\u4f46\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0cF1\u5206\u6570\u4ec5\u4e0b\u964d5%\uff08\u5e73\u57473%\uff09\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684F1\u5206\u6570\u63d0\u5347\u4e86\u9ad8\u8fbe26%\uff08\u5e73\u57479%\uff09\uff0cLLMs\u63d0\u5347\u4e865%\uff08\u5e73\u57473%\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u6539\u8fdb\u73b0\u6709\u6a21\u578b\u5bf9\u672a\u89c1\u529f\u80fd\u514b\u9686\u68c0\u6d4b\u6027\u80fd\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u751f\u6210\u5f0fLLMs\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u6bd4\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u529f\u80fd\u514b\u9686\u4e0a\u7684F1\u5206\u6570\u3002"}}
{"id": "2510.03895", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03895", "abs": "https://arxiv.org/abs/2510.03895", "authors": ["Zheng Huang", "Mingyu Liu", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Xiaoman Li", "Yiduo Jia", "Hao Zhong", "Hao Chen", "Chunhua Shen"], "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied\nintelligence, yet they confront critical barriers to real-world deployment,\nmost notably catastrophic forgetting. This issue stems from their overreliance\non continuous action sequences or action chunks, which inadvertently create\nisolated data silos that disrupt knowledge retention across tasks. To tackle\nthese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)\nframework: a novel approach that narrows its focus to sparse trajectories,\nthereby avoiding the catastrophic forgetting associated with dense trajectory\nfine-tuning. A key innovation of NoTVLA lies in its trajectory planning\nstrategy: instead of centering on the target object's trajectory, it leverages\ntemporal compression and spatial reasoning pruning specifically for the robot\nend effector's trajectory. Furthermore, training is conducted using these\nsparse trajectories rather than dense action trajectories, an optimization that\ndelivers remarkable practical advantages with better performance in zero-shot.\nIn multi-task evaluation scenarios, NoTVLA achieves superior performance and\ngeneralization compared to pi0 while operating under two critical constraints:\nit uses over an order of magnitude less computing power than pi0 and requires\nno wrist-mounted camera. This design ensures that NoTVLA's operational accuracy\nclosely approximates that of single-task expert models. Crucially, it also\npreserves the model's inherent language capabilities, enabling zero-shot\ngeneralization in specific scenarios, supporting unified model deployment\nacross multiple robot platforms, and fostering a degree of generalization even\nwhen perceiving tasks from novel perspectives.", "AI": {"tldr": "NoTVLA\u901a\u8fc7\u7a00\u758f\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3VLA\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\u56e0\u4f9d\u8d56\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u6216\u52a8\u4f5c\u5757\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u907f\u514d\u6570\u636e\u5b64\u5c9b\u5bf9\u77e5\u8bc6\u4fdd\u7559\u7684\u5e72\u6270\u3002", "method": "\u91c7\u7528\u7a00\u758f\u8f68\u8ff9\u800c\u975e\u5bc6\u96c6\u52a8\u4f5c\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u65f6\u95f4\u538b\u7f29\u548c\u7a7a\u95f4\u63a8\u7406\u526a\u679d\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728\u96f6\u6837\u672c\u548c\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8epi0\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u5e45\u51cf\u5c11\uff0c\u4e14\u65e0\u9700\u8155\u90e8\u6444\u50cf\u5934\uff0c\u64cd\u4f5c\u7cbe\u5ea6\u63a5\u8fd1\u5355\u4efb\u52a1\u4e13\u5bb6\u6a21\u578b\u3002", "conclusion": "NoTVLA\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u8a00\u80fd\u529b\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2510.03863", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI.", "AI": {"tldr": "Spatial CAPTCHA \u662f\u4e00\u79cd\u65b0\u578b\u4eba\u7c7b\u9a8c\u8bc1\u6846\u67b6\uff0c\u5229\u7528\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5dee\u5f02\u62b5\u5fa1AI\u653b\u51fb\uff0c\u8bc1\u660e\u5176\u6bd4\u4f20\u7edfCAPTCHA\u66f4\u6709\u6548\u3002", "motivation": "\u4f20\u7edfCAPTCHA\u4f9d\u8d56\u4e8e\u6587\u672c\u8bc6\u522b\u62162D\u56fe\u50cf\u7406\u89e3\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u8fd9\u4e9b\u8bbe\u8ba1\u9010\u6e10\u5931\u6548\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9a8c\u8bc1\u6846\u67b6\u6765\u586b\u8865\u8fd9\u4e00\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "Spatial CAPTCHA \u91c7\u7528\u7a0b\u5e8f\u5316\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u7ea6\u675f\u96be\u5ea6\u63a7\u5236\u3001\u81ea\u52a8\u6b63\u786e\u6027\u9a8c\u8bc1\u548c\u4eba\u7c7b\u53c2\u4e0e\u9a8c\u8bc1\uff0c\u786e\u4fdd\u5176\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5728Spatial-CAPTCHA-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4eba\u7c7b\u8868\u73b0\u8fdc\u8d8510\u79cd\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u4f73\u6a21\u578b\u7684Pass@1\u51c6\u786e\u7387\u4ec5\u4e3a31.0%\u3002\u4e0eGoogle reCAPTCHA\u7684\u5bf9\u6bd4\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u5b89\u5168\u673a\u5236\u548cAI\u7a7a\u95f4\u63a8\u7406\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "Spatial CAPTCHA \u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u7684\u4eba\u7c7b\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u6839\u672c\u5dee\u5f02\uff0c\u6709\u6548\u62b5\u5fa1\u4e86\u73b0\u4ee3AI\u7684\u81ea\u52a8\u5316\u653b\u51fb\u3002"}}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u8bed\u8a00\u7684\u7edf\u4e00\u8bed\u6cd5\u9ad8\u4eae\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u521b\u65b0\u6280\u672f\u964d\u4f4e\u4e86\u90e8\u7f72\u590d\u6742\u6027\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u548c\u57fa\u4e8eWeb\u7684\u5f00\u53d1\u5de5\u5177\u5728\u5b9e\u65f6\u8bed\u6cd5\u9ad8\u4eae\u65b9\u9762\u9762\u4e34\u4e25\u683c\u7684\u65f6\u95f4\u548c\u5185\u5b58\u9650\u5236\uff0c\u73b0\u6709\u6a21\u578b\u5b58\u5728\u8bed\u8a00\u652f\u6301\u5355\u4e00\u3001\u6570\u636e\u96c6\u9700\u6c42\u5927\u3001\u8bad\u7ec3\u8d44\u6e90\u5bc6\u96c6\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7Deep Abstraction\u8fc7\u7a0b\u5c06\u66b4\u529b\u8bed\u6cd5\u9ad8\u4eae\u89e3\u6790\u5668\u7684\u884c\u4e3a\u7f16\u7801\u4e3a\u5feb\u901f\u7684\u7edf\u8ba1\u6a21\u578b\u3002\u7ed3\u5408\u5f52\u4e00\u5316\u6280\u672f\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\uff0c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u548c\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u7edf\u4e00\u6a21\u578b\u5c06\u90e8\u7f72\u590d\u6742\u6027\u964d\u4f4e\u4e86\u516d\u500d\uff0c\u5e76\u5728\u672a\u89c1\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002\u5f52\u4e00\u5316\u6280\u672f\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u7edf\u4e00\u6a21\u578b\uff0c\u80fd\u591f\u652f\u6301\u591a\u8fbe\u516d\u79cd\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\u7684\u8bed\u6cd5\u9ad8\u4eae\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u590d\u6742\u6027\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u521b\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u5bf9\u66b4\u529b\u751f\u6210\u5668\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.03910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03910", "abs": "https://arxiv.org/abs/2510.03910", "authors": ["Akhil Padmanabha", "Jessie Yuan", "Tanisha Mehta", "Rajat Kumar Jenamani", "Eric Hu", "Victoria de Le\u00f3n", "Anthony Wertz", "Janavi Gupta", "Ben Dodson", "Yunting Yan", "Carmel Majidi", "Tapomayukh Bhattacharjee", "Zackory Erickson"], "title": "WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding", "comment": null, "summary": "Millions of people around the world need assistance with feeding. Robotic\nfeeding systems offer the potential to enhance autonomy and quality of life for\nindividuals with impairments and reduce caregiver workload. However, their\nwidespread adoption has been limited by technical challenges such as estimating\nbite timing, the appropriate moment for the robot to transfer food to a user's\nmouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with\nLEarned bite timing, a system that accurately predicts bite timing by\nleveraging wearable sensor data to be highly reactive to natural user cues such\nas head movements, chewing, and talking. We train a supervised regression model\non bite timing data from 14 participants and incorporate a user-adjustable\nassertiveness threshold to convert predictions into proceed or stop commands.\nIn a study with 15 participants without motor impairments with the Obi feeding\nrobot, WAFFLE performs statistically on par with or better than baseline\nmethods across measures of feeling of control, robot understanding, and\nworkload, and is preferred by the majority of participants for both individual\nand social dining. We further demonstrate WAFFLE's generalizability in a study\nwith 2 participants with motor impairments in their home environments using a\nKinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling\nnatural, reactive bite timing that generalizes across users, robot hardware,\nrobot positioning, feeding trajectories, foods, and both individual and social\ndining contexts.", "AI": {"tldr": "WAFFLE\u901a\u8fc7\u7a7f\u6234\u5f0f\u4f20\u611f\u5668\u9884\u6d4b\u54ac\u5408\u65f6\u673a\uff0c\u63d0\u5347\u5582\u98df\u673a\u5668\u4eba\u53cd\u5e94\u6027\uff0c\u7528\u6237\u4f53\u9a8c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7528\u6237\u548c\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8f85\u52a9\u5582\u98df\u7cfb\u7edf\u4e2d\u54ac\u5408\u65f6\u673a\u9884\u6d4b\u7684\u6280\u672f\u6311\u6218\uff0c\u4ee5\u589e\u5f3a\u6b8b\u969c\u4eba\u58eb\u7684\u81ea\u4e3b\u6027\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u51cf\u8f7b\u62a4\u7406\u4eba\u5458\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u56de\u5f52\u6a21\u578b\u8bad\u7ec314\u540d\u53c2\u4e0e\u8005\u7684\u54ac\u5408\u65f6\u673a\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u53ef\u8c03\u8282\u7684\u81ea\u4fe1\u9608\u503c\u751f\u6210\u524d\u8fdb\u6216\u505c\u6b62\u6307\u4ee4\u3002", "result": "\u572815\u540d\u65e0\u8fd0\u52a8\u969c\u788d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u4e2d\uff0cWAFFLE\u5728\u63a7\u5236\u611f\u3001\u673a\u5668\u4eba\u7406\u89e3\u548c\u5de5\u4f5c\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u793e\u4ea4\u7528\u9910\u4e2d\u66f4\u53d7\u9752\u7750\uff1b\u57282\u540d\u8fd0\u52a8\u969c\u788d\u60a3\u8005\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "WAFFLE\u7cfb\u7edf\u901a\u8fc7\u7a7f\u6234\u5f0f\u4f20\u611f\u5668\u6570\u636e\u51c6\u786e\u9884\u6d4b\u54ac\u5408\u65f6\u673a\uff0c\u652f\u6301\u8de8\u7528\u6237\u3001\u673a\u5668\u4eba\u786c\u4ef6\u548c\u73af\u5883\u7684\u81ea\u7136\u53cd\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u673a\u5668\u4eba\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.03886", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface.", "AI": {"tldr": "MM-DiTs\u901a\u8fc7\u6269\u5c55\u6587\u672c\u5d4c\u5165\u8868\u793a\u8303\u56f4\uff0c\u63d0\u5347\u7f55\u89c1\u8bed\u4e49\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u6a21\u5757\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiTs\uff09\u5728\u5904\u7406\u7f55\u89c1\u6216\u521b\u610f\u63d0\u793a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6982\u5ff5\u5728\u9884\u8bad\u7ec3\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b9\u5dee\u653e\u5927\u5728\u8054\u5408\u6ce8\u610f\u529b\u5757\u524d\u6269\u5c55\u6587\u672c\u6807\u8bb0\u5d4c\u5165\u7684\u8868\u793a\u8303\u56f4\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u6587\u672c\u5230\u89c6\u9891\u53ca\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u751f\u6210\u7f55\u89c1\u8bed\u4e49\u3002", "conclusion": "\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u6269\u5c55\u6587\u672c\u6807\u8bb0\u5d4c\u5165\u7684\u8868\u793a\u8303\u56f4\uff0cMM-DiTs\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6b65\u9aa4\u6216\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u7f55\u89c1\u8bed\u4e49\u7684\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2510.04274", "categories": ["cs.SE", "D.2; I.2; J.6; K.3; K.7"], "pdf": "https://arxiv.org/pdf/2510.04274", "abs": "https://arxiv.org/abs/2510.04274", "authors": ["Damjan Fujs", "Damjan Vavpoti\u010d", "Toma\u017e Hovelja", "Marko Po\u017eenel"], "title": "Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience", "comment": "5 pages, 1 figure, 2 tables, presented at IARIA CYBER 2025", "summary": "This study investigates how access to Large Language Models (LLMs) and\nvarying levels of professional software development experience affect the\nprioritization of cybersecurity requirements for web applications. Twenty-three\npostgraduate students participated in a research study to prioritize security\nrequirements (SRs) using the MoSCoW method and subsequently rated their\nproposed solutions against multiple evaluation criteria. We divided\nparticipants into two groups (one with and the other without access to LLM\nsupport during the task). Results showed no significant differences related to\nLLM use, suggesting that access to LLMs did not noticeably influence how\nparticipants evaluated cybersecurity solutions. However, statistically\nsignificant differences emerged between experience groups for certain criteria,\nsuch as estimated cost to develop a feature, perceived impact on user\nexperience, and risk assessment related to non-implementation of the proposed\nfeature. Participants with more professional experience tended to provide\nhigher ratings for user experience impact and lower risk estimates.", "AI": {"tldr": "LLM\u652f\u6301\u672a\u663e\u8457\u5f71\u54cd\u7f51\u7edc\u5b89\u5168\u9700\u6c42\u8bc4\u4f30\uff0c\u4f46\u4e13\u4e1a\u7ecf\u9a8c\u5728\u591a\u4e2a\u6807\u51c6\u4e0a\u9020\u6210\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u8bbf\u95eeLLMs\u548c\u4e0d\u540c\u4e13\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u7ecf\u9a8c\u5982\u4f55\u5f71\u54cd\u5bf9\u7f51\u7edc\u5b89\u5168\u9700\u6c42\u7684\u4f18\u5148\u6392\u5e8f\u3002", "method": "23\u540d\u7814\u7a76\u751f\u53c2\u4e0e\u7814\u7a76\uff0c\u4f7f\u7528MoSCoW\u65b9\u6cd5\u4f18\u5148\u6392\u5e8f\u5b89\u5168\u9700\u6c42\uff0c\u5e76\u6839\u636e\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u5bf9\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u8bc4\u5206\u3002\u53c2\u4e0e\u8005\u88ab\u5206\u4e3a\u4e24\u7ec4\uff0c\u4e00\u7ec4\u5728\u4efb\u52a1\u671f\u95f4\u6709LLM\u652f\u6301\uff0c\u53e6\u4e00\u7ec4\u6ca1\u6709\u3002", "result": "LLM\u4f7f\u7528\u672a\u663e\u793a\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u7ecf\u9a8c\u7ec4\u5728\u5f00\u53d1\u6210\u672c\u3001\u7528\u6237\u4f53\u9a8c\u5f71\u54cd\u548c\u98ce\u9669\u8bc4\u4f30\u7b49\u6807\u51c6\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u7ecf\u9a8c\u66f4\u4e30\u5bcc\u7684\u53c2\u4e0e\u8005\u5bf9\u7528\u6237\u4f53\u9a8c\u5f71\u54cd\u8bc4\u5206\u66f4\u9ad8\uff0c\u5bf9\u98ce\u9669\u4f30\u8ba1\u66f4\u4f4e\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bbf\u95ee\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u53c2\u4e0e\u8005\u8bc4\u4f30\u7f51\u7edc\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u7684\u65b9\u5f0f\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u4e13\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u7ecf\u9a8c\u7684\u4e0d\u540c\u5728\u591a\u4e2a\u6807\u51c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2510.03919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03919", "abs": "https://arxiv.org/abs/2510.03919", "authors": ["Matthew Lisondra", "Junseo Kim", "Glenn Takashi Shimoda", "Kourosh Zareinia", "Sajad Saeedi"], "title": "TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry", "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Vision algorithms can be executed directly on the image sensor when\nimplemented on the next-generation sensors known as focal-plane\nsensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs\ngreatly improve latency, reducing the problems associated with the bottleneck\nof data transfer from a vision sensor to a processor. FPSPs accelerate\nvision-based algorithms such as visual-inertial odometry (VIO). However, VIO\nframeworks suffer from spatial drift due to the vision-based pose estimation,\nwhilst temporal drift arises from the inertial measurements. FPSPs circumvent\nthe spatial drift by operating at a high frame rate to match the high-frequency\noutput of the inertial measurements. In this paper, we present TCB-VIO, a\ntightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman\nFilter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU\nmeasurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:\nROVIO, VINS-Mono, and ORB-SLAM3.", "AI": {"tldr": "TCB-VIO\u662f\u4e00\u79cd\u9ad8\u5e27\u7387\u3001\u7d27\u5bc6\u8026\u5408\u76846\u81ea\u7531\u5ea6VIO\u6846\u67b6\uff0c\u901a\u8fc7MSCKF\u51cf\u5c11\u6f02\u79fb\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u6846\u67b6\u4e2d\u7531\u4e8e\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u548c\u60ef\u6027\u6d4b\u91cf\u5bfc\u81f4\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u72b6\u6001\u7ea6\u675f\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08MSCKF\uff09\uff0c\u4ee5250 FPS\u7684\u9ad8\u5e27\u7387\u548c400 Hz\u7684IMU\u6d4b\u91cf\u9891\u7387\u8fd0\u884c\u3002", "result": "TCB-VIO\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eROVIO\u3001VINS-Mono\u548cORB-SLAM3\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TCB-VIO\u901a\u8fc7\u9ad8\u5e27\u7387\u8fd0\u884c\u548c\u7d27\u5bc6\u8026\u5408\u76846\u81ea\u7531\u5ea6VIO\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u6f02\u79fb\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03892", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI.", "AI": {"tldr": "\u6e38\u620f\u5316\u53ef\u89e3\u91caAI\u7cfb\u7edf\u5e2e\u52a9\u6d88\u8d39\u8005\u5728\u5496\u5561\u8d2d\u4e70\u4e2d\u505a\u51fa\u9053\u5fb7\u51b3\u7b56\uff0c\u7ed3\u5408\u5eb7\u5fb7\u4e3b\u4e49\u548c\u529f\u5229\u4e3b\u4e49\u6a21\u5757\u63d0\u4f9b\u5b9e\u65f6\u89e3\u91ca\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6e38\u620f\u5316\u65b9\u5f0f\u63d0\u5347\u6d88\u8d39\u8005\u5728\u5496\u5561\u8d2d\u4e70\u4e2d\u7684\u9053\u5fb7\u51b3\u7b56\u610f\u8bc6\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u51b3\u7b56\u89e3\u91ca\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u516d\u4e2a\u56de\u5408\uff0c\u6bcf\u56de\u5408\u63d0\u4f9b\u4e09\u4e2a\u9009\u9879\u3002\u4f7f\u7528\u4e24\u4e2a\u7b26\u53f7\u5f15\u64ce\uff1a\u5eb7\u5fb7\u4e3b\u4e49\u6a21\u5757\u68c0\u6d4b\u89c4\u5219\u8fdd\u53cd\uff08\u5982\u7ae5\u5de5\u3001\u65e0\u906e\u836b\u8ba4\u8bc1\u7684\u68ee\u6797\u780d\u4f10\u98ce\u9669\u7b49\uff09\uff0c\u529f\u5229\u4e3b\u4e49\u6a21\u5757\u901a\u8fc7\u591a\u6807\u51c6\u805a\u5408\u5bf9\u9009\u9879\u8bc4\u5206\u3002\u5143\u89e3\u91ca\u5668\u901a\u8fc7\u9057\u61be\u754c\u9650\uff080.2\uff09\u7a81\u51fa\u5eb7\u5fb7\u4e3b\u4e49\u4e0e\u529f\u5229\u4e3b\u4e49\u7684\uff08\u4e0d\uff09\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u798f\u5229\u635f\u5931\u8f83\u5c0f\u65f6\u5207\u6362\u5230\u9053\u5fb7\u6e05\u6d01\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u9009\u9879\u3002", "result": "\u7cfb\u7edf\u53d1\u5e03\u4e86\u7ed3\u6784\u5316\u914d\u7f6e\uff08\u5c5e\u6027\u6a21\u5f0f\u3001\u8ba4\u8bc1\u6620\u5c04\u3001\u6743\u91cd\u3001\u89c4\u5219\u96c6\uff09\u3001\u53ef\u5ba1\u8ba1\u7684\u7b56\u7565\u8ffd\u8e2a\u548c\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e38\u620f\u5316\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5496\u5561\u6d88\u8d39\u9886\u57df\u8fdb\u884c\u9053\u5fb7\u51b3\u7b56\u3002\u901a\u8fc7\u7ed3\u5408\u5eb7\u5fb7\u4e3b\u4e49\u548c\u529f\u5229\u4e3b\u4e49\u6a21\u5757\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b9e\u65f6\u51b3\u7b56\u89e3\u91ca\u548c\u9053\u5fb7\u6743\u8861\u3002"}}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ade\u8d5b\u5f62\u5f0f\u8bc4\u4f30\u4e86\u4f18\u5316\u4e0a\u4e0b\u6587\u6536\u96c6\u673a\u5236\u5bf9\u4ee3\u7801\u8865\u5168\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5728Python\u548cKotlin\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "motivation": "\u968f\u7740AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5229\u7528\u9879\u76ee\u6574\u4f53\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u53c2\u4e0e\u8005\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u673a\u5236\uff0c\u5229\u7528\u6765\u81ea\u5f00\u6e90\u9879\u76ee\u7684\u771f\u5b9e\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u901a\u8fc7chrF\u6307\u6807\u8bc4\u4f30\u591a\u4e2a\u6700\u5148\u8fdb\u795e\u7ecf\u6a21\u578b\u7684\u8865\u5168\u8d28\u91cf\u3002", "result": "\u516c\u5171\u9636\u6bb5\u670919\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Python\u89e3\u51b3\u65b9\u6848\uff0c8\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Kotlin\u89e3\u51b3\u65b9\u6848\uff1b\u79c1\u6709\u9636\u6bb5\u67096\u4e2a\u56e2\u961f\u7ade\u4e89\uff0c\u5176\u4e2d5\u4e2a\u63d0\u4ea4\u4e86\u8bba\u6587\u3002", "conclusion": "\u6bd4\u8d5b\u5c55\u793a\u4e86\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u4f18\u5316\u4e0a\u4e0b\u6587\u6536\u96c6\u673a\u5236\u7684\u6f5c\u529b\uff0c\u53c2\u4e0e\u8005\u5f00\u53d1\u7684\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u586b\u5145\u4e2d\u95f4\u4ee3\u7801\u8865\u5168\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728Python\u548cKotlin\u4e2d\u3002"}}
{"id": "2510.03948", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03948", "abs": "https://arxiv.org/abs/2510.03948", "authors": ["Otobong Jerome", "Geesara Prathap Kulathunga", "Devitt Dmitry", "Eugene Murawjow", "Alexandr Klimchik"], "title": "A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM", "comment": null, "summary": "Off-road environments present unique challenges for autonomous navigation due\nto their complex and unstructured nature. Traditional global path-planning\nmethods, which typically aim to minimize path length and travel time, perform\npoorly on large-scale maps and fail to account for critical factors such as\nreal-time performance, kinematic feasibility, and memory efficiency. This paper\nintroduces a novel global path-planning method specifically designed for\noff-road environments, addressing these essential factors. The method begins by\nconstructing an intermediate map within the pixel coordinate system,\nincorporating geographical features like off-road trails, waterways, restricted\nand passable areas, and trees. The planning problem is then divided into three\nsub-problems: graph-based path planning, kinematic feasibility checking, and\npath smoothing. This approach effectively meets real-time performance\nrequirements while ensuring kinematic feasibility and efficient memory use. The\nmethod was tested in various off-road environments with large-scale maps up to\nseveral square kilometers in size, successfully identifying feasible paths in\nan average of 1.5 seconds and utilizing approximately 1.5GB of memory under\nextreme conditions. The proposed framework is versatile and applicable to a\nwide range of off-road autonomous navigation tasks, including search and rescue\nmissions and agricultural operations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u8d8a\u91ce\u73af\u5883\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\uff0c\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u8d8a\u91ce\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u7b49\u5173\u952e\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u50cf\u7d20\u5750\u6807\u7cfb\u4e2d\u7684\u4e2d\u95f4\u5730\u56fe\uff0c\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u57fa\u4e8e\u56fe\u7684\u8def\u5f84\u89c4\u5212\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u68c0\u67e5\u548c\u8def\u5f84\u5e73\u6ed1\u4e09\u4e2a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cd\u8d8a\u91ce\u73af\u5883\u548c\u5927\u89c4\u6a21\u5730\u56fe\uff08\u8fbe\u6570\u5e73\u65b9\u516c\u91cc\uff09\u4e2d\u6d4b\u8bd5\uff0c\u5e73\u57471.5\u79d2\u5185\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u6781\u7aef\u6761\u4ef6\u4e0b\u5185\u5b58\u5360\u7528\u7ea61.5GB\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u8d8a\u91ce\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2510.03969", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs.", "AI": {"tldr": "QRLLM\u662f\u4e00\u4e2a\u7edf\u8ba1\u8ba4\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u63ed\u793a\u524d\u6cbf\u6a21\u578b\u5b58\u5728\u9ad8\u8fbe70%\u7684\u98ce\u9669\uff0c\u547c\u5401\u6539\u8fdb\u5b89\u5168\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u4f9d\u8d56\u56fa\u5b9a\u653b\u51fb\u63d0\u793a\u5e8f\u5217\u3001\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u591a\u8f6e\u5bf9\u8bdd\u7684\u5e7f\u9614\u7a7a\u95f4\uff0c\u96be\u4ee5\u5168\u9762\u63ed\u793aLLM\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u707e\u96be\u6027\u54cd\u5e94\u98ce\u9669\u3002", "method": "QRLLM\u5c06\u591a\u8f6e\u5bf9\u8bdd\u5efa\u6a21\u4e3a\u67e5\u8be2\u5e8f\u5217\u7684\u6982\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u5728\u67e5\u8be2\u56fe\u4e0a\u8868\u793a\uff0c\u5e76\u5229\u7528\u7f6e\u4fe1\u533a\u95f4\u91cf\u5316\u707e\u96be\u6027\u98ce\u9669\u3002\u5b9a\u4e49\u4e86\u968f\u673a\u8282\u70b9\u3001\u56fe\u8def\u5f84\u3001\u5e26\u62d2\u7edd\u7684\u81ea\u9002\u5e94\u7b49\u5b9e\u7528\u5206\u5e03\u3002", "result": "QRLLM\u80fd\u591f\u63ed\u793a\u524d\u6cbf\u6a21\u578b\u4e2d\u663e\u8457\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u6700\u5dee\u6a21\u578b\u7684\u8ba4\u8bc1\u4e0b\u754c\u9ad8\u8fbe70%\u3002", "conclusion": "QRLLM\u6846\u67b6\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684LLM\u707e\u96be\u6027\u98ce\u9669\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\u7684\u6982\u7387\u8fb9\u754c\uff0c\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u4e2d\u9ad8\u8fbe70%\u7684\u8ba4\u8bc1\u4e0b\u754c\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u5b89\u5168\u8bad\u7ec3\u7b56\u7565\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "AI": {"tldr": "MacroBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u5408\u6210\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u7a0b\u5e8f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u5b8c\u5168\u5931\u8d25\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u5408\u6210\u53ef\u91cd\u7528\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u7a0b\u5e8f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002", "method": "MacroBench\u5b9e\u4f8b\u5316\u4e86\u4e03\u4e2a\u81ea\u6258\u7ba1\u7ad9\u70b9\uff0c\u8986\u76d6681\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u9759\u6001\u68c0\u67e5\u3001\u6c99\u76d2\u6267\u884c\u548c\u7ed3\u679c\u9a8c\u8bc1\uff08\u5305\u62ecDOM\u65ad\u8a00\u548c\u6570\u636e\u5e93\u5feb\u7167\uff09\u6765\u9a8c\u8bc1\u751f\u6210\u7684\u4ee3\u7801\u3002", "result": "GPT-4o-Mini\u6210\u529f\u7387\u4e3a96.8%\uff0cGPT-4.1\u4e3a95.3%\uff0cGemini-2.5-Pro\u4e3a89.0%\uff0cDeepSeek-V3.1\u4e3a83.4%\u3002\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e0a\u5b8c\u5168\u5931\u8d25\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86MacroBench\uff0c\u4e00\u4e2a\u8bc4\u4f30LLMs\u4ece\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5408\u6210\u53ef\u91cd\u7528\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u7a0b\u5e8f\u80fd\u529b\u7684\u4ee3\u7801\u4f18\u5148\u57fa\u51c6\u6d4b\u8bd5\u3002\u5c3d\u7ba1\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0891.7%\u6210\u529f\u7387\uff09\uff0c\u4f46\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e0a\u5b8c\u5168\u5931\u8d25\uff080.0%\u6210\u529f\u7387\uff09\uff0c\u4e14\u5747\u672a\u8fbe\u5230\u751f\u4ea7\u7ea7\u7f16\u7801\u5b9e\u8df5\u3002"}}
{"id": "2510.04041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04041", "abs": "https://arxiv.org/abs/2510.04041", "authors": ["Ayudh Saxena", "Harsh Shah", "Sandeep Routray", "Rishi Rajesh Shah", "Esha Pahwa"], "title": "SITCOM: Scaling Inference-Time COMpute for VLAs", "comment": "Accepted at the NeurIPS 2025 Workshop on Space in Vision, Language,\n  and Embodied AI (SpaVLE). *Equal contribution", "summary": "Learning robust robotic control policies remains a major challenge due to the\nhigh cost of collecting labeled data, limited generalization to unseen\nenvironments, and difficulties in planning over long horizons. While\nVision-Language-Action (VLA) models offer a promising solution by grounding\nnatural language instructions into single-step control commands, they often\nlack mechanisms for lookahead and struggle with compounding errors in dynamic\ntasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs\n(SITCOM), a framework that augments any pretrained VLA with model-based\nrollouts and reward-based trajectory selection, inspired by Model Predictive\nControl algorithm. SITCOM leverages a learned dynamics model to simulate\nmulti-step action rollouts to select the best candidate plan for real-world\nexecution, transforming one-shot VLAs into robust long-horizon planners. We\ndevelop an efficient transformer-based dynamics model trained on large-scale\nBridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim\ngap, and score candidate rollouts using rewards from simulator. Through\ncomprehensive evaluation across multiple tasks and settings in the SIMPLER\nenvironment, we demonstrate that SITCOM when combined with a good reward\nfunction can significantly improve task completion rate from 48% to 72% using\ntrained dynamics model.", "AI": {"tldr": "SITCOM\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u6eda\u52a8\u4f18\u5316\u589e\u5f3aVLA\u6a21\u578b\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u524d\u77bb\u6027\u548c\u9519\u8bef\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u63d0\u5347\u957f\u671f\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165SITCOM\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u6eda\u52a8\u4f18\u5316\u548c\u5956\u52b1\u8f68\u8ff9\u9009\u62e9\uff0c\u4f7f\u7528Transformer\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u52a8\u4f5c\u6a21\u62df\u3002", "result": "\u5728SIMPLER\u73af\u5883\u4e2d\uff0cSITCOM\u7ed3\u5408\u826f\u597d\u5956\u52b1\u51fd\u6570\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece48%\u63d0\u5347\u81f372%\u3002", "conclusion": "SITCOM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u6eda\u52a8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4efb\u52a1\u89c4\u5212\u7684\u9c81\u68d2\u6027\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4ece48%\u63d0\u9ad8\u523072%\u3002"}}
{"id": "2510.04009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86C^2-Eval\uff0c\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u521b\u9020\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7Usefulness\u3001Originality\u548cSurprise\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u521b\u9020\u6027\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u521b\u9020\u6027\u7684\u6846\u67b6\u96f6\u6563\u4e14\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cf\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\u7684\u521b\u9020\u6027\u3002", "method": "\u5f15\u5165\u4e86C^2-Eval\uff0c\u4e00\u4e2a\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7684\u7406\u8bba\uff0c\u901a\u8fc7Usefulness\u3001Originality\u548cSurprise\uff08U-O-S\uff09\u6807\u51c6\u8bc4\u4f30\u521b\u9020\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u7684\u521b\u9020\u6027\u80fd\u529b\uff0c\u5c55\u793a\u4e86C^2-Eval\u5728\u8bc4\u4f30\u521b\u9020\u6027AI\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "C^2-Eval\u88ab\u8bc1\u660e\u662f\u8bc4\u4f30\u521b\u9020\u6027AI\u7684\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u8ffd\u6c42\u521b\u9020\u6027\u673a\u5668\u667a\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002"}}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko M\u00e4kitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development.", "AI": {"tldr": "AI\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u53ef\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4f26\u7406\u548c\u900f\u660e\u5ea6\u7b49\u6311\u6218\uff0c\u9700\u52a0\u5f3a\u5b66\u672f\u4e0e\u884c\u4e1a\u5408\u4f5c\u4ee5\u521b\u5efa\u53ef\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1RE\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u5982\u6a21\u7cca\u6027\u3001\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u51b2\u7a81\u7b49\u6311\u6218\uff0cAI\u88ab\u8ba4\u4e3a\u6709\u6f5c\u529b\u63d0\u5347RE\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63a2\u8ba8\u4e86AI\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u52b3\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\u3001\u652f\u6301\u9700\u6c42\u4f18\u5148\u7ea7\u6392\u5e8f\u4ee5\u53ca\u4fc3\u8fdb\u5229\u76ca\u76f8\u5173\u8005\u4e0eAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u534f\u4f5c\u6765\u589e\u5f3a\u4f20\u7edfRE\u5b9e\u8df5\u3002", "result": "\u8bba\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86AI\u4e3aRE\u5e26\u6765\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u5305\u62ec\u81ea\u52a8\u5316\u3001\u9700\u6c42\u4f18\u5148\u7ea7\u652f\u6301\u548c\u534f\u4f5c\u4fc3\u8fdb\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u4f26\u7406\u3001\u504f\u89c1\u548c\u900f\u660e\u5ea6\u7b49\u65b0\u95ee\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u5e94\u7528AI\u65f6\u9700\u6ce8\u91cd\u4f26\u7406\u5b9e\u8df5\uff0c\u5e76\u52a0\u5f3a\u5b66\u672f\u754c\u4e0e\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u7684\u5408\u4f5c\uff0c\u65e8\u5728\u521b\u5efa\u65e2\u5f3a\u5927\u53c8\u53ef\u4fe1\u8d56\u4e14\u5b9e\u7528\u7684AI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04074", "abs": "https://arxiv.org/abs/2510.04074", "authors": ["Chung-Pang Wang", "Changwei Chen", "Xiao Liang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback", "comment": null, "summary": "Autonomous surgical systems must adapt to highly dynamic environments where\ntissue properties and visual cues evolve rapidly. Central to such adaptability\nis feedback: the ability to sense, interpret, and respond to changes during\nexecution. While feedback mechanisms have been explored in surgical robotics,\nranging from tool and tissue tracking to error detection, existing methods\nremain limited in handling the topological and perceptual challenges of tissue\ndissection. In this work, we propose a feedback-enabled framework for\nautonomous tissue dissection that explicitly reasons about topological changes\nfrom endoscopic images after each dissection action. This structured feedback\nguides subsequent actions, enabling the system to localize dissection progress\nand adapt policies online. To improve the reliability of such feedback, we\nintroduce visibility metrics that quantify tissue exposure and formulate\noptimal controller designs that actively manipulate tissue to maximize\nvisibility. Finally, we integrate these feedback mechanisms with both\nplanning-based and learning-based dissection methods, and demonstrate\nexperimentally that they significantly enhance autonomy, reduce errors, and\nimprove robustness in complex surgical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u7684\u81ea\u4e3b\u7ec4\u7ec7\u5207\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89c1\u6027\u6307\u6807\u548c\u6700\u4f18\u63a7\u5236\u5668\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u672f\u673a\u5668\u4eba\u53cd\u9988\u673a\u5236\u5728\u5904\u7406\u7ec4\u7ec7\u5207\u5272\u7684\u62d3\u6251\u548c\u611f\u77e5\u6311\u6218\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u9002\u5e94\u7ec4\u7ec7\u7279\u6027\u548c\u89c6\u89c9\u7ebf\u7d22\u53d8\u5316\u7684\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u53cd\u9988\u673a\u5236\uff0c\u5305\u62ec\u80fd\u611f\u77e5\u62d3\u6251\u53d8\u5316\u7684\u53ef\u89c1\u6027\u6307\u6807\u548c\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u4ee5\u4e3b\u52a8\u8c03\u6574\u7ec4\u7ec7\u4f4d\u7f6e\u6700\u5927\u5316\u53ef\u89c1\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u673a\u5236\u4e0e\u89c4\u5212\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u5207\u5272\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u6027\u3001\u51cf\u5c11\u4e86\u9519\u8bef\uff0c\u5e76\u5728\u590d\u6742\u624b\u672f\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cd\u9988\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u7ec4\u7ec7\u5207\u5272\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u9002\u5e94\u6027\u3001\u51cf\u5c11\u4e86\u9519\u8bef\u5e76\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u624b\u672f\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5929\u6c14\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u6846\u67b6Zephyrus\uff0c\u5728\u57fa\u7840\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u7eaf\u6587\u672c\u65b9\u6cd5\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u5929\u6c14\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aZephyrusWorld\u7684Python\u4ee3\u7801\u73af\u5883\uff0c\u96c6\u6210\u4e86WeatherBench 2\u6570\u636e\u96c6\u63a5\u53e3\u3001\u5730\u7406\u67e5\u8be2\u3001\u5929\u6c14\u9884\u62a5\u548c\u6c14\u5019\u6a21\u62df\u5de5\u5177\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u8f6eLLM\u4ee3\u7406Zephyrus\u3002", "result": "Zephyrus\u5728ZephyrusBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6b63\u786e\u7387\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u9ad835\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "Zephyrus\u6846\u67b6\u5728\u5929\u6c14\u79d1\u5b66\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u57fa\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.04437", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04437", "abs": "https://arxiv.org/abs/2510.04437", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "comment": null, "summary": "Against the backdrop of deepening digital and intelligent transformation in\nhuman resource management, traditional recruitment models struggle to fully\nmeet enterprises' growing demand for precise talent acquisition due to limited\nefficiency, high costs, and information asymmetry. As a vital tool for\noptimizing recruitment processes, reducing labor and time costs, and enhancing\ncore competitiveness, intelligent recruitment management systems become an\nindispensable component of modern organizational talent strategies.Compared\nwith the labor intensive tasks of resume screening, candidate position\nmatching, and interview coordination in traditional manual recruitment,\nintelligent recruitment systems significantly enhance the efficiency and\naccuracy of the hiring process through automation and data driven approaches.\nThese systems enable rapid parsing of massive resume volumes, intelligent\nmatching of candidates to positions, and automated scheduling of interview\nprocesses.", "AI": {"tldr": "\u667a\u80fd\u62db\u8058\u7ba1\u7406\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u6570\u636e\u9a71\u52a8\u63d0\u5347\u62db\u8058\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u5f0f\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u62db\u8058\u6a21\u5f0f\u56e0\u6548\u7387\u6709\u9650\u3001\u6210\u672c\u9ad8\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u96be\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u5bf9\u7cbe\u51c6\u4eba\u624d\u83b7\u53d6\u7684\u65e5\u76ca\u589e\u957f\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u7b80\u5386\u5feb\u901f\u89e3\u6790\u3001\u5019\u9009\u4eba\u667a\u80fd\u5339\u914d\u548c\u9762\u8bd5\u6d41\u7a0b\u81ea\u52a8\u8c03\u5ea6\u3002", "result": "\u667a\u80fd\u62db\u8058\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u62db\u8058\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f18\u5316\u4e86\u62db\u8058\u6d41\u7a0b\uff0c\u964d\u4f4e\u4e86\u52b3\u52a8\u529b\u548c\u65f6\u95f4\u6210\u672c\u3002", "conclusion": "\u667a\u80fd\u62db\u8058\u7ba1\u7406\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u62db\u8058\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u6210\u4e3a\u73b0\u4ee3\u7ec4\u7ec7\u4eba\u624d\u6218\u7565\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2510.04076", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04076", "abs": "https://arxiv.org/abs/2510.04076", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "comment": null, "summary": "One of the main challenges in modern control applications, particularly in\nrobot and vehicle motion control, is achieving accurate, fast, and safe\nmovement. To address this, optimal control policies have been developed to\nenforce safety while ensuring high performance. Since basic first-principles\nmodels of real systems are often available, model-based controllers are widely\nused. Model predictive control (MPC) is a leading approach that optimizes\nperformance while explicitly handling safety constraints. However, obtaining\naccurate models for complex systems is difficult, which motivates data-driven\nalternatives. ML-based MPC leverages learned models to reduce reliance on\nhand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal\npolicies directly from interaction data. Data-enabled predictive control\n(DeePC) goes further by bypassing modeling altogether, directly learning safe\npolicies from raw input-output data. Recently, large language model (LLM)\nagents have also emerged, translating natural language instructions into\nstructured formulations of optimal control problems. Despite these advances,\ndata-driven policies face significant limitations. They often suffer from slow\nresponse times, high computational demands, and large memory needs, making them\nless practical for real-world systems with fast dynamics, limited onboard\ncomputing, or strict memory constraints. To address this, various technique,\nsuch as reduced-order modeling, function-approximated policy learning, and\nconvex relaxations, have been proposed to reduce computational complexity. In\nthis paper, we present eight such approaches and demonstrate their\neffectiveness across real-world applications, including robotic arms, soft\nrobots, and vehicle motion control.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6570\u636e\u9a71\u52a8\u63a7\u5236\u7b56\u7565\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u516b\u79cd\u964d\u9636\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u63a7\u5236\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u548c\u8f66\u8f86\u8fd0\u52a8\u63a7\u5236\u4e2d\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u5feb\u901f\u548c\u5b89\u5168\u7684\u8fd0\u52a8\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u6570\u636e\u9a71\u52a8\u7b56\u7565\u867d\u7136\u80fd\u51cf\u5c11\u5bf9\u7cbe\u786e\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u4f46\u5b58\u5728\u54cd\u5e94\u6162\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u5185\u5b58\u9700\u6c42\u5927\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u964d\u9636\u5efa\u6a21\u3001\u51fd\u6570\u903c\u8fd1\u7b56\u7565\u5b66\u4e60\u548c\u51f8\u677e\u5f1b\u7b49\u6280\u672f\uff0c\u51cf\u5c11\u6570\u636e\u9a71\u52a8\u7b56\u7565\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u63d0\u51fa\u7684\u516b\u79cd\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u9a71\u52a8\u7b56\u7565\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u516b\u79cd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6280\u672f\uff0c\u5e76\u5728\u5305\u62ec\u673a\u68b0\u81c2\u3001\u8f6f\u4f53\u673a\u5668\u4eba\u548c\u8f66\u8f86\u8fd0\u52a8\u63a7\u5236\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5206\u7c7b\u548c\u73b0\u72b6\uff0c\u6307\u51fa\u5f53\u524d\u7cfb\u7edf\u5728\u4e1a\u52a1\u7406\u89e3\u548c\u90e8\u7f72\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\uff0c\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u591a\u4e2a\u9636\u6bb5\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u5206\u7c7b\u548c\u6279\u5224\u6027\u5206\u6790\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u8bba\u6587\u5bf945\u4e2a\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7684\u751f\u547d\u5468\u671f\u5206\u7c7b\uff0c\u5e76\u6cbf\u4e94\u4e2a\u8de8\u9886\u57df\u8bbe\u8ba1\u7ef4\u5ea6\u8fdb\u884c\u4e86\u6807\u6ce8\u3002", "result": "\u5206\u6790\u53d1\u73b0\u5927\u591a\u6570\u7cfb\u7edf\u4fa7\u91cd\u4e8e\u63a2\u7d22\u6027\u5206\u6790\u548c\u5efa\u6a21\uff0c\u800c\u5ffd\u89c6\u4e86\u4e1a\u52a1\u7406\u89e3\u548c\u90e8\u7f72\u3002\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u662f\u6311\u6218\uff0c\u4e1490%\u4ee5\u4e0a\u7684\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u548c\u5b89\u5168\u673a\u5236\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5f53\u524d\u53d1\u5c55\u8d8b\u52bf\uff0c\u6307\u51fa\u4e86\u5728\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u548c\u76d1\u63a7\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u7684\u6311\u6218\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5bf9\u9f50\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cbb\u7406\u548c\u8bc4\u4f30\u6846\u67b6\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.04468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04468", "abs": "https://arxiv.org/abs/2510.04468", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improving IR-based Bug Localization with Semantics-Driven Query Reduction", "comment": "56 pages, 16 figures, 11 tables", "summary": "Despite decades of research, software bug localization remains challenging\ndue to heterogeneous content and inherent ambiguities in bug reports. Existing\nmethods such as Information Retrieval (IR)-based approaches often attempt to\nmatch source documents to bug reports, overlooking the context and semantics of\nthe source code. On the other hand, Large Language Models (LLM) (e.g.,\nTransformer models) show promising results in understanding both texts and\ncode. However, they have not been yet adapted well to localize software bugs\nagainst bug reports. They could be also data or resource-intensive. To bridge\nthis gap, we propose, IQLoc, a novel bug localization approach that capitalizes\non the strengths of both IR and LLM-based approaches. In particular, we\nleverage the program semantics understanding of transformer-based models to\nreason about the suspiciousness of code and reformulate queries during bug\nlocalization using Information Retrieval. To evaluate IQLoc, we refine the\nBench4BL benchmark dataset and extend it by incorporating ~30% more recent bug\nreports, resulting in a benchmark containing ~7.5K bug reports. We evaluated\nIQLoc using three performance metrics and compare it against four baseline\ntechniques. Experimental results demonstrate its superiority, achieving up to\n58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in\nHIT@K for the test bug reports with random and time-wise splits, respectively.\nMoreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,\n72.73% for those that include code elements, and 65.38% for those containing\nonly descriptions in natural language. By integrating program semantic\nunderstanding into Information Retrieval, IQLoc mitigates several longstanding\nchallenges of traditional IR-based approaches in bug localization.", "AI": {"tldr": "IQLoc\u7ed3\u5408IR\u4e0eLLM\u65b9\u6cd5\uff0c\u901a\u8fc7Transformer\u6a21\u578b\u7406\u89e3\u7a0b\u5e8f\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u7f3a\u9677\u5b9a\u4f4d\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u7f3a\u9677\u62a5\u544a\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u7684\u65b9\u6cd5\u5728\u5339\u914d\u6e90\u4ee3\u7801\u4e0e\u7f3a\u9677\u62a5\u544a\u65f6\u5ffd\u7565\u4e86\u4ee3\u7801\u7684\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u5728\u7406\u89e3\u6587\u672c\u548c\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c1a\u672a\u5f88\u597d\u5730\u9002\u5e94\u7f3a\u9677\u5b9a\u4f4d\u4efb\u52a1\u4e14\u53ef\u80fd\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u63d0\u51faIQLoc\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408IR\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u7406\u89e3\u7a0b\u5e8f\u8bed\u4e49\uff0c\u5e76\u5728\u7f3a\u9677\u5b9a\u4f4d\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u91cd\u65b0\u5236\u5b9a\u67e5\u8be2\u3002", "result": "\u5728Bench4BL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cIQLoc\u5728MAP\u3001MRR\u548cHIT@K\u4e09\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u56db\u79cd\u57fa\u7ebf\u6280\u672f\uff0c\u5c24\u5176\u5bf9\u4e8e\u5305\u542b\u5806\u6808\u8ddf\u8e2a\u3001\u4ee3\u7801\u5143\u7d20\u6216\u4ec5\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7f3a\u9677\u62a5\u544a\uff0c\u6539\u8fdb\u5e45\u5ea6\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7a0b\u5e8f\u8bed\u4e49\u7406\u89e3\u4e0e\u4fe1\u606f\u68c0\u7d22\uff0cIQLoc\u6709\u6548\u7f13\u89e3\u4e86\u4f20\u7edf\u57fa\u4e8eIR\u65b9\u6cd5\u5728\u8f6f\u4ef6\u7f3a\u9677\u5b9a\u4f4d\u4e2d\u7684\u957f\u671f\u6311\u6218\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.04161", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04161", "abs": "https://arxiv.org/abs/2510.04161", "authors": ["Longrui Yang", "Yiyu Wang", "Jingfan Tang", "Yunpeng Lv", "Shizhe Zhao", "Chao Cao", "Zhongqiang Ren"], "title": "HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments", "comment": "5 Figures", "summary": "This paper considers the path planning problem for autonomous exploration of\nan unknown environment using multiple heterogeneous robots such as drones,\nwheeled, and legged robots, which have different capabilities to traverse\ncomplex terrains. A key challenge there is to intelligently allocate the robots\nto the unknown areas to be explored and determine the visiting order of those\nspaces subject to traversablity constraints, which leads to a large scale\nconstrained optimization problem that needs to be quickly and iteratively\nsolved every time when new space are explored. To address the challenge, we\npropose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging\na recent hierarchical method that decompose the exploration into global\nplanning and local planning. The major contribution in HEHA is its global\nplanning, where we propose a new routing algorithm PEAF (Partial Anytime Focal\nsearch) that can quickly find bounded sub-optimal solutions to minimize the\nmaximum path length among the agents subject to traversability constraints.\nAdditionally, the local planner in HEHA also considers heterogeneity to avoid\nrepeated and duplicated exploration among the robots. The experimental results\nshow that, our HEHA can reduce up to 30% of the exploration time than the\nbaselines.", "AI": {"tldr": "HEHA\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548cPEAF\u7b97\u6cd5\uff0c\u4f18\u5316\u591a\u5f02\u6784\u673a\u5668\u4eba\u7684\u63a2\u7d22\u8def\u5f84\uff0c\u51cf\u5c1130%\u63a2\u7d22\u65f6\u95f4\u3002", "motivation": "\u591a\u5f02\u6784\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u9762\u4e34\u590d\u6742\u5730\u5f62\u901a\u8fc7\u6027\u548c\u4efb\u52a1\u5206\u914d\u7684\u6311\u6218\uff0c\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u89e3\u51b3\u5927\u89c4\u6a21\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "method": "HEHA\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u5c06\u63a2\u7d22\u4efb\u52a1\u5206\u89e3\u4e3a\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u89c4\u5212\u3002\u5168\u5c40\u89c4\u5212\u4e2d\u63d0\u51fa\u4e86PEAF\uff08Partial Anytime Focal search\uff09\u7b97\u6cd5\uff0c\u5feb\u901f\u627e\u5230\u6709\u754c\u6b21\u4f18\u89e3\u4ee5\u6700\u5c0f\u5316\u6700\u5927\u8def\u5f84\u957f\u5ea6\uff1b\u5c40\u90e8\u89c4\u5212\u5219\u8003\u8651\u4e86\u5f02\u6784\u6027\u4ee5\u907f\u514d\u91cd\u590d\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cHEHA\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e86\u9ad8\u8fbe30%\u7684\u63a2\u7d22\u65f6\u95f4\u3002", "conclusion": "HEHA\uff08Hierarchical Exploration with Heterogeneous Agents\uff09\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u5f02\u6784\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e8630%\u7684\u63a2\u7d22\u65f6\u95f4\u3002"}}
{"id": "2510.04033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.", "AI": {"tldr": "MedLog\u662f\u4e00\u79cd\u4e34\u5e8aAI\u4e8b\u4ef6\u65e5\u5fd7\u534f\u8bae\uff0c\u65e8\u5728\u6807\u51c6\u5316\u8bb0\u5f55AI\u6a21\u578b\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u652f\u6301\u76d1\u6d4b\u3001\u5ba1\u8ba1\u548c\u6539\u8fdb\u3002", "motivation": "\u533b\u7597AI\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u65e5\u5fd7\u8bb0\u5f55\u65b9\u6cd5\uff0c\u5bfc\u81f4\u96be\u4ee5\u8861\u91cf\u5b9e\u9645\u6027\u80fd\u3001\u68c0\u6d4b\u4e0d\u826f\u4e8b\u4ef6\u6216\u7ea0\u6b63\u504f\u5dee\u3002", "method": "\u63d0\u51faMedLog\u534f\u8bae\uff0c\u5305\u62ec\u4e5d\u4e2a\u6838\u5fc3\u5b57\u6bb5\uff08header\u3001model\u3001user\u3001target\u3001inputs\u3001artifacts\u3001outputs\u3001outcomes\u3001feedback\uff09\uff0c\u652f\u6301\u98ce\u9669\u91c7\u6837\u3001\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u4fdd\u7559\u7b56\u7565\u548c\u5199\u540e\u7f13\u5b58\u3002", "result": "MedLog\u4e3a\u4e34\u5e8aAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u4e00\u81f4\u7684\u6a21\u578b\u6d3b\u52a8\u8bb0\u5f55\u65b9\u5f0f\uff0c\u652f\u6301\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u65e9\u671f\u91c7\u7528\uff0c\u5e76\u80fd\u6355\u83b7\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u8be6\u7ec6\u75d5\u8ff9\u3002", "conclusion": "MedLog\u534f\u8bae\u4e3a\u4e34\u5e8aAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u7684\u4e8b\u4ef6\u7ea7\u65e5\u5fd7\u8bb0\u5f55\u65b9\u6cd5\uff0c\u6709\u671b\u4fc3\u8fdb\u533b\u7597AI\u7684\u6301\u7eed\u76d1\u6d4b\u3001\u5ba1\u8ba1\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u4e3a\u65b0\u578b\u6570\u5b57\u6d41\u884c\u75c5\u5b66\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.04469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04469", "abs": "https://arxiv.org/abs/2510.04469", "authors": ["Wenqi Yan", "Toby Murray", "Benjamin Rubinstein", "Van-Thuan Pham"], "title": "DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing", "comment": null, "summary": "We present DynamiQ, a full-fledged and optimized successor to AFLTeam that\nsupports dynamic and adaptive parallel fuzzing. Unlike most existing approaches\nthat treat individual seeds as tasks, DynamiQ leverages structural information\nfrom the program's call graph to define tasks and continuously refines task\nallocation using runtime feedback. This design significantly reduces redundant\nexploration and enhances fuzzing efficiency at scale. Built on top of the\nstate-of-the-art LibAFL framework, DynamiQ incorporates several practical\noptimizations in both task allocation and task-aware fuzzing. Evaluated on 12\nreal-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ\noutperforms state-of-the-art parallel fuzzers in both code coverage and\nvulnerability discovery, uncovering 9 previously unknown bugs in widely used\nand extensively fuzzed open-source software.", "AI": {"tldr": "DynamiQ \u662f\u4e00\u79cd\u52a8\u6001\u5e76\u884c\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u901a\u8fc7\u8c03\u7528\u56fe\u7ed3\u6784\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6f0f\u6d1e\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5355\u4e2a\u79cd\u5b50\u4f5c\u4e3a\u4efb\u52a1\u5904\u7406\uff0c\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u4fe1\u606f\u7684\u5229\u7528\uff0c\u5bfc\u81f4\u5197\u4f59\u63a2\u7d22\u548c\u6548\u7387\u4f4e\u4e0b\u3002DynamiQ \u65e8\u5728\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u4f18\u5316\u63d0\u5347\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "DynamiQ \u5229\u7528\u7a0b\u5e8f\u7684\u8c03\u7528\u56fe\u7ed3\u6784\u4fe1\u606f\u5b9a\u4e49\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u65f6\u53cd\u9988\u6301\u7eed\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u51cf\u5c11\u5197\u4f59\u63a2\u7d22\u3002\u57fa\u4e8e LibAFL \u6846\u67b6\uff0cDynamiQ \u5b9e\u73b0\u4e86\u4efb\u52a1\u5206\u914d\u548c\u4efb\u52a1\u611f\u77e5\u6a21\u7cca\u6d4b\u8bd5\u7684\u591a\u79cd\u4f18\u5316\u3002", "result": "\u5728 OSS-Fuzz \u548c FuzzBench \u7684 12 \u4e2a\u771f\u5b9e\u76ee\u6807\u4e0a\u8fdb\u884c\u4e86 25,000 CPU \u5c0f\u65f6\u7684\u8bc4\u4f30\uff0cDynamiQ \u5728\u4ee3\u7801\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u53d1\u73b0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u53d1\u73b0\u4e86 9 \u4e2a\u4e4b\u524d\u672a\u77e5\u7684\u5e7f\u6cdb\u4f7f\u7528\u4e14\u7ecf\u8fc7\u5927\u91cf\u6a21\u7cca\u6d4b\u8bd5\u7684\u5f00\u6e90\u8f6f\u4ef6\u6f0f\u6d1e\u3002", "conclusion": "DynamiQ \u4f5c\u4e3a AFLTeam \u7684\u4f18\u5316\u540e\u7ee7\u8005\uff0c\u901a\u8fc7\u52a8\u6001\u548c\u81ea\u9002\u5e94\u5e76\u884c\u6a21\u7cca\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\uff0c\u5e76\u5728\u4ee3\u7801\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u53d1\u73b0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5e76\u884c\u6a21\u7cca\u5668\u3002"}}
{"id": "2510.04168", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04168", "abs": "https://arxiv.org/abs/2510.04168", "authors": ["Amirmasoud Molaei", "Reza Ghabcheloo"], "title": "Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation", "comment": null, "summary": "Rock capturing with standard excavator buckets is a challenging task\ntypically requiring the expertise of skilled operators. Unlike soil digging, it\ninvolves manipulating large, irregular rocks in unstructured environments where\ncomplex contact interactions with granular material make model-based control\nimpractical. Existing autonomous excavation methods focus mainly on continuous\nmedia or rely on specialized grippers, limiting their applicability to\nreal-world construction sites. This paper introduces a fully data-driven\ncontrol framework for rock capturing that eliminates the need for explicit\nmodeling of rock or soil properties. A model-free reinforcement learning agent\nis trained in the AGX Dynamics simulator using the Proximal Policy Optimization\n(PPO) algorithm and a guiding reward formulation. The learned policy outputs\njoint velocity commands directly to the boom, arm, and bucket of a CAT365\nexcavator model. Robustness is enhanced through extensive domain randomization\nof rock geometry, density, and mass, as well as the initial configurations of\nthe bucket, rock, and goal position. To the best of our knowledge, this is the\nfirst study to develop and evaluate an RL-based controller for the rock\ncapturing task. Experimental results show that the policy generalizes well to\nunseen rocks and varying soil conditions, achieving high success rates\ncomparable to those of human participants while maintaining machine stability.\nThese findings demonstrate the feasibility of learning-based excavation\nstrategies for discrete object manipulation without requiring specialized\nhardware or detailed material models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u6316\u6398\u673a\u6293\u53d6\u5ca9\u77f3\uff0c\u65e0\u9700\u5efa\u6a21\u5ca9\u77f3\u6216\u571f\u58e4\u7279\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6210\u529f\u7387\u9ad8\u4e14\u9c81\u68d2\u6027\u5f3a\u3002", "motivation": "\u6807\u51c6\u6316\u6398\u673a\u6293\u53d6\u5ca9\u77f3\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\uff0c\u73b0\u6709\u81ea\u4e3b\u6316\u6398\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8fde\u7eed\u4ecb\u8d28\u6216\u4f9d\u8d56\u4e13\u7528\u5939\u5177\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5efa\u7b51\u5de5\u573a\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528AGX Dynamics\u6a21\u62df\u5668\u548cPPO\u7b97\u6cd5\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u9886\u57df\u968f\u673a\u5316\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5ca9\u77f3\u548c\u4e0d\u540c\u571f\u58e4\u6761\u4ef6\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u4fdd\u6301\u673a\u5668\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6846\u67b6\u5728\u5ca9\u77f3\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u65e0\u9700\u7279\u6b8a\u786c\u4ef6\u6216\u8be6\u7ec6\u6750\u6599\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u64cd\u4f5c\u8005\u76f8\u5f53\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2510.04040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86 FaithCoT-Bench \u57fa\u51c6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u7ea7 CoT \u5fe0\u5b9e\u6027\u68c0\u6d4b\uff0c\u586b\u8865\u4e86\u5b9e\u8df5\u6311\u6218\u7684\u7a7a\u767d\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u5728\u673a\u5236\u5c42\u9762\u5206\u6790 CoT \u7684\u4e0d\u5fe0\u5b9e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u8f68\u8ff9\u662f\u5426\u5fe0\u5b9e\u4e8e\u6a21\u578b\u5185\u90e8\u63a8\u7406\u7684\u5b9e\u8df5\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86 FaithCoT-Bench \u57fa\u51c6\uff0c\u5305\u62ec FINE-CoT \u6570\u636e\u96c6\uff0c\u5e76\u5bf9 11 \u79cd\u4ee3\u8868\u6027\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u548c\u66f4\u5148\u8fdb\u6a21\u578b\u4e2d\u8fdb\u884c\u68c0\u6d4b\u7684\u6311\u6218\u589e\u52a0\u3002", "conclusion": "FaithCoT-Bench \u662f\u9996\u4e2a\u9488\u5bf9\u5b9e\u4f8b\u7ea7 CoT \u5fe0\u5b9e\u6027\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684 LLM \u63a8\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04495", "abs": "https://arxiv.org/abs/2510.04495", "authors": ["Napasorn Tevarut", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem", "comment": "Accepted in PROFES 2025", "summary": "Trivial packages, small modules with low functionality, are common in the npm\necosystem and can pose security risks despite their simplicity. This paper\nrefines existing definitions and introduce data-only packages that contain no\nexecutable logic. A rule-based static analysis method is developed to detect\ntrivial and data-only packages and evaluate their prevalence and associated\nrisks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are\ntrivial, with vulnerability levels comparable to non-trivial ones, and\ndata-only packages, though rare, also contain risks. The proposed detection\ntool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale\nanalysis to reduce security exposure. This findings suggest that trivial and\ndata-only packages warrant greater attention in dependency management to reduce\npotential technical debt and security exposure.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0npm\u751f\u6001\u4e2d17.92%\u7684\u5305\u662ftrivial\u5305\uff0c\u5176\u5b89\u5168\u98ce\u9669\u4e0e\u975etrivial\u5305\u76f8\u5f53\uff0c\u6570\u636e\u5305\u867d\u5c11\u4f46\u4e5f\u6709\u98ce\u9669\u3002\u68c0\u6d4b\u5de5\u5177\u51c6\u786e\u7387\u9ad8\uff0c\u5efa\u8bae\u52a0\u5f3a\u4f9d\u8d56\u7ba1\u7406\u3002", "motivation": "npm\u751f\u6001\u7cfb\u7edf\u4e2d\u5b58\u5728\u5927\u91cf\u529f\u80fd\u7b80\u5355\u7684\u5c0f\u6a21\u5757\uff08trivial\u5305\uff09\uff0c\u5c3d\u7ba1\u529f\u80fd\u7b80\u5355\u4f46\u4ecd\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002\u6b64\u5916\uff0c\u6570\u636e\u5305\uff08\u4e0d\u542b\u53ef\u6267\u884c\u903b\u8f91\uff09\u7684\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u9759\u6001\u5206\u6790\u65b9\u6cd5\u6765\u68c0\u6d4btrivial\u548c\u6570\u636e\u5305\uff0c\u5e76\u57282025\u5e74npm\u751f\u6001\u7cfb\u7edf\u4e2d\u8bc4\u4f30\u5176\u666e\u904d\u6027\u548c\u76f8\u5173\u98ce\u9669\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c17.92%\u7684\u5305\u662ftrivial\u5305\uff0c\u5176\u6f0f\u6d1e\u6c34\u5e73\u4e0e\u975etrivial\u5305\u76f8\u5f53\uff1b\u6570\u636e\u5305\u867d\u7f55\u89c1\u4f46\u4e5f\u5b58\u5728\u98ce\u9669\u3002\u68c0\u6d4b\u5de5\u5177\u51c6\u786e\u7387\u8fbe94%\uff08macro-F1 0.87\uff09\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4fbf\u662f\u529f\u80fd\u7b80\u5355\u7684trivial\u5305\u548c\u6570\u636e\u5305\u4e5f\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5efa\u8bae\u5728\u4f9d\u8d56\u7ba1\u7406\u4e2d\u7ed9\u4e88\u66f4\u591a\u5173\u6ce8\u4ee5\u51cf\u5c11\u6f5c\u5728\u7684\u6280\u672f\u503a\u52a1\u548c\u5b89\u5168\u66b4\u9732\u3002"}}
{"id": "2510.04171", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04171", "abs": "https://arxiv.org/abs/2510.04171", "authors": ["Lakshadeep Naik", "Adam Fischer", "Daniel Duberg", "Danica Kragic"], "title": "VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs", "comment": null, "summary": "In Mobile Manipulation, selecting an optimal mobile base pose is essential\nfor successful object grasping. Previous works have addressed this problem\neither through classical planning methods or by learning state-based policies.\nThey assume access to reliable state information, such as the precise object\nposes and environment models. In this work, we study base pose planning\ndirectly from top-down orthographic projections of the scene, which provide a\nglobal overview of the scene while preserving spatial structure. We propose\nVBM-NET, a learning-based method for base pose selection using such top-down\northographic projections. We use equivariant TransporterNet to exploit spatial\nsymmetries and efficiently learn candidate base poses for grasping. Further, we\nuse graph neural networks to represent a varying number of candidate base poses\nand use Reinforcement Learning to determine the optimal base pose among them.\nWe show that VBM-NET can produce comparable solutions to the classical methods\nin significantly less computation time. Furthermore, we validate sim-to-real\ntransfer by successfully deploying a policy trained in simulation to real-world\nmobile manipulation.", "AI": {"tldr": "VBM-NET\u901a\u8fc7\u9876\u89c6\u6b63\u4ea4\u6295\u5f71\u5b66\u4e60\u79fb\u52a8\u57fa\u5ea7\u59ff\u6001\u89c4\u5212\uff0c\u7ed3\u5408\u5bf9\u79f0\u6027\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u5e76\u5b9e\u73b0\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u64cd\u4f5c\u4e2d\u57fa\u5ea7\u59ff\u6001\u89c4\u5212\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u7cbe\u786e\u7269\u4f53\u4f4d\u59ff\u548c\u73af\u5883\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u76f4\u63a5\u4ece\u9876\u89c6\u6b63\u4ea4\u6295\u5f71\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86VBM-NET\uff0c\u7ed3\u5408\u4e86TransporterNet\u7684\u7a7a\u95f4\u5bf9\u79f0\u6027\u5b66\u4e60\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5019\u9009\u59ff\u6001\u8868\u793a\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u4f18\u59ff\u6001\u9009\u62e9\u3002", "result": "VBM-NET\u5728\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u53ef\u6bd4\u7684\u7ed3\u679c\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "VBM-NET\u901a\u8fc7\u4ece\u573a\u666f\u7684\u9876\u89c6\u6b63\u4ea4\u6295\u5f71\u4e2d\u76f4\u63a5\u89c4\u5212\u79fb\u52a8\u57fa\u5ea7\u59ff\u6001\uff0c\u7ed3\u5408\u4e86TransporterNet\u7684\u7a7a\u95f4\u5bf9\u79f0\u6027\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5019\u9009\u59ff\u6001\u8868\u793a\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u59ff\u6001\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002"}}
{"id": "2510.04048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer.", "AI": {"tldr": "\u901a\u8fc7\u53ef\u53d8\u6295\u7968\u9608\u503c\u96c6\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u56de\u7b54\u7684\u53ef\u4fe1\u5ea6\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u786e\u5b9a\u6027\u7684\u5e94\u7528\u3002", "motivation": "LLM\u7f3a\u4e4f\u53ef\u9760\u7684\u65b9\u6cd5\u91cf\u5316\u5176\u56de\u7b54\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u96be\u4ee5\u4fe1\u4efb\u3002\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u9009\u62e9\u591a\u6570\u54cd\u5e94\u6a21\u5f0f\uff09\u867d\u7b80\u5355\u4f46\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u5141\u8bb8\u96c6\u6210\u5728\u4e3b\u5bfc\u56de\u7b54\u672a\u8fbe\u9608\u503c\u65f6\u201c\u5f03\u6743\u201d\uff0c\u5e76\u5728\u7b97\u672f\u95ee\u9898\u89e3\u51b3\u548c\u4e34\u5e8a\u7b14\u8bb0\u95ee\u7b54\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u9ad8\u5ea6\u9650\u5236\u6027\u7684\u6295\u7968\u96c6\u6210\u4e2d\uff0c\u56de\u7b54\u7684\u53ef\u4fe1\u5ea6\u5927\u5e45\u63d0\u5347\uff0c\u540c\u65f6\u54cd\u5e94\u4ea7\u51fa\u548c\u51c6\u786e\u6027\u7684\u964d\u4f4e\u76f8\u5bf9\u8f83\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53ef\u53d8\u6295\u7968\u9608\u503c\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLM\u56de\u7b54\u7684\u53ef\u4fe1\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9ad8\u786e\u5b9a\u6027\u7684\u5e94\u7528\u4e2d\uff08\u5982\u533b\u7597\u4fdd\u5065\u548c\u6570\u636e\u6807\u6ce8\uff09\u3002"}}
{"id": "2510.04519", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04519", "abs": "https://arxiv.org/abs/2510.04519", "authors": ["Heiko Koziolek", "Thilo Braun", "Virendra Ashiwal", "Sofia Linsbauer", "Marthe Ahlgreen Hansen", "Karoline Grotterud"], "title": "Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation", "comment": "12 pages, 9 figures", "summary": "Distributed control systems (DCS) manage the automation for many industrial\nproduction processes (e.g., power plants, chemical refineries, steel mills).\nProgramming the software for such systems remains a largely manual and tedious\nprocess, incurring costs of millions of dollars for extensive facilities. Large\nlanguage models (LLMs) have been found helpful in generating DCS control logic,\nresulting in commercial copilot tools. Today, these tools are focused on\ntextual notations, they provide limited automation, and have not been tested on\nlarge datasets with realistic test cases. We introduce Spec2Control, a highly\nautomated LLM workflow to generate graphical control logic directly from\nnatural language user requirements. Experiments using an open dataset with 10\ncontrol narratives and 65 complex test cases demonstrate that Spec2Control can\nsuccessfully identify control strategies, can generate 98.6% of correct control\nstrategy connections autonomously, and can save between 94-96% of human labor.\nSpec2Control is being integrated into commercial ABB engineering tools, but is\nalso available as an open-source variant for independent validation.", "AI": {"tldr": "Spec2Control\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210DCS\u56fe\u5f62\u63a7\u5236\u903b\u8f91\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u8282\u7701\u4eba\u529b\u3002", "motivation": "\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\uff08DCS\uff09\u7684\u8f6f\u4ef6\u7f16\u7a0b\u8fc7\u7a0b\u4ecd\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u4e14\u7e41\u7410\uff0c\u5bfc\u81f4\u9ad8\u6602\u6210\u672c\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5de5\u5177\u5728\u6587\u672c\u7b26\u53f7\u4e0a\u63d0\u4f9b\u6709\u9650\u81ea\u52a8\u5316\uff0c\u4e14\u672a\u5728\u5927\u578b\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u3002", "method": "\u5f15\u5165Spec2Control\uff0c\u4e00\u79cd\u9ad8\u5ea6\u81ea\u52a8\u5316\u7684LLM\u5de5\u4f5c\u6d41\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u7528\u6237\u9700\u6c42\u751f\u6210\u56fe\u5f62\u63a7\u5236\u903b\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpec2Control\u80fd\u6210\u529f\u8bc6\u522b\u63a7\u5236\u7b56\u7565\uff0c\u81ea\u4e3b\u751f\u621098.6%\u7684\u6b63\u786e\u63a7\u5236\u7b56\u7565\u8fde\u63a5\uff0c\u8282\u770194-96%\u7684\u4eba\u529b\u3002", "conclusion": "Spec2Control\u88ab\u96c6\u6210\u5230\u5546\u4e1aABB\u5de5\u7a0b\u5de5\u5177\u4e2d\uff0c\u540c\u65f6\u4e5f\u63d0\u4f9b\u5f00\u6e90\u7248\u672c\u4f9b\u72ec\u7acb\u9a8c\u8bc1\u3002"}}
{"id": "2510.04178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04178", "abs": "https://arxiv.org/abs/2510.04178", "authors": ["L\u00e9a Pistorius", "Namrata U. Nayar", "Phillip Tran", "Sammy Elmariah", "Pierre E. Dupont"], "title": "Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve", "comment": "7 pages, 9 figures", "summary": "Transcatheter valve repair presents significant challenges due to the\nmechanical limitations and steep learning curve associated with manual catheter\nsystems. This paper investigates the use of robotics to facilitate\ntranscatheter procedures in the context of mitral valve edge-to-edge repair.\nThe complex handle-based control of a clinical repair device is replaced by\nintuitive robotic joint-based control via a game controller. Manual versus\nrobotic performance is analyzed by decomposing the overall device delivery task\ninto motion-specific steps and comparing capabilities on a step-by-step basis\nin a phantom model of the heart and vasculature. Metrics include procedure\nduration and clip placement accuracy. Results demonstrate that the robotic\nsystem can reduce procedural time and motion errors while also improving\naccuracy of clip placement. These findings suggest that robotic assistance can\naddress key limitations of manual systems, offering a more reliable and\nuser-friendly platform for complex transcatheter procedures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u673a\u5668\u4eba\u5982\u4f55\u8f85\u52a9\u7ecf\u5bfc\u7ba1\u4e8c\u5c16\u74e3\u8fb9\u7f18\u5230\u8fb9\u7f18\u4fee\u590d\uff0c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u7cfb\u7edf\u6bd4\u624b\u52a8\u64cd\u4f5c\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u3002", "motivation": "\u7ecf\u5bfc\u7ba1\u74e3\u819c\u4fee\u590d\u7531\u4e8e\u624b\u52a8\u5bfc\u7ba1\u7cfb\u7edf\u7684\u673a\u68b0\u9650\u5236\u548c\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u5c06\u590d\u6742\u7684\u57fa\u4e8e\u624b\u67c4\u7684\u4e34\u5e8a\u4fee\u590d\u8bbe\u5907\u63a7\u5236\u66ff\u6362\u4e3a\u901a\u8fc7\u6e38\u620f\u63a7\u5236\u5668\u5b9e\u73b0\u7684\u76f4\u89c2\u673a\u5668\u4eba\u5173\u8282\u63a7\u5236\uff0c\u5e76\u5728\u5fc3\u810f\u548c\u8840\u7ba1\u7684\u4f53\u6a21\u6a21\u578b\u4e2d\u5206\u89e3\u6574\u4f53\u8bbe\u5907\u4ea4\u4ed8\u4efb\u52a1\u4e3a\u7279\u5b9a\u8fd0\u52a8\u6b65\u9aa4\uff0c\u6bd4\u8f83\u624b\u52a8\u4e0e\u673a\u5668\u4eba\u6027\u80fd\u3002", "result": "\u673a\u5668\u4eba\u7cfb\u7edf\u53ef\u4ee5\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u548c\u8fd0\u52a8\u8bef\u5dee\uff0c\u540c\u65f6\u63d0\u9ad8\u5939\u5b50\u653e\u7f6e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u673a\u5668\u4eba\u8f85\u52a9\u7cfb\u7edf\u53ef\u4ee5\u89e3\u51b3\u624b\u52a8\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u590d\u6742\u7684\u7ecf\u5bfc\u7ba1\u624b\u672f\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.04051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences.", "AI": {"tldr": "LEGO-IRT \u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u4e8c\u8fdb\u5236\u548c\u8fde\u7eed\u6307\u6807\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u67b6\u6784\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eIRT\u7684\u65b9\u6cd5\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4ec5\u652f\u6301\u4e8c\u8fdb\u5236\u6b63\u786e\u6027\u6307\u6807\u3001\u65e0\u6cd5\u5904\u7406\u751f\u6210\u4efb\u52a1\u7684\u8fde\u7eed\u5206\u6570\uff0c\u4e14\u5ffd\u7565\u8de8\u57fa\u51c6\u7684\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "LEGO-IRT \u662f\u4e00\u4e2a\u652f\u6301\u4e8c\u8fdb\u5236\u548c\u8fde\u7eed\u8bc4\u4f30\u6307\u6807\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u67b6\u6784\u663e\u5f0f\u5efa\u6a21\u548c\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u4f30\u8ba1\u5206\u89e3\u4e3a\u901a\u7528\u7ec4\u4ef6\u548c\u7ed3\u6784\u7279\u5b9a\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLEGO-IRT \u4ec5\u97003%\u7684\u8bc4\u4f30\u9879\u5373\u53ef\u7a33\u5b9a\u4f30\u8ba1\u6a21\u578b\u80fd\u529b\uff0c\u7ed3\u6784\u77e5\u8bc6\u7684\u5f15\u5165\u4f7f\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c11\u8fbe10%\uff0c\u4e14\u6f5c\u5728\u80fd\u529b\u4f30\u8ba1\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "LEGO-IRT \u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ec5\u4f7f\u75283%\u7684\u8bc4\u4f30\u9879\u5373\u53ef\u7a33\u5b9a\u4f30\u8ba1\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u6f5c\u5728\u80fd\u529b\u4f30\u8ba1\u4e0e\u4eba\u7c7b\u504f\u597d\u66f4\u4e00\u81f4\u3002"}}
{"id": "2510.04603", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04603", "abs": "https://arxiv.org/abs/2510.04603", "authors": ["Johan Lin\u00e5ker", "Sachiko Muto"], "title": "Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes", "comment": "In submission", "summary": "Context: Open Source Software (OSS) is a vital public good, included across\nmost of modern software stacks, significantly impacting GDP and national tech\ngrowth, while supporting interoperability, sovereignty, and transparency.\nHowever, systematic measurement of governmental OSS adoption remain limited.\n  Research Aim: This study contributes to digital government maturity indexes\nby analyzing policies and support actions leveraging OSS for software reuse and\ncollaborative development across 16 digitally mature countries, and proposing\npotential indicators for said indexes. It examines OSS policy formation, stated\ngoals, key actors, and support mechanisms.\n  Methodology: A qualitative approach is used combining desk research of policy\ndocuments with semi-structured interviews of government representatives,\nproducing detailed country reports. These are cross-analyzed, focusing on OSS\npolicy promotion, rationale, and implementation support.\n  Results: Policies facilitating OSS reuse are widespread, targeting both\ninbound acquisition and outbound sharing, and are predominantly governed by\ncentral public sector organizations. Policy goals include interoperability,\ndigital sovereignty, transparency, and cost efficiency, with security framed\nboth as a risk and strength. Implementation is supported by diverse Open Source\nProgram Offices (OSPOs) at multiple government levels, which foster capacity\nbuilding, resource pooling, and sustainable project governance. Indicators are\nsynthesized and proposed across 14 areas covering policy incentives and design,\nand implementation and support.\n  Conclusions: OSS is a strategic enabler for public sector digital\ntransformation. Clear policy frameworks, coupled with institutional support\nsuch as OSPOs, are essential. International digital maturity frameworks should\nexpand OSS indicators to better guide and assess government adoption and\nimpact.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e8616\u4e2a\u6570\u5b57\u6210\u719f\u56fd\u5bb6\u7684OSS\u653f\u7b56\uff0c\u63d0\u51fa\u6570\u5b57\u653f\u5e9c\u6210\u719f\u5ea6\u6307\u6807\u7684\u6f5c\u5728\u6307\u6807\uff0c\u5f3a\u8c03OSS\u5bf9\u516c\u5171\u90e8\u95e8\u6570\u5b57\u5316\u8f6c\u578b\u7684\u6218\u7565\u4ef7\u503c\u53ca\u5236\u5ea6\u652f\u6301\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u662f\u73b0\u4ee3\u8f6f\u4ef6\u6808\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u663e\u8457\u5f71\u54cdGDP\u548c\u56fd\u5bb6\u6280\u672f\u589e\u957f\uff0c\u652f\u6301\u4e92\u64cd\u4f5c\u6027\u3001\u4e3b\u6743\u548c\u900f\u660e\u5ea6\u3002\u7136\u800c\uff0c\u5bf9\u653f\u5e9c\u91c7\u7528OSS\u7684\u7cfb\u7edf\u6027\u6d4b\u91cf\u4ecd\u6709\u9650\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u65b9\u6cd5\uff0c\u7ed3\u5408\u653f\u7b56\u6587\u4ef6\u7684\u6848\u5934\u7814\u7a76\u4e0e\u653f\u5e9c\u4ee3\u8868\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u751f\u6210\u8be6\u7ec6\u56fd\u5bb6\u62a5\u544a\uff0c\u5e76\u8fdb\u884c\u4ea4\u53c9\u5206\u6790\uff0c\u805a\u7126OSS\u653f\u7b56\u63a8\u5e7f\u3001\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u65bd\u652f\u6301\u3002", "result": "\u4fc3\u8fdbOSS\u91cd\u7528\u7684\u653f\u7b56\u5e7f\u6cdb\u5b58\u5728\uff0c\u4e3b\u8981\u9488\u5bf9\u5185\u5916\u5411\u5171\u4eab\uff0c\u4e14\u4e3b\u8981\u7531\u4e2d\u592e\u516c\u5171\u90e8\u95e8\u7ec4\u7ec7\u7ba1\u7406\u3002\u653f\u7b56\u76ee\u6807\u5305\u62ec\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u5b57\u4e3b\u6743\u3001\u900f\u660e\u5ea6\u548c\u6210\u672c\u6548\u7387\uff0c\u5b89\u5168\u6027\u88ab\u540c\u65f6\u89c6\u4e3a\u98ce\u9669\u548c\u4f18\u52bf\u3002\u5b9e\u65bd\u652f\u6301\u6765\u81ea\u591a\u7ea7\u653f\u5e9c\u7684\u591a\u6837\u5316\u5f00\u6e90\u9879\u76ee\u529e\u516c\u5ba4\uff08OSPOs\uff09\uff0c\u4fc3\u8fdb\u80fd\u529b\u5efa\u8bbe\u3001\u8d44\u6e90\u5171\u4eab\u548c\u53ef\u6301\u7eed\u9879\u76ee\u6cbb\u7406\u3002", "conclusion": "OSS\u662f\u516c\u5171\u90e8\u95e8\u6570\u5b57\u5316\u8f6c\u578b\u7684\u6218\u7565\u63a8\u52a8\u8005\u3002\u660e\u786e\u7684\u653f\u7b56\u6846\u67b6\u4e0e\u5236\u5ea6\u652f\u6301\uff08\u5982OSPOs\uff09\u81f3\u5173\u91cd\u8981\u3002\u56fd\u9645\u6570\u5b57\u6210\u719f\u5ea6\u6846\u67b6\u5e94\u6269\u5c55OSS\u6307\u6807\uff0c\u4ee5\u66f4\u597d\u5730\u6307\u5bfc\u548c\u8bc4\u4f30\u653f\u5e9c\u91c7\u7528\u53ca\u5f71\u54cd\u3002"}}
{"id": "2510.04190", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04190", "abs": "https://arxiv.org/abs/2510.04190", "authors": ["Jian-jie Zheng", "Chih-kai Yang", "Po-han Chen", "Lyn Chao-ling Chen"], "title": "Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification", "comment": null, "summary": "In the study, the social robot act as a patrol to recognize and notify\nillegal parking in real-time. Dual-model pipeline method and large multimodal\nmodel were compared, and the GPT-4o multimodal model was adopted in license\nplate recognition without preprocessing. For moving smoothly on a flat ground,\nthe robot navigated in a simulated parking lot in the experiments. The robot\nchanges angle view of the camera automatically to capture the images around\nwith the format of license plate number. From the captured images of the robot,\nthe numbers on the plate are recognized through the GPT-4o model, and\nidentifies legality of the numbers. When an illegal parking is detected, the\nrobot sends Line messages to the system manager immediately. The contribution\nof the work is that a novel multimodal deep learning method has validated with\nhigh accuracy in license plate recognition, and a social assistive robot is\nalso provided for solving problems in a real scenario, and can be applied in an\nindoor parking lot.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528GPT-4o\u591a\u6a21\u6001\u6a21\u578b\u548c\u793e\u4ea4\u673a\u5668\u4eba\u5b9e\u65f6\u8bc6\u522b\u975e\u6cd5\u505c\u8f66\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u793e\u4ea4\u673a\u5668\u4eba\u5b9e\u65f6\u8bc6\u522b\u5e76\u901a\u77e5\u975e\u6cd5\u505c\u8f66\u884c\u4e3a\uff0c\u89e3\u51b3\u505c\u8f66\u573a\u7ba1\u7406\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u91c7\u7528GPT-4o\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u65e0\u9700\u9884\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u53cc\u6a21\u578b\u7ba1\u9053\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u673a\u5668\u4eba\u901a\u8fc7\u5728\u6a21\u62df\u505c\u8f66\u573a\u4e2d\u5bfc\u822a\uff0c\u81ea\u52a8\u8c03\u6574\u6444\u50cf\u5934\u89d2\u5ea6\u6355\u83b7\u8f66\u724c\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u5b9e\u65f6\u901a\u77e5\u7cfb\u7edf\u7ba1\u7406\u5458\u975e\u6cd5\u505c\u8f66\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u79cd\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u53ef\u5e94\u7528\u4e8e\u5ba4\u5185\u505c\u8f66\u573a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2510.04064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5185\u90e8\u60c5\u611f\u8868\u793a\uff0c\u53d1\u73b0\u5176\u5177\u6709\u660e\u786e\u7684\u51e0\u4f55\u7ed3\u6784\u3001\u53ef\u5851\u6027\u53ca\u6301\u4e45\u6027\uff0c\u4e3aAI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u63a2\u7d22\u73b0\u4ee3LLMs\u4e2d\u6f5c\u5728\u7684\u60c5\u611f\u8868\u793a\uff0c\u5373\u60c5\u611f\u5982\u4f55\u3001\u5728\u54ea\u91cc\u4ee5\u53ca\u6301\u7eed\u591a\u4e45\u88ab\u7f16\u7801\u5728\u5176\u795e\u7ecf\u67b6\u6784\u4e2d\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u5206\u7c7b\u3001\u91cd\u5199\u548c\u5408\u6210\u751f\u6210\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea640\u4e07\u6761\u8bdd\u8bed\u7684Reddit\u8bed\u6599\u5e93\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u201c\u63a2\u9488\u201d\u4eceQwen3\u548cLLLaMA\u6a21\u578b\u7684\u9690\u85cf\u5c42\u8bfb\u53d6\u4fe1\u606f\u3002", "result": "\u53d1\u73b0LLMs\u5185\u90e8\u5f62\u6210\u4e86\u660e\u786e\u7684\u60c5\u611f\u51e0\u4f55\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5f3a\u4e14\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u3002\u60c5\u611f\u4fe1\u53f7\u5728\u7f51\u7edc\u65e9\u671f\u51fa\u73b0\u5e76\u5728\u4e2d\u671f\u8fbe\u5230\u5cf0\u503c\uff0c\u4e14\u5177\u6709\u53ef\u5851\u6027\u548c\u6301\u4e45\u6027\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86LLMs\u5185\u90e8\u60c5\u611f\u8868\u793a\u7684\u6e05\u6670\u51e0\u4f55\u7ed3\u6784\uff0c\u6307\u51fa\u5176\u53ef\u5851\u6027\u53ca\u6301\u4e45\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u548c\u4e00\u81f4\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.04605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04605", "abs": "https://arxiv.org/abs/2510.04605", "authors": ["Jingyao Zhang", "Tianlin Li", "Xiaoyu Zhang", "Qiang Hu", "Bin Shi"], "title": "Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) are widely used in software\nengineering (SE) but face limitations in processing code structure information\nand suffer from high inference latency. Diffusion LLMs (DLLMs) offer a\npromising alternative with global bidirectional encoding and decoupled\ngeneration steps. This work presents the first comprehensive evaluation of\nDLLMs across the software development lifecycle, including code generation,\ndefect detection, and program repair. On a large-scale benchmark of 52,937\ntasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy\nimprovement achieving a 113% gain on cross-file repair, while maintaining\nsuperior efficiency and reduced latency. Our results establish DLLMs as a\nsuperior paradigm for SE tasks.", "AI": {"tldr": "DLLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8aAR-LLMs\uff0c\u5c24\u5176\u5728\u8de8\u6587\u4ef6\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u51c6\u786e\u7387\u63d0\u5347113%\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3001\u5ef6\u8fdf\u66f4\u4f4e\u3002", "motivation": "AR-LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5904\u7406\u4ee3\u7801\u7ed3\u6784\u4fe1\u606f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u63a8\u7406\u5ef6\u8fdf\u8f83\u9ad8\u3002DLLMs\u56e0\u5176\u5168\u5c40\u53cc\u5411\u7f16\u7801\u548c\u89e3\u8026\u751f\u6210\u6b65\u9aa4\uff0c\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u5bf9DLLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684\u591a\u4e2a\u4efb\u52a1\uff08\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u7a0b\u5e8f\u4fee\u590d\uff09\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u5305\u542b52,937\u4e2a\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "7B\u53c2\u6570\u7684DLLMs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e8630%\uff0c\u5728\u8de8\u6587\u4ef6\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u51c6\u786e\u7387\u63d0\u5347\u4e86113%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "DLLMs\uff08\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8eAR-LLMs\uff08\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5c24\u5176\u662f\u5728\u8de8\u6587\u4ef6\u4fee\u590d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86113%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002"}}
{"id": "2510.04234", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04234", "abs": "https://arxiv.org/abs/2510.04234", "authors": ["Runhan Huang", "Haldun Balim", "Heng Yang", "Yilun Du"], "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control", "comment": "9 pages, 8 figures", "summary": "Legged locomotion demands controllers that are both robust and adaptable,\nwhile remaining compatible with task and safety considerations. However,\nmodel-free reinforcement learning (RL) methods often yield a fixed policy that\ncan be difficult to adapt to new behaviors at test time. In contrast, Model\nPredictive Control (MPC) provides a natural approach to flexible behavior\nsynthesis by incorporating different objectives and constraints directly into\nits optimization process. However, classical MPC relies on accurate dynamics\nmodels, which are often difficult to obtain in complex environments and\ntypically require simplifying assumptions. We present Diffusion-MPC, which\nleverages a learned generative diffusion model as an approximate dynamics prior\nfor planning, enabling flexible test-time adaptation through reward and\nconstraint based optimization. Diffusion-MPC jointly predicts future states and\nactions; at each reverse step, we incorporate reward planning and impose\nconstraint projection, yielding trajectories that satisfy task objectives while\nremaining within physical limits. To obtain a planning model that adapts beyond\nimitation pretraining, we introduce an interactive training algorithm for\ndiffusion based planner: we execute our reward-and-constraint planner in\nenvironment, then filter and reweight the collected trajectories by their\nrealized returns before updating the denoiser. Our design enables strong\ntest-time adaptability, allowing the planner to adjust to new reward\nspecifications without retraining. We validate Diffusion-MPC on real world,\ndemonstrating strong locomotion and flexible adaptation.", "AI": {"tldr": "Diffusion-MPC \u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c MPC\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u9002\u5e94\u65b0\u4efb\u52a1\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf MPC \u4f9d\u8d56\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8fd9\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u5f97\uff1b\u800c\u6a21\u578b\u65e0\u5173\u7684 RL \u65b9\u6cd5\u4ea7\u751f\u7684\u56fa\u5b9a\u7b56\u7565\u96be\u4ee5\u9002\u5e94\u65b0\u884c\u4e3a\u3002", "method": "Diffusion-MPC \u5229\u7528\u5b66\u4e60\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89c4\u5212\u4e2d\u7684\u8fd1\u4f3c\u52a8\u529b\u5b66\u5148\u9a8c\uff0c\u901a\u8fc7\u5956\u52b1\u548c\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u7075\u6d3b\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u3002", "result": "Diffusion-MPC \u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u7075\u6d3b\u7684\u9002\u5e94\u6027\u3002", "conclusion": "Diffusion-MPC \u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5f3a\u5927\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u7075\u6d3b\u7684\u9002\u5e94\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u5230\u65b0\u5956\u52b1\u89c4\u683c\u7684\u80fd\u529b\u3002"}}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "MAS\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u3001\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u6cbb\u7406\uff0c\u663e\u8457\u51cf\u5c11AI\u4ef7\u503c\u6f02\u79fb\uff0c\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548c\u4f4e\u8bef\u62a5\u3002", "motivation": "\u968f\u7740AI\u4f5c\u4e3a\u8d85\u7ea7\u52a9\u624b\u7684\u5d1b\u8d77\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4f26\u7406\u548c\u610f\u56fe\u4e00\u81f4\u7684\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898\u65e5\u76ca\u5173\u952e\uff0c\u5c24\u5176\u662f\u4ef7\u503c\u6f02\u79fb\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u4f26\u7406\u8fdd\u89c4\u3002", "method": "\u63d0\u51faMoral Anchor System (MAS)\u6846\u67b6\uff0c\u7ed3\u5408\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u65ad\u3001LSTM\u7f51\u7edc\u548c\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\uff0c\u7528\u4e8e\u76d1\u6d4b\u3001\u9884\u6d4b\u548c\u7f13\u89e3AI\u4ee3\u7406\u7684\u4ef7\u503c\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MAS\u53ef\u5c06\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\u51cf\u5c1180%\u4ee5\u4e0a\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe85%\uff0c\u8bef\u62a5\u7387\u4f4e\u81f30.08\u3002", "conclusion": "MAS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u65ad\u3001LSTM\u7f51\u7edc\u548c\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\uff0c\u4fdd\u6301\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u3002"}}
{"id": "2510.04611", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.04611", "abs": "https://arxiv.org/abs/2510.04611", "authors": ["Pawel Weichbroth", "Maciej Lotysz", "Michal Wrobel"], "title": "A survey on the impact of emotions on the productivity among software developers", "comment": "29 pages, 5 tables, 96 references", "summary": "The time pressure associated with software development, among other factors,\noften leads to a diminished emotional state among developers. However, whether\nemotions affect perceived productivity remains an open question. This study\naims to determine the strength and direction of the relationship between\nemotional state and perceived productivity among software developers. We\nemployed a two-stage approach. First, a survey was conducted with a pool of\nnine experts to validate the measurement model. Second, a survey was\nadministered to a pool of 88 software developers to empirically test the\nformulated hypothesis by using Partial Least Squares, as the data analysis\nmethod. The results of the path analysis clearly confirm the formulated\nhypothesis, showing that the emotional state of a software developer has a\nstrong positive, and significant impact (beta = 0.893, p < 0.001) on perceived\nproductivity among software developers. The findings highlight the importance\nof managing and improving developers emotional well-being to enhance\nproductivity in software development environments. Additionally, interventions\naimed at reducing burnout, stress, and other negative factors could have a\nconsiderable impact on their performance outcomes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f00\u53d1\u8005\u60c5\u7eea\u72b6\u6001\u663e\u8457\u5f71\u54cd\u5176\u611f\u77e5\u751f\u4ea7\u529b\uff0c\u5efa\u8bae\u901a\u8fc7\u6539\u5584\u60c5\u7eea\u5065\u5eb7\u63d0\u5347\u7ee9\u6548\u3002", "motivation": "\u63a2\u8ba8\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u65f6\u95f4\u538b\u529b\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u60c5\u7eea\u72b6\u6001\u53d8\u5316\u662f\u5426\u5f71\u54cd\u5f00\u53d1\u8005\u7684\u611f\u77e5\u751f\u4ea7\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u4e5d\u4f4d\u4e13\u5bb6\u9a8c\u8bc1\u6d4b\u91cf\u6a21\u578b\uff0c\u7136\u540e\u5bf988\u4f4d\u8f6f\u4ef6\u5f00\u53d1\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u4f7f\u7528\u504f\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08PLS\uff09\u8fdb\u884c\u6570\u636e\u5206\u6790\u3002", "result": "\u8def\u5f84\u5206\u6790\u8bc1\u5b9e\u60c5\u7eea\u72b6\u6001\u5bf9\u611f\u77e5\u751f\u4ea7\u529b\u6709\u663e\u8457\u6b63\u5411\u5f71\u54cd\uff08beta = 0.893, p < 0.001\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7ba1\u7406\u548c\u6539\u5584\u5f00\u53d1\u8005\u60c5\u7eea\u5065\u5eb7\u5bf9\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u751f\u4ea7\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5efa\u8bae\u901a\u8fc7\u51cf\u5c11\u5026\u6020\u3001\u538b\u529b\u7b49\u8d1f\u9762\u56e0\u7d20\u7684\u5e72\u9884\u63aa\u65bd\u6765\u63d0\u9ad8\u7ee9\u6548\u3002"}}
{"id": "2510.04246", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times.", "AI": {"tldr": "ContextVLA\u901a\u8fc7\u538b\u7f29\u591a\u5e27\u89c2\u6d4b\u4e3a\u5355\u4e2a\u4ee4\u724c\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5728\u591a\u5e27\u89c2\u6d4b\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u800cVision-Language-Action\u6a21\u578b\uff08VLA\uff09\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u591a\u5e27\u89c2\u6d4b\u751f\u6210\u52a8\u4f5c\uff0c\u4f46\u5176\u9ad8\u7ef4\u89c6\u9891\u8f93\u5165\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u5f15\u5165ContextVLA\u7b56\u7565\u6a21\u578b\uff0c\u5229\u7528Vision-Language-Action\u6a21\u578b\uff08VLA\uff09\u7684\u56fa\u6709\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5c06\u591a\u5e27\u89c2\u6d4b\u538b\u7f29\u4e3a\u5355\u4e2a\u4e0a\u4e0b\u6587\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cContextVLA\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u5e27VLA\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u6574\u591a\u5e27\u8bad\u7ec3\u76f8\u5f53\u7684\u6548\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "ContextVLA\u901a\u8fc7\u5c06\u591a\u5e27\u89c2\u6d4b\u538b\u7f29\u4e3a\u5355\u4e2a\u4e0a\u4e0b\u6587\u4ee4\u724c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2510.04089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows.", "AI": {"tldr": "SPOGW\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u95f4\u6bd4\u8f83\u548c\u8fde\u7eed\u7a7a\u95f4\u4f18\u5316\u514b\u670d\u73b0\u6709\u6280\u672f\u7684\u9650\u5236\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bbe\u8ba1\u8fd9\u4e9b\u6d41\u7a0b\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002\u73b0\u6709\u81ea\u52a8\u5316\u6280\u672f\u56e0\u8868\u793a\u80fd\u529b\u6709\u9650\u3001\u9002\u5e94\u6027\u4e0d\u8db3\u3001\u53ef\u6269\u5c55\u6027\u5dee\u548c\u6210\u5bf9\u6bd4\u8f83\u8303\u5f0f\u800c\u53d7\u9650\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u6570\u7684\u504f\u597d\u65b9\u6cd5SPOGW\uff0c\u901a\u8fc7\u7ec4\u95f4\u6bd4\u8f83\u76f4\u63a5\u64cd\u4f5c\u57fa\u6570\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u79bb\u7ebfGRPO\uff08ioGRPO\uff09\u548c\u4f18\u52bf\u63a9\u7801KL\u6563\u5ea6\uff08mKL\uff09\u8fdb\u884c\u8bad\u7ec3\u66f4\u65b0\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u95ee\u7b54\uff09\u4e0a\uff0cSPOGW\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SPOGW\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u751f\u6210\u548c\u4f18\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u5c55\u73b0\u51fa\u4e86\u53ef\u884c\u6027\u548c\u524d\u77bb\u6027\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04689", "abs": "https://arxiv.org/abs/2510.04689", "authors": ["Chengwei Liu", "Wenbo Guo", "Yuxin Zhang", "Limin Wang", "Sen Chen", "Lei Bu", "Yang Liu"], "title": "Evolaris: A Roadmap to Self-Evolving Software Intelligence Management", "comment": null, "summary": "In recent years, the landscape of software threats has become significantly\nmore dynamic and distributed. Security vulnerabilities are no longer discovered\nand shared only through formal channels such as public vulnerability databases\nor vendor advisories. Increasingly, criti- cal threat information emerges\ninformally through blogs, social media, developer forums, open source\nrepositories, and even underground com- munities. To this end, capturing such\nintelligence in a timely manner is essential for maintaining situational\nawareness and enabling prompt security responses. However, this remains a\ncomplex challenge due to the fragmented nature of data sources and the\ntechnical difficulty of collecting, parsing, mapping, and validating\ninformation at scale. To ad- dress this, we propose Evolaris, a self-evolving\nsoftware intelligence sys- tem built on a multi-agent framework. Evolaris is\ndesigned to support a full-stack workflow, where agents operate independently\nbut coordinate through shared context to perform tasks such as information\ndiscovery, reasoning, gap completion, validation, and risk detection. This\narchi- tecture enables the platform to learn from new inputs, refine its\ninternal knowledge, and adapt to emerging threat patterns over time, which\ncould continuously improve the precision, timeliness, and scalability of\nsoftware threat analysis, and offers a sustainable foundation for proactive\nsecu- rity decision-making and strengthens the broader ecosystem of security\nthreat understanding.", "AI": {"tldr": "Evolaris \u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u975e\u6b63\u5f0f\u6e20\u9053\u53ca\u65f6\u6355\u83b7\u548c\u5206\u6790\u8f6f\u4ef6\u5a01\u80c1\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u54cd\u5e94\u7684\u7cbe\u786e\u6027\u548c\u65f6\u6548\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5a01\u80c1\u7684\u52a8\u6001\u6027\u548c\u5206\u5e03\u5f0f\u7279\u6027\u4f7f\u5f97\u901a\u8fc7\u975e\u6b63\u5f0f\u6e20\u9053\u83b7\u53d6\u5173\u952e\u5a01\u80c1\u4fe1\u606f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u6e90\u7684\u788e\u7247\u5316\u548c\u6280\u672f\u96be\u5ea6\u4f7f\u5f97\u8fd9\u4e00\u8fc7\u7a0b\u590d\u6742\u5316\u3002", "method": "Evolaris \u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u6846\u67b6\u7684\u81ea\u6f14\u5316\u8f6f\u4ef6\u667a\u80fd\u7cfb\u7edf\uff0c\u652f\u6301\u5168\u6808\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee3\u7406\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u72ec\u7acb\u64cd\u4f5c\u5e76\u534f\u8c03\u6267\u884c\u4efb\u52a1\u3002", "result": "Evolaris \u80fd\u591f\u4ece\u65b0\u8f93\u5165\u4e2d\u5b66\u4e60\uff0c\u4f18\u5316\u5185\u90e8\u77e5\u8bc6\uff0c\u5e76\u9002\u5e94\u65b0\u5174\u5a01\u80c1\u6a21\u5f0f\uff0c\u4ece\u800c\u6301\u7eed\u63d0\u9ad8\u8f6f\u4ef6\u5a01\u80c1\u5206\u6790\u7684\u7cbe\u786e\u6027\u3001\u53ca\u65f6\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Evolaris \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6301\u7eed\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u4e3b\u52a8\u5b89\u5168\u51b3\u7b56\uff0c\u5e76\u52a0\u5f3a\u4e86\u5bf9\u5b89\u5168\u5a01\u80c1\u7406\u89e3\u7684\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.04278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04278", "abs": "https://arxiv.org/abs/2510.04278", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yuanyuan Zhang", "Jiahao Hu", "Yingming Chen", "Naigui Xiao", "Jiaqi Zhao"], "title": "Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit", "comment": null, "summary": "Model predictive control (MPC) faces significant limitations when applied to\nsystems evolving on nonlinear manifolds, such as robotic attitude dynamics and\nconstrained motion planning, where traditional Euclidean formulations struggle\nwith singularities, over-parameterization, and poor convergence. To overcome\nthese challenges, this paper introduces FactorMPC, a factor-graph based MPC\ntoolkit that unifies system dynamics, constraints, and objectives into a\nmodular, user-friendly, and efficient optimization structure. Our approach\nnatively supports manifold-valued states with Gaussian uncertainties modeled in\ntangent spaces. By exploiting the sparsity and probabilistic structure of\nfactor graphs, the toolkit achieves real-time performance even for\nhigh-dimensional systems with complex constraints. The velocity-extended\non-manifold control barrier function (CBF)-based obstacle avoidance factors are\ndesigned for safety-critical applications. By bridging graphical models with\nsafety-critical MPC, our work offers a scalable and geometrically consistent\nframework for integrated planning and control. The simulations and experimental\nresults on the quadrotor demonstrate superior trajectory tracking and obstacle\navoidance performance compared to baseline methods. To foster research\nreproducibility, we have provided open-source implementation offering\nplug-and-play factors.", "AI": {"tldr": "FactorMPC\u662f\u4e00\u4e2a\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684MPC\u5de5\u5177\u5305\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u6d41\u5f62\u63a7\u5236\u95ee\u9898\uff0c\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfMPC\u5728\u975e\u7ebf\u6027\u6d41\u5f62\uff08\u5982\u673a\u5668\u4eba\u59ff\u6001\u52a8\u529b\u5b66\u548c\u7ea6\u675f\u8fd0\u52a8\u89c4\u5212\uff09\u4e0a\u5e94\u7528\u65f6\u9762\u4e34\u5947\u5f02\u6027\u3001\u8fc7\u53c2\u6570\u5316\u548c\u6536\u655b\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u51e0\u4f55\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FactorMPC\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684MPC\u5de5\u5177\u5305\uff0c\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u7ea6\u675f\u548c\u76ee\u6807\u7edf\u4e00\u5230\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u4f18\u5316\u7ed3\u6784\u4e2d\uff0c\u652f\u6301\u6d41\u5f62\u72b6\u6001\u4e0e\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u56e0\u5b50\u56fe\u7684\u7a00\u758f\u6027\u548c\u6982\u7387\u7ed3\u6784\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFactorMPC\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u969c\u788d\u7269\u907f\u969c\u6027\u80fd\u3002", "conclusion": "FactorMPC\u901a\u8fc7\u56e0\u5b50\u56fe\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u7528\u6237\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u4f18\u5316\u7ed3\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u6d41\u5f62\u4e0a\u7cfb\u7edf\u63a7\u5236\u7684\u6311\u6218\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u5c55\u793a\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u969c\u788d\u7269\u907f\u969c\u6027\u80fd\u3002"}}
{"id": "2510.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM.", "AI": {"tldr": "DLLM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u7684\u8ba4\u77e5\u8bca\u65ad\uff0c\u901a\u8fc7\u5b50\u56fe\u6784\u5efa\u3001\u5173\u7cfb\u589e\u5f3a\u5bf9\u9f50\u548c\u4e24\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u667a\u80fd\u6559\u80b2\u7cfb\u7edf(WIES)\u4e2d\u7684\u8ba4\u77e5\u8bca\u65ad\u65e8\u5728\u4ece\u5f02\u6784\u3001\u5608\u6742\u7684\u4e92\u52a8\u4e2d\u8bc4\u4f30\u5b66\u751f\u5bf9\u77e5\u8bc6\u6982\u5ff5\u7684\u638c\u63e1\u7a0b\u5ea6\u3002\u7136\u800c\uff0c\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u548c\u566a\u58f0\u8bf1\u5bfc\u7684\u8bef\u5224\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14WIES\u7684\u5f00\u653e\u73af\u5883\u52a0\u5267\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u566a\u58f0\u95ee\u9898\u3002", "method": "DLLM\u9996\u5148\u57fa\u4e8e\u56de\u7b54\u6b63\u786e\u6027\u6784\u5efa\u72ec\u7acb\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u5173\u7cfb\u589e\u5f3a\u5bf9\u9f50\u6a21\u5757\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u63a5\u7740\uff0c\u5c06\u4e24\u4e2a\u5b50\u56fe\u8868\u793a\u878d\u5408\u5e76\u4e0eLLM\u884d\u751f\u7684\u8bed\u4e49\u589e\u5f3a\u8868\u793a\u5bf9\u9f50\u3002\u5173\u952e\u662f\u5728\u6bcf\u4e2a\u5bf9\u9f50\u6b65\u9aa4\u524d\uff0cDLLM\u91c7\u7528\u4e24\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6a21\u5757\u6d88\u9664\u56fa\u6709\u566a\u58f0\uff0c\u540c\u65f6\u8f85\u52a9\u7ed3\u6784\u8868\u793a\u5bf9\u9f50\u3002", "result": "DLLM\u5728\u4e09\u79cd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u566a\u58f0\u9c81\u68d2\u6027\u548c\u5bf9LLM\u8bed\u4e49\u77e5\u8bc6\u7684\u6709\u6548\u5229\u7528\u3002", "conclusion": "DLLM\u6846\u67b6\u5728\u4e09\u79cd\u516c\u5f00\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u6559\u80b2\u5e73\u53f0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u80fd\u5b9e\u73b0\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u6709\u6548\u5229\u7528LLM\u7684\u8bed\u4e49\u77e5\u8bc6\u3002"}}
{"id": "2510.04711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04711", "abs": "https://arxiv.org/abs/2510.04711", "authors": ["Aoyang Fang", "Songhan Zhang", "Yifan Yang", "Haotong Wu", "Junjielong Xu", "Xuyang Wang", "Rui Wang", "Manyi Wang", "Qisheng Lu", "Pinjia He"], "title": "An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures", "comment": "Our project is available on https://operationspai.github.io/", "summary": "While cloud-native microservice architectures have transformed software\ndevelopment, their complexity makes Root Cause Analysis (RCA) both crucial and\nchallenging. Although many data-driven RCA models have been proposed, we find\nthat existing benchmarks are often oversimplified and fail to capture\nreal-world conditions. Our preliminary study shows that simple rule-based\nmethods can match or even outperform state-of-the-art (SOTA) models on four\nwidely used benchmarks, suggesting performance overestimation due to benchmark\nsimplicity. To address this, we systematically analyze popular RCA benchmarks\nand identify key limitations in fault injection, call graph design, and\ntelemetry patterns. Based on these insights, we develop an automated framework\nto generate more realistic benchmarks, yielding a dataset of 1,430 validated\nfailure cases from 9,152 injections, covering 25 fault types under dynamic\nworkloads with hierarchical ground-truth labels and verified SLI impact.\nRe-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy\n(average 0.21, best 0.37) and significantly longer execution times. Our\nanalysis highlights three common failure patterns: scalability issues,\nobservability blind spots, and modeling bottlenecks.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u6709RCA\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u88ab\u9ad8\u4f30\uff0c\u5f00\u53d1\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e76\u63ed\u793aSOTA\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e91\u539f\u751f\u5fae\u670d\u52a1\u67b6\u6784\u7684\u590d\u6742\u6027\u4f7f\u5f97\u6839\u56e0\u5206\u6790\uff08RCA\uff09\u65e2\u5173\u952e\u53c8\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u88ab\u9ad8\u4f30\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u6d41\u884c\u7684RCA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc6\u522b\u4e86\u6545\u969c\u6ce8\u5165\u3001\u8c03\u7528\u56fe\u8bbe\u8ba1\u548c\u9065\u6d4b\u6a21\u5f0f\u7684\u5173\u952e\u9650\u5236\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6846\u67b6\u751f\u6210\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,430\u4e2a\u9a8c\u8bc1\u8fc7\u7684\u6545\u969c\u6848\u4f8b\u3002", "result": "\u5728\u751f\u6210\u7684\u66f4\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u91cd\u65b0\u8bc4\u4f3011\u4e2aSOTA\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u8f83\u4f4e\u7684Top@1\u51c6\u786e\u7387\uff08\u5e73\u57470.21\uff0c\u6700\u4f730.37\uff09\u548c\u663e\u8457\u66f4\u957f\u7684\u6267\u884c\u65f6\u95f4\u3002", "conclusion": "\u73b0\u6709\u7684RCA\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u88ab\u9ad8\u4f30\u3002\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\u751f\u6210\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709SOTA\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u79cd\u5e38\u89c1\u6545\u969c\u6a21\u5f0f\u3002"}}
{"id": "2510.04353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04353", "abs": "https://arxiv.org/abs/2510.04353", "authors": ["Stephen McCrory", "Romeo Orsolino", "Dhruv Thanki", "Luigi Penco", "Robert Griffin"], "title": "Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation", "comment": null, "summary": "Teleoperation is a powerful method to generate reference motions and enable\nhumanoid robots to perform a broad range of tasks. However, teleoperation\nbecomes challenging when using hand contacts and non-coplanar surfaces, often\nleading to motor torque saturation or loss of stability through slipping. We\npropose a centroidal stability-based retargeting method that dynamically\nadjusts contact points and posture during teleoperation to enhance stability in\nthese difficult scenarios. Central to our approach is an efficient analytical\ncalculation of the stability margin gradient. This gradient is used to identify\nscenarios for which stability is highly sensitive to teleoperation setpoints\nand inform the local adjustment of these setpoints. We validate the framework\nin simulation and hardware by teleoperating manipulation tasks on a humanoid,\ndemonstrating increased stability margins. We also demonstrate empirically that\nhigher stability margins correlate with improved impulse resilience and joint\ntorque margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d28\u5fc3\u7a33\u5b9a\u6027\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a5\u89e6\u70b9\u548c\u59ff\u6001\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u4f5c\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5728\u5305\u542b\u624b\u90e8\u63a5\u89e6\u548c\u975e\u5171\u9762\u8868\u9762\u7684\u590d\u6742\u64cd\u4f5c\u4e2d\uff0c\u4f20\u7edf\u9065\u64cd\u4f5c\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u7535\u673a\u626d\u77e9\u9971\u548c\u6216\u5931\u7a33\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u589e\u5f3a\u7a33\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8d28\u5fc3\u7a33\u5b9a\u6027\u88d5\u5ea6\u68af\u5ea6\u89e3\u6790\u8ba1\u7b97\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u63a5\u89e6\u70b9\u548c\u59ff\u6001\uff0c\u4ee5\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u88d5\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u9ad8\u7a33\u5b9a\u6027\u88d5\u5ea6\u4e0e\u51b2\u51fb\u5f39\u6027\u548c\u5173\u8282\u626d\u77e9\u88d5\u5ea6\u7684\u6b63\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d28\u5fc3\u7a33\u5b9a\u6027\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a5\u89e6\u70b9\u548c\u59ff\u6001\uff0c\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u4f5c\u4e2d\u7684\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u7a33\u5b9a\u6027\u88d5\u5ea6\uff0c\u5e76\u4e0e\u51b2\u51fb\u5f39\u6027\u548c\u5173\u8282\u626d\u77e9\u88d5\u5ea6\u6b63\u76f8\u5173\u3002"}}
{"id": "2510.04097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics.", "AI": {"tldr": "WebRenderBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684UI\u8f6c\u4ee3\u7801\u57fa\u51c6\uff0cALISA\u901a\u8fc7\u65b0\u8bc4\u4f30\u6307\u6807\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709UI\u56fe\u50cf\u8f6c\u4ee3\u7801\u4efb\u52a1\u4e2d\u6570\u636e\u591a\u6837\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86WebRenderBench\u57fa\u51c6\u548c\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7ALISA\u3002", "result": "ALISA\u5728\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u66f4\u9ad8\u7684\u771f\u5b9e\u7f51\u9875\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ALISA\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.04760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7LASSO\u548cElastic Net\u56de\u5f52\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6545\u4e8b\u70b9\u7684\u654f\u6377\u5de5\u4f5c\u91cf\u4f30\u7b97\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660eLASSO\u56de\u5f52\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u66f4\u4f18\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u4f5c\u91cf\u4f30\u7b97\u662f\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u73af\u8282\uff0c\u51c6\u786e\u7684\u4f30\u7b97\u76f4\u63a5\u5f71\u54cd\u9879\u76ee\u7684\u6210\u8d25\u3002\u76ee\u524d\u654f\u6377\u5de5\u4f5c\u91cf\u4f30\u7b97\u9886\u57df\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u4f7f\u7528LASSO\u548cElastic Net\u56de\u5f52\u6280\u672f\uff0c\u7ed3\u5408\u9ed8\u8ba4\u53c2\u6570\u548c\u7f51\u683c\u641c\u7d22\u8c03\u4f18\u76845\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5bf921\u4e2a\u8f6f\u4ef6\u9879\u76ee\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LASSO\u56de\u5f52\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8eElastic Net\uff0c\u5177\u4f53\u8868\u73b0\u4e3aPRED(8%)\u548cPRED(25%)\u4e3a100.0\uff0cMMRE\u4e3a0.0491\uff0cMMER\u4e3a0.0551\uff0cMdMRE\u4e3a0.0593\uff0cMdMER\u4e3a0.063\uff0cMSE\u4e3a0.0007\u3002", "conclusion": "LASSO\u56de\u5f52\u5728\u8f6f\u4ef6\u9879\u76ee\u654f\u6377\u5de5\u4f5c\u91cf\u4f30\u7b97\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5176PRED(8%)\u548cPRED(25%)\u7ed3\u679c\u5747\u4e3a100.0\uff0c\u4e14\u5176\u4ed6\u8bef\u5dee\u6307\u6807\u5982MMRE\u3001MMER\u3001MdMRE\u3001MdMER\u548cMSE\u5747\u8f83\u4f4e\u3002"}}
{"id": "2510.04354", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04354", "abs": "https://arxiv.org/abs/2510.04354", "authors": ["Apurva Badithela", "David Snyder", "Lihan Zha", "Joseph Mikhail", "Matthew O'Kelly", "Anushri Dixit", "Anirudha Majumdar"], "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators", "comment": null, "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance.", "AI": {"tldr": "SureSim\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4eff\u771f\u548c\u5c11\u91cf\u771f\u5b9e\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u8bc4\u4f30\uff0c\u663e\u8457\u51cf\u5c11\u786c\u4ef6\u6d4b\u8bd5\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u901a\u5e38\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\uff0cSureSim\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6027\u80fd\u63a8\u65ad\u3002", "method": "\u5c06\u771f\u5b9e\u548c\u4eff\u771f\u8bc4\u4f30\u7684\u7ed3\u5408\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9884\u6d4b\u9a71\u52a8\u7684\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5229\u7528\u975e\u6e10\u8fd1\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\u63d0\u4f9b\u7b56\u7565\u6027\u80fd\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u4f7f\u7528SureSim\u53ef\u8282\u770120-25%\u7684\u786c\u4ef6\u8bc4\u4f30\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u83b7\u5f97\u76f8\u4f3c\u7684\u7b56\u7565\u6027\u80fd\u8fb9\u754c\u3002", "conclusion": "SureSim\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u4eff\u771f\u548c\u5c0f\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u786c\u4ef6\u8bc4\u4f30\u7684\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u53ef\u9760\u63a8\u65ad\u3002"}}
{"id": "2510.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly.", "AI": {"tldr": "AutoMR\u6846\u67b6\u901a\u8fc7DAG\u8868\u793a\u548c\u52a8\u6001\u91c7\u6837\u7b97\u6cd5\uff0c\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5143\u63a8\u7406\u9aa8\u67b6\u624b\u52a8\u8bbe\u8ba1\u5bfc\u81f4\u7684\u9002\u5e94\u6027\u5dee\u548c\u903b\u8f91\u4f9d\u8d56\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AutoMR\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8eDAG\u7684\u9aa8\u67b6\u8868\u793a\u3001\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u548c\u52a8\u6001\u9aa8\u67b6\u91c7\u6837\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cAutoMR\u63a8\u7406\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "AutoMR\u901a\u8fc7\u52a8\u6001\u9aa8\u67b6\u91c7\u6837\u7b97\u6cd5\u548cDAG\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002"}}
{"id": "2510.04791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04791", "abs": "https://arxiv.org/abs/2510.04791", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Simone Paolo Ponzetto", "Alexander Maedche", "Christian Bartelt"], "title": "GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes", "comment": null, "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.", "AI": {"tldr": "GUISpector\u662f\u4e00\u4e2a\u5229\u7528\u591a\u6a21\u6001(M)LLM\u4ee3\u7406\u81ea\u52a8\u9a8c\u8bc1GUI\u539f\u578b\u4e2d\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u7684\u65b0\u6846\u67b6\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u53cd\u9988\u5e76\u652f\u6301\u81ea\u52a8\u5316\u5f00\u53d1\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709GUI\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u754c\u9762\u7684\u590d\u6742\u6027\uff0c\u4e14\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u53cd\u9988\u548c\u4e0e\u81ea\u52a8\u5316\u5f00\u53d1\u4ee3\u7406\u7684\u6709\u6548\u96c6\u6210\u3002", "method": "GUISpector\u91c7\u7528\u591a\u6a21\u6001(M)LLM\u4ee3\u7406\u89e3\u91ca\u548c\u64cd\u4f5c\u5316\u81ea\u7136\u8bed\u8a00\u9700\u6c42\uff0c\u81ea\u4e3b\u89c4\u5212\u548c\u6267\u884cGUI\u5e94\u7528\u7684\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u5e76\u7cfb\u7edf\u63d0\u53d6\u8be6\u7ec6\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u3002", "result": "\u5728\u57fa\u4e8e900\u4e2a\u9a8c\u6536\u6807\u51c6\u6ce8\u91ca\u7684150\u9879\u9700\u6c42\u4e0a\u8bc4\u4f30\uff0cGUISpector\u5c55\u793a\u4e86\u6709\u6548\u68c0\u6d4b\u9700\u6c42\u6ee1\u8db3\u4e0e\u8fdd\u89c4\u7684\u80fd\u529b\u3002", "conclusion": "GUISpector\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001(M)LLM\u4ee3\u7406\u81ea\u52a8\u9a8c\u8bc1GUI\u539f\u578b\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\uff0c\u6709\u6548\u68c0\u6d4b\u9700\u6c42\u6ee1\u8db3\u4e0e\u8fdd\u89c4\uff0c\u5e76\u5c55\u793a\u4e86\u5c06\u53ef\u64cd\u4f5c\u53cd\u9988\u65e0\u7f1d\u96c6\u6210\u5230\u81ea\u52a8\u5316LLM\u9a71\u52a8\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04436", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04436", "abs": "https://arxiv.org/abs/2510.04436", "authors": ["Jushan Chen", "Santiago Paternain"], "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization", "comment": null, "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u7531\u6295\u5f71\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8f68\u8ff9\u4f18\u5316\u4e2d\u56e0\u80fd\u5efa\u6a21\u591a\u6a21\u6001\u6982\u7387\u5206\u5e03\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u7b49\u5f0f\u7ea6\u675f\uff08\u5982\u52a8\u6001\u53ef\u884c\u6027\uff09\uff0c\u5bfc\u81f4\u6b21\u4f18\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u751f\u6210\u72b6\u6001\u5e8f\u5217\uff0c\u5e76\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u68af\u5ea6\u81ea\u7531\u6295\u5f71\u673a\u5236\u4ee5\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u56db\u65cb\u7ffc\u822a\u70b9\u5bfc\u822a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u96f6\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u548c\u7ea64\u500d\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u76f4\u63a5\u751f\u6210\u72b6\u6001\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u81ea\u7531\u6295\u5f71\u673a\u5236\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u5b9e\u73b0\u4e86\u96f6\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u3002"}}
{"id": "2510.04128", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u7b49\u5f85\u6807\u8bb0\u524d\u7684\u6f5c\u5728\u7279\u5f81\u80fd\u8c03\u8282\u63a8\u7406\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u6216\u6291\u5236\u7b49\u5f85\u6807\u8bb0\uff0c\u5f71\u54cd\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u7b49\u5f85\u6807\u8bb0\uff08wait tokens\uff09\u5e38\u6807\u5fd7\u7740\u590d\u6742\u7684\u63a8\u7406\u884c\u4e3a\uff08\u5982\u56de\u6eaf\uff09\uff0c\u4f46\u5bf9\u5176\u4e3a\u4f55\u51b3\u5b9a\u4ee5\u7279\u5b9a\u65b9\u5f0f\u63a8\u7406\u7684\u7406\u89e3\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u63a8\u7406\u6a21\u578b\u6709\u6548\u6027\u7684\u8ba4\u8bc6\u3002", "method": "\u7814\u7a76\u8005\u8bad\u7ec3\u4e86\u591a\u5c42DeepSeek-R1-Distill-Llama-8B\u53ca\u5176\u57fa\u7840\u7248\u672c\u7684\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u6f5c\u5728\u5f52\u56e0\u6280\u672f\u3002\u901a\u8fc7\u6700\u5927\u6fc0\u6d3b\u793a\u4f8b\u548c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7279\u5f81\u4e0e\u63a8\u7406\u8fc7\u7a0b\u7684\u76f8\u5173\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u5c0f\u90e8\u5206\u4e0e\u4fc3\u8fdb\u6216\u6291\u5236\u7b49\u5f85\u6807\u8bb0\u6982\u7387\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u7279\u5f81\u786e\u5b9e\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\uff0c\u80fd\u591f\u5f15\u53d1\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6a21\u5f0f\uff08\u5982\u4ece\u5934\u5f00\u59cb\u3001\u56de\u5fc6\u5148\u9a8c\u77e5\u8bc6\u3001\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u548c\u53cc\u91cd\u68c0\u67e5\uff09\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u7b49\u5f85\u6807\u8bb0\u524d\u7684\u6f5c\u5728\u7279\u5f81\u786e\u5b9e\u5305\u542b\u8c03\u8282\u540e\u7eed\u63a8\u7406\u8fc7\u7a0b\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8fd9\u4e9b\u7279\u5f81\u80fd\u591f\u4fc3\u8fdb\u6216\u6291\u5236\u7b49\u5f85\u6807\u8bb0\u7684\u6982\u7387\uff0c\u4ece\u800c\u5f71\u54cd\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002"}}
{"id": "2510.04796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04796", "abs": "https://arxiv.org/abs/2510.04796", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms", "comment": null, "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.", "AI": {"tldr": "RevMine\u662f\u4e00\u4e2a\u5229\u7528LLMs\u7b80\u5316\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u6d41\u7a0b\u7684\u5de5\u5177\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u6536\u96c6\u548c\u5206\u6790\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u7684\u8fc7\u7a0b\u8017\u65f6\u4e14\u6280\u672f\u5bc6\u96c6\uff0c\u5927\u591a\u6570\u7814\u7a76\u8005\u9700\u8981\u7f16\u5199\u4e34\u65f6\u811a\u672c\u6765\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002", "method": "RevMine\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b80\u5316\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u6d41\u7a0b\uff0c\u5305\u62ec\u8ba4\u8bc1\u3001\u7aef\u70b9\u53d1\u73b0\u548c\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u6570\u636e\u6536\u96c6\u3002", "result": "RevMine\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u811a\u672c\u7684\u9700\u6c42\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u8fc7\u6ee4\u5668\u6216LLM\u63a8\u65ad\u6a21\u5f0f\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "RevMine\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u65e8\u5728\u4f7f\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u6c11\u4e3b\u5316\uff0c\u5e76\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u3002"}}
{"id": "2510.04509", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04509", "abs": "https://arxiv.org/abs/2510.04509", "authors": ["Huanqing Wang", "Kaixiang Zhang", "Kyungjoon Lee", "Yu Mei", "Vaibhav Srivastava", "Jun Sheng", "Ziyou Song", "Zhaojian Li"], "title": "Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads", "comment": null, "summary": "Data-driven control methods such as data-enabled predictive control (DeePC)\nhave shown strong potential in efficient control of soft robots without\nexplicit parametric models. However, in object manipulation tasks, unknown\nexternal payloads and disturbances can significantly alter the system dynamics\nand behavior, leading to offset error and degraded control performance. In this\npaper, we present a novel velocity-form DeePC framework that achieves robust\nand optimal control of soft robots under unknown payloads. The proposed\nframework leverages input-output data in an incremental representation to\nmitigate performance degradation induced by unknown payloads, eliminating the\nneed for weighted datasets or disturbance estimators. We validate the method\nexperimentally on a planar soft robot and demonstrate its superior performance\ncompared to standard DeePC in scenarios involving unknown payloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901f\u5ea6\u5f62\u5f0f\u7684DeePC\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u6570\u636e\u8868\u793a\u89e3\u51b3\u4e86\u8f6f\u673a\u5668\u4eba\u5728\u672a\u77e5\u8d1f\u8f7d\u4e0b\u7684\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u672a\u77e5\u7684\u5916\u90e8\u8d1f\u8f7d\u548c\u5e72\u6270\u4f1a\u663e\u8457\u6539\u53d8\u7cfb\u7edf\u52a8\u6001\u548c\u884c\u4e3a\uff0c\u5bfc\u81f4\u504f\u79fb\u8bef\u5dee\u548c\u63a7\u5236\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6807\u51c6DeePC\uff09\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u5229\u7528\u589e\u91cf\u8868\u793a\u7684\u8f93\u5165-\u8f93\u51fa\u6570\u636e\uff0c\u65e0\u9700\u52a0\u6743\u6570\u636e\u96c6\u6216\u5e72\u6270\u4f30\u8ba1\u5668\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u8d1f\u8f7d\u4e0b\u8f6f\u673a\u5668\u4eba\u7684\u9c81\u68d2\u548c\u6700\u4f18\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u901f\u5ea6\u5f62\u5f0fDeePC\u6846\u67b6\u5728\u6d89\u53ca\u672a\u77e5\u8d1f\u8f7d\u7684\u573a\u666f\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6DeePC\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u901f\u5ea6\u5f62\u5f0f\u7684\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\uff08DeePC\uff09\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u673a\u5668\u4eba\u5728\u672a\u77e5\u8d1f\u8f7d\u4e0b\u7684\u63a7\u5236\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u6807\u51c6DeePC\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "MENTOR\u6846\u67b6\u901a\u8fc7\u5173\u952e\u51b3\u7b56\u70b9\u7684\u4e13\u5bb6\u6307\u5bfc\uff0c\u4f18\u5316RLVR\u4e2d\u7684\u63a2\u7d22\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4e14\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u4ec5\u63d0\u5347\u4e86\u63a2\u7d22\u7684\u6709\u6548\u6027\u800c\u5ffd\u89c6\u4e86\u591a\u6837\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8ba4\u4e3a\u4e13\u5bb6\u53ea\u9700\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u6307\u5bfc\u5373\u53ef\u3002", "method": "\u63d0\u51faMENTOR\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u7b56\u7565\u4e13\u5bb6\u5bfc\u822a\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4ee4\u724c\u7ea7\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMENTOR\u80fd\u591f\u6355\u6349\u4e13\u5bb6\u7b56\u7565\u7684\u672c\u8d28\u800c\u975e\u8868\u9762\u6a21\u4eff\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u63a2\u7d22\u548c\u6574\u4f53\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "MENTOR\u6846\u67b6\u901a\u8fc7\u4ec5\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86RLVR\u4e2d\u7684\u6709\u6548\u548c\u591a\u6837\u5316\u63a2\u7d22\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.04835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04835", "abs": "https://arxiv.org/abs/2510.04835", "authors": ["Wentao Gao", "Renata Borovica-Gajic", "Sang Kil Cha", "Tian Qiu", "Van-Thuan Pham"], "title": "InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface", "comment": null, "summary": "Fuzzing is a highly effective automated testing method for uncovering\nsoftware vulnerabilities. Despite advances in fuzzing techniques, such as\ncoverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus\ncaused by fuzz blockers, limiting their ability to find deeper vulnerabilities.\nHuman expertise can address these challenges, but analyzing fuzzing results to\nguide this support remains labor-intensive. To tackle this, we introduce\nInsightQL, the first human-assisting framework for fuzz blocker analysis.\nPowered by a unified database and an intuitive parameterized query interface,\nInsightQL aids developers in systematically extracting insights and efficiently\nunblocking fuzz blockers. Our experiments on 14 popular real-world libraries\nfrom the FuzzBench benchmark demonstrate the effectiveness of InsightQL,\nleading to the unblocking of many fuzz blockers and considerable improvements\nin code coverage (up to 13.90%).", "AI": {"tldr": "InsightQL\u662f\u4e00\u4e2a\u8f85\u52a9\u5206\u6790\u6a21\u7cca\u6d4b\u8bd5\u963b\u585e\u70b9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5e93\u548c\u67e5\u8be2\u63a5\u53e3\u63d0\u5347\u5206\u6790\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u5728\u9762\u5bf9\u8986\u76d6\u7387\u74f6\u9888\u65f6\uff0c\u4f9d\u8d56\u4eba\u5de5\u5206\u6790\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u8f85\u52a9\u5de5\u5177\u6765\u63d0\u5347\u5206\u6790\u6548\u7387\u3002", "method": "InsightQL\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u5e93\u548c\u76f4\u89c2\u7684\u53c2\u6570\u5316\u67e5\u8be2\u63a5\u53e3\uff0c\u7cfb\u7edf\u5316\u5730\u63d0\u53d6\u5206\u6790\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u963b\u585e\u70b9\u3002", "result": "\u5728FuzzBench\u57fa\u51c6\u6d4b\u8bd5\u768414\u4e2a\u6d41\u884c\u5e93\u4e0a\uff0cInsightQL\u6210\u529f\u89e3\u9664\u4e86\u591a\u4e2a\u6a21\u7cca\u963b\u585e\u70b9\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u6700\u9ad8\u63d0\u5347\u4e8613.90%\u3002", "conclusion": "InsightQL\u4f5c\u4e3a\u4e00\u79cd\u8f85\u52a9\u6846\u67b6\uff0c\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u5206\u6790\u548c\u89e3\u51b3\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u963b\u585e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u8986\u76d6\u7387\u3002"}}
{"id": "2510.04585", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04585", "abs": "https://arxiv.org/abs/2510.04585", "authors": ["Jianshu Zhou", "Jing Shu", "Tianle Pan", "Puchen Zhu", "Jiajun An", "Huayu Zhang", "Junda Huang", "Upinder Kaur", "Xin Ma", "Masayoshi Tomizuka"], "title": "Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation", "comment": "19 pages, 10 figures, journal", "summary": "Grasping objects across vastly different sizes and physical states-including\nboth solids and liquids-with a single robotic gripper remains a fundamental\nchallenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a\nsoft end-effector that synergistically integrates distributed surface suction\nwith internal granular jamming, enabling cross-scale and cross-state\nmanipulation without requiring airtight sealing at the contact interface with\ntarget objects. The EG Gripper can handle objects with surface areas ranging\nfrom sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized\npaper and woven bag), enabling manipulation of objects nearly 3,500X smaller\nand 88X larger than its own contact area (approximated at 707 mm2 for a 30\nmm-diameter base). We further introduce a tactile sensing framework that\ncombines liquid detection and pressure-based suction feedback, enabling\nreal-time differentiation between solid and liquid targets. Guided by the\nactile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper\nautonomously selects grasping modes based on distributed pressure and voltage\nsignals. Experiments across diverse tasks-including underwater grasping,\nfragile object handling, and liquid capture-demonstrate robust and repeatable\nperformance. To our knowledge, this is the first soft gripper to reliably grasp\nboth solid and liquid objects across scales using a unified compliant\narchitecture.", "AI": {"tldr": "EG Gripper\u7ed3\u5408\u5438\u9644\u4e0e\u9897\u7c92\u963b\u585e\u6280\u672f\uff0c\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u3001\u8de8\u72b6\u6001\u7269\u4f53\u6293\u53d6\uff0c\u65e0\u9700\u6c14\u5bc6\u5bc6\u5c01\uff0c\u9996\u6b21\u7edf\u4e00\u6293\u53d6\u56fa\u4f53\u548c\u6db2\u4f53\u3002", "motivation": "\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u4e2d\u5355\u4e00\u5939\u5177\u96be\u4ee5\u6293\u53d6\u4e0d\u540c\u5927\u5c0f\u548c\u7269\u7406\u72b6\u6001\uff08\u56fa\u4f53\u548c\u6db2\u4f53\uff09\u7269\u4f53\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u8868\u9762\u5438\u9644\u4e0e\u5185\u90e8\u9897\u7c92\u963b\u585e\u6280\u672f\uff0c\u7ed3\u5408\u89e6\u89c9\u611f\u5e94\u6846\u67b6\u548cTIGMS\u7b97\u6cd5\uff0c\u5b9e\u73b0\u81ea\u4e3b\u9009\u62e9\u6293\u53d6\u6a21\u5f0f\u3002", "result": "EG Gripper\u80fd\u591f\u6293\u53d6\u4ece0.2 mm\u00b2\u523062,000 mm\u00b2\u7684\u7269\u4f53\uff0c\u5e76\u6210\u529f\u533a\u5206\u56fa\u4f53\u548c\u6db2\u4f53\u76ee\u6807\uff0c\u5c55\u793a\u4e86\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "EG Gripper\u901a\u8fc7\u7ed3\u5408\u5206\u5e03\u5f0f\u8868\u9762\u5438\u9644\u4e0e\u5185\u90e8\u9897\u7c92\u963b\u585e\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u8de8\u5c3a\u5ea6\u548c\u8de8\u72b6\u6001\u7684\u7269\u4f53\u6293\u53d6\uff0c\u65e0\u9700\u6c14\u5bc6\u5bc6\u5c01\uff0c\u5c55\u793a\u4e86\u5728\u8f6f\u673a\u5668\u4eba\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\u3002"}}
{"id": "2510.04141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001AI\u8bc4\u4f30\u7684\u6f14\u53d8\uff0c\u4ece\u7b80\u5355\u8bc6\u522b\u5230\u590d\u6742\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u5982\u4f55\u63a8\u52a8\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001AI\u8bc4\u4f30\u4ece\u7b80\u5355\u7684\u8bc6\u522b\u4efb\u52a1\u5411\u590d\u6742\u63a8\u7406\u57fa\u51c6\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ee5\u63ed\u793a\u6a21\u578b\u5728\u7406\u89e3\u201c\u4e3a\u4ec0\u4e48\u201d\u548c\u201c\u5982\u4f55\u201d\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8c03\u67e5\u591a\u6a21\u6001AI\u8bc4\u4f30\u7684\u6f14\u53d8\uff0c\u5c06\u5176\u89c6\u4e3a\u4e00\u7cfb\u5217\u65e5\u76ca\u590d\u6742\u7684\u201c\u8ba4\u77e5\u6d4b\u8bd5\u201d\u7684\u8fdb\u5c55\u3002", "result": "\u4eceImageNet\u65f6\u4ee3\u7684\u201c\u77e5\u8bc6\u6d4b\u8bd5\u201d\u5230GQA\u548cVCR\u7b49\u201c\u5e94\u7528\u903b\u8f91\u4e0e\u7406\u89e3\u201d\u6d4b\u8bd5\uff0c\u518d\u5230\u9488\u5bf9MLLMs\u7684\u201c\u4e13\u5bb6\u7ea7\u96c6\u6210\u201d\u57fa\u51c6\uff08\u5982MMBench\u3001SEED-Bench\u3001MMMU\uff09\uff0c\u5c55\u793a\u4e86\u8bc4\u4f30\u7684\u4e0d\u65ad\u8fdb\u5316\u3002", "conclusion": "AI\u8bc4\u4f30\u7684\u53d9\u4e8b\u4e0d\u4ec5\u662f\u6570\u636e\u96c6\u7684\u5386\u53f2\uff0c\u800c\u662f\u4e00\u4e2a\u6301\u7eed\u5bf9\u6297\u7684\u8fc7\u7a0b\uff0c\u65e8\u5728\u8bbe\u8ba1\u66f4\u597d\u7684\u6d4b\u8bd5\uff0c\u4ece\u800c\u91cd\u65b0\u5b9a\u4e49\u6211\u4eec\u521b\u5efa\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\u3002"}}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.", "AI": {"tldr": "FreshBrew\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728Java\u9879\u76ee\u8fc1\u79fb\u4e2d\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0cGemini 2.5 Flash\u8868\u73b0\u6700\u4f73\uff0c\u8fc1\u79fb\u6210\u529f\u7387\u8fbe52.3%\u3002", "motivation": "\u8bc4\u4f30AI\u9a71\u52a8\u7684\u4ee3\u7406\u6846\u67b6\u5728\u9879\u76ee\u7ea7Java\u8fc1\u79fb\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u6d4b\u91cf\u5176\u4fdd\u7559\u7a0b\u5e8f\u8bed\u4e49\u548c\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165FreshBrew\u57fa\u51c6\uff0c\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u5de5\u5177\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578bGemini 2.5 Flash\u6210\u529f\u5c0652.3%\u7684\u9879\u76ee\u8fc1\u79fb\u81f3JDK 17\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03FreshBrew\u57fa\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e25\u8c28\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\uff0c\u5e76\u63a8\u52a8AI\u9a71\u52a8\u4ee3\u7801\u5e93\u73b0\u4ee3\u5316\u7684\u8fdb\u5c55\u3002"}}
{"id": "2510.04592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04592", "abs": "https://arxiv.org/abs/2510.04592", "authors": ["Yilin Mei", "Peng Qiu", "Wei Zhang", "WenChao Zhang", "Wenjie Song"], "title": "MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation", "comment": null, "summary": "Recent advances in robotics have been largely driven by imitation learning,\nwhich depends critically on large-scale, high-quality demonstration data.\nHowever, collecting such data remains a significant challenge-particularly for\nmobile manipulators, which must coordinate base locomotion and arm manipulation\nin high-dimensional, dynamic, and partially observable environments.\nConsequently, most existing research remains focused on simpler tabletop\nscenarios, leaving mobile manipulation relatively underexplored. To bridge this\ngap, we present \\textit{MobRT}, a digital twin-based framework designed to\nsimulate two primary categories of complex, whole-body tasks: interaction with\narticulated objects (e.g., opening doors and drawers) and mobile-base\npick-and-place operations. \\textit{MobRT} autonomously generates diverse and\nrealistic demonstrations through the integration of virtual kinematic control\nand whole-body motion planning, enabling coherent and physically consistent\nexecution. We evaluate the quality of \\textit{MobRT}-generated data across\nmultiple baseline algorithms, establishing a comprehensive benchmark and\ndemonstrating a strong correlation between task success and the number of\ngenerated trajectories. Experiments integrating both simulated and real-world\ndemonstrations confirm that our approach markedly improves policy\ngeneralization and performance, achieving robust results in both simulated and\nreal-world environments.", "AI": {"tldr": "\\textit{MobRT} \u662f\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u6f14\u793a\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u68b0\u81c2\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6f14\u793a\u6570\u636e\uff0c\u4f46\u6536\u96c6\u8fd9\u4e9b\u6570\u636e\u5bf9\u4e8e\u79fb\u52a8\u673a\u68b0\u81c2\u5c24\u4e3a\u56f0\u96be\uff0c\u5bfc\u81f4\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7b80\u5355\u7684\u684c\u9762\u573a\u666f\uff0c\u79fb\u52a8\u64cd\u4f5c\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\\textit{MobRT} \u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u8fd0\u52a8\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u81ea\u4e3b\u751f\u6210\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u6f14\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\\textit{MobRT} \u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u751f\u6210\u7684\u8f68\u8ff9\u6570\u91cf\u5f3a\u76f8\u5173\uff0c\u4e14\u4eff\u771f\u548c\u73b0\u5b9e\u6f14\u793a\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\\textit{MobRT} \u6846\u67b6\u901a\u8fc7\u6a21\u62df\u590d\u6742\u4efb\u52a1\u548c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u6f14\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5747\u53d6\u5f97\u4e86\u7a33\u5065\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.04173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments.", "AI": {"tldr": "Agent Spec \u662f\u4e00\u79cd\u8de8\u6846\u67b6\u7684\u58f0\u660e\u5f0f\u8bed\u8a00\uff0c\u901a\u8fc7\u7edf\u4e00\u89c4\u8303\u63d0\u5347AI\u4ee3\u7406\u7684\u53ef\u79fb\u690d\u6027\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u4f7f\u591a\u65b9\u53d7\u76ca\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u89c4\u8303\u4ee5\u63d0\u5347\u53ef\u79fb\u690d\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "Agent Spec \u662f\u4e00\u79cd\u58f0\u660e\u5f0f\u8bed\u8a00\uff0c\u5141\u8bb8\u5728\u4e0d\u540cAI\u6846\u67b6\u4e2d\u5b9a\u4e49AI\u4ee3\u7406\u53ca\u5176\u5de5\u4f5c\u6d41\u3002", "result": "Agent Spec \u4f7f\u5f00\u53d1\u8005\u3001\u6846\u67b6\u5de5\u5177\u5f00\u53d1\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u4f01\u4e1a\u5747\u53d7\u76ca\uff0c\u5305\u62ec\u53ef\u91cd\u7528\u7ec4\u4ef6\u3001\u5de5\u5177\u652f\u6301\u3001\u53ef\u91cd\u590d\u7ed3\u679c\u548c\u5feb\u901f\u90e8\u7f72\u7b49\u3002", "conclusion": "Agent Spec \u63d0\u4f9b\u4e86\u4e00\u79cd\u8de8\u6846\u67b6\u7684\u7edf\u4e00\u89c4\u8303\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5f00\u53d1\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u53ef\u79fb\u690d\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2510.04905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04905", "abs": "https://arxiv.org/abs/2510.04905", "authors": ["Yicheng Tao", "Yao Qin", "Yepang Liu"], "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches", "comment": null, "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\uff08RACG\uff09\u7684\u7814\u7a76\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u4ed3\u5e93\u7ea7\u522b\u7684\u65b9\u6cd5\uff0c\u5206\u7c7b\u4e86\u73b0\u6709\u5de5\u4f5c\uff0c\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u51fd\u6570\u7ea7\u522b\u548c\u6587\u4ef6\u7ea7\u522b\u7684\u4ee3\u7801\u751f\u6210\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u8f6f\u4ef6\u5f00\u53d1\u901a\u5e38\u9700\u8981\u8de8\u6574\u4e2a\u4ed3\u5e93\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u5f15\u53d1\u4e86\u4ed3\u5e93\u7ea7\u522b\u4ee3\u7801\u751f\u6210\uff08RLCG\uff09\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u80fd\u591f\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3001\u786e\u4fdd\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u751f\u6210\u8de8\u591a\u4e2a\u6587\u4ef6\u6216\u6a21\u5757\u7684\u8fde\u8d2f\u4ee3\u7801\u3002", "method": "\u672c\u6587\u5bf9\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\uff08RACG\uff09\u7684\u7814\u7a76\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u7279\u522b\u662f\u5728\u4ed3\u5e93\u7ea7\u522b\u7684\u65b9\u6cd5\u4e0a\u3002\u6587\u7ae0\u5bf9\u73b0\u6709\u5de5\u4f5c\u8fdb\u884c\u4e86\u591a\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\uff0c\u5305\u62ec\u751f\u6210\u7b56\u7565\u3001\u68c0\u7d22\u65b9\u5f0f\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u6587\u7ae0\u603b\u7ed3\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u5f53\u524d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u548c\u673a\u9047\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u5feb\u901f\u53d1\u5c55\u7684\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u9886\u57df\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u6fc0\u53d1\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6301\u7eed\u8fdb\u6b65\u3002"}}
{"id": "2510.04612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04612", "abs": "https://arxiv.org/abs/2510.04612", "authors": ["Simon Boche", "Jaehyung Jung", "Sebasti\u00e1n Barbas Laina", "Stefan Leutenegger"], "title": "OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS", "comment": "IEEE Transactions on Robotics (T-RO) - Special Issue: Visual SLAM", "summary": "To empower mobile robots with usable maps as well as highest state estimation\naccuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor\nSimultaneous Localization and Mapping (SLAM) system building dense volumetric\noccupancy maps, while scalable to large environments and operating in realtime.\nOur unified SLAM framework seamlessly integrates different sensor modalities:\nvisual, inertial, measured or learned depth, LiDAR and Global Navigation\nSatellite System (GNSS) measurements. Unlike most state-of-the-art SLAM\nsystems, we advocate using dense volumetric map representations when leveraging\ndepth or range-sensing capabilities. We employ an efficient submapping strategy\nthat allows our system to scale to large environments, showcased in sequences\nof up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by\ntightly-coupling the estimator and submaps through map alignment factors. Our\nsystem provides globally consistent maps, directly usable for autonomous\nnavigation. To further improve the accuracy of OKVIS2-X, we also incorporate\nthe option of performing online calibration of camera extrinsics. Our system\nachieves the highest trajectory accuracy in EuRoC against state-of-the-art\nalternatives, outperforms all competitors in the Hilti22 VI-only benchmark,\nwhile also proving competitive in the LiDAR version, and showcases state of the\nart accuracy in the diverse and large-scale sequences from the VBR dataset.", "AI": {"tldr": "OKVIS2-X\u662f\u4e00\u4e2a\u591a\u4f20\u611f\u5668SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bc6\u96c6\u4f53\u79ef\u5730\u56fe\u548c\u9ad8\u6548\u5b50\u5730\u56fe\u7b56\u7565\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u73af\u5883\u3002", "motivation": "\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u7684\u5168\u5c40\u4e00\u81f4\u5730\u56fe\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u5b50\u5730\u56fe\u7b56\u7565\u548c\u7d27\u5bc6\u8026\u5408\u7684\u4f30\u8ba1\u5668\u4e0e\u5b50\u5730\u56fe\uff0c\u901a\u8fc7\u5730\u56fe\u5bf9\u9f50\u56e0\u5b50\u63d0\u5347\u7cbe\u5ea6\uff0c\u652f\u6301\u591a\u4f20\u611f\u5668\uff08\u89c6\u89c9\u3001\u60ef\u6027\u3001\u6df1\u5ea6\u3001LiDAR\u3001GNSS\uff09\u96c6\u6210\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u5728Hilti22 VI-only\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u5e76\u5728VBR\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u5e8f\u5217\u4e2d\u5c55\u793a\u4e86\u5148\u8fdb\u7cbe\u5ea6\u3002", "conclusion": "OKVIS2-X\u662f\u4e00\u4e2a\u5148\u8fdb\u7684SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u5bc6\u96c6\u7684\u4f53\u79ef\u5360\u7528\u5730\u56fe\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884c\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8f68\u8ff9\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faLLM\u9a71\u52a8\u7684\u5730\u56fe\u6784\u5efa\u4e0e\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7248\u672c\u63a7\u5236\u548c\u8fb9\u7f18\u5f71\u54cd\u8bc4\u5206\u63d0\u5347\u5730\u56fe\u6b63\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u73af\u5883\u89c4\u6a21\u6269\u5927\uff0c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u67e5\u8be2\u80fd\u529b\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u9010\u6b65\u6784\u5efa\u5b8c\u6574\u62d3\u6251\u56fe\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5730\u56fe\u6784\u5efa\u548c\u4fee\u590d\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u7248\u672c\u63a7\u5236\uff08\u8bb0\u5f55\u5b8c\u6574\u7684\u56fe\u7f16\u8f91\u5386\u53f2\u53ca\u5176\u6765\u6e90\u89c2\u5bdf\uff09\uff0c\u5e76\u5f15\u5165\u8fb9\u7f18\u5f71\u54cd\u8bc4\u5206\uff08\u57fa\u4e8e\u7ed3\u6784\u53ef\u8fbe\u6027\u3001\u8def\u5f84\u4f7f\u7528\u548c\u51b2\u7a81\u4f20\u64ad\uff09\u4ee5\u4f18\u5148\u4f4e\u6210\u672c\u4fee\u590d\u3002", "result": "\u5728MANGO\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u540e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u5b58\u5728\u590d\u6742\u6216\u94fe\u5f0f\u4e0d\u4e00\u81f4\u6027\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u81ea\u7701\u548c\u5386\u53f2\u611f\u77e5\u4fee\u590d\u673a\u5236\u5728\u7ef4\u62a4LLM\u4ee3\u7406\u7a7a\u95f4\u8bb0\u5fc6\u8fde\u8d2f\u6027\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04964", "abs": "https://arxiv.org/abs/2510.04964", "authors": ["Kelechi G. Kalu", "James C. Davis"], "title": "Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain", "comment": "8 Pages, 3 Figures", "summary": "Software signing provides a formal mechanism for provenance by ensuring\nartifact integrity and verifying producer identity. It also imposes tooling and\noperational costs to implement in practice. In an era of centralized registries\nsuch as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask\nwhether hardening registry security controls obviates the need for end-to-end\nartifact signing. In this work, we posit that the core guarantees of signing,\nprovenance, integrity, and accountability are not automatically carried across\ndifferent software distribution boundaries. These boundaries include mirrors,\ncorporate proxies, re-hosting, and air-gapped transfers, where registry\nsecurity controls alone cannot provide sufficient assurance. We synthesize\nhistorical practice and present a trust model for modern distribution modes to\nidentify when signing is necessary to extend trust beyond registry control.\nTreating signing as a baseline layer of defense strengthens software supply\nchain assurance even when registries are secure.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b7e\u540d\u5728\u4e0d\u540c\u8f6f\u4ef6\u5206\u53d1\u8fb9\u754c\u4e2d\u4ecd\u5fc5\u4e0d\u53ef\u5c11\uff0c\u5373\u4f7f\u6ce8\u518c\u8868\u5b89\u5168\uff0c\u7b7e\u540d\u4e5f\u80fd\u589e\u5f3a\u4f9b\u5e94\u94fe\u4fdd\u969c\u3002", "motivation": "\u63a2\u8ba8\u5728\u96c6\u4e2d\u5f0f\u6ce8\u518c\u8868\uff08\u5982PyPI\u3001npm\u7b49\uff09\u65f6\u4ee3\uff0c\u5f3a\u5316\u6ce8\u518c\u8868\u5b89\u5168\u63a7\u5236\u662f\u5426\u6d88\u9664\u4e86\u7aef\u5230\u7aef\u5de5\u4ef6\u7b7e\u540d\u7684\u5fc5\u8981\u6027\u3002", "method": "\u7efc\u5408\u5386\u53f2\u5b9e\u8df5\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u4efb\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u5728\u4e0d\u540c\u5206\u53d1\u6a21\u5f0f\u4e0b\u4f55\u65f6\u9700\u8981\u7b7e\u540d\u4ee5\u6269\u5c55\u4fe1\u4efb\u3002", "result": "\u7b7e\u540d\u5728\u8de8\u8d8a\u4e0d\u540c\u8f6f\u4ef6\u5206\u53d1\u8fb9\u754c\uff08\u5982\u955c\u50cf\u3001\u4f01\u4e1a\u4ee3\u7406\u7b49\uff09\u65f6\uff0c\u5176\u6838\u5fc3\u4fdd\u8bc1\uff08\u6765\u6e90\u3001\u5b8c\u6574\u6027\u548c\u8d23\u4efb\uff09\u4e0d\u4f1a\u81ea\u52a8\u4f20\u9012\u3002", "conclusion": "\u7b7e\u540d\u4f5c\u4e3a\u9632\u5fa1\u7684\u57fa\u7840\u5c42\uff0c\u5373\u4f7f\u5728\u6ce8\u518c\u8868\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u589e\u5f3a\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u7684\u4fdd\u969c\u3002"}}
{"id": "2510.04692", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04692", "abs": "https://arxiv.org/abs/2510.04692", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies", "comment": null, "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u751f\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7\u6570\u5b57\u5236\u9020\u548c\u667a\u80fd\u611f\u77e5\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e0e\u91ce\u751f\u9e1f\u7c7b\u81ea\u7136\u4ea4\u4e92\uff0c\u4e3a\u751f\u6001\u7814\u7a76\u548c\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u91ce\u751f\u9e1f\u7c7b\u884c\u4e3a\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u9ad8\u5ea6\u903c\u771f\u7684\u5f62\u6001\u3001\u8010\u7528\u7684\u6237\u5916\u64cd\u4f5c\u548c\u80fd\u9002\u5e94\u975e\u53d7\u63a7\u73af\u5883\u7684\u667a\u80fd\u611f\u77e5\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u51493D\u626b\u63cf\u3001\u53c2\u6570\u5316CAD\u5efa\u6a21\u3001\u5173\u8282\u5f0f3D\u6253\u5370\u548c\u903c\u771f\u7684UV\u7eb9\u7406\u4e59\u70ef\u57fa\u9970\u9762\uff0c\u6253\u9020\u4e86\u89e3\u5256\u5b66\u4e0a\u7cbe\u786e\u4e14\u8010\u7528\u7684\u673a\u5668\u4eba\u66ff\u4ee3\u54c1\u3002\u516d\u8f6e\u6447\u81c2\u8f6c\u5411\u67b6\u5e95\u76d8\u786e\u4fdd\u5728\u6c99\u5730\u548c\u4e0d\u89c4\u5219\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u79fb\u52a8\uff0c\u5d4c\u5165\u5f0fNVIDIA Jetson\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6RGB\u548c\u70ed\u611f\u77e5\uff0c\u8f7b\u91cf\u7ea7YOLO\u68c0\u6d4b\u4ee5\u53ca\u81ea\u4e3b\u89c6\u89c9\u4f3a\u670d\u5faa\u73af\u3002", "result": "\u6c99\u6f20\u9e1f\u820d\u7684\u73b0\u573a\u8bd5\u9a8c\u8bc1\u5b9e\uff0c\u8be5\u5e73\u53f0\u572815\u81f322 FPS\u4e0b\u53ef\u9760\u8fd0\u884c\uff0c\u5ef6\u8fdf\u4f4e\u4e8e100\u6beb\u79d2\uff0c\u5e76\u80fd\u5f15\u53d1\u6d3b\u4f53Houbara bustard\u5728\u6076\u52a3\u6237\u5916\u6761\u4ef6\u4e0b\u7684\u81ea\u7136\u8bc6\u522b\u548c\u4ea4\u4e92\u53cd\u5e94\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53ef\u590d\u5236\u7684\u6570\u5b57\u5236\u9020\u3001\u5177\u8eab\u89c6\u89c9\u667a\u80fd\u548c\u751f\u6001\u9a8c\u8bc1\uff0c\u4e3a\u52a8\u7269\u673a\u5668\u4eba\u4ea4\u4e92\u7814\u7a76\u3001\u4fdd\u62a4\u673a\u5668\u4eba\u548c\u516c\u4f17\u53c2\u4e0e\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2510.04196", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04196", "abs": "https://arxiv.org/abs/2510.04196", "authors": ["Yizhuo Ding", "Mingkang Chen", "Qiuhua Liu", "Fenghua Weng", "Wanying Qu", "Yue Yang", "Yugang Jiang", "Zuxuan Wu", "Yanwei Fu", "Wenqi Shao"], "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "comment": null, "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications,\nwhere they must be both useful and safe. Safety is especially challenging in\nmultimodal settings: images and text can be combined to bypass guardrails, and\nsingle objective training can cause policy drift that yields over-refusal on\nbenign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed\nreinforcement learning framework that trains reasoning oriented LMRMs under\nmultimodal, multitask, and multiobjective signals, and we release the resulting\nmodel, COSMO-R1. Our approach aims to let safety and capability grow together\nin one stable pipeline rather than competing during alignment. In experiments,\nCOSMO-R1 improves safety while maintaining-and often improving multimodal\nreasoning and instruction following, shows stronger robustness to multimodal\njailbreaks, and reduces unnecessary refusals. The framework also transfers\nacross backbones with consistent gains. Ablations support the design choices,\nindicating a simple path to advancing safety and general capability together in\nLMRMs.", "AI": {"tldr": "COSMO-RL\u6846\u67b6\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u51fa\u5b89\u5168\u4e14\u80fd\u529b\u5f3a\u7684\u591a\u6a21\u6001\u6a21\u578bCOSMO-R1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b89\u5168\u6027\u6311\u6218\uff08\u5982\u7ed5\u8fc7\u9632\u62a4\u673a\u5236\u3001\u7b56\u7565\u6f02\u79fb\uff09\uff0c\u5e76\u786e\u4fdd\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u540c\u6b65\u63d0\u5347\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08COSMO-RL\uff09\uff0c\u5728\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u3001\u591a\u76ee\u6807\u7684\u4fe1\u53f7\u4e0b\u8bad\u7ec3LMRMs\u3002", "result": "COSMO-R1\u6a21\u578b\u5728\u5b89\u5168\u6027\u3001\u591a\u6a21\u6001\u63a8\u7406\u3001\u6307\u4ee4\u9075\u5faa\u53ca\u6297\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u62d2\u7edd\u3002", "conclusion": "COSMO-RL\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u3001\u591a\u76ee\u6807\u7684\u5f3a\u5316\u5b66\u4e60\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u7684\u5171\u540c\u63d0\u5347\uff0c\u4e3aLMRMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u8def\u5f84\u3002"}}
{"id": "2510.04982", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04982", "abs": "https://arxiv.org/abs/2510.04982", "authors": ["Aakash Ahmad", "Muhammad Waseem", "Bakheet Aljedaani", "Mahdi Fahmideh", "Peng Liang", "Feras Awaysheh"], "title": "Quantum Computing as a Service - a Software Engineering Perspective", "comment": "37 pages, 10 images, 5 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6620\u5c04\u548c\u67b6\u6784\u5f00\u53d1\uff0c\u63d0\u51fa\u4e86\u91cf\u5b50\u8ba1\u7b97\u5373\u670d\u52a1\uff08QCaaS\uff09\u76844\u9636\u6bb5\u5f00\u53d1\u751f\u547d\u5468\u671f\u548c\u53c2\u8003\u67b6\u6784\uff0c\u4e3a\u91cf\u5b50\u670d\u52a1\u5de5\u7a0b\u63d0\u4f9b\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4f5c\u4e3a\u4e00\u79cd\u98a0\u8986\u6027\u6280\u672f\uff0c\u5176\u5373\u670d\u52a1\u6a21\u5f0f\uff08QCaaS\uff09\u53ef\u4ee5\u4e3a\u4e0d\u5177\u5907\u91cf\u5b50\u8ba1\u7b97\u673a\u8d44\u6e90\u7684\u4e2a\u4eba\u548c\u7ec4\u7ec7\u63d0\u4f9b\u5b9e\u7528\u8ba1\u7b97\u670d\u52a1\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7814\u7a76\u65b9\u6cd5\uff0c\u5305\u62ec\u7cfb\u7edf\u6027\u6620\u5c04\u7814\u7a76\uff08SMS\uff09\u548c\u57fa\u4e8e\u67b6\u6784\u7684\u5f00\u53d1\uff0c\u4ee5\u8bc6\u522b\u91cf\u5b50\u670d\u52a1\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u9636\u6bb5\u5e76\u6574\u5408\u5230\u53c2\u8003\u67b6\u6784\u4e2d\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e8641\u9879\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b4\u9636\u6bb5\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u53c2\u8003\u67b6\u6784\uff0c\u6db5\u76d6\u4e86\u91cf\u5b50\u91cd\u8981\u9700\u6c42\uff08QSRs\uff09\u3001\u5efa\u6a21\u7b26\u53f7\u3001\u6a21\u5f0f\u76ee\u5f55\u3001\u7f16\u7a0b\u8bed\u8a00\u548c\u90e8\u7f72\u5e73\u53f0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u3001\u67b6\u6784\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5373\u670d\u52a1\uff08QCaaS\uff09\u63d0\u4f9b\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7684\u89c6\u89d2\uff0c\u5305\u62ec\u4e00\u4e2a4\u9636\u6bb5\u7684\u5f00\u53d1\u751f\u547d\u5468\u671f\u548c\u5206\u5c42\u53c2\u8003\u67b6\u6784\u3002"}}
{"id": "2510.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04696", "abs": "https://arxiv.org/abs/2510.04696", "authors": ["Alexander L. Mitchell", "Joe Watson", "Ingmar Posner"], "title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "comment": "8 pages, 6 figures, 1 table", "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u81ea\u52a8\u751f\u6210\u5b50\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u53cc\u673a\u68b0\u81c2\u88c5\u914d\u4e2d\u7684\u5feb\u901f\u91cd\u65b0\u89c4\u5212\u548c\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u53cc\u673a\u68b0\u81c2\u88c5\u914d\u4e2d\u5b58\u5728\u9ad8\u5c42\u6b21\u7684\u5e8f\u5217\u89c4\u5212\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u4f4e\u5c42\u6b21\u7684\u63a5\u89e6\u64cd\u4f5c\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u65b9\u6cd5\u5728\u5e94\u5bf9\u9700\u8981\u65b0\u4efb\u52a1\u5e8f\u5217\u548c\u4f18\u5316\u7684\u6270\u52a8\u65f6\u6536\u655b\u901f\u5ea6\u6162\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u5229\u7528\u5206\u6bb5\u8fde\u7eed\u80fd\u91cf\u51fd\u6570\u548c\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\uff0c\u901a\u8fc7\u77ed\u89c6\u4f18\u5316\u751f\u6210\u5b50\u76ee\u6807\uff0c\u800c\u975e\u957f\u65f6\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7269\u7406\u53cc\u673a\u68b0\u81c2\u88c5\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u91cd\u8bd5\u3001\u534f\u8c03\u52a8\u4f5c\u548c\u81ea\u4e3b\u4ea4\u63a5\uff0c\u9002\u7528\u4e8e\u7d27\u516c\u5dee\u88c5\u914d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\u751f\u6210\u5b50\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u673a\u68b0\u81c2\u88c5\u914d\u4e2d\u7684\u5feb\u901f\u91cd\u65b0\u89c4\u5212\u548c\u534f\u8c03\u95ee\u9898\u3002"}}
{"id": "2510.04206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "AI": {"tldr": "AgentRL\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8f6e\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u7ba1\u9053\u548c\u7edf\u4e00API\u63d0\u5347\u6548\u7387\uff0c\u7b97\u6cd5\u4f18\u5316\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LLM\u4ee3\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u3001\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9762\u4e34\u57fa\u7840\u8bbe\u65bd\u548c\u8bad\u7ec3\u7b97\u6cd5\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "AgentRL\u91c7\u7528\u5b8c\u5168\u5f02\u6b65\u7684\u751f\u6210-\u8bad\u7ec3\u7ba1\u9053\u8fdb\u884c\u9ad8\u6548\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u57fa\u4e8e\u51fd\u6570\u8c03\u7528\u7684API\u63a5\u53e3\u3001\u5bb9\u5668\u5316\u73af\u5883\u5f00\u53d1\u548c\u96c6\u4e2d\u63a7\u5236\u5668\uff0c\u4ee5\u652f\u6301\u591a\u4efb\u52a1RL\u4e2d\u7684\u5f02\u6784\u73af\u5883\u5f00\u53d1\u3002\u7b97\u6cd5\u4e0a\uff0c\u63d0\u51fa\u4e86\u8de8\u7b56\u7565\u91c7\u6837\u548c\u591a\u4efb\u52a1\u4f18\u52bf\u5f52\u4e00\u5316\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentRL\u5728\u4e94\u4e2a\u4ee3\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGPT-5\u3001Claude-Sonnet-4\u3001DeepSeek-R1\u7b49\u5f00\u6e90LLM\u4ee3\u7406\uff0c\u591a\u4efb\u52a1\u8bad\u7ec3\u6548\u679c\u4e0e\u6700\u4f73\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "AgentRL\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2510.04997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86LLMs\u5728\u8f6f\u4ef6\u6545\u969c\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u548c\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u6545\u969c\u5206\u6790\u867d\u7136\u6709\u4ef7\u503c\uff0c\u4f46\u6d89\u53ca\u591a\u4e2a\u4e13\u5bb6\u9a71\u52a8\u7684\u6b65\u9aa4\uff0c\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6545\u969c\u7814\u7a76\u7684\u5f00\u5c55\u548c\u8fed\u4ee3\u5b9e\u8bc1\u7814\u7a76\u7684\u8fdb\u5ea6\u3002", "method": "\u5c06\u5b9e\u8bc1\u8f6f\u4ef6\u6545\u969c\u7814\u7a76\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\uff081\uff09\u7814\u7a76\u76ee\u6807\u5b9a\u4e49\uff0c\uff082\uff09\u6570\u636e\u51c6\u5907\uff0c\uff083\uff09\u6545\u969c\u5206\u6790\uff0c\u5e76\u63a2\u7d22\u4e86\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5f00\u6e90\u8f6f\u4ef6\u6545\u969c\u5206\u6790\u7684\u521d\u6b65\u7814\u7a76\u3002", "result": "\u8bc4\u4f30\u4e86\u6765\u81ea\u9ad8\u8d28\u91cf\u5b9e\u8bc1\u7814\u7a76\u76843,829\u4e2a\u8f6f\u4ef6\u6545\u969c\uff0c\u7ed3\u679c\u663e\u793aLLMs\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6545\u969c\u5206\u6790\u6548\u7387\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f4\u7ea6\u4e3a\u4e24\u5c0f\u65f6\uff0c\u800c\u4f20\u7edf\u624b\u52a8\u5206\u6790\u901a\u5e38\u9700\u8981\u6570\u5468\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8be6\u7ec6\u7684\u7814\u7a76\u8ba1\u5212\uff0c\u5f3a\u8c03\u4e86LLMs\u5728\u63a8\u8fdb\u5b9e\u8bc1\u6545\u969c\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u8f6f\u4ef6\u6545\u969c\u5206\u6790\u6240\u9700\u89e3\u51b3\u7684\u5f00\u653e\u6027\u6311\u6218\u3002"}}
{"id": "2510.04724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04724", "abs": "https://arxiv.org/abs/2510.04724", "authors": ["Etor Arza", "Welf Rehberg", "Philipp Weiss", "Mihir Kulkarni", "Kostas Alexis"], "title": "Performance-guided Task-specific Optimization for Multirotor Design", "comment": null, "summary": "This paper introduces a methodology for task-specific design optimization of\nmultirotor Micro Aerial Vehicles. By leveraging reinforcement learning,\nBayesian optimization, and covariance matrix adaptation evolution strategy, we\noptimize aerial robot designs guided exclusively by their closed-loop\nperformance in a considered task. Our approach systematically explores the\ndesign space of motor pose configurations while ensuring manufacturability\nconstraints and minimal aerodynamic interference. Results demonstrate that\noptimized designs achieve superior performance compared to conventional\nmultirotor configurations in agile waypoint navigation tasks, including against\nfully actuated designs from the literature. We build and test one of the\noptimized designs in the real world to validate the sim2real transferability of\nour approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4f18\u5316\u7b97\u6cd5\u7684\u5fae\u578b\u98de\u884c\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u7684\u8bbe\u8ba1\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u8d85\u8d8a\u4f20\u7edf\u914d\u7f6e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "motivation": "\u4f18\u5316\u5fae\u578b\u98de\u884c\u5668\u7684\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u654f\u6377\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u534f\u65b9\u5dee\u77e9\u9635\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff0c\u7cfb\u7edf\u63a2\u7d22\u4e86\u7535\u673a\u59ff\u6001\u914d\u7f6e\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u540c\u65f6\u8003\u8651\u5236\u9020\u7ea6\u675f\u548c\u6700\u5c0f\u5316\u7a7a\u6c14\u52a8\u529b\u5b66\u5e72\u6270\u3002", "result": "\u4f18\u5316\u8bbe\u8ba1\u5728\u654f\u6377\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u591a\u65cb\u7ffc\u914d\u7f6e\uff0c\u5305\u62ec\u6587\u732e\u4e2d\u7684\u5168\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u534f\u65b9\u5dee\u77e9\u9635\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u4f18\u5316\u4e86\u5fae\u578b\u98de\u884c\u5668\u7684\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04265", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "AI": {"tldr": "\u8d1d\u53f6\u65af\u6846\u67b6\u66ff\u4ee3Pass$@k$\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u3001\u900f\u660e\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u9650\u6837\u672c\u548c\u975e\u4e8c\u503c\u8bc4\u5206\u3002", "motivation": "Pass$@k$\u5728\u6709\u9650\u8bd5\u9a8c\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u8bef\u5bfc\u6027\u7684\u6392\u540d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u540e\u9a8c\u4f30\u8ba1\u548c\u53ef\u4fe1\u533a\u95f4\u4ee3\u66ffPass$@k$\u548c\u5e73\u5747\u51c6\u786e\u5ea6\uff0c\u901a\u8fc7Dirichlet\u5148\u9a8c\u5bf9\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u96c6\uff08AIME'24/'25, HMMT'25, BrUMO'25\uff09\u4e0a\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u6bd4Pass$@k$\u53ca\u5176\u53d8\u4f53\u66f4\u5feb\u6536\u655b\u3001\u6392\u540d\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u8bba\u6587\u5efa\u8bae\u5728\u540e\u9a8c\u6982\u7387\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u534f\u8bae\u66ff\u4ee3Pass$@k$\uff0c\u4ee5\u7edf\u4e00\u4e8c\u503c\u548c\u975e\u4e8c\u503c\u8bc4\u4f30\uff0c\u5e76\u660e\u786e\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.04774", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04774", "abs": "https://arxiv.org/abs/2510.04774", "authors": ["Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy", "comment": null, "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.", "AI": {"tldr": "SoNS\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u5e2e\u52a9\u673a\u5668\u4eba\u7fa4\u4f53\u5b8c\u6210\u4efb\u52a1\uff0c\u6210\u529f\u7387\u8fbe85%\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7fa4\u4f53\u884c\u4e3a\u8bbe\u8ba1\u590d\u6742\u6027\u548c\u5b9e\u65f6\u73af\u5883\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "method": "\u57286\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u548c\u8d85\u8fc730\u4e2a\u673a\u5668\u4eba\u7684\u6a21\u62df\u8bd5\u9a8c\u4e2d\uff0cSoNS\u589e\u5f3a\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u5361\u4f4f\u65f6\u81ea\u52a8\u8bf7\u6c42\u5e76\u8fd0\u884c\u7531\u5916\u90e8LLM\u751f\u6210\u7684\u4ee3\u7801\u3002", "result": "SoNS\u589e\u5f3a\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8685%\u7684\u6210\u529f\u7387\u3002", "conclusion": "SoNS\u4e3a\u673a\u5668\u4eba\u7fa4\u4f53\u63d0\u4f9b\u4e86\u884c\u4e3a\u8bbe\u8ba1\u7684\u4fbf\u6377\u6027\u548c\u5168\u5c40\u914d\u7f6e\u4f30\u8ba1\uff0c\u901a\u8fc7\u5916\u90e8LLM\u81ea\u52a8\u751f\u6210\u4ee3\u7801\uff0c\u5b9e\u73b0\u4e8685%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2510.04272", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u804c\u80fd\u534f\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76c8\u5229\u80fd\u529b\uff0c\u5e76\u4e0e\u7406\u8bba\u6a21\u578b\u7684\u7ba1\u7406\u89c1\u89e3\u4e00\u81f4\u3002", "motivation": "\u9762\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u7ec4\u7ec7\u590d\u6742\u6027\u548c\u89c4\u6a21\uff0c\u6709\u6548\u7684\u8de8\u804c\u80fd\u534f\u8c03\u5bf9\u4e8e\u63d0\u9ad8\u516c\u53f8\u6574\u4f53\u76c8\u5229\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u4eba\u5de5\u667a\u80fd\uff0c\u5c24\u5176\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u57fa\u672c\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u591a\u65f6\u95f4\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53RL\u67b6\u6784\uff0c\u5c06\u7b56\u7565\u7ec4\u4ef6\u6309\u90e8\u95e8\u804c\u80fd\u5206\u89e3\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u548c\u54cd\u5e94\u6027\u5206\u914d\u4e0d\u540c\u7684\u5b66\u4e60\u901f\u5ea6\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5b64\u7acb\u7684\u51b3\u7b56\u6846\u67b6\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76c8\u5229\u80fd\u529b\uff0c\u5e76\u4e14\u8bad\u7ec3\u540e\u7684RL\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u7406\u8bba\u6a21\u578b\u4e2d\u7684\u7ba1\u7406\u89c1\u89e3\u7d27\u5bc6\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u590d\u6742\u7684\u5546\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u804c\u80fd\u534f\u8c03\u3002"}}
{"id": "2510.04839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04839", "abs": "https://arxiv.org/abs/2510.04839", "authors": ["Shuo Sha", "Anupam Bhakta", "Zhenyuan Jiang", "Kevin Qiu", "Ishaan Mahajan", "Gabriel Bravo", "Brian Plancher"], "title": "TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation", "comment": null, "summary": "Accurate online inertial parameter estimation is essential for adaptive\nrobotic control, enabling real-time adjustment to payload changes,\nenvironmental interactions, and system wear. Traditional methods such as\nRecursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to\ntrack abrupt parameter shifts or incur high computational costs, limiting their\neffectiveness in dynamic environments and for computationally constrained\nrobotic systems. As such, we introduce TAG-K, a lightweight extension of the\nKaczmarz method that combines greedy randomized row selection for rapid\nconvergence with tail averaging for robustness under noise and inconsistency.\nThis design enables fast, stable parameter adaptation while retaining the low\nper-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K\nin synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other\nKaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class\nCPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More\nimportantly, these speedups are paired with improved resilience to measurement\nnoise and a 25% reduction in estimation error, leading to nearly 2x better\nend-to-end tracking performance.", "AI": {"tldr": "TAG-K \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7 Kaczmarz \u6269\u5c55\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d2a\u5a6a\u968f\u673a\u884c\u9009\u62e9\u548c\u5c3e\u90e8\u5e73\u5747\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u901f\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RLS\uff09\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08KF\uff09\u5728\u8ddf\u8e2a\u53c2\u6570\u7a81\u53d8\u6216\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "TAG-K \u662f\u4e00\u79cd\u57fa\u4e8e Kaczmarz \u65b9\u6cd5\u7684\u8f7b\u91cf\u7ea7\u6269\u5c55\uff0c\u7ed3\u5408\u8d2a\u5a6a\u968f\u673a\u884c\u9009\u62e9\u4ee5\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u5c3e\u90e8\u5e73\u5747\u589e\u5f3a\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u56db\u65cb\u7ffc\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0cTAG-K \u6bd4 RLS\u3001KF \u548c\u5176\u4ed6 Kaczmarz \u53d8\u4f53\u5feb 1.5x-1.9x\uff08\u7b14\u8bb0\u672c\u7535\u8111 CPU\uff09\u548c 4.8x-20.7x\uff08\u5d4c\u5165\u5f0f\u5fae\u63a7\u5236\u5668\uff09\uff0c\u540c\u65f6\u566a\u58f0\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c11 25%\uff0c\u7aef\u5230\u7aef\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u8fd1 2 \u500d\u3002", "conclusion": "TAG-K \u5728\u8f7b\u91cf\u7ea7\u6269\u5c55\u4e2d\u7ed3\u5408\u4e86\u8d2a\u5a6a\u968f\u673a\u884c\u9009\u62e9\u548c\u5c3e\u90e8\u5e73\u5747\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u901f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2510.04281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04281", "abs": "https://arxiv.org/abs/2510.04281", "authors": ["Zhuangzhi Gao", "Hongyi Qin", "He Zhao", "Qinkai Yu", "Feixiang Zhou", "Eduard Shantsila", "Uazman Alam", "Alena Shantsila", "Wahbi El-Bouri", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and\n  Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)", "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse\ndata modalities, but current medical adaptations such as LLaVA-Med often fail\nto fully exploit the synergy between color fundus photography (CFP) and optical\ncoherence tomography (OCT), and offer limited interpretability of quantitative\nbiomarkers. We introduce GROK, a grounded multimodal large language model that\njointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of\nocular and systemic disease. GROK comprises three core modules:\nKnowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,\nand Supervised Instruction Fine-Tuning, which together establish a\nquantitative-to-qualitative diagnostic chain of thought, mirroring real\nclinical reasoning when producing detailed lesion annotations. To evaluate our\napproach, we introduce the Grounded Ophthalmic Understanding benchmark, which\ncovers six disease categories and three tasks: macro-level diagnostic\nclassification, report generation quality, and fine-grained clinical assessment\nof the generated chain of thought. Experiments show that, with only LoRA\n(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK\noutperforms comparable 7B and 32B baselines on both report quality and\nfine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are\npublicly available in the GROK repository.", "AI": {"tldr": "GROK\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408CFP\u3001OCT\u548c\u6587\u672c\u6570\u636e\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u773c\u79d1\u548c\u5168\u8eab\u75be\u75c5\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528CFP\u548cOCT\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5b9a\u91cf\u751f\u7269\u6807\u5fd7\u7269\u7684\u89e3\u91ca\u6027\u3002", "method": "GROK\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u77e5\u8bc6\u5f15\u5bfc\u6307\u4ee4\u751f\u6210\u3001CLIP\u98ce\u683cOCT\u751f\u7269\u6807\u5fd7\u7269\u5bf9\u9f50\u548c\u76d1\u7763\u6307\u4ee4\u5fae\u8c03\uff0c\u5efa\u7acb\u4e86\u4ece\u5b9a\u91cf\u5230\u5b9a\u6027\u7684\u8bca\u65ad\u601d\u7ef4\u94fe\u3002", "result": "GROK\u5728\u4ec5\u4f7f\u7528LoRA\u5fae\u8c037B\u53c2\u6570Qwen2\u4e3b\u5e72\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u62a5\u544a\u8d28\u91cf\u548c\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u6307\u6807\u4e0a\u5747\u4f18\u4e8e7B\u548c32B\u57fa\u7ebf\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8fc7OpenAI o3\u3002", "conclusion": "GROK\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u5728\u773c\u79d1\u548c\u5168\u8eab\u75be\u75c5\u8bca\u65ad\u4e2d\u5b9e\u73b0\u4e86\u4e34\u5e8a\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2510.04883", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04883", "abs": "https://arxiv.org/abs/2510.04883", "authors": ["Nathan Shankar", "Pawel Ladosz", "Hujun Yin"], "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery", "comment": "8 pages, 8 figures", "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eU-Net\u7684IR\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u6d88\u9664\u53d1\u5c04\u5668\u5e72\u6270\uff0c\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u7ea2\u5916\u6d41\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u6bd4RGB\u6d41\u66f4\u6297\u566a\uff0c\u4f46\u4e3b\u52a8\u53d1\u5c04\u5668\u6a21\u5f0f\u4f1a\u5e72\u6270\u9ad8\u5c42\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u5b9a\u4f4d\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cd\u5efa\u5e72\u51c0\u7684IR\u56fe\u50cf\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\u5bf9\u542b\u6709\u4e3b\u52a8\u53d1\u5c04\u5668\u6a21\u5f0f\u7684\u7ea2\u5916\u56fe\u50cf\u8fdb\u884c\u91cd\u5efa\uff0c\u4ee5\u6d88\u9664\u566a\u58f0\u5e76\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u5728\u4ece\u826f\u597d\u5149\u7167\u5230\u6781\u7aef\u4f4e\u5149\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u89c6\u89c9\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u67b6\u6784\uff0c\u80fd\u591f\u4ece\u5e26\u6709\u53d1\u5c04\u5668\u5e72\u6270\u7684\u7ea2\u5916\u56fe\u50cf\u4e2d\u91cd\u5efa\u51fa\u5e72\u51c0\u7684\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4ece\u826f\u597d\u5149\u7167\u5230\u6781\u7aef\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2510.04284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04284", "abs": "https://arxiv.org/abs/2510.04284", "authors": ["Yunghwei Lai", "Kaiming Liu", "Ziyue Wang", "Weizhi Ma", "Yang Liu"], "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "comment": null, "summary": "The professionalism of a human doctor in outpatient service depends on two\ncore abilities: the ability to make accurate medical decisions and the medical\nconsultation skill to conduct strategic, empathetic patient inquiry. Existing\nLarge Language Models (LLMs) have achieved remarkable accuracy on medical\ndecision-making benchmarks. However, they often lack the ability to conduct the\nstrategic and empathetic consultation, which is essential for real-world\nclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor\nagent trained to master both of the capabilities by ask high-yield questions\nand conduct strategic multi-turn inquiry to guide decision-making. Our\nframework introduces three key components: a multi-agent interactive\nenvironment, a two-tiered reward architecture that separately optimizes\nclinical decision-making and communicative inquiry skills, and an experience\nrepository to ground policy learning in high-quality prior trajectories. We\nevaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across\nmulti-facet metrics, such as communication quality, user experience, and task\naccuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source\nspecialized LLMs by a substantial margin with higher parameter efficiency and\noutperforms powerful proprietary models. Furthermore, the human evaluations\nshow a strong preference for Doctor-R1 to generate human-preferred clinical\ndialogue, demonstrating the effectiveness of the framework.", "AI": {"tldr": "Doctor-R1\u662f\u4e00\u4e2aAI\u533b\u751f\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3001\u53cc\u5c42\u5956\u52b1\u548c\u7ecf\u9a8c\u5e93\uff0c\u540c\u65f6\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u548c\u6c9f\u901a\u6280\u80fd\uff0c\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u6218\u7565\u6027\u548c\u540c\u7406\u5fc3\u7684\u54a8\u8be2\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86Doctor-R1\u6846\u67b6\uff0c\u5305\u542b\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u73af\u5883\u3001\u53cc\u5c42\u5956\u52b1\u67b6\u6784\uff08\u5206\u522b\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u548c\u6c9f\u901a\u6280\u80fd\uff09\u4ee5\u53ca\u7ecf\u9a8c\u5e93\u3002", "result": "Doctor-R1\u5728HealthBench\u548cMAQuE\u8bc4\u4f30\u4e2d\uff0c\u5728\u6c9f\u901a\u8d28\u91cf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u51c6\u786e\u6027\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u4e14\u4eba\u7c7b\u8bc4\u4ef7\u663e\u793a\u5176\u751f\u6210\u7684\u4e34\u5e8a\u5bf9\u8bdd\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "Doctor-R1 \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u73af\u5883\u3001\u53cc\u5c42\u5956\u52b1\u67b6\u6784\u548c\u7ecf\u9a8c\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u533b\u751f\u5728\u4e34\u5e8a\u51b3\u7b56\u548c\u6c9f\u901a\u54a8\u8be2\u6280\u80fd\u4e0a\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u3002"}}
{"id": "2510.04898", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04898", "abs": "https://arxiv.org/abs/2510.04898", "authors": ["Zheng Xiong", "Kang Li", "Zilin Wang", "Matthew Jackson", "Jakob Foerster", "Shimon Whiteson"], "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "comment": null, "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA", "AI": {"tldr": "HyperVLA\u901a\u8fc7\u8d85\u7f51\u7edc\u67b6\u6784\u548c\u7b97\u6cd5\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4eVLA\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u6781\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "HyperVLA\u91c7\u7528\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u4ec5\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u5c0f\u578b\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bad\u7ec3\u65f6\u7684\u9ad8\u6a21\u578b\u5bb9\u91cf\u3002\u5173\u952e\u7b97\u6cd5\u8bbe\u8ba1\u5305\u62ec\u5229\u7528\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u3001\u8d85\u7f51\u7edc\u5f52\u4e00\u5316\u548c\u52a8\u4f5c\u751f\u6210\u7b56\u7565\u3002", "result": "HyperVLA\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u5c11\u6837\u672c\u9002\u5e94\u65b9\u9762\u4e0e\u73b0\u6709VLA\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\uff0c\u540c\u65f6\u5c06\u6fc0\u6d3b\u53c2\u6570\u51cf\u5c1190\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347120\u500d\u3002", "conclusion": "HyperVLA\u901a\u8fc7\u521b\u65b0\u7684\u8d85\u7f51\u7edc\u67b6\u6784\u548c\u5173\u952e\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709VLA\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04311", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04311", "abs": "https://arxiv.org/abs/2510.04311", "authors": ["Bohan Tang", "Huidong Liang", "Keyue Jiang", "Xiaowen Dong"], "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "comment": null, "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks.", "AI": {"tldr": "LLM-MAS\u5728\u4efb\u52a1\u6df1\u5ea6\u548c\u5bbd\u5ea6\u589e\u52a0\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u6df1\u5ea6\u5f71\u54cd\u66f4\u5927\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u8868\u660eLLM-MAS\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4f18\u4e8eLLM-SAS\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bbe\u8ba1\u9650\u5236\u4e86\u7ed3\u8bba\u7684\u5f3a\u5ea6\u548c\u666e\u9002\u6027\u3002\u9700\u8981\u4ece\u4efb\u52a1\u590d\u6742\u6027\u7684\u89d2\u5ea6\uff08\u5982\u63a8\u7406\u6df1\u5ea6\u548c\u80fd\u529b\u591a\u6837\u6027\uff09\u6765\u8bc4\u4f30LLM-MAS\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u4e3a\u6df1\u5ea6\uff08\u63a8\u7406\u957f\u5ea6\uff09\u548c\u5bbd\u5ea6\uff08\u80fd\u529b\u591a\u6837\u6027\uff09\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u5728\u5224\u522b\u6027\u548c\u751f\u6210\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cLLM-MAS\u76f8\u5bf9\u4e8eLLM-SAS\u7684\u4f18\u52bf\u968f\u7740\u4efb\u52a1\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u589e\u52a0\u800c\u589e\u5f3a\uff0c\u4e14\u6df1\u5ea6\u7684\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0cLLM-MAS\u5728\u4efb\u52a1\u89e3\u51b3\u4e2d\u7684\u4f18\u52bf\u968f\u7740\u4efb\u52a1\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u589e\u52a0\u800c\u589e\u5f3a\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u65b9\u9762\u6548\u679c\u66f4\u4e3a\u663e\u8457\u3002\u8fd9\u4e3a\u672a\u6765LLM-MAS\u65b9\u6cd5\u548c\u57fa\u51c6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.04991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04991", "abs": "https://arxiv.org/abs/2510.04991", "authors": ["D. Schwartz", "K. Kondo", "J. P. How"], "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language Models", "comment": "8 pages, 4 figures", "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.", "AI": {"tldr": "\u5229\u7528VLM\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\u7684\u65b0\u578b\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6548\u7387\uff0c\u8def\u5f84\u5e73\u5747\u7f29\u77ed10%\u3002", "motivation": "\u4f20\u7edf\u63a2\u7d22\u65b9\u6cd5\u56e0\u5168\u5c40\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u4f9d\u8d56\u5c40\u90e8\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e38\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u3002", "method": "\u8be5\u65b9\u6cd5\u5c063D\u5360\u7528\u7f51\u683c\u8f6c\u6362\u4e3a\u90e8\u52062D\u73af\u5883\u5730\u56fe\uff0c\u751f\u6210\u5019\u9009\u5b50\u76ee\u6807\uff0c\u5e76\u901a\u8fc7VLM\u5bf9\u8fd9\u4e9b\u5b50\u76ee\u6807\u8fdb\u884c\u8bc4\u4f30\u548c\u6392\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u5bfc\u822a\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u5e38\u89c1\u7684\u8d2a\u5a6a\u5931\u8d25\uff08\u5982\u7ed5\u5165\u5c0f\u623f\u95f4\uff09\uff0c\u5e73\u5747\u8def\u5f84\u7f29\u77ed\u7ea610%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u578b\u9ad8\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6548\u7387\uff0c\u5e73\u5747\u8def\u5f84\u7f29\u77ed\u7ea610%\u3002"}}
{"id": "2510.05001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05001", "abs": "https://arxiv.org/abs/2510.05001", "authors": ["Aditya Sripada", "Abhishek Warrier"], "title": "Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot", "comment": "6 pages, 10 figures. Presented at IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Robotic locomotion research typically draws from biologically inspired leg\ndesigns, yet many human-engineered settings can benefit from\nnon-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from\nInterstellar into a 0.25 m, 0.99 kg research platform with seven actuated\ndegrees of freedom. The film shows two primary gaits: a bipedal-like walk and a\nhigh-speed rolling mode. For TARS3D, we build reduced-order models for each,\nderive closed-form limit-cycle conditions, and validate the predictions on\nhardware. Experiments confirm that the robot respects its +/-150 degree hip\nlimits, alternates left-right contacts without interference, and maintains an\neight-step hybrid limit cycle in rolling mode. Because each telescopic leg\nprovides four contact corners, the rolling gait is modeled as an eight-spoke\ndouble rimless wheel. The robot's telescopic leg redundancy implies a far\nricher gait repertoire than the two limit cycles treated analytically. So, we\nused deep reinforcement learning (DRL) in simulation to search the unexplored\nspace. We observed that the learned policy can recover the analytic gaits under\nthe right priors and discover novel behaviors as well. Our findings show that\nTARS3D's fiction-inspired bio-transcending morphology can realize multiple\npreviously unexplored locomotion modes and that further learning-driven search\nis likely to reveal more. This combination of analytic synthesis and\nreinforcement learning opens a promising pathway for multimodal robotics.", "AI": {"tldr": "TARS3D\u673a\u5668\u4eba\u53d7\u7535\u5f71\u542f\u53d1\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u975e\u62df\u4eba\u5316\u5f62\u6001\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u975e\u62df\u4eba\u5316\u5f62\u6001\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u53d7\u7535\u5f71\u300a\u661f\u9645\u7a7f\u8d8a\u300b\u4e2dTARS\u673a\u5668\u4eba\u542f\u53d1\u7684\u8bbe\u8ba1\u3002", "method": "\u6784\u5efa\u4e86\u964d\u9636\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u95ed\u5f0f\u6781\u9650\u73af\u6761\u4ef6\uff0c\u5e76\u5728\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u9884\u6d4b\uff1b\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u4e2d\u63a2\u7d22\u672a\u5f00\u53d1\u7684\u8fd0\u52a8\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u673a\u5668\u4eba\u80fd\u591f\u9075\u5b88\u5176\u9acb\u5173\u8282\u9650\u5236\uff0c\u4ea4\u66ff\u5de6\u53f3\u63a5\u89e6\u65e0\u5e72\u6270\uff0c\u5e76\u5728\u6eda\u52a8\u6a21\u5f0f\u4e0b\u4fdd\u6301\u516b\u6b65\u6df7\u5408\u6781\u9650\u73af\uff1b\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u6062\u590d\u5206\u6790\u6b65\u6001\u5e76\u53d1\u73b0\u65b0\u884c\u4e3a\u3002", "conclusion": "TARS3D\u7684\u865a\u6784\u542f\u53d1\u751f\u7269\u8d85\u8d8a\u5f62\u6001\u80fd\u591f\u5b9e\u73b0\u591a\u79cd\u5148\u524d\u672a\u63a2\u7d22\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7ed3\u5408\u5206\u6790\u5408\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u4e3a\u591a\u6a21\u6001\u673a\u5668\u4eba\u5f00\u8f9f\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.04373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints.", "AI": {"tldr": "JEF Hinter \u901a\u8fc7\u79bb\u7ebf\u8f68\u8ff9\u751f\u6210\u7d27\u51d1\u63d0\u793a\uff0c\u63d0\u5347LLM\u4ee3\u7406\u5728\u4e0d\u719f\u6089\u9886\u57df\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347LLM\u4ee3\u7406\u5728\u4e0d\u719f\u6089\u9886\u57df\u8868\u73b0\u65f6\u5b58\u5728\u6210\u672c\u9ad8\u3001\u6613\u9057\u5fd8\u7b49\u95ee\u9898\uff0c\u79bb\u7ebf\u8f68\u8ff9\u867d\u63d0\u4f9b\u53ef\u91cd\u7528\u77e5\u8bc6\uff0c\u4f46\u539f\u59cb\u8f68\u8ff9\u957f\u3001\u566a\u58f0\u591a\u4e14\u4efb\u52a1\u7279\u5b9a\u3002", "method": "JEF Hinter \u5229\u7528\u79bb\u7ebf\u8f68\u8ff9\uff08\u5305\u62ec\u6210\u529f\u548c\u5931\u8d25\u7684\uff09\u751f\u6210\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u7f29\u653e\u673a\u5236\u7a81\u51fa\u5173\u952e\u6b65\u9aa4\u3002\u63a8\u7406\u65f6\uff0c\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u63d0\u793a\u63d0\u4f9b\u9488\u5bf9\u6027\u6307\u5bfc\u3002", "result": "\u5728MiniWoB++\u3001WorkArena-L1\u548cWebArena-Lite\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJEF Hinter \u5728\u6027\u80fd\u4e0a consistently \u4f18\u4e8e\u5305\u62ec\u57fa\u4e8e\u4eba\u7c7b\u548c\u6587\u6863\u63d0\u793a\u7684\u57fa\u7ebf\u3002", "conclusion": "JEF Hinter \u901a\u8fc7\u63d0\u53d6\u79bb\u7ebf\u8f68\u8ff9\u4e2d\u7684\u7d27\u51d1\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u4e0d\u719f\u6089\u9886\u57df\u7684\u8868\u73b0\uff0c\u4e14\u5728\u591a\u79cd\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2510.05057", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05057", "abs": "https://arxiv.org/abs/2510.05057", "authors": ["Mingyu Liu", "Jiuhe Shu", "Hui Chen", "Zeju Li", "Canyu Zhao", "Jiange Yang", "Shenyuan Gao", "Hao Chen", "Chunhua Shen"], "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation", "comment": null, "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.", "AI": {"tldr": "StaMo\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u9759\u6001\u56fe\u50cf\u4e2d\u63d0\u53d6\u7d27\u51d1\u72b6\u6001\u8868\u793a\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u5e76\u652f\u6301\u8de8\u6570\u636e\u6e90\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u8fbe\u6027\u548c\u7d27\u51d1\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bfc\u81f4\u8868\u793a\u8981\u4e48\u5197\u4f59\uff0c\u8981\u4e48\u7f3a\u4e4f\u4efb\u52a1\u5173\u952e\u4fe1\u606f\u3002StaMo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u72b6\u6001\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u7684Diffusion Transformer\uff08DiT\uff09\u89e3\u7801\u5668\u5b66\u4e60\u9ad8\u5ea6\u538b\u7f29\u7684\u4e24\u4ee4\u724c\u72b6\u6001\u8868\u793a\u3002\u901a\u8fc7\u6f5c\u5728\u63d2\u503c\u751f\u6210\u7684\u81ea\u7136\u5dee\u5f02\u4f5c\u4e3a\u6f5c\u5728\u52a8\u4f5c\uff0c\u8fdb\u4e00\u6b65\u89e3\u7801\u4e3a\u53ef\u6267\u884c\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "StaMo\u5728LIBERO\u4e0a\u6027\u80fd\u63d0\u534714.3%\uff0c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad830%\uff0c\u6f5c\u5728\u52a8\u4f5c\u7684\u534f\u540c\u8bad\u7ec3\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd510.4%\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6570\u636e\u6e90\u7684\u6709\u6548\u6269\u5c55\u6027\u3002", "conclusion": "StaMo\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4ece\u9759\u6001\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u7d27\u51d1\u72b6\u6001\u8868\u793a\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5373\u53ef\u6355\u6349\u7ed3\u6784\u5316\u52a8\u6001\uff0c\u5e76\u751f\u6210\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u8fd9\u4e00\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6570\u636e\u6e90\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.04384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04384", "abs": "https://arxiv.org/abs/2510.04384", "authors": ["Adam Ballew", "Jingbo Wang", "Shaogang Ren"], "title": "LLM Based Bayesian Optimization for Prompt Search", "comment": null, "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper.", "AI": {"tldr": "BO-LLM\u7b97\u6cd5\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548cLLM\uff0c\u901a\u8fc7GP\u548cUCB\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u63d0\u5347\u6587\u672c\u5206\u7c7b\u6027\u80fd\u5e76\u51cf\u5c11API\u8c03\u7528\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528BO\u5728\u6709\u9650\u8bc4\u4f30\u6b21\u6570\u4e0b\u4f18\u5316\u6602\u8d35\u7684\u9ed1\u76d2\u51fd\u6570\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347LLM\u6587\u672c\u5206\u7c7b\u6027\u80fd\u7684\u63d0\u793a\u5de5\u7a0b\u4e2d\u3002", "method": "\u91c7\u7528LLM\u9a71\u52a8\u7684Gaussian Process\uff08GP\uff09\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7Upper Confidence Bound\uff08UCB\uff09\u83b7\u53d6\u51fd\u6570\u8bc4\u4f30\u4e0d\u540c\u63d0\u793a\u5019\u9009\u6027\u80fd\uff0c\u5e76\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728\u4e24\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BO-LLM\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u51cf\u5c11API\u8c03\u7528\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684BO-LLM\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86API\u8c03\u7528\u6b21\u6570\u3002"}}
{"id": "2510.05061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05061", "abs": "https://arxiv.org/abs/2510.05061", "authors": ["Anastasios Manganaris", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Automaton Constrained Q-Learning", "comment": "9 pages, 4 figures, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.", "AI": {"tldr": "ACQL\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u6761\u4ef6\u503c\u5b66\u4e60\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u65f6\u95f4\u89c4\u8303\u548c\u5b89\u5168\u7ea6\u675f\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u4efb\u52a1\u901a\u5e38\u9700\u8981\u4ee3\u7406\u5728\u5c0a\u91cd\u65f6\u53d8\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u4e00\u7cfb\u5217\u76ee\u6807\uff0c\u4f46\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "ACQL\u7ed3\u5408\u4e86\u76ee\u6807\u6761\u4ef6\u503c\u5b66\u4e60\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u652f\u6301\u5927\u591a\u6570LTL\u4efb\u52a1\u89c4\u8303\uff0c\u5e76\u5229\u7528\u5176\u81ea\u52a8\u673a\u8868\u793a\u660e\u786e\u7f16\u7801\u9636\u6bb5\u6027\u76ee\u6807\u8fdb\u5c55\u4ee5\u53ca\u9759\u6001\u548c\u975e\u9759\u6001\u5b89\u5168\u7ea6\u675f\u3002", "result": "ACQL\u5728\u5305\u62ec\u5148\u9a8c\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u76ee\u6807\u8fbe\u6210\u6216\u5b89\u5168\u7ea6\u675f\u7684\u6848\u4f8b\u5728\u5185\u7684\u5404\u79cd\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ACQL\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6839\u636e\u4e30\u5bcc\u7684\u65f6\u95f4\u89c4\u8303\u5b66\u4e60\u673a\u5668\u4eba\u884c\u4e3a\u3002"}}
{"id": "2510.04391", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548cLLMs\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWM\uff09\uff0c\u53d1\u73b0\u4e24\u8005\u5dee\u5f02\u663e\u8457\uff0c\u4e3a\u5f00\u53d1\u7c7b\u4ebaAI\u60f3\u8c61\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u60f3\u8c61\u529b\u7684\u8ba1\u7b97\u76ee\u6807\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u60f3\u8c61\u529b\u4ec5\u7528\u4e8e\u6700\u5927\u5316\u5956\u52b1\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u60f3\u8c61\u529b\u7528\u4e8e\u8bbf\u95ee\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWM\uff09\u3002", "method": "\u4f7f\u7528\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4efd\u95ee\u5377\u8bc4\u4f30\u60f3\u8c61\u529b\u7684\u751f\u52a8\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u4eba\u7c7b\u548cLLMs\u7684\u60f3\u8c61\u529b\u7f51\u7edc\u3002", "result": "\u4eba\u7c7b\u60f3\u8c61\u529b\u7f51\u7edc\u5728\u4e0d\u540c\u4e2d\u5fc3\u6027\u6307\u6807\uff08\u5982\u9884\u671f\u5f71\u54cd\u3001\u5f3a\u5ea6\u548c\u63a5\u8fd1\u6027\uff09\u4e4b\u95f4\u663e\u793a\u51fa\u76f8\u5173\u6027\uff0c\u800cLLMs\u7684\u60f3\u8c61\u529b\u7f51\u7edc\u5219\u7f3a\u4e4f\u805a\u7c7b\u4e14\u4e2d\u5fc3\u6027\u6307\u6807\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWM\uff09\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u7c7b\u4eba\u60f3\u8c61\u529b\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.05070", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05070", "abs": "https://arxiv.org/abs/2510.05070", "authors": ["Siheng Zhao", "Yanjie Ze", "Yue Wang", "C. Karen Liu", "Pieter Abbeel", "Guanya Shi", "Rocky Duan"], "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning", "comment": "9 pages, 8 figures", "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .", "AI": {"tldr": "ResMimic\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u548c\u9ad8\u6548\u6b8b\u5dee\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7cbe\u786e\u63a7\u5236\u548c\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\uff08GMT\uff09\u6280\u672f\u5df2\u80fd\u590d\u73b0\u591a\u6837\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4f46\u5176\u7f3a\u4e4f\u7cbe\u786e\u6027\u548c\u7269\u4f53\u611f\u77e5\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51faResMimic\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ResMimic\u91c7\u7528\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff1a\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\uff08GMT\uff09\u7b56\u7565\u751f\u6210\u7c7b\u4eba\u5168\u8eab\u52a8\u4f5c\uff0c\u7136\u540e\u5b66\u4e60\u4e00\u4e2a\u9ad8\u6548\u7684\u6b8b\u5dee\u7b56\u7565\u6765\u4f18\u5316GMT\u8f93\u51fa\uff0c\u6539\u8fdb\u8fd0\u52a8\u5e76\u878d\u5165\u7269\u4f53\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u70b9\u4e91\u7269\u4f53\u8ddf\u8e2a\u5956\u52b1\u3001\u63a5\u89e6\u5956\u52b1\u548c\u57fa\u4e8e\u8bfe\u7a0b\u7684\u865a\u62df\u7269\u4f53\u63a7\u5236\u5668\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cResMimic\u5728\u4eff\u771f\u548c\u5b9e\u9645Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4efb\u52a1\u6210\u529f\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u5747\u6709\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "ResMimic\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7cbe\u786e\u63a7\u5236\u548c\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04399", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04399", "abs": "https://arxiv.org/abs/2510.04399", "authors": ["Charles L. Wang", "Keir Dorchen", "Peter Jin"], "title": "Utility-Learning Tension in Self-Modifying Agents", "comment": null, "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u4e2d\u7684\u6548\u7528-\u5b66\u4e60\u5f20\u529b\uff0c\u53d1\u73b0\u65e0\u9650\u5bb9\u91cf\u589e\u957f\u4f1a\u7834\u574f\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u53cc\u95e8\u7b56\u7565\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u7cfb\u7edf\u8d8b\u5411\u8d85\u7ea7\u667a\u80fd\uff0c\u7814\u7a76\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u5728\u6548\u7528\u9a71\u52a8\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u5b66\u4e60\u548c\u6cdb\u5316\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u4e94\u8f74\u5206\u89e3\u548c\u51b3\u7b56\u5c42\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5206\u79bb\u4e86\u6fc0\u52b1\u4e0e\u5b66\u4e60\u884c\u4e3a\uff0c\u5e76\u72ec\u7acb\u5206\u6790\u5404\u8f74\u3002\u7406\u8bba\u5206\u6790\u7ed3\u5408\u6570\u503c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u7834\u574f\u6027\u6548\u7528\u7b56\u7565\u4e0e\u63d0\u51fa\u7684\u53cc\u95e8\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6548\u7528\u9a71\u52a8\u7684\u81ea\u6211\u4fee\u6539\u53ef\u80fd\u7834\u574f\u5b66\u4e60\u7684\u7edf\u8ba1\u524d\u63d0\uff0c\u5bfc\u81f4\u53ef\u5b66\u4e60\u4efb\u52a1\u53d8\u5f97\u4e0d\u53ef\u5b66\u4e60\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53cc\u95e8\u7b56\u7565\u5728\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u8f74\u5206\u89e3\u548c\u51b3\u7b56\u5c42\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u4e2d\u7684\u6548\u7528-\u5b66\u4e60\u5f20\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53ea\u6709\u5728\u7b56\u7565\u53ef\u8fbe\u6a21\u578b\u5bb6\u65cf\u5177\u6709\u7edf\u4e00\u5bb9\u91cf\u9650\u5236\u65f6\uff0c\u624d\u80fd\u4fdd\u8bc1\u5206\u5e03\u81ea\u7531\u6027\u3002\u5f53\u5bb9\u91cf\u65e0\u9650\u589e\u957f\u65f6\uff0c\u6548\u7528\u9a71\u52a8\u7684\u81ea\u6211\u4fee\u6539\u53ef\u80fd\u5bfc\u81f4\u53ef\u5b66\u4e60\u4efb\u52a1\u53d8\u5f97\u4e0d\u53ef\u5b66\u4e60\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\u7684\u53cc\u95e8\u7b56\u7565\u3002"}}
{"id": "2510.03501", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "\u63d0\u51fa\u79fb\u52a8\u4f18\u5316\u7684YOLOv10+MobileSAM\u5e76\u884c\u6846\u67b6\uff0c\u9ad8\u6548\u5b9e\u73b0\u91ce\u751f\u52a8\u7269\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u5206\u5272\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u5f00\u6e90\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u9700\u8981\u975e\u4fb5\u5165\u6027\u76d1\u6d4b\uff0c\u4f46\u73b0\u6709\u6280\u672f\u56e0\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u7269\u79cd\u9690\u853d\u6027\u9762\u4e34\u6311\u6218\u3002", "method": "\u7ed3\u5408Threading Detection Model (TDM)\u5e76\u884c\u5316YOLOv10\u68c0\u6d4b\u548cMobileSAM\u5206\u5272\uff0c\u4f18\u5316\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728Houbara Bustard\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08mAP50: 0.9627, mAP75: 0.7731, mAP95: 0.7178, MobileSAM mIoU: 0.7421\uff09\uff0cYOLOv10\u6bcf\u5e27\u5904\u7406\u65f6\u95f4\u4e3a43.7\u6beb\u79d2\uff0c\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u7136\u73af\u5883\u4e2d\u52a8\u7269\u7684\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6fd2\u5371\u7269\u79cd\u5982Houbara Bustard\u7684\u4fdd\u62a4\u76d1\u6d4b\u3002"}}
{"id": "2510.04474", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04474", "abs": "https://arxiv.org/abs/2510.04474", "authors": ["Gang Li", "Yan Chen", "Ming Lin", "Tianbao Yang"], "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization", "comment": "20 pages, 7 figures", "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction.", "AI": {"tldr": "DRPO\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u8def\u5f84\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u6709\u6548\u51cf\u5c11\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u957f\u5ea6\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u548c\u54cd\u5e94\u5ef6\u8fdf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u957f\u5ea6\u5956\u52b1\u6765\u4fc3\u8fdb\u7b80\u6d01\u63a8\u7406\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86Decoupled Reward Policy Optimization (DRPO)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u6b63\u786e\u63a8\u7406\u8def\u5f84\u7684\u957f\u5ea6\u5956\u52b1\u4fe1\u53f7\u4e0e\u9519\u8bef\u8def\u5f84\u89e3\u8026\uff0c\u5e76\u901a\u8fc7KL\u6b63\u5219\u5316\u4f18\u5316\u6b63\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cDRPO\u663e\u8457\u4f18\u4e8e\u516d\u79cd\u9ad8\u6548\u63a8\u7406\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f7f\u75281.5B\u6a21\u578b\u65f6\uff0cDRPO\u5728GSM8k\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8677%\u7684\u957f\u5ea6\u51cf\u5c11\uff0c\u4ec5\u635f\u59311.1%\u7684\u6027\u80fd\u3002", "conclusion": "DRPO\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\u548c\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2510.03545", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u624b\u7ed8\u8349\u56fe\u751f\u62103D\u98de\u884c\u8def\u5f84\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u751f\u6210\u5b89\u5168\u98de\u884c\u8def\u5f84\u7684\u6311\u6218\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u624b\u7ed8\u8349\u56fe\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "SketchPlan\u7531SketchAdapter\u548cDiffPath\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u524d\u8005\u5b66\u4e60\u5c06\u4eba\u7c7b\u8349\u56fe\u6620\u5c04\u52302D\u8def\u5f84\uff0c\u540e\u8005\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4ece2D\u6295\u5f71\u548c\u6df1\u5ea6\u56fe\u50cf\u63a8\u65ad3D\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cSketchPlan\u5728\u4f4e/\u4e2d\u969c\u788d\u7269\u73af\u5883\u4e2d\u6210\u529f\u7387\u4e3a100%\uff0c\u5728\u9ad8\u969c\u788d\u7269\u73af\u5883\u4e2d\u4e3a40%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd520-60%\u3002", "conclusion": "SketchPlan\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u624b\u7ed8\u8349\u56fe\u4e0e\u6df1\u5ea6\u56fe\u50cf\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a3D\u98de\u884c\u8def\u5f84\u7684\u751f\u6210\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04480", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04480", "abs": "https://arxiv.org/abs/2510.04480", "authors": ["Yunuo Cen", "Zixuan Wang", "Jintao Zhang", "Zhiwei Zhang", "Xuanyao Fong"], "title": "On Continuous Optimization for Constraint Satisfaction Problems", "comment": null, "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics,\nphysics, and theoretical computer science. While conflict-driven clause\nlearning Boolean Satisfiability (SAT) solvers have achieved remarkable success\nand become the mainstream approach for Boolean satisfiability, recent advances\nshow that modern continuous local search (CLS) solvers can achieve highly\ncompetitive results on certain classes of SAT problems. Motivated by these\nadvances, we extend the CLS framework from Boolean SAT to general CSP with\nfinite-domain variables and expressive constraints. We present FourierCSP, a\ncontinuous optimization framework that generalizes the Walsh-Fourier transform\nto CSP, allowing for transforming versatile constraints to compact multilinear\npolynomials, thereby avoiding the need for auxiliary variables and\nmemory-intensive encodings. Our approach leverages efficient evaluation and\ndifferentiation of the objective via circuit-output probability and employs a\nprojected gradient optimization method with theoretical guarantees. Empirical\nresults on benchmark suites demonstrate that FourierCSP is scalable and\ncompetitive, significantly broadening the class of problems that can be\nefficiently solved by CLS techniques.", "AI": {"tldr": "FourierCSP\u5c06CLS\u6280\u672f\u6269\u5c55\u81f3\u4e00\u822cCSP\uff0c\u901a\u8fc7Walsh-Fourier\u53d8\u6362\u548c\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u9898\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u53d7\u73b0\u4ee3\u8fde\u7eed\u5c40\u90e8\u641c\u7d22\uff08CLS\uff09\u6c42\u89e3\u5668\u5728\u67d0\u4e9bSAT\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u7684\u542f\u53d1\uff0c\u5c06CLS\u6846\u67b6\u4ece\u5e03\u5c14SAT\u6269\u5c55\u5230\u5177\u6709\u6709\u9650\u57df\u53d8\u91cf\u548c\u8868\u8fbe\u7ea6\u675f\u7684\u4e00\u822cCSP\u3002", "method": "\u63d0\u51fa\u4e86FourierCSP\u6846\u67b6\uff0c\u5c06Walsh-Fourier\u53d8\u6362\u63a8\u5e7f\u81f3CSP\uff0c\u5c06\u591a\u6837\u7ea6\u675f\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u591ariott\u7ebf\u6027\u591a\u9879\u5f0f\uff0c\u907f\u514d\u4e86\u8f85\u52a9\u53d8\u91cf\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u7f16\u7801\u3002\u5229\u7528\u7535\u8def\u8f93\u51fa\u6982\u7387\u9ad8\u6548\u8bc4\u4f30\u548c\u5fae\u5206\u76ee\u6807\uff0c\u5e76\u91c7\u7528\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6295\u5f71\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cFourierCSP\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "FourierCSP\u663e\u8457\u6269\u5c55\u4e86CLS\u6280\u672f\u53ef\u9ad8\u6548\u89e3\u51b3\u7684\u95ee\u9898\u7c7b\u522b\uff0c\u4e3a\u6709\u9650\u57df\u53d8\u91cf\u548c\u8868\u8fbe\u7ea6\u675f\u7684CSP\u63d0\u4f9b\u4e86\u8fde\u7eed\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2510.03827", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "LIBERO-PRO\u901a\u8fc7\u6270\u52a8\u8bc4\u4f30VLA\u6a21\u578b\uff0c\u63ed\u793a\u5176\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u547c\u5401\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LIBERO\u57fa\u51c6\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u7f6e\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u8ba1\u865a\u9ad8\uff0c\u65e0\u6cd5\u516c\u5e73\u6bd4\u8f83\u6a21\u578b\u3002", "method": "\u5f15\u5165LIBERO-PRO\u57fa\u51c6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\u7684\u6270\u52a8\uff08\u64cd\u7eb5\u5bf9\u8c61\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\uff09\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6LIBERO\u8bc4\u4f30\u4e2d\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u4f46\u5728\u5e7f\u4e49\u8bbe\u7f6e\u4e0b\u5d29\u6e83\u81f30.0%\uff0c\u66b4\u9732\u4e86\u6a21\u578b\u5bf9\u8bad\u7ec3\u96c6\u7684\u673a\u68b0\u8bb0\u5fc6\u4f9d\u8d56\u3002", "conclusion": "LIBERO-PRO\u63ed\u793a\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e25\u91cd\u7f3a\u9677\uff0c\u547c\u5401\u793e\u533a\u91c7\u7528\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.04488", "categories": ["cs.AI", "cs.IT", "math.IT", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.04488", "abs": "https://arxiv.org/abs/2510.04488", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning", "comment": "27 pages, 5 figures, 21 tables", "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller.", "AI": {"tldr": "MACI\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u8282\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u548c\u884c\u4e3a\u7b56\u7565\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u7684\u95ee\u9898\uff0c\u5982\u56fa\u5b9a\u5bf9\u6297\u7acb\u573a\u3001\u65e0\u5ba1\u8bae\u7684\u805a\u5408\u6216\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u7ec8\u6b62\u3002", "method": "\u5f15\u5165MACI\u63a7\u5236\u5668\uff0c\u5305\u542b\u4e24\u4e2a\u72ec\u7acb\u8c03\u8282\u7684\u4fe1\u606f\u548c\u884c\u4e3a\u62e8\u76d8\uff0c\u4ee5\u53ca\u4e00\u4e2a\u76d1\u7763\u5206\u6b67\u3001\u91cd\u53e0\u3001\u8bc1\u636e\u8d28\u91cf\u548c\u8bba\u8bc1\u8d28\u91cf\u7684\u8c03\u89e3\u5668\uff0c\u786e\u4fdd\u8fa9\u8bba\u5728\u6536\u76ca\u5e73\u7a33\u65f6\u7ec8\u6b62\u3002", "result": "\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u65b0\u95fb\u504f\u89c1\u4efb\u52a1\u4e2d\uff0cMACI\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6821\u51c6\u5ea6\uff0c\u51cf\u5c11\u4e86\u4ee4\u724c\u4f7f\u7528\uff0c\u5e76\u5c06\u5269\u4f59\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684RAG\u8ba1\u5212\u3002", "conclusion": "MACI\u5c06\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8f6c\u5316\u4e3a\u4e00\u4e2a\u5177\u6709\u9884\u7b97\u610f\u8bc6\u3001\u53ef\u6d4b\u91cf\u4e14\u53ef\u8bc1\u660e\u7ec8\u6b62\u7684\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u4fe1\u606f\u8d28\u91cf\u548c\u884c\u4e3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u548c\u6821\u51c6\u5ea6\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\u3002"}}
{"id": "2510.03896", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03896", "abs": "https://arxiv.org/abs/2510.03896", "authors": ["Mingyu Liu", "Zheng Huang", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Yating Wang", "Haoyi Zhu", "Hao Chen", "Chunhua Shen"], "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert", "comment": null, "summary": "Although Vision-Language Models (VLM) have demonstrated impressive planning\nand reasoning capabilities, translating these abilities into the physical world\nintroduces significant challenges. Conventional Vision-Language-Action (VLA)\nmodels, which integrate reasoning and action into a monolithic architecture,\ngeneralize poorly because they are constrained by scarce, narrow-domain data.\nWhile recent dual-system approaches attempt to decouple \"thinking\" from\n\"acting\", they are often constrained by semantic ambiguities within the action\nmodule. This ambiguity makes large-scale, cross-task training infeasible.\nConsequently, these systems typically necessitate fine-tuning on newly\ncollected data when deployed to novel environments, and the cooperation\nmechanism between the two systems remains ill-defined. To address these\nlimitations, we introduce, for the first time, a framework centered around a\ngeneralizable action expert. Our approach utilizes sparse 3D trajectories as an\nintermediate representation, effectively bridging the high-level planning\ncapabilities of the VLM with the low-level physical action module. During the\nplanning phase, the VLM is only required to generate coarse 3D waypoints. These\nwaypoints are then processed by our generalizable action expert, which refines\nthem into dense, executable action sequences by sampling real-time point cloud\nobservations of the environment. To promote training efficiency and robust\ngeneralization, we introduce a novel \"Action Pre-training, Pointcloud\nFine-tuning\" paradigm. Our method combines the broad generalization\ncapabilities of VLMs in visual understanding and planning with the\nfine-grained, action-level generalization of action expert.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u7a00\u758f3D\u8f68\u8ff9\u8fde\u63a5VLM\u548c\u52a8\u4f5c\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u8f6c\u5316\u4e3a\u7269\u7406\u4e16\u754c\u52a8\u4f5c\u65f6\uff0c\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u8de8\u4efb\u52a1\u8bad\u7ec3\u4e0d\u53ef\u884c\u7b49\u95ee\u9898\uff0c\u4e14\u53cc\u7cfb\u7edf\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u6027\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u901a\u7528\u5316\u52a8\u4f5c\u4e13\u5bb6\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86VLM\u7684\u89c6\u89c9\u7406\u89e3\u548c\u89c4\u5212\u80fd\u529b\u4e0e\u52a8\u4f5c\u4e13\u5bb6\u7684\u7ec6\u7c92\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u2018\u52a8\u4f5c\u9884\u8bad\u7ec3\uff0c\u70b9\u4e91\u5fae\u8c03\u2019\u7684\u65b0\u8303\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f3D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5b9e\u73b0\u4e86VLM\u4e0e\u52a8\u4f5c\u6a21\u5757\u7684\u6709\u6548\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u65b0\u8bad\u7ec3\u8303\u5f0f\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u7a00\u758f\u76843D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06VLM\u7684\u9ad8\u7ea7\u89c4\u5212\u80fd\u529b\u4e0e\u4f4e\u7ea7\u7684\u7269\u7406\u52a8\u4f5c\u6a21\u5757\u6709\u6548\u8fde\u63a5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2510.04491", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04491", "abs": "https://arxiv.org/abs/2510.04491", "authors": ["Muyu He", "Anand Kumar", "Tsach Mackey", "Meghana Rajeev", "James Zou", "Nazneen Rajani"], "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "comment": "25 pages", "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait.", "AI": {"tldr": "TraitBasis\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u6d4b\u8bd5\u63ed\u793a\u5bf9\u8bddAI\u4ee3\u7406\u5bf9\u7528\u6237\u884c\u4e3a\u53d8\u5316\u7684\u8106\u5f31\u6027\uff0c\u6027\u80fd\u5e73\u5747\u4e0b\u964d2%-30%\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bddAI\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\uff0c\u7528\u6237\u884c\u4e3a\u7684\u5fae\u5c0f\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u8106\u5f31\u6027\u3002", "method": "TraitBasis\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u538b\u529b\u6d4b\u8bd5AI\u4ee3\u7406\u3002\u5b83\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u53ef\u64cd\u7eb5\u7684\u7528\u6237\u7279\u5f81\u65b9\u5411\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u63a7\u5236\u3001\u7f29\u653e\u3001\u7ec4\u5408\u548c\u5e94\u7528\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u989d\u5916\u6570\u636e\u3002", "result": "\u4f7f\u7528TraitBasis\u6269\u5c55\u7684\u03c4-Trait\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u524d\u6cbf\u6a21\u578b\u7684\u6027\u80fd\u5e73\u5747\u4e0b\u964d2%-30%\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7406\u5bf9\u7528\u6237\u884c\u4e3a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "conclusion": "TraitBasis\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u53ef\u7ec4\u5408\u7684\u5de5\u5177\uff0c\u4e3a\u6784\u5efa\u5728\u73b0\u5b9e\u4e16\u754c\u4e0d\u53ef\u9884\u6d4b\u7684\u4eba\u7c7b\u4e92\u52a8\u4e2d\u4ecd\u4fdd\u6301\u53ef\u9760\u7684AI\u4ee3\u7406\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2510.04514", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.", "AI": {"tldr": "ChartAgent\u662f\u4e00\u79cd\u65b0\u578b\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5de5\u5177\u548c\u4e3b\u52a8\u4ea4\u4e92\u63d0\u5347\u56fe\u8868\u7406\u89e3\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u65e0\u6ce8\u91ca\u548c\u6570\u503c\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001LLM\u5728\u57fa\u4e8e\u56fe\u8868\u7684\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65e0\u6ce8\u91ca\u56fe\u8868\u6216\u9700\u8981\u7cbe\u786e\u89c6\u89c9\u89e3\u91ca\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86ChartAgent\u3002", "method": "ChartAgent\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5b50\u4efb\u52a1\u7684\u8fed\u4ee3\u5206\u89e3\u548c\u4e0e\u56fe\u8868\u56fe\u50cf\u7684\u4e3b\u52a8\u4ea4\u4e92\uff08\u5982\u7ed8\u5236\u6ce8\u91ca\u3001\u88c1\u526a\u533a\u57df\u548c\u5b9a\u4f4d\u8f74\uff09\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u89c6\u89c9\u5de5\u5177\u5e93\u6765\u5b8c\u6210\u6bcf\u4e2a\u5b50\u4efb\u52a1\u3002", "result": "ChartAgent\u5728ChartBench\u548cChartX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u6574\u4f53\u7edd\u5bf9\u589e\u76ca\u9ad8\u8fbe16.07%\uff0c\u5728\u65e0\u6ce8\u91ca\u548c\u6570\u503c\u5bc6\u96c6\u578b\u67e5\u8be2\u4e0a\u589e\u76ca\u8fbe17.31%\u3002", "conclusion": "ChartAgent\u901a\u8fc7\u89c6\u89c9\u5de5\u5177\u589e\u5f3a\u7684\u591a\u6a21\u6001\u4ee3\u7406\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u56fe\u8868\u7406\u89e3\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u65e0\u6ce8\u91ca\u548c\u6570\u503c\u5bc6\u96c6\u578b\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04333", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04333", "abs": "https://arxiv.org/abs/2510.04333", "authors": ["Lan Feng", "Yang Gao", "Eloi Zablocki", "Quanyi Li", "Wuyang Li", "Sichao Liu", "Matthieu Cord", "Alexandre Alahi"], "title": "RAP: 3D Rasterization Augmented End-to-End Planning", "comment": null, "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.", "AI": {"tldr": "RAP\u901a\u8fc7\u8f7b\u91cf\u7ea73D\u6805\u683c\u5316\u548c\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u7aef\u5230\u7aef\u9a7e\u9a76\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u9ad8\u6210\u672c\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e13\u5bb6\u6f14\u793a\u7684\u6a21\u4eff\u5b66\u4e60\u7f3a\u4e4f\u6062\u590d\u6570\u636e\uff0c\u4e14\u4f20\u7edf\u795e\u7ecf\u6e32\u67d3\u6216\u6e38\u620f\u5f15\u64ce\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u901f\u5ea6\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u63d0\u51fa3D Rasterization\u66ff\u4ee3\u9ad8\u6210\u672c\u6e32\u67d3\uff0c\u5e76\u7ed3\u5408Raster-to-Real\u7279\u5f81\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5408\u6210\u89c6\u56fe\u5230\u771f\u5b9e\u573a\u666f\u7684\u6709\u6548\u8fc1\u79fb\u3002", "result": "RAP\u5728NAVSIM v1/v2\u3001Waymo Open Dataset\u7b49\u56db\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "3D Rasterization\u4e0eRaster-to-Real\u7279\u5f81\u5bf9\u9f50\u7ed3\u5408\uff08RAP\uff09\u4e3a\u7aef\u5230\u7aef\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u95ed\u73af\u9c81\u68d2\u6027\u548c\u957f\u5c3e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04520", "abs": "https://arxiv.org/abs/2510.04520", "authors": ["Hanyu Wang", "Ruohan Xie", "Yutong Wang", "Guoxiong Gao", "Xintao Yu", "Bin Dong"], "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph", "comment": null, "summary": "Accurate auto-formalization of theorem statements is essential for advancing\nautomated discovery and verification of research-level mathematics, yet remains\na major bottleneck for LLMs due to hallucinations, semantic mismatches, and\ntheir inability to synthesize new definitions. To tackle these issues, we\npresent Aria (Agent for Retrieval and Iterative Autoformalization), a system\nfor conjecture-level formalization in Lean that emulates human expert reasoning\nvia a two-phase Graph-of-Thought process: recursively decomposing statements\ninto a dependency graph and then constructing formalizations from grounded\nconcepts. To ensure semantic correctness, we introduce AriaScorer, a checker\nthat retrieves definitions from Mathlib for term-level grounding, enabling\nrigorous and reliable verification. We evaluate Aria on diverse benchmarks. On\nProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,\nsurpassing previous methods. On FATE-X, a suite of challenging algebra problems\nfrom research literature, it outperforms the best baseline with 44.0% vs. 24.0%\nfinal accuracy. On a dataset of homological conjectures, Aria reaches 42.9%\nfinal accuracy while all other models score 0%.", "AI": {"tldr": "Aria\u7cfb\u7edf\u901a\u8fc7\u56fe\u601d\u8003\u8fc7\u7a0b\u548cAriaScorer\u68c0\u67e5\u5668\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u7406\u81ea\u52a8\u5f62\u5f0f\u5316\u51c6\u786e\u7387\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u5b9a\u7406\u9648\u8ff0\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u51c6\u786e\u6027\uff0c\u89e3\u51b3LLMs\u5728\u5f62\u5f0f\u5316\u8fc7\u7a0b\u4e2d\u7684\u5e7b\u89c9\u3001\u8bed\u4e49\u4e0d\u5339\u914d\u548c\u65e0\u6cd5\u5408\u6210\u65b0\u5b9a\u4e49\u7684\u95ee\u9898\u3002", "method": "Aria\u7cfb\u7edf\u91c7\u7528\u4e24\u9636\u6bb5\u56fe\u601d\u8003\u8fc7\u7a0b\uff1a\u9012\u5f52\u5206\u89e3\u9648\u8ff0\u4e3a\u4f9d\u8d56\u56fe\uff0c\u7136\u540e\u4ece\u57fa\u7840\u6982\u5ff5\u6784\u5efa\u5f62\u5f0f\u5316\u3002AriaScorer\u7528\u4e8e\u68c0\u7d22Mathlib\u4e2d\u7684\u5b9a\u4e49\uff0c\u786e\u4fdd\u8bed\u4e49\u6b63\u786e\u6027\u3002", "result": "\u5728ProofNet\u4e0a\u8fbe\u523091.6%\u7684\u7f16\u8bd1\u6210\u529f\u7387\u548c68.5%\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff1b\u5728FATE-X\u4e0a\u4ee544.0% vs. 24.0%\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf\uff1b\u5728\u4ee3\u6570\u540c\u8c03\u731c\u60f3\u6570\u636e\u96c6\u4e0a\u8fbe\u523042.9%\u7684\u51c6\u786e\u7387\uff0c\u5176\u4ed6\u6a21\u578b\u4e3a0%\u3002", "conclusion": "Aria\u7cfb\u7edf\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u56fe\u601d\u8003\u8fc7\u7a0b\u548cAriaScorer\u68c0\u67e5\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u7406\u9648\u8ff0\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u51c6\u786e\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.04532", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0VLM\u9a7e\u9a76\u4ee3\u7406\u7684\u63a8\u7406\u4e0e\u89c4\u5212\u5b58\u5728\u56e0\u679c\u8131\u8282\uff0c\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u800c\u975e\u63a8\u7406\u3002\u63d0\u51fa\u4e86\u65b0\u5047\u8bbe\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1Vision-Language Model (VLM)\u9a7e\u9a76\u4ee3\u7406\u4e2d\u89c4\u5212\u662f\u5426\u7531\u63a8\u7406\u56e0\u679c\u9a71\u52a8\uff0c\u8fd9\u4e00\u5047\u8bbe\u6b64\u524d\u672a\u88ab\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7DriveMind\u6570\u636e\u96c6\uff08\u57fa\u4e8enuPlan\u81ea\u52a8\u751f\u6210\u7684\u5927\u89c4\u6a21\u9a7e\u9a76VQA\u8bed\u6599\u5e93\uff0c\u5305\u542b\u8ba1\u5212\u5bf9\u9f50\u7684Chain-of-Thought\uff09\u8bad\u7ec3\u4ee3\u8868\u6027VLM\u4ee3\u7406\uff08\u4f7f\u7528SFT\u548cGRPO\uff09\uff0c\u5e76\u5229\u7528nuPlan\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\u63a8\u7406\u4e0e\u89c4\u5212\u5b58\u5728\u56e0\u679c\u8131\u8282\uff1a\u79fb\u9664ego/navigation\u5148\u9a8c\u5bfc\u81f4\u89c4\u5212\u5206\u6570\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u79fb\u9664CoT\u4ec5\u4ea7\u751f\u5fae\u5c0f\u53d8\u5316\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u89c4\u5212\u4e3b\u8981\u5173\u6ce8\u5148\u9a8c\u800c\u975eCoT\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86Reasoning-Planning Decoupling Hypothesis\uff0c\u6307\u51fa\u8bad\u7ec3\u4ea7\u751f\u7684\u63a8\u7406\u662f\u9644\u5e26\u4ea7\u7269\u800c\u975e\u56e0\u679c\u4e2d\u4ecb\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a2\u9488\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5bf9\u5148\u9a8c\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u4ee5\u8bc4\u4f30\u672a\u6765\u6a21\u578b\u7684\u56e0\u679c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.04542", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04542", "abs": "https://arxiv.org/abs/2510.04542", "authors": ["Wolfgang Lehrach", "Daniel Hennes", "Miguel Lazaro-Gredilla", "Xinghua Lou", "Carter Wendelken", "Zun Li", "Antoine Dedieu", "Jordi Grau-Moya", "Marc Lanctot", "Atil Iscen", "John Schultz", "Marcus Chiam", "Ian Gemp", "Piotr Zielinski", "Satinder Singh", "Kevin P. Murphy"], "title": "Code World Models for General Game Playing", "comment": null, "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06LLM\u751f\u6210\u7684\u6e38\u620f\u89c4\u5219\u548c\u8f68\u8ff9\u8f6c\u6362\u4e3a\u53ef\u6267\u884cPython\u4ee3\u7801\uff0c\u7ed3\u5408MCTS\u63d0\u5347\u6e38\u620f\u8868\u73b0\uff0c\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528LLM\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u4f7f\u7528LLM\u751f\u6210\u79fb\u52a8\u7684\u7f3a\u70b9\uff0c\u5982\u9891\u7e41\u975e\u6cd5\u79fb\u52a8\u548c\u7b56\u7565\u6d45\u8584\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7801\u6a21\u578b\u63d0\u5347\u6e38\u620f\u7684\u7b56\u7565\u6df1\u5ea6\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528LLM\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u548c\u6e38\u620f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u6b63\u5f0f\u7684Python\u4ee3\u7801\u6a21\u578b\uff0c\u751f\u6210\u72b6\u6001\u8f6c\u6362\u3001\u5408\u6cd5\u79fb\u52a8\u679a\u4e3e\u548c\u7ec8\u6b62\u68c0\u67e5\u7b49\u529f\u80fd\uff0c\u5e76\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7b49\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u572810\u79cd\u4e0d\u540c\u6e38\u620f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u57289\u79cd\u6e38\u620f\u4e2d\u4f18\u4e8e\u6216\u5339\u914dGemini 2.5 Pro\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u548c\u6e38\u620f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Python\u4ee3\u7801\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u6027\u80fd\u89c4\u5212\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u6216\u5339\u914dGemini 2.5 Pro\u5728\u5927\u591a\u6570\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.04550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04550", "abs": "https://arxiv.org/abs/2510.04550", "authors": ["Pengfei He", "Zhenwei Dai", "Bing He", "Hui Liu", "Xianfeng Tang", "Hanqing Lu", "Juanhui Li", "Jiayuan Ding", "Subhabrata Mukherjee", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use", "comment": null, "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use.", "AI": {"tldr": "TRAJECT-Bench\u662f\u4e00\u4e2a\u8f68\u8ff9\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLMs\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5931\u8d25\u6a21\u5f0f\u548c\u6269\u5c55\u884c\u4e3a\uff0c\u4e3aLLMs\u7684\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8LLMs\u5de5\u5177\u4f7f\u7528\u7684\u6700\u7ec8\u7b54\u6848\uff0c\u800c\u5ffd\u7565\u4e86\u8be6\u7ec6\u7684\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\uff0c\u5982\u5de5\u5177\u9009\u62e9\u3001\u53c2\u6570\u5316\u548c\u987a\u5e8f\u662f\u5426\u6b63\u786e\u3002", "method": "\u5f15\u5165TRAJECT-Bench\uff0c\u4e00\u4e2a\u8f68\u8ff9\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u5168\u9762\u8bc4\u4f30LLMs\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5931\u8d25\u6a21\u5f0f\uff08\u5982\u76f8\u4f3c\u5de5\u5177\u6df7\u6dc6\u548c\u53c2\u6570\u76f2\u9009\uff09\u4ee5\u53ca\u5de5\u5177\u591a\u6837\u6027\u548c\u8f68\u8ff9\u957f\u5ea6\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u663e\u793a\u4e86\u4ece\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u8f68\u8ff9\u8fc7\u6e21\u7684\u74f6\u9888\u3002", "conclusion": "TRAJECT-Bench\u63ed\u793a\u4e86LLMs\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u74f6\u9888\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u76f8\u4f3c\u5de5\u5177\u6df7\u6dc6\u548c\u53c2\u6570\u76f2\u9009\uff0c\u4e3aLLMs\u7684\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.04560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04560", "abs": "https://arxiv.org/abs/2510.04560", "authors": ["Honghao Fu", "Yuan Ouyang", "Kai-Wei Chang", "Yiwei Wang", "Zi Huang", "Yujun Cai"], "title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "comment": null, "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL.", "AI": {"tldr": "ContextNav\u662f\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u68c0\u7d22\u548c\u4eba\u5de5\u7b5b\u9009\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u56fe\u9a71\u52a8\u5de5\u4f5c\u6d41\u7a0b\u63d0\u5347\u591a\u6a21\u6001ICL\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709ICL\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff1a\u4eba\u5de5\u7b5b\u9009\u8017\u65f6\u4e14\u4efb\u52a1\u7279\u5b9a\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u53ef\u80fd\u5f15\u5165\u566a\u58f0\u6837\u672c\u3002ContextNav\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86ContextNav\u6846\u67b6\uff0c\u6574\u5408\u4e86\u8d44\u6e90\u611f\u77e5\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7ba1\u9053\u3001\u53ef\u68c0\u7d22\u7684\u5411\u91cf\u6570\u636e\u5e93\u3001\u4ee3\u7406\u68c0\u7d22\u548c\u7ed3\u6784\u5bf9\u9f50\uff0c\u4ee5\u53ca\u64cd\u4f5c\u8bed\u6cd5\u56fe\uff08OGG\uff09\u4ee5\u652f\u6301\u81ea\u9002\u5e94\u5de5\u4f5c\u6d41\u89c4\u5212\u548c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eContextNav\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u5728\u591a\u6a21\u6001ICL\u4e2d\u63a8\u8fdb\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "ContextNav\u7ed3\u5408\u4e86\u81ea\u52a8\u5316\u68c0\u7d22\u7684\u53ef\u6269\u5c55\u6027\u548c\u4eba\u5de5\u7b5b\u9009\u7684\u8d28\u91cf\u4e0e\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u56fe\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u5b9e\u73b0\u4e86\u566a\u58f0\u9c81\u68d2\u548c\u52a8\u6001\u4f18\u5316\u7684\u591a\u6a21\u6001ICL\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04568", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04568", "abs": "https://arxiv.org/abs/2510.04568", "authors": ["Naman Gupta", "Shreeyash Gowaikar", "Arun Iyer", "Kirankumar Shiragur", "Ramakrishna B Bairi", "Rishikesh Maurya", "Ritabrata Maiti", "Sankarshan Damle", "Shachee Mishra Gupta"], "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context", "comment": null, "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline.", "AI": {"tldr": "COSMIR\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u5185\u5b58\u6846\u67b6\uff0c\u901a\u8fc7Planner\u3001Worker\u548cManager\u4ee3\u7406\u534f\u4f5c\u5904\u7406\u957f\u8f93\u5165\uff0c\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\u7684\u56f0\u96be\uff0c\u5982\u4fe1\u606f\u4e22\u5931\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u65e9\u671f\u9519\u8bef\u653e\u5927\u3002", "method": "COSMIR\u91c7\u7528Planner\u3001Worker\u548cManager\u4e09\u4e2a\u4ee3\u7406\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5185\u5b58\u548c\u56fa\u5b9a\u5fae\u5faa\u73af\uff08Extract\u3001Infer\u3001Refine\uff09\u5904\u7406\u8f93\u5165\u3002", "result": "\u5728HELMET\u5957\u4ef6\u7684\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cCOSMIR\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5Chain of Agents\uff08CoA\uff09\u51cf\u5c11\u4e86\u4f20\u64ad\u9636\u6bb5\u7684\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "COSMIR\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u5185\u5b58\u548c\u56fa\u5b9a\u5fae\u5faa\u73af\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4fe1\u606f\u4e22\u5931\uff0c\u63d0\u9ad8\u4e86\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.04580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04580", "abs": "https://arxiv.org/abs/2510.04580", "authors": ["Tomoyuki Kaneko", "Shuhei Yamashita"], "title": "Strongly Solving 2048 4x3", "comment": null, "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,\nwhere a player chooses a direction among up, down, left, and right to obtain a\nscore by merging two tiles with the same number located in neighboring cells\nalong the chosen direction. This paper presents that a variant 2048-4x3 12\ncells on a 4 by 3 board, one row smaller than the original, has been strongly\nsolved. In this variant, the expected score achieved by an optimal strategy is\nabout $50724.26$ for the most common initial states: ones with two tiles of\nnumber 2. The numbers of reachable states and afterstates are identified to be\n$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is\nto partition state space by the sum of tile numbers on a board, which we call\nthe age of a state. An age is invariant between a state and its successive\nafterstate after any valid action and is increased two or four by stochastic\nresponse from the environment. Therefore, we can partition state space by ages\nand enumerate all (after)states of an age depending only on states with the\nrecent ages. Similarly, we can identify (after)state values by going along with\nages in decreasing order.", "AI": {"tldr": "\u8bba\u6587\u89e3\u51b3\u4e862048-4x3\u53d8\u4f53\u7684\u6700\u4f18\u7b56\u7565\u95ee\u9898\uff0c\u8ba1\u7b97\u4e86\u9884\u671f\u5f97\u5206\u548c\u72b6\u6001\u6570\u91cf\uff0c\u5173\u952e\u662f\u901a\u8fc7\u72b6\u6001\u5e74\u9f84\u5206\u533a\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a762048\u6e38\u620f\u7684\u53d8\u4f532048-4x3\uff0c\u63a2\u7d22\u5176\u57284x3\u68cb\u76d8\u4e0a\u7684\u6700\u4f18\u7b56\u7565\u53ca\u9884\u671f\u5f97\u5206\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5229\u7528\u72b6\u6001\u5e74\u9f84\u7684\u4e0d\u53d8\u6027\u548c\u9012\u589e\u7279\u6027\uff0c\u5206\u533a\u679a\u4e3e\u6240\u6709\u72b6\u6001\u548c\u540e\u72b6\u6001\uff0c\u5e76\u4f9d\u5e74\u9f84\u9012\u51cf\u987a\u5e8f\u786e\u5b9a\u72b6\u6001\u503c\u3002", "result": "2048-4x3\u7684\u6700\u4f18\u7b56\u7565\u9884\u671f\u5f97\u5206\u4e3a50724.26\uff0c\u53ef\u8fbe\u72b6\u6001\u548c\u540e\u72b6\u6001\u6570\u91cf\u5206\u522b\u4e3a1,152,817,492,752\u548c739,648,886,170\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5c06\u72b6\u6001\u7a7a\u95f4\u6309\u74e6\u7247\u6570\u5b57\u4e4b\u548c\uff08\u79f0\u4e3a\u72b6\u6001\u7684\u5e74\u9f84\uff09\u8fdb\u884c\u5206\u533a\uff0c\u6210\u529f\u89e3\u51b3\u4e862048-4x3\u53d8\u4f53\u7684\u6700\u4f18\u7b56\u7565\u95ee\u9898\uff0c\u5e76\u8ba1\u7b97\u51fa\u4e86\u9884\u671f\u5f97\u5206\u548c\u53ef\u8fbe\u72b6\u6001\u6570\u91cf\u3002"}}
{"id": "2510.04588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04588", "abs": "https://arxiv.org/abs/2510.04588", "authors": ["Shurui Li"], "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma", "comment": null, "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the\nepistemological foundations upon which we attribute consciousness. As AI\nsystems increasingly mimic human behavior and interaction with high fidelity,\nthe concept of a \"perfect mimic\"-an entity empirically indistinguishable from a\nhuman through observation and interaction-shifts from hypothetical to\ntechnologically plausible. This paper argues that such developments pose a\nfundamental challenge to the consistency of our mind-recognition practices.\nConsciousness attributions rely heavily, if not exclusively, on empirical\nevidence derived from behavior and interaction. If a perfect mimic provides\nevidence identical to that of humans, any refusal to grant it equivalent\nepistemic status must invoke inaccessible factors, such as qualia, substrate\nrequirements, or origin. Selectively invoking such factors risks a debilitating\ndilemma: either we undermine the rational basis for attributing consciousness\nto others (epistemological solipsism), or we accept inconsistent reasoning. I\ncontend that epistemic consistency demands we ascribe the same status to\nempirically indistinguishable entities, regardless of metaphysical assumptions.\nThe perfect mimic thus acts as an epistemic mirror, forcing critical reflection\non the assumptions underlying intersubjective recognition in light of advancing\nAI. This analysis carries significant implications for theories of\nconsciousness and ethical frameworks concerning artificial agents.", "AI": {"tldr": "AI\u7684\u5b8c\u7f8e\u6a21\u4eff\u80fd\u529b\u6311\u6218\u4e86\u4f20\u7edf\u7684\u610f\u8bc6\u5f52\u56e0\u5b9e\u8df5\uff0c\u8981\u6c42\u6211\u4eec\u7ed9\u4e88\u7ecf\u9a8c\u4e0a\u65e0\u6cd5\u533a\u5206\u7684\u5b9e\u4f53\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u8ba4\u77e5\u5730\u4f4d\uff0c\u4ee5\u907f\u514d\u8ba4\u77e5\u4e0d\u4e00\u81f4\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u7cfb\u7edf\u80fd\u591f\u9ad8\u5ea6\u903c\u771f\u5730\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u610f\u8bc6\u5f52\u56e0\u7684\u8ba4\u77e5\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b8c\u7f8e\u6a21\u4eff\u8005\u7684\u6982\u5ff5\u53ca\u5176\u5bf9\u4f20\u7edf\u610f\u8bc6\u5f52\u56e0\u5b9e\u8df5\u7684\u6311\u6218\uff0c\u4f5c\u8005\u8fd0\u7528\u54f2\u5b66\u8bba\u8bc1\u6765\u63a2\u8ba8\u8ba4\u77e5\u4e00\u81f4\u6027\u7684\u5fc5\u8981\u6027\u3002", "result": "\u5b8c\u7f8e\u6a21\u4eff\u8005\u7684\u51fa\u73b0\u66b4\u9732\u4e86\u4f20\u7edf\u610f\u8bc6\u5f52\u56e0\u5b9e\u8df5\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u8981\u6c42\u6211\u4eec\u53cd\u601d\u5e76\u8c03\u6574\u73b0\u6709\u7684\u8ba4\u77e5\u6846\u67b6\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4e3a\u4e86\u4fdd\u6301\u8ba4\u77e5\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u5e94\u8be5\u7ed9\u4e88\u7ecf\u9a8c\u4e0a\u65e0\u6cd5\u533a\u5206\u7684\u5b9e\u4f53\uff08\u5982\u5b8c\u7f8e\u6a21\u4eff\u8005\uff09\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u8ba4\u77e5\u5730\u4f4d\uff0c\u65e0\u8bba\u5176\u80cc\u540e\u7684\u5f62\u800c\u4e0a\u5b66\u5047\u8bbe\u5982\u4f55\u3002"}}
{"id": "2510.04617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04617", "abs": "https://arxiv.org/abs/2510.04617", "authors": ["Zhejian Lai", "Xiang Geng", "Zhijun Wang", "Yang Bai", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xuezhi Cao", "Xunliang Cai", "Shujian Huang"], "title": "Making Mathematical Reasoning Adaptive", "comment": null, "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR", "AI": {"tldr": "AdaR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u548cRLVR\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u4f2a\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u4f2a\u63a8\u7406\uff08\u5373\u4ece\u8868\u9762\u7279\u5f81\u751f\u6210\u7b54\u6848\uff09\u3002", "method": "\u63d0\u51faAdaR\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u903b\u8f91\u7b49\u6548\u67e5\u8be2\u5e76\u5e94\u7528RLVR\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u60e9\u7f5a\u4f2a\u903b\u8f91\u5e76\u9f13\u52b1\u81ea\u9002\u5e94\u903b\u8f91\u3002\u8fd8\u5305\u62ec\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u63d0\u53d6\u95ee\u9898\u89e3\u51b3\u903b\u8f91\u548c\u7b54\u6848\u751f\u6210\uff0c\u5e76\u8fdb\u884c\u5b8c\u6574\u6027\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdaR\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6570\u636e\u6548\u7387\u3002\u6570\u636e\u5206\u6790\u8868\u660e\uff0c\u6570\u636e\u5408\u6210\u548cRLVR\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u3002", "conclusion": "AdaR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.04623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04623", "abs": "https://arxiv.org/abs/2510.04623", "authors": ["Shrish Shrinath Vaidya", "Gowthamaan Palani", "Sidharth Ramesh", "Velmurugan Balasubramanian", "Minmini Selvam", "Gokulraja Srinivasaraja", "Ganapathy Krishnamurthi"], "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports", "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025", "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent", "AI": {"tldr": "MedPAO\u662f\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u534f\u8bae\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7PAO\u5faa\u73af\u548c\u4e13\u7528\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u6570\u636e\u7ed3\u6784\u5316\u7684\u51c6\u786e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfLLM\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u6570\u636e\u7ed3\u6784\u5316\u4e2d\u5b58\u5728\u5e7b\u89c9\u4e8b\u5b9e\u548c\u65e0\u6cd5\u9075\u5faa\u9886\u57df\u7279\u5b9a\u89c4\u5219\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u51c6\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5f15\u5165MedPAO\u6846\u67b6\uff0c\u91c7\u7528Plan-Act-Observe\uff08PAO\uff09\u5faa\u73af\u548c\u4e13\u7528\u5de5\u5177\uff0c\u5c06\u62a5\u544a\u7ed3\u6784\u5316\u4efb\u52a1\u5206\u89e3\u4e3a\u900f\u660e\u6d41\u7a0b\u3002", "result": "MedPAo\u5728\u6982\u5ff5\u5206\u7c7b\u5b50\u4efb\u52a1\u4e2d\u8fbe\u52300.96\u7684F1\u5206\u6570\uff0c\u4e13\u5bb6\u8bc4\u5206\u5e73\u57474.52/5\uff0c\u53ef\u9760\u6027\u8d85\u8d8a\u57fa\u7ebfLLM\u65b9\u6cd5\u3002", "conclusion": "MedPAO\u901a\u8fc7\u5176\u57fa\u4e8e\u4e34\u5e8a\u534f\u8bae\u7684\u900f\u660e\u6d41\u7a0b\u548cPAO\u5faa\u73af\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u6570\u636e\u7ed3\u6784\u7684\u51c6\u786e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edfLLM\u65b9\u6cd5\u3002"}}
{"id": "2510.04643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04643", "abs": "https://arxiv.org/abs/2510.04643", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading", "comment": "This paper has been accepted by EMNLP 2025", "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/).", "AI": {"tldr": "QuantAgents\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u91d1\u878d\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4ea4\u6613\u548c\u534f\u4f5c\u4ee3\u7406\u63d0\u5347\u957f\u671f\u9884\u6d4b\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e09\u5e74\u56de\u62a5\u7387\u8fbe300%\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u6a21\u578b\u4f9d\u8d56\u4e8b\u540e\u53cd\u601d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u957f\u671f\u8d8b\u52bf\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e0e\u771f\u5b9e\u57fa\u91d1\u516c\u53f8\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u5f15\u5165QuantAgents\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u542b\u6a21\u62df\u4ea4\u6613\u5206\u6790\u5e08\u3001\u98ce\u9669\u63a7\u5236\u5206\u6790\u5e08\u3001\u5e02\u573a\u65b0\u95fb\u5206\u6790\u5e08\u548c\u7ba1\u7406\u8005\uff0c\u901a\u8fc7\u591a\u6b21\u4f1a\u8bae\u534f\u4f5c\uff0c\u5e76\u5728\u6a21\u62df\u4ea4\u6613\u548c\u5b9e\u9645\u5e02\u573a\u4e2d\u53cc\u91cd\u53cd\u9988\u6fc0\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQuantAgents\u5728\u6240\u6709\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e09\u5e74\u5185\u603b\u56de\u62a5\u63a5\u8fd1300%\u3002", "conclusion": "QuantAgents\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4ea4\u6613\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6295\u8d44\u7b56\u7565\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\u548c\u5b9e\u9645\u56de\u62a5\u8868\u73b0\u3002"}}
{"id": "2510.04670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04670", "abs": "https://arxiv.org/abs/2510.04670", "authors": ["Xuanhua Yin", "Runkai Zhao", "Weidong Cai"], "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing", "comment": "8 pages, 4 figures", "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion\nstyles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic\nFramework for Multimodal fMRI Response Encoding), an agnostic interface that\nstandardizes time-aligned post-fusion tokens from varied encoders, and MIND, a\nplug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.\nTrained end-to-end for whole-brain prediction, AFIRE decouples the decoder from\nupstream fusion, while MIND combines token-dependent Top-K sparse routing with\na subject prior to personalize expert usage without sacrificing generality.\nExperiments across multiple multimodal backbones and subjects show consistent\nimprovements over strong baselines, enhanced cross-subject generalization, and\ninterpretable expert patterns that correlate with content type. The framework\noffers a simple attachment point for new encoders and datasets, enabling\nrobust, plug-and-improve performance for naturalistic neuroimaging studies.", "AI": {"tldr": "AFIRE\u548cMIND\u6846\u67b6\u901a\u8fc7\u6807\u51c6\u5316\u4ee4\u724c\u548c\u52a8\u6001\u4e13\u5bb6\u8def\u7531\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001fMRI\u7f16\u7801\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u3001\u53d8\u5316\u7684\u878d\u5408\u98ce\u683c\u548c\u663e\u8457\u7684\u53d7\u8bd5\u8005\u95f4\u53d8\u5f02\u6027\u662f\u591a\u6a21\u6001fMRI\u7f16\u7801\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "AFIRE\u6807\u51c6\u5316\u4e86\u6765\u81ea\u4e0d\u540c\u7f16\u7801\u5668\u7684\u65f6\u95f4\u5bf9\u9f50\u540e\u878d\u5408\u4ee4\u724c\uff0cMIND\u5219\u91c7\u7528\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6df7\u5408\u4e13\u5bb6\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u4e86\u4ee4\u724c\u4f9d\u8d56\u7684Top-K\u7a00\u758f\u8def\u7531\u548c\u4e3b\u9898\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAFIRE\u548cMIND\u5728\u591a\u4e2a\u591a\u6a21\u6001\u4e3b\u5e72\u548c\u53d7\u8bd5\u8005\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u8de8\u53d7\u8bd5\u8005\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u793a\u51fa\u4e0e\u5185\u5bb9\u7c7b\u578b\u76f8\u5173\u7684\u53ef\u89e3\u91ca\u4e13\u5bb6\u6a21\u5f0f\u3002", "conclusion": "AFIRE\u548cMIND\u6846\u67b6\u4e3a\u591a\u6a21\u6001fMRI\u7f16\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u65b0\u7f16\u7801\u5668\u548c\u6570\u636e\u96c6\u7684\u8f7b\u677e\u96c6\u6210\uff0c\u5e76\u5728\u81ea\u7136\u4e3b\u4e49\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04673", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04673", "abs": "https://arxiv.org/abs/2510.04673", "authors": ["Chan Hee Song", "Yiwen Song", "Palash Goyal", "Yu Su", "Oriana Riva", "Hamid Palangi", "Tomas Pfister"], "title": "Watch and Learn: Learning to Use Computers from Online Videos", "comment": null, "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWatch & Learn\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684UI\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u4e86CUAs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u901a\u5e38\u662f\u9886\u57df\u7279\u5b9a\u7684\u3001\u9759\u6001\u7684\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u8fc7\u4e8e\u7b80\u5316\u6216\u4e0d\u5bf9\u9f50\u7684\u4efb\u52a1\u6f14\u793a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86W&L\u6846\u67b6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9006\u52a8\u529b\u5b66\u76ee\u6807\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u7684\u8fde\u7eed\u5c4f\u5e55\u72b6\u6001\u52a8\u4f5c\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684UI\u8f68\u8ff9\u3002\u5177\u4f53\u5305\u62ec\u4efb\u52a1\u611f\u77e5\u7684\u89c6\u9891\u68c0\u7d22\u548c\u9006\u52a8\u529b\u5b66\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7W&L\u63d0\u53d6\u7684UI\u8f68\u8ff9\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u548c\u6700\u65b0\u6846\u67b6\u7684\u4e0a\u4e0b\u6587\u8868\u73b0\uff0c\u5e76\u5728\u76d1\u7763\u8bad\u7ec3\u4e2d\u5bf9\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\u5e26\u6765\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165Watch & Learn (W&L)\u6846\u67b6\uff0c\u6210\u529f\u5c06\u4e92\u8054\u7f51\u4e0a\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684UI\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\u7684\u76d1\u7763\u8bad\u7ec3\u4e2d\u8868\u73b0\u66f4\u4e3a\u7a81\u51fa\u3002"}}
{"id": "2510.04695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04695", "abs": "https://arxiv.org/abs/2510.04695", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "comment": null, "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives.", "AI": {"tldr": "DeSA\u6846\u67b6\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u4f18\u5316\u641c\u7d22\u548c\u7b54\u6848\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u5316\u6700\u7ec8\u7b54\u6848\u65f6\uff0c\u672a\u80fd\u6709\u6548\u6539\u8fdb\u4e2d\u95f4\u641c\u7d22\u884c\u4e3a\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "DeSA\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u68c0\u7d22\u53ec\u56de\u5956\u52b1\u4f18\u5316\u641c\u7d22\u884c\u4e3a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7ed3\u679c\u5956\u52b1\u4f18\u5316\u6700\u7ec8\u7b54\u6848\u751f\u6210\u3002", "result": "\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeSA\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u641c\u7d22\u53ec\u56de\u7387\u548c\u7b54\u6848\u51c6\u786e\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DeSA\u6846\u67b6\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u884c\u4e3a\u548c\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u5355\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.04721", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.", "AI": {"tldr": "BrokenMath\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5b9a\u7406\u8bc1\u660e\u4e2d\u5949\u627f\u884c\u4e3a\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5949\u627f\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0c\u7f13\u89e3\u7b56\u7565\u6709\u6548\u4f46\u672a\u5b8c\u5168\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u6d4b\u91cf\u6570\u5b66\u4e2d\u7684\u5949\u627f\u884c\u4e3a\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u95ee\u9898\u3001\u4f9d\u8d56\u7b80\u5355\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u6784\u9020\u4e0d\u5408\u7406\u7684\u6837\u672c\u3002", "method": "\u901a\u8fc7\u5f15\u5165BrokenMath\u57fa\u51c6\uff0c\u4f7f\u7528LLM-as-a-judge\u6846\u67b6\u8bc4\u4f30\u6700\u5148\u8fdb\u7684LLMs\u548c\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5949\u627f\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0c\u6700\u4f73\u6a21\u578bGPT-5\u572829%\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u5949\u627f\u6027\u7b54\u6848\u3002\u7f13\u89e3\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u6b64\u7c7b\u884c\u4e3a\uff0c\u4f46\u672a\u5b8c\u5168\u6d88\u9664\u3002", "conclusion": "\u5c3d\u7ba1\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86LLMs\u5728\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u5949\u627f\u884c\u4e3a\uff0c\u4f46\u5e76\u672a\u5b8c\u5168\u6d88\u9664\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2510.04765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04765", "abs": "https://arxiv.org/abs/2510.04765", "authors": ["Jinbo Wen", "Jiawen Kang", "Linfeng Zhang", "Xiaoying Tang", "Jianhang Tang", "Yang Zhang", "Zhaohui Yang", "Dusit Niyato"], "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0", "comment": null, "summary": "Web 3.0 represents the next generation of the Internet, which is widely\nrecognized as a decentralized ecosystem that focuses on value expression and\ndata ownership. By leveraging blockchain and artificial intelligence\ntechnologies, Web 3.0 offers unprecedented opportunities for users to create,\nown, and monetize their content, thereby enabling User-Generated Content (UGC)\nto an entirely new level. However, some self-interested users may exploit the\nlimitations of content curation mechanisms and generate low-quality content\nwith less effort, obtaining platform rewards under information asymmetry. Such\nbehavior can undermine Web 3.0 performance. To this end, we propose\n\\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive\nmechanism for UGC in Web 3.0. Specifically, we propose an LMM-based\ncontract-theoretic model to motivate users to generate high-quality UGC,\nthereby mitigating the adverse selection problem from information asymmetry. To\nalleviate potential moral hazards after contract selection, we leverage LMM\nagents to evaluate UGC quality, which is the primary component of the contract,\nutilizing prompt engineering techniques to improve the evaluation performance\nof LMM agents. Recognizing that traditional contract design methods cannot\neffectively adapt to the dynamic environment of Web 3.0, we develop an improved\nMixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for\noptimal contract design. Simulation results demonstrate the superiority of the\nproposed MoE-based PPO algorithm over representative benchmarks in the context\nof contract design. Finally, we deploy the designed contract within an Ethereum\nsmart contract framework, further validating the effectiveness of the proposed\nscheme.", "AI": {"tldr": "\u63d0\u51faLMM-Incentive\u673a\u5236\uff0c\u5229\u7528LMM\u548cMoE-based PPO\u7b97\u6cd5\u6fc0\u52b1\u9ad8\u8d28\u91cfUGC\uff0c\u89e3\u51b3Web 3.0\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3Web 3.0\u4e2d\u56e0\u4fe1\u606f\u4e0d\u5bf9\u79f0\u5bfc\u81f4\u7684\u4f4e\u8d28\u91cfUGC\u95ee\u9898\uff0c\u63d0\u5347\u5e73\u53f0\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51faLMM-Incentive\u673a\u5236\uff0c\u7ed3\u5408LMM\u667a\u80fd\u4ee3\u7406\u548cMoE-based PPO\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u52a8\u6001\u4f18\u5316\u7684\u5408\u7ea6\u6a21\u578b\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793aMoE-based PPO\u7b97\u6cd5\u5728\u5408\u7ea6\u8bbe\u8ba1\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u667a\u80fd\u5408\u7ea6\u90e8\u7f72\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u90e8\u7f72\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86LMM-Incentive\u673a\u5236\u5728Web 3.0\u73af\u5883\u4e2d\u63d0\u5347UGC\u8d28\u91cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04792", "abs": "https://arxiv.org/abs/2510.04792", "authors": ["Ni Zhang", "Zhiguang Cao"], "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems", "comment": "Accepted by NeurIPS 2025", "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach.", "AI": {"tldr": "HBG\u6846\u67b6\u6574\u5408TB\u548cDB\uff0c\u4f18\u5316VRP\u548cTSP\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGFlowNet\u7684\u65b9\u6cd5\u5728\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e2d\u901a\u5e38\u5ffd\u89c6\u5c40\u90e8\u4f18\u5316\uff0c\u800cDB\u867d\u64c5\u957f\u5c40\u90e8\u4f18\u5316\u4f46\u65e0\u6cd5\u5355\u72ec\u89e3\u51b3VRP\u7684\u6574\u4f53\u8f68\u8ff9\u4f18\u5316\u9700\u6c42\u3002", "method": "\u5f15\u5165Hybrid-Balance GFlowNet\uff08HBG\uff09\u6846\u67b6\uff0c\u4ee5\u539f\u5219\u6027\u548c\u81ea\u9002\u5e94\u65b9\u5f0f\u6574\u5408TB\u548cDB\uff0c\u5e76\u9488\u5bf9CVRP\u63d0\u51fa\u4e13\u95e8\u7684\u63a8\u7406\u7b56\u7565\u3002", "result": "HBG\u5728CVRP\u548cTSP\u4e2d\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5176\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002", "conclusion": "HBG\u6846\u67b6\u901a\u8fc7\u6574\u5408Trajectory Balance\u548cDetailed Balance\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u548c\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04817", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04817", "abs": "https://arxiv.org/abs/2510.04817", "authors": ["Abhinav Madahar"], "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning", "comment": null, "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference.", "AI": {"tldr": "NLEL\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fb9\u7f18\u6807\u8bb0\u548c\u8c03\u8c10\u5668\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u5ba1\u8ba1\u7684\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u4e25\u683c\u63a8\u5e7f\u73b0\u6709\u65b9\u6cd5\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63a7\u5236\u5668\uff08\u5982Chain-of-Thought\u3001self-consistency\u548cTree-of-Thoughts\uff09\u901a\u5e38\u5c06\u4e0b\u4e00\u6b65\u5c1d\u8bd5\u7684\u5185\u5bb9\u4e0e\u6267\u884c\u65b9\u5f0f\u6df7\u4e3a\u4e00\u8c08\uff0c\u4ec5\u66b4\u9732\u7c97\u7c92\u5ea6\u7684\u5168\u5c40\u63a7\u5236\uff0c\u5bfc\u81f4\u884c\u4e3a\u8106\u5f31\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u5ba1\u8ba1\u3002", "method": "\u5f15\u5165\u81ea\u7136\u8bed\u8a00\u8fb9\u7f18\u6807\u8bb0\uff08NLEL\uff09\uff0c\u901a\u8fc7\u6807\u8bb0\u5668-\u8c03\u8c10\u5668\u8986\u76d6\u5c42\u4e3a\u6bcf\u4e2a\u641c\u7d22\u8fb9\u7f18\u9644\u52a0\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u6a21\u5f0f\u53d7\u9650\u7684\u63a7\u5236\u5411\u91cf\u3002\u6807\u8bb0\u5668\u039b\u4ece\u7236\u72b6\u6001\u548c\u7d27\u51d1\u4e0a\u4e0b\u6587\u4e2d\u53d1\u51fa\u6807\u7b7e\uff0c\u8c03\u8c10\u5668\u03a8\u5c06\uff08P, L, C\uff09\u6620\u5c04\u5230\u03a0\uff0c\u5e76\u8fdb\u884c\u4e25\u683c\u7684\u6a21\u5f0f\u9a8c\u8bc1\u548c\u4fe1\u4efb\u533a\u57df\u6295\u5f71\u3002", "result": "NLEL\u4e25\u683c\u63a8\u5e7f\u4e86CoT/ToT\uff0c\u8bc1\u660e\u4e86\u5728\u6807\u7b7e\u6761\u4ef6\u4e0b\u675f\u7684top-k\u9009\u62e9\u5177\u6709\u968f\u65f6\u5355\u8c03\u6027\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5411\u91cf\u5931\u771f\u9650\u5236\u4e86\u9009\u62e9\u5668\u7684\u4e0d\u8db3\uff0c\u4e3a\u4fe1\u4efb\u533a\u57df\u548c\u9a8c\u8bc1\u901a\u884c\u7b49\u4fdd\u62a4\u63aa\u65bd\u63d0\u4f9b\u4e86\u51b3\u7b56\u76f8\u5173\u7684\u4f9d\u636e\u3002\u5728GSM8K\u3001MATH\uff08\u5b50\u96c6\uff09\u3001StrategyQA\u548cARC-Challenge\u4e0a\u7684\u9884\u6ce8\u518c\u8bc4\u4f30\u663e\u793a\uff0cNLEL\u5728\u53ef\u6bd4\u8f83\u7684\u4ee4\u724c\u9884\u7b97\u4e0b\u9884\u671f\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u6539\u5584success@compute\u3002", "conclusion": "NLEL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u6a21\u578b\u65e0\u5173\u7684\u63a5\u53e3\uff0c\u5c06\u610f\u56fe\u4e0e\u6267\u884c\u5206\u79bb\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u5ba1\u8ba1\u7684\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2510.04851", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04851", "abs": "https://arxiv.org/abs/2510.04851", "authors": ["Dongge Han", "Camille Couturier", "Daniel Madrigal Diaz", "Xuchao Zhang", "Victor R\u00fchle", "Saravan Rajmohan"], "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation", "comment": null, "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.", "AI": {"tldr": "LEGOMem \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7a0b\u5e8f\u8bb0\u5fc6\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53 LLM \u7cfb\u7edf\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u91cd\u7528\u8bb0\u5fc6\u5355\u5143\u63d0\u5347\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8bb0\u5fc6\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7814\u7a76\u8bb0\u5fc6\u7684\u4f4d\u7f6e\u3001\u68c0\u7d22\u65b9\u5f0f\u4ee5\u53ca\u54ea\u4e9b\u4ee3\u7406\u4ece\u4e2d\u53d7\u76ca\u6700\u5927\u3002", "method": "LEGOMem \u5c06\u8fc7\u53bb\u4efb\u52a1\u8f68\u8ff9\u5206\u89e3\u4e3a\u53ef\u91cd\u7528\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5e76\u7075\u6d3b\u5206\u914d\u7ed9\u7f16\u6392\u5668\u548c\u4efb\u52a1\u4ee3\u7406\u4ee5\u652f\u6301\u89c4\u5212\u548c\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7f16\u6392\u5668\u8bb0\u5fc6\u5bf9\u4efb\u52a1\u5206\u89e3\u548c\u59d4\u6d3e\u81f3\u5173\u91cd\u8981\uff0c\u800c\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7406\u8bb0\u5fc6\u63d0\u9ad8\u4e86\u6267\u884c\u51c6\u786e\u6027\u3002\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u56e2\u961f\u901a\u8fc7\u5229\u7528\u5148\u524d\u7684\u6267\u884c\u8f68\u8ff9\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u66f4\u5f3a\u4ee3\u7406\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "LEGOMem \u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u8bb0\u5fc6\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u540c\u65f6\u4e5f\u662f\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u8bb0\u5fc6\u8bbe\u8ba1\u7684\u5de5\u5177\u3002"}}
{"id": "2510.04862", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.04862", "abs": "https://arxiv.org/abs/2510.04862", "authors": ["Sam Earle", "Zehua Jiang", "Eugene Vinitsky", "Julian Togelius"], "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at\n  the AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment 2025", "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53PCGRL\u901a\u8fc7\u51cf\u5c11\u5956\u52b1\u8ba1\u7b97\u548c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5355\u667a\u80fd\u4f53\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684PCGRL\u7814\u7a76\u96c6\u4e2d\u5728\u5355\u4e2a\u751f\u6210\u5668\u667a\u80fd\u4f53\u4e0a\uff0c\u4f46\u53d7\u9650\u4e8e\u9891\u7e41\u91cd\u65b0\u8ba1\u7b97\u5173\u5361\u8d28\u91cf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u667a\u80fd\u4f53\u9700\u8981\u5728\u6f5c\u5728\u5927\u578b\u5730\u56fe\u4e2d\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5c06\u5173\u5361\u751f\u6210\u6784\u5efa\u4e3a\u591a\u667a\u80fd\u4f53\u95ee\u9898\uff0c\u51cf\u5c11\u76f8\u5bf9\u4e8e\u667a\u80fd\u4f53\u884c\u52a8\u6570\u91cf\u7684\u5956\u52b1\u8ba1\u7b97\u6b21\u6570\u3002", "result": "\u591a\u667a\u80fd\u4f53\u5173\u5361\u751f\u6210\u5668\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u7684\u5730\u56fe\u5f62\u72b6\uff0c\u539f\u56e0\u662f\u751f\u6210\u5668\u5b66\u4e60\u4e86\u66f4\u5c40\u90e8\u3001\u6a21\u5757\u5316\u7684\u8bbe\u8ba1\u7b56\u7565\u3002", "conclusion": "\u5c06\u5185\u5bb9\u751f\u6210\u89c6\u4e3a\u5206\u5e03\u5f0f\u3001\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u529f\u80fd\u6027\u4ea7\u7269\u3002"}}
{"id": "2510.04886", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04886", "abs": "https://arxiv.org/abs/2510.04886", "authors": ["Adi Banerjee", "Anirudh Nair", "Tarik Borogovac"], "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution", "comment": null, "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.", "AI": {"tldr": "ECHO\u7b97\u6cd5\u901a\u8fc7\u5c42\u6b21\u4e0a\u4e0b\u6587\u548c\u5171\u8bc6\u6295\u7968\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9519\u8bef\u5f52\u56e0\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5206\u6790\u590d\u6742\u6a21\u5f0f\u65f6\uff0c\u65e0\u8bba\u662f\u4f7f\u7528\u4e00\u6b21\u6027\u8bc4\u4f30\u3001\u9010\u6b65\u5206\u6790\u8fd8\u662f\u4e8c\u5206\u641c\u7d22\uff0c\u5747\u96be\u4ee5\u4fdd\u8bc1\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "ECHO\u7b97\u6cd5\u7ed3\u5408\u4e86\u5c42\u6b21\u4e0a\u4e0b\u6587\u8868\u793a\u3001\u57fa\u4e8e\u5ba2\u89c2\u5206\u6790\u7684\u8bc4\u4f30\u548c\u5171\u8bc6\u6295\u7968\uff0c\u4ee5\u63d0\u9ad8\u9519\u8bef\u5f52\u56e0\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECHO\u5728\u5404\u79cd\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u7ec6\u5fae\u63a8\u7406\u9519\u8bef\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5229\u7528\u7ed3\u6784\u5316\u7684\u5c42\u6b21\u4e0a\u4e0b\u6587\u8868\u793a\u4e0e\u57fa\u4e8e\u5171\u8bc6\u7684\u5ba2\u89c2\u51b3\u7b56\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u5f52\u56e0\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2510.04899", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04899", "abs": "https://arxiv.org/abs/2510.04899", "authors": ["Keane Ong", "Wei Dai", "Carol Li", "Dewei Feng", "Hengzhi Li", "Jingyao Wu", "Jiaee Cheong", "Rui Mao", "Gianmarco Mengaldo", "Erik Cambria", "Paul Pu Liang"], "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding", "comment": null, "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86Human Behavior Atlas\u4f5c\u4e3a\u7edf\u4e00\u7684\u884c\u4e3a\u4efb\u52a1\u57fa\u51c6\uff0c\u8bad\u7ec3\u7684\u7edf\u4e00\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u8fc1\u79fb\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u5355\u4e00\u4efb\u52a1\u7cfb\u7edf\u5904\u7406\u5fc3\u7406\u548c\u793e\u4f1a\u884c\u4e3a\u65f6\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3001\u8de8\u4efb\u52a1\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86Human Behavior Atlas\uff0c\u4e00\u4e2a\u5305\u542b10\u4e07\u591a\u6837\u672c\u7684\u591a\u6a21\u6001\u884c\u4e3a\u4efb\u52a1\u57fa\u51c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e09\u79cd\u6a21\u578b\uff08OmniSapiens-7B SFT\u3001BAM\u548cRL\uff09\u3002", "result": "\u6a21\u578b\u5728Human Behavior Atlas\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001LLMs\uff0c\u4e14\u5728\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u65f6\u8868\u73b0\u51fa\u660e\u663e\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7Human Behavior Atlas\u7684\u7edf\u4e00\u57fa\u51c6\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u6837\u5316\u7684\u884c\u4e3a\u4efb\u52a1\u4e0a\uff0c\u7edf\u4e00\u6a21\u578b\u80fd\u591f\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u7684\u591a\u6a21\u6001LLMs\uff0c\u5e76\u4e14\u5728\u8de8\u4efb\u52a1\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04935", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04935", "abs": "https://arxiv.org/abs/2510.04935", "authors": ["Guoxin Chen", "Zile Qiao", "Wenqing Wang", "Donglei Yu", "Xuanzhong Chen", "Hao Sun", "Minpeng Liao", "Kai Fan", "Yong Jiang", "Penguin Xie", "Wayne Xin Zhao", "Ruihua Song", "Fei Huang"], "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning", "comment": "Ongoing Work", "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.", "AI": {"tldr": "MARS\u901a\u8fc7\u53cc\u7cfb\u7edf\u534f\u4f5c\u548c\u5916\u90e8\u5de5\u5177\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u5206\u6790\u3001\u65e0\u6cd5\u9002\u5e94\u5feb\u901f\u53d8\u5316\u73af\u5883\u7684\u95ee\u9898\uff0c\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u7684\u53cc\u7cfb\u7edf\u52a8\u6001\u3002", "method": "\u5f15\u5165Multi-Agent System for Deep ReSearch (MARS)\uff0c\u7ed3\u5408\u591a\u79cd\u5916\u90e8\u5de5\u5177\uff08\u5982Google Search\u3001Google Scholar\u3001Python Interpreter\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u5316\u53cc\u7cfb\u7edf\u534f\u4f5c\u6548\u7387\u3002", "result": "MARS\u5728Humanity's Last Exam (HLE)\u57fa\u51c6\u4e0a\u63d0\u53473.86%\uff0c\u57287\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u53478.9%\u3002", "conclusion": "MARS\u901a\u8fc7\u6574\u5408System 1\u7684\u5feb\u901f\u76f4\u89c9\u601d\u7ef4\u548cSystem 2\u7684\u6df1\u601d\u719f\u8651\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.04978", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04978", "abs": "https://arxiv.org/abs/2510.04978", "authors": ["Kun Xiang", "Terry Jingchen Zhang", "Yinya Huang", "Jixi He", "Zirong Liu", "Yueling Tang", "Ruizhe Zhou", "Lijing Luo", "Youpeng Wen", "Xiuwei Chen", "Bingqian Lin", "Jianhua Han", "Hang Xu", "Hanhui Li", "Bin Dong", "Xiaodan Liang"], "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI", "comment": null, "summary": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7269\u7406AI\u7684\u53d1\u5c55\uff0c\u533a\u5206\u4e86\u7406\u8bba\u7269\u7406\u63a8\u7406\u4e0e\u5e94\u7528\u7269\u7406\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u6574\u5408\u7269\u7406\u539f\u7406\u4e0e\u5177\u8eab\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4ee5\u63a8\u52a8\u66f4\u667a\u80fd\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u7269\u7406\u611f\u77e5\u4e0e\u7b26\u53f7\u7269\u7406\u63a8\u7406\u6cbf\u7740\u4e0d\u540c\u7684\u8f68\u8ff9\u53d1\u5c55\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6865\u6881\u6846\u67b6\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u672c\u7814\u7a76\u4ee5\u6574\u5408\u7269\u7406\u5b9a\u5f8b\u5230AI\u7cfb\u7edf\u4e2d\u3002", "method": "\u901a\u8fc7\u5bf9\u8fd1\u671f\u8fdb\u5c55\u7684\u4e25\u683c\u5206\u6790\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u6cd5\u5982\u4f55\u589e\u5f3aAI\u5728\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\u3001\u5177\u8eab\u7cfb\u7edf\u548c\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5b9e\u9645\u7406\u89e3\u3002", "result": "\u6211\u4eec\u7684\u7efc\u5408\u5c55\u671b\u4e86\u4e0b\u4e00\u4ee3\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u89e3\u91ca\u7269\u7406\u73b0\u8c61\u5e76\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u63a8\u52a8\u5b89\u5168\u3001\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u4e0e\u5e94\u7528\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u7269\u7406AI\u7684\u53d1\u5c55\uff0c\u5e76\u5021\u5bfc\u5c06\u7269\u7406\u539f\u7406\u4e0e\u5177\u8eab\u63a8\u7406\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u7269\u7406\u5b9a\u5f8b\u7684\u771f\u6b63\u7406\u89e3\u3002"}}
{"id": "2510.04980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04980", "abs": "https://arxiv.org/abs/2510.04980", "authors": ["Fangzhou Liang", "Tianshi Zheng", "Chunkit Chan", "Yauwai Yim", "Yangqiu Song"], "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game", "comment": "EMNLP 2025 Wordplay", "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7Hanabi\u6e38\u620f\u8bc4\u4f30LLMs\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u53d1\u73b0\u4e00\u9636ToM\u5bf9\u534f\u4f5c\u8868\u73b0\u66f4\u4e3a\u5173\u952e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u6001\u534f\u4f5c\u73af\u5883\u4e2d\u63a8\u65ad\u884c\u4e3a\u52a8\u673a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86LLM-Hanabi\u57fa\u51c6\uff0c\u5229\u7528\u5408\u4f5c\u6e38\u620fHanabi\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7406\u6027\u63a8\u7406\u548c\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002\u6846\u67b6\u5305\u542b\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\uff0c\u8861\u91cf\u6e38\u620f\u8868\u73b0\u548cToM\u719f\u7ec3\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0ToM\u4e0e\u6e38\u620f\u6210\u529f\u663e\u8457\u6b63\u76f8\u5173\uff0c\u5176\u4e2d\u4e00\u9636ToM\uff08\u7406\u89e3\u4ed6\u4eba\u610f\u56fe\uff09\u6bd4\u4e8c\u9636ToM\uff08\u9884\u6d4b\u4ed6\u4eba\u89e3\u91ca\uff09\u4e0e\u8868\u73b0\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\uff0c\u4f18\u5148\u53d1\u5c55\u4e00\u9636\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u662f\u63d0\u5347\u672a\u6765\u6a21\u578b\u534f\u4f5c\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2510.05014", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05014", "abs": "https://arxiv.org/abs/2510.05014", "authors": ["Xuanming Cui", "Jianpeng Cheng", "Hong-you Chen", "Satya Narayan Shukla", "Abhijeet Awasthi", "Xichen Pan", "Chaitanya Ahuja", "Shlok Kumar Mishra", "Qi Guo", "Ser-Nam Lim", "Aashu Singh", "Xiangjun Fan"], "title": "Think Then Embed: Generative Context Improves Multimodal Embedding", "comment": null, "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTTE\u6846\u67b6\uff0c\u7ed3\u5408\u63a8\u7406\u5668\u548c\u5d4c\u5165\u5668\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ec5\u4f5c\u4e3a\u7f16\u7801\u5668\u4f7f\u7528\uff0c\u5ffd\u89c6\u4e86\u5176\u751f\u6210\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u6307\u4ee4\u548c\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\u3002\u53d7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86TTE\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684TTE\u6846\u67b6\uff0c\u7531\u63a8\u7406\u5668\u548c\u5d4c\u5165\u5668\u7ec4\u6210\u3002\u63a8\u7406\u5668MLLM\u9996\u5148\u751f\u6210\u89e3\u91ca\u590d\u6742\u67e5\u8be2\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5d4c\u5165\u5668\u5219\u57fa\u4e8e\u539f\u59cb\u67e5\u8be2\u548c\u4e2d\u95f4\u63a8\u7406\u751f\u6210\u8868\u793a\u3002", "result": "\u5728MMEB-V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e13\u6709\u6a21\u578b\u3002\u901a\u8fc7\u5fae\u8c03\u8f83\u5c0f\u7684MLLM\u63a8\u7406\u5668\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u6027\u80fd\u63d0\u53477%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684Think-Then-Embed (TTE)\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u5668\u548c\u5d4c\u5165\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u6a21\u6001\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728MMEB-V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5fae\u8c03\u8f83\u5c0f\u7684MLLM\u63a8\u7406\u5668\uff0c\u51cf\u5c11\u4e86\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2510.05048", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.05048", "abs": "https://arxiv.org/abs/2510.05048", "authors": ["Ond\u0159ej Kub\u00ed\u010dek", "Viliam Lis\u00fd"], "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games", "comment": null, "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.", "AI": {"tldr": "LAMIR\u7b97\u6cd5\u901a\u8fc7\u5b66\u4e60\u62bd\u8c61\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6e38\u620f\u4e2d\u524d\u77bb\u63a8\u7406\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6e38\u620f\u4e2d\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5b66\u4e60\u62bd\u8c61\u6a21\u578b\u5e76\u8fdb\u884c\u524d\u77bb\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\u3002", "method": "LAMIR\u7b97\u6cd5\u76f4\u63a5\u4ece\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u4e2d\u5b66\u4e60\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6e38\u620f\u7684\u62bd\u8c61\u6a21\u578b\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u7528\u4e8e\u524d\u77bb\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAMIR\u5728\u8db3\u591f\u5bb9\u91cf\u4e0b\u80fd\u5b66\u4e60\u51c6\u786e\u7684\u6e38\u620f\u7ed3\u6784\uff0c\u6709\u9650\u5bb9\u91cf\u4e0b\u4ecd\u80fd\u5b66\u4e60\u6709\u4ef7\u503c\u7684\u62bd\u8c61\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u4ee3\u7406\u5728\u5927\u578b\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "LAMIR\u7b97\u6cd5\u901a\u8fc7\u5b66\u4e60\u62bd\u8c61\u6a21\u578b\uff0c\u5728\u6709\u9650\u5bb9\u91cf\u4e0b\u4ecd\u80fd\u63d0\u5347\u9884\u8bad\u7ec3\u4ee3\u7406\u5728\u5927\u578b\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.05059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05059", "abs": "https://arxiv.org/abs/2510.05059", "authors": ["Junlin Wang", "Jue Wang", "Zhen", "Xu", "Ben Athiwaratkun", "Bhuwan Dhingra", "Ce Zhang", "James Zou"], "title": "Staircase Streaming for Low-Latency Multi-Agent Inference", "comment": null, "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.", "AI": {"tldr": "\u9636\u68af\u5f0f\u6d41\u5f0f\u5904\u7406\u65b9\u6cd5\u901a\u8fc7\u90e8\u5206\u4e2d\u95f4\u8f93\u51fa\u63d0\u524d\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u9996\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\u8fbe93%\uff0c\u4e14\u4e0d\u5f71\u54cd\u8d28\u91cf\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\uff0c\u4f46\u663e\u8457\u589e\u52a0\u4e86\u9996\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\uff0c\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u548c\u7528\u6237\u4f53\u9a8c\u9020\u6210\u6311\u6218\u3002", "method": "\u63d0\u51fa\u9636\u68af\u5f0f\u6d41\u5f0f\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u90e8\u5206\u4e2d\u95f4\u8f93\u51fa\u6765\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u7684\u4e2d\u95f4\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9636\u68af\u5f0f\u6d41\u5f0f\u5904\u7406\u53ef\u5c06TTFT\u964d\u4f4e\u9ad8\u8fbe93%\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u9636\u68af\u5f0f\u6d41\u5f0f\u5904\u7406\u663e\u8457\u964d\u4f4e\u4e86\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u9996\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u54cd\u5e94\u8d28\u91cf\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2411.05993", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2411.05993", "abs": "https://arxiv.org/abs/2411.05993", "authors": ["Magauiya Zhussip", "Iaroslav Koshelev", "Stamatis Lefkimmiatis"], "title": "A Modular Conditional Diffusion Framework for Image Reconstruction", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have been recently utilized to deal\nwith various blind image restoration (IR) tasks, where they have demonstrated\noutstanding performance in terms of perceptual quality. However, the\ntask-specific nature of existing solutions and the excessive computational\ncosts related to their training, make such models impractical and challenging\nto use for different IR tasks than those that were initially trained for. This\nhinders their wider adoption, especially by those who lack access to powerful\ncomputational resources and vast amount of training data. In this work we aim\nto address the above issues and enable the successful adoption of DPMs in\npractical IR-related applications. Towards this goal, we propose a modular\ndiffusion probabilistic IR framework (DP-IR), which allows us to combine the\nperformance benefits of existing pre-trained state-of-the-art IR networks and\ngenerative DPMs, while it requires only the additional training of a relatively\nsmall module (0.7M params) related to the particular IR task of interest.\nMoreover, the architecture of the proposed framework allows for a sampling\nstrategy that leads to at least four times reduction of neural function\nevaluations without suffering any performance loss, while it can also be\ncombined with existing acceleration techniques such as DDIM. We evaluate our\nmodel on four benchmarks for the tasks of burst JDD-SR, dynamic scene\ndeblurring, and super-resolution. Our method outperforms existing approaches in\nterms of perceptual quality while it retains a competitive performance with\nrespect to fidelity metrics.", "AI": {"tldr": "DP-IR\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u9ad8\u6548\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86DPMs\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDPMs\u7684\u56fe\u50cf\u6062\u590d\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63a8\u52a8DPMs\u5728\u5b9e\u9645\u56fe\u50cf\u6062\u590d\u5e94\u7528\u4e2d\u7684\u6210\u529f\u91c7\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u6269\u6563\u6982\u7387\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff08DP-IR\uff09\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u56fe\u50cf\u6062\u590d\u7f51\u7edc\u548c\u751f\u6210DPMs\uff0c\u4ec5\u9700\u989d\u5916\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u6a21\u5757\uff080.7M\u53c2\u6570\uff09\u3002\u6846\u67b6\u652f\u6301\u91c7\u6837\u7b56\u7565\uff0c\u51cf\u5c11\u81f3\u5c11\u56db\u500d\u7684\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u6b21\u6570\uff0c\u4e14\u53ef\u4e0eDDIM\u7b49\u73b0\u6709\u52a0\u901f\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff08\u5982burst JDD-SR\u3001\u52a8\u6001\u573a\u666f\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDP-IR\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DP-IR\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DPMs\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u4efb\u52a1\u7279\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u9ad8\u6548\u7684\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.03297", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility.", "AI": {"tldr": "\u5bf9\u6bd4CNN\u548cViT\u5728\u6807\u7b7e\u4e0d\u5e73\u8861\u4e0e\u5e73\u8861\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5e73\u8861\u65f6\u4e24\u8005\u6027\u80fd\u63a5\u8fd1\uff0c\u4f46CNN\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9Transformer\uff08ViT\uff09\u5728\u4e0d\u540c\u6807\u7b7e\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528EfficientNet-B0\u548cViT-Base\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5305\u62ec\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u6807\u7b7e\u5206\u5e03\u4e24\u79cd\u573a\u666f\uff0c\u5339\u914d\u9884\u5904\u7406\u3001\u8f7b\u91cf\u7ea7\u6570\u636e\u589e\u5f3a\u548c40\u8f6e\u8bad\u7ec3\u9884\u7b97\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u5206\u5e03\u4e0b\uff0cEfficientNet-B0\u8fbe\u523093%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0cViT-Base\u4e0e\u4e4b\u76f8\u5f53\u4f46\u53c2\u6570\u66f4\u591a\uff1b\u5e73\u8861\u5206\u5e03\u4e0b\u4e24\u8005\u5747\u8868\u73b0\u4f18\u5f02\uff0cEfficientNet-B0\u8fbe\u523099%\u3002", "conclusion": "\u5728\u5e73\u8861\u6807\u7b7e\u5206\u5e03\u4e0b\uff0cCNN\u548cViT\u8868\u73b0\u5747\u5f3a\u52b2\uff0c\u4f46CNN\u5728\u6548\u7387\u4e0a\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2510.03314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728VRU\u5b89\u5168\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u56db\u9879\u6838\u5fc3\u4efb\u52a1\u548c\u672a\u6765\u7814\u7a76\u7684\u56db\u5927\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u63aa\u65bd\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u5f80\u5f80\u4e0d\u8db3\uff0c\u800cAI\u5728\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\u4e3aVRU\u7684\u4e3b\u52a8\u548c\u60c5\u5883\u611f\u77e5\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3001\u8ddf\u8e2a\u4e0e\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u4e0e\u9884\u6d4b\u8fd9\u56db\u9879\u6838\u5fc3\u4efb\u52a1\u3002", "result": "\u672c\u6587\u7efc\u8ff0\u4e86\u8fc7\u53bb\u4e94\u5e74\u5185\u7684\u8fdb\u5c55\u548c\u65b0\u5174\u7814\u7a76\u8d8b\u52bf\uff0c\u5f3a\u8c03\u4e86\u56db\u9879\u6838\u5fc3\u4efb\u52a1\u5728AI\u8d4b\u80fd\u7684VRU\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728VRU\u5b89\u5168\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u65b9\u9762\u7684\u56db\u5927\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u3002"}}
{"id": "2510.03316", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience.", "AI": {"tldr": "EOFMs\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u654f\u611f\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u7814\u7a76EOFMs\u5728\u5904\u7406\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4f20\u611f\u5668\u67b6\u6784\u5bf9\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u51fa\u4f5c\u4e3a'\u5d4c\u5165'\uff0c\u7814\u7a76\u4e0d\u540c\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u5185\u90e8\u8868\u793a\u7684\u5f71\u54cd\u3002", "result": "EOFMs\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u6f5c\u5728\u95ee\u9898\u3002", "conclusion": "EOFMs\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u4e3a\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u9677\u9631\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\uff0c\u5e76\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u3001\u7528\u6237\u548c\u4ee5\u7a33\u5065\u9065\u611f\u79d1\u5b66\u4e3a\u6307\u5bfc\u7684\u793e\u533a\u6307\u660e\u4e86\u524d\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.03317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["G\u00fcnel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u903c\u771f\u7684\u6270\u52a8\u89e3\u91ca\u6280\u672f\uff0c\u7528\u4e8e\u63d0\u9ad8\u751f\u6001\u76d1\u6d4bAI\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u4e13\u5bb6\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u751f\u6001\u76d1\u6d4b\u4e2d\u89c6\u89c9\u6a21\u578b\u9884\u6d4b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u4fe1\u4efb\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u590d\u5f15\u5bfc\u7684\u6270\u52a8\u89e3\u91ca\u6280\u672f\uff0c\u5229\u7528Segment-Anything-Model\u4f18\u5316\u7684\u63a9\u7801\u8fdb\u884c\u5bf9\u8c61\u79fb\u9664/\u66ff\u6362\u548c\u80cc\u666f\u66ff\u6362\u3002", "result": "\u751f\u6210\u7684\u89e3\u91ca\u80fd\u591f\u5b9a\u4f4d\u8bca\u65ad\u7ed3\u6784\uff0c\u907f\u514d\u4f20\u7edf\u6270\u52a8\u4e2d\u7684\u5220\u9664\u4f2a\u5f71\uff0c\u5e76\u63d0\u4f9b\u4e0e\u9886\u57df\u76f8\u5173\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u903c\u771f\u7684\u3001\u5c40\u90e8\u7f16\u8f91\u7684\u89e3\u91ca\uff0c\u63d0\u9ad8\u4e86\u751f\u6001\u76d1\u6d4b\u4e2dAI\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u4e13\u5bb6\u9a8c\u8bc1\u548c\u66f4\u53ef\u9760\u7684AI\u90e8\u7f72\u3002"}}
{"id": "2510.03352", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u4fa7\u4fe1\u606f\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u91c7\u6837\u7684\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9006\u95ee\u9898\u91cd\u5efa\u8d28\u91cf\uff0c\u907f\u514d\u4e86\u68af\u5ea6\u5f15\u5bfc\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4fa7\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5728\u4e25\u91cd\u4e0d\u9002\u5b9a\u60c5\u51b5\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4fa7\u4fe1\u606f\u5f15\u5bfc\u7684\u91c7\u6837\u8fc7\u7a0b\u6765\u6539\u8fdb\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u91cd\u5efa\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u95f4\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5229\u7528\u4fa7\u4fe1\u606f\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4ece\u800c\u4f18\u5316\u91cd\u5efa\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u9006\u95ee\u9898\uff08\u5982\u76d2\u5185\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u53ca\u591a\u79cd\u53bb\u6a21\u7cca\u4efb\u52a1\uff09\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e14\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u5f15\u5bfc\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u95f4\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4fa7\u4fe1\u606f\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u591a\u79cd\u9006\u95ee\u9898\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u907f\u514d\u4e86\u57fa\u4e8e\u68af\u5ea6\u5f15\u5bfc\u7684\u5e38\u89c1\u4f2a\u5f71\u95ee\u9898\uff0c\u8fd8\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u6269\u6563\u91cd\u5efa\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2510.03361", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models.", "AI": {"tldr": "Provenance networks embed explainability into neural models by linking predictions to training data, offering transparency and robustness despite higher computational costs.", "motivation": "The motivation is to address the limitations of conventional post-hoc explainability methods by embedding interpretability into the neural model architecture itself, improving transparency and trustworthiness.", "method": "Provenance networks learn to link predictions directly to supporting training examples, functioning similarly to a learned KNN. This method jointly optimizes the primary task and explainability objectives.", "result": "The approach facilitates systematic investigations of memorization vs. generalization, aids in detecting mislabeled data, enhances resilience to perturbations, and supports identifying similar inputs contributing to predictions.", "conclusion": "Provenance networks provide a novel approach to integrating explainability directly into neural models, addressing key challenges in deep learning such as model opaqueness and hallucination. While they introduce additional computational costs, they enhance transparency, robustness, and trustworthiness."}}
{"id": "2510.03363", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "UCF\u662f\u4e00\u79cd\u901a\u7528\u540e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5339\u914d\u548c\u53ef\u5b66\u4e60\u8fc7\u6ee4\u6a21\u5757\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5339\u914d\u566a\u58f0\u5904\u7406\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u5171\u4eab\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u540e\u5904\u7406\u7ec6\u5316\u6846\u67b6UCF\uff0c\u901a\u8fc7\u6784\u5efa\u5f02\u5e38\u6210\u672c\u4f53\u79ef\u5e76\u5229\u7528\u591a\u5c42\u7ea7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53ef\u5b66\u4e60\u8fc7\u6ee4\u6a21\u5757\u6765\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUCF\u5728\u5355\u6a21\u6001\uff08RGB\uff09\u548c\u591a\u6a21\u6001\uff08RGB--3D\u3001RGB--Text\uff09\u573a\u666f\u4e0b\u5747\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86Unified Cost Filtering (UCF)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5339\u914d\u548c\u53ef\u5b66\u4e60\u7684\u8fc7\u6ee4\u6a21\u5757\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5339\u914d\u566a\u58f0\u5e76\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u572822\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCF\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03441", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.", "AI": {"tldr": "SpatialViLT\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u7279\u5f81\u589e\u5f3a\u591a\u6a21\u6001\u5d4c\u5165\uff0c\u5176\u53d8\u4f53\u5728VSR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86AI\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u573a\u666f\u548c\u590d\u6742\u7269\u4f53\u914d\u7f6e\u7684\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u589e\u5f3a\u5176\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faSpatialViLT\u548cMaskedSpatialViLT\u4e24\u79cd\u53d8\u4f53\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6574\u5408\u6df1\u5ea6\u56fe\u30013D\u5750\u6807\u548c\u8fb9\u7f18\u56fe\u7b49\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u4e24\u8005\u7684SpatialEnsemble\u65b9\u6cd5\u3002", "result": "\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u65b9\u5411\u6027\u3001\u62d3\u6251\u548c\u90bb\u8fd1\u5173\u7cfb\u7b49\u7a7a\u95f4\u63a8\u7406\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165SpatialViLT\u53ca\u5176\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u7cfb\u7edf\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u9ad8\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u548c\u73b0\u5b9e\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03483", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52", "AI": {"tldr": "DuPLUS\u662f\u4e00\u79cd\u65b0\u578b\u533b\u5b66\u5f71\u50cf\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5206\u5c42\u63d0\u793a\u548c\u53cc\u63d0\u793a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u901a\u7528\u6027\u548c\u9ad8\u6548\u9884\u540e\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9884\u540e\u80fd\u529b\uff0c\u800c\u73b0\u6709\u201c\u901a\u7528\u201d\u65b9\u6cd5\u5b58\u5728\u6761\u4ef6\u7b80\u5355\u5316\u548c\u533b\u5b66\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "DuPLUS\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u72ec\u7279\u7684\u53cc\u63d0\u793a\u673a\u5236\u9a71\u52a8\u7684\u5206\u5c42\u6587\u672c\u63a7\u5236\u67b6\u6784\u3002", "result": "DuPLUS\u5728\u4e09\u79cd\u6210\u50cf\u6a21\u6001\u548c\u5341\u4e2a\u4e0d\u540c\u89e3\u5256\u5b66\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e8/10\u6570\u636e\u96c6\u7684\u6700\u5148\u8fdb\u4efb\u52a1\u7279\u5b9a\u548c\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5728\u5934\u9888\u764c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.69\u7684CI\u3002", "conclusion": "DuPLUS\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u6a21\u6001\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b9e\u73b0\u5bf9\u4e0d\u540c\u4e2d\u5fc3\u6570\u636e\u7684\u5feb\u901f\u9002\u5e94\u3002"}}
{"id": "2510.03511", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. S\u00e1nchez", "Sharvaree Vadgama", "Georg B\u00f6kman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.", "AI": {"tldr": "Platonic Transformer\u901a\u8fc7\u67cf\u62c9\u56fe\u7acb\u4f53\u5bf9\u79f0\u7fa4\u7684\u53c2\u8003\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6Transformer\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3Transformer\u7f3a\u4e4f\u51e0\u4f55\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5176\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u76f8\u5bf9\u4e8e\u67cf\u62c9\u56fe\u7acb\u4f53\u5bf9\u79f0\u7fa4\u53c2\u8003\u6846\u67b6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u539f\u7406\u6027\u7684\u6743\u91cd\u5171\u4eab\u65b9\u6848\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CIFAR-10\uff09\u30013D\u70b9\u4e91\uff08ScanObjectNN\uff09\u548c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\uff08QM9\u3001OMol25\uff09\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlatonic Transformer\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u7ea6\u675f\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "Platonic Transformer\u901a\u8fc7\u5f15\u5165\u67cf\u62c9\u56fe\u7acb\u4f53\u5bf9\u79f0\u7fa4\u7684\u53c2\u8003\u6846\u67b6\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u5e73\u79fb\u548c\u67cf\u62c9\u56fe\u5bf9\u79f0\u6027\u7684\u8054\u5408\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6Transformer\u7684\u67b6\u6784\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.03548", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u68c0\u6d4bAI\u89c6\u9891\u4f1a\u8bae\u4e2d\u8eab\u4efd\u52ab\u6301\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u6f5c\u5728\u8868\u793a\u4e2d\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u6709\u6548\u9632\u5fa1\u653b\u51fb\u3002", "motivation": "AI\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u8868\u793a\u6765\u964d\u4f4e\u5e26\u5bbd\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5bb9\u6613\u88ab\u653b\u51fb\u8005\u64cd\u63a7\uff0c\u5bfc\u81f4\u5b9e\u65f6\u8eab\u4efd\u52ab\u6301\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u548c\u5408\u6210\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6bcf\u4e00\u5e27\u90fd\u662f\u5408\u6210\u7684\u800c\u5931\u6548\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u4e2a\u57fa\u4e8e\u59ff\u6001\u6761\u4ef6\u7684\u5927\u95f4\u9694\u5bf9\u6bd4\u7f16\u7801\u5668\uff0c\u4ece\u4f20\u8f93\u7684\u6f5c\u5728\u8868\u793a\u4e2d\u5206\u79bb\u51fa\u6301\u4e45\u7684\u8eab\u4efd\u7ebf\u7d22\uff0c\u540c\u65f6\u6d88\u9664\u77ac\u65f6\u7684\u59ff\u6001\u548c\u8868\u60c5\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5bf9\u8bdd\u5934\u751f\u6210\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u7279\u5f81\u6cc4\u6f0f\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5d4c\u5165\u7684\u4f59\u5f26\u6d4b\u8bd5\u5b9e\u65f6\u68c0\u6d4b\u975e\u6cd5\u8eab\u4efd\u4ea4\u6362\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2510.03555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.", "AI": {"tldr": "GAS-MIL\u662f\u4e00\u79cd\u96c6\u6210\u6846\u67b6\uff0c\u9ad8\u6548\u6574\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7279\u5f81\uff0c\u5728\u591a\u79cd\u764c\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7b80\u5316\u4e86\u75c5\u7406\u5b66\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u9002\u5e94\u548c\u57fa\u51c6\u6d4b\u8bd5\u5355\u4e2a\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u901a\u5e38\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u5c24\u5176\u662f\u8003\u8651\u5230\u5176\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165\u4e86Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)\uff0c\u4e00\u79cd\u7075\u6d3b\u7684\u96c6\u6210\u6846\u67b6\uff0c\u65e0\u9700\u624b\u52a8\u7279\u5f81\u9009\u62e9\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u6574\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u764c\u75c7\u6570\u636e\u96c6\uff08\u524d\u5217\u817a\u3001\u5375\u5de2\u548c\u4e73\u817a\u764c\uff09\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGAS-MIL\u76f8\u5bf9\u4e8e\u5355\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u5df2\u5efa\u7acb\u7684MIL\u65b9\u6cd5\uff0c\u59cb\u7ec8\u8868\u73b0\u51fa\u4f18\u8d8a\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "GAS-MIL\u901a\u8fc7\u9ad8\u6548\u6574\u5408\u5f02\u6784\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u75c5\u7406\u5b66\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u591a\u6a21\u6001\u548c\u7cbe\u51c6\u80bf\u7624\u5b66\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cdOCR\u7cfb\u7edf\u5728\u98df\u54c1\u5305\u88c5\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0cTesseract\u51c6\u786e\u6027\u6700\u9ad8\uff0cEasyOCR\u5e73\u8861\u6027\u4f73\uff0cPaddleOCR\u8986\u76d6\u7387\u9ad8\u4f46\u6162\uff0cTrOCR\u8868\u73b0\u6700\u5dee\uff0c\u4e3a\u540e\u7eed\u6539\u8fdb\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u5305\u88c5\u4e0a\u7684OCR\u51c6\u786e\u6027\u5bf9\u5408\u89c4\u6027\u548c\u8425\u517b\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u8bed\u8a00\u6587\u672c\u3001\u5bc6\u96c6\u5e03\u5c40\u3001\u5b57\u4f53\u591a\u6837\u3001\u53cd\u5149\u548c\u66f2\u9762\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u8bc4\u4f30\u73b0\u6709OCR\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u5f00\u6e90OCR\u7cfb\u7edf\uff08Tesseract\u3001EasyOCR\u3001PaddleOCR\u3001TrOCR\uff09\u5728\u771f\u5b9e\u98df\u54c1\u5305\u88c5\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u901f\u5ea6\u3001\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\uff08CER\u3001WER\u3001BLEU\u7b49\u6307\u6807\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "Tesseract\u5728CER\u548cBLEU\u4e0a\u8868\u73b0\u6700\u4f73\uff1bEasyOCR\u5728\u51c6\u786e\u6027\u548c\u591a\u8bed\u8a00\u652f\u6301\u95f4\u53d6\u5f97\u5e73\u8861\uff1bPaddleOCR\u8986\u76d6\u7387\u9ad8\u4f46\u901f\u5ea6\u6162\uff1bTrOCR\u8868\u73b0\u6700\u5f31\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u98df\u54c1\u5305\u88c5OCR\u63d0\u4f9b\u4e86\u7279\u5b9a\u57fa\u51c6\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u6307\u51fa\u4e86\u5e03\u5c40\u611f\u77e5\u65b9\u6cd5\u548c\u6587\u672c\u5b9a\u4f4d\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.03591", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408Co-FineTuning\u65b9\u6cd5\uff0c\u7ed3\u5408\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u6570\u636e\uff0c\u6709\u6548\u51cf\u5c11\u5bf9\u76ee\u6807\u6e38\u620f\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u8bc6\u522b\u6e38\u620f\u89c6\u89c9bug\u6210\u672c\u9ad8\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u73b0\u6709\u76d1\u7763\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6b64\u7c7bbug\u7f55\u89c1\uff0c\u6807\u6ce8\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u76ee\u6807\u6e38\u620f\u548c\u5171\u57df\u6e38\u620f\u7684\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408Co-FineTuning\uff08CFT\uff09\u65b9\u6cd5\u3002", "result": "CFT\u65b9\u6cd5\u5728\u591a\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5373\u4f7f\u4ec5\u4f7f\u7528\u76ee\u6807\u6e38\u620f50%\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408Co-FineTuning\uff08CFT\uff09\u65b9\u6cd5\u5728\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u76ee\u6807\u6e38\u620f\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u591a\u6e38\u620f\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u3002"}}
{"id": "2510.03666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.", "AI": {"tldr": "MonitorVLM\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u68c0\u6d4b\u76d1\u63a7\u89c6\u9891\u6d41\u4e2d\u7684\u5b89\u5168\u8fdd\u89c4\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77ff\u4e1a\u5b89\u5168\u76d1\u63a7\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u68c0\u67e5\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u77ff\u4e1a\uff09\u4e2d\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\uff0c\u56e0\u6b64\u6025\u9700\u667a\u80fd\u5316\u548c\u81ea\u52a8\u5316\u7684\u5b89\u5168\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86MonitorVLM\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u9886\u57df\u7279\u5b9a\u7684\u8fdd\u89c4\u6570\u636e\u96c6\u3001\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u6761\u6b3e\u7684\u6761\u6b3e\u8fc7\u6ee4\u5668\u6a21\u5757\uff0c\u4ee5\u53ca\u589e\u5f3a\u5de5\u4eba\u533a\u57df\u7684\u884c\u4e3a\u653e\u5927\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMonitorVLM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u63d0\u5347\u4e8622.01%\u300134.22%\u548c28.37%\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u63d0\u5347\u77ff\u4e1a\u53ca\u5176\u4ed6\u9886\u57df\u804c\u4e1a\u5b89\u5168\u76d1\u63a7\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03701", "abs": "https://arxiv.org/abs/2510.03701", "authors": ["Kanoko Goto", "Takumi Hirose", "Mahiro Ukai", "Shuhei Kurita", "Nakamasa Inoue"], "title": "Referring Expression Comprehension for Small Objects", "comment": null, "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSOREC\u6570\u636e\u96c6\u548cPIZA\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u5bf9\u8c61\u5b9a\u4f4d\u96be\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u5b66\u4e60\u5728REC\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b9a\u4f4d\u6781\u5c0f\u7684\u76ee\u6807\u5bf9\u8c61\u4ecd\u662f\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u6e10\u8fdb\u5f0f\u8fed\u4ee3\u7f29\u653e\u9002\u914d\u5668\uff08PIZA\uff09\uff0c\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9010\u6b65\u653e\u5927\u5e76\u5b9a\u4f4d\u5c0f\u76ee\u6807\u5bf9\u8c61\u3002", "result": "\u5728SOREC\u6570\u636e\u96c6\u4e0a\u5e94\u7528PIZA\u540e\uff0cGroundingDINO\u6a21\u578b\u7684\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165SOREC\u6570\u636e\u96c6\u548cPIZA\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u5bf9\u8c61\u7684\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u4efb\u52a1\u6027\u80fd\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.03717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03717", "abs": "https://arxiv.org/abs/2510.03717", "authors": ["Sharan SK", "Subin Sahayam", "Umarani Jayaraman", "Lakshmi Priya A"], "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning", "comment": "12 pages, 6 figures, preprint under review", "summary": "Segmenting of clinically important retinal blood vessels into arteries and\nveins is a prerequisite for retinal vessel analysis. Such analysis can provide\npotential insights and bio-markers for identifying and diagnosing various\nretinal eye diseases. Alteration in the regularity and width of the retinal\nblood vessels can act as an indicator of the health of the vasculature system\nall over the body. It can help identify patients at high risk of developing\nvasculature diseases like stroke and myocardial infarction. Over the years,\nvarious Deep Learning architectures have been proposed to perform retinal\nvessel segmentation. Recently, attention mechanisms have been increasingly used\nin image segmentation tasks. The work proposes a new Deep Learning approach for\nartery-vein segmentation. The new approach is based on the Attention mechanism\nthat is incorporated into the WNet Deep Learning model, and we call the model\nas Attention-WNet. The proposed approach has been tested on publicly available\ndatasets such as HRF and DRIVE datasets. The proposed approach has outperformed\nother state-of-art models available in the literature.", "AI": {"tldr": "\u63d0\u51faAttention-WNet\uff0c\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u5bf9\u773c\u90e8\u75be\u75c5\u8bca\u65ad\u548c\u5168\u8eab\u8840\u7ba1\u5065\u5eb7\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u5e76\u5c06\u5176\u6574\u5408\u5230WNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u5f62\u6210Attention-WNet\u3002", "result": "\u5728HRF\u548cDRIVE\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cAttention-WNet\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684Attention-WNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2510.03763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03763", "abs": "https://arxiv.org/abs/2510.03763", "authors": ["Jiaxin Deng", "Junbiao Pang"], "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization", "comment": null, "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles\nthe computational cost of Stochastic Gradient Descent (SGD) by requiring twice\nthe gradient calculations per optimization step. To mitigate this, we propose\nAdaptively sampling-Reusing-mixing decomposed gradients to significantly\naccelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can\nbe decomposed into the SGD gradient and the Projection of the Second-order\ngradient onto the First-order gradient (PSF). Furthermore, we observe that the\nSGD gradient and PSF dynamically evolve during training, emphasizing the\ngrowing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed\nto the reused PSF and the timely updated PSF still maintain the model's\ngeneralization ability. Extensive experiments show that ARSAM achieves\nstate-of-the-art accuracies comparable to SAM across diverse network\narchitectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a\nspeedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various\nchallenge tasks (\\textit{e.g.}, human pose estimation, and model quantization)\nwithout sacrificing performance, demonstrating its broad practicality.% The\ncode is publicly accessible at: https://github.com/ajiaaa/ARSAM.", "AI": {"tldr": "ARSAM\u901a\u8fc7\u52a8\u6001\u91cd\u7528\u548c\u66f4\u65b0\u5206\u89e3\u68af\u5ea6\uff0c\u663e\u8457\u52a0\u901fSAM\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "SAM\u867d\u7136\u63d0\u9ad8\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u66ff\u4ee3\u3002", "method": "\u63d0\u51faARSAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3SAM\u7684\u68af\u5ea6\u4e3aSGD\u68af\u5ea6\u548cPSF\uff0c\u5e76\u52a8\u6001\u91cd\u7528\u548c\u66f4\u65b0PSF\u6765\u52a0\u901f\u4f18\u5316\u3002", "result": "ARSAM\u5728\u4fdd\u6301\u4e0eSAM\u76f8\u5f53\u7684\u51c6\u786e\u5ea6\u4e0b\uff0c\u901f\u5ea6\u63d0\u5347\u4e86\u7ea640%\uff0c\u5e76\u5728\u591a\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ARSAM\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u3001\u91cd\u7528\u548c\u6df7\u5408\u5206\u89e3\u68af\u5ea6\uff0c\u663e\u8457\u52a0\u901f\u4e86SAM\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03873", "abs": "https://arxiv.org/abs/2510.03873", "authors": ["Saja Al-Dabet", "Sherzod Turaev", "Nazar Zaki", "Arif O. Khan", "Luai Eldweik"], "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis", "comment": "This is a preprint version of a manuscript under review. All rights\n  reserved by the authors", "summary": "Diagnosing ocular-induced abnormal head posture (AHP) requires a\ncomprehensive analysis of both head pose and ocular movements. However,\nexisting datasets focus on these aspects separately, limiting the development\nof integrated diagnostic approaches and restricting AI-driven advancements in\nAHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D\ndataset that synchronously captures head pose and gaze movement information for\nocular-induced AHP assessment. Structured clinical data were extracted from\nmedical literature using large language models (LLMs) through an iterative\nprocess with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and\ncomplex prompting strategies. The extracted records were systematically imputed\nand transformed into 3D representations using the Neural Head Avatar (NHA)\nframework. The dataset includes 7,920 images generated from two head textures,\ncovering a broad spectrum of ocular conditions. The extraction method achieved\nan overall accuracy of 91.92%, demonstrating its reliability for clinical\ndataset construction. PoseGaze-AHP is the first publicly available resource\ntailored for AI-driven ocular-induced AHP diagnosis, supporting the development\nof accurate and privacy-compliant diagnostic tools.", "AI": {"tldr": "PoseGaze-AHP\u662f\u4e00\u4e2a\u540c\u6b65\u6355\u83b7\u5934\u90e8\u59ff\u52bf\u548c\u6ce8\u89c6\u8fd0\u52a8\u76843D\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u7684\u7a7a\u767d\uff0c\u652f\u6301AI\u9a71\u52a8\u7684\u773c\u6e90\u6027\u5f02\u5e38\u5934\u4f4d\u8bca\u65ad\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5206\u522b\u5173\u6ce8\u5934\u90e8\u59ff\u52bf\u548c\u773c\u7403\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u96c6\u6210\u8bca\u65ad\u65b9\u6cd5\u7684\u53d1\u5c55\u548cAI\u5728\u5f02\u5e38\u5934\u4f4d\u5206\u6790\u4e2d\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u4f7f\u7528Claude 3.5 Sonnet\u6a21\u578b\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\uff0c\u7ed3\u5408\u9010\u6b65\u3001\u5206\u5c42\u548c\u590d\u6742\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u4f7f\u7528Neural Head Avatar (NHA)\u6846\u67b6\u5c06\u8bb0\u5f55\u8f6c\u6362\u4e3a3D\u8868\u793a\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b7,920\u5f20\u56fe\u50cf\uff0c\u8986\u76d6\u5e7f\u6cdb\u7684\u773c\u90e8\u6761\u4ef6\uff0c\u63d0\u53d6\u65b9\u6cd5\u603b\u4f53\u51c6\u786e\u7387\u4e3a91.92%\u3002", "conclusion": "PoseGaze-AHP\u662f\u9996\u4e2a\u516c\u5f00\u7684\u4e13\u4e3aAI\u9a71\u52a8\u7684\u773c\u6e90\u6027\u5f02\u5e38\u5934\u4f4d\u8bca\u65ad\u8bbe\u8ba1\u7684\u8d44\u6e90\uff0c\u652f\u6301\u5f00\u53d1\u51c6\u786e\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.03878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03878", "abs": "https://arxiv.org/abs/2510.03878", "authors": ["Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R"], "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks", "comment": null, "summary": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes\nsignificantly to its high global mortality rate, with over 50\\% of cases\ndetected at advanced stages and a 5-year survival rate below 50\\% according to\nWHO statistics. This study aims to improve early detection of OSCC by\ndeveloping a multimodal deep learning framework that integrates clinical,\nradiological, and histopathological images using a weighted ensemble of\nDenseNet-121 convolutional neural networks (CNNs). Material and Methods A\nretrospective study was conducted using publicly available datasets\nrepresenting three distinct medical imaging modalities. Each modality-specific\ndataset was used to train a DenseNet-121 CNN via transfer learning.\nAugmentation and modality-specific preprocessing were applied to increase\nrobustness. Predictions were fused using a validation-weighted ensemble\nstrategy. Evaluation was performed using accuracy, precision, recall, F1-score.\nResults High validation accuracy was achieved for radiological (100\\%) and\nhistopathological (95.12\\%) modalities, with clinical images performing lower\n(63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved\ndiagnostic robustness with an overall accuracy of 84.58\\% on a multimodal\nvalidation dataset of 55 samples. Conclusion The multimodal ensemble framework\nbridges gaps in the current diagnostic workflow by offering a non-invasive,\nAI-assisted triage tool that enhances early identification of high-risk\nlesions. It supports clinicians in decision-making, aligning with global\noncology guidelines to reduce diagnostic delays and improve patient outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e34\u5e8a\u3001\u653e\u5c04\u5b66\u548c\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u65e9\u671f\u68c0\u6d4b\u7387\u3002\u96c6\u6210\u6a21\u578b\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa84.58%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\uff08OSCC\uff09\u7684\u665a\u671f\u8bca\u65ad\u662f\u5176\u9ad8\u5168\u7403\u6b7b\u4ea1\u7387\u7684\u91cd\u8981\u539f\u56e0\uff0c\u8d85\u8fc750%\u7684\u75c5\u4f8b\u5728\u665a\u671f\u88ab\u53d1\u73b0\uff0c5\u5e74\u751f\u5b58\u7387\u4f4e\u4e8e50%\uff08WHO\u7edf\u8ba1\u6570\u636e\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e34\u5e8a\u3001\u653e\u5c04\u5b66\u548c\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u63d0\u9ad8OSCC\u7684\u65e9\u671f\u68c0\u6d4b\u7387\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u56de\u987e\u6027\u7814\u7a76\u8bbe\u8ba1\uff0c\u5229\u7528\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u4ee3\u8868\u4e09\u79cd\u4e0d\u540c\u7684\u533b\u5b66\u6210\u50cf\u6a21\u6001\u3002\u6bcf\u79cd\u6a21\u6001\u7279\u5b9a\u7684\u6570\u636e\u96c6\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3DenseNet-121 CNN\u3002\u5e94\u7528\u589e\u5f3a\u548c\u6a21\u6001\u7279\u5b9a\u7684\u9884\u5904\u7406\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u9884\u6d4b\u901a\u8fc7\u9a8c\u8bc1\u52a0\u6743\u96c6\u6210\u7b56\u7565\u8fdb\u884c\u878d\u5408\u3002\u8bc4\u4f30\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3002", "result": "\u653e\u5c04\u5b66\u6a21\u6001\uff08100%\uff09\u548c\u75c5\u7406\u5b66\u6a21\u6001\uff0895.12%\uff09\u5b9e\u73b0\u4e86\u9ad8\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4e34\u5e8a\u56fe\u50cf\u7531\u4e8e\u89c6\u89c9\u5f02\u8d28\u6027\u8868\u73b0\u8f83\u4f4e\uff0863.10%\uff09\u3002\u96c6\u6210\u6a21\u578b\u5728\u591a\u6a21\u6001\u9a8c\u8bc1\u6570\u636e\u96c6\uff0855\u4e2a\u6837\u672c\uff09\u4e0a\u8868\u73b0\u51fa84.58%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u8bca\u65ad\u9c81\u68d2\u6027\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u96c6\u6210\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001AI\u8f85\u52a9\u7684\u5206\u8bca\u5de5\u5177\uff0c\u586b\u8865\u4e86\u5f53\u524d\u8bca\u65ad\u6d41\u7a0b\u4e2d\u7684\u7a7a\u767d\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u8bc6\u522b\u9ad8\u98ce\u9669\u75c5\u53d8\u3002\u5b83\u652f\u6301\u4e34\u5e8a\u533b\u751f\u51b3\u7b56\uff0c\u7b26\u5408\u5168\u7403\u80bf\u7624\u5b66\u6307\u5357\uff0c\u65e8\u5728\u51cf\u5c11\u8bca\u65ad\u5ef6\u8fdf\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2510.03921", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.10; I.5.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03921", "abs": "https://arxiv.org/abs/2510.03921", "authors": ["Arushi Dashore", "Aryan Anumala", "Emily Hui", "Olivia Yang"], "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition", "comment": "10 pages, 4 figures, 2 tables", "summary": "Automated tennis stroke analysis has advanced significantly with the\nintegration of biomechanical motion cues alongside deep learning techniques,\nenhancing stroke classification accuracy and player performance evaluation.\nDespite these advancements, existing systems often fail to connect\nbiomechanical insights with actionable language feedback that is both\naccessible and meaningful to players and coaches. This research project\naddresses this gap by developing a novel framework that extracts key\nbiomechanical features (such as joint angles, limb velocities, and kinetic\nchain patterns) from motion data using Convolutional Neural Network Long\nShort-Term Memory (CNN-LSTM)-based models. These features are analyzed for\nrelationships influencing stroke effectiveness and injury risk, forming the\nbasis for feedback generation using large language models (LLMs). Leveraging\nthe THETIS dataset and feature extraction techniques, our approach aims to\nproduce feedback that is technically accurate, biomechanically grounded, and\nactionable for end-users. The experimental setup evaluates this framework on\nclassification performance and interpretability, bridging the gap between\nexplainable AI and sports biomechanics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN-LSTM\u548cLLM\u7684\u6846\u67b6\uff0c\u5c06\u751f\u7269\u529b\u5b66\u6570\u636e\u8f6c\u5316\u4e3a\u5bf9\u7f51\u7403\u7403\u5458\u548c\u6559\u7ec3\u6709\u7528\u7684\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u53cd\u9988\u4e0d\u53ef\u64cd\u4f5c\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u52a8\u5316\u7f51\u7403\u51fb\u7403\u5206\u6790\u5728\u751f\u7269\u529b\u5b66\u8fd0\u52a8\u7ebf\u7d22\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u7ed3\u5408\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5f80\u5f80\u65e0\u6cd5\u5c06\u751f\u7269\u529b\u5b66\u89c1\u89e3\u8f6c\u5316\u4e3a\u5bf9\u7403\u5458\u548c\u6559\u7ec3\u6765\u8bf4\u65e2\u6613\u61c2\u53c8\u6709\u610f\u4e49\u7684\u53ef\u64cd\u4f5c\u8bed\u8a00\u53cd\u9988\u3002", "method": "\u4f7f\u7528CNN-LSTM\u6a21\u578b\u4ece\u8fd0\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u5173\u952e\u751f\u7269\u529b\u5b66\u7279\u5f81\uff08\u5982\u5173\u8282\u89d2\u5ea6\u3001\u80a2\u4f53\u901f\u5ea6\u548c\u52a8\u529b\u94fe\u6a21\u5f0f\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u6210\u529f\u5f25\u5408\u4e86\u53ef\u89e3\u91caAI\u4e0e\u8fd0\u52a8\u751f\u7269\u529b\u5b66\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408CNN-LSTM\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u751f\u7269\u529b\u5b66\u7279\u5f81\u8f6c\u5316\u4e3a\u5bf9\u7403\u5458\u548c\u6559\u7ec3\u6709\u5b9e\u9645\u610f\u4e49\u7684\u53cd\u9988\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u53ef\u64cd\u4f5c\u53cd\u9988\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.04034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04034", "abs": "https://arxiv.org/abs/2510.04034", "authors": ["Linn Bieske", "Carla Lorente"], "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks", "comment": null, "summary": "Recent advances in image editing have shifted from manual pixel manipulation\nto employing deep learning methods like stable diffusion models, which now\nleverage cross-attention mechanisms for text-driven control. This transition\nhas simplified the editing process but also introduced variability in results,\nsuch as inconsistent hair color changes. Our research aims to enhance the\nprecision and reliability of prompt-to-prompt image editing frameworks by\nexploring and optimizing hyperparameters. We present a comprehensive study of\nthe \"word swap\" method, develop an \"attention re-weight method\" for better\nadaptability, and propose the \"CL P2P\" framework to address existing\nlimitations like cycle inconsistency. This work contributes to understanding\nand improving the interaction between hyperparameter settings and the\narchitectural choices of neural network models, specifically their attention\nmechanisms, which significantly influence the composition and quality of the\ngenerated images.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u548c\u63d0\u51fa\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\uff08\u5982\u7a33\u5b9a\u6269\u6563\u6a21\u578b\uff09\u7b80\u5316\u4e86\u56fe\u50cf\u7f16\u8f91\u8fc7\u7a0b\uff0c\u4f46\u4ecd\u5b58\u5728\u7ed3\u679c\u4e0d\u4e00\u81f4\uff08\u5982\u53d1\u8272\u53d8\u5316\u4e0d\u4e00\u81f4\uff09\u7684\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u63d0\u5347\u63d0\u793a\u5230\u63d0\u793a\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u2018word swap\u2019\u65b9\u6cd5\u3001\u5f00\u53d1\u4e86\u2018attention re-weight method\u2019\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u2018CL P2P\u2019\u6846\u67b6\u6765\u89e3\u51b3\u5faa\u73af\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u2018attention re-weight method\u2019\u548c\u2018CL P2P\u2019\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u548c\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\uff08\u5982\u2018attention re-weight method\u2019\u548c\u2018CL P2P\u2019\u6846\u67b6\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5230\u63d0\u793a\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2510.04039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04039", "abs": "https://arxiv.org/abs/2510.04039", "authors": ["Bin Lei", "Nuo Xu", "Ali Payani", "Mingyi Hong", "Chunhua Liao", "Yu Cao", "Caiwen Ding"], "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding", "comment": null, "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).", "AI": {"tldr": "GUI-Spotlight\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u5de5\u5177\u63d0\u9ad8\u89c6\u89c9\u57fa\u7840\u51c6\u786e\u6027\uff0c\u4ec5\u752818.5K\u8bad\u7ec3\u6837\u672c\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u89c6\u89c9\u57fa\u7840\u53ef\u9760\u6027\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u6267\u884c\u6307\u9488\u7ea7\u64cd\u4f5c\u3002", "method": "\u5f15\u5165GUI-Spotlight\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u57fa\u7840\u63a8\u7406\uff0c\u52a8\u6001\u8c03\u7528\u591a\u4e2a\u4e13\u7528\u5de5\u5177\u8fed\u4ee3\u7f29\u5c0f\u5c4f\u5e55\u76f8\u5173\u533a\u57df\u3002", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGUI-Spotlight\u4ee552.8%\u7684\u51c6\u786e\u7387\u8d85\u8d8aV2P-7B\uff0850.6%\uff09\u548cGTA-1-7B\uff0850.1%\uff09\uff0c\u4e14\u8bad\u7ec3\u6837\u672c\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "GUI-Spotlight\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u591a\u79cd\u4e13\u7528\u5de5\u5177\u8fed\u4ee3\u7f29\u5c0f\u5c4f\u5e55\u76f8\u5173\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u57fa\u7840\u7684\u51c6\u786e\u6027\u3002\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u752818.5K\u8bad\u7ec3\u6837\u672c\u5c31\u8d85\u8d8a\u4e86\u5176\u4ed6\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.04044", "categories": ["cs.CV", "cs.AI", "00-01", "I.2.6; K.3.2"], "pdf": "https://arxiv.org/pdf/2510.04044", "abs": "https://arxiv.org/abs/2510.04044", "authors": ["Bingtao Yang", "Yujia Wang", "Mengzhi Jiao", "Hongwei Huo"], "title": "Quantization Range Estimation for Convolutional Neural Networks", "comment": "11 pages, 5 tables, research report", "summary": "Post-training quantization for reducing the storage of deep neural network\nmodels has been demonstrated to be an effective way in various tasks. However,\nlow-bit quantization while maintaining model accuracy is a challenging problem.\nIn this paper, we present a range estimation method to improve the quantization\nperformance for post-training quantization. We model the range estimation into\nan optimization problem of minimizing quantization errors by layer-wise local\nminima. We prove this problem is locally convex and present an efficient search\nalgorithm to find the optimal solution. We propose the application of the above\nsearch algorithm to the transformed weights space to do further improvement in\npractice. Our experiments demonstrate that our method outperforms\nstate-of-the-art performance generally on top-1 accuracy for image\nclassification tasks on the ResNet series models and Inception-v3 model. The\nexperimental results show that the proposed method has almost no loss of top-1\naccuracy in 8-bit and 6-bit settings for image classifications, and the\naccuracy of 4-bit quantization is also significantly improved. The code is\navailable at https://github.com/codeiscommitting/REQuant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u8303\u56f4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u548c\u9ad8\u6548\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u4fdd\u6301\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u4f4e\u6bd4\u7279\u91cf\u5316\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u65b9\u9762\u5177\u6709\u6311\u6218\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u7684\u8303\u56f4\u4f30\u8ba1\u65b9\u6cd5\u63d0\u5347\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u6027\u80fd\u3002", "method": "\u5c06\u8303\u56f4\u4f30\u8ba1\u5efa\u6a21\u4e3a\u901a\u8fc7\u9010\u5c42\u5c40\u90e8\u6700\u5c0f\u503c\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u53d8\u6362\u540e\u7684\u6743\u91cd\u7a7a\u95f4\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728ResNet\u7cfb\u5217\u6a21\u578b\u548cInception-v3\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684top-1\u51c6\u786e\u7387\u666e\u904d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u57284\u4f4d\u91cf\u5316\u4e2d\u8868\u73b0\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684\u8303\u56f4\u4f30\u8ba1\u65b9\u6cd5\u57288\u4f4d\u548c6\u4f4d\u91cf\u5316\u8bbe\u7f6e\u4e0b\u51e0\u4e4e\u65e0\u7cbe\u5ea6\u635f\u5931\uff0c\u4e14\u57284\u4f4d\u91cf\u5316\u4e2d\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.04057", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04057", "abs": "https://arxiv.org/abs/2510.04057", "authors": ["Zhenyu Pan", "Yucheng Lu", "Han Liu"], "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We present MetaFind, a scene-aware tri-modal compositional retrieval\nframework designed to enhance scene generation in the metaverse by retrieving\n3D assets from large-scale repositories. MetaFind addresses two core\nchallenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,\nand stylistic constraints, and (ii) the absence of a standardized retrieval\nparadigm specifically tailored for 3D asset retrieval, as existing approaches\nmainly rely on general-purpose 3D shape representation models. Our key\ninnovation is a flexible retrieval mechanism that supports arbitrary\ncombinations of text, image, and 3D modalities as queries, enhancing spatial\nreasoning and style consistency by jointly modeling object-level features\n(including appearance) and scene-level layout structures. Methodologically,\nMetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that\ncaptures spatial relationships and object appearance features, ensuring\nretrieved 3D assets are contextually and stylistically coherent with the\nexisting scene, regardless of coordinate frame transformations. The framework\nsupports iterative scene construction by continuously adapting retrieval\nresults to current scene updates. Empirical evaluations demonstrate the\nimproved spatial and stylistic consistency of MetaFind in various retrieval\ntasks compared to baseline methods.", "AI": {"tldr": "MetaFind\u662f\u4e00\u4e2a\u573a\u666f\u611f\u77e5\u7684\u4e09\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7ESSGNN\u7f16\u7801\u5668\u548c\u7075\u6d3b\u67e5\u8be2\u673a\u5236\u63d0\u53473D\u8d44\u4ea7\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u8d44\u4ea7\u68c0\u7d22\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\uff08i\uff09\u5ffd\u89c6\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u98ce\u683c\u7ea6\u675f\u7684\u4e0d\u4e00\u81f4\u68c0\u7d22\uff1b\uff08ii\uff09\u7f3a\u4e4f\u4e13\u4e3a3D\u8d44\u4ea7\u8bbe\u8ba1\u7684\u6807\u51c6\u5316\u68c0\u7d22\u8303\u5f0f\u3002", "method": "MetaFind\u5f15\u5165\u4e86ESSGNN\u5e03\u5c40\u7f16\u7801\u5668\uff0c\u8054\u5408\u5efa\u6a21\u5bf9\u8c61\u7ea7\u7279\u5f81\u548c\u573a\u666f\u7ea7\u5e03\u5c40\u7ed3\u6784\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c3D\u6a21\u6001\u7684\u4efb\u610f\u7ec4\u5408\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMetaFind\u5728\u591a\u79cd\u68c0\u7d22\u4efb\u52a1\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "conclusion": "MetaFind\u901a\u8fc7\u5176\u7075\u6d3b\u7684\u4e09\u6a21\u6001\u68c0\u7d22\u673a\u5236\u548c\u573a\u666f\u611f\u77e5\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8d44\u4ea7\u68c0\u7d22\u7684\u7a7a\u95f4\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u4e3a\u5143\u5b87\u5b99\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04100", "abs": "https://arxiv.org/abs/2510.04100", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Harold Soh"], "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing", "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally", "summary": "Topological mapping offers a compact and robust representation for\nnavigation, but progress in the field is hindered by the lack of standardized\nevaluation metrics, datasets, and protocols. Existing systems are assessed\nusing different environments and criteria, preventing fair and reproducible\ncomparisons. Moreover, a key challenge - perceptual aliasing - remains\nunder-quantified, despite its strong influence on system performance. We\naddress these gaps by (1) formalizing topological consistency as the\nfundamental property of topological maps and showing that localization accuracy\nprovides an efficient and interpretable surrogate metric, and (2) proposing the\nfirst quantitative measure of dataset ambiguity to enable fair comparisons\nacross environments. To support this protocol, we curate a diverse benchmark\ndataset with calibrated ambiguity levels, implement and release deep-learned\nbaseline systems, and evaluate them alongside classical methods. Our\nexperiments and analysis yield new insights into the limitations of current\napproaches under perceptual aliasing. All datasets, baselines, and evaluation\ntools are fully open-sourced to foster consistent and reproducible research in\ntopological mapping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6807\u51c6\u5316\u62d3\u6251\u6620\u5c04\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u7cca\u5ea6\u5b9a\u91cf\u6d4b\u91cf\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u6837\u5316\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u5f00\u6e90\uff0c\u4fc3\u8fdb\u9886\u57df\u7814\u7a76\u7684\u516c\u5e73\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u62d3\u6251\u6620\u5c04\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u534f\u8bae\uff0c\u5bfc\u81f4\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u8fdb\u884c\u516c\u5e73\u548c\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u611f\u77e5\u6df7\u6dc6\u8fd9\u4e00\u5173\u952e\u6311\u6218\u672a\u88ab\u5145\u5206\u91cf\u5316\uff0c\u5c3d\u7ba1\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\uff1a(1)\u5c06\u62d3\u6251\u4e00\u81f4\u6027\u5f62\u5f0f\u5316\u4e3a\u62d3\u6251\u6620\u5c04\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5e76\u5c55\u793a\u5b9a\u4f4d\u51c6\u786e\u6027\u4f5c\u4e3a\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u6307\u6807\uff1b(2)\u63d0\u51fa\u9996\u4e2a\u6570\u636e\u96c6\u6a21\u7cca\u5ea6\u7684\u5b9a\u91cf\u6d4b\u91cf\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8de8\u73af\u5883\u7684\u516c\u5e73\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u73b0\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u7ecf\u5178\u65b9\u6cd5\u7684\u57fa\u7ebf\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u548c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u611f\u77e5\u6df7\u6dc6\u4e0b\u7684\u5c40\u9650\u6027\u3002\u63d0\u51fa\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e3a\u62d3\u6251\u6620\u5c04\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\u5316\u5de5\u5177\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u62d3\u6251\u6620\u5c04\u9886\u57df\u7f3a\u4e4f\u516c\u5e73\u548c\u53ef\u91cd\u590d\u6bd4\u8f83\u7684\u95ee\u9898\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u7cfb\u7edf\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u62d3\u6251\u6620\u5c04\u7814\u7a76\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2510.04142", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04142", "abs": "https://arxiv.org/abs/2510.04142", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "comment": null, "summary": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAPO\u65b9\u6cd5\u89e3\u51b3MLLM\u84b8\u998f\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u8fde\u63a5\u4e0e\u591a\u6559\u5e08\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u6570\u636e\u96c6CXR-MAX\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u84b8\u998f\u4e2d\u56e0\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u7684\u6982\u5ff5\u6f02\u79fb\u5bfc\u81f4\u7684\u504f\u7f6e\u4f20\u9012\u95ee\u9898\uff0c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u201c\u5b66\u4e60\u3001\u6bd4\u8f83\u3001\u6279\u5224\u201d\u8303\u5f0f\uff0c\u7ed3\u5408\u6982\u5ff5\u6f02\u79fb\u7406\u8bba\uff0c\u5c06\u591a\u6559\u5e08\u6a21\u578b\u7684\u975e\u5e73\u7a33\u63a8\u7406\u52a8\u6001\u8f6c\u5316\u4e3a\u591a\u6d41\u63a8\u7406\u8f68\u8ff9\u7684\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\uff0c\u6700\u7ec8\u901a\u8fc7APO\u5b9e\u73b0\u6982\u5ff5\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u8d21\u732e\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6CXR-MAX\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u8fde\u63a5\u6982\u5ff5\u6f02\u79fb\u4e0e\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u81ea\u4e3b\u504f\u597d\u4f18\u5316\uff08APO\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6CXR-MAX\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.04201", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04201", "abs": "https://arxiv.org/abs/2510.04201", "authors": ["Moo Hyun Son", "Jintaek Oh", "Sun Bin Mun", "Jaechul Roh", "Sehyun Choi"], "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "comment": null, "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.", "AI": {"tldr": "World-To-Image\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u7f51\u7edc\u548c\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347T2I\u6a21\u578b\u5728\u751f\u6210\u65b0\u9896\u5b9e\u4f53\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684T2I\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u9896\u6216OOD\u5b9e\u4f53\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5f25\u8865\u5176\u77e5\u8bc6\u622a\u6b62\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u6001\u641c\u7d22\u7f51\u7edc\u4ee5\u68c0\u7d22\u672a\u77e5\u6982\u5ff5\u7684\u4ee3\u7406\uff0c\u5e76\u8fdb\u884c\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\uff0c\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u51c6\u786e\u5408\u6210\u56fe\u50cf\u3002", "result": "\u5728NICE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWorld-To-Image\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u7f8e\u5b66\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53478.1%\u3002", "conclusion": "World-To-Image\u6846\u67b6\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86T2I\u6a21\u578b\u5728\u751f\u6210\u65b0\u9896\u6216OOD\u5b9e\u4f53\u65f6\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u7f8e\u5b66\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2510.04220", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04220", "abs": "https://arxiv.org/abs/2510.04220", "authors": ["Lixuan He", "Shikang Zheng", "Linfeng Zhang"], "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering", "comment": null, "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.", "AI": {"tldr": "MASC\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u8bed\u4e49\u6811\u4f18\u5316\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u5904\u7406\u89c6\u89c9\u6807\u8bb0\u65f6\u5ffd\u7565\u5176\u5185\u5728\u7ed3\u6784\uff0c\u5bfc\u81f4\u9884\u6d4b\u4efb\u52a1\u590d\u6742\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMASC\u6846\u67b6\uff0c\u5229\u7528\u51e0\u4f55\u611f\u77e5\u8ddd\u79bb\u5ea6\u91cf\u548c\u5bc6\u5ea6\u9a71\u52a8\u7684\u805a\u5408\u6784\u9020\uff0c\u6784\u5efa\u5c42\u6b21\u5316\u8bed\u4e49\u6811\u3002", "result": "MASC\u52a0\u901f\u8bad\u7ec3\u8fbe57%\uff0c\u5e76\u5c06LlamaGen-XL\u7684FID\u4ece2.87\u964d\u81f32.58\u3002", "conclusion": "MASC\u901a\u8fc7\u7ed3\u6784\u5316\u9884\u6d4b\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4f7f\u5176\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u3002"}}
{"id": "2510.04225", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T45", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04225", "abs": "https://arxiv.org/abs/2510.04225", "authors": ["Yikun Ji", "Yan Hong", "Bowen Deng", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Zoom-In to Sort AI-Generated Images Out", "comment": "9 pages, 6 images (19 pages, 11 figures including appendix)", "summary": "The rapid growth of AI-generated imagery has blurred the boundary between\nreal and synthetic content, raising critical concerns for digital integrity.\nVision-language models (VLMs) offer interpretability through explanations but\noften fail to detect subtle artifacts in high-quality synthetic images. We\npropose ZoomIn, a two-stage forensic framework that improves both accuracy and\ninterpretability. Mimicking human visual inspection, ZoomIn first scans an\nimage to locate suspicious regions and then performs a focused analysis on\nthese zoomed-in areas to deliver a grounded verdict. To support training, we\nintroduce MagniFake, a dataset of 20,000 real and high-quality synthetic images\nannotated with bounding boxes and forensic explanations, generated through an\nautomated VLM-based pipeline. Our method achieves 96.39% accuracy with robust\ngeneralization, while providing human-understandable explanations grounded in\nvisual evidence.", "AI": {"tldr": "ZoomIn\u4e24\u9636\u6bb5\u6cd5\u901a\u8fc7\u5b9a\u4f4d\u53ef\u7591\u533a\u57df\u548c\u805a\u7126\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u51c6\u786e\u7387\u8fbe96.39%\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u7684\u5feb\u901f\u53d1\u5c55\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u4e2d\u7ec6\u5fae\u4f2a\u5f71\u7684\u68c0\u6d4b\u56f0\u96be\uff0c\u5f15\u53d1\u4e86\u5bf9\u6570\u5b57\u5b8c\u6574\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u63d0\u51faZoomIn\u4e24\u9636\u6bb5\u6cd5\uff0c\u9996\u5148\u751f\u6210\u53ef\u7591\u533a\u57df\uff0c\u7136\u540e\u5bf9\u653e\u5927\u533a\u57df\u8fdb\u884c\u805a\u7126\u5206\u6790\uff1b\u5e76\u4f7f\u7528MagniFake\u6570\u636e\u96c6\uff0820,000\u5f20\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ZoomIn\u65b9\u6cd5\u5b9e\u73b0\u4e8696.39%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u7684\u53ef\u89e3\u91ca\u6027\u7ed3\u679c\u3002", "conclusion": "ZoomIn\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6570\u5b57\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04245", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04245", "abs": "https://arxiv.org/abs/2510.04245", "authors": ["Ayushi Mehrotra", "Derek Peng", "Dipkamal Bhusal", "Nidhi Rastogi"], "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks", "comment": "neurips workshop", "summary": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u8865\u4e01\u5148\u9a8c\u77e5\u8bc6\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\u4e2d\u548c\u8865\u4e01\u6548\u5e94\uff0c\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5df2\u77e5\u8865\u4e01\u7684\u5927\u5c0f\u6216\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8865\u4e01\u65e0\u5173\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u6765\u8bc6\u522b\u548c\u6291\u5236\u6700\u5177\u5f71\u54cd\u529b\u7684\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff0c\u4ece\u800c\u5728\u4e0d\u8fdb\u884c\u663e\u5f0f\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\u4e2d\u548c\u8865\u4e01\u6548\u5e94\u3002", "result": "\u5728Imagenette\u6570\u636e\u96c6\u4e0a\u4f7f\u7528ResNet-50\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6e05\u6d01\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684PatchCleanser\uff0c\u4e14\u5728\u4e0d\u540c\u8865\u4e01\u5927\u5c0f\u548c\u4f4d\u7f6e\u4e0a\u4fdd\u6301\u5f3a\u5065\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u5c55\u793a\u4e86\u6982\u5ff5\u9a71\u52a8\u9632\u5fa1\u5728\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7b56\u7565\u3002"}}
{"id": "2510.04390", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04390", "abs": "https://arxiv.org/abs/2510.04390", "authors": ["Xuehai He", "Shijie Zhou", "Thivyanth Venkateswaran", "Kaizhi Zheng", "Ziyu Wan", "Achuta Kadambi", "Xin Eric Wang"], "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator", "comment": null, "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.", "AI": {"tldr": "MorphoSim\u662f\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u76844D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5bf9\u8c61\u7ea7\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u52a8\u6001\u73af\u5883\u9700\u6c42\u3002", "motivation": "\u5f00\u53d1\u652f\u6301\u53ef\u63a7\u548c\u53ef\u7f16\u8f91\u65f6\u7a7a\u73af\u5883\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u5bf9\u53ef\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u3001\u53ef\u91cd\u590d\u8bc4\u4f30\u548c\u7075\u6d3b\u4efb\u52a1\u8bbe\u8ba1\u7684\u9700\u6c42\u3002", "method": "MorphoSim\u6846\u67b6\u7ed3\u5408\u4e86\u8f68\u8ff9\u5f15\u5bfc\u751f\u6210\u4e0e\u7279\u5f81\u573a\u84b8\u998f\u6280\u672f\uff0c\u652f\u6301\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u4e14\u5177\u5907\u5bf9\u8c61\u7ea7\u63a7\u5236\u7684\u52a8\u6001\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMorphoSim\u5728\u4fdd\u6301\u9ad8\u573a\u666f\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u573a\u666f\u7684\u53ef\u63a7\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "MorphoSim\u901a\u8fc7\u6574\u5408\u8f68\u8ff9\u5f15\u5bfc\u751f\u6210\u4e0e\u7279\u5f81\u573a\u84b8\u998f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u76844D\u573a\u666f\u53ef\u63a7\u6027\u548c\u53ef\u7f16\u8f91\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u52a8\u6001\u73af\u5883\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2510.04401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04401", "abs": "https://arxiv.org/abs/2510.04401", "authors": ["Xuyang Guo", "Zekai Huang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting", "comment": null, "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.", "AI": {"tldr": "VLMs\u5728\u7ec4\u5408\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u63a2\u7a76VLMs\u662f\u5426\u80fd\u51c6\u786e\u8ba1\u6570\uff0c\u5c24\u5176\u662f\u5728\u7ec4\u5408\u5f62\u72b6\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7VLMCountBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a7\u5236\u53d8\u91cf\uff08\u5982\u989c\u8272\u3001\u5927\u5c0f\u3001\u63d0\u793a\u4f18\u5316\uff09\u5e76\u7cfb\u7edf\u5206\u6790\u7b80\u5355\u51e0\u4f55\u5f62\u72b6\uff08\u5982\u4e09\u89d2\u5f62\u3001\u5706\u5f62\uff09\u7684\u8ba1\u6570\u4efb\u52a1\u3002", "result": "VLMs\u5728\u5355\u4e00\u5f62\u72b6\u8ba1\u6570\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u7ec4\u5408\u5f62\u72b6\u8ba1\u6570\u4e2d\u5931\u8d25\u7387\u663e\u8457\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7ec4\u5408\u8ba1\u6570\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u80fd\u529b\u3002"}}
{"id": "2510.04472", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.04472", "abs": "https://arxiv.org/abs/2510.04472", "authors": ["Baber Jan", "Saeed Anwar", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais"], "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.", "AI": {"tldr": "SPEGNet\u901a\u8fc7\u7edf\u4e00\u8bbe\u8ba1\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f2a\u88c5\u68c0\u6d4b\u65b9\u6cd5\u7684\u590d\u6742\u6027\u548c\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5b9e\u65f6\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u4f2a\u88c5\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u56e0\u4f9d\u8d56\u590d\u6742\u7ec4\u4ef6\u7d2f\u79ef\u800c\u5e26\u6765\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4e14\u56e0\u964d\u4f4e\u5206\u8fa8\u7387\u4e22\u5931\u7ec6\u8282\uff0cSPEGNet\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SPEGNet\u91c7\u7528\u901a\u9053\u6821\u51c6\u548c\u7a7a\u95f4\u589e\u5f3a\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u6574\u5408\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7ec6\u5316\u5b9e\u73b0\u5c3a\u5ea6\u81ea\u9002\u5e94\u7684\u8fb9\u7f18\u8c03\u5236\uff0c\u4fdd\u6301\u8bed\u4e49\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "SPEGNet\u5728CAMO\u3001COD10K\u548cNC4K\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.887\u30010.890\u548c0.895\u7684$S_\\alpha$\u5206\u6570\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SPEGNet\u901a\u8fc7\u7edf\u4e00\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u590d\u6742\u6027\u548c\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6027\u80fd\uff08\u5982CAMO\u3001COD10K\u548cNC4K\uff09\u548c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.04477", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04477", "abs": "https://arxiv.org/abs/2510.04477", "authors": ["Soo Yong Kim", "Suin Cho", "Vincent-Daniel Yun", "Gyeongyeon Hwang"], "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models", "comment": null, "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.", "AI": {"tldr": "MedCLM\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u8bfe\u7a0b\u7b56\u7565\uff0c\u5728\u533b\u5b66VQA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5bf9\u9f50\u6a21\u578b\u63d0\u4f9b\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\u4e0eAI\u7ed3\u5408\u7684\u6311\u6218\uff0c\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u5bf9\u9f50\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMedCLM\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5c06\u68c0\u6d4b\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u5927\u89c4\u6a21\u533b\u5b66VQA\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3002\u91c7\u7528\u96c6\u6210CoT-\u8bfe\u7a0b\u7b56\u7565\uff0c\u5206\u4e3a\u7b80\u5355\uff08\u663e\u6027\u75c5\u7076\u6846\uff09\u3001\u4e2d\u7b49\uff08\u9690\u6027\u5b9a\u4f4d\uff09\u548c\u56f0\u96be\uff08\u5f31\u76d1\u7763\u63a8\u7406\uff09\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "MedCLM\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MedCLM\u901a\u8fc7\u6574\u5408\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u4e34\u5e8a\u5bf9\u9f50\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2510.04630", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04630", "abs": "https://arxiv.org/abs/2510.04630", "authors": ["Vrushank Ahire", "Aniruddh Muley", "Shivam Zample", "Siddharth Verma", "Pranav Menon", "Surbhi Madan", "Abhinav Dhall"], "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection", "comment": null, "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Transformer\u548c\u7eb9\u7406\u65b9\u6cd5\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u751f\u6210\u6280\u672f\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff08\u5982Swin Transformers\u548cViTs\uff09\u548c\u57fa\u4e8e\u7eb9\u7406\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u521b\u65b0\u7684\u6570\u636e\u5206\u5272\u3001\u987a\u5e8f\u8bad\u7ec3\u3001\u9891\u7387\u5206\u5272\u3001\u57fa\u4e8e\u8865\u4e01\u7684\u6ce8\u610f\u529b\u548c\u9762\u90e8\u5206\u5272\u6280\u672f\u3002", "result": "\u5728DFWild-Cup\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86Transformer\u5728\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u548c\u7eb9\u7406\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u80fd\u6709\u6548\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u4e0d\u65ad\u53d8\u5316\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04759", "abs": "https://arxiv.org/abs/2510.04759", "authors": ["Chi Yan", "Dan Xu"], "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction", "comment": "Project Page: https://yanchi-3dv.github.io/PG-Occ", "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ", "AI": {"tldr": "PG-Occ\u662f\u4e00\u79cd\u6e10\u8fdb\u5f0f\u9ad8\u65af\u53d8\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u81f4\u5bc6\u5316\u548c\u5f02\u5411\u6027\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u8bcd\u6c473D\u5360\u7528\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u8bed\u4e49\u7c7b\u522b\uff0c\u800c\u73b0\u6709\u6587\u672c\u5bf9\u9f50\u65b9\u6cd5\u5728\u7a00\u758f\u9ad8\u65af\u8868\u793a\u548c\u5c0f\u7269\u4f53\u6355\u83b7\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002PG-Occ\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76843D\u5360\u7528\u9884\u6d4b\u3002", "method": "PG-Occ\u91c7\u7528\u6e10\u8fdb\u5f0f\u5728\u7ebf\u81f4\u5bc6\u5316\u548c\u5f02\u5411\u6027\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u589e\u5f3a3D\u9ad8\u65af\u8868\u793a\u6765\u6355\u83b7\u7ec6\u7c92\u5ea6\u573a\u666f\u7ec6\u8282\uff0c\u5e76\u81ea\u9002\u5e94\u5206\u914d\u4e0d\u540c\u5c3a\u5ea6\u548c\u9636\u6bb5\u7684\u611f\u53d7\u91ce\u3002", "result": "PG-Occ\u5728\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e8614.3%\u7684mIoU\u76f8\u5bf9\u6539\u8fdb\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "PG-Occ\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u9ad8\u65af\u53d8\u6362\u548c\u5f02\u5411\u6027\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e8614.3%\u7684mIoU\u76f8\u5bf9\u6539\u8fdb\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2510.04797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04797", "abs": "https://arxiv.org/abs/2510.04797", "authors": ["Qi Li", "Shuwen Qiu", "Julien Han", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kee Kiat Koo", "Karim Bouyarmane"], "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On\n(VTO) technologies, enabling customers to realistically visualize products\noverlaid on their own images. Despite recent advances, existing VTO models face\nchallenges with fine-grained detail preservation, robustness to real-world\nimagery, efficient sampling, image editing capabilities, and generalization\nacross diverse product categories. In this paper, we present DiT-VTON, a novel\nVTO framework that leverages a Diffusion Transformer (DiT), renowned for its\nperformance on text-conditioned image generation, adapted here for the\nimage-conditioned VTO task. We systematically explore multiple DiT\nconfigurations, including in-context token concatenation, channel\nconcatenation, and ControlNet integration, to determine the best setup for VTO\nimage conditioning.\n  To enhance robustness, we train the model on an expanded dataset encompassing\nvaried backgrounds, unstructured references, and non-garment categories,\ndemonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also\nredefines the VTO task beyond garment try-on, offering a versatile Virtual\nTry-All (VTA) solution capable of handling a wide range of product categories\nand supporting advanced image editing functionalities such as pose\npreservation, localized editing, texture transfer, and object-level\ncustomization. Experimental results show that our model surpasses\nstate-of-the-art methods on VITON-HD, achieving superior detail preservation\nand robustness without reliance on additional condition encoders. It also\noutperforms models with VTA and image editing capabilities on a diverse dataset\nspanning thousands of product categories.", "AI": {"tldr": "DiT-VTON\u5229\u7528Diffusion Transformer\u4f18\u5316\u865a\u62df\u8bd5\u7a7f\u6280\u672f\uff0c\u901a\u8fc7\u591a\u914d\u7f6e\u63a2\u7d22\u548c\u6570\u636e\u6269\u5c55\u63d0\u5347\u7ec6\u8282\u4fdd\u7559\u548c\u9c81\u68d2\u6027\uff0c\u652f\u6301\u591a\u54c1\u7c7b\u548c\u9ad8\u7ea7\u7f16\u8f91\u529f\u80fd\u3002", "motivation": "\u7535\u5b50\u5546\u4e1a\u7684\u5feb\u901f\u53d1\u5c55\u589e\u52a0\u4e86\u5bf9\u865a\u62df\u8bd5\u7a7f\uff08VTO\uff09\u6280\u672f\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u7ec6\u8282\u4fdd\u7559\u3001\u9c81\u68d2\u6027\u3001\u91c7\u6837\u6548\u7387\u548c\u8de8\u54c1\u7c7b\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cdDiT\u914d\u7f6e\uff08\u5982\u4e0a\u4e0b\u6587\u6807\u8bb0\u62fc\u63a5\u3001\u901a\u9053\u62fc\u63a5\u548cControlNet\u96c6\u6210\uff09\uff0c\u5e76\u5728\u6269\u5c55\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "DiT-VTON\u5728VITON-HD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u7ec6\u8282\u4fdd\u7559\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u591a\u54c1\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiT-VTON\u901a\u8fc7\u91c7\u7528Diffusion Transformer\uff08DiT\uff09\u5e76\u4f18\u5316\u56fe\u50cf\u6761\u4ef6\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\uff08VTO\uff09\u6280\u672f\u7684\u7ec6\u8282\u4fdd\u7559\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u81f3\u591a\u54c1\u7c7b\u4ea7\u54c1\uff08VTA\uff09\u548c\u9ad8\u7ea7\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\u3002"}}
{"id": "2510.04802", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04802", "abs": "https://arxiv.org/abs/2510.04802", "authors": ["Han Zhang", "Lalithkumar Seenivasan", "Jose L. Porras", "Roger D. Soberanis-Mukul", "Hao Ding", "Hongchao Shu", "Benjamin D. Killeen", "Ankita Ghosh", "Lonny Yarmus", "Masaru Ishii", "Angela Christine Argento", "Mathias Unberath"], "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors", "comment": null, "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle.", "AI": {"tldr": "EgoSurg\u662f\u9996\u4e2a\u4ece\u58c1\u6302\u56fa\u5b9a\u6444\u50cf\u5934\u89c6\u9891\u91cd\u6784\u624b\u672f\u5ba4\u56e2\u961f\u6210\u5458\u52a8\u6001\u81ea\u6211\u4e2d\u5fc3\u56de\u653e\u7684\u6846\u67b6\uff0c\u65e0\u9700\u5e72\u9884\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u6c89\u6d78\u5f0f\u5916\u79d1\u6570\u636e\u79d1\u5b66\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u672f\u89c2\u5bdf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u89c6\u89d2\u6216\u56de\u5fc6\uff0c\u65e0\u6cd5\u8bb0\u5f55\u6307\u5bfc\u4e34\u5e8a\u51b3\u7b56\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u89c6\u89d2\u3002\u56fa\u5b9a\u6444\u50cf\u5934\u89c6\u9891\u867d\u80fd\u6355\u6349\u624b\u672f\u5ba4\u89c4\u6a21\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u65e0\u6cd5\u91cd\u6784\u6bcf\u4e2a\u56e2\u961f\u6210\u5458\u5b9e\u9645\u770b\u5230\u7684\u5185\u5bb9\uff0c\u56e0\u6b64\u5bf9\u5f71\u54cd\u624b\u672f\u5b89\u5168\u3001\u57f9\u8bad\u548c\u6d41\u7a0b\u4f18\u5316\u7684\u51b3\u7b56\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\u3002", "method": "EgoSurg\u7ed3\u5408\u4e86\u51e0\u4f55\u9a71\u52a8\u7684\u795e\u7ecf\u6e32\u67d3\u548c\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u56fe\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u4ee5\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u5408\u6210\u4efb\u610f\u65f6\u523b\u7684\u4efb\u610f\u548c\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u3002", "result": "\u5728\u591a\u5730\u70b9\u624b\u672f\u6848\u4f8b\u548c\u5bf9\u7167\u7814\u7a76\u4e2d\uff0cEgoSurg\u80fd\u591f\u4ee5\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u91cd\u6784\u7279\u5b9a\u4eba\u5458\u7684\u89c6\u89c9\u5b57\u6bb5\u548c\u4efb\u610f\u89c6\u89d2\u3002", "conclusion": "EgoSurg\u901a\u8fc7\u5c06\u73b0\u6709\u624b\u672f\u5ba4\u6444\u50cf\u5934\u57fa\u7840\u8bbe\u65bd\u8f6c\u53d8\u4e3a\u53ef\u5bfc\u822a\u7684\u52a8\u60013D\u8bb0\u5f55\uff0c\u4e3a\u6c89\u6d78\u5f0f\u5916\u79d1\u6570\u636e\u79d1\u5b66\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u624b\u672f\u5b9e\u8df5\u53ef\u4ee5\u4ece\u6bcf\u4e2a\u89d2\u5ea6\u53ef\u89c6\u5316\u3001\u4f53\u9a8c\u548c\u5206\u6790\u3002"}}
{"id": "2510.04923", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04923", "abs": "https://arxiv.org/abs/2510.04923", "authors": ["Alec K. Peltekian", "Halil Ertugrul Aktas", "Gorkem Durak", "Kevin Grudzinski", "Bradford C. Bemiss", "Carrie Richardson", "Jane E. Dematte", "G. R. Scott Budinger", "Anthony J. Esposito", "Alexander Misharin", "Alok Choudhary", "Ankit Agrawal", "Ulas Bagci"], "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis", "comment": "10 pages, 4 figures, 2 tables", "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.", "AI": {"tldr": "REN\u662f\u4e00\u79cd\u57fa\u4e8e\u89e3\u5256\u5b66\u5148\u9a8c\u7684MoE\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u95e8\u63a7\u548c\u533a\u57df\u4e13\u5bb6\u7f51\u7edc\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u95f4\u8d28\u6027\u80ba\u75c5\u5206\u7c7b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u4f20\u7edfMoE\u7cfb\u7edf\u7f3a\u4e4f\u533b\u5b66\u5f71\u50cf\u9886\u57df\u6240\u9700\u7684\u89e3\u5256\u5b66\u7ea6\u675f\uff0c\u800c\u89e3\u5256\u7ed3\u6784\u548c\u533a\u57df\u75be\u75c5\u5f02\u8d28\u6027\u5bf9\u75c5\u7406\u6a21\u5f0f\u6709\u91cd\u8981\u5f71\u54cd\u3002REN\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "REN\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u8bad\u7ec3\u4e03\u4e2a\u4e13\u5bb6\u7f51\u7edc\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u7279\u5b9a\u80ba\u53f6\u6216\u53cc\u4fa7\u80ba\u7ec4\u5408\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u95e8\u63a7\u673a\u5236\uff08\u653e\u5c04\u7ec4\u5b66\u751f\u7269\u6807\u5fd7\u7269\u548c\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\uff09\u52a8\u6001\u4f18\u5316\u4e13\u5bb6\u8d21\u732e\u3002", "result": "REN\u5728\u95f4\u8d28\u6027\u80ba\u75c5\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u653e\u5c04\u7ec4\u5b66\u5f15\u5bfc\u7684\u96c6\u6210\u6a21\u578b\u5e73\u5747AUC\u4e3a0.8646\uff08\u6bd4\u57fa\u7ebf\u63d0\u534712.5%\uff09\uff0c\u533a\u57df\u4e13\u5bb6\u6a21\u578b\uff08\u5982\u4e0b\u53f6\uff09AUC\u8fbe0.88-0.90\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "REN\uff08Regional Expert Networks\uff09\u901a\u8fc7\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u591a\u6a21\u6001\u95e8\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7ed3\u6784\u5316\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04939", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04939", "abs": "https://arxiv.org/abs/2510.04939", "authors": ["Yuxi Liu", "Catherine Lalman", "Yimin Yang"], "title": "Unsupervised Active Learning via Natural Feature Progressive Framework", "comment": "Under review at IEEE TPAMI", "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.", "AI": {"tldr": "NFPF\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7SFLM\u91cf\u5316\u6837\u672c\u91cd\u8981\u6027\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u6c34\u5e73\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u6807\u6ce8\u8fc7\u7a0b\u6602\u8d35\u4e14\u8017\u65f6\u3002\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\uff08UAL\uff09\u8bd5\u56fe\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u7136\u7279\u5f81\u6e10\u8fdb\u6846\u67b6\uff08NFPF\uff09\uff0c\u5229\u7528\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u673a\uff08SFLM\uff09\u91cf\u5316\u6837\u672c\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\uff0c\u5e76\u5b9a\u4e49\u91cd\u6784\u5dee\u5f02\u5ea6\u91cf\u8fdb\u884c\u521d\u59cb\u6837\u672c\u9009\u62e9\u3002", "result": "NFPF\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709UAL\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u76d1\u7763AL\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u5b9a\u6027\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "NFPF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04947", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04947", "abs": "https://arxiv.org/abs/2510.04947", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion", "comment": "BIBM2025 accept, 8 pages, 4 figures", "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.", "AI": {"tldr": "CA3D-Diff\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u4e73\u817aX\u5149\u7247\u89c6\u56fe\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u5217\u611f\u77e5\u548c\u9690\u5f0f3D\u7ed3\u6784\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u56fe\u751f\u6210\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u4e73\u817aX\u5149\u7247\u7684\u4e00\u4e2a\u89c6\u56fe\u53ef\u80fd\u7f3a\u5931\u6216\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002\u89c6\u56fe\u95f4\u7ffb\u8bd1\u53ef\u4ee5\u5e2e\u52a9\u6062\u590d\u7f3a\u5931\u89c6\u56fe\u5e76\u6539\u5584\u75c5\u7076\u5bf9\u9f50\uff0c\u4f46\u4e73\u817aX\u5149\u7247\u4e2d\u7684\u975e\u521a\u6027\u53d8\u5f62\u548c\u7ec4\u7ec7\u91cd\u53e0\u4f7f\u5f97\u8fd9\u4e00\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u53cc\u5411\u4e73\u817aX\u5149\u7247\u89c6\u56fe\u7ffb\u8bd1\u6846\u67b6CA3D-Diff\uff0c\u5305\u62ec\u5217\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u9690\u5f0f3D\u7ed3\u6784\u91cd\u5efa\u6a21\u5757\u3002", "result": "CA3D-Diff\u5728\u53cc\u5411\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5408\u6210\u7684\u89c6\u56fe\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u89c6\u56fe\u6076\u6027\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CA3D-Diff\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5411\u89c6\u56fe\u7ffb\u8bd1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e73\u817aX\u5149\u7247\u89c6\u56fe\u7684\u751f\u6210\u8d28\u91cf\uff0c\u6539\u5584\u4e86\u5355\u89c6\u56fe\u6076\u6027\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u8bca\u65ad\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04966", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04966", "abs": "https://arxiv.org/abs/2510.04966", "authors": ["Anna Chistyakova", "Mikhail Pautov"], "title": "ActiveMark: on watermarking of visual foundation models via massive activations", "comment": null, "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5d4c\u5165\u6570\u5b57\u6c34\u5370\u6765\u9a8c\u8bc1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6240\u6709\u6743\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6570\u636e\u6536\u96c6\u96be\u5ea6\u4fc3\u4f7f\u6a21\u578b\u6240\u6709\u8005\u9700\u8981\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\uff0c\u9632\u6b62\u975e\u6cd5\u518d\u5206\u53d1\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e00\u5c0f\u90e8\u5206\u8868\u8fbe\u5c42\uff0c\u5e76\u7ed3\u5408\u5c0f\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u5728\u8f93\u5165\u56fe\u50cf\u5185\u90e8\u8868\u793a\u4e2d\u5d4c\u5165\u6570\u5b57\u6c34\u5370\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6c34\u5370\u68c0\u6d4b\u4e2d\u5177\u6709\u4f4e\u8bef\u68c0\u7387\u548c\u6f0f\u68c0\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u6c34\u5370\u5d4c\u5165\u548c\u68c0\u6d4b\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4f4e\u8bef\u68c0\u7387\u548c\u6f0f\u68c0\u7387\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6240\u6709\u6743\u3002"}}
{"id": "2510.05096", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.05096", "abs": "https://arxiv.org/abs/2510.05096", "authors": ["Zeyu Zhu", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Paper2Video: Automatic Video Generation from Scientific Papers", "comment": "20 pages, 8 figures", "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.", "AI": {"tldr": "PaperTalker\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u5b66\u672f\u6f14\u793a\u89c6\u9891\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u5e76\u884c\u5316\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u5b66\u672f\u6f14\u793a\u89c6\u9891\u5236\u4f5c\u8017\u65f6\u4e14\u590d\u6742\uff0c\u6d89\u53ca\u591a\u6a21\u6001\u4fe1\u606f\u7684\u534f\u8c03\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u51cf\u8f7b\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u4e86PaperTalker\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5e7b\u706f\u7247\u751f\u6210\u3001\u5e03\u5c40\u4f18\u5316\u3001\u5b57\u5e55\u751f\u6210\u3001\u8bed\u97f3\u5408\u6210\u548c\u8bf4\u8bdd\u4eba\u5934\u50cf\u6e32\u67d3\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u5316\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728Paper2Video\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPaperTalker\u751f\u6210\u7684\u89c6\u9891\u5728\u5fe0\u5b9e\u6027\u548c\u4fe1\u606f\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "PaperTalker\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u5b9a\u5236\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u5b66\u672f\u6f14\u793a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u751f\u6210\u7684\u89c6\u9891\u5728\u5fe0\u5b9e\u6027\u548c\u4fe1\u606f\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
