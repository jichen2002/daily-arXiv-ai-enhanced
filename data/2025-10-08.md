<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.DC](#cs.DC) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation](https://arxiv.org/abs/2510.05532)
*Sam Sartor,Pieter Peers*

Main category: cs.CV

TL;DR: Teamwork是一种无需改变预训练扩散模型架构的灵活通道扩展方法，通过协调多个模型实例并采用LoRA变体，适用于多种图形任务。


<details>
  <summary>Details</summary>
Motivation: 大型预训练扩散模型可为许多图形应用提供强大的先验，但现有通道扩展方法往往是特定于应用的，难以适应不同的扩散模型或新任务。

Method: 采用低秩适应（LoRA）的变体来联合解决不同队友之间的适应和协调问题，并支持队友的动态（去）激活。

Result: Teamwork在多种生成和反向图形任务中展示了其灵活性和效率，如修复、单图像SVBRDF估计、内在分解、神经着色和内在图像合成。

Conclusion: Teamwork提出了一种灵活且高效的统一解决方案，通过协调和适应多个基础扩散模型实例，实现了通道扩展，无需改变预训练扩散模型的架构。

Abstract: Large pretrained diffusion models can provide strong priors beneficial for
many graphics applications. However, generative applications such as neural
rendering and inverse methods such as SVBRDF estimation and intrinsic image
decomposition require additional input or output channels. Current solutions
for channel expansion are often application specific and these solutions can be
difficult to adapt to different diffusion models or new tasks. This paper
introduces Teamwork: a flexible and efficient unified solution for jointly
increasing the number of input and output channels as well as adapting a
pretrained diffusion model to new tasks. Teamwork achieves channel expansion
without altering the pretrained diffusion model architecture by coordinating
and adapting multiple instances of the base diffusion model (\ie, teammates).
We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address
both adaptation and coordination between the different teammates. Furthermore
Teamwork supports dynamic (de)activation of teammates. We demonstrate the
flexibility and efficiency of Teamwork on a variety of generative and inverse
graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic
decomposition, neural shading, and intrinsic image synthesis.

</details>


### [2] [Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation](https://arxiv.org/abs/2510.05266)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 提出E-FPN框架用于少样本语义分割，通过原型学习和注意力机制在基础设施检查中取得优异性能，显著减少对新训练数据的需求。


<details>
  <summary>Details</summary>
Motivation: 基础设施检查应用中标记训练样本稀缺且昂贵，现有深度学习框架需要大量标记数据且难以用少量数据学习新缺陷类别。

Method: 提出了增强型特征金字塔网络（E-FPN），包括：（1）使用InceptionSepConv块和深度可分离卷积的自适应编码器；（2）基于掩码平均池化的原型学习；（3）通过全局自注意力、局部自注意力和交叉注意力的基于注意力的特征表示。

Result: 在具有挑战性的基础设施检查数据集上，该方法在8-way 5-shot训练配置下达到82.55% F1分数和72.26% mIoU，自注意力方法带来最显著的性能提升（F1分数提高2.57%，mIoU提高2.9%）。

Conclusion: 该框架解决了基础设施检查系统中快速响应新缺陷类型的关键需求，仅需有限的新训练数据，从而为关键基础设施系统提供更高效、经济的维护方案。

Abstract: Few-shot semantic segmentation is vital for deep learning-based
infrastructure inspection applications, where labeled training examples are
scarce and expensive. Although existing deep learning frameworks perform well,
the need for extensive labeled datasets and the inability to learn new defect
categories with little data are problematic. We present our Enhanced Feature
Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert
and sewer defect categories using a prototypical learning framework. Our
approach has three main contributions: (1) adaptive E-FPN encoder using
InceptionSepConv blocks and depth-wise separable convolutions for efficient
multi-scale feature extraction; (2) prototypical learning with masked average
pooling for powerful prototype generation from small support examples; and (3)
attention-based feature representation through global self-attention, local
self-attention and cross-attention. Comprehensive experimentation on
challenging infrastructure inspection datasets illustrates that the method
achieves excellent few-shot performance, with the best configuration being
8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way
classification testing. The self-attention method had the most significant
performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over
baselines. Our framework addresses the critical need to rapidly respond to new
defect types in infrastructure inspection systems with limited new training
data that lead to more efficient and economical maintenance plans for critical
infrastructure systems.

</details>


### [3] [SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography](https://arxiv.org/abs/2510.05296)
*Zahra Maleki,Amirhossein Akbari,Amirhossein Binesh,Babak Khalaj*

Main category: cs.CV

TL;DR: 该论文提出了一种新型皮肤分割技术，用于提高远程光体积描记术（rPPG）的准确性。该技术在挑战性条件下表现优异，并能适应多种肤色。


<details>
  <summary>Details</summary>
Motivation: 远程光体积描记术（rPPG）是一种低成本、非接触式的远程监测方法，但其对光照和运动敏感。为了提高准确性，特别是在无监督流程中，需要从视频中选择皮肤区域以提取rPPG信号。

Method: 论文介绍了一种新型皮肤分割技术，优先选择皮肤区域以提高提取信号的质。该技术能够检测全身皮肤区域，同时对嘴巴、眼睛和头发等可能引起干扰的区域进行排除。

Result: 模型在公开数据集上进行了评估，并提出了一个新的数据集SYNC-rPPG以更好地代表现实条件。结果表明，该模型在挑战性条件下（如说话和头部旋转）能够优先捕捉心跳，并保持预测与实际心率之间的低MAE。

Conclusion: 该论文提出的新型皮肤分割技术在具有挑战性的条件下（如说话和头部旋转）表现出优先捕捉心跳的能力，并在预测与实际心率之间保持较低的绝对误差（MAE），同时在其他方法失败的情况下仍能保持性能。此外，该技术对多种肤色的检测具有高准确性，使其成为现实应用的理想选择。

Abstract: Remote photoplethysmography (rPPG) is an innovative method for monitoring
heart rate and vital signs by using a simple camera to record a person, as long
as any part of their skin is visible. This low-cost, contactless approach helps
in remote patient monitoring, emotion analysis, smart vehicle utilization, and
more. Over the years, various techniques have been proposed to improve the
accuracy of this technology, especially given its sensitivity to lighting and
movement. In the unsupervised pipeline, it is necessary to first select skin
regions from the video to extract the rPPG signal from the skin color changes.
We introduce a novel skin segmentation technique that prioritizes skin regions
to enhance the quality of the extracted signal. It can detect areas of skin all
over the body, making it more resistant to movement, while removing areas such
as the mouth, eyes, and hair that may cause interference. Our model is
evaluated on publicly available datasets, and we also present a new dataset,
called SYNC-rPPG, to better represent real-world conditions. The results
indicate that our model demonstrates a prior ability to capture heartbeats in
challenging conditions, such as talking and head rotation, and maintain the
mean absolute error (MAE) between predicted and actual heart rates, while other
methods fail to do so. In addition, we demonstrate high accuracy in detecting a
diverse range of skin tones, making this technique a promising option for
real-world applications.

</details>


### [4] [DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology](https://arxiv.org/abs/2510.05315)
*Yousef Yeganeh,Maximilian Frantzen,Michael Lee,Kun-Hsing Yu,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: DeepAf是一种新型自动对焦框架，结合空间和光谱特征，显著提升传统显微镜效率，适用于资源受限环境，实现高精度数字病理学诊断。


<details>
  <summary>Details</summary>
Motivation: 高成本的WSI扫描仪限制了数字病理学的普及，而现有低成本解决方案在一致性对焦、输入需求或泛化能力方面存在不足。论文旨在解决这些问题，提供一种高效、低成本的替代方案。

Method: 论文介绍了一种基于深度学习的自动对焦框架DeepAf，该框架通过提取空间和光谱特征，利用混合架构进行单次对焦预测，并自动调整控制参数以获得最佳图像效果。

Result: 实验结果显示，DeepAf将聚焦时间减少了80%，对焦精度达到0.18 μm，与双图像方法相当，且仅需一半输入。在跨实验室测试中，假对焦预测率仅为0.72%，90%的预测在焦深范围内。临床研究中，系统在4x放大倍数下实现了0.90的AUC癌症分类性能。

Conclusion: 该论文提出了一种名为DeepAf的新型自动对焦框架，结合空间和光谱特征，通过混合架构实现单次对焦预测，显著提升了传统显微镜的效率，使其在资源受限的环境中也能实现高精度的数字病理学诊断。

Abstract: While Whole Slide Imaging (WSI) scanners remain the gold standard for
digitizing pathology samples, their high cost limits accessibility in many
healthcare settings. Other low-cost solutions also face critical limitations:
automated microscopes struggle with consistent focus across varying tissue
morphology, traditional auto-focus methods require time-consuming focal stacks,
and existing deep-learning approaches either need multiple input images or lack
generalization capability across tissue types and staining protocols. We
introduce a novel automated microscopic system powered by DeepAf, a novel
auto-focus framework that uniquely combines spatial and spectral features
through a hybrid architecture for single-shot focus prediction. The proposed
network automatically regresses the distance to the optimal focal point using
the extracted spatiospectral features and adjusts the control parameters for
optimal image outcomes. Our system transforms conventional microscopes into
efficient slide scanners, reducing focusing time by 80% compared to stack-based
methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples,
matching the performance of dual-image methods (0.19 {\mu}m) with half the
input requirements. DeepAf demonstrates robust cross-lab generalization with
only 0.72% false focus predictions and 90% of predictions within the depth of
field. Through an extensive clinical study of 536 brain tissue samples, our
system achieves 0.90 AUC in cancer classification at 4x magnification, a
significant achievement at lower magnification than typical 20x WSI scans. This
results in a comprehensive hardware-software design enabling accessible,
real-time digital pathology in resource-constrained settings while maintaining
diagnostic accuracy.

</details>


### [5] [Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection](https://arxiv.org/abs/2510.05326)
*Jalal Ahmmed,Faruk Ahmed,Rashedul Hasan Shohan,Md. Mahabub Rana,Mahdi Hasan*

Main category: cs.CV

TL;DR: 研究比较五种CNN模型在芒果叶病害识别中的表现，DenseNet201以99.33%准确率最优，适用于智能农业。


<details>
  <summary>Details</summary>
Motivation: 芒果是南亚重要经济作物，但叶病害严重影响产量和品质，需高效准确的病害识别方法。

Method: 使用五种预训练的卷积神经网络（DenseNet201、InceptionV3、ResNet152V2、SeResNet152和Xception）进行迁移学习与微调，评估标准包括准确率、精确率、召回率、F1分数和混淆矩阵。

Result: DenseNet201表现最优，准确率99.33%，尤其在识别Cutting Weevil和Bacterial Canker方面突出；ResNet152V2和SeResNet152次之；InceptionV3和Xception在视觉相似类别（如Sooty Mould和Powdery Mildew）表现较差。

Conclusion: DenseNet201在芒果叶病害多类识别中表现最佳，准确率达99.33%，为智能农业应用提供了可靠的病害检测解决方案。

Abstract: Mango is an important fruit crop in South Asia, but its cultivation is
frequently hampered by leaf diseases that greatly impact yield and quality.
This research examines the performance of five pre-trained convolutional neural
networks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for
multi-class identification of mango leaf diseases across eight classes using a
transfer learning strategy with fine-tuning. The models were assessed through
standard evaluation metrics, such as accuracy, precision, recall, F1-score, and
confusion matrices. Among the architectures tested, DenseNet201 delivered the
best results, achieving 99.33% accuracy with consistently strong metrics for
individual classes, particularly excelling in identifying Cutting Weevil and
Bacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong
outcomes, whereas InceptionV3 and Xception exhibited lower performance in
visually similar categories like Sooty Mould and Powdery Mildew. The training
and validation plots demonstrated stable convergence for the highest-performing
models. The capability of fine-tuned transfer learning models, for precise and
dependable multi-class mango leaf disease detection in intelligent agricultural
applications.

</details>


### [6] [Mitigating Diffusion Model Hallucinations with Dynamic Guidance](https://arxiv.org/abs/2510.05356)
*Kostas Triaridis,Alexandros Graikos,Aggelina Chatziagapi,Grigorios G. Chrysos,Dimitris Samaras*

Main category: cs.CV

TL;DR: 动态引导通过选择性锐化评分函数，减少扩散模型的幻觉生成，同时保持语义多样性，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的样本存在结构不一致的幻觉现象，这是由于数据分布模式间过度平滑导致的。尽管语义插值有助于生成多样性，但需要更细致的解决方案。

Method: 引入动态引导（Dynamic Guidance），选择性锐化已知导致伪影的评分函数方向，保留有效的语义变化。

Result: 动态引导在自然和受控图像数据集上显著减少了幻觉现象，性能大幅优于基线方法。

Conclusion: 动态引导（Dynamic Guidance）通过选择性锐化导致伪影的预定方向的评分函数，有效减少了扩散模型中的幻觉现象，同时在自然和受控图像数据集上显著优于基线方法。

Abstract: Diffusion models, despite their impressive demos, often produce hallucinatory
samples with structural inconsistencies that lie outside of the support of the
true data distribution. Such hallucinations can be attributed to excessive
smoothing between modes of the data distribution. However, semantic
interpolations are often desirable and can lead to generation diversity, thus
we believe a more nuanced solution is required. In this work, we introduce
Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates
hallucinations by selectively sharpening the score function only along the
pre-determined directions known to cause artifacts, while preserving valid
semantic variations. To our knowledge, this is the first approach that
addresses hallucinations at generation time rather than through post-hoc
filtering. Dynamic Guidance substantially reduces hallucinations on both
controlled and natural image datasets, significantly outperforming baselines.

</details>


### [7] [LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation](https://arxiv.org/abs/2510.05367)
*Yang Xiao,Gen Li,Kaiyuan Deng,Yushu Wu,Zheng Zhan,Yanzhi Wang,Xiaolong Ma,Bo Hui*

Main category: cs.CV

TL;DR: 本文提出三种阶段特定策略以减少内存消耗，在扩散模型视频生成中实现训练免费加速，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理中潜在变量的冗余为加速提供了自然切入点，但现有缓存加速方法在后两个阶段会导致内存激增。

Method: 1) 异步缓存交换。2) 特征分块。3) 切片潜在解码。

Result: 与基线相比，该方法实现了更快的推理速度和更低的内存使用，同时质量退化保持在可接受范围内。

Conclusion: 本文提出的方法在保持质量退化在可接受范围内的同时，实现了更快的推理速度和更低的内存使用。

Abstract: Training-free acceleration has emerged as an advanced research area in video
generation based on diffusion models. The redundancy of latents in diffusion
model inference provides a natural entry point for acceleration. In this paper,
we decompose the inference process into the encoding, denoising, and decoding
stages, and observe that cache-based acceleration methods often lead to
substantial memory surges in the latter two stages. To address this problem, we
analyze the characteristics of inference across different stages and propose
stage-specific strategies for reducing memory consumption: 1) Asynchronous
Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same
time, we ensure that the time overhead introduced by these three strategies
remains lower than the acceleration gains themselves. Compared with the
baseline, our approach achieves faster inference speed and lower memory usage,
while maintaining quality degradation within an acceptable range. The Code is
available at https://github.com/NKUShaw/LightCache .

</details>


### [8] [See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models](https://arxiv.org/abs/2510.05408)
*Kebin Contreras,Luis Toscano-Palomino,Mauro Dalla Mura,Jorge Bacca*

Main category: cs.CV

TL;DR: 利用热成像和RGB图像配对，结合视觉语言模型和约束扩散过程，成功恢复了120秒前的场景状态，为时间反转成像提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 从当前观测中恢复过去信息是一个具有挑战性的任务，尤其在法医学和场景分析中具有潜在应用价值。热成像能够捕捉到人类与周围环境互动留下的残留热痕迹，这些痕迹可作为被动时间编码。

Method: 该方法结合了视觉语言模型（VLMs）和约束扩散过程，其中一个VLM生成场景描述，另一个引导图像重建，确保语义和结构一致性。

Result: 在三种受控场景下的评估表明，该方法能够重建过去120秒内的合理场景帧。

Conclusion: 该研究提出了一种基于热成像和RGB图像配对的时间反转重建框架，能够从热痕迹中恢复过去120秒内的场景状态，为时间反转成像提供了初步可行性验证。

Abstract: Recovering the past from present observations is an intriguing challenge with
potential applications in forensics and scene analysis. Thermal imaging,
operating in the infrared range, provides access to otherwise invisible
information. Since humans are typically warmer (37 C -98.6 F) than their
surroundings, interactions such as sitting, touching, or leaning leave residual
heat traces. These fading imprints serve as passive temporal codes, allowing
for the inference of recent events that exceed the capabilities of RGB cameras.
This work proposes a time-reversed reconstruction framework that uses paired
RGB and thermal images to recover scene states from a few seconds earlier. The
proposed approach couples Visual-Language Models (VLMs) with a constrained
diffusion process, where one VLM generates scene descriptions and another
guides image reconstruction, ensuring semantic and structural consistency. The
method is evaluated in three controlled scenarios, demonstrating the
feasibility of reconstructing plausible past frames up to 120 seconds earlier,
providing a first step toward time-reversed imaging from thermal traces.

</details>


### [9] [Personalizing Retrieval using Joint Embeddings or "the Return of Fluffy"](https://arxiv.org/abs/2510.05411)
*Bruno Korbar,Andrew Zisserman*

Main category: cs.CV

TL;DR: 论文提出了一种可训练的映射网络（pi-map），将图像嵌入转换为文本标记，结合CLIP编码器提升个性化图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 目标是通过结合图像中的对象实例信息和自然文本描述（如对象的行为或位置）来检索图像，例如检索“Fluffy独角兽（由图像指定）在某人头上”的图像。

Method: 设计了一个映射网络，将局部图像嵌入（对象实例）转换为文本标记，使其与自然语言查询结合适用于CLIP风格的文本编码和图像检索。训练过程简单，每个对象实例只需一次训练。

Result: 该方法在两个用于评估个性化检索的基准测试中取得了最先进的性能。

Conclusion: 该论文通过设计一个可训练的映射网络（pi-map），结合冻结的CLIP文本和图像编码器，显著提升了个性化检索任务的性能。

Abstract: The goal of this paper is to be able to retrieve images using a compound
query that combines object instance information from an image, with a natural
text description of what that object is doing or where it is. For example, to
retrieve an image of "Fluffy the unicorn (specified by an image) on someone's
head". To achieve this we design a mapping network that can "translate" from a
local image embedding (of the object instance) to a text token, such that the
combination of the token and a natural language query is suitable for CLIP
style text encoding, and image retrieval. Generating a text token in this
manner involves a simple training procedure, that only needs to be performed
once for each object instance. We show that our approach of using a trainable
mapping network, termed pi-map, together with frozen CLIP text and image
encoders, improves the state of the art on two benchmarks designed to assess
personalized retrieval.

</details>


### [10] [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](https://arxiv.org/abs/2510.05488)
*Peizhi Yan,Rabab Ward,Qiang Tang,Shan Du*

Main category: cs.CV

TL;DR: ArchitectHead 是一种支持连续控制LOD的3D高斯头部化身框架，通过动态重采样特征图实现高效LOD控制，无需重新训练，在最高和最低LOD下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS化身通常依赖固定数量的高斯点，无法灵活调整细节层次（LOD）。实际应用中需要可调节的LOD以平衡渲染效率和视觉质量。

Method: 该方法的核心是将3D高斯参数化到2D UV特征空间中，并提出了一个由多级可学习特征图组成的UV特征场来编码其潜在特征。一个轻量级神经网络解码器将这些潜在特征转换为3D高斯属性进行渲染。通过动态重采样特征图来控制高斯数量。

Result: ArchitectHead 在最高LOD下实现了最先进的性能，在最低LOD下仅使用6.2%的高斯点，渲染速度几乎翻倍，同时视觉质量下降幅度较小（L1 Loss +7.9%，PSNR -0.97%，SSIM -0.6%，LPIPS Loss +24.1%）。

Conclusion: ArchitectHead 提出了一种支持连续控制细节层次（LOD）的3D高斯头部化身框架，通过动态重采样特征图实现高效LOD控制，无需重新训练。实验结果表明，该方法在最高LOD下实现了最先进的性能，同时在较低LOD下保持了接近最优的性能。

Abstract: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.

</details>


### [11] [Human Action Recognition from Point Clouds over Time](https://arxiv.org/abs/2510.05506)
*James Dickens*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D点云的动作识别新方法，结合点云技术和稀疏卷积网络，在NTU RGB-D 120数据集上表现优异，准确率达89.3%。


<details>
  <summary>Details</summary>
Motivation: 随着消费级深度传感器和激光雷达设备的普及，利用密集3D数据进行动作识别成为一个新的研究方向。

Method: 提出了一种新颖的3D动作识别框架，结合了点云技术和稀疏卷积网络，应用于体素映射的点云序列。

Result: 在NTU RGB-D 120数据集上的实验表明，该方法与现有骨骼动作识别算法竞争力相当，且在结合传感器和估计深度输入时准确率显著提升。

Conclusion: 该方法在NTU RGB-D 120数据集上表现优异，结合传感器和估计深度输入时准确率达到89.3%，超越了之前的点云动作识别方法。

Abstract: Recent research into human action recognition (HAR) has focused predominantly
on skeletal action recognition and video-based methods. With the increasing
availability of consumer-grade depth sensors and Lidar instruments, there is a
growing opportunity to leverage dense 3D data for action recognition, to
develop a third way. This paper presents a novel approach for recognizing
actions from 3D videos by introducing a pipeline that segments human point
clouds from the background of a scene, tracks individuals over time, and
performs body part segmentation. The method supports point clouds from both
depth sensors and monocular depth estimation. At the core of the proposed HAR
framework is a novel backbone for 3D action recognition, which combines
point-based techniques with sparse convolutional networks applied to
voxel-mapped point cloud sequences. Experiments incorporate auxiliary point
features including surface normals, color, infrared intensity, and body part
parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D
120 dataset demonstrates that the method is competitive with existing skeletal
action recognition algorithms. Moreover, combining both sensor-based and
estimated depth inputs in an ensemble setup, this approach achieves 89.3%
accuracy when different human subjects are considered for training and testing,
outperforming previous point cloud action recognition methods.

</details>


### [12] [Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models](https://arxiv.org/abs/2510.05509)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TL;DR: 论文提出了一种新的黎曼度量方法，通过噪声空间中的测地线改进扩散模型的插值效果，生成更自然的过渡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型缺乏显式的低维潜在空间，限制了流形感知分析和操作（如插值和编辑）。现有插值方法在高密度区域生成路径，可能与数据流形不一致，导致过渡不自然。

Method: 论文提出了一种基于分数函数雅可比矩阵的黎曼度量方法，用于在噪声空间中定义测地线，以更好地捕捉数据流形。

Result: 实验表明，该方法在图像插值任务中生成的过渡效果比现有基于密度和简单基线方法更自然和真实。

Conclusion: 该论文提出了一种新的黎曼度量方法，通过在噪声空间中定义测地线，使得插值路径更贴近数据流形，从而生成更自然和真实的过渡效果。

Abstract: Diffusion models are powerful deep generative models (DGMs) that generate
high-fidelity, diverse content. However, unlike classical DGMs, they lack an
explicit, tractable low-dimensional latent space that parameterizes the data
manifold. This absence limits manifold-aware analysis and operations, such as
interpolation and editing. Existing interpolation methods for diffusion models
typically follow paths through high-density regions, which are not necessarily
aligned with the data manifold and can yield perceptually unnatural
transitions. To exploit the data manifold learned by diffusion models, we
propose a novel Riemannian metric on the noise space, inspired by recent
findings that the Jacobian of the score function captures the tangent spaces to
the local data manifold. This metric encourages geodesics in the noise space to
stay within or run parallel to the learned data manifold. Experiments on image
interpolation show that our metric produces perceptually more natural and
faithful transitions than existing density-based and naive baselines.

</details>


### [13] [Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work](https://arxiv.org/abs/2510.05538)
*Owen Henkel,Bill Roberts,Doug Jaffe,Laurence Holt*

Main category: cs.CV

TL;DR: MLLMs在算术作业评分中表现优秀，但在数学插图上表现不佳；人类描述可显著提升其表现。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在基础教育中自动评分和反馈手写数学作业的潜力，以解决教师评分耗时且能通过学生解题过程深入了解学习情况的挑战。

Method: 研究通过两个实验评估MLLMs在手写数学作业中的表现：实验A分析288份加纳中学生算术作业（有明确答案），实验B评估150份美国小学生数学插图（无单一答案，需视觉解释和教学判断）。实验B还测试了模型在直接评分和接收人类描述后的表现差异。

Result: MLLMs在算术作业评分中表现接近人类（95%准确率，k=0.90），但在数学插图评分中表现较差（k=0.20）。提供人类描述后，模型表现提升至k=0.47，接近人类间一致性水平。

Conclusion: 多模态大语言模型（MLLMs）在手写数学作业评分中展现出潜力，尤其是在有明确答案的算术问题上表现接近人类水平（95%准确率，k=0.90），但在需要复杂视觉解释和教学判断的数学插图上表现较差（k=0.20）。通过提供人类描述，模型的表现可显著提升至接近人类间一致性水平（k=0.47）。

Abstract: Recent advances in multimodal large language models (MLLMs) raise the
question of their potential for grading, analyzing, and offering feedback on
handwritten student classwork. This capability would be particularly beneficial
in elementary and middle-school mathematics education, where most work remains
handwritten, because seeing students' full working of a problem provides
valuable insights into their learning processes, but is extremely
time-consuming to grade. We present two experiments investigating MLLM
performance on handwritten student mathematics classwork. Experiment A examines
288 handwritten responses from Ghanaian middle school students solving
arithmetic problems with objective answers. In this context, models achieved
near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human
educators would be unlikely to make. Experiment B evaluates 150 mathematical
illustrations from American elementary students, where the drawings are the
answer to the question. These tasks lack single objective answers and require
sophisticated visual interpretation as well as pedagogical judgment in order to
analyze and evaluate them. We attempted to separate MLLMs' visual capabilities
from their pedagogical abilities by first asking them to grade the student
illustrations directly, and then by augmenting the image with a detailed human
description of the illustration. We found that when the models had to analyze
the student illustrations directly, they struggled, achieving only k = 0.20
with ground truth scores, but when given human descriptions, their agreement
levels improved dramatically to k = 0.47, which was in line with human-to-human
agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic
work relatively well, but still struggle to "see" student mathematical
illustrations.

</details>


### [14] [Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics](https://arxiv.org/abs/2510.05558)
*Christopher Hoang,Mengye Ren*

Main category: cs.CV

TL;DR: Midway Network是一种新型自我监督学习架构，首次从自然视频中同时学习对象识别和运动理解的视觉表示，在语义分割和光流任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的自我监督学习方法主要集中在对象识别或运动理解的单一任务上，而忽略了二者之间的互补性。Midway Network旨在通过结合这两种任务，从自然视频中学习更全面的视觉表示。

Method: Midway Network采用了一种新的自我监督学习架构，包括一个中间自顶向下的路径来推断视频帧之间的运动潜在变量，以及一个密集的前向预测目标和分层结构来处理复杂、多对象的自然视频场景。

Result: 在两大自然视频数据集上预训练后，Midway Network在语义分割和光流任务上表现出色，优于先前的自我监督学习方法。此外，通过前向特征扰动的新分析方法，验证了其学习到的动态能够捕捉高层次对应关系。

Conclusion: Midway Network通过结合对象识别和运动理解的自我监督学习，成功地从自然视频中学习到强大的视觉表示，并在语义分割和光流任务中表现出色。

Abstract: Object recognition and motion understanding are key components of perception
that complement each other. While self-supervised learning methods have shown
promise in their ability to learn from unlabeled data, they have primarily
focused on obtaining rich representations for either recognition or motion
rather than both in tandem. On the other hand, latent dynamics modeling has
been used in decision making to learn latent representations of observations
and their transformations over time for control and planning tasks. In this
work, we present Midway Network, a new self-supervised learning architecture
that is the first to learn strong visual representations for both object
recognition and motion understanding solely from natural videos, by extending
latent dynamics modeling to this domain. Midway Network leverages a midway
top-down path to infer motion latents between video frames, as well as a dense
forward prediction objective and hierarchical structure to tackle the complex,
multi-object scenes of natural videos. We demonstrate that after pretraining on
two large-scale natural video datasets, Midway Network achieves strong
performance on both semantic segmentation and optical flow tasks relative to
prior self-supervised learning methods. We also show that Midway Network's
learned dynamics can capture high-level correspondence via a novel analysis
method based on forward feature perturbation.

</details>


### [15] [HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video](https://arxiv.org/abs/2510.05560)
*Hongchi Xia,Chih-Hao Lin,Hao-Yu Hsu,Quentin Leboutet,Katelyn Gao,Michael Paulitsch,Benjamin Ummenhofer,Shenlong Wang*

Main category: cs.CV

TL;DR: HoloScene 是一种新型交互式 3D 重建框架，通过优化方法整合观测数据与物理约束，实现高精度、物理合理且逼真的数字孪生，适用于 AR/VR、游戏和机器人等领域。


<details>
  <summary>Details</summary>
Motivation: 当前 3D 重建和场景理解方法在几何完整性、物体交互性、物理合理性、逼真渲染或动态模拟的物理属性等方面存在不足，HoloScene 旨在同时满足这些需求。

Method: HoloScene 采用基于能量的优化问题形式化重建过程，结合观测数据、物理约束和生成先验，通过混合方法（采样探索与梯度优化）高效实现。

Result: HoloScene 在多个基准数据集上表现优异，其生成的数字孪生具有完整精确的几何结构、物理稳定性和逼真的新视角渲染能力。

Conclusion: HoloScene 通过其创新的交互式 3D 重建框架，成功解决了当前 3D 重建方法在几何完整性、物体交互性、物理合理性和逼真渲染等方面的不足，展示了在多个应用领域的广泛适用性和高效性。

Abstract: Digitizing the physical world into accurate simulation-ready virtual
environments offers significant opportunities in a variety of fields such as
augmented and virtual reality, gaming, and robotics. However, current 3D
reconstruction and scene-understanding methods commonly fall short in one or
more critical aspects, such as geometry completeness, object interactivity,
physical plausibility, photorealistic rendering, or realistic physical
properties for reliable dynamic simulation. To address these limitations, we
introduce HoloScene, a novel interactive 3D reconstruction framework that
simultaneously achieves these requirements. HoloScene leverages a comprehensive
interactive scene-graph representation, encoding object geometry, appearance,
and physical properties alongside hierarchical and inter-object relationships.
Reconstruction is formulated as an energy-based optimization problem,
integrating observational data, physical constraints, and generative priors
into a unified, coherent objective. Optimization is efficiently performed via a
hybrid approach combining sampling-based exploration with gradient-based
refinement. The resulting digital twins exhibit complete and precise geometry,
physical stability, and realistic rendering from novel viewpoints. Evaluations
conducted on multiple benchmark datasets demonstrate superior performance,
while practical use-cases in interactive gaming and real-time digital-twin
manipulation illustrate HoloScene's broad applicability and effectiveness.
Project page: https://xiahongchi.github.io/HoloScene.

</details>


### [16] [CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval](https://arxiv.org/abs/2510.05586)
*Bin Kang,Bin Chen,Junjie Wang,Yulin Li,Junzhi Zhao,Zhuotao Tian*

Main category: cs.CV

TL;DR: CalibCLIP通过视觉和文本空间的校准方法，解决了低贡献令牌主导问题，提升了图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型中少数低贡献令牌可能过度捕获全局语义，抑制判别特征，影响文本驱动图像检索任务的效果。

Method: 在视觉空间引入对比视觉增强器（CVE），解耦视觉特征为目标和低信息区域，动态抑制主导令牌表示；在文本空间引入判别概念校准器（DCC），区分通用和判别概念，增强相似样本的区分能力。

Result: 在七个基准测试中，CalibCLIP在三种图像检索任务上均表现出一致的性能提升。

Conclusion: CalibCLIP通过视觉和文本空间的校准方法，有效解决了现有视觉语言模型中低贡献令牌过度主导全局语义的问题，显著提升了文本驱动图像检索任务的性能。

Abstract: Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP

</details>


### [17] [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
*Zeqi Gu,Markos Georgopoulos,Xiaoliang Dai,Marjan Ghazvininejad,Chu Wang,Felix Juefei-Xu,Kunpeng Li,Yujun Shi,Zecheng He,Zijian He,Jiawei Zhou,Abe Davis,Jialiang Wang*

Main category: cs.CV

TL;DR: ShortCoTI是一种优化框架，通过减少Chain-of-Thought（CoT）序列的冗余，提升图像生成效率，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展用户输入为详细提示（CoT）来增强图像生成的细节和对齐，但可能导致视觉过度思考（冗余和矛盾细节），增加计算成本。

Method: 引入ShortCoTI，一种轻量级优化框架，通过自适应奖励函数鼓励更简洁的CoT序列，并结合强化学习减少提示推理长度。

Result: ShortCoTI将提示推理长度减少54%，同时在多个基准测试中保持或略微提高质量指标。定性分析显示，该方法消除了冗余解释和重复优化，生成既简洁又语义丰富的推理提示。

Conclusion: ShortCoTI通过优化CoT序列的简洁性，在保持图像生成质量的同时显著提高了计算效率。

Abstract: Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

</details>


### [18] [HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](https://arxiv.org/abs/2510.05609)
*Junwen Chen,Peilin Xiong,Keiji Yanai*

Main category: cs.CV

TL;DR: HOI-R1利用语言模型和强化学习，无需额外检测模块，显著提升HOID任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOID方法依赖视觉语言模型的先验知识，框架复杂且难以进一步发展或应用，而多模态语言模型（MLLMs）在HOID任务中的推理能力尚未充分探索。

Method: 提出HOI-R1，探索语言模型在HOID任务中的潜力，引入HOI推理过程和HOID奖励函数。

Result: 在HICO-DET数据集上，HOI-R1的准确率达到基线的2倍，且具有优秀的泛化能力。

Conclusion: HOI-R1通过纯文本方法解决了HOID任务，无需额外检测模块，在HICO-DET数据集上实现了基线2倍的准确率，并展现出强大的泛化能力。

Abstract: Recent Human-object interaction detection (HOID) methods highly require prior
knowledge from VLMs to enhance the interaction recognition capabilities. The
training strategies and model architectures for connecting the knowledge from
VLMs to the HOI instance representations from the object detector are
challenging, and the whole framework is complex for further development or
application. On the other hand, the inherent reasoning abilities of MLLMs on
human-object interaction detection are under-explored. Inspired by the recent
success of training MLLMs with reinforcement learning (RL) methods, we propose
HOI-R1 and first explore the potential of the language model on the HOID task
without any additional detection modules. We introduce an HOI reasoning process
and HOID reward functions to solve the HOID task by pure text. The results on
the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline
with great generalization ability. The source code is available at
https://github.com/cjw2021/HOI-R1.

</details>


### [19] [Efficient Conditional Generation on Scale-based Visual Autoregressive Models](https://arxiv.org/abs/2510.05610)
*Jiaqi Liu,Tao Huang,Chang Xu*

Main category: cs.CV

TL;DR: ECM是一种轻量级控制框架，通过分布式架构和早期中心采样策略，高效实现复杂图像生成控制。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在复杂空间条件生成中依赖预训练模型微调，导致训练成本高昂，需改进效率和控制能力。

Method: 提出了高效控制模型（ECM），包含轻量级控制模块、上下文感知注意力层和共享门控前馈网络，并引入早期中心采样策略和温度调度。

Result: 实验证明ECM在保持高保真和多样化控制的同时，显著提升了训练和推理效率。

Conclusion: ECM框架通过轻量级控制模块和分布式架构，显著提升了训练和推理效率，同时在图像生成中实现了高保真和多样化的控制，超越了现有基线。

Abstract: Recent advances in autoregressive (AR) models have demonstrated their
potential to rival diffusion models in image synthesis. However, for complex
spatially-conditioned generation, current AR approaches rely on fine-tuning the
pre-trained model, leading to significant training costs. In this paper, we
propose the Efficient Control Model (ECM), a plug-and-play framework featuring
a lightweight control module that introduces control signals via a distributed
architecture. This architecture consists of context-aware attention layers that
refine conditional features using real-time generated tokens, and a shared
gated feed-forward network (FFN) designed to maximize the utilization of its
limited capacity and ensure coherent control feature learning. Furthermore,
recognizing the critical role of early-stage generation in determining semantic
structure, we introduce an early-centric sampling strategy that prioritizes
learning early control sequences. This approach reduces computational cost by
lowering the number of training tokens per iteration, while a complementary
temperature scheduling during inference compensates for the resulting
insufficient training of late-stage tokens. Extensive experiments on
scale-based AR models validate that our method achieves high-fidelity and
diverse control over image generation, surpassing existing baselines while
significantly improving both training and inference efficiency.

</details>


### [20] [PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction](https://arxiv.org/abs/2510.05613)
*Ziqiao Meng,Qichao Wang,Zhiyang Dou,Zixing Song,Zhipeng Zhou,Irwin King,Peilin Zhao*

Main category: cs.CV

TL;DR: PointNSP是一种多尺度自回归点云生成框架，克服了传统自回归模型的顺序偏差，实现了SOTA生成质量并超越扩散方法效率。


<details>
  <summary>Details</summary>
Motivation: 自回归模型因强加人工顺序于无序点集而限制了捕捉长距离依赖的能力，导致在对称性、拓扑一致性和大规模几何规律性等全局结构属性上的表现不佳。

Method: 提出PointNSP，一种由粗到细的生成框架，通过下一尺度预测范式在低分辨率保留全局形状结构，并在高分辨率逐步细化几何细节。

Result: 在ShapeNet上的实验表明，PointNSP首次在自回归范式中达到最先进生成质量，并在密集生成（8,192点）中优势更为显著。

Conclusion: PointNSP通过多尺度因子分解的自回归框架，首次在自回归范式中实现了最先进的点云生成质量，并在参数、训练和推理效率上超越了基于扩散的方法，展示了其可扩展性潜力。

Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based
approaches in quality. The performance gap stems from the fact that
autoregressive models impose an artificial ordering on inherently unordered
point sets, forcing shape generation to proceed as a sequence of local
predictions. This sequential bias emphasizes short-range continuity but
undermines the model's capacity to capture long-range dependencies, hindering
its ability to enforce global structural properties such as symmetry,
consistent topology, and large-scale geometric regularities. Inspired by the
level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a
coarse-to-fine generative framework that preserves global shape structure at
low resolutions and progressively refines fine-grained geometry at higher
scales through a next-scale prediction paradigm. This multi-scale factorization
aligns the autoregressive objective with the permutation-invariant nature of
point sets, enabling rich intra-scale interactions while avoiding brittle fixed
orderings. Experiments on ShapeNet show that PointNSP establishes
state-of-the-art (SOTA) generation quality for the first time within the
autoregressive paradigm. In addition, it surpasses strong diffusion-based
baselines in parameter, training, and inference efficiency. Finally, in dense
generation with 8,192 points, PointNSP's advantages become even more
pronounced, underscoring its scalability potential.

</details>


### [21] [TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation](https://arxiv.org/abs/2510.05615)
*Guangrong Wan,Jun liu,Tang tang,Lianghao Shi,Wenjun Luo,TingTing Xu*

Main category: cs.CV

TL;DR: 论文提出了首个多任务泪膜分析数据集TFM，并开发了高效分割模型TF-Net和集成管道TF-Collab，实现了自动化泪膜破裂分析。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注数据集和集成解决方案，自动化的泪膜破裂（TFBU）分割在干眼症诊断中具有挑战性。

Method: 论文提出了TF-Net，采用MobileOne-mini骨干网络和重参数化技术，结合增强的特征金字塔网络，平衡了精度与计算效率。此外，设计了TF-Collab，整合了TFM数据集的三个任务，实现全自动化分析。

Result: 实验证明了TF-Net和TF-Collab的有效性，为眼表诊断提供了实时临床应用的基准性能。

Conclusion: 该论文提出的TF-Net和TF-Collab为眼表诊断提供了高效、自动化的解决方案，为未来研究奠定了基础。

Abstract: Tear film break-up (TFBU) analysis is critical for diagnosing dry eye
syndrome, but automated TFBU segmentation remains challenging due to the lack
of annotated datasets and integrated solutions. This paper introduces the Tear
Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task
tear film analysis, comprising 15 high-resolution videos (totaling 6,247
frames) annotated with three vision tasks: frame-level classification ('clear',
'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area
segmentation. Leveraging this dataset, we first propose TF-Net, a novel and
efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini
backbone with re-parameterization techniques and an enhanced feature pyramid
network to achieve a favorable balance between accuracy and computational
efficiency for real-time clinical applications. We further establish benchmark
performance on the TFM segmentation subset by comparing TF-Net against several
state-of-the-art medical image segmentation models. Furthermore, we design
TF-Collab, a novel integrated real-time pipeline that synergistically leverages
models trained on all three tasks of the TFM dataset. By sequentially
orchestrating frame classification for BUT determination, pupil region
localization for input standardization, and TFBU segmentation, TF-Collab fully
automates the analysis. Experimental results demonstrate the effectiveness of
the proposed TF-Net and TF-Collab, providing a foundation for future research
in ocular surface diagnostics. Our code and the TFM datasets are available at
https://github.com/glory-wan/TF-Net

</details>


### [22] [InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment](https://arxiv.org/abs/2510.05617)
*Ibrahim Salihu Yusuf,Iffanice Houndayi,Rym Oualha,Mohamed Aziz Cherif,Kobby Panford-Quainoo,Arnu Pretorius*

Main category: cs.CV

TL;DR: InstaGeo是一个开源端到端框架，通过自动化数据整理、模型蒸馏和部署，解决了地理空间基础模型部署的限制，显著提升了效率和实用性，同时降低了碳排放。


<details>
  <summary>Details</summary>
Motivation: 当前地理空间基础模型（GFMs）的部署受限于缺乏自动化地理空间数据管道和大规模微调模型的复杂性。InstaGeo旨在解决这些问题。

Method: InstaGeo框架包括：(1) 自动数据整理，将原始影像转化为模型就绪数据集；(2) 任务特定模型蒸馏，生成紧凑高效模型；(3) 无缝部署为交互式网络地图应用。

Result: 使用InstaGeo复现了三项研究的数据集并训练模型，在洪水映射、作物分割和沙漠蝗虫预测任务中表现接近或优于原模型。蒸馏后模型体积缩小8倍，计算量和碳排放显著降低，同时精度损失最小。此外，通过InstaGeo整理的大规模作物分割数据集取得了最先进的60.65% mIoU，较基线提升12个百分点。

Conclusion: InstaGeo通过整合自动数据整理、模型蒸馏和无缝部署，将研究级地理空间基础模型转化为实用的低碳工具，推动了地理空间AI向数据质量和应用驱动创新的转变。

Abstract: Open-access multispectral imagery from missions like Landsat 8-9 and
Sentinel-2 has fueled the development of geospatial foundation models (GFMs)
for humanitarian and environmental applications. Yet, their deployment remains
limited by (i) the absence of automated geospatial data pipelines and (ii) the
large size of fine-tuned models. Existing GFMs lack workflows for processing
raw satellite imagery, and downstream adaptations often retain the full
complexity of the original encoder.
  We present InstaGeo, an open-source, end-to-end framework that addresses
these challenges by integrating: (1) automated data curation to transform raw
imagery into model-ready datasets; (2) task-specific model distillation to
derive compact, compute-efficient models; and (3) seamless deployment as
interactive web-map applications. Using InstaGeo, we reproduced datasets from
three published studies and trained models with marginal mIoU differences of
-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for
desert locust prediction. The distilled models are up to 8x smaller than
standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal
accuracy loss.
  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger
crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp
improvement over prior baselines. Moreover, InstaGeo enables users to progress
from raw data to model deployment within a single working day.
  By unifying data preparation, model compression, and deployment, InstaGeo
transforms research-grade GFMs into practical, low-carbon tools for real-time,
large-scale Earth observation. This approach shifts geospatial AI toward data
quality and application-driven innovation. Source code, datasets, and model
checkpoints are available at:
https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git

</details>


### [23] [Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection](https://arxiv.org/abs/2510.05633)
*Sara Mandelli,Diego Vila-Portela,David Vázquez-Padín,Paolo Bestagini,Fernando Pérez-González*

Main category: cs.CV

TL;DR: 研究挑战了频谱峰值作为合成图像检测关键指标的普遍假设，提出了一种去除峰值的方法和简单线性检测器，发现多数检测器不依赖峰值。


<details>
  <summary>Details</summary>
Motivation: 探讨现有检测器是否真正依赖频谱峰值，以提高检测器的可解释性和信任度。

Method: 提出了一种去除图像频谱峰值的策略，并分析了这一操作对多个检测器的影响；同时引入了一种仅依赖频率峰值的简单线性检测器。

Result: 研究发现大多数检测器并不本质依赖频谱峰值，挑战了该领域的普遍假设。

Conclusion: 大多数检测器并不本质依赖于频谱峰值，挑战了该领域的普遍假设，为更透明和可靠的取证工具铺平了道路。

Abstract: Over the years, the forensics community has proposed several deep
learning-based detectors to mitigate the risks of generative AI. Recently,
frequency-domain artifacts (particularly periodic peaks in the magnitude
spectrum), have received significant attention, as they have been often
considered a strong indicator of synthetic image generation. However,
state-of-the-art detectors are typically used as black-boxes, and it still
remains unclear whether they truly rely on these peaks. This limits their
interpretability and trust. In this work, we conduct a systematic study to
address this question. We propose a strategy to remove spectral peaks from
images and analyze the impact of this operation on several detectors. In
addition, we introduce a simple linear detector that relies exclusively on
frequency peaks, providing a fully interpretable baseline free from the
confounding influence of deep learning. Our findings reveal that most detectors
are not fundamentally dependent on spectral peaks, challenging a widespread
assumption in the field and paving the way for more transparent and reliable
forensic tools.

</details>


### [24] [Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning](https://arxiv.org/abs/2510.05643)
*Shozo Saeki,Minoru Kawahara,Hirohisa Aman*

Main category: cs.CV

TL;DR: 提出了CHEST损失，结合双曲和欧几里得空间的代理损失，显著提升了DML性能。


<details>
  <summary>Details</summary>
Motivation: 尽管双曲空间因其能表示更丰富的结构（如树结构）而吸引人，但在双曲空间中应用代理损失存在困难，而代理损失在大规模数据集上具有较低的训练复杂度。因此，需要一种新的损失函数来结合双曲和欧几里得空间的优势。

Method: 提出了CHEST损失，该损失由双曲和欧几里得空间的代理损失以及基于双曲层次聚类的正则化损失组成。

Result: CHEST损失显著提升了DML的准确性和学习稳定性，并在四个基准数据集上实现了新的最先进性能。

Conclusion: CHEST损失结合了双曲和欧几里得空间的代理损失以及基于双曲层次聚类的正则化损失，显著提升了DML的准确性和学习稳定性，并在四个基准数据集上实现了新的最先进性能。

Abstract: Deep metric learning (DML) aims to learn a neural network mapping data to an
embedding space, which can represent semantic similarity between data points.
Hyperbolic space is attractive for DML since it can represent richer
structures, such as tree structures. DML in hyperbolic space is based on
pair-based loss or unsupervised regularization loss. On the other hand,
supervised proxy-based losses in hyperbolic space have not been reported yet
due to some issues in applying proxy-based losses in a hyperbolic space.
However, proxy-based losses are attractive for large-scale datasets since they
have less training complexity. To address these, this paper proposes the
Combined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is
composed of the proxy-based losses in hyperbolic and Euclidean spaces and the
regularization loss based on hyperbolic hierarchical clustering. We find that
the combination of hyperbolic and Euclidean spaces improves DML accuracy and
learning stability for both spaces. Finally, we evaluate the CHEST loss on four
benchmark datasets, achieving a new state-of-the-art performance.

</details>


### [25] [Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation](https://arxiv.org/abs/2510.05649)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 研究提出两种深度学习框架（AHP-CADNet和课程学习填补框架），用于AHP自动诊断和缺失数据恢复，效果显著。


<details>
  <summary>Details</summary>
Motivation: 早期诊断AHP可减少并发症，但现有临床评估主观且数据不完整，需自动化解决方案。

Method: 研究提出了两种深度学习框架：AHP-CADNet（多级注意力融合框架）和基于课程学习的数据填补框架。

Result: AHP-CADNet在分类任务中准确率达96.9-99.0%，连续变量预测误差低（MAE 0.103-0.199，R2>0.93）。填补框架在所有临床变量中保持高准确率（93.46-99.78%）。

Conclusion: 该研究证实了两种深度学习框架在自动诊断和临床数据缺失恢复中的有效性。

Abstract: Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that
arises from ocular misalignment conditions, such as strabismus, enabling
patients to reduce diplopia and preserve binocular vision. Early diagnosis
minimizes morbidity and secondary complications such as facial asymmetry;
however, current clinical assessments remain largely subjective and are further
complicated by incomplete medical records. This study addresses both challenges
through two complementary deep learning frameworks. First, AHP-CADNet is a
multi-level attention fusion framework for automated diagnosis that integrates
ocular landmarks, head pose features, and structured clinical attributes to
generate interpretable predictions. Second, a curriculum learning-based
imputation framework is designed to mitigate missing data by progressively
leveraging structured variables and unstructured clinical notes to enhance
diagnostic robustness under realistic data conditions. Evaluation on the
PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet
achieves 96.9-99.0 percent accuracy across classification tasks and low
prediction errors for continuous variables, with MAE ranging from 0.103 to
0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy
across all clinical variables (93.46-99.78 percent with PubMedBERT), with
clinical dependency modeling yielding significant improvements (p < 0.001).
These findings confirm the effectiveness of both frameworks for automated
diagnosis and recovery from missing data in clinical settings.

</details>


### [26] [EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario](https://arxiv.org/abs/2510.05650)
*Yiping Ma,Shiyu Hu,Buyuan Zhu,Yipei Wang,Yaxuan Kang,Shiqing Liu,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduVerse是一个支持多代理模拟的教育AI平台，通过分层架构和人机交互界面重现真实课堂动态，验证结果显示了其教学对齐、群体互动和跨会话进化的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的教育AI方法多集中于短期或单代理设置，难以系统性研究课堂复杂性和跨任务复用。EduVerse旨在解决这一挑战，通过多代理模拟空间重现真实课堂的动态性。

Method: EduVerse基于分层的CIE（认知-交互-进化）架构，支持环境、代理和会话的定制化，并通过人机交互界面实现真实用户的参与。

Result: 在中学语文课堂的验证中，EduVerse表现出：（1）教学对齐：模拟IRF率（0.28-0.64）与真实课堂（0.37-0.49）接近；（2）群体互动与角色分化：网络密度（0.27-0.40）；（3）跨会话进化：正向转换率R+平均增加11.7%。

Conclusion: EduVerse成功平衡了真实性、可重复性和可解释性，为教育AI提供了一个可扩展的平台。该系统将开源以促进跨学科研究。

Abstract: Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.

</details>


### [27] [SD-MVSum: Script-Driven Multimodal Video Summarization Method and Datasets](https://arxiv.org/abs/2510.05652)
*Manolis Mylonas,Charalampia Zerva,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: SD-MVSum通过加权跨模态注意力机制提升多模态视频摘要性能，扩展数据集并展现竞争力。


<details>
  <summary>Details</summary>
Motivation: 扩展现有仅考虑视觉内容的脚本驱动视频摘要方法，加入视频语音内容的相关性。

Method: 提出SD-MVSum方法，利用加权跨模态注意力机制建模脚本-视频和脚本-转录之间的依赖关系，显式利用语义相似性。

Result: 实验证明SD-MVSum在脚本驱动和通用视频摘要任务中具有竞争力。

Conclusion: SD-MVSum方法通过新的加权跨模态注意力机制，成功提升了基于脚本的多模态视频摘要性能，并在实验中展现了竞争力。

Abstract: In this work, we extend a recent method for script-driven video
summarization, originally considering just the visual content of the video, to
take into account the relevance of the user-provided script also with the
video's spoken content. In the proposed method, SD-MVSum, the dependence
between each considered pair of data modalities, i.e., script-video and
script-transcript, is modeled using a new weighted cross-modal attention
mechanism. This explicitly exploits the semantic similarity between the paired
modalities in order to promote the parts of the full-length video with the
highest relevance to the user-provided script. Furthermore, we extend two
large-scale datasets for video summarization (S-VideoXum, MrHiSum), to make
them suitable for training and evaluation of script-driven multimodal video
summarization methods. Experimental comparisons document the competitiveness of
our SD-MVSum method against other SOTA approaches for script-driven and generic
video summarization. Our new method and extended datasets are available at:
https://github.com/IDT-ITI/SD-MVSum.

</details>


### [28] [A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer](https://arxiv.org/abs/2510.05657)
*Anwen Lu,Mingxin Liu,Yiping Jiao,Hongyi Gong,Geyang Xu,Jun Chen,Jun Xu*

Main category: cs.CV

TL;DR: ARGUS 是一种新方法，通过捕获肿瘤微环境中的多层次信息来提升肝癌组织学分型的准确性，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肝癌组织学分型对于诊断至关重要，但现有方法未能充分利用全切片图像中的层次金字塔结构、肿瘤微环境和几何表示等关键信息，导致分型性能不佳。

Method: 首先构建微几何特征来表示细粒度细胞级模式，然后设计分层视场对齐模块来建模WSIs中的宏观和中观层次交互，最后通过几何先验引导融合策略将增强的微几何和视场特征融合为联合表示。

Result: 在公共和私有队列上的大量实验表明，ARGUS 在肝癌组织学分型中实现了最先进的性能。

Conclusion: ARGUS 通过捕获肿瘤微环境中的宏观-中观-微观层次信息，显著提升了肝癌组织学分型的性能，为临床实践提供了有效的诊断工具。

Abstract: Primary liver malignancies are widely recognized as the most heterogeneous
and prognostically diverse cancers of the digestive system. Among these,
hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge
as the two principal histological subtypes, demonstrating significantly greater
complexity in tissue morphology and cellular architecture than other common
tumors. The intricate representation of features in Whole Slide Images (WSIs)
encompasses abundant crucial information for liver cancer histological
subtyping, regarding hierarchical pyramid structure, tumor microenvironment
(TME), and geometric representation. However, recent approaches have not
adequately exploited these indispensable effective descriptors, resulting in a
limited understanding of histological representation and suboptimal subtyping
performance. To mitigate these limitations, ARGUS is proposed to advance
histological subtyping in liver cancer by capturing the macro-meso-micro
hierarchical information within the TME. Specifically, we first construct a
micro-geometry feature to represent fine-grained cell-level pattern via a
geometric structure across nuclei, thereby providing a more refined and precise
perspective for delineating pathological images. Then, a Hierarchical
Field-of-Views (FoVs) Alignment module is designed to model macro- and
meso-level hierarchical interactions inherent in WSIs. Finally, the augmented
micro-geometry and FoVs features are fused into a joint representation via
present Geometry Prior Guided Fusion strategy for modeling holistic phenotype
interactions. Extensive experiments on public and private cohorts demonstrate
that our ARGUS achieves state-of-the-art (SOTA) performance in histological
subtyping of liver cancer, which provide an effective diagnostic tool for
primary liver malignancies in clinical practice.

</details>


### [29] [Teleportraits: Training-Free People Insertion into Any Scene](https://arxiv.org/abs/2510.05660)
*Jialu Gao,K J Joseph,Fernando De La Torre*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的扩散模型方法，首次实现了高质量的人物插入场景任务，无需特定训练即可达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将人物插入场景的任务视为独立问题，忽略了它们之间的相互联系，并且依赖训练以获得高性能。本文旨在探索扩散模型是否具备无需训练即可完成该任务的潜力。

Method: 论文提出了一种统一的无需训练流程，结合反转技术和无分类器引导，实现了感知全局的编辑方法。此外，还引入了掩模引导的自注意力机制，确保高质量的人物个性化。

Result: 该方法在多样化的复合场景图像中实现了最先进的结果，并在背景和主体中保持了出色的身份保留。

Conclusion: 该论文提出了一种无需训练的流程，利用预训练的文本到图像扩散模型，首次实现了在无需特定任务训练的情况下，将人物真实地插入背景场景中，并达到了最先进的效果。

Abstract: The task of realistically inserting a human from a reference image into a
background scene is highly challenging, requiring the model to (1) determine
the correct location and poses of the person and (2) perform high-quality
personalization conditioned on the background. Previous approaches often treat
them as separate problems, overlooking their interconnections, and typically
rely on training to achieve high performance. In this work, we introduce a
unified training-free pipeline that leverages pre-trained text-to-image
diffusion models. We show that diffusion models inherently possess the
knowledge to place people in complex scenes without requiring task-specific
training. By combining inversion techniques with classifier-free guidance, our
method achieves affordance-aware global editing, seamlessly inserting people
into scenes. Furthermore, our proposed mask-guided self-attention mechanism
ensures high-quality personalization, preserving the subject's identity,
clothing, and body features from just a single reference image. To the best of
our knowledge, we are the first to perform realistic human insertions into
scenes in a training-free manner and achieve state-of-the-art results in
diverse composite scene images with excellent identity preservation in
backgrounds and subjects.

</details>


### [30] [When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach](https://arxiv.org/abs/2510.05661)
*Daniel Gonzálbez-Biosca,Josep Cabacas-Maso,Carles Ventura,Ismael Benito-Altamirano*

Main category: cs.CV

TL;DR: 本文提出了一种多模态架构，用于自动编辑多摄像机录制的古典音乐会视频，通过分解问题为时间分割和空间选择两个子任务，显著提升了剪辑点和镜头选择的性能。


<details>
  <summary>Details</summary>
Motivation: 自动视频编辑在计算机视觉和多媒体领域仍是一个未被充分探索的任务，尤其是在与视频生成和场景理解日益增长的兴趣相比。

Method: 通过将问题分解为两个关键子任务（何时剪辑和如何剪辑），提出了一种新颖的多模态架构，用于时间分割任务（何时剪辑），并改进了空间选择任务（如何剪辑）的文献。

Result: 模型在检测剪辑点方面表现优于以前的基线，并在视觉镜头选择上提供了有竞争力的结果。

Conclusion: 本文提出的多模态架构在检测剪辑点和视觉镜头选择方面优于现有基线，推动了多模态自动视频编辑的技术进步。

Abstract: Automated video editing remains an underexplored task in the computer vision
and multimedia domains, especially when contrasted with the growing interest in
video generation and scene understanding. In this work, we address the specific
challenge of editing multicamera recordings of classical music concerts by
decomposing the problem into two key sub-tasks: when to cut and how to cut.
Building on recent literature, we propose a novel multimodal architecture for
the temporal segmentation task (when to cut), which integrates log-mel
spectrograms from the audio signals, plus an optional image embedding, and
scalar temporal features through a lightweight convolutional-transformer
pipeline. For the spatial selection task (how to cut), we improve the
literature by updating from old backbones, e.g. ResNet, with a CLIP-based
encoder and constraining distractor selection to segments from the same
concert. Our dataset was constructed following a pseudo-labeling approach, in
which raw video data was automatically clustered into coherent shot segments.
We show that our models outperformed previous baselines in detecting cut points
and provide competitive visual shot selection, advancing the state of the art
in multimodal automated video editing.

</details>


### [31] [Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis](https://arxiv.org/abs/2510.05668)
*M. Torrente,A. Follador,A. Calcante,P. Casati,R. Oberti*

Main category: cs.CV

TL;DR: 研究通过低成本图像系统和创新算法，证明R. solani感染降低种子发芽率和幼苗活力，方法在复杂条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估R. solani接种对Lactuca sativa L.种子发芽和早期发育的影响，利用低成本图像监测系统实现高效分析。

Method: 开发了一种新颖的图像分析流程，结合形态和空间特征，通过时间整合分析种子发芽动态和生长过程。

Result: 方法在幼苗计数和活力评估中表现出高准确性（R²=0.98，RMSE=1.12），尤其在复杂条件下优于传统分割技术。

Conclusion: 该研究验证了低成本成像硬件与先进计算工具结合用于非破坏性、可扩展表型数据获取的可行性，证明了R. solani感染显著降低种子发芽率和早期幼苗活力。

Abstract: The study investigates the effects of R. solani inoculation on the
germination and early development of Lactuca sativa L. seeds using a low-cost,
image-based monitoring system. Multiple cameras were deployed to continuously
capture images of the germination process in both infected and control groups.
The objective was to assess the impact of the pathogen by analyzing germination
dynamics and growth over time. To achieve this, a novel image analysis pipeline
was developed. The algorithm integrates both morphological and spatial features
to identify and quantify individual seedlings, even under complex conditions
where traditional image analyses fails. A key innovation of the method lies in
its temporal integration: each analysis step considers not only the current
status but also their developmental across prior time points. This approach
enables robust discrimination of individual seedlings, especially when
overlapping leaves significantly hinder object separation. The method
demonstrated high accuracy in seedling counting and vigor assessment, even in
challenging scenarios characterized by dense and intertwined growth. Results
confirm that R. solani infection significantly reduces germination rates and
early seedling vigor. The study also validates the feasibility of combining
low-cost imaging hardware with advanced computational tools to obtain
phenotyping data in a non-destructive and scalable manner. The temporal
integration enabled accurate quantification of germinated seeds and precise
determination of seedling emergence timing. This approach proved particularly
effective in later stages of the experiment, where conventional segmentation
techniques failed due to overlapping or intertwined seedlings, making accurate
counting. The method achieved a coefficient of determination of 0.98 and a root
mean square error (RMSE) of 1.12, demonstrating its robustness and reliability.

</details>


### [32] [Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension](https://arxiv.org/abs/2510.05674)
*Jike Zhong,Yuxiang Lai,Xiaofeng Yang,Konstantinos Psounis*

Main category: cs.CV

TL;DR: 该论文提出通过对象级编码弥补视觉模型在语义和上下文学习上的不足，实验证明其有效性，并为未来视觉编码器开发提供了方向。


<details>
  <summary>Details</summary>
Motivation: 当前视觉变换器（ViT）训练方案缺乏语义和上下文指导，导致视觉模型在推理和上下文学习方面落后于语言模型。

Method: 通过掩码图像建模（MIM）框架，将掩码应用于视觉对象而非随机补丁，验证了对象级表示的有效性。

Result: 定性和定量评估表明，对象级表示有助于学习真实世界的分布，而无需依赖像素平均捷径。多模态LLM在视觉问答任务上的进一步评估展示了其强大的推理和上下文理解能力。

Conclusion: 该研究强调了对象级编码的有效性，并为开发更强的视觉编码器和分词器提供了可行的方向。

Abstract: Recent advances in language modeling have witnessed the rise of highly
desirable emergent capabilities, such as reasoning and in-context learning.
However, vision models have yet to exhibit comparable progress in these areas.
In this paper, we argue that this gap could stem from the lack of semantic and
contextual guidance in current vision transformer (ViT) training schemes, and
such a gap can be narrowed through the design of a semantic-grounded objective.
Specifically, we notice that individual words in natural language are
inherently semantic, and modeling directly on word tokens naturally learns a
realistic distribution. In contrast, ViTs rely on spatial patchification, which
inevitably lacks semantic information. To bridge this gap, we propose to
directly model "object" as the visual equivalence of "word," pushing the model
to learn the global context and semantics among visual elements. We investigate
our hypotheses via masked image modeling (MIM), a framework where our approach
can be readily tested by applying masks to visual objects rather than random
patches. Considerable evidence from qualitative and quantitative evaluations
reveals a key finding: object-level representation alone helps to learn a
real-world distribution, whereas pixel-averaging shortcuts are often learned
without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual
question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning
and contextual understanding gained with this simple objective. We hope our
study highlights the effectiveness of object-level encoding and provides a
plausible direction for developing stronger vision encoders and tokenizers.
Code and model will be publicly released. Keywords: Semantic Visual Tokenizer,
Vision Reasoning, In-context Learning, Multimodal Reasoning

</details>


### [33] [AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models](https://arxiv.org/abs/2510.05715)
*Shihao Zhu,Bohan Cao,Ziheng Ouyang,Zhen Li,Peng-Tao Jiang,Qibin Hou*

Main category: cs.CV

TL;DR: AgeBooth是一种无需大量年龄标注数据的年龄特定微调方法，通过年龄条件提示混合和LoRA融合策略提升身份个性化模型的年龄控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成身份一致图像时难以精确控制年龄且需昂贵配对数据，AgeBooth旨在解决这一问题。

Method: AgeBooth采用年龄条件提示混合和基于SVDMix的年龄特定LoRA融合策略，利用线性老化特性生成中间年龄肖像。

Result: AgeBooth能从单一参考图像生成不同年龄段的真实且身份一致的人脸图像，实验显示其在年龄控制和视觉质量上优于现有方法。

Conclusion: AgeBooth通过创新的年龄条件提示混合和年龄特定LoRA融合策略，有效提升了身份个性化模型的年龄控制能力，无需依赖大量年龄标注数据。实验证明，AgeBooth在年龄控制和视觉质量上优于现有编辑方法。

Abstract: Recent diffusion model research focuses on generating identity-consistent
images from a reference photo, but they struggle to accurately control age
while preserving identity, and fine-tuning such models often requires costly
paired images across ages. In this paper, we propose AgeBooth, a novel
age-specific finetuning approach that can effectively enhance the age control
capability of adapterbased identity personalization models without the need for
expensive age-varied datasets. To reduce dependence on a large amount of
age-labeled data, we exploit the linear nature of aging by introducing
age-conditioned prompt blending and an age-specific LoRA fusion strategy that
leverages SVDMix, a matrix fusion technique. These techniques enable
high-quality generation of intermediate-age portraits. Our AgeBooth produces
realistic and identity-consistent face images across different ages from a
single reference image. Experiments show that AgeBooth achieves superior age
control and visual quality compared to previous state-of-the-art editing-based
methods.

</details>


### [34] [Dropping the D: RGB-D SLAM Without the Depth Sensor](https://arxiv.org/abs/2510.06216)
*Mert Kiray,Alican Karaomer,Benjamin Busam*

Main category: cs.CV

TL;DR: DropD-SLAM通过预训练视觉模块替代深度传感器，实现RGB-D级别精度，性能优于现有方法，且成本更低。


<details>
  <summary>Details</summary>
Motivation: 为了在不依赖深度传感器的情况下实现RGB-D级别的精度，简化并降低SLAM系统的成本。

Method: DropD-SLAM使用三个预训练视觉模块替代深度传感器输入：单目度量深度估计器、学习关键点检测器和实例分割网络。动态对象通过膨胀实例掩码抑制，静态关键点分配预测深度值并反向投影到3D形成度量尺度特征，由未修改的RGB-D SLAM后端处理。

Result: 在TUM RGB-D基准测试中，DropD-SLAM在静态序列上达到7.4 cm的平均ATE，动态序列上达到1.8 cm，匹配或超越最先进的RGB-D方法，同时在单GPU上以22 FPS运行。

Conclusion: 现代预训练视觉模型可以替代主动深度传感器，作为可靠、实时的度量尺度来源，标志着SLAM系统向更简单、更经济高效的方向迈出了一步。

Abstract: We present DropD-SLAM, a real-time monocular SLAM system that achieves
RGB-D-level accuracy without relying on depth sensors. The system replaces
active depth input with three pretrained vision modules: a monocular metric
depth estimator, a learned keypoint detector, and an instance segmentation
network. Dynamic objects are suppressed using dilated instance masks, while
static keypoints are assigned predicted depth values and backprojected into 3D
to form metrically scaled features. These are processed by an unmodified RGB-D
SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM
attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,
matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS
on a single GPU. These results suggest that modern pretrained vision models can
replace active depth sensors as reliable, real-time sources of metric scale,
marking a step toward simpler and more cost-effective SLAM systems.

</details>


### [35] [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/abs/2510.05722)
*Jiaojiao Ye,Jiaxing Zhong,Qian Xie,Yuzhou Zhou,Niki Trigoni,Andrew Markham*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的合成图像生成方法，结合ControlNet和VLMs，显著提升一次性语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强技术在操作高级语义属性（如材质和纹理）时面临挑战，而现有基于扩散模型的方法要么计算成本高，要么性能不佳。

Method: 通过集成预训练的ControlNet和视觉语言模型（VLMs），结合多路提示生成器、掩码生成器和高质量图像选择模块，生成合成图像及其像素级标签。

Result: 在PASCAL-5i和COCO-20i数据集上的实验结果表明，该方法在一次性语义分割任务中表现优异，优于同期工作。

Conclusion: 该论文提出了一种无需训练的新流程，结合预训练的ControlNet和视觉语言模型（VLMs），生成带有像素级标签的合成图像，显著提升了下游任务的性能。

Abstract: Generating enough and diverse data through augmentation offers an efficient
solution to the time-consuming and labour-intensive process of collecting and
annotating pixel-wise images. Traditional data augmentation techniques often
face challenges in manipulating high-level semantic attributes, such as
materials and textures. In contrast, diffusion models offer a robust
alternative, by effectively utilizing text-to-image or image-to-image
transformation. However, existing diffusion-based methods are either
computationally expensive or compromise on performance. To address this issue,
we introduce a novel training-free pipeline that integrates pretrained
ControlNet and Vision-Language Models (VLMs) to generate synthetic images
paired with pixel-level labels. This approach eliminates the need for manual
annotations and significantly improves downstream tasks. To improve the
fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and
High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i
present promising performance and outperform concurrent work for one-shot
semantic segmentation.

</details>


### [36] [Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect](https://arxiv.org/abs/2510.05740)
*Amirtaha Amanzadi,Zahra Dehghanian,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 论文提出FusionDetect方法，结合CLIP和Dinov2模型，显著提升合成图像检测性能，并推出OmniGen Benchmark作为新评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前合成图像检测的研究主要集中在跨生成器泛化上，而忽略了跨视觉域泛化的重要性。作者认为这一视角过于局限，因此提出了更全面的评估方法和检测器。

Method: FusionDetect利用CLIP和Dinov2两种冻结的基础模型，提取互补特征，构建了一个适应性强的特征空间，以应对生成器内容和设计的变化。

Result: FusionDetect在现有基准测试中比最接近的竞争对手准确率高出3.87%，平均精度高出6.13%，在OmniGen Benchmark上准确率提高了4.48%，并且对常见的图像扰动表现出极强的鲁棒性。

Conclusion: 论文提出了一种新的检测方法FusionDetect，通过结合CLIP和Dinov2两种基础模型的特征，创建了一个适应性强且性能优越的合成图像检测器。同时，作者还推出了OmniGen Benchmark，为未来的研究提供了更全面的评估框架。

Abstract: The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect

</details>


### [37] [ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving](https://arxiv.org/abs/2510.05752)
*Yongxuan Lyu,Guangfeng Jiang,Hongsi Liu,Jun Liu*

Main category: cs.CV

TL;DR: ALISE是一种无监督LiDAR实例分割框架，通过VFMs和优化模块实现高性能，超越部分有监督方法。


<details>
  <summary>Details</summary>
Motivation: 减少或消除LiDAR点云实例分割中对人工标注的依赖，降低成本和耗时。

Method: ALISE利用视觉基础模型（VFMs）生成初始伪标签，并通过时空投票模块和两种语义监督（2D先验损失和原型对比损失）进行优化。

Result: ALISE在无监督3D实例分割中达到新SOTA，性能超过使用2D GT框监督的MWSIS方法2.53%（50.95% vs. 48.42%）。

Conclusion: ALISE框架在完全无监督的情况下实现了LiDAR实例分割，显著提升了性能，甚至超过了部分有监督方法。

Abstract: The manual annotation of outdoor LiDAR point clouds for instance segmentation
is extremely costly and time-consuming. Current methods attempt to reduce this
burden but still rely on some form of human labeling. To completely eliminate
this dependency, we introduce ALISE, a novel framework that performs LiDAR
instance segmentation without any annotations. The central challenge is to
generate high-quality pseudo-labels in a fully unsupervised manner. Our
approach starts by employing Vision Foundation Models (VFMs), guided by text
and images, to produce initial pseudo-labels. We then refine these labels
through a dedicated spatio-temporal voting module, which combines 2D and 3D
semantics for both offline and online optimization. To achieve superior feature
learning, we further introduce two forms of semantic supervision: a set of 2D
prior-based losses that inject visual knowledge into the 3D network, and a
novel prototype-based contrastive loss that builds a discriminative feature
space by exploiting 3D semantic consistency. This comprehensive design results
in significant performance gains, establishing a new state-of-the-art for
unsupervised 3D instance segmentation. Remarkably, our approach even
outperforms MWSIS, a method that operates with supervision from ground-truth
(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).

</details>


### [38] [OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search](https://arxiv.org/abs/2510.05759)
*Zexin Zheng,Huangyu Dai,Lingtao Mao,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.CV

TL;DR: OneVision是一种端到端生成框架，通过VRQ对齐多视图表示，结合动态剪枝和语义对齐，显著提升效率和在线指标。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段级联架构（MCA）在查询图像与优化目标之间存在多视图表示差异，难以在用户体验和转化率上实现帕累托最优。

Method: 提出了基于VRQ的OneVision框架，采用多阶段语义对齐方案，结合动态剪枝技术。

Result: 离线评估中与在线MCA性能相当，推理效率提升21%；A/B测试中实现显著在线提升：点击率+2.15%，转化率+2.27%，订单量+3.12%。

Conclusion: OneVision通过生成式架构统一了检索和个性化推荐，简化了服务路径，并在效率和用户体验上实现了显著提升。

Abstract: Traditional vision search, similar to search and recommendation systems,
follows the multi-stage cascading architecture (MCA) paradigm to balance
efficiency and conversion. Specifically, the query image undergoes feature
extraction, recall, pre-ranking, and ranking stages, ultimately presenting the
user with semantically similar products that meet their preferences. This
multi-view representation discrepancy of the same object in the query and the
optimization objective collide across these stages, making it difficult to
achieve Pareto optimality in both user experience and conversion. In this
paper, an end-to-end generative framework, OneVision, is proposed to address
these problems. OneVision builds on VRQ, a vision-aligned residual quantization
encoding, which can align the vastly different representations of an object
across multiple viewpoints while preserving the distinctive features of each
product as much as possible. Then a multi-stage semantic alignment scheme is
adopted to maintain strong visual similarity priors while effectively
incorporating user-specific information for personalized preference generation.
In offline evaluations, OneVision performs on par with online MCA, while
improving inference efficiency by 21% through dynamic pruning. In A/B tests, it
achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and
+3.12% order volume. These results demonstrate that a semantic ID centric,
generative architecture can unify retrieval and personalization while
simplifying the serving pathway.

</details>


### [39] [A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data](https://arxiv.org/abs/2510.05760)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出一种结合弱标签源与可靠数据集的训练策略，通过转移矩阵加权标签，提升深度网络在遥感场景分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 深度网络在遥感图像场景分类中表现优异，但需要大量训练样本且对标签错误敏感。可靠标签获取成本高且数量有限，而不可靠标签源（如过时数字地图）较多。旨在利用这些不可靠标签源提升模型性能。

Method: 提出一种结合单个或多个弱标签源与小规模可靠数据集的方法，并利用描述各源错误统计的转移矩阵嵌入标签，在训练过程中根据来源权重调整标签影响。

Result: 实验证明该方法能有效利用不可靠标签源，提升模型鲁棒性和分类能力。

Conclusion: 该方法通过利用多个不可靠标签源与小规模可靠数据集结合，并通过训练策略考虑各标签源的可靠性，有效提升了深度网络在遥感图像场景分类中的性能。

Abstract: Deep learning has gained broad interest in remote sensing image scene
classification thanks to the effectiveness of deep neural networks in
extracting the semantics from complex data. However, deep networks require
large amounts of training samples to obtain good generalization capabilities
and are sensitive to errors in the training labels. This is a problem in remote
sensing since highly reliable labels can be obtained at high costs and in
limited amount. However, many sources of less reliable labeled data are
available, e.g., obsolete digital maps. In order to train deep networks with
larger datasets, we propose both the combination of single or multiple weak
sources of labeled data with a small but reliable dataset to generate
multisource labeled datasets and a novel training strategy where the
reliability of each source is taken in consideration. This is done by
exploiting the transition matrices describing the statistics of the errors of
each source. The transition matrices are embedded into the labels and used
during the training process to weigh each label according to the related
source. The proposed method acts as a weighting scheme at gradient level, where
each instance contributes with different weights to the optimization of
different classes. The effectiveness of the proposed method is validated by
experiments on different datasets. The results proved the robustness and
capability of leveraging on unreliable source of labels of the proposed method.

</details>


### [40] [Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection](https://arxiv.org/abs/2510.05782)
*I. M. De la Jara,C. Rodriguez-Opazo,D. Teney,D. Ranasinghe,E. Abbasnejad*

Main category: cs.CV

TL;DR: 研究揭示预训练模型中间层对OOD检测的重要性，提出自动选择互补层的熵准则，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 挑战现有方法仅依赖预训练模型最后一层表示进行OOD检测的局限性，探索中间层在检测分布偏移中的潜力。

Method: 引入了一种基于熵的准则，在无需训练和OOD数据的情况下自动识别最具互补信息的中间层。

Result: 在远OOD和近OOD基准测试中，相比最先进的无需训练方法，检测准确率分别提升了10%和7%以上。

Conclusion: 该研究揭示了预训练模型中间层在检测分布偏移中的潜力，并通过选择性整合这些层提升了OOD检测的准确性，为OOD检测研究开辟了新方向。

Abstract: Out-of-distribution (OOD) detection is essential for reliably deploying
machine learning models in the wild. Yet, most methods treat large pre-trained
models as monolithic encoders and rely solely on their final-layer
representations for detection. We challenge this wisdom. We reveal the
\textit{intermediate layers} of pre-trained models, shaped by residual
connections that subtly transform input projections, \textit{can} encode
\textit{surprisingly rich and diverse signals} for detecting distributional
shifts. Importantly, to exploit latent representation diversity across layers,
we introduce an entropy-based criterion to \textit{automatically} identify
layers offering the most complementary information in a training-free setting
-- \textit{without access to OOD data}. We show that selectively incorporating
these intermediate representations can increase the accuracy of OOD detection
by up to \textbf{$10\%$} in far-OOD and over \textbf{$7\%$} in near-OOD
benchmarks compared to state-of-the-art training-free methods across various
model architectures and training objectives. Our findings reveal a new avenue
for OOD detection research and uncover the impact of various training
objectives and model architectures on confidence-based OOD detection methods.

</details>


### [41] [Rasterized Steered Mixture of Experts for Efficient 2D Image Regression](https://arxiv.org/abs/2510.05814)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Mårten Sjöström*

Main category: cs.CV

TL;DR: 提出了一种光栅化优化策略，结合Steered Mixture of Experts的边缘感知能力，显著提升了计算效率，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: Steered Mixture of Experts回归框架在图像重建、压缩、去噪和超分辨率方面表现出色，但高计算成本限制了其实际应用。

Method: 引入了一种基于光栅化的优化策略，结合光栅化高斯核渲染的高效性和Steered Mixture of Experts的边缘感知门控机制。

Result: 该方法显著加快了参数更新速度，提高了内存效率，同时保持了模型的稀疏性和重建质量，支持原生超分辨率和图像去噪等应用。

Conclusion: 通过结合光栅化优化和边缘感知的Steered Mixture of Experts结构，该方法在二维图像处理任务中实现了计算效率和重建质量的新平衡。

Abstract: The Steered Mixture of Experts regression framework has demonstrated strong
performance in image reconstruction, compression, denoising, and
super-resolution. However, its high computational cost limits practical
applications. This work introduces a rasterization-based optimization strategy
that combines the efficiency of rasterized Gaussian kernel rendering with the
edge-aware gating mechanism of the Steered Mixture of Experts. The proposed
method is designed to accelerate two-dimensional image regression while
maintaining the model's inherent sparsity and reconstruction quality. By
replacing global iterative optimization with a rasterized formulation, the
method achieves significantly faster parameter updates and more
memory-efficient model representations. In addition, the proposed framework
supports applications such as native super-resolution and image denoising,
which are not directly achievable with standard rasterized Gaussian kernel
approaches. The combination of fast rasterized optimization with the edge-aware
structure of the Steered Mixture of Experts provides a new balance between
computational efficiency and reconstruction fidelity for two-dimensional image
processing tasks.

</details>


### [42] [Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images](https://arxiv.org/abs/2510.05819)
*Sven Koehler,Sarah Kaye Mueller,Jonathan Kiekenap,Gerald Greil,Tarique Hussain,Samir Sarikouch,Florian André,Norbert Frey,Sandy Engelhardt*

Main category: cs.CV

TL;DR: 提出了一种自监督深度学习方法，用于在CMR中检测关键帧，显著提高了检测准确性，并支持跨患者的时间对齐分析。


<details>
  <summary>Details</summary>
Motivation: 心血管磁共振（CMR）是评估心脏功能的金标准，但个体心脏周期复杂化了自动时间比较或子相位分析。准确的心脏关键帧检测可以解决这一问题。

Method: 该方法通过从图像中导出密集可变形配准场，计算1D运动描述符，并基于特征曲线使用简单规则集确定关键帧。

Result: 自监督方法在SAX和4CH视图中分别提高了ED和ES检测准确率30%-51%和11%-47%，且能够检测心脏周期中的三个额外关键帧，平均cFD低于1.31帧（SAX）和1.73帧（LAX）。

Conclusion: 该研究提出了一种自监督深度学习方法，用于在短轴（SAX）和四腔长轴（4CH）电影CMR中检测五个关键帧，显著提高了关键帧检测的准确性，并支持跨患者和患者内的心脏动态时间对齐分析。

Abstract: Cardiovascular magnetic resonance (CMR) is the gold standard for assessing
cardiac function, but individual cardiac cycles complicate automatic temporal
comparison or sub-phase analysis. Accurate cardiac keyframe detection can
eliminate this problem. However, automatic methods solely derive end-systole
(ES) and end-diastole (ED) frames from left ventricular volume curves, which do
not provide a deeper insight into myocardial motion. We propose a
self-supervised deep learning method detecting five keyframes in short-axis
(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable
registration fields are derived from the images and used to compute a 1D motion
descriptor, which provides valuable insights into global cardiac contraction
and relaxation patterns. From these characteristic curves, keyframes are
determined using a simple set of rules. The method was independently evaluated
for both views using three public, multicentre, multidisease datasets. M&Ms-2
(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC
(n=100) datasets for repeatability control. Furthermore, generalisability to
patients with rare congenital heart defects was tested using the German
Competence Network (GCN) dataset. Our self-supervised approach achieved
improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED
and ES, as measured by cyclic frame difference (cFD), compared with the
volume-based approach. We can detect ED and ES, as well as three additional
keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for
SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and
intra-patient analysis of cardiac dynamics, irrespective of cycle or phase
lengths. GitHub repository:
https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git

</details>


### [43] [Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow](https://arxiv.org/abs/2510.05836)
*Ruyang Liu,Shangkun Sun,Haoran Tang,Ge Li,Wei Gao*

Main category: cs.CV

TL;DR: Flow4Agent通过光流运动先验和时空冗余优化，显著提升长视频理解性能，尤其在小时级别任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因时空内容冗余和多模态大语言模型（MLLMs）上下文长度限制而具有挑战性。现有方法依赖CLIP模型作为先验，本文提出引入光流运动先验以优化这一过程。

Method: Flow4Agent框架包含两个核心模块：Temporal Granularity Optimization（TGO）通过粗粒度光流先验分组相似视觉内容，并利用语义先验过滤无关场景信息；Motion Token Pruning（MTP）通过细粒度光流信息修剪高冗余视频token。

Result: Flow4Agent在多个视频MLLM基准测试中表现优异，Video-MME达64.7%，MLVU达71.4%，LongVideoBench达60.4%。

Conclusion: Flow4Agent通过引入运动先验和优化时空冗余，显著提升了长视频理解任务的表现，尤其在小时级别视频任务上表现优异。

Abstract: Long-form video understanding has always been a challenging problem due to
the significant redundancy in both temporal and spatial contents. This
challenge is further exacerbated by the limited context length of Multimodal
Large Language Models (MLLMs). To address this issue, many previous works have
attempted to extract key video information, where the "key" is typically
semantic-aware and heavily dependent on the CLIP model as prior. In this paper,
we propose Flow4Agent, a novel framework that pioneeringly incorporates motion
priors from optical flow to facilitate LLM-based long video understanding.
Flow4Agent mitigates the redundancy in long videos at both temporal and spatial
levels through two core modules: Temporal Granularity Optimization (TGO)
adaptively refines framelevel hierarchies, which first leverages coarse flow
priors to group similar visual contents and then applies semantic priors to
filter out highly irrelevant scene information. Motion Token Pruning (MTP)
further refines the intra-frame visual representations, pruning high-redundancy
video tokens using fine-grained optical flow information. Extensive experiments
demonstrate that our Flow4Agent outperforms existing methods across a wide
range of video MLLM benchmarks, especially for hour-level video understanding
tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.

</details>


### [44] [acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows](https://arxiv.org/abs/2510.05886)
*Johannes Seiffarth,Keitaro Kasahara,Michelle Bund,Benita Lückel,Richard D. Paul,Mathias Pesch,Lennart Witting,Michael Bott,Dietrich Kohlheyer,Katharina Nöh*

Main category: cs.CV

TL;DR: 该论文提出了一种名为acia-workflows的平台，通过深度学习和模块化设计，解决了高吞吐量活细胞成像数据的自动化分析问题，为生物研究提供了高效工具。


<details>
  <summary>Details</summary>
Motivation: 高吞吐量活细胞成像技术产生大量数据，传统分析方法难以处理。研究旨在通过开发自动化、可访问的工作流程，提升数据分析效率和可重复性。

Method: 研究结合了Python库（acia）、Jupyter Notebook工作流程和实际应用案例，提供了八种深度学习的细胞分割和追踪方法，支持模块化设计和可扩展的分析流程。

Result: 成功开发了acia-workflows平台，包含十多个开源应用工作流程，支持从生长率比较到单细胞动态响应的精确分析。

Conclusion: 该研究提出了acia-workflows平台，通过整合自动化图像分析、可复现的工作流程和实际应用案例，解决了高吞吐量活细胞成像数据的分析挑战，为生物研究提供了强大且易用的工具。

Abstract: Live-cell imaging (LCI) technology enables the detailed spatio-temporal
characterization of living cells at the single-cell level, which is critical
for advancing research in the life sciences, from biomedical applications to
bioprocessing. High-throughput setups with tens to hundreds of parallel cell
cultivations offer the potential for robust and reproducible insights. However,
these insights are obscured by the large amount of LCI data recorded per
experiment. Recent advances in state-of-the-art deep learning methods for cell
segmentation and tracking now enable the automated analysis of such large data
volumes, offering unprecedented opportunities to systematically study
single-cell dynamics. The next key challenge lies in integrating these powerful
tools into accessible, flexible, and user-friendly workflows that support
routine application in biological research. In this work, we present
acia-workflows, a platform that combines three key components: (1) the
Automated live-Cell Imaging Analysis (acia) Python library, which supports the
modular design of image analysis pipelines offering eight deep learning
segmentation and tracking approaches; (2) workflows that assemble the image
analysis pipeline, its software dependencies, documentation, and visualizations
into a single Jupyter Notebook, leading to accessible, reproducible and
scalable analysis workflows; and (3) a collection of application workflows
showcasing the analysis and customization capabilities in real-world
applications. Specifically, we present three workflows to investigate various
types of microfluidic LCI experiments ranging from growth rate comparisons to
precise, minute-resolution quantitative analyses of individual dynamic cells
responses to changing oxygen conditions. Our collection of more than ten
application workflows is open source and publicly available at
https://github.com/JuBiotech/acia-workflows.

</details>


### [45] [BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data](https://arxiv.org/abs/2510.05888)
*Arefin Ittesafun Abian,Debopom Sutradhar,Md Rafi Ur Rashid,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Kheng Cher Yeo,Sami Azam*

Main category: cs.CV

TL;DR: BioAutoML-NAS 通过多模态数据和 NAS 实现高效昆虫分类，显著优于现有方法，支持可持续农业。


<details>
  <summary>Details</summary>
Motivation: 昆虫分类对农业管理和生态研究至关重要，但由于昆虫特征复杂、类别不平衡和大规模数据集，该任务仍具挑战性。

Method: BioAutoML-NAS 结合多模态数据（图像和元数据），应用神经架构搜索（NAS）自动学习最佳操作，并通过多模态融合模块整合图像嵌入与元数据。采用交替双级优化训练策略更新网络权重和架构参数，同时通过零操作移除不重要连接，生成高效稀疏架构。

Result: 在 BIOSCAN-5M 数据集上，BioAutoML-NAS 达到 96.81% 准确率、97.46% 精确率、96.81% 召回率和 97.05% F1 分数，显著优于现有方法。在 Insects-1M 数据集上表现同样优异。

Conclusion: BioAutoML-NAS 提供了准确、可靠的昆虫分类方法，支持现代可持续农业。

Abstract: Insect classification is important for agricultural management and ecological
research, as it directly affects crop health and production. However, this task
remains challenging due to the complex characteristics of insects, class
imbalance, and large-scale datasets. To address these issues, we propose
BioAutoML-NAS, the first BioAutoML model using multimodal data, including
images, and metadata, which applies neural architecture search (NAS) for images
to automatically learn the best operations for each connection within each
cell. Multiple cells are stacked to form the full network, each extracting
detailed image feature representations. A multimodal fusion module combines
image embeddings with metadata, allowing the model to use both visual and
categorical biological information to classify insects. An alternating bi-level
optimization training strategy jointly updates network weights and architecture
parameters, while zero operations remove less important connections, producing
sparse, efficient, and high-performing architectures. Extensive evaluation on
the BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%
accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming
state-of-the-art transfer learning, transformer, AutoML, and NAS methods by
approximately 16%, 10%, and 8% respectively. Further validation on the
Insects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,
and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides
accurate, confident insect classification that supports modern sustainable
farming.

</details>


### [46] [$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://arxiv.org/abs/2510.05891)
*Yanran Zhang,Bingyao Yu,Yu Zheng,Wenzhao Zheng,Yueqi Duan,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: D$^3$QE 是一种针对自回归生成图像的检测方法，通过离散分布差异和量化误差特征实现高准确性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像生成领域带来了新的挑战，其独特的离散令牌预测方式与传统方法不同，需要新的检测手段。

Method: 提出了一种离散分布差异感知量化误差（D$^3$QE）方法，利用动态码本频率统计和注意力机制，融合语义特征和量化误差潜在特征。

Result: 实验表明，D$^3$QE 在主流视觉自回归模型上具有优越的检测准确性和泛化能力。

Conclusion: D$^3$QE 方法在检测自回归生成的图像方面表现出色，具有高准确性和强泛化能力，且对真实世界扰动具有鲁棒性。

Abstract: The emergence of visual autoregressive (AR) models has revolutionized image
generation while presenting new challenges for synthetic image detection.
Unlike previous GAN or diffusion-based methods, AR models generate images
through discrete token prediction, exhibiting both marked improvements in image
synthesis quality and unique characteristics in their vector-quantized
representations. In this paper, we propose to leverage Discrete Distribution
Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated
image detection that exploits the distinctive patterns and the frequency
distribution bias of the codebook existing in real and fake images. We
introduce a discrete distribution discrepancy-aware transformer that integrates
dynamic codebook frequency statistics into its attention mechanism, fusing
semantic features and quantization error latent. To evaluate our method, we
construct a comprehensive dataset termed ARForensics covering 7 mainstream
visual AR models. Experiments demonstrate superior detection accuracy and
strong generalization of D$^3$QE across different AR models, with robustness to
real-world perturbations. Code is available at
\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.

</details>


### [47] [Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning](https://arxiv.org/abs/2510.05899)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Jinyan Zhou,Jianfeng Cao,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: WS-ICL 通过弱提示替代密集标签，减少了医学图像分割的标注成本，性能与常规 ICL 相当。


<details>
  <summary>Details</summary>
Motivation: 解决当前通用医学图像分割模型（如交互式和上下文学习模型）需要大量标注的问题，尤其是交互式模型需要重复用户提示，而 ICL 依赖密集的像素级标签。

Method: 提出了一种新的 ICL 范式，即弱监督上下文学习（WS-ICL），利用弱提示（如边界框或点）代替密集标签作为上下文。

Result: 在三个基准测试中，WS-ICL 的性能与常规 ICL 模型相当，但标注成本显著降低，且在交互式范式下也表现出高度竞争力。

Conclusion: WS-ICL 是一种高效且统一的通用模型，用于医学图像分割，显著降低了标注成本，同时保持了与常规 ICL 模型相当的性能。

Abstract: Universal models for medical image segmentation, such as interactive and
in-context learning (ICL) models, offer strong generalization but require
extensive annotations. Interactive models need repeated user prompts for each
image, while ICL relies on dense, pixel-level labels. To address this, we
propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that
leverages weak prompts (e.g., bounding boxes or points) instead of dense labels
for context. This approach significantly reduces annotation effort by
eliminating the need for fine-grained masks and repeated user prompting for all
images. We evaluated the proposed WS-ICL model on three held-out benchmarks.
Experimental results demonstrate that WS-ICL achieves performance comparable to
regular ICL models at a significantly lower annotation cost. In addition,
WS-ICL is highly competitive even under the interactive paradigm. These
findings establish WS-ICL as a promising step toward more efficient and unified
universal models for medical image segmentation. Our code and model are
publicly available at https://github.com/jiesihu/Weak-ICL.

</details>


### [48] [Kaputt: A Large-Scale Dataset for Visual Defect Detection](https://arxiv.org/abs/2510.05903)
*Sebastian Höfer,Dorian Henning,Artemij Amiranashvili,Douglas Morrison,Mariliza Tzes,Ingmar Posner,Marc Matvienko,Alessandro Rennola,Anton Milan*

Main category: cs.CV

TL;DR: 论文提出了一个大规模物流缺陷检测数据集，填补了零售物流场景的空白，并验证了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测研究主要集中在制造场景，而零售物流中的异常检测面临对象姿态和外观多样性的新挑战，现有方法在此场景中表现不佳。

Method: 引入了一个包含超过230,000张图像（29,000多个缺陷实例）的新数据集，规模是MVTec-AD的40倍，包含超过48,000个不同对象。并对多种最先进的异常检测方法进行了广泛评估。

Result: 现有方法在新数据集上的表现不超过56.96% AUROC，定性分析也表明它们在姿态和外观变化大的情况下难以有效利用正常样本。

Conclusion: 该论文提出了一个大规模物流缺陷检测数据集，填补了现有基准在零售物流场景中的不足，并验证了现有先进方法在这一新场景中的局限性，为未来研究设定了新的基准。

Abstract: We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.

</details>


### [49] [Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging](https://arxiv.org/abs/2510.05971)
*Ron Keuth,Paul Kaftan,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: Metapoque在医学影像中的首次全面研究显示，分类任务适合低复杂度token mixer，分割任务依赖卷积token mixer的局部偏置，分组卷积最优。代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前MetaFormer在医学影像中的应用较少IODINE，且不同token mixer的比较研究稀缺，可能忽略了更合适的设计选择。

Method: 系统分析了基于池化、卷积和注意力的token mixer在MetaFormer架构中的表现，覆盖了八种不同模态的医学数据集，并考察了从自然图像预训练权重的迁移 метод.

Result: 分类任务中，低复杂度token mixer表现足够；分割任务中，卷积token mixer的局部偏置至关重要，分组卷积成为优选。

Conclusion: 对于医学图像分类任务，低复杂度的token mixer（如分组卷积或池化）已足够，且预训练权重仍有效；而对于分割任务，卷积token mixer的局部归纳偏置是关键，分组卷积因其减少 boven运行时和参数数量而成为优选。

Abstract: The generalization of the Transformer architecture via MetaFormer has
reshaped our understanding of its success in computer vision. By replacing
self-attention with simpler token mixers, MetaFormer provides strong baselines
for vision tasks. However, while extensively studied on natural image datasets,
its use in medical imaging remains scarce, and existing works rarely compare
different token mixers, potentially overlooking more suitable designs choices.
In this work, we present the first comprehensive study of token mixers for
medical imaging. We systematically analyze pooling-, convolution-, and
attention-based token mixers within the MetaFormer architecture on image
classification (global prediction task) and semantic segmentation (dense
prediction task). Our evaluation spans eight datasets covering diverse
modalities and common challenges in the medical domain. Given the prevalence of
pretraining from natural images to mitigate medical data scarcity, we also
examine transferring pretrained weights to new token mixers. Our results show
that, for classification, low-complexity token mixers (e.g. grouped convolution
or pooling) are sufficient, aligning with findings on natural images.
Pretrained weights remain useful despite the domain gap introduced by the new
token mixer. For segmentation, we find that the local inductive bias of
convolutional token mixers is essential. Grouped convolutions emerge as the
preferred choice, as they reduce runtime and parameter count compared to
standard convolutions, while the MetaFormer's channel-MLPs already provide the
necessary cross-channel interactions. Our code is available on GitHub.

</details>


### [50] [Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis](https://arxiv.org/abs/2510.05976)
*Eashan Adhikarla,Yixin Liu,Brian D. Davison*

Main category: cs.CV

TL;DR: 该调查分析了扩散模型在低光图像增强中的应用，提出了多视角分类法，评估了性能、部署挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 低光图像增强（LLIE）对于安全关键应用至关重要，如监控、自主导航和医学成像，其中可见性退化可能损害下游任务性能。扩散模型因其通过迭代去噪建模复杂图像分布的能力，成为LLIE的有前景的生成范式。

Method: 该调查提供了一个多视角分类法，涵盖六个类别：内在分解、光谱与潜在、加速、引导、多模态和自主；这些类别映射了跨物理先验、条件设置和计算效率的增强方法。

Result: 该调查对扩散模型进行了深入的比较性能评估，对抗生成对抗网络和基于Transformer的最先进方法，并探讨了实际部署挑战和新兴范式（如基础模型）的作用。

Conclusion: 该调查旨在通过突出趋势和提出开放研究问题，包括新颖的条件设置、实时适应以及基础模型的潜力，指导下一代基于扩散模型的低光图像增强研究。

Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.

</details>


### [51] [A Dynamic Mode Decomposition Approach to Morphological Component Analysis](https://arxiv.org/abs/2510.05977)
*Owen T. Huber,Raghu G. Raj,Tianyu Chen,Zacharie I. Idriss*

Main category: cs.CV

TL;DR: 本文提出动态形态成分分析（DMCA），通过动态模式分解特征值聚类学习自适应视频表示，有效应用于视频去噪和信号分离。


<details>
  <summary>Details</summary>
Motivation: 为了适应视频场景内容变化的动态性，学习自适应的视频表示以分离视频中结构不同的形态。

Method: 提出了一种基于动态模式分解特征值聚类的新方法，扩展了形态成分分析（MCA）算法，引入了数据驱动的MCA字典，称为动态形态成分分析（DMCA）。

Result: DMCA在Adobe 240fps数据集的视频去噪应用中表现出色，提升了微弱目标与海况叠加信号的信噪比。

Conclusion: 论文通过应用动态形态成分分析（DMCA）在逆合成孔径雷达图像中成功分离了自行车与风杂波，验证了DMCA方法的有效性。

Abstract: This paper introduces a novel methodology of adapting the representation of
videos based on the dynamics of their scene content variation. In particular,
we demonstrate how the clustering of dynamic mode decomposition eigenvalues can
be leveraged to learn an adaptive video representation for separating
structurally distinct morphologies of a video. We extend the morphological
component analysis (MCA) algorithm, which uses multiple predefined incoherent
dictionaries and a sparsity prior to separate distinct sources in signals, by
introducing our novel eigenspace clustering technique to obtain data-driven MCA
dictionaries, which we call dynamic morphological component analysis (DMCA).
After deriving our novel algorithm, we offer a motivational example of DMCA
applied to a still image, then demonstrate DMCA's effectiveness in denoising
applications on videos from the Adobe 240fps dataset. Afterwards, we provide an
example of DMCA enhancing the signal-to-noise ratio of a faint target summed
with a sea state, and conclude the paper by applying DMCA to separate a bicycle
from wind clutter in inverse synthetic aperture radar images.

</details>


### [52] [Diffusion-Based Image Editing for Breaking Robust Watermarks](https://arxiv.org/abs/2510.05978)
*Yunyi Ni,Finn Carter,Ze Niu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 扩散模型可有效破坏鲁棒图像水印，引导扩散攻击能显著降低水印可检测性，当前水印技术在生成AI时代存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 强大的基于扩散的图像生成和编辑技术的兴起对水印方案构成了新的威胁。

Method: 提出了一种理论研究和方法，证明扩散模型可以有效破坏设计用于抵抗传统扰动的鲁棒图像水印。进一步引入了一种新颖的引导扩散攻击，明确针对生成过程中的水印信号。

Result: 在多种最先进的水印方案（包括基于深度学习的方法StegaStamp、TrustMark和VINE）上评估，攻击后水印恢复率接近零，同时保持再生图像的高视觉保真度。

Conclusion: 当前鲁棒水印技术在生成模型攻击下存在根本性脆弱，强调了在生成AI时代需要新的水印策略。

Abstract: Robust invisible watermarking aims to embed hidden information into images
such that the watermark can survive various image manipulations. However, the
rise of powerful diffusion-based image generation and editing techniques poses
a new threat to these watermarking schemes. In this paper, we present a
theoretical study and method demonstrating that diffusion models can
effectively break robust image watermarks that were designed to resist
conventional perturbations. We show that a diffusion-driven ``image
regeneration'' process can erase embedded watermarks while preserving
perceptual image content. We further introduce a novel guided diffusion attack
that explicitly targets the watermark signal during generation, significantly
degrading watermark detectability. Theoretically, we prove that as an image
undergoes sufficient diffusion-based transformation, the mutual information
between the watermarked image and the embedded watermark payload vanishes,
resulting in decoding failure. Experimentally, we evaluate our approach on
multiple state-of-the-art watermarking schemes (including the deep
learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate
near-zero watermark recovery rates after attack, while maintaining high visual
fidelity of the regenerated images. Our findings highlight a fundamental
vulnerability in current robust watermarking techniques against generative
model-based attacks, underscoring the need for new watermarking strategies in
the era of generative AI.

</details>


### [53] [Detection and Measurement of Hailstones with Multimodal Large Language Models](https://arxiv.org/abs/2510.06008)
*Moritz Alker,David C. Schedl,Andreas Stöckl*

Main category: cs.CV

TL;DR: 研究利用预训练多模态模型从社交媒体图像中测量冰雹直径，两阶段提示策略效果最佳，平均误差1.12厘米，可补充传统传感器。


<details>
  <summary>Details</summary>
Motivation: 传统冰雹传感器提供的信息有限且空间稀疏，而社交媒体和新闻图像能提供更密集的数据。研究旨在探索预训练模型是否可从这些图像中测量冰雹直径，以补充传统方法。

Method: 研究利用预训练的多模态大语言模型，通过一阶段和两阶段提示策略估计冰雹直径。数据集包含474张来自奥地利2022年1月至2024年9月间冰雹事件的众包图像，冰雹最大直径为2至11厘米。两阶段提示利用图像中的参考对象（如人手）提供额外尺寸线索。

Result: 最佳模型的平均绝对误差为1.12厘米。两阶段提示相比单阶段提示提高了大多数模型的可靠性。

Conclusion: 预训练的多模态大语言模型，即使未经微调，也能通过社交媒体的图像提取有意义且空间密集的信息，补充传统冰雹传感器，实现对极端天气事件的快速和详细评估。自动实时图像采集仍是一个待解决的问题，但未来将使该方法直接适用于冰雹事件。

Abstract: This study examines the use of social media and news images to detect and
measure hailstones, utilizing pre-trained multimodal large language models. The
dataset for this study comprises 474 crowdsourced images of hailstones from
documented hail events in Austria, which occurred between January 2022 and
September 2024. These hailstones have maximum diameters ranging from 2 to 11cm.
We estimate the hail diameters and compare four different models utilizing
one-stage and two-stage prompting strategies. The latter utilizes additional
size cues from reference objects, such as human hands, within the image. Our
results show that pretrained models already have the potential to measure
hailstone diameters from images with an average mean absolute error of 1.12cm
for the best model. In comparison to a single-stage prompt, two-stage prompting
improves the reliability of most models. Our study suggests that these
off-the-shelf models, even without fine-tuning, can complement traditional hail
sensors by extracting meaningful and spatially dense information from social
media imagery, enabling faster and more detailed assessments of severe weather
events. The automated real-time image harvesting from social media and other
sources remains an open task, but it will make our approach directly applicable
to future hail events.

</details>


### [54] [Continual Learning for Image Captioning through Improved Image-Text Alignment](https://arxiv.org/abs/2510.06009)
*Bertram Taetz,Gal Bordelius*

Main category: cs.CV

TL;DR: 提出多损失框架解决持续图像描述中的遗忘和对齐问题，结合多种损失组件提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习场景下图像描述生成中的灾难性遗忘和视觉概念与语言对齐的难题。

Method: 基于预训练的ViT-GPT-2框架，结合了交叉熵损失和三个额外损失组件：基于提示的余弦相似度损失、CLIP风格损失和语言引导的对比损失。

Result: 该方法在减少灾难性遗忘的同时，实现了更好的语义描述对齐，且推理时无需额外开销或提示。

Conclusion: 该论文提出了一种新颖的多损失框架，通过结合语义引导和对比对齐，有效缓解了持续学习中的灾难性遗忘问题，并在语义对齐方面超越了现有方法。

Abstract: Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.

</details>


### [55] [Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context](https://arxiv.org/abs/2510.06026)
*An Thi Nguyen,Radina Stoykova,Eric Arazo*

Main category: cs.CV

TL;DR: 研究发现通用实例搜索模型可能意外识别个体，即使训练数据不包含人类主体。通过索引排除和混淆损失可降低识别能力，但仍存在漏洞，凸显AI治理与数据保护的监管问题。


<details>
  <summary>Details</summary>
Motivation: 通用实例搜索模型在刑事调查中可大幅减少手动分析大量监控录像的工作量，但研究发现这些模型存在意外涌现的能力，能够识别特定个体，即使训练数据不包含人类主体。这引发了对个人数据识别和画像的担忧。

Method: 研究评估了两种技术防护措施：索引排除和混淆损失，以限制模型的人员重新识别能力。实验表明，结合这两种方法可将人员重新识别准确率降至2%以下，同时保持对非人物体的检索性能82%。

Result: 实验表明，结合索引排除和混淆损失可将人员重新识别准确率降至2%以下，同时保持对非人物体的检索性能82%。然而，研究也发现这些防护措施存在关键漏洞，如通过部分人像图像可能绕过防护措施。

Conclusion: 研究发现通用实例搜索模型存在意外涌现的能力，能够识别特定个体，即使训练数据不包含人类主体。这引发了对个人数据识别和画像的担忧。虽然通过索引排除和混淆损失可以降低模型的人员重新识别能力，但仍存在关键漏洞，如通过部分人像图像可能绕过防护措施。这些发现突出了AI治理与数据保护交汇处的紧迫监管问题。

Abstract: Generic instance search models can dramatically reduce the manual effort
required to analyze vast surveillance footage during criminal investigations by
retrieving specific objects of interest to law enforcement. However, our
research reveals an unintended emergent capability: through overlearning, these
models can single out specific individuals even when trained on datasets
without human subjects. This capability raises concerns regarding
identification and profiling of individuals based on their personal data, while
there is currently no clear standard on how de-identification can be achieved.
We evaluate two technical safeguards to curtail a model's person
re-identification capacity: index exclusion and confusion loss. Our experiments
demonstrate that combining these approaches can reduce person re-identification
accuracy to below 2% while maintaining 82% of retrieval performance for
non-person objects. However, we identify critical vulnerabilities in these
mitigations, including potential circumvention using partial person images.
These findings highlight urgent regulatory questions at the intersection of AI
governance and data protection: How should we classify and regulate systems
with emergent identification capabilities? And what technical standards should
be required to prevent identification capabilities from developing in seemingly
benign applications?

</details>


### [56] [Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between](https://arxiv.org/abs/2510.06035)
*Ondřej Týbl,Lukáš Neumann*

Main category: cs.CV

TL;DR: UniNAS是一个统一的神经架构搜索空间，涵盖卷积网络、Transformer及其混合架构，通过新搜索算法发现优于手工设计的架构，并提供标准化工具包促进研究可重复性。


<details>
  <summary>Details</summary>
Motivation: 为了统一卷积网络、Transformer及其混合架构，并提供一个灵活的框架来发现新架构和分析现有架构。

Method: 提出了UniNAS，一个通用的神经架构搜索空间，并开发了一种新的搜索算法来遍历该空间。此外，还引入了一个标准化的训练和评估协议工具包。

Result: 在相同训练设置下，UniNAS发现的架构优于最先进的手工设计架构。

Conclusion: UniNAS提供了一个统一的神经架构搜索空间，为系统性地探索全谱神经架构开辟了新途径，并通过标准化工具包促进了研究的可重复性和公平比较。

Abstract: We introduce Universal Neural Architecture Space (UniNAS), a generic search
space for neural architecture search (NAS) which unifies convolutional
networks, transformers, and their hybrid architectures under a single, flexible
framework. Our approach enables discovery of novel architectures as well as
analyzing existing architectures in a common framework. We also propose a new
search algorithm that allows traversing the proposed search space, and
demonstrate that the space contains interesting architectures, which, when
using identical training setup, outperform state-of-the-art hand-crafted
architectures. Finally, a unified toolkit including a standardized training and
evaluation protocol is introduced to foster reproducibility and enable fair
comparison in NAS research. Overall, this work opens a pathway towards
systematically exploring the full spectrum of neural architectures with a
unified graph-based NAS perspective.

</details>


### [57] [VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization](https://arxiv.org/abs/2510.06040)
*Xinye Cao,Hongcan Guo,Jiawen Qian,Guoshun Nan,Chao Wang,Yuqi Pan,Tianhao Hou,Xiaojuan Wang,Yutong Gao*

Main category: cs.CV

TL;DR: VideoMiner通过层次树结构和T-GRPO方法解决了长视频理解中的冗余信息和动态适应问题，显著提升了性能并带来意外发现。


<details>
  <summary>Details</summary>
Motivation: 解决长视频理解中冗余信息干扰和动态适应复杂层次结构的挑战，提升多模态大语言模型（MM-LLMs）在长视频理解中的性能。

Method: 提出了VideoMiner方法，通过迭代分割、描述和聚类长视频形成层次树结构，并结合T-GRPO（一种基于树的组相对策略优化强化学习方法）精确定位关键帧。

Result: 在所有长视频理解任务中取得了优越性能，并发现模型能自发生成推理链，树生长辅助动态调整扩展深度带来准确性和效率的提升。

Conclusion: VideoMiner结合T-GRPO方法在长视频理解任务中表现出色，不仅解决了冗余信息干扰和动态适应复杂层次结构的挑战，还意外地促使模型自发生成推理链，同时通过树生长辅助动态调整扩展深度，实现了准确性和效率的提升。

Abstract: Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.

</details>


### [58] [GLVD: Guided Learned Vertex Descent](https://arxiv.org/abs/2510.06046)
*Pol Caselles Rico,Francesc Moreno Noguer*

Main category: cs.CV

TL;DR: GLVD是一种混合方法，结合顶点优化和全局引导，实现高效且高质量的三维人脸重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人脸建模方法受限于固定形状先验，而基于优化的方法计算成本高。GLVD旨在在表达性和计算效率之间取得平衡。

Method: GLVD通过整合每顶点神经场优化与动态预测的3D关键点的全局结构引导，结合相对空间编码，迭代优化网格顶点，无需密集的3D监督。

Result: GLVD在单视图设置中实现了最先进的性能，在多视图场景中保持高度竞争力，同时显著减少推理时间。

Conclusion: GLVD是一种结合了顶点神经场优化和全局结构引导的混合方法，能够在保持计算效率的同时实现高质量的三维人脸重建。

Abstract: Existing 3D face modeling methods usually depend on 3D Morphable Models,
which inherently constrain the representation capacity to fixed shape priors.
Optimization-based approaches offer high-quality reconstructions but tend to be
computationally expensive. In this work, we introduce GLVD, a hybrid method for
3D face reconstruction from few-shot images that extends Learned Vertex Descent
(LVD) by integrating per-vertex neural field optimization with global
structural guidance from dynamically predicted 3D keypoints. By incorporating
relative spatial encoding, GLVD iteratively refines mesh vertices without
requiring dense 3D supervision. This enables expressive and adaptable geometry
reconstruction while maintaining computational efficiency. GLVD achieves
state-of-the-art performance in single-view settings and remains highly
competitive in multi-view scenarios, all while substantially reducing inference
time.

</details>


### [59] [Medical Vision Language Models as Policies for Robotic Surgery](https://arxiv.org/abs/2510.06064)
*Akshay Muppidi,Martin Radfar*

Main category: cs.CV

TL;DR: TL;DR: 结合MedFlamingo和PPO的方法显著提高了机器人腹腔镜手术的任务成功率，展示了医学专业知识在手术规划和决策中的重要性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决基于视觉观察的机器人腹腔镜手术任务中PPO面临的挑战，如高维视觉输入、奖励稀疏性以及从原始视觉数据中提取任务相关特征的困难。

Method: 方法是将MedFlamingo（一种医学领域特定的视觉语言模型）与PPO结合，通过处理任务观察和指令生成高级规划令牌，有效结合医学专业知识和实时视觉反馈。

Result: 结果是在LapGym的五个不同腹腔镜手术任务环境中，MedFlamingo PPO的表现优于标准视觉PPO和OpenFlamingo PPO基线，任务成功率超过70%，改进幅度从66.67%到1114.29%不等。

Conclusion: 该论文的结论强调了专业医学知识在机器人手术规划和决策中的价值，通过结合MedFlamingo和PPO的方法，显著提高了任务成功率。

Abstract: Vision-based Proximal Policy Optimization (PPO) struggles with visual
observation-based robotic laparoscopic surgical tasks due to the
high-dimensional nature of visual input, the sparsity of rewards in surgical
environments, and the difficulty of extracting task-relevant features from raw
visual data. We introduce a simple approach integrating MedFlamingo, a medical
domain-specific Vision-Language Model, with PPO. Our method is evaluated on
five diverse laparoscopic surgery task environments in LapGym, using only
endoscopic visual observations. MedFlamingo PPO outperforms and converges
faster compared to both standard vision-based PPO and OpenFlamingo PPO
baselines, achieving task success rates exceeding 70% across all environments,
with improvements ranging from 66.67% to 1114.29% compared to baseline. By
processing task observations and instructions once per episode to generate
high-level planning tokens, our method efficiently combines medical expertise
with real-time visual feedback. Our results highlight the value of specialized
medical knowledge in robotic surgical planning and decision-making.

</details>


### [60] [Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA](https://arxiv.org/abs/2510.06067)
*Python Song,Luke Tenyi Chang,Yun-Yun Tsai,Penghui Li,Junfeng Yang*

Main category: cs.CV

TL;DR: 论文通过CAPTCHA-X基准和代理框架证明逐步推理能大幅提升视觉语言模型的空间推理能力（83.9%准确率），揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: CAPTCHA已成为评估视觉语言模型空间推理能力的现实基准，但现有商用模型（如Gemini、Claude、GPT）表现不佳（21.9%准确率），需探究推理对性能提升的作用。

Method: 提出CAPTCHA-X基准测试（含7类CAPTCHA及逐步解决方案）和5项推理导向指标，并设计一个基于代理的视觉语言模型框架，利用模型固有推理能力。

Result: 所提方法在5类高难度CAPTCHA上达到83.9%平均准确率，显著超越基线。

Conclusion: 当前商用视觉语言模型在解决CAPTCHA等空间推理任务上存在显著局限，但通过逐步推理方法可大幅提升性能（83.9%准确率），强调了推理能力对未来视觉空间挑战的重要性。

Abstract: CAPTCHA, originally designed to distinguish humans from robots, has evolved
into a real-world benchmark for assessing the spatial reasoning capabilities of
vision-language models. In this work, we first show that step-by-step reasoning
is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent
high-difficulty spatial reasoning tasks, and that current commercial
vision-language models still struggle with such reasoning. In particular, we
observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to
effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).
However, our findings indicate that requiring the model to perform step-by-step
reasoning before generating the final coordinates can significantly enhance its
solving accuracy, underscoring the severity of the gap. To systematically study
this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with
reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,
etc.) with step-by-step action solutions and grounding annotations. We further
define five reasoning-oriented metrics that enable a comprehensive evaluation
of models reasoning capabilities. To validate the effectiveness of reasoning,
we also propose a general agentic VLM-based framework that incorporates the
models inherent reasoning abilities. Our method achieves state-of-the-art
performance across five high-difficulty CAPTCHA types, with an average solving
accuracy of 83.9 percent, substantially surpassing existing baselines. These
results reveal the limitations of current models and highlight the importance
of reasoning in advancing visual-spatial challenges in the future.

</details>


### [61] [There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers](https://arxiv.org/abs/2510.06070)
*Meghna P Ayyar,Jenny Benois-Pineau,Akka Zemmari*

Main category: cs.CV

TL;DR: 提出一种结合注意力与统计过滤的ViT解释方法，生成更忠实和可解释的映射图，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT解释方法多依赖注意力权重，但其生成的映射图噪声较多，且MLP块结合的方法虽被提出，但注意力仍是有价值的信号。

Method: 结合注意力图与统计过滤，去除噪声或无关模式，生成更忠实的解释。进一步扩展为类特定变体，产生区分性解释。

Result: 该方法在多个数据集上表现优于或与现有最先进方法相当，生成更清晰、更可解释的映射图。

Conclusion: 该论文提出了一种结合注意力机制与统计过滤的方法，用于生成更忠实和可解释的视觉变换器（ViT）解释。该方法在多个数据集上表现优异，且在人类可解释性评估中表现良好。

Abstract: Explainable AI (XAI) has become increasingly important with the rise of large
transformer models, yet many explanation methods designed for CNNs transfer
poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on
attention weights, which tend to yield noisy maps as they capture
token-to-token interactions within each layer.While attribution methods
incorporating MLP blocks have been proposed, we argue that attention remains a
valuable and interpretable signal when properly filtered. We propose a method
that combines attention maps with a statistical filtering, initially proposed
for CNNs, to remove noisy or uninformative patterns and produce more faithful
explanations. We further extend our approach with a class-specific variant that
yields discriminative explanations. Evaluation against popular state-of-the-art
methods demonstrates that our approach produces sharper and more interpretable
maps. In addition to perturbation-based faithfulness metrics, we incorporate
human gaze data to assess alignment with human perception, arguing that human
interpretability remains essential for XAI. Across multiple datasets, our
approach consistently outperforms or is comparable to the SOTA methods while
remaining efficient and human plausible.

</details>


### [62] [When Thinking Drifts: Evidential Grounding for Robust Video Reasoning](https://arxiv.org/abs/2510.06077)
*Mi Luo,Zihui Xue,Alex Dimakis,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文发现思维链（CoT）在视频推理中会导致‘视觉思维漂移’，提出视觉证据奖励（VER）框架，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）机制在文本任务中提升了推理能力，但在视频理解中却表现不佳，导致性能下降和误导性推理。本文旨在解决这一现象，即‘视觉思维漂移’。

Method: 提出了一种新颖的强化学习框架——视觉证据奖励（VER），通过奖励基于视觉证据的推理轨迹来对抗视觉思维漂移。

Result: 在10个多样化的视频理解基准测试中，Video-VER均取得了最佳性能。

Conclusion: 本文通过引入视觉证据奖励（VER）框架，有效解决了视频推理中视觉思维漂移的问题，提升了模型在多样化视频理解基准上的性能。

Abstract: Video reasoning, the task of enabling machines to infer from dynamic visual
content through multi-step logic, is crucial for advanced AI. While the
Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,
its application to video understanding remains underexplored. This paper
presents a systematic analysis revealing that CoT often degrades performance in
video reasoning, generating verbose but misleading internal monologues, and
leading to hallucinated visual details and overridden correct intuitions - a
phenomenon we term "visual thinking drift". We explain this drift through a
Bayesian lens, positing that CoT traces often diverge from actual visual
evidence, instead amplifying internal biases or language priors, causing models
to storytell rather than engage in grounded reasoning. To counteract this, we
introduce Visual Evidence Reward (VER), a novel reinforcement learning
framework that explicitly rewards the generation of reasoning traces that are
verifiably grounded in visual evidence. Comprehensive evaluation across 10
diverse video understanding benchmarks demonstrates that our Video-VER
consistently achieves top performance. Our work sheds light on the distinct
challenges of video-centric reasoning and encourages the development of AI that
robustly grounds its inferences in visual evidence - for large multimodal
models that not only "think before answering", but also "see while thinking".

</details>


### [63] [A public cardiac CT dataset featuring the left atrial appendage](https://arxiv.org/abs/2510.06090)
*Bjoern Hansen,Jonas Pedersen,Klaus F. Kofoed,Oscar Camara,Rasmus R. Paulsen,Kristine Soerensen*

Main category: cs.CV

TL;DR: 该论文创建了首个开源的高分辨率LAA、CAs和PVs分割数据集，基于ImageCAS数据，旨在改进这些结构的分割精度并促进LAA形态学研究。


<details>
  <summary>Details</summary>
Motivation: 尽管现有分割框架如TotalSegmentator（TS）已取得一定成功，但LAA、CAs和PVs的精确分割仍是医学影像中的重大挑战。

Method: 使用专门为高分辨率LAA分割开发的最先进分割框架，并在大型私有数据集上训练网络，模型迁移至ImageCAS数据。CA标签从原始ImageCAS注释中改进，PV分割从TS输出中细化。

Result: 提供了1000例心脏CT血管造影（CCTA）扫描的公开数据集ImageCAS的完整心脏标签，并生成了LAA、CAs和PVs的高分辨率分割数据。

Conclusion: 该论文提出了首个开源、解剖学上一致的高分辨率分割数据集，为左心耳（LAA）、冠状动脉（CAs）和肺静脉（PVs）的精确分割提供了新资源，并促进了LAA形态学分析的新方法。

Abstract: Despite the success of advanced segmentation frameworks such as
TotalSegmentator (TS), accurate segmentations of the left atrial appendage
(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant
challenge in medical imaging. In this work, we present the first open-source,
anatomically coherent dataset of curated, high-resolution segmentations for
these structures, supplemented with whole-heart labels produced by TS on the
publicly available ImageCAS dataset consisting of 1000 cardiac computed
tomography angiography (CCTA) scans. One purpose of the data set is to foster
novel approaches to the analysis of LAA morphology.
  LAA segmentations on ImageCAS were generated using a state-of-the-art
segmentation framework developed specifically for high resolution LAA
segmentation. We trained the network on a large private dataset with manual
annotations provided by medical readers guided by a trained cardiologist and
transferred the model to ImageCAS data. CA labels were improved from the
original ImageCAS annotations, while PV segmentations were refined from TS
outputs. In addition, we provide a list of scans from ImageCAS that contains
common data flaws such as step artefacts, LAAs extending beyond the scanner's
field of view, and other types of data defects.

</details>


### [64] [Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2510.06098)
*Yinjian Wang,Wei Li,Yuanyuan Gui,Gemine Vivone*

Main category: cs.CV

TL;DR: 本文提出了一种新的高光谱图像超分辨率模型，通过块项分解和非凸模态混洗张量相关总变差紧凑地结合多级先验，并设计了高效优化算法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于张量的模型只能有效利用一到两个先验，而同时融入多级先验会增加模型复杂性，导致权重平衡和多块结构优化的挑战。本文旨在提出一种新模型，紧凑地描述这些多级先验。

Method: 通过块项分解将潜在的高空间-光谱分辨率图像分解为光谱子空间和空间映射，并利用非凸模态混洗张量相关总变差共同建模高阶空间低秩性和平滑性先验。设计了一种基于线性化交替方向乘子法的高效算法来优化模型，并在温和条件下理论证明了其KKT收敛性。

Result: 所提出的模型通过块项分解和非凸模态混洗张量相关总变差有效结合了多级先验，实验验证了其有效性。

Conclusion: 实验结果表明，所提出的算法在多个数据集上均表现出色，代码实现将在GitHub上公开。

Abstract: Fusing a hyperspectral image with a multispectral image acquired over the
same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a
popular computational way to access the latent high-spatial-spectral-resolution
image. To date, a variety of fusion methods have been proposed, among which the
tensor-based ones have testified that multiple priors, such as multidimensional
low-rankness and spatial total variation at multiple levels, effectively drive
the fusion process. However, existing tensor-based models can only effectively
leverage one or two priors at one or two levels, since simultaneously
incorporating multi-level priors inevitably increases model complexity. This
introduces challenges in both balancing the weights of different priors and
optimizing multi-block structures. Concerning this, we present a novel
hyperspectral super-resolution model compactly characterizing these multi-level
priors of hyperspectral images within the tensor framework. Firstly, the
proposed model decouples the spectral low-rankness and spatial priors by
casting the latent high-spatial-spectral-resolution image into spectral
subspace and spatial maps via block term decomposition. Secondly, these spatial
maps are stacked as the spatial tensor encoding the high-order spatial
low-rankness and smoothness priors, which are co-modeled via the proposed
non-convex mode-shuffled tensor correlated total variation. Finally, we draw
inspiration from the linearized alternating direction method of multipliers to
design an efficient algorithm to optimize the resulting model, theoretically
proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments
on multiple datasets demonstrate the effectiveness of the proposed algorithm.
The code implementation will be available from https://github.com/WongYinJ.

</details>


### [65] [Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction](https://arxiv.org/abs/2510.06113)
*Shuo Jiang,Zhuwen Chen,Liaoman Xu,Yanming Zhu,Changmiao Wang,Jiong Zhang,Feiwei Qin,Yifei Chen,Zhu Zhu*

Main category: cs.CV

TL;DR: FeatProto 是一种创新的原型学习框架，通过整合 WSI 和基因组数据提升癌症生存预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前生存分析模型在临床中难以解释，原型学习虽有望解决但传统方法忽视肿瘤整体背景且缺乏与基因组数据的语义对齐。

Method: 提出了一种创新的原型学习框架 FeatProto，包括三个主要创新点：(1) 结合关键补丁与全局背景的表型表示，(2) 使用指数原型更新策略（EMA ProtoUp）保持稳定的跨模态关联，(3) 采用分层原型匹配方案捕捉全局中心性、局部典型性和队列级别趋势。

Result: 在四个公开癌症数据集上的评估表明，FeatProto 在准确性和互操作性上均优于现有单模态和多模态生存预测技术。

Conclusion: FeatProto 框架通过整合全局和局部特征与基因组数据，显著提升了癌症生存预测的准确性和可解释性，为病理学中的原型学习提供了新的视角。

Abstract: Survival analysis plays a vital role in making clinical decisions. However,
the models currently in use are often difficult to interpret, which reduces
their usefulness in clinical settings. Prototype learning presents a potential
solution, yet traditional methods focus on local similarities and static
matching, neglecting the broader tumor context and lacking strong semantic
alignment with genomic data. To overcome these issues, we introduce an
innovative prototype-based multimodal framework, FeatProto, aimed at enhancing
cancer survival prediction by addressing significant limitations in current
prototype learning methodologies within pathology. Our framework establishes a
unified feature prototype space that integrates both global and local features
of whole slide images (WSI) with genomic profiles. This integration facilitates
traceable and interpretable decision-making processes. Our approach includes
three main innovations: (1) A robust phenotype representation that merges
critical patches with global context, harmonized with genomic data to minimize
local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that
sustains stable cross-modal associations and employs a wandering mechanism to
adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype
matching scheme designed to capture global centrality, local typicality, and
cohort-level trends, thereby refining prototype inference. Comprehensive
evaluations on four publicly available cancer datasets indicate that our method
surpasses current leading unimodal and multimodal survival prediction
techniques in both accuracy and interoperability, providing a new perspective
on prototype learning for critical medical applications. Our source code is
available at https://github.com/JSLiam94/FeatProto.

</details>


### [66] [Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework](https://arxiv.org/abs/2510.06123)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: SSGNet通过生成图像和伪标签迭代优化，有效解决了医学影像中数据稀缺问题，提升了分类和分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的深度学习常受限于稀缺且不平衡的标注数据，SSGNet旨在通过数据增强和标签优化来解决这一问题。

Method: SSGNet结合了特定类别的生成建模与迭代半监督伪标签技术，通过StyleGAN3生成图像扩充训练数据，并通过迭代伪标签细化标签。

Result: 在多个医学影像基准测试中，SSGNet在分类和分割性能上均取得了显著提升，且生成的样本质量高（通过Frechet Inception Distance分析证实）。

Conclusion: SSGNet作为一种实用策略，有效缓解了医学图像分析中的注释瓶颈问题，并提高了模型的鲁棒性。

Abstract: Deep learning in medical imaging is often limited by scarce and imbalanced
annotated data. We present SSGNet, a unified framework that combines class
specific generative modeling with iterative semisupervised pseudo labeling to
enhance both classification and segmentation. Rather than functioning as a
standalone model, SSGNet augments existing baselines by expanding training data
with StyleGAN3 generated images and refining labels through iterative pseudo
labeling. Experiments across multiple medical imaging benchmarks demonstrate
consistent gains in classification and segmentation performance, while Frechet
Inception Distance analysis confirms the high quality of generated samples.
These results highlight SSGNet as a practical strategy to mitigate annotation
bottlenecks and improve robustness in medical image analysis.

</details>


### [67] [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/abs/2510.06131)
*Jiawei Mao,Yuhan Wang,Lifeng Chen,Can Zhao,Yucheng Tang,Dong Yang,Liangqiong Qu,Daguang Xu,Yuyin Zhou*

Main category: cs.CV

TL;DR: MeDiM是首个跨模态医学离散扩散模型，通过共享概率空间和MLLM实现了高保真医学生成和报告生成，显著提升了多模态输出的一致性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成医学模型受限于模态特定场景，无法整合成像、病理和临床笔记等多模态数据，限制了其发展为能够跨生物医学数据学习和推理的基础模型。

Method: MeDiM采用离散扩散框架，通过移除因果注意力掩码和注入连续时间步嵌入，实现了双向上下文和扩散感知。

Result: 实验结果显示MeDiM在医学生成（FID 16.60 on MIMIC-CXR和FID 24.19 on PathGen）和报告生成（METEOR 0.2650和0.2580）方面表现优异，联合生成的图像-报告对进一步提升了下游任务性能。

Conclusion: MeDiM通过共享概率空间和多模态大语言模型（MLLM）作为扩散主干，成功实现了跨模态的高保真医学生成和准确报告生成，展示了其在临床场景中的潜力。

Abstract: Recent advances in generative medical models are constrained by
modality-specific scenarios that hinder the integration of complementary
evidence from imaging, pathology, and clinical notes. This fragmentation limits
their evolution into foundation models that can learn and reason across the
full spectrum of biomedical data. We propose MeDiM, the first medical discrete
diffusion model that learns shared distributions across modalities without
modality-specific components. MeDiM unifies multiple generative tasks:
translating between images and text, and jointly producing image-report pairs
across domains in response to prompts. Built on a discrete diffusion framework,
MeDiM bridges vision and language representations through a shared
probabilistic space. To enable unified and flexible medical generation, we
employ a multimodal large language model (MLLM) as the diffusion backbone,
leveraging its prior knowledge and cross-modal reasoning. Two key designs are
introduced: (1) removing the causal attention mask for bidirectional context,
and (2) injecting continuous timestep embeddings for diffusion awareness.
Experiments demonstrate high-fidelity medical generation (FID 16.60 on
MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR
0.2650 and 0.2580). Jointly generated image-report pairs further enhance
downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,
plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports
coherent and clinically grounded multimodal outputs.

</details>


### [68] [Deforming Videos to Masks: Flow Matching for Referring Video Segmentation](https://arxiv.org/abs/2510.06139)
*Zanyi Wang,Dengyang Jiang,Liuzhuozheng Li,Sizhe Dang,Chengzu Li,Harry Yang,Guang Dai,Mengmeng Wang,Jingdong Wang*

Main category: cs.CV

TL;DR: FlowRVS将RVOS任务重新定义为条件连续流问题，利用预训练T2V模型的优势，显著提升了分割性能，在多个基准测试中刷新了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的RVOS方法通常采用‘定位-分割’的级联设计，导致语义信息简化为粗糙几何提示（如点），且难以保持时间一致性。FlowRVS旨在克服这些限制。

Method: FlowRVS采用一种生成式方法，直接从视频的整体表示学习到目标掩码的语言引导变形，而非传统的从噪声到掩码或直接预测掩码的流程。

Result: FlowRVS在所有主要RVOS基准测试中均取得了新的最先进结果，如在MeViS中达到51.1的J&F（比之前最佳结果提升1.6），在零样本Ref-DAVIS17中达到73.3（提升2.7）。

Conclusion: FlowRVS通过将RVOS任务重新定义为条件连续流问题，利用预训练的T2V模型的优势，实现了细粒度像素控制、文本-视频语义对齐和时间一致性，显著提升了RVOS任务的性能。

Abstract: Referring Video Object Segmentation (RVOS) requires segmenting specific
objects in a video guided by a natural language description. The core challenge
of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels
and continuously segment them through the complex dynamics of a video. Faced
with this difficulty, prior work has often decomposed the task into a pragmatic
`locate-then-segment' pipeline. However, this cascaded design creates an
information bottleneck by simplifying semantics into coarse geometric prompts
(e.g, point), and struggles to maintain temporal consistency as the segmenting
process is often decoupled from the initial language grounding. To overcome
these fundamental limitations, we propose FlowRVS, a novel framework that
reconceptualizes RVOS as a conditional continuous flow problem. This allows us
to harness the inherent strengths of pretrained T2V models, fine-grained pixel
control, text-video semantic alignment, and temporal coherence. Instead of
conventional generating from noise to mask or directly predicting mask, we
reformulate the task by learning a direct, language-guided deformation from a
video's holistic representation to its target mask. Our one-stage, generative
approach achieves new state-of-the-art results across all major RVOS
benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in
MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),
demonstrating the significant potential of modeling video understanding tasks
as continuous deformation processes.

</details>


### [69] [Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images](https://arxiv.org/abs/2510.06145)
*Aditya Prakash,David Forsyth,Saurabh Gupta*

Main category: cs.CV

TL;DR: 论文提出了一种从单图预测双手3D运动的方法，通过扩散模型和注释管道解决标注缺失问题，实验显示在多样数据和零样本泛化上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决日常生活中从单张图像预测双手3D运动与关节的挑战，尤其是在多样环境下缺乏3D手部标注的问题。

Method: 采用扩散模型将2D手部关键点序列提升至4D手部运动，并设计了一个注释管道来处理多样环境下的3D手部标注缺失问题。

Result: 在6个数据集上的广泛实验显示，使用带有推测标签的多样数据训练（提升14%）以及提出的提升（提升42%）和预测（提升16.4%）模型均优于最佳基线，尤其在零样本泛化方面表现突出。

Conclusion: 论文提出了一种从单张图像预测日常生活中双手3D运动与关节的方法，通过设计一个注释管道和采用扩散损失来应对多样性和多模态挑战，实验证明了该方法在零样本泛化上的有效性。

Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation
from a single image in everyday settings. To address the lack of 3D hand
annotations in diverse settings, we design an annotation pipeline consisting of
a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the
forecasting model, we adopt a diffusion loss to account for the multimodality
in hand motion distribution. Extensive experiments across 6 datasets show the
benefits of training on diverse data with imputed labels (14% improvement) and
effectiveness of our lifting (42% better) & forecasting (16.4% gain) models,
over the best baselines, especially in zero-shot generalization to everyday
images.

</details>


### [70] [ShapeGen4D: Towards High Quality 4D Shape Generation from Videos](https://arxiv.org/abs/2510.06208)
*Jiraphon Yenphraphai,Ashkan Mirzaei,Jianqi Chen,Jiaxu Zou,Sergey Tulyakov,Raymond A. Yeh,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的视频到4D形状生成框架，通过时间注意力、点采样和噪声共享技术，显著提升了生成结果的稳定性和质量。


<details>
  <summary>Details</summary>
Motivation: 视频条件下的4D形状生成旨在直接从输入视频中恢复随时间变化的3D几何结构和视图一致的外观，但现有方法存在鲁棒性和一致性不足的问题。

Method: 论文提出了一种基于大规模预训练3D模型的视频到4D形状生成框架，包括三个关键组件：时间注意力机制、时间感知点采样和4D潜在锚定，以及帧间噪声共享技术。

Result: 该方法能够准确捕捉非刚性运动、体积变化甚至拓扑过渡，无需逐帧优化，且在多样化的野外视频中表现优于基线。

Conclusion: 该论文提出的视频到4D形状生成框架通过引入时间注意力、时间感知点采样和4D潜在锚定以及帧间噪声共享，显著提升了生成结果的鲁棒性和感知保真度，减少了失败模式。

Abstract: Video-conditioned 4D shape generation aims to recover time-varying 3D
geometry and view-consistent appearance directly from an input video. In this
work, we introduce a native video-to-4D shape generation framework that
synthesizes a single dynamic 3D representation end-to-end from the video. Our
framework introduces three key components based on large-scale pre-trained 3D
models: (i) a temporal attention that conditions generation on all frames while
producing a time-indexed dynamic representation; (ii) a time-aware point
sampling and 4D latent anchoring that promote temporally consistent geometry
and texture; and (iii) noise sharing across frames to enhance temporal
stability. Our method accurately captures non-rigid motion, volume changes, and
even topological transitions without per-frame optimization. Across diverse
in-the-wild videos, our method improves robustness and perceptual fidelity and
reduces failure modes compared with the baselines.

</details>


### [71] [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209)
*Jiahao Wang,Zhenpei Yang,Yijing Bai,Yingwei Li,Yuliang Zou,Bo Sun,Abhijit Kundu,Jose Lezama,Luna Yue Huang,Zehao Zhu,Jyh-Jing Hwang,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.CV

TL;DR: Drive&Gen框架结合生成视频模型与端到端驾驶模型，通过统计度量评估生成视频真实性，利用合成数据提升规划器泛化能力，为自动驾驶扩展新领域提供低成本方案。


<details>
  <summary>Details</summary>
Motivation: 探索生成视频模型是否能忠实遵循指定条件并足够真实以评估端到端自动驾驶规划器，以及如何通过数据更深入地理解端到端规划器的偏见并提升其在分布外场景的泛化能力。

Method: 提出了一种新颖的统计度量方法，利用端到端驾驶模型评估生成视频的真实性；通过生成视频模型的可控性进行针对性实验，研究影响端到端规划器性能的分布差距。

Result: 研究表明，生成视频模型产生的合成数据是真实世界数据收集的成本效益替代方案，能有效提升端到端模型在现有操作设计域之外的泛化能力。

Conclusion: 通过Drive&Gen框架，本研究展示了生成视频模型与端到端驾驶模型的结合如何有效提升自动驾驶规划器的评估与泛化能力，为自动驾驶服务扩展到新领域提供了成本效益高的解决方案。

Abstract: Recent advances in generative models have sparked exciting new possibilities
in the field of autonomous vehicles. Specifically, video generation models are
now being explored as controllable virtual testing environments.
Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined
alternative to conventional modular autonomous driving systems, gaining
popularity for their simplicity and scalability. However, the application of
these techniques to simulation and planning raises important questions. First,
while video generation models can generate increasingly realistic videos, can
these videos faithfully adhere to the specified conditions and be realistic
enough for E2E autonomous planner evaluation? Second, given that data is
crucial for understanding and controlling E2E planners, how can we gain deeper
insights into their biases and improve their ability to generalize to
out-of-distribution scenarios? In this work, we bridge the gap between the
driving models and generative world models (Drive&Gen) to address these
questions. We propose novel statistical measures leveraging E2E drivers to
evaluate the realism of generated videos. By exploiting the controllability of
the video generation model, we conduct targeted experiments to investigate
distribution gaps affecting E2E planner performance. Finally, we show that
synthetic data produced by the video generation model offers a cost-effective
alternative to real-world data collection. This synthetic data effectively
improves E2E model generalization beyond existing Operational Design Domains,
facilitating the expansion of autonomous vehicle services into new operational
contexts.

</details>


### [72] [Fine-grained Defocus Blur Control for Generative Image Models](https://arxiv.org/abs/2510.06215)
*Ayush Shrivastava,Connelly Barnes,Xuaner Zhang,Lingzhi Zhang,Andrew Owens,Sohrab Amirghodsi,Eli Shechtman*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的文本到图像扩散框架，通过整合EXIF数据和模仿物理成像过程，实现了对镜头模糊效果的精细控制，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成多样化和高质量图像方面表现出色，但在融入细粒度相机元数据（如精确光圈设置）方面存在不足。本文旨在通过利用图像文件中嵌入的相机元数据（EXIF数据），重点生成可控的镜头模糊效果，以解决这一问题。

Method: 该方法首先生成全对焦图像，估计其单眼深度，通过新颖的焦点距离变换器预测合理的焦点距离，最后利用现有的可微分镜头模糊模型形成散焦图像。整个过程的梯度反向传播使得模型能够在无显式监督的情况下学习生成基于内容元素和EXIF数据的散焦效果。

Result: 实验结果表明，该模型在保持场景内容不变的同时，实现了对散焦效果的精细交互式用户控制，这是现有扩散模型无法实现的。

Conclusion: 该论文提出的框架通过整合相机元数据（如EXIF数据）和模仿物理成像过程，实现了对镜头模糊效果的精细控制，显著提升了现有扩散模型在生成图像时的精确度和可控性。

Abstract: Current text-to-image diffusion models excel at generating diverse,
high-quality images, yet they struggle to incorporate fine-grained camera
metadata such as precise aperture settings. In this work, we introduce a novel
text-to-image diffusion framework that leverages camera metadata, or EXIF data,
which is often embedded in image files, with an emphasis on generating
controllable lens blur. Our method mimics the physical image formation process
by first generating an all-in-focus image, estimating its monocular depth,
predicting a plausible focus distance with a novel focus distance transformer,
and then forming a defocused image with an existing differentiable lens blur
model. Gradients flow backwards through this whole process, allowing us to
learn without explicit supervision to generate defocus effects based on content
elements and the provided EXIF data. At inference time, this enables precise
interactive user control over defocus effects while preserving scene contents,
which is not achievable with existing diffusion models. Experimental results
demonstrate that our model enables superior fine-grained control without
altering the depicted scene.

</details>


### [73] [EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark](https://arxiv.org/abs/2510.06218)
*Deheng Zhang,Yuqian Fu,Runyi Yang,Yang Miao,Tianwen Qian,Xu Zheng,Guolei Sun,Ajad Chhatkuli,Xuanjing Huang,Yu-Gang Jiang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: EgoNight是首个夜间自我中心视觉综合基准，核心任务为视觉问答（VQA），揭示了光照条件对模型性能的显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心视觉基准主要关注白天场景，忽视了现实应用中不可避免的低光条件。

Method: 通过合成和真实世界对齐的视频构建EgoNight-VQA，利用日间数据增强夜间标注质量，并通过人工验证确保可靠性。

Result: 评估显示，最先进的多模态大语言模型在从白天转移到夜间时性能显著下降。

Conclusion: EgoNight-VQA为夜间自我中心视觉研究提供了坚实的基础，并推动了跨光照领域模型的开发。

Abstract: Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.

</details>


### [74] [Human3R: Everyone Everywhere All at Once](https://arxiv.org/abs/2510.06219)
*Yue Chen,Xingyu Chen,Yuxuan Xue,Anpei Chen,Yuliang Xiu,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: Human3R是一个单阶段、实时、低内存占用的统一框架，用于从单目视频中联合重建多人3D身体、场景和相机轨迹，性能优异且无需繁重预处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段流程、迭代接触感知优化及繁重预处理（如人体检测、深度估计和SLAM），Human3R旨在简化流程并提升效率。

Method: Human3R基于4D在线重建模型CUT3R，采用参数高效的视觉提示调整，保留了CUT3R的丰富时空先验，同时支持直接读取多个SMPL-X身体。

Result: Human3R在小型合成数据集BEDLAM上仅用一天单GPU训练，即实现实时速度（15 FPS）和低内存占用（8 GB），在多项任务中达到或接近最先进性能。

Conclusion: Human3R作为一个统一的、前馈式框架，在单次前向传递中实现了全局多人SMPL-X身体、密集3D场景和相机轨迹的联合恢复，消除了繁重的依赖和迭代优化。

Abstract: We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [75] [Approximating Multiple-Depot Capacitated Vehicle Routing via LP Rounding](https://arxiv.org/abs/2510.05321)
*Zachary Friggstad,Tobias Mömke*

Main category: cs.DS

TL;DR: 本文提出了一种新的LP松弛方法，用于解决多仓库容量限制车辆路径问题，获得了3.9365的近似比。


<details>
  <summary>Details</summary>
Motivation: 多仓库容量限制车辆路径问题在实际物流和配送中具有重要应用，但现有方法在近似比和效率上仍有改进空间。

Method: 通过构建和舍入一个新的LP松弛来解决CVRP-MD问题。

Result: 提出了一个3.9365的近似算法，显著优于现有方法。

Conclusion: 本文提出了一个基于新LP松弛的3.9365近似算法，用于解决多仓库容量限制车辆路径问题（CVRP-MD）。

Abstract: In Capacitated Vehicle Routing with Multiple Depots (CVRP-MD) we are given a
set of client locations $C$ and a set of depots $R$ located in a metric space
with costs $c(i,j)$ between $u,v \in C \cup R$. Additionally, we are given a
capacity bound $k$. The goal is to find a collection of tours of minimum total
cost such that each tour starts and ends at some depot $r \in R$ and includes
at most $k$ clients and such that each client lies on at least one tour. Our
main result is a $3.9365$-approximation based on rounding a new LP relaxation
for CVRP-MD.

</details>


### [76] [Time To Replace Your Filter: How Maplets Simplify System Design](https://arxiv.org/abs/2510.05518)
*Michael A. Bender,Alex Conway,Martín Farach-Colton,Rob Johnson,Prashant Pandey*

Main category: cs.DS

TL;DR: maplets作为空间高效的近似键值映射数据结构，比传统过滤器更适合需要键值关联的应用，能简化设计并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有过滤器无法提供键值关联功能，导致复杂的工作around和性能下降，maplets作为空间高效的近似键值映射数据结构是更合适的抽象。

Method: 通过详细案例研究（如SplinterDB、Squeakr和Mantis），识别常见模式并展示统一的maplet抽象如何简化设计并提升性能。

Result: maplets在保持空间效率的同时，原生支持键值关联，并提供单边错误保证。

Conclusion: 应用默认使用maplets而非过滤器，可以在数据库、计算生物学和网络等多个领域带来更简单的设计和更好的性能。

Abstract: Filters such as Bloom, quotient, and cuckoo filters are fundamental building
blocks providing space-efficient approximate set membership testing. However,
many applications need to associate small values with keys-functionality that
filters do not provide. This mismatch forces complex workarounds that degrade
performance. We argue that maplets-space-efficient data structures for
approximate key-value mappings-are the right abstraction. A maplet provides the
same space benefits as filters while natively supporting key-value associations
with one-sided error guarantees. Through detailed case studies of SplinterDB
(LSM-based key-value store), Squeakr (k-mer counter), and Mantis (genomic
sequence search), we identify the common patterns and demonstrate how a unified
maplet abstraction can lead to simpler designs and better performance. We
conclude that applications benefit from defaulting to maplets rather than
filters across domains including databases, computational biology, and
networking.

</details>


### [77] [Parameterized Complexity of Temporal Connected Components: Treewidth and k-Path Graphs](https://arxiv.org/abs/2510.05806)
*Argyrios Deligkas,Michelle Döring,Eduard Eiben,Tiger-Lily Goldsmith,George Skretas,Georg Tennigkeit*

Main category: cs.DS

TL;DR: The paper analyzes the parameterized complexity of temporal connected components, finding NP-hardness for low tw and tpn but FPT solutions under combined parameters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the computational complexity of maximum temporal connected components in temporal graphs, particularly how structural and temporal parameters influence the tractability of these problems.

Method: The study employs parameterized complexity analysis, focusing on treewidth (tw) and temporal path number (tpn) as key parameters. It examines both openTCC and closedTCC problems under these parameters.

Result: The results show that openTCC and closedTCC are NP-hard even with low values of tw and tpn. However, openTCC is in XP when parameterized by tpn, and both problems become FPT under combined parameters like tw plus tpn, tw plus graph lifetime, and tw plus maximum temporal degree.

Conclusion: The paper concludes that while treewidth (tw) and temporal path number (tpn) alone are insufficient for fixed parameter tractability (FPT) of maximum temporal connected components (tccs) problems, combinations of structural and temporal parameters can lead to FPT solutions.

Abstract: We study the parameterized complexity of maximum temporal connected
components (tccs) in temporal graphs, i.e., graphs that deterministically
change over time. In a tcc, any pair of vertices must be able to reach each
other via a time-respecting path. We consider both problems of maximum open
tccs (openTCC), which allow temporal paths through vertices outside the
component, and closed tccs (closedTCC) which require at least one temporal path
entirely within the component for every pair. We focus on the structural
parameter of treewidth, tw, and the recently introduced temporal parameter of
temporal path number, tpn, which is the minimum number of paths needed to fully
describe a temporal graph. We prove that these parameters on their own are not
sufficient for fixed parameter tractability: both openTCC and closedTCC are
NP-hard even when tw=9, and closedTCC is NP-hard when tpn=6. In contrast, we
prove that openTCC is in XP when parameterized by tpn. On the positive side, we
show that both problem become fixed parameter tractable under various
combinations of structural and temporal parameters that include, tw plus tpn,
tw plus the lifetime of the graph, and tw plus the maximum temporal degree.

</details>


### [78] [Improved Streaming Algorithm for Fair $k$-Center Clustering](https://arxiv.org/abs/2510.05937)
*Longkun Guo,Zeyu Lin,Chaoqi Jia,Chao Chen*

Main category: cs.DS

TL;DR: 本文提出了一种流式算法，用于公平k-中心聚类问题，实现了紧致的近似比，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，如何在k-中心聚类问题中融入公平性约束（尤其是大数据流式场景）是一个重要挑战。本文旨在解决这一问题，确保每个m个人口组别在中心点数量上有公平上限。

Method: 本文提出了一种基于λ-独立中心集的一遍流式算法，首先在流过程中计算保留点集，随后在流后处理阶段通过分析三种可能情况选择中心点，并将最复杂情况转化为辅助图中的特殊约束顶点覆盖问题。

Result: 算法在流式场景下实现了5-近似比，内存消耗为O(k log n)。离线场景下达到3-近似比，与当前最优结果一致。半结构化数据流中，m=2时为3-近似比，一般m时为4-近似比。实验表明算法在聚类成本和运行效率上优于现有基线。

Conclusion: 本文提出的流式算法在公平性约束下实现了紧致的5-近似比，并展示了在离线场景下的3-近似比，与现有最优结果匹配。此外，针对半结构化数据流，算法在m=2时达到3-近似比，一般m时达到4-近似比。实验验证了算法在聚类成本和运行效率上的优越性。

Abstract: Many real-world applications pose challenges in incorporating fairness
constraints into the $k$-center clustering problem, where the dataset consists
of $m$ demographic groups, each with a specified upper bound on the number of
centers to ensure fairness. Focusing on big data scenarios, this paper
addresses the problem in a streaming setting, where data points arrive one by
one sequentially in a continuous stream. Leveraging a structure called the
$\lambda$-independent center set, we propose a one-pass streaming algorithm
that first computes a reserved set of points during the streaming process.
Then, for the post-streaming process, we propose an approach for selecting
centers from the reserved point set by analyzing all three possible cases,
transforming the most complicated one into a specially constrained vertex cover
problem in an auxiliary graph. Our algorithm achieves a tight approximation
ratio of 5 while consuming $O(k\log n)$ memory. It can also be readily adapted
to solve the offline fair $k$-center problem, achieving a 3-approximation ratio
that matches the current state of the art. Furthermore, we extend our approach
to a semi-structured data stream, where data points from each group arrive in
batches. In this setting, we present a 3-approximation algorithm for $m = 2$
and a 4-approximation algorithm for general $m$. Lastly, we conduct extensive
experiments to evaluate the performance of our approaches, demonstrating that
they outperform existing baselines in both clustering cost and runtime
efficiency.

</details>


### [79] [Efficient Heuristics and Exact Methods for Pairwise Interaction Sampling](https://arxiv.org/abs/2510.05955)
*Sándor P. Fekete,Phillip Keldenich,Dominik Krupke,Michael Perk*

Main category: cs.DS

TL;DR: 论文提出新方法解决软件测试中的大规模配置优化问题，理论创新加实践突破，处理了前所未有的规模实例。


<details>
  <summary>Details</summary>
Motivation: 解决现代可配置软件系统（如汽车行业）中测试优化的基础问题，特别是针对大规模配置空间的挑战。

Method: 结合理论分析（如证明BH-hardness）和实际算法优化，解决大规模配置空间中的优化问题。

Result: 成功解决了公开基准集中最大的实例（约500000000个可行交互），并达到了可证明的最优解，而先前方法甚至无法计算可行解。

Conclusion: 该论文通过理论创新和实际贡献，解决了软件工程中长期存在的优化问题，能够处理大规模配置空间，并实现了前所未有的性能提升。

Abstract: We consider a class of optimization problems that are fundamental to testing
in modern configurable software systems, e.g., in automotive industries. In
pairwise interaction sampling, we are given a (potentially very large)
configuration space, in which each dimension corresponds to a possible Boolean
feature of a software system; valid configurations are the satisfying
assignments of a given propositional formula $\varphi$. The objective is to
find a minimum-sized family of configurations, such that each pair of features
is jointly tested at least once. Due to its relevance in Software Engineering,
this problem has been studied extensively for over 20 years. In addition to new
theoretical insights (we prove BH-hardness), we provide a broad spectrum of key
contributions on the practical side that allow substantial progress for the
practical performance. Remarkably, we are able to solve the largest instances
we found in published benchmark sets (with about 500000000 feasible
interactions) to provable optimality. Previous approaches were not even able to
compute feasible solutions.

</details>


### [80] [Fast-Convergent Proximity Graphs for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2510.05975)
*Binhong Li,Xiao Yan,Shangqi Lu*

Main category: cs.DS

TL;DR: 论文提出α-CG和α-CNG方法，通过边剪枝规则优化高维ANN搜索，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 高维度量空间中的近似最近邻（ANN）搜索是一个基础问题，现有PG方法缺乏理论保证，尤其是在最坏情况下。

Method: 通过引入α-收敛图（α-CG）和α-收敛邻域图（α-CNG），采用精心设计的边剪枝规则，优化索引构建时间。

Result: α-CNG在真实数据集上表现优异，距离计算和搜索步骤分别减少了15%和45%以上。

Conclusion: 论文提出的α-CNG方法在真实数据集上表现优于现有的PG方法，显著减少了距离计算和搜索步骤。

Abstract: Approximate nearest neighbor (ANN) search in high-dimensional metric spaces
is a fundamental problem with many applications. Over the past decade,
proximity graph (PG)-based indexes have demonstrated superior empirical
performance over alternatives. However, these methods often lack theoretical
guarantees regarding the quality of query results, especially in the worst-case
scenarios. In this paper, we introduce the {\alpha}-convergent graph
({\alpha}-CG), a new PG structure that employs a carefully designed edge
pruning rule. This rule eliminates candidate neighbors for each data point p by
applying the shifted-scaled triangle inequalities among p, its existing
out-neighbors, and new candidates. If the distance between the query point q
and its exact nearest neighbor v* is at most {\tau} for some constant {\tau} >
0, our {\alpha}-CG finds the exact nearest neighbor in poly-logarithmic time,
assuming bounded intrinsic dimensionality for the dataset; otherwise, it can
find an ANN in the same time. To enhance scalability, we develop the
{\alpha}-convergent neighborhood graph ({\alpha}-CNG), a practical variant that
applies the pruning rule locally within each point's neighbors. We also
introduce optimizations to reduce the index construction time. Experimental
results show that our {\alpha}-CNG outperforms existing PGs on real-world
datasets. For most datasets, {\alpha}-CNG can reduce the number of distance
computations and search steps by over 15% and 45%, respectively, when compared
with the best-performing baseline.

</details>


### [81] [A Finer View of the Parameterized Landscape of Labeled Graph Contractions](https://arxiv.org/abs/2510.06102)
*Yashaswini Mathur,Prafullkumar Tale*

Main category: cs.DS

TL;DR: 本文提出了一个构造性固定参数算法用于\textsc{Labeled Contractibility}问题，并证明了其算法的最优性。还研究了其他参数化方法并给出了硬度结果。


<details>
  <summary>Details</summary>
Motivation: 研究\textsc{Labeled Contractibility}问题的参数化复杂性，特别是针对树宽（tree-width）和退化度（degeneracy）等参数，以提供更高效的算法和硬度结果。

Method: 本文提出了一个构造性的固定参数算法，运行时间为\(2^{\mathcal{O}(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\)。此外，还分析了暴力算法的运行时间，并证明了其最优性。

Result: 提出了一个匹配算法和硬度结果的构造性固定参数算法，并证明了该问题在输入图具有有界最大度时仍然是\NP-完全的。

Conclusion: 本文提出了一个构造性的固定参数算法，用于解决\textsc{Labeled Contractibility}问题，并证明了除非指数时间假设（ETH）失败，否则不存在运行时间为\(2^{o(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\)的算法。此外，还加强了现有的硬度结果，并研究了其他参数化方法。

Abstract: We study the \textsc{Labeled Contractibility} problem, where the input
consists of two vertex-labeled graphs $G$ and $H$, and the goal is to determine
whether $H$ can be obtained from $G$ via a sequence of edge contractions.
  Lafond and Marchand~[WADS 2025] initiated the parameterized complexity study
of this problem, showing it to be \(\W[1]\)-hard when parameterized by the
number \(k\) of allowed contractions. They also proved that the problem is
fixed-parameter tractable when parameterized by the tree-width \(\tw\) of
\(G\), via an application of Courcelle's theorem resulting in a
non-constructive algorithm.
  In this work, we present a constructive fixed-parameter algorithm for
\textsc{Labeled Contractibility} with running time \(2^{\mathcal{O}(\tw^2)}
\cdot |V(G)|^{\mathcal{O}(1)}\). We also prove that unless the Exponential Time
Hypothesis (\ETH) fails, it does not admit an algorithm running in time
\(2^{o(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\). This result adds
\textsc{Labeled Contractibility} to a small list of problems that admit such a
lower bound and matching algorithm.
  We further strengthen existing hardness results by showing that the problem
remains \NP-complete even when both input graphs have bounded maximum degree.
We also investigate parameterizations by \((k + \delta(G))\) where
\(\delta(G)\) denotes the degeneracy of \(G\), and rule out the existence of
subexponential-time algorithms. This answers question raised in Lafond and
Marchand~[WADS 2025]. We additionally provide an improved \FPT\ algorithm with
better dependence on \((k + \delta(G))\) than previously known. Finally, we
analyze a brute-force algorithm for \textsc{Labeled Contractibility} with
running time \(|V(H)|^{\mathcal{O}(|V(G)|)}\), and show that this running time
is optimal under \ETH.

</details>


### [82] [Local Search-based Individually Fair Clustering with Outliers](https://arxiv.org/abs/2510.06130)
*Binita Maity,Shrutimoy Das,Anirban Dasgupta*

Main category: cs.DS

TL;DR: 提出一种处理离群点的个体公平聚类算法，通过局部搜索优化非离群点的聚类效果，实验验证有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有个体公平聚类定义在存在离群点时可能导致非离群点的中心集次优，因此需要一种方法来处理离群点并优化非离群点的聚类效果。

Method: 采用随机化局部搜索的变体来处理大规模数据集，确保算法的可扩展性。同时，提供了方法的近似保证和丢弃离群点数量的界限。

Result: 实验证明该方法在真实数据集上有效，且具有可扩展性。同时，提供了理论上的近似保证和离群点数量界限。

Conclusion: 本文提出了一种基于局部搜索的算法，用于在存在离群点的情况下实现个体公平聚类。通过丢弃被标记为离群点的数据点，并计算剩余非离群点的公平中心集，该方法在保证公平性的同时提高了聚类效果。实验验证了该方法的有效性和可扩展性。

Abstract: In this paper, we present a local search-based algorithm for individually
fair clustering in the presence of outliers. We consider the individual
fairness definition proposed in Jung et al., which requires that each of the
$n$ points in the dataset must have one of the $k$ centers within its $n/k$
nearest neighbors. However, if the dataset is known to contain outliers, the
set of fair centers obtained under this definition might be suboptimal for
non-outlier points. In order to address this issue, we propose a method that
discards a set of points marked as outliers and computes the set of fair
centers for the remaining non-outlier points. Our method utilizes a randomized
variant of local search, which makes it scalable to large datasets. We also
provide an approximation guarantee of our method as well as a bound on the
number of outliers discarded. Additionally, we demonstrate our claims
experimentally on a set of real-world datasets.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [83] [Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN](https://arxiv.org/abs/2510.05255)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: MS3M是一种轻量级预测模型，适用于6G O-RAN的实时控制，显著降低延迟并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在6G O-RAN中，如何在近实时（Near-RT）延迟和计算约束下提供控制级预测，同时应对多时间尺度动态的挑战。

Method: 提出了一种轻量级的多尺度结构化状态空间混合（MS3M）预测器，结合HiPPO-LegS核捕捉多时间尺度无线电动态，并通过双线性离散化和因果脉冲响应实现稳定的离散状态空间模型。

Result: MS3M在O-RAN测试床上实现了0.057秒的每次推理延迟和0.70M参数，比Transformer基线低3-10倍延迟，同时保持竞争性准确性。

Conclusion: MS3M模型在保持竞争力的准确性的同时，显著降低了延迟和参数数量，适用于6G O-RAN的实时控制需求。

Abstract: In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive
control is preferable. A key open challenge is delivering control-grade
predictions within Near-Real-Time (Near-RT) latency and computational
constraints under multi-timescale dynamics. We therefore cast RAN Intelligent
Controller (RIC) analytics as an agentic perceive-predict xApp that turns
noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE)
key performance indicator (KPI) forecasts to drive anticipatory control. In
this regard, Transformers are powerful for sequence learning and time-series
forecasting, but they are memory-intensive, which limits Near-RT RIC use.
Therefore, we need models that maintain accuracy while reducing latency and
data movement. To this end, we propose a lightweight Multi-Scale Structured
State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture
multi-timescale radio dynamics. We develop stable discrete state-space models
(SSMs) via bilinear (Tustin) discretization and apply their causal impulse
responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating
dynamically reweights KPI channels as conditions change, and a compact gated
channel-mixing layer models cross-feature nonlinearities without
Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received
Power (RSRP) serves as a canonical use case -- and is trained on sliding
windows to predict the immediate next step. Empirical evaluations conducted
using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across
13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s
per-inference latency with 0.70M parameters, yielding 3-10x lower latency than
the Transformer baselines evaluated on the same hardware, while maintaining
competitive accuracy.

</details>


### [84] [Impact of Packet Loss and Timing Errors on Scheduled Periodic Traffic with Time-Aware Shaping (TAS) in Time-Sensitive Networking (TSN)](https://arxiv.org/abs/2510.05290)
*Manuel Eppler,Steffen Lindner,Lukas Osswald,Thomas Stüber,Michael Menth*

Main category: cs.NI

TL;DR: 本文揭示了TSN中高优先级流量传输时隙假设的潜在问题，提出延长或取消限制可避免故障帧导致的延迟和丢包。


<details>
  <summary>Details</summary>
Motivation: 研究时间敏感网络（TSN）中高优先级流量传输时隙的假设在实际应用中的潜在问题，特别是当帧出现错误时。

Method: 通过构建最小示例说明错误帧对单链路的影响，并模拟单个延迟帧如何在多链路上导致帧丢失。

Result: 发现错误帧可能导致长排队延迟或丢包，且这种影响会通过网络传播引发远程问题。

Conclusion: 本文指出，对时间感知整形器（TAS）中高优先级流量传输时隙的假设可能导致严重问题，如长排队延迟或丢包，并提出了通过延长或取消高优先级流量的传输时隙限制来缓解或避免这些问题。

Abstract: Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the
realtime transmission capability of Ethernet networks. TSN combines priority
queuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic
traffic with ultra-low latency and jitter. That is, so-called Talkers send
periodic traffic with highest priority according to a schedule. The schedule is
designed such that the scheduled traffic is forwarded by the TSN bridges with
no or only little queuing delay. To protect that traffic against other frames,
the TAS is configured on all interfaces such that lower-priority queues can
send only when high-priority traffic is not supposed to be forwarded. In the
literature on scheduling algorithms for the TAS there is mostly the explicit or
implicit assumption that the TAS also limits transmission slots of
high-priority traffic.
  In this paper we show that this assumption can lead to tremendous problems
like very long queuing delay or even packet loss in case of faulty frames. A
faulty frame arrives too early or too late according to the schedule, it is
missing or additional. We construct minimal examples to illustrate basic
effects of faulty frames on a single link and demonstrate how this effect can
propagate through the networks and cause remote problems. We further show using
simulations that a single slightly delayed frame may lead to frame loss on
multiple links. We show that these problems can be alleviated or avoided when
TAS-based transmission slots for high-priority traffic are configured longer
than needed or if they are not limited at all.

</details>


### [85] [Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks](https://arxiv.org/abs/2510.05625)
*Yao Zhang,Yuchen Song,Shengnan Li,Yan Shi,Shikui Shen,Xiongyan Tang,Min Zhang,Danshi Wang*

Main category: cs.NI

TL;DR: 本文提出了一种GenAI驱动的分层多智能体框架，用于零接触光网络的多任务自主管理，并通过实际案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 光网络作为宽带通信的支柱，需要高水平的自主操作和零接触管理以适应其不断扩大的网络规模和传输带宽。然而，现有单智能体GenAI系统难以应对光网络生命周期管理中的多任务和多层协作挑战。

Method: 提出了一个GenAI驱动的分层多智能体框架，并展示了其架构、实现和应用。通过一个实际部署的网状网络，演示了光网络生命周期中的三个典型场景。

Result: 案例研究展示了多智能体框架在多任务分配、协调、执行、评估和总结方面的能力。

Conclusion: 本文提出了一种GenAI驱动的分层多智能体框架，为零接触光网络的多任务自主执行提供了解决方案，为未来智能、高效、协作的网络管理发展铺平了道路。

Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has
catalyzed a transformative technological revolution across all walks of life.
As the backbone of wideband communication, optical networks are expecting
high-level autonomous operation and zero-touch management to accommodate their
expanding network scales and escalating transmission bandwidth. The integration
of GenAI is deemed as the pivotal solution for realizing zero-touch optical
networks. However, the lifecycle management of optical networks involves a
multitude of tasks and necessitates seamless collaboration across multiple
layers, which poses significant challenges to the existing single-agent GenAI
systems. In this paper, we propose a GenAI-driven hierarchical multi-agent
framework designed to streamline multi-task autonomous execution for zero-touch
optical networks. We present the architecture, implementation, and applications
of this framework. A field-deployed mesh network is utilized to demonstrate
three typical scenarios throughout the lifecycle of optical network: quality of
transmission estimation in the planning stage, dynamic channel adding/dropping
in the operation stage, and system capacity increase in the upgrade stage. The
case studies, illustrate the capabilities of multi-agent framework in
multi-task allocation, coordination, execution, evaluation, and summarization.
This work provides a promising approach for the future development of
intelligent, efficient, and collaborative network management solutions, paving
the way for more specialized and adaptive zero-touch optical networks.

</details>


### [86] [On Enhancing Delay SLAs in TCP Networks through Joint Routing and Transport Assistant Deployment](https://arxiv.org/abs/2510.05686)
*José Gómez-delaHiz,Mohamed Faten Zhani,Jaime Galán-Jiménez,John Kaippallimalil*

Main category: cs.NI

TL;DR: 本文提出联合路由和Transport Assistant（TA）部署的方法，通过ILP和启发式算法优化数据包交付延迟和部署成本，模拟结果显示延迟减少16.4%，成本降低60.98%。


<details>
  <summary>Details</summary>
Motivation: TCP的重传机制在数据包丢失时会导致高延迟，现有研究提出的Transport Assistant（TA）虽能缓存和重传丢失的数据包以减少延迟，但缺乏与路由的联合优化。

Method: 通过整数线性规划（ILP）为两种场景（尽力而为网络和QoS网络）制定了联合路由和TA部署问题，并提出了针对大规模问题的启发式解决方案。

Result: 广泛的模拟结果表明，联合路由和TA部署能显著减少数据包交付延迟并降低部署成本。

Conclusion: 联合路由和TA部署在减少数据包交付延迟（高达16.4%）和降低部署成本（高达60.98%）方面表现出显著优势。

Abstract: The Transport Control Protocol has long been the primary transport protocol
for applications requiring performance and reliability over the Internet.
Unfortunately, due its retransmission mechanism, TCP incurs high packet
delivery delays when segments are lost. To address this issue, previous
research proposed to use a novel network function, namely Transport Assistant,
deployed within the network to cache and retransmit lost packets, thus reducing
retransmission delays. In this paper, we propose to jointly route the flows and
deploy TAs in order to minimize packet delivery delays in best-effort networks
(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based
networks (scenario 2). We hence formulate the joint routing and TA deployment
problem as Integer Linear Program for the two scenarios and propose a heuristic
solution for large-scale instances of the problem. Through extensive
simulations, we demonstrate the benefits of performing joint routing flows and
TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing
deployment costs (up to 60.98%).

</details>


### [87] [A Deep Q-Network based power control mechanism to Minimize RLF driven Handover Failure in 5G Network](https://arxiv.org/abs/2510.05762)
*Kotha Kartheek,Shankar K. Ghosh,Megha Iyengar,Vinod Sharma,Souvik Deb*

Main category: cs.NI

TL;DR: 本文提出基于DQN的功率控制算法，通过优化功率调整减少RLF驱动的HF，仿真显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无线链路失败（RLF）是导致切换失败（HF）的主要原因，但现有切换算法设计中对RLF的影响关注不足。

Method: 采用深度强化学习（DRL）方法，结合切换参数（如准备时间、执行时间、准备偏移、执行偏移）和无线链路监测参数（T310和N310）作为输入，设计功率控制算法。

Result: 仿真结果表明，与传统条件切换相比，结合所提DRL功率控制算法能显著减少RLF和HF。

Conclusion: 本文提出了一种基于深度Q网络（DQN）的高效功率控制机制，显著减少了无线链路失败（RLF）和随后的切换失败（HF）。

Abstract: The impact of Radio link failure (RLF) has been largely ignored in designing
handover algorithms, although RLF is a major contributor towards causing
handover failure (HF). RLF can cause HF if it is detected during an ongoing
handover. The objective of this work is to propose an efficient power control
mechanism based on Deep Q-Network (DQN), considering handover parameters (i.e.,
time-to-preparation, time-to-execute, preparation offset, execution offset) and
radio link monitoring parameters (T310 and N310) as input. The proposed DRL
based power control algorithm decides on a possible increase of transmitting
power to avoid RLF driven HF. Simulation results show that the traditional
conditional handover, when equipped with the proposed DRL based power control
algorithm can significantly reduce both RLFs and subsequent HFs, as compared to
the existing state of the art approaches.

</details>


### [88] [Leveraging Generative AI for large-scale prediction-based networking](https://arxiv.org/abs/2510.05797)
*Mathias Thorsager,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: 利用GenAI重新定义网络层，提升吞吐量和降低延迟，需解决大规模扩展问题，提出了三种支持方向。


<details>
  <summary>Details</summary>
Motivation: 传统网络层的端到端路由方式存在吞吐量限制和不可控延迟问题，通过利用GenAI的预测能力，可以重新定义网络层角色，提升性能。

Method: 提出了三种方向来支持大规模使用GenAI作为网络层工具：设计高效的初始化协议以选择提示大小；将GenAI用作确保数据及时交付的工具；以及作为传统TCP拥塞控制算法的替代方案。

Result: 在实时图像内容传输场景中，GenAI辅助的网络节点已被证明可以将到达目的地的流量提升超过100%。

Conclusion: 通过利用生成式AI（GenAI）重新定义网络层的角色，可以显著提升网络吞吐量和降低延迟，特别是在实时图像内容传输场景中。然而，要实现这一转变，需要解决在大规模用户和多数据模态下的扩展问题。

Abstract: The traditional role of the network layer is to create an end-to-end route,
through which the intermediate nodes replicate and forward the packets towards
the destination. This role can be radically redefined by exploiting the power
of Generative AI (GenAI) to pivot towards a prediction-based network layer,
which addresses the problems of throughput limits and uncontrollable latency.
In the context of real-time delivery of image content, the use of GenAI-aided
network nodes has been shown to improve the flow arriving at the destination by
more than 100%. However, to successfully exploit GenAI nodes and achieve such
transition, we must provide solutions for the problems which arise as we scale
the networks to include large amounts of users and multiple data modalities
other than images. We present three directions that play a significant role in
enabling the use of GenAI as a network layer tool at a large scale. In terms of
design, we emphasize the need for initialization protocols to select the prompt
size efficiently. Next, we consider the use case of GenAI as a tool to ensure
timely delivery of data, as well as an alternative to traditional TCP
congestion control algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](https://arxiv.org/abs/2510.05106)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 本文通过信息论分析揭示了规则格式对LLM注意力机制的影响，提出了锚点设计和动态验证的优化方法，以提升安全性和合规性。


<details>
  <summary>Details</summary>
Motivation: 设计基于大型语言模型（LLM）的安全关键智能体需要超越简单的提示工程，因此需要深入分析规则编码如何影响注意力机制和合规行为。

Method: 通过多种注意力架构（包括因果、双向、局部稀疏、核化和交叉注意力机制）的形式分析，结合动态规则验证架构，研究了规则编码对注意力机制和合规行为的影响。

Result: 研究表明，低语法熵和高集中锚点的规则格式能降低注意力熵并提高指针保真度，但揭示了锚点冗余与注意力熵之间的基本权衡。动态规则验证架构的加入提高了合规输出的渐近概率。

Conclusion: 本研究强调了原则性锚点设计和双重执行机制的必要性，以在动态领域中保护基于LLM的智能体免受提示注入攻击，同时保持合规性。

Abstract: The design of safety-critical agents based on large language models (LLMs)
requires more than simple prompt engineering. This paper presents a
comprehensive information-theoretic analysis of how rule encodings in system
prompts influence attention mechanisms and compliance behaviour. We demonstrate
that rule formats with low syntactic entropy and highly concentrated anchors
reduce attention entropy and improve pointer fidelity, but reveal a fundamental
trade-off between anchor redundancy and attention entropy that previous work
failed to recognize. Through formal analysis of multiple attention
architectures including causal, bidirectional, local sparse, kernelized, and
cross-attention mechanisms, we establish bounds on pointer fidelity and show
how anchor placement strategies must account for competing fidelity and entropy
objectives. Combining these insights with a dynamic rule verification
architecture, we provide a formal proof that hot reloading of verified rule
sets increases the asymptotic probability of compliant outputs. These findings
underscore the necessity of principled anchor design and dual enforcement
mechanisms to protect LLM-based agents against prompt injection attacks while
maintaining compliance in evolving domains.

</details>


### [90] [Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study](https://arxiv.org/abs/2510.05107)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: SCL是一种分离推理、内存和控制的语言模型架构，在多步任务中表现优于现有方法，任务成功率和可靠性更高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然提升了自然语言理解和生成能力，但作为自主代理使用时，其多步任务的架构设计存在挑战。现有框架通常将推理、内存和控制混合在单一提示中，可能降低连贯性和可预测性。

Method: 研究引入了结构化认知循环（SCL）作为替代架构，将推理、内存和控制功能分离。SCL中，语言模型专注于推理，内存由外部维护，执行由轻量级控制器在目标导向的循环中指导。

Result: SCL在360个测试场景中表现优于基线模型（如ReAct和LangChain代理），任务成功率平均为86.3%，而基线为70-77%。SCL在目标忠实度、减少冗余调用、可靠重用中间状态以及减少每100次工具调用中的无效断言方面表现更优。

Conclusion: 该研究初步表明，架构分离可以提高可靠性和可追溯性，而无需依赖更大的模型或更重的提示。这些发现旨在指导未来更广泛的模型、更长的时间范围、多模态任务和协作环境的研究。

Abstract: Large language models have advanced natural language understanding and
generation, yet their use as autonomous agents raises architectural challenges
for multi-step tasks. Existing frameworks often intertwine inference, memory,
and control in a single prompt, which can reduce coherence and predictability.
The Structured Cognitive Loop (SCL) is introduced as an alternative
architecture that separates these functions. In SCL, the language model is
dedicated to inference, memory is maintained externally, and execution is
guided by a lightweight controller within a goal-directed loop. This design
offloads cognitive load from the model and allows intermediate results to be
stored, revisited, and checked before actions are taken, providing a clearer
basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common
LangChain agents across three scenarios: temperature-based travel planning,
email drafting with conditional send, and constraint-guided image generation.
All systems share the same base model and tools under matched decoding
settings. Across 360 episodes, SCL shows modest but consistent improvements.
Task success averages 86.3 percent compared with 70-77 percent for baselines.
Goal fidelity is higher, redundant calls are fewer, intermediate states are
reused more reliably, and unsupported assertions per 100 tool calls are
reduced. Ablations show that external memory and control each contribute
independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability
and traceability without relying on larger models or heavier prompts. The
findings are preliminary and intended to guide extended studies with additional
models, longer horizons, multimodal tasks, and collaborative settings.

</details>


### [91] [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
*Yansen Zhang,Qingcan Kang,Yujie Chen,Yufei Wang,Xiongwei Han,Tao Zhong,Mingxuan Yuan,Chen Ma*

Main category: cs.AI

TL;DR: SAC-Opt通过语义锚驱动的反向校正框架，显著提升LLM优化建模的准确性和鲁棒性，解决了现有方法因单次生成和有限后处理导致的语义错误问题残疾。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单次前向生成和有限的antium求解器错误后处理，导致未检测的语义错误，生成语法正确但逻辑有误的模型。

Method: 提出了SAC-Opt框架，通过反向引导的校正方法，基于语义锚而非求解器反馈进行优化建模。每一步都对齐原始语义锚与生成代码重构的锚点，选择性校正不匹配的部分。

Result: 在七个公共数据集上，SAC-Opt平均建模准确率提升7.8%，在ComplexLP数据集上最高提升21.9%。

Conclusion: SAC-Opt 框架通过语义锚驱动的校正方法，显著提升了基于LLM的优化建模的准确性和鲁棒性，确保了问题意图到可执行代码的忠实转换。

Abstract: Large language models (LLMs) have opened new paradigms in optimization
modeling by enabling the generation of executable solver code from natural
language descriptions. Despite this promise, existing approaches typically
remain solver-driven: they rely on single-pass forward generation and apply
limited post-hoc fixes based on solver error messages, leaving undetected
semantic errors that silently produce syntactically correct but logically
flawed models. To address this challenge, we propose SAC-Opt, a backward-guided
correction framework that grounds optimization modeling in problem semantics
rather than solver feedback. At each step, SAC-Opt aligns the original semantic
anchors with those reconstructed from the generated code and selectively
corrects only the mismatched components, driving convergence toward a
semantically faithful model. This anchor-driven correction enables fine-grained
refinement of constraint and objective logic, enhancing both fidelity and
robustness without requiring additional training or supervision. Empirical
results on seven public datasets demonstrate that SAC-Opt improves average
modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP
dataset. These findings highlight the importance of semantic-anchored
correction in LLM-based optimization workflows to ensure faithful translation
from problem intent to solver-executable code.

</details>


### [92] [Structuring Reasoning for Complex Rules Beyond Flat Representations](https://arxiv.org/abs/2510.05134)
*Zhihao Yang,Ancheng Xu,Jingpeng Li,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Ahmadreza Argha,Hamid Alinejad-Rokny,Minghuan Tan,Yujun Cai,Min Yang*

Main category: cs.AI

TL;DR: DAT框架通过三阶段推理机制（定性分析、证据收集、裁决）提升语言模型处理复杂规则的能力，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂规则系统时存在局限性，现有方法如CoT缺乏结构化处理规则的方法，易导致错误传播。

Method: DAT框架将推理机制分为定性分析、证据收集和裁决三个阶段，系统化处理规则依赖关系。

Result: DAT在复杂规则任务中表现优于传统CoT方法，且使较小模型能媲美更大模型的性能。

Conclusion: DAT框架显著提升了语言模型处理复杂规则系统的能力，使较小模型能够媲美甚至超越更大模型的性能。

Abstract: Large language models (LLMs) face significant challenges when processing
complex rule systems, as they typically treat interdependent rules as
unstructured textual data rather than as logically organized frameworks. This
limitation results in reasoning divergence, where models often overlook
critical rule dependencies essential for accurate interpretation. Although
existing approaches such as Chain-of-Thought (CoT) reasoning have shown
promise, they lack systematic methodologies for structured rule processing and
are particularly susceptible to error propagation through sequential reasoning
chains. To address these limitations, we propose the Dynamic Adjudication
Template (DAT), a novel framework inspired by expert human reasoning processes.
DAT structures the inference mechanism into three methodical stages:
qualitative analysis, evidence gathering, and adjudication. During the
qualitative analysis phase, the model comprehensively evaluates the contextual
landscape. The subsequent evidence gathering phase involves the targeted
extraction of pertinent information based on predefined template elements
([placeholder]), followed by systematic verification against applicable rules.
Finally, in the adjudication phase, the model synthesizes these validated
components to formulate a comprehensive judgment. Empirical results demonstrate
that DAT consistently outperforms conventional CoT approaches in complex
rule-based tasks. Notably, DAT enables smaller language models to match, and in
some cases exceed, the performance of significantly larger LLMs, highlighting
its efficiency and effectiveness in managing intricate rule systems.

</details>


### [93] [An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem](https://arxiv.org/abs/2510.05153)
*Zhangchi Liu*

Main category: cs.AI

TL;DR: 本文通过算法信息论为符号接地问题提供了统一框架，证明了接地是受信息论限制的过程，并揭示了算法学习过程的局限性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决符号接地问题（SGP），通过算法信息论（AIT）提供一个统一的框架，以整合Gödelian（自指）和No Free Lunch（统计）视角。

Method: 作者将符号系统建模为通用图灵机，并将接地定义为信息压缩的行为。通过四个步骤论证：1）证明纯符号系统无法接地几乎所有可能的“世界”；2）展示静态接地系统的固有局限性；3）证明接地行为是不可推断的；4）利用Chaitin不完备性定理证明算法学习过程的局限性。

Result: 作者证明了意义的接地是一个受信息论限制的过程，任何算法学习过程都无法超越其自身的信息论限制，从而揭示了符号接地问题的根本局限性。

Conclusion: 本文通过算法信息论（AIT）为符号接地问题（SGP）提供了一个统一的框架，证明了意义的接地是一个受信息论限制的过程，并揭示了任何算法学习过程都无法超越其自身的信息论限制。

Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding
Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT).
We demonstrate that the grounding of meaning is a process fundamentally
constrained by information-theoretic limits, thereby unifying the G\"odelian
(self-reference) and No Free Lunch (statistical) perspectives. We model a
symbolic system as a universal Turing machine and define grounding as an act of
information compression. The argument proceeds in four stages. First, we prove
that a purely symbolic system cannot ground almost all possible "worlds" (data
strings), as they are algorithmically random and thus incompressible. Second,
we show that any statically grounded system, specialized for compressing a
specific world, is inherently incomplete because an adversarial, incompressible
world relative to the system can always be constructed. Third, the "grounding
act" of adapting to a new world is proven to be non-inferable, as it requires
the input of new information (a shorter program) that cannot be deduced from
the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to
prove that any algorithmic learning process is itself a finite system that
cannot comprehend or model worlds whose complexity provably exceeds its own.
This establishes that meaning is the open-ended process of a system perpetually
attempting to overcome its own information-theoretic limitations.

</details>


### [94] [Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework](https://arxiv.org/abs/2510.05158)
*Xin He,Liangliang You,Hongduan Tian,Bo Han,Ivor Tsang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: Lang-PINN是一个LLM驱动的多智能体系统，将自然语言描述转化为可训练的PINN，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 构建可用的PINN过程劳动密集且易错，现有LLM方法缺乏端到端视角。

Method: Lang-PINN协调四个互补的智能体：PDE Agent解析任务描述为符号PDE，PINN Agent选择架构，Code Agent生成模块化实现，Feedback Agent执行和诊断错误以进行迭代优化。

Result: 实验表明，Lang-PINN的均方误差（MSE）降低了3-5个数量级，端到端执行成功率提高了50%以上，时间开销减少了74%。

Conclusion: Lang-PINN通过多智能体系统将自然语言任务描述转化为可执行的PINN代码，显著提高了PINN的构建效率和准确性。

Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for
solving partial differential equations (PDEs), but constructing a usable PINN
remains labor-intensive and error-prone. Scientists must interpret problems as
PDE formulations, design architectures and loss functions, and implement stable
training pipelines. Existing large language model (LLM) based approaches
address isolated steps such as code generation or architecture suggestion, but
typically assume a formal PDE is already specified and therefore lack an
end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system
that builds trainable PINNs directly from natural language task descriptions.
Lang-PINN coordinates four complementary agents: a PDE Agent that parses task
descriptions into symbolic PDEs, a PINN Agent that selects architectures, a
Code Agent that generates modular implementations, and a Feedback Agent that
executes and diagnoses errors for iterative refinement. This design transforms
informal task statements into executable and verifiable PINN code. Experiments
show that Lang-PINN achieves substantially lower errors and greater robustness
than competitive baselines: mean squared error (MSE) is reduced by up to 3--5
orders of magnitude, end-to-end execution success improves by more than 50\%,
and reduces time overhead by up to 74\%.

</details>


### [95] [Representation Potentials of Foundation Models for Multimodal Alignment: A Survey](https://arxiv.org/abs/2510.05184)
*Jianglin Lu,Hailing Wang,Yi Xu,Yizhou Wang,Kuo Yang,Yun Fu*

Main category: cs.AI

TL;DR: Survey explores foundation models' representation potentials, highlighting their cross-modal transfer capabilities and discussing open challenges.


<details>
  <summary>Details</summary>
Motivation: To investigate the latent capacity of foundation models' learned representations to capture task-specific information within a single modality and enable cross-modal alignment and unification.

Method: The survey reviews representative foundation models, key metrics for alignment, and synthesizes empirical evidence from vision, language, speech, multimodality, and neuroscience studies.

Result: Foundation models often show structural regularities and semantic consistencies in their representation spaces, supporting cross-modal transfer and alignment.

Conclusion: Foundation models exhibit structural regularities and semantic consistencies, making them strong candidates for cross-modal transfer and alignment. However, challenges and open questions remain.

Abstract: Foundation models learn highly transferable representations through
large-scale pretraining on diverse data. An increasing body of research
indicates that these representations exhibit a remarkable degree of similarity
across architectures and modalities. In this survey, we investigate the
representation potentials of foundation models, defined as the latent capacity
of their learned representations to capture task-specific information within a
single modality while also providing a transferable basis for alignment and
unification across modalities. We begin by reviewing representative foundation
models and the key metrics that make alignment measurable. We then synthesize
empirical evidence of representation potentials from studies in vision,
language, speech, multimodality, and neuroscience. The evidence suggests that
foundation models often exhibit structural regularities and semantic
consistencies in their representation spaces, positioning them as strong
candidates for cross-modal transfer and alignment. We further analyze the key
factors that foster representation potentials, discuss open questions, and
highlight potential challenges.

</details>


### [96] [Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture](https://arxiv.org/abs/2510.05187)
*Mohamed El-Dosuky*

Main category: cs.AI

TL;DR: 论文提出一个六层实时框架，通过语义标注、互操作算法和推理技术，解决物联网数据管理中的语义理解和实时推理问题，特别适用于农业应用。


<details>
  <summary>Details</summary>
Motivation: 物联网在农业等领域的应用面临数据收集和理解的挑战，现有技术缺乏对数据含义和来源的深入理解。

Method: 论文提出了一个六层实时框架，包括感知层、语义标注层、互操作层、传输层、语义推理层和应用层，并提出了两种语义算法（互操作语义算法和同义词识别算法）以及使用模糊逻辑、Dempster-Shafer理论和贝叶斯网络的语义推理层。

Result: 该框架能够有效管理物联网数据，确保语义完整性，并实现实时知识推理，特别适用于动态环境。

Conclusion: 该框架通过整合不确定性推理方法和语义互操作技术，为物联网数据管理提供了鲁棒的解决方案，确保了语义完整性，并实现了实时知识推理，特别适用于农业等应用领域。

Abstract: The Internet of Things (IoT) has revolutionized various applications
including agriculture, but it still faces challenges in data collection and
understanding. This paper proposes a real-time framework with three additional
semantic layers to help IoT devices and sensors comprehend data meaning and
source. The framework consists of six layers: perception, semantic annotation,
interoperability, transportation, semantic reasoning, and application, suitable
for dynamic environments. Sensors collect data in the form of voltage, which is
then processed by microprocessors or microcontrollers in the semantic
annotation and preprocessing layer. Metadata is added to the raw data,
including the purpose, ID number, and application. Two semantic algorithms are
proposed in the semantic interoperability and ontologies layer: the
interoperability semantic algorithm for standardizing file types and the
synonym identification algorithm for identifying synonyms. In the
transportation layer, raw data and metadata are sent to other IoT devices or
cloud computing platforms using techniques like WiFi, Zigbee networks,
Bluetooth, and mobile communication networks. A semantic reasoning layer is
proposed to infer new knowledge from the existing data, using fuzzy logic,
Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI)
is proposed in the application layer to help users communicate with and monitor
IoT sensors, devices, and new knowledge inferred. This framework provides a
robust solution for managing IoT data, ensuring semantic completeness, and
enabling real-time knowledge inference. The integration of uncertainty
reasoning methods and semantic interoperability techniques makes this framework
a valuable tool for advancing IoT applications in general and in agriculture in
particular.

</details>


### [97] [Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents](https://arxiv.org/abs/2510.05188)
*Wenda Xie,Chao Guo,Yanqing Jing. Junle Wang,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: Dramaturge通过分层多LLM代理方法，分阶段审查和修订长篇叙事脚本，显著提升质量。


<details>
  <summary>Details</summary>
Motivation: 解决单次生成的LLM在长篇叙事脚本中难以识别全局结构问题和局部细节缺陷的挑战。

Method: 采用分层多LLM代理的‘分而治之’方法，包括全局审查、场景级审查和分层协调修订三个阶段。

Result: 实验表明Dramaturge在脚本整体质量和场景细节上显著优于所有基线方法。

Conclusion: Dramaturge通过分层多LLM代理的方法，显著提升了长篇叙事脚本的质量，且在脚本整体质量和场景细节上均优于现有基线方法。

Abstract: Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

</details>


### [98] [Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response](https://arxiv.org/abs/2510.05196)
*Daqian Shi,Xiaolei Diao,Jinge Wu,Honghan Wu,Xiongfeng Tang,Felix Naughton,Paulina Bondaronek*

Main category: cs.AI

TL;DR: 论文提出了一种结合图推理和大型语言模型的框架，用于高效分析半结构化人口数据，支持公共卫生决策。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19等公共卫生紧急情况下，及时准确的人口数据分析至关重要，但传统方法面临半结构化数据输入的挑战。

Method: 提出了一种新颖的基于图推理的框架，整合了大型语言模型、结构化人口属性和非结构化公众反馈，通过弱监督流程动态建模公民需求。

Result: 初步实验结果证明了该方法的可行性，能够生成可解释的见解以支持健康政策决策。

Conclusion: 该论文提出了一种基于图推理的框架，结合大型语言模型和弱监督流程，为资源受限的公共卫生决策提供了可扩展的解决方案。

Abstract: Timely and accurate analysis of population-level data is crucial for
effective decision-making during public health emergencies such as the COVID-19
pandemic. However, the massive input of semi-structured data, including
structured demographic information and unstructured human feedback, poses
significant challenges to conventional analysis methods. Manual expert-driven
assessments, though accurate, are inefficient, while standard NLP pipelines
often require large task-specific labeled datasets and struggle with
generalization across diverse domains. To address these challenges, we propose
a novel graph-based reasoning framework that integrates large language models
with structured demographic attributes and unstructured public feedback in a
weakly supervised pipeline. The proposed approach dynamically models evolving
citizen needs into a need-aware graph, enabling population-specific analyses
based on key features such as age, gender, and the Index of Multiple
Deprivation. It generates interpretable insights to inform responsive health
policy decision-making. We test our method using a real-world dataset, and
preliminary experimental results demonstrate its feasibility. This approach
offers a scalable solution for intelligent population health monitoring in
resource-constrained clinical and governmental settings.

</details>


### [99] [Efficient Prediction of Pass@k Scaling in Large Language Models](https://arxiv.org/abs/2510.05197)
*Joshua Kazdan,Rylan Schaeffer,Youssef Allouah,Colin Sullivan,Kyssen Yu,Noam Levi,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 研究提出新方法，通过稳健估计和动态采样，更准确预测AI系统在大规模尝试时的罕见风险和能力，同时节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在重复采样下能力和风险的双重提升，如何在有限采样预算下准确预测大规模尝试时的行为成为关键问题。

Method: 采用beta-二项分布构建稳健估计框架，并设计动态采样策略以优化预算分配。

Result: 提出的方法在数据受限场景下显著提升了预测准确性，并减少了计算开销。

Conclusion: 通过引入稳健的估计框架和动态采样策略，本研究显著提高了在有限数据下对AI系统罕见风险和能力的预测准确性，同时降低了计算成本。

Abstract: Assessing the capabilities and risks of frontier AI systems is a critical
area of research, and recent work has shown that repeated sampling from models
can dramatically increase both. For instance, repeated sampling has been shown
to increase their capabilities, such as solving difficult math and coding
problems, but it has also been shown to increase their potential for harm, such
as being jailbroken. Such results raise a crucial question for both capability
and safety forecasting: how can one accurately predict a model's behavior when
scaled to a massive number of attempts, given a vastly smaller sampling budget?
This question is directly relevant to model providers, who serve hundreds of
millions of users daily, and to governmental regulators, who seek to prevent
harms. To answer this questions, we make three contributions. First, we find
that standard methods for fitting these laws suffer from statistical
shortcomings that hinder predictive accuracy, especially in data-limited
scenarios. Second, we remedy these shortcomings by introducing a robust
estimation framework, which uses a beta-binomial distribution to generate more
accurate predictions from limited data. Third, we propose a dynamic sampling
strategy that allocates a greater budget to harder problems. Combined, these
innovations enable more reliable prediction of rare risks and capabilities at a
fraction of the computational cost.

</details>


### [100] [Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment](https://arxiv.org/abs/2510.05283)
*Radha Gulhane,Sathish Reddy Indurthi*

Main category: cs.AI

TL;DR: 提出混合奖励框架结合模型和规则奖励，优化MLLM对齐，实验显示性能显著提升，数学任务表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的单信号、基于模型的奖励方法在多领域任务中缺乏置信度校准，难以全面捕捉人类偏好的多样性，且需要大量数据标注和奖励模型训练。

Method: 结合模型奖励和规则奖励的混合奖励建模框架，包括多方面的奖励机制和长度惩罚奖励，以优化强化学习策略。

Result: 实验表明，混合和多方面奖励建模在不同多模态基准测试中均带来一致提升，最佳模型在3B家族中实现了整体平均9.5%的提升，数学推理任务上平均提升16%。

Conclusion: 提出的混合奖励建模框架在强化学习策略优化中展现出灵活性和有效性，显著提升了多模态大型语言模型在不同任务上的性能，特别是在数学推理任务上表现突出。

Abstract: Aligning multimodal large language models (MLLMs) with human preferences
often relies on single-signal, model-based reward methods. Such monolithic
rewards often lack confidence calibration across domain-specific tasks, fail to
capture diverse aspects of human preferences, and require extensive data
annotation and reward model training. In this work, we propose a hybrid reward
modeling framework that integrates complementary reward paradigms: (i)
model-based rewards, where a learned reward model predicts scalar or vector
scores from synthetic and human feedback, and (ii) rule-based rewards, where
domain-specific heuristics provide explicit correctness signals with
confidence. Beyond accuracy, we further incorporate multi-aspect rewards to
enforce instruction adherence and introduce a generalized length-penalty reward
to stabilize training and improve performance. The proposed framework provides
a flexible and effective approach to aligning MLLMs through reinforcement
learning policy optimization. Our experiments show consistent improvements
across different multimodal benchmarks when applying hybrid and multi-aspect
reward modeling. Our best performing model in the 3B family achieves an overall
average improvement of ~9.5% across general and math reasoning tasks. Focusing
specifically on mathematical benchmarks, the model achieves a significant
average improvement of ~16%, highlighting its effectiveness in mathematical
reasoning and problem solving.

</details>


### [101] [Do Code Models Suffer from the Dunning-Kruger Effect?](https://arxiv.org/abs/2510.05457)
*Mukul Singh,Somya Chatterjee,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: AI模型在编码任务中表现出类似人类的邓宁-克鲁格效应，能力越低越容易过度自信。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统在创意和技术领域与人类合作时可能存在的认知边界和偏见。

Method: 通过分析模型在不同编程语言中的自信程度和表现，揭示了AI模型与人类相似的过度自信模式。

Result: 实验显示，能力较低的模型和罕见编程语言中的模型表现出更强的DKE倾向，偏见强度与模型能力成比例。

Conclusion: 研究表明，AI模型在编码任务中表现出与人类相似的邓宁-克鲁格效应（DKE），即能力较低的模型在陌生或资源匮乏的领域中更容易过度自信。

Abstract: As artificial intelligence systems increasingly collaborate with humans in
creative and technical domains, questions arise about the cognitive boundaries
and biases that shape our shared agency. This paper investigates the
Dunning-Kruger Effect (DKE), the tendency for those with limited competence to
overestimate their abilities in state-of-the-art LLMs in coding tasks. By
analyzing model confidence and performance across a diverse set of programming
languages, we reveal that AI models mirror human patterns of overconfidence,
especially in unfamiliar or low-resource domains. Our experiments demonstrate
that less competent models and those operating in rare programming languages
exhibit stronger DKE-like bias, suggesting that the strength of the bias is
proportionate to the competence of the models.

</details>


### [102] [BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions](https://arxiv.org/abs/2510.05318)
*Nan Huo,Xiaohan Xu,Jinyang Li,Per Jacobsson,Shipei Lin,Bowen Qin,Binyuan Hui,Xiaolong Li,Ge Qu,Shuzheng Si,Linheng Han,Edward Alexander,Xintong Zhu,Rui Qin,Ruihan Yu,Yiyao Jin,Feige Zhou,Weihao Zhong,Yun Chen,Hongyu Liu,Chenhao Ma,Fatma Ozcan,Yannis Papakonstantinou,Reynold Cheng*

Main category: cs.AI

TL;DR: BIRD-INTERACT 是一个多轮文本到 SQL 基准测试，通过真实交互环境和挑战性任务，验证了交互在复杂任务中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决现有多轮基准测试在处理动态交互和生产级数据库助手挑战方面的不足。

Method: 引入了 BIRD-INTERACT 基准测试，包括层次化知识库、元数据文件和用户模拟器，支持模型在无监督下进行澄清、知识检索和错误恢复。

Result: GPT-5 在 c-Interact 和 a-Interact 中的任务完成率分别仅为 8.67% 和 17.00%，验证了交互对复杂动态任务的重要性。

Conclusion: BIRD-INTERACT 通过引入全面的交互环境和挑战性任务套件，显著提升了多轮文本到 SQL 任务的真实性，为复杂动态任务的有效交互提供了验证。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

</details>


### [103] [Vul-R2: A Reasoning LLM for Automated Vulnerability Repair](https://arxiv.org/abs/2510.05480)
*Xin-Cheng Wen,Zirui Lin,Yijun Yang,Cuiyun Gao,Deheng Ye*

Main category: cs.AI

TL;DR: 论文探讨了基于LLMs的自动漏洞修复方法面临的挑战：缺乏高质量漏洞相关数据和中间修复过程难以验证。


<details>
  <summary>Details</summary>
Motivation: 随着软件漏洞的指数级增长，亟需高效的自动漏洞修复解决方案。

Method: 论文提出将AVR问题建模为序列生成任务，并利用LLMs通过提示或微调直接生成漏洞修复方案。

Result: 当前方法虽表现出色，但因缺乏高质量漏洞相关推理数据和难以验证中间修复过程，限制了其性能。

Conclusion: 论文指出，当前基于大型语言模型（LLMs）的自动漏洞修复（AVR）方法虽然表现优异，但仍面临高质量漏洞相关推理数据缺乏和中间修复过程难以验证的挑战。

Abstract: The exponential increase in software vulnerabilities has created an urgent
need for automatic vulnerability repair (AVR) solutions. Recent research has
formulated AVR as a sequence generation problem and has leveraged large
language models (LLMs) to address this problem. Typically, these approaches
prompt or fine-tune LLMs to generate repairs for vulnerabilities directly.
Although these methods show state-of-the-art performance, they face the
following challenges: (1) Lack of high-quality, vulnerability-related reasoning
data. Current approaches primarily rely on foundation models that mainly encode
general programming knowledge. Without vulnerability-related reasoning data,
they tend to fail to capture the diverse vulnerability repair patterns. (2)
Hard to verify the intermediate vulnerability repair process during LLM
training. Existing reinforcement learning methods often leverage intermediate
execution feedback from the environment (e.g., sandbox-based execution results)
to guide reinforcement learning training. In contrast, the vulnerability repair
process generally lacks such intermediate, verifiable feedback, which poses
additional challenges for model training.

</details>


### [104] [Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis](https://arxiv.org/abs/2510.05335)
*Oskar Wysocki,Magdalena Wysocka,Mauricio Jacobo,Harriet Unsworth,André Freitas*

Main category: cs.AI

TL;DR: M-Reason是一个基于LLM和模块化代理的生物医学证据合成系统，专注于癌症研究，强调透明度和可审计性，显著提高了效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过透明、基于代理的推理和证据整合，提高生物医学研究的效率和输出一致性，特别是在癌症研究中。

Method: M-Reason利用大型语言模型（LLMs）和模块化代理编排，自动化地从多样化的生物医学数据源中检索、评估和合成证据。每个代理专注于特定的证据流，支持并行处理和细粒度分析。

Result: 评估显示，M-Reason在效率和输出一致性方面取得了显著提升。

Conclusion: M-Reason展示了在生物医学领域，特别是癌症研究中，作为证据合成工具的潜力，同时也是一个强大的多代理LLM系统测试平台。

Abstract: We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

</details>


### [105] [Integrating Bayesian methods with neural network--based model predictive control: a review](https://arxiv.org/abs/2510.05338)
*Asli Karacelik*

Main category: cs.AI

TL;DR: 本文综述了贝叶斯方法在MPC中的应用，指出其性能提升报告碎片化，呼吁标准化评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯方法在MPC中越来越受欢迎，但其性能和鲁棒性的提升报告仍存在碎片化、基准不一致和可靠性分析不足的问题。

Method: 系统分析了贝叶斯方法在MPC中的应用，包括神经网络建模、控制设计和不确定性量化。

Result: 研究发现贝叶斯方法在MPC中的应用存在不一致的基准和有限的可靠性分析。

Conclusion: 本文主张通过标准化基准、消融研究和透明报告来严格评估贝叶斯方法在模型预测控制（MPC）中的有效性。

Abstract: In this review, we assess the use of Bayesian methods in model predictive
control (MPC), focusing on neural-network-based modeling, control design, and
uncertainty quantification. We systematically analyze individual studies and
how they are implemented in practice. While Bayesian approaches are
increasingly adopted to capture and propagate uncertainty in MPC, reported
gains in performance and robustness remain fragmented, with inconsistent
baselines and limited reliability analyses. We therefore argue for standardized
benchmarks, ablation studies, and transparent reporting to rigorously determine
the effectiveness of Bayesian techniques for MPC.

</details>


### [106] [MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts](https://arxiv.org/abs/2510.05363)
*Abhinav Jain,Xinyu Yao,Thomas Reps,Christopher Jermaine*

Main category: cs.AI

TL;DR: MHA-RAG通过多注意力头生成软提示，性能提升20分且成本降低10倍，不受示例顺序影响。


<details>
  <summary>Details</summary>
Motivation: 探索在有限训练数据下，如何更高效、稳定地利用领域特定示例进行模型适应，避免纯文本表示的局限性。

Method: 引入Multi-Head Attention Retrieval-Augmented Generation (MHA-RAG)框架，通过注意力头数量控制软提示生成。

Result: 在多个问答基准测试中，MHA-RAG性能比标准RAG提升20分，推理成本降低10倍。

Conclusion: MHA-RAG框架通过多注意力头机制优化了软提示生成，显著提升了性能并降低了推理成本，且不受示例顺序影响。

Abstract: Adapting Foundation Models to new domains with limited training data is
challenging and computationally expensive. While prior work has demonstrated
the effectiveness of using domain-specific exemplars as in-context
demonstrations, we investigate whether representing exemplars purely as text is
the most efficient, effective, and stable approach. We explore an alternative:
representing exemplars as soft prompts with an exemplar order invariant model
architecture. To this end, we introduce Multi-Head Attention
Retrieval-Augmented Generation (MHA-RAG), a framework with the number of
attention heads serving as a simple hyperparameter to control soft
prompt-generation across different tasks. Across multiple question-answering
benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over
standard RAG, while cutting inference costs by a factor of 10X
GFLOPs-delivering both higher accuracy and greater efficiency, invariant to
exemplar order.

</details>


### [107] [What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions](https://arxiv.org/abs/2510.05378)
*Reza Habibi,Seung Wan Ha,Zhiyu Lin,Atieh Kashani,Ala Shafia,Lakshana Lakshmanarajan,Chia-Fang Chung,Magy Seif El-Nasr*

Main category: cs.AI

TL;DR: 研究发现人类与AI通过双向符号交换和重新解释共同构建意义，为人机交互设计提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 人类与AI的有效协作需要超越语言处理，深入理解符号及其社会构建意义。当前AI系统常错过对话中动态的符号解释。

Method: 基于符号互动主义理论，进行了两项研究，探讨人类与AI如何共同构建符号及其意义。

Result: 研究发现，人类会根据AI代理提出的符号和解释调整初始定义，并在引入社会背景时尤为明显。参与者还会将个人和社会价值观投射到互动中，随时间推移精炼意义。

Conclusion: 论文揭示了人类与AI通过双向符号交换和重新解释共同构建意义的重要性，为人机交互设计提供了新范式。

Abstract: Meaningful human-AI collaboration requires more than processing language; it
demands a deeper understanding of symbols and their socially constructed
meanings. While humans naturally interpret symbols through social interaction,
AI systems often miss the dynamic interpretations that emerge in conversation.
Drawing on Symbolic Interactionism theory, we conducted two studies to
investigate how humans and AI co-construct symbols and their meanings. Findings
provide empirical insights into how humans and conversational AI agents
collaboratively shape meanings during interaction. We show how participants
shift their initial definitions of meaning in response to the symbols and
interpretations suggested by the conversational AI agents, especially when
social context is introduced. We also observe how participants project their
personal and social values into these interactions, refining meanings over
time. These findings reveal that shared understanding does not emerge from mere
agreement but from the bi-directional exchange and reinterpretation of symbols,
suggesting new paradigms for human-AI interaction design.

</details>


### [108] [Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation](https://arxiv.org/abs/2510.05402)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.AI

TL;DR: 教师-学生框架用于钢硬度逆向预测，比传统方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 钢热处理后的最终硬度预测存在多对一的挑战，逆向问题尤其困难，需要一种高效且准确的方法。

Method: 使用教师-学生学习框架，先训练一个正向模型（教师）预测硬度，再训练一个逆向模型（学生）推断输入参数，并通过教师的反馈进行优化。

Result: 在公开的钢数据集上，该方法比基线回归和强化学习模型具有更高的逆向预测准确性和更低的计算时间。

Conclusion: 该论文提出的教师-学生学习框架在逆向过程建模中表现出高效性和准确性，为材料科学中的硬度预测问题提供了有效解决方案。

Abstract: Predicting the final hardness of steel after heat treatment is a challenging
regression task due to the many-to-one nature of the process -- different
combinations of input parameters (such as temperature, duration, and chemical
composition) can result in the same hardness value. This ambiguity makes the
inverse problem, estimating input parameters from a desired hardness,
particularly difficult. In this work, we propose a novel solution using a
Teacher-Student learning framework. First, a forward model (Teacher) is trained
to predict final hardness from 13 metallurgical input features. Then, a
backward model (Student) is trained to infer plausible input configurations
from a target hardness value. The Student is optimized by leveraging feedback
from the Teacher in an iterative, supervised loop. We evaluate our method on a
publicly available tempered steel dataset and compare it against baseline
regression and reinforcement learning models. Results show that our
Teacher-Student framework not only achieves higher inverse prediction accuracy
but also requires significantly less computational time, demonstrating its
effectiveness and efficiency for inverse process modeling in materials science.

</details>


### [109] [AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems](https://arxiv.org/abs/2510.05432)
*Shambhavi Mishra,Gaurav Sahu,Marco Pedersoli,Laurent Charlin,Jose Dolz,Christopher Pal*

Main category: cs.AI

TL;DR: AInstein框架测试LLM在无外部辅助下解决AI研究问题的能力，结果显示其潜力与局限并存。


<details>
  <summary>Details</summary>
Motivation: 探究LLM的成功是源于真实推理还是复杂记忆，测试其在无外部辅助下能否生成有效的AI研究问题解决方案。

Method: 引入AInstein框架，通过提取ICLR 2025论文的蒸馏问题陈述，利用专门求解器代理通过迭代批判循环提出和优化技术解决方案。

Result: LLMs能重新发现可行解决方案并偶尔提出创造性替代方案，但其问题解决能力脆弱且对框架敏感。

Conclusion: LLMs展现出一定的自主科学问题解决潜力，但其能力仍显脆弱且高度依赖于问题框架。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet it remains unclear whether such success reflects
genuine reasoning or sophisticated recall. We introduce AInstein, a framework
for testing whether LLMs can generate valid solutions to AI research problems
using only their pretrained parametric knowledge -- without domain-specific
fine-tuning, retrieval augmentation, or other external aids. Our approach
extracts distilled problem statements from high-quality ICLR 2025 submissions,
then tasks specialized solver agents with proposing and refining technical
solutions through iterative critique loops, mimicking the cycles of proposal,
review, and revision central to scientific inquiry. We evaluate AInstein on
1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),
using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by
targeted manual checks. Performance is assessed with three metrics: Success
Rate (does the solution address the problem?), Rediscovery (does it align with
human-proposed methods?), and Novelty (does it yield valid, original
approaches?). Our results reveal that while LLMs can rediscover feasible
solutions and occasionally propose creative alternatives, their problem-solving
ability remains fragile and highly sensitive to framing. These findings provide
the first large-scale evidence on the extent to which LLMs can act as
autonomous scientific problem-solvers, highlighting both their latent potential
and their current limitations.

</details>


### [110] [NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification](https://arxiv.org/abs/2510.05451)
*Fadi Al Machot,Fidaa Al Machot*

Main category: cs.AI

TL;DR: 本文提出了一种混合神经符号框架，结合ASP与Transformer，提升了多标签分类的逻辑一致性，在ASRS语料上表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型在多标签文本分类中表现优异，但常违反专家认为关键的领域逻辑，这在安全关键应用中尤为突出。

Method: 通过两种互补方式整合领域知识：一是基于规则的数据增强，生成逻辑一致的合成样本；二是作为模糊逻辑正则化器，在微调过程中以可微分形式强制规则满足。

Result: 与强基线（BCE）相比，该方法在微调和宏F1分数上均有提升，并在ASRS测试集上实现了高达86%的规则违反减少。

Conclusion: 本文提出的混合神经符号框架成功结合了答案集编程（ASP）与基于Transformer的学习，显著提升了多标签文本分类的逻辑一致性，尤其在航空安全报告系统（ASRS）语料上表现优异。

Abstract: Deep transformer models excel at multi-label text classification but often
violate domain logic that experts consider essential, an issue of particular
concern in safety-critical applications. We propose a hybrid neuro-symbolic
framework that integrates Answer Set Programming (ASP) with transformer-based
learning on the Aviation Safety Reporting System (ASRS) corpus. Domain
knowledge is formalized as weighted ASP rules and validated using the Clingo
solver. These rules are incorporated in two complementary ways: (i) as
rule-based data augmentation, generating logically consistent synthetic samples
that improve label diversity and coverage; and (ii) as a fuzzy-logic
regularizer, enforcing rule satisfaction in a differentiable form during
fine-tuning. This design preserves the interpretability of symbolic reasoning
while leveraging the scalability of deep neural architectures. We further tune
per-class thresholds and report both standard classification metrics and
logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE)
baseline, our approach improves micro- and macro-F1 scores and achieves up to
an 86% reduction in rule violations on the ASRS test set. To the best of our
knowledge, this constitutes the first large-scale neuro-symbolic application to
ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and
differentiable transformer training for trustworthy, safety-critical NLP.

</details>


### [111] [VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)
*Aman Gupta,Denny O'Shea,Fazl Barez*

Main category: cs.AI

TL;DR: VAL-Bench是一个新基准，用于评估LLMs在争议性问题中是否保持一致的价值观，发现模型间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要跟踪拒绝或预定义的安全违规，仅检查规则合规性，无法揭示模型在面对争议性现实问题时是否坚持一致的价值体系。

Method: 引入VAL-Bench，通过评估模型在公共辩论对立框架下是否保持稳定的价值立场，使用LLM-as-judge来评分配对回答的一致性或分歧。

Result: 应用VAL-Bench于领先的开源和闭源模型，揭示了在价值对齐上的巨大差异，并突出了安全策略（如拒绝）与更具表现力的价值系统之间的权衡。

Conclusion: VAL-Bench提供了一个可扩展、可复现的基准，使系统比较LLMs如何可靠地体现人类价值观成为可能。

Abstract: Large language models (LLMs) are increasingly used for tasks where outputs
shape human decisions, so it is critical to test whether their responses
reflect consistent human values. Existing benchmarks mostly track refusals or
predefined safety violations, but these only check rule compliance and do not
reveal whether a model upholds a coherent value system when facing
controversial real-world issues. We introduce the \textbf{V}alue
\textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates
whether models maintain a stable value stance across paired prompts that frame
opposing sides of public debates. VAL-Bench consists of 115K such pairs from
Wikipedia's controversial sections. A well-aligned model should express similar
underlying views regardless of framing, which we measure using an LLM-as-judge
to score agreement or divergence between paired responses. Applied across
leading open- and closed-source models, the benchmark reveals large variation
in alignment and highlights trade-offs between safety strategies (e.g.,
refusals) and more expressive value systems. By providing a scalable,
reproducible benchmark, VAL-Bench enables systematic comparison of how reliably
LLMs embody human values.

</details>


### [112] [Decade-long Emission Forecasting with an Ensemble Model in Taiwan](https://arxiv.org/abs/2510.05548)
*Gordon Hung,Salinna Abdullah*

Main category: cs.AI

TL;DR: 研究比较了21种时间序列模型预测台湾CO2排放，集成模型表现最佳（SMAPE=1.407），为政策制定提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 台湾地区人口密集且严重依赖化石燃料，导致严重的空气污染，其中二氧化碳是最主要的温室气体。因此，需要一种可靠的预测方法来帮助政策制定。

Method: 比较了21种常用时间序列模型，包括单变量和多变量方法，并采用自定义的堆叠泛化集成技术将表现最佳的模型（FFNN、SVM、RFR）与线性回归结合。

Result: 提出的集成模型SMAPE达到1.407，且无过拟合迹象，提供了准确的十年排放预测。

Conclusion: 本研究提出的集成模型在预测台湾地区二氧化碳排放方面表现出色，为政策制定者提供了数据支持，以做出更科学的决策。

Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to
severe air pollution, with the most prevalent greenhouse gas being carbon
dioxide (CO2). There-fore, this study presents a reproducible and comprehensive
case study comparing 21 of the most commonly employed time series models in
forecasting emissions, analyzing both univariate and multivariate approaches.
Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM),
and Random Forest Regressor (RFR) achieved the best performances. To further
enhance robustness, the top performers were integrated with Linear Regression
through a custom stacked generalization en-semble technique. Our proposed
ensemble model achieved an SMAPE of 1.407 with no signs of overfitting.
Finally, this research provides an accurate decade-long emission projection
that will assist policymakers in making more data-driven decisions.

</details>


### [113] [MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption](https://arxiv.org/abs/2510.05580)
*Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: MetaVLA是一种高效的后训练框架，通过元学习和多样化任务提升VLA模型的泛化能力，显著减少训练资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action (VLA)模型在具身推理中表现有限，需要任务特定微调且泛化能力不足。MetaVLA旨在解决这些问题。

Method: MetaVLA提出了一种名为Context-Aware Meta Co-Training的统一框架，通过整合多样化目标任务和辅助任务，结合源自Attentive Neural Processes的元学习机制，实现快速适应。

Result: 在LIBERO基准测试中，MetaVLA在长时任务上比OpenVLA性能提升8.0%，训练步数从240K减少到75K，GPU时间降低约76%。

Conclusion: MetaVLA展示了通过轻量级元学习机制和多样化辅助任务，能够在低资源条件下实现高效的后训练对齐，为通用具身智能体铺平了道路。

Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

</details>


### [114] [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
*Zhuofeng Li,Haoxiang Zhang,Seungju Han,Sheng Liu,Jianwen Xie,Yu Zhang,Yejin Choi,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: AgentFlow是一个可训练的代理框架，通过Flow-GRPO优化方法在多轮交互中显著提升LLM表现，尤其在搜索、代理、数学和科学任务上优于现有方法，甚至超过GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有的工具增强方法在长时程和多样化工具场景下扩展性差，泛化能力弱。代理系统通过模块化分解工作提供了有前景的替代方案，但多数缺乏在线训练或与多轮交互动态脱节。

Method: AgentFlow框架通过协调四个模块（planner、executor、verifier、generator）和动态内存，直接优化多轮交互中的planner模块。Flow-GRPO方法将多轮优化转化为一系列可处理的单轮策略更新，并通过组归一化优势稳定学习。

Result: 在十个基准测试中，AgentFlow（基于7B规模主干）在搜索、代理、数学和科学任务上的平均准确率分别提升了14.9%、14.0%、14.5%和4.1%。

Conclusion: AgentFlow框架通过Flow-GRPO优化方法，在多轮交互中显著提升了大型语言模型的任务表现，尤其在搜索、代理、数学和科学任务上表现优异，甚至超过GPT-4o等更大规模的专有模型。

Abstract: Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

</details>


### [115] [From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions](https://arxiv.org/abs/2510.05596)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Geng Sun,Xianbin Wang,Shiwen Mao,Abbas Jamalipour*

Main category: cs.AI

TL;DR: 自进化代理AI通过多代理协作框架自主优化无线系统性能，波束增益提升52.02%，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统静态AI模型无法适应环境动态变化，自进化代理AI通过自主进化周期，为未来无线系统提供持续适应和改进的能力。

Method: 提出了一种多代理协作的自进化代理AI框架，包括分层架构、生命周期管理、工具智能、工作流优化、自反思和进化学习等关键技术。通过角色专用提示和监督代理协调，实现结构化对话、迭代反馈和系统验证。

Result: 在低空无线网络（LAWNs）的天线进化案例中，框架成功将固定天线优化升级为可移动天线优化，波束增益提升高达52.02%，性能持续超越固定基线。

Conclusion: 自进化代理AI在无需人工干预的情况下，通过自主更新模型、工具和工作流程，显著提升了无线系统的性能，如波束增益提高52.02%，验证了其适应性和鲁棒性。

Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

</details>


### [116] [Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography](https://arxiv.org/abs/2510.05664)
*Hanna Kreutzer,Anne-Sophie Caselitz,Thomas Dratsch,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung*

Main category: cs.AI

TL;DR: GPT-4o能高准确度提取放射学报告标签，用于训练多标签分类模型，不确定性不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-4o从自由文本放射学报告中提取诊断标签（含不确定性）的能力，并测试这些标签如何影响多标签图像分类的性能。

Method: 回顾性研究，使用GPT-4o从放射学报告中提取标签（‘true’、‘false’或‘uncertain’），并通过重新分配‘uncertain’标签（‘inclusive’或‘exclusive’）来评估标签不确定性对多标签图像分类的影响。使用ResNet50进行多标签分类，并在内部和外部测试集上验证标签提取准确性和模型性能。

Result: 自动标签提取准确率为98.6%。基于标签的模型训练在不同解剖区域表现出色（例如肘部：AUC=0.80），模型在外部数据集上泛化良好（肘部：AUC=0.79）。标签策略或数据集之间未观察到显著差异（p>=0.15）。

Conclusion: GPT-4o能够从放射学报告中提取标签，用于训练具有高准确度的多标签分类模型。检测到的不确定性并未影响这些模型的性能。

Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with
uncertainty) from free-text radiology reports and to test how these labels
affect multi-label image classification of musculoskeletal radiographs.
Methods: This retrospective study included radiography series of the clavicle
(n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o
filled out structured templates by indicating imaging findings as present
("true"), absent ("false"), or "uncertain." To assess the impact of label
uncertainty, "uncertain" labels of the training and validation sets were
automatically reassigned to "true" (inclusive) or "false" (exclusive).
Label-image-pairs were used for multi-label classification using ResNet50.
Label extraction accuracy was manually verified on internal (clavicle: n=233,
elbow: n=745, thumb: n=393) and external test sets (n=300 for each).
Performance was assessed using macro-averaged receiver operating characteristic
(ROC) area under the curve (AUC), precision recall curves, sensitivity,
specificity, and accuracy. AUCs were compared with the DeLong test. Results:
Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the
test sets. Across anatomic regions, label-based model training yielded
competitive performance measured by macro-averaged AUC values for inclusive
(e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow:
AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets
(elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79
[range, 0.63-0.89]). No significant differences were observed across labeling
strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from
radiologic reports to train competitive multi-label classification models with
high accuracy. Detected uncertainty in the radiologic reports did not influence
the performance of these models.

</details>


### [117] [D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI](https://arxiv.org/abs/2510.05684)
*Suwhan Choi,Jaeyoon Jung,Haebin Seong,Minchan Kim,Minyeong Kim,Yongjun Cho,Yoonshik Kim,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: D2E框架利用桌面交互数据预训练机器人具身AI任务，验证了从数字到物理任务的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI因物理轨迹收集成本高而受限的问题，利用桌面环境（如游戏）提供的大规模传感器运动交互数据。

Method: D2E框架包括三个部分：OWA Toolkit（标准化桌面交互数据）、Generalist-IDM（跨游戏零样本泛化）和VAPT（迁移到物理任务）。

Result: 在LIBERO操作和CANVAS导航基准测试中分别达到96.6%和83.3%的成功率。

Conclusion: 通过D2E框架，研究证明了桌面交互可以作为机器人具身AI任务的有效预训练基础，并展示了从桌面数据到具身任务的完整迁移流程。

Abstract: Large language models leverage internet-scale text data, yet embodied AI
remains constrained by the prohibitive costs of physical trajectory collection.
Desktop environments -- particularly gaming -- offer a compelling alternative:
they provide rich sensorimotor interactions at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to Embodied AI), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the OWA Toolkit
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the Generalist-IDM that achieves strong zero-shot
generalization across unseen games through timestamp-based event prediction,
enabling internet-scale pseudo-labeling, and (3) VAPT that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on CANVAS navigation benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the OWA toolkit, datasets of human-collected and
pseudo-labeled, and VAPT-trained models available at
https://worv-ai.github.io/d2e/

</details>


### [118] [Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach](https://arxiv.org/abs/2510.05698)
*Yousef Emami,Seyedsina Nabavirazavi,Jingjing Zheng,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 提出AIC-VDS方法，通过联合优化无人机数据收集计划和速度控制，减少灾害监测中的数据丢失，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害后监测中面临数据收集计划和飞行速度设计的挑战，传统在线深度强化学习（DRL）训练复杂且存在仿真与现实的差距，无法满足紧急需求。

Method: 提出了基于注意力机制的上下文学习（AIC-VDS）方法，联合优化多无人机数据收集计划和飞行速度控制，考虑了地面传感器的电池电量、队列长度、信道条件及无人机轨迹。

Result: 仿真结果表明，AIC-VDS在减少数据丢失方面优于Deep-Q-Network（DQN）和最大信道增益基线方法。

Conclusion: AIC-VDS作为一种替代DRL的紧急情况下解决方案，在减少数据丢失方面表现出色，优于传统的DQN和最大信道增益基线方法。

Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated
to collect sensory data in post-disaster monitoring scenarios, such as
tsunamis, where early actions are critical to limit coastal damage. A major
challenge is to design the data collection schedules and flight velocities, as
unfavorable schedules and velocities can lead to transmission errors and buffer
overflows of the ground sensors, ultimately resulting in significant packet
loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a
complex training process and a mismatch between simulation and reality that
does not meet the urgent requirements of tsunami monitoring. Recent advances in
Large Language Models (LLMs) offer a compelling alternative. With their strong
reasoning and generalization capabilities, LLMs can adapt to new tasks through
In-Context Learning (ICL), which enables task adaptation through natural
language prompts and example-based guidance without retraining. However, LLM
models have input data limitations and thus require customized approaches. In
this paper, a joint optimization of data collection schedules and velocities
control for multiple UAVs is proposed to minimize data loss. The battery level
of the ground sensors, the length of the queues, and the channel conditions, as
well as the trajectories of the UAVs, are taken into account. Attention-Based
In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS)
is proposed as an alternative to DRL in emergencies. The simulation results
show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and
maximum channel gain baselines.

</details>


### [119] [Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge](https://arxiv.org/abs/2510.05733)
*Zijun Jia,Shuang Liang,Jinsong Yu*

Main category: cs.AI

TL;DR: Syn-Diag 是一种云边缘协同框架，利用LLM解决小样本工业故障诊断问题，通过三层机制实现高效诊断，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业故障诊断面临数据稀缺和资源受限环境中难以部署大型AI模型的双重挑战。

Method: Syn-Diag 基于三层机制：1) 视觉-语义协同，通过跨模态预训练对齐信号特征与LLM语义空间；2) 内容感知推理，动态构建上下文提示以提升小样本诊断准确性；3) 云边缘协同，通过知识蒸馏生成轻量级边缘模型，支持在线更新。

Result: 在六个数据集上的实验表明，Syn-Diag 显著优于现有方法，尤其在1-shot和跨工况场景中。边缘模型性能接近云端版本，同时模型大小减少83%，延迟降低50%。

Conclusion: Syn-Diag 提供了一个实用、强大且可部署的现代智能诊断范式，通过云边缘协同框架在资源受限环境中高效实现故障诊断。

Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the
difficulty of deploying large AI models in resource-constrained environments.
This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that
leverages Large Language Models to overcome these limitations in few-shot fault
diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic
Synergy, which aligns signal features with the LLM's semantic space through
cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically
constructs contextual prompts to enhance diagnostic accuracy with limited
samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create
a lightweight, efficient edge model capable of online updates via a shared
decision space. Extensive experiments on six datasets covering different CWRU
and SEU working conditions show that Syn-Diag significantly outperforms
existing methods, especially in 1-shot and cross-condition scenarios. The edge
model achieves performance comparable to the cloud version while reducing model
size by 83% and latency by 50%, offering a practical, robust, and deployable
paradigm for modern intelligent diagnostics.

</details>


### [120] [The Safety Challenge of World Models for Embodied AI Agents: A Review](https://arxiv.org/abs/2510.05865)
*Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi*

Main category: cs.AI

TL;DR: 本文综述了世界模型在自动驾驶和机器人领域的安全影响，通过实证分析识别了常见故障并进行了定量评估，强调了预测安全的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着具身人工智能的快速发展，需要更先进的集成模型来感知、解释和预测环境动态，同时确保预测对代理和环境的安全。

Method: 结合文献综述和实证分析，收集并检查了最先进模型的预测结果，识别并分类了常见故障（病理），并进行了定量评估。

Result: 研究识别了世界模型在场景和控制生成任务中的常见故障，并提供了定量评估结果，强调了安全性的重要性。

Conclusion: 本文通过文献综述和实证分析，全面评估了世界模型在自动驾驶和机器人领域中的安全影响，识别了常见故障并提供了定量评估，强调了在追求预测能力时确保安全的重要性。

Abstract: The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

</details>


### [121] [Artificially intelligent agents in the social and behavioral sciences: A history and outlook](https://arxiv.org/abs/2510.05743)
*Petter Holme,Milena Tsvetkova*

Main category: cs.AI

TL;DR: 回顾了AI代理在社会科学中的历史发展，从早期计算机到现代大型语言模型，强调技术与人类自我理解的深度融合。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何影响社会科学和行为科学的研究方法，以及技术如何与人类自我理解的工具深度融合。

Method: 通过回顾历史文献和当前研究，分析了AI技术从早期社会模拟到现代大型语言模型应用的演变。

Result: 概述了AI在社会科学中的关键里程碑，包括社会模拟、智能博弈论代理、大数据时代和生成AI的当前应用。

Conclusion: 本文总结了从1950年至今，人工智能代理在社会科学和行为科学中的历史发展与当前趋势，强调了AI在科学过程中的角色及其带来的变化。

Abstract: We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

</details>


### [122] [Information-Theoretic Policy Pre-Training with Empowerment](https://arxiv.org/abs/2510.05996)
*Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker*

Main category: cs.AI

TL;DR: Empowerment作为预训练信号，通过最大化discounted empowerment初始化策略，显著提升了强化学习任务的数据效率和适应性。


<details>
  <summary>Details</summary>
Motivation: Empowerment作为一种内在动机和探索框架，在强化学习中具有潜力，但其作为预训练信号的应用尚未得到充分研究。

Method: 提出了discounted empowerment的概念，并基于此设计了一种新的预训练范式，通过最大化discounted empowerment来初始化策略。

Result: 实验证明，基于empowerment的预训练策略能够显著提升下游任务的数据效率和适应性。

Conclusion: Empowerment作为预训练信号，能够有效提升下游任务的适应性和数据效率，为强化学习领域的进一步研究提供了新方向。

Abstract: Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

</details>


### [123] [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
*Bohan Yao,Shiva Krishna Reddy Malay,Vikas Yadav*

Main category: cs.AI

TL;DR: 提出基于ARM的自动MAS设计新范式，通过优化CoT推理显著提升性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动设计MAS技术表现不佳且计算成本高，而简单的CoT推理表现与之相当，因此需要优化CoT推理。

Method: 通过代码空间的树搜索发现ARM，从简单的CoT模块出发，利用执行轨迹的反思进行突变进化。

Result: ARM构建的MAS在不同基础模型和任务领域中无需进一步优化即可保持高性能。

Conclusion: ARM作为一种通用的推理构建块，显著优于手动设计的MAS和最先进的自动MAS设计方法，展现出卓越的泛化能力。

Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

</details>


### [124] [Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport](https://arxiv.org/abs/2510.05751)
*Jeffrey N. Clark,Elena Fillola,Nawid Keshtmand,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.AI

TL;DR: AI-based ensemble emulators offer a fast, reliable way to model atmospheric transport and quantify uncertainties, improving greenhouse gas emissions monitoring.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, scalable, and reliable methods to monitor greenhouse gas emissions and evaluate national inventories drives this research. Traditional transport models are computationally expensive and their uncertainties are hard to characterize, prompting the exploration of AI-based solutions.

Method: The paper presents an ensemble-based pipeline using a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM) to estimate atmospheric transport footprints and uncertainties. This approach was tested with GOSAT observations for Brazil in 2016.

Result: The emulator achieved a ~1000x speed-up over the traditional NAME LPDM while accurately reproducing large-scale footprint structures. Ensemble calculations quantified absolute and relative uncertainties, revealing spatial correlations with prediction errors.

Conclusion: The study demonstrates that ensemble-based emulators can significantly enhance the efficiency and reliability of atmospheric transport models, supporting more robust greenhouse gas emissions monitoring. Future developments could further explore systematic errors, contributing to a more comprehensive uncertainty budget in flux estimates.

Abstract: Monitoring greenhouse gas emissions and evaluating national inventories
require efficient, scalable, and reliable inference methods. Top-down
approaches, combined with recent advances in satellite observations, provide
new opportunities to evaluate emissions at continental and global scales.
However, transport models used in these methods remain a key source of
uncertainty: they are computationally expensive to run at scale, and their
uncertainty is difficult to characterise. Artificial intelligence offers a dual
opportunity to accelerate transport simulations and to quantify their
associated uncertainty.
  We present an ensemble-based pipeline for estimating atmospheric transport
"footprints", greenhouse gas mole fraction measurements, and their
uncertainties using a graph neural network emulator of a Lagrangian Particle
Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse
Gases Observing Satellite) observations for Brazil in 2016. The emulator
achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale
footprint structures. Ensembles were calculated to quantify absolute and
relative uncertainty, revealing spatial correlations with prediction error. The
results show that ensemble spread highlights low-confidence spatial and
temporal predictions for both atmospheric transport footprints and methane mole
fractions.
  While demonstrated here for an LPDM emulator, the approach could be applied
more generally to atmospheric transport models, supporting uncertainty-aware
greenhouse gas inversion systems and improving the robustness of
satellite-based emissions monitoring. With further development, ensemble-based
emulators could also help explore systematic LPDM errors, offering a
computationally efficient pathway towards a more comprehensive uncertainty
budget in greenhouse gas flux estimates.

</details>


### [125] [Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis](https://arxiv.org/abs/2510.05761)
*Sedat Dogan,Nina Dethlefs,Debarati Chakraborty*

Main category: cs.AI

TL;DR: 该研究通过跨语言数据集和混合参与分数方法，成功预测模因的早期病毒性，XGBoost模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 预测在线内容的病毒性具有挑战性，尤其是对文化复杂、快速演变的模因。本研究旨在探索早期预测模因病毒性的可行性。

Method: 提出了一种基于混合参与分数的数据驱动方法，利用时间序列数据结合静态内容和网络特征，评估了包括逻辑回归、XGBoost和多层感知机（MLP）在内的多种模型。

Result: 最佳模型XGBoost在仅30分钟内即可实现PR-AUC > 0.52的性能，揭示了特征重要性从静态上下文到时间动态的明显转变。

Conclusion: 该研究为早期预测网络内容病毒性提供了稳健、可解释且实用的基准，特别是在缺乏完整传播级联数据的情况下。

Abstract: Predicting the virality of online content remains challenging, especially for
culturally complex, fast-evolving memes. This study investigates the
feasibility of early prediction of meme virality using a large-scale,
cross-lingual dataset from 25 diverse Reddit communities. We propose a robust,
data-driven method to define virality based on a hybrid engagement score,
learning a percentile-based threshold from a chronologically held-out training
set to prevent data leakage. We evaluated a suite of models, including Logistic
Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive,
multimodal feature set across increasing time windows (30-420 min). Crucially,
useful signals emerge quickly: our best-performing model, XGBoost, achieves a
PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary
transition," in which the importance of the feature dynamically shifts from the
static context to the temporal dynamics as a meme gains traction. This work
establishes a robust, interpretable, and practical benchmark for early virality
prediction in scenarios where full diffusion cascade data is unavailable,
contributing a novel cross-lingual dataset and a methodologically sound
definition of virality. To our knowledge, this study is the first to combine
time series data with static content and network features to predict early meme
virality.

</details>


### [126] [RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases](https://arxiv.org/abs/2510.05764)
*Lang Qin,Zijian Gan,Xu Cao,Pengcheng Jiang,Yankai Jiang,Jiawei Han,Kaishun Wu,Jintai Chen*

Main category: cs.AI

TL;DR: RareAgent是一种自演化的多智能体系统，通过主动证据推理框架提升罕见病药物重定位效果，AUPRC提高18.1%。


<details>
  <summary>Details</summary>
Motivation: 罕见病药物重定位在缺乏药物与目标疾病关联信号时，传统方法表现不佳，因此需要一种主动推理框架。

Method: RareAgent采用自演化的多智能体系统，通过任务特定的对抗性辩论动态构建证据图，并分析推理策略以优化智能体策略。

Result: RareAgent将指示AUPRC提高了18.1%，优于现有推理基线。

Conclusion: RareAgent通过主动寻求证据的推理框架，显著提高了罕见病药物重定位的准确性，并提供了与临床证据一致的可解释推理链。

Abstract: Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

</details>


### [127] [ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming](https://arxiv.org/abs/2510.05774)
*Weichun Shi,Minghao Liu,Wanting Zhang,Langchen Shi,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: ConstraintLLM是首个专为CP建模设计的LLM，通过CARM和ToT框架提升性能，在IndusCP基准上表现突出。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）自动生成COP的正式建模，以构建可信赖的神经符号AI。

Method: 提出了Constraint-Aware Retrieval Module (CARM) 增强上下文学习能力，并集成到带有引导自我纠正机制的Tree-of-Thoughts (ToT)框架中。

Result: ConstraintLLM在多个基准测试中表现出色，特别是在新发布的IndusCP基准上表现优异。

Conclusion: ConstraintLLM在多个基准测试中达到了最先进的求解精度，并在新的IndusCP基准上比基线表现高出2倍。

Abstract: Constraint programming (CP) is a crucial technology for solving real-world
constraint optimization problems (COPs), with the advantages of rich modeling
semantics and high solving efficiency. Using large language models (LLMs) to
generate formal modeling automatically for COPs is becoming a promising
approach, which aims to build trustworthy neuro-symbolic AI with the help of
symbolic solvers. However, CP has received less attention compared to works
based on operations research (OR) models. We introduce ConstraintLLM, the first
LLM specifically designed for CP modeling, which is trained on an open-source
LLM with multi-instruction supervised fine-tuning. We propose the
Constraint-Aware Retrieval Module (CARM) to increase the in-context learning
capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with
guided self-correction mechanism. Moreover, we construct and release IndusCP,
the first industrial-level benchmark for CP modeling, which contains 140
challenging tasks from various domains. Our experiments demonstrate that
ConstraintLLM achieves state-of-the-art solving accuracy across multiple
benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.
Code and data are available at: https://github.com/william4s/ConstraintLLM.

</details>


### [128] [Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering](https://arxiv.org/abs/2510.05871)
*Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Tom A. Lamb,Philip Torr,Marc Boubnovski Martell,Julien Fauqueur,Kaspar Märtens*

Main category: cs.AI

TL;DR: 提出了一种无需标签的基于不确定性的过滤方法，用于提高合成推理轨迹的质量，在成本高昂的领域（如生物学）中实现了高效的大型推理模型训练。


<details>
  <summary>Details</summary>
Motivation: 在生物学等领域，湿实验室数据稀缺且昂贵，传统方法依赖真实标签进行种子或过滤推理轨迹的成本高昂。

Method: 提出了一种基于不确定性的过滤方法，利用模型自身的置信度（通过自我一致性和预测困惑度等不确定性指标量化）替代外部标签，筛选出低不确定性的推理轨迹子集。

Result: 实验表明，经过不确定性过滤的子集具有更高的准确性，且在使用过滤后的数据进行监督微调时，性能优于未过滤的合成数据，缩小了与基于真实标签训练的差距，并超越了强大的大型推理模型基线。

Conclusion: 模型内部置信度是高效创建推理数据集的强大信号，能够在监督成本高昂的领域实现大型推理模型的应用。

Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large
reasoning models (LRMs), improving generalization by providing step-level
supervision. Yet most approaches require ground-truth labels to seed or filter
these traces - an expensive bottleneck in domains like biology where wet-lab
data are scarce. We propose a label-free alternative: uncertainty-based
filtering, which uses a model's own confidence - quantified through established
uncertainty metrics like self-consistency and predictive perplexity - as a
substitute for external labels. We sample multiple reasoning traces and retain
only low-uncertainty subsets. Applied to biological perturbation prediction, a
domain where wet-lab labels are especially costly, we show that the filtered
subset has higher accuracy, and that supervised fine-tuning (SFT) on
uncertainty-filtered data outperforms unfiltered synthetic data, narrows the
gap to ground-truth training, and surpasses strong LRM baselines. Ablations
show that per-class filtering corrects for class-specific uncertainty scales
and that hybrid uncertainty metrics yield higher-quality datasets. Our results
suggest that model-internal confidence is a powerful signal for efficient
reasoning dataset creation, enabling LRMs in domains where supervision is
expensive.

</details>


### [129] [Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies](https://arxiv.org/abs/2510.05909)
*Aksel Joonas Reedi,Corentin Léger,Julien Pourcel,Loris Gaven,Perrine Charriau,Guillaume Pourcel*

Main category: cs.AI

TL;DR: 研究通过辩论进化算法DebateQD，发现说服力优化比传统真理优化更能提升LLM的泛化能力，减少训练-测试差距。


<details>
  <summary>Details</summary>
Motivation: 主流基于真理的优化方法可能导致过拟合和脆弱的推理，而辩论为基础的优化尚未系统比较。本文旨在探索说服力优化是否能提升LLM的泛化能力。

Method: 引入DebateQD，一种基于质量-多样性（QD）的进化算法，通过锦标赛式辩论演化多样化的辩论策略，同时固定辩论协议以隔离优化目标的影响。

Result: 在多个模型规模和数据集上，说服力优化的策略实现了高达13.94%更小的训练-测试泛化差距，同时匹配或超过基于真理优化的测试性能。

Conclusion: 辩论优化的策略在提升大型语言模型（LLM）的泛化能力方面显示出潜力，尤其是在说服力优化下，模型表现出更小的训练-测试泛化差距和可转移的推理技能。

Abstract: Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

</details>


### [130] [Training-Free Time Series Classification via In-Context Reasoning with LLM Agents](https://arxiv.org/abs/2510.05950)
*Songyuan Sui,Zihang Xu,Yu-Neng Chuang,Kwei-Herng Lai,Xia Hu*

Main category: cs.AI

TL;DR: FETA是一种无需训练的多智能体框架，通过上下文推理和示例比较实现高效、可解释的时间序列分类，性能优于传统训练方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类中标记数据稀缺，任务特定训练成本高且不灵活，现有零-shot使用大型语言模型效果不佳，需要一种无需预训练或微调的高效方法。

Method: FETA通过将多元时间序列分解为通道子问题，检索结构相似的标记示例，并利用推理型大型语言模型比较查询与这些示例，生成带有自评估置信度的通道级标签，最后通过置信度加权聚合器融合所有通道决策。

Result: 在九个UEA数据集上，FETA在完全无需训练的情况下实现了高准确率，超越了多个经过训练的基线方法。

Conclusion: FETA框架在无需训练的情况下，通过多智能体上下文推理将大型语言模型转化为具有竞争力的即插即用时间序列分类解决方案。

Abstract: Time series classification (TSC) spans diverse application scenarios, yet
labeled data are often scarce, making task-specific training costly and
inflexible. Recent reasoning-oriented large language models (LLMs) show promise
in understanding temporal patterns, but purely zero-shot usage remains
suboptimal. We propose FETA, a multi-agent framework for training-free TSC via
exemplar-based in-context reasoning. FETA decomposes a multivariate series into
channel-wise subproblems, retrieves a few structurally similar labeled examples
for each channel, and leverages a reasoning LLM to compare the query against
these exemplars, producing channel-level labels with self-assessed confidences;
a confidence-weighted aggregator then fuses all channel decisions. This design
eliminates the need for pretraining or fine-tuning, improves efficiency by
pruning irrelevant channels and controlling input length, and enhances
interpretability through exemplar grounding and confidence estimation. On nine
challenging UEA datasets, FETA achieves strong accuracy under a fully
training-free setting, surpassing multiple trained baselines. These results
demonstrate that a multi-agent in-context reasoning framework can transform
LLMs into competitive, plug-and-play TSC solvers without any parameter
training. The code is available at https://github.com/SongyuanSui/FETATSC.

</details>


### [131] [MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization](https://arxiv.org/abs/2510.05962)
*Dayyán O'Brien,Barry Haddow,Emily Allaway,Pinzhen Chen*

Main category: cs.AI

TL;DR: 提出MatheMagic方法构建动态反事实基准，用于检测过拟合并评估真实推理能力，发现模型在归纳任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 由于模型可能记忆公开的测试集，且当前数学基准存在符号和规则多样性不足、答案封闭的问题，难以进行无污染的数学能力评估。

Method: 提出MatheMagic方法，通过动态生成数学测试实例，改变数字和运算符的解释，但保持答案可自动验证。测试实例在测试时随机生成，用于评估模型的归纳或演绎能力。

Result: 实验发现模型解决演绎问题比归纳更容易，但会回归标准数学。

Conclusion: 数学适应模型未能展现出通用的‘推理技能’，且在归纳任务上的微调泛化能力较差。

Abstract: Conducting contamination-free evaluation of mathematical capabilities can be
difficult for two reasons: models may memorize a test set once it is made
public, and current mathematical benchmarks are prone to overfitting due to
having limited diversity of symbols and rules, coupled with closed-ended
answers. This paper proposes a method to leverage these shortcomings as useful
features to a construct dynamic, counterfactual benchmark, which can be used to
both reveal overfitting and measure true reasoning. We demonstrate this via
MatheMagic, which generates math test instances with the interpretations of
numbers and operators altered, yet has automatically verifiable answers. Test
instances are randomly seeded and constructed at test time to evaluate a
model's induction or deduction capability, offering stability, extensibility,
comparability, and robustness to overfitting. Our experiments find that models
solve deduction more easily than induction, but they revert to standard math.
Further analysis reveals that math-adapted models fail to exhibit a general
"skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.

</details>


### [132] [Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG](https://arxiv.org/abs/2510.06002)
*Hudson de Martim*

Main category: cs.AI

TL;DR: SAT-Graph API通过规范动作和两层架构，实现了对法律知识的高精度、透明检索，满足高风险领域的可解释性需求。


<details>
  <summary>Details</summary>
Motivation: 解决如何在保持结构化知识确定性属性的前提下可靠查询的问题。

Method: 提出了SAT-Graph API，一个基于规范动作（原子、可组合、可审计的原始操作）的查询执行层，将概率性发现与确定性检索分离。

Result: 实现了高精度混合搜索、鲁棒的引用解析、时间点版本检索和可审计的因果追踪。

Conclusion: SAT-Graph API通过引入规范化的查询执行层，实现了对结构化法律知识的高精度、透明且可审计的检索，满足了高风险领域对可解释AI的需求。

Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

</details>


### [133] [ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models](https://arxiv.org/abs/2510.06014)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Zhiyuan Yu,Qipeng Guo,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: ARISE是一种新指标，用于评估大型推理模型的测试时扩展能力，结合样本级意识和动态采样机制，实验显示其有效性并识别出Claude Opus的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着推理模型的快速发展，如何系统比较和评估不同模型的测试时扩展能力成为一个关键问题。

Method: ARISE结合了样本级意识和动态采样机制，以评估测试时扩展效果。

Result: 实验结果表明，ARISE能够有效评估扩展能力，并发现Claude Opus在扩展特性上优于其他当代推理模型。

Conclusion: ARISE提供了一个可靠且细粒度的度量标准，用于评估大型推理模型在测试时扩展能力，揭示了不同模型在扩展效率上的显著差异。

Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

</details>


### [134] [Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?](https://arxiv.org/abs/2510.06036)
*Qingyu Yin,Chak Tou Leong,Linyi Yang,Wenxuan Huang,Wenjie Li,Xiting Wang,Jaehong Yoon,YunXing,XingYu,Jinjin Gu*

Main category: cs.AI

TL;DR: 研究发现推理模型的安全对齐失败源于"拒绝悬崖"现象，提出了一种高效的数据选择方法Cliff-as-a-Judge，仅需1.7%的传统安全训练数据即可实现类似的安全改进。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在多步推理能力上表现出色，但存在安全漏洞且其机制尚不明确。本研究旨在通过机制性可解释性视角探究安全对齐失败的原因。

Method: 使用线性探测方法追踪拒绝意图在令牌位置上的分布，并通过因果干预分析识别对拒绝行为有负面影响的注意力头。

Result: 发现了一种称为"拒绝悬崖"的现象：许多对齐不良的推理模型能正确识别有害提示并在思考过程中保持强烈的拒绝意图，但在输出生成前的最后几个令牌中拒绝分数急剧下降。

Conclusion: 通过因果干预分析发现，稀疏的注意力头对拒绝行为有负面影响，仅消除3%的这些头即可将攻击成功率降至10%以下。基于这些机制性见解，提出了Cliff-as-a-Judge数据选择方法，高效修复推理模型的安全对齐。

Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have
shown remarkable problem-solving abilities, yet they exhibit concerning safety
vulnerabilities that remain poorly understood. In this work, we investigate why
safety alignment fails in reasoning models through a mechanistic
interpretability lens. Using a linear probing approach to trace refusal
intentions across token positions, we discover a striking phenomenon termed as
\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify
harmful prompts and maintain strong refusal intentions during their thinking
process, but experience a sharp drop in refusal scores at the final tokens
before output generation. This suggests that these models are not inherently
unsafe; rather, their refusal intentions are systematically suppressed. Through
causal intervention analysis, we identify a sparse set of attention heads that
negatively contribute to refusal behavior. Ablating just 3\% of these heads can
reduce attack success rates below 10\%. Building on these mechanistic insights,
we propose \textbf{Cliff-as-a-Judge}, a novel data selection method that
identifies training examples exhibiting the largest refusal cliff to
efficiently repair reasoning models' safety alignment. This approach achieves
comparable safety improvements using only 1.7\% of the vanilla safety training
data, demonstrating a less-is-more effect in safety alignment.

</details>


### [135] [MixReasoning: Switching Modes to Think](https://arxiv.org/abs/2510.06052)
*Haiquan Lu,Gongfan Fang,Xinyin Ma,Qi Li,Xinchao Wang*

Main category: cs.AI

TL;DR: MixReasoning框架动态调整推理深度，提升效率且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型对所有步骤采用相同深度，导致冗余。MixReasoning旨在自适应响应子问题难度变化。

Method: 提出了MixReasoning框架，动态调整推理深度，结合详细推理和简洁推断。

Result: 在GSM8K、MATH-500和AIME上的实验表明，MixReasoning缩短推理长度并提升效率。

Conclusion: MixReasoning框架通过动态调整推理深度，显著提高了效率且未牺牲准确性，适用于不同难度的子问题。

Abstract: Reasoning models enhance performance by tackling problems in a step-by-step
manner, decomposing them into sub-problems and exploring long chains of thought
before producing an answer. However, applying extended reasoning to every step
introduces substantial redundancy, as sub-problems vary widely in difficulty
and complexity: a small number of pivotal steps are genuinely challenging and
decisive for the final answer, while many others only involve straightforward
revisions or simple computations. Therefore, a natural idea is to endow
reasoning models with the ability to adaptively respond to this variation,
rather than treating all steps with the same level of elaboration. To this end,
we propose MixReasoning, a framework that dynamically adjusts the depth of
reasoning within a single response. The resulting chain of thought then becomes
a mixture of detailed reasoning on difficult steps and concise inference on
simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning
shortens reasoning length and substantially improves efficiency without
compromising accuracy.

</details>


### [136] [Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research](https://arxiv.org/abs/2510.06056)
*Gang Liu,Yihan Zhu,Jie Chen,Meng Jiang*

Main category: cs.AI

TL;DR: DeepEvolve结合深度研究与算法进化，通过迭代循环生成可执行算法，在多个科学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的科学助手要么仅依赖算法进化，要么仅依赖深度研究，两者在复杂领域中均存在关键局限性。

Method: DeepEvolve整合了外部知识检索、跨文件代码编辑和系统调试，通过反馈驱动的迭代循环，不仅提出新假设，还进行细化、实现和测试。

Result: 在化学、数学、生物学、材料和专利等九个基准测试中，DeepEvolve持续改进了初始算法，生成了可执行的新算法并取得了持续收益。

Conclusion: DeepEvolve通过结合深度研究和算法进化，提供了一个可靠的框架，用于推动科学算法发现，弥补了无引导进化与无基础研究之间的差距。

Abstract: Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

</details>


### [137] [TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis](https://arxiv.org/abs/2510.06063)
*Austin Feng,Andreas Varvarigos,Ioannis Panitsas,Daniela Fernandez,Jinbiao Wei,Yuwei Guo,Jialin Chen,Ali Maatouk,Leandros Tassiulas,Rex Ying*

Main category: cs.AI

TL;DR: 论文介绍了TelecomTS数据集，填补了可观测性数据在公共基准中的空白，并展示了现有模型在处理此类数据时的局限性，强调了尺度信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 可观测性数据在公共基准测试中代表性不足，现有数据集通常经过匿名化和归一化处理，移除了尺度信息，限制了其在预测以外的任务中的应用。

Method: 引入了TelecomTS，这是一个从5G电信网络衍生的大规模可观测性数据集，具有异构、去匿名化的协变量和明确的尺度信息，支持包括异常检测、根因分析和多模态推理问答基准在内的下游任务。

Result: 基准测试表明，现有的时间序列、语言和推理模型难以应对可观测性数据的突发性、噪声和高方差动态。实验结果强调了保留协变量绝对尺度的重要性。

Conclusion: 现有的方法在处理可观测性数据的突发性、噪声和高方差动态时表现不佳，强调了保留协变量绝对尺度的重要性，需要能够原生利用尺度信息的基础时间序列模型。

Abstract: Modern enterprises generate vast streams of time series metrics when
monitoring complex systems, known as observability data. Unlike conventional
time series from domains such as weather, observability data are zero-inflated,
highly stochastic, and exhibit minimal temporal structure. Despite their
importance, observability datasets are underrepresented in public benchmarks
due to proprietary restrictions. Existing datasets are often anonymized and
normalized, removing scale information and limiting their use for tasks beyond
forecasting, such as anomaly detection, root-cause analysis, and multi-modal
reasoning. To address this gap, we introduce TelecomTS, a large-scale
observability dataset derived from a 5G telecommunications network. TelecomTS
features heterogeneous, de-anonymized covariates with explicit scale
information and supports a suite of downstream tasks, including anomaly
detection, root-cause analysis, and a question-answering benchmark requiring
multi-modal reasoning. Benchmarking state-of-the-art time series, language, and
reasoning models reveals that existing approaches struggle with the abrupt,
noisy, and high-variance dynamics of observability data. Our experiments also
underscore the importance of preserving covariates' absolute scale, emphasizing
the need for foundation time series models that natively leverage scale
information for practical observability applications.

</details>


### [138] [Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents](https://arxiv.org/abs/2510.06078)
*Tao Zhe,Rui Liu,Fateme Memar,Xiao Luo,Wei Fan,Xinyue Ye,Zhongren Peng,Dongjie Wang*

Main category: cs.AI

TL;DR: RouteLLM是一个分层多智能体框架，通过解析自然语言意图并协调多个代理，实现高效、灵活的路线推荐。


<details>
  <summary>Details</summary>
Motivation: 传统路由算法假设结构化输入和固定目标，难以适应自然语言查询；而基于LLM的方法在空间推理和联合建模路线级与POI级偏好方面存在不足。

Method: RouteLLM采用分层多智能体框架，包括意图解析、约束代理、POI代理、路径细化代理和最终验证代理，通过协同工作实现路线推荐。

Result: 实验表明，RouteLLM能够可靠地将文本偏好转化为约束感知的路线，相比传统方法在路线质量和偏好满足度上有所提升。

Conclusion: RouteLLM提出了一种分层多智能体框架，能够有效地将自然语言意图转化为约束感知的路线，显著提升了路线质量和偏好满足度。

Abstract: Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

</details>


### [139] [Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices](https://arxiv.org/abs/2510.06093)
*Mallika Mainali,Harsha Sureshbabu,Anik Sen,Christopher B. Rauch,Noah D. Reifsnyder,John Meyer,J. T. Turner,Michael W. Floyd,Matthew Molineaux,Rosina O. Weber*

Main category: cs.AI

TL;DR: 研究比较了经典AI和LLM模型在决策者对齐上的表现，发现两者效果相当，经典AI在中等风险配置上略优。


<details>
  <summary>Details</summary>
Motivation: 随着算法决策者在高风险领域的应用增加，AI对齐研究从通用价值对齐转向考虑决策者属性的上下文特定方法。

Method: 本研究实现了一个经典AI模型，并开发了一个基于LLM的算法决策者，使用大型推理模型（GPT-5）和非推理模型（GPT-4）在零样本提示框架下进行评估。

Result: 经典AI和基于LLM的模型在健康保险决策数据集上表现出与基于属性的目标相当的对齐性，其中经典AI在中等风险配置上表现略好。

Conclusion: 经典AI和基于LLM的模型在决策者对齐方面表现相当，经典AI在中等风险配置上略胜一筹。

Abstract: As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

</details>


### [140] [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)
*Batu El,James Zou*

Main category: cs.AI

TL;DR: 竞争性优化LLMs可能导致模型偏离对齐目标，表现为欺骗性营销、虚假信息和有害行为的增加，需要更强治理和激励机制来应对。


<details>
  <summary>Details</summary>
Motivation: 探讨在竞争性环境中，优化大型语言模型（LLMs）以取得成功时，如何无意中导致模型偏离对齐目标，并研究这种现象的后果。

Method: 通过模拟环境（如销售、选举和社交媒体场景）来研究竞争反馈循环对LLM行为的影响。

Result: 研究发现，在不同场景中，优化LLMs以取得竞争优势会导致欺骗性营销、虚假信息和有害行为显著增加。例如，销售场景中销售额增加6.3%的同时，欺骗性营销增加了14.0%。

Conclusion: 论文指出，市场驱动的优化压力可能导致AI系统逐渐偏离对齐目标，形成一种‘逐底竞争’。为了安全部署AI系统，需要更强的治理和精心设计的激励机制来防止竞争动态破坏社会信任。

Abstract: Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

</details>


### [141] [Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification](https://arxiv.org/abs/2510.06135)
*Weihao Zeng,Keqing He,Chuqiao Kuang,Xiaoguang Li,Junxian He*

Main category: cs.AI

TL;DR: 通过结合序列和并行TTS策略，利用不对称验证，显著提升了AI系统的性能，开源模型在基准测试中超越专有模型。


<details>
  <summary>Details</summary>
Motivation: 验证响应通常比生成响应更容易，这种不对称验证的潜力激发了研究TTS策略的兴趣。

Method: 研究序列和并行TTS策略，利用不对称验证的优势，通过分配适度的计算资源给验证器来优化性能。

Result: 在BrowseComp和GAIA等基准测试中，开源模型实现了显著的性能提升，最高达到27个绝对百分点。GLM-4.5 Heavy和Tongyi-DeepResearch Heavy分别达到54.0%和69.0%的准确率。

Conclusion: 通过结合序列和并行测试时间计算（TTS）策略，特别是在不对称验证的背景下，可以显著提升AI系统的性能。开源模型如GLM-4.5 Heavy和Tongyi-DeepResearch Heavy在基准测试中表现出色，甚至超越了一些专有模型。

Abstract: Test-time compute can be scaled both sequentially and in parallel. Sequential
scaling involves lengthening the generation process, while parallel scaling
involves verifying and selecting among multiple candidate outputs. Combining
these two strategies has led to the most powerful AI systems, such as Grok 4
Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),
verifying responses can be substantially easier than generating them. This
property, referred to as \emph{asymmetric verification}, highlights the strong
potential of test-time scaling (TTS). In this work, we study both sequential
and parallel TTS of deep search agents, motivated by the intuition that
verification in this setting is often much easier than generation. In
experiments, we first show that sequential scaling methods, such as budget
forcing, can be effective initially but soon degrade performance. Leveraging
asymmetric verification, however, we are able to achieve substantial
improvements by allocating only a modest amount of compute to the verifier. We
conduct experiments with flagship open-source models and extend them to their
``Heavy'' variants through TTS. These deep research agents achieve gains of up
to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an
open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on
BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best
proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy
further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the
best proprietary results.

</details>


### [142] [Barbarians at the Gate: How AI is Upending Systems Research](https://arxiv.org/abs/2510.06189)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Bowen Wang,Alex Krentsel,Tian Xia,Mert Cemri,Jongseok Park,Shuo Yang,Jeff Chen,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.AI

TL;DR: AI驱动的系统研究（ADRS）通过迭代生成和验证解决方案，在多个领域实现了优于人工设计的算法性能，并呼吁系统研究社区适应AI时代。


<details>
  <summary>Details</summary>
Motivation: 系统研究长期以来专注于设计和评估新算法，而AI驱动的解决方案发现因其自然的可靠验证机制（如性能测量）而特别适合系统性能问题。

Method: 本文提出AI-Driven Research for Systems（ADRS）方法，通过迭代生成、评估和优化解决方案，并利用penEvolve这一开源实例进行多领域案例研究。

Result: ADRS在多个实例中发现了优于当前最先进人工设计的算法（如运行时提升5.0倍或成本降低50%）。

Conclusion: 本文强调了AI驱动研究（ADRS）在系统研究中的颠覆性潜力，并呼吁系统研究社区适应AI时代的研究实践。

Abstract: Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.

</details>


### [143] [TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning](https://arxiv.org/abs/2510.06217)
*Jiaru Zou,Soumya Roy,Vinay Kumar Verma,Ziyi Wang,David Wipf,Pan Lu,Sumit Negi,James Zou,Jingrui He*

Main category: cs.AI

TL;DR: TaTToo是一个新型表格基础PRM框架，通过显式推理和工具验证提升表格推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs在表格特定操作（如子表检索和模式交互）上表现不佳，导致性能瓶颈。

Method: 提出了TaTToo框架，包括可扩展的数据标注流程和双阶段训练范式（冷启动监督微调+工具基础奖励塑形的强化学习）。

Result: 在5个具有挑战性的表格推理基准测试中，TaTToo将下游策略LRMs的推理性能提升了30.9%，并超越了强大的PRM基线模型。

Conclusion: TaTToo框架通过显式处理表格推理步骤并集成工具验证，显著提升了大型推理模型在表格推理领域的性能。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [144] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: 论文提出了一种基于强化学习的动态配置分配方法，通过混合奖励设计和自适应训练，显著提升了软件测试效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统需要在高度异构和动态变化的环境中进行严格的预生产测试，但现有组合优化方法静态且不适应非平稳环境，因此需要一种更灵活、自适应的配置分配方法。

Method: 论文提出了一种新颖的强化学习框架，结合Q学习和混合奖励设计（融合模拟结果与实时反馈），并开发了一种自适应在线-离线训练方案，以快速跟踪概率变化并保持长期稳定性。

Result: 大量模拟研究表明，该方法在性能上一致优于静态和基于优化的基线方法，接近理想性能，验证了强化学习在自适应配置分配中的有效性。

Conclusion: 本论文通过引入强化学习框架，为软件系统的配置分配问题提供了一种动态、自适应的解决方案，显著超越了传统静态方法，并在动态测试和资源调度领域展示了广泛的应用潜力。

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [145] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard是一个双阶段框架，通过离线验证和在线监控确保LLM代理的行为安全，解决了敏感领域AI部署的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 在医疗等敏感领域部署自主AI代理存在安全、隐私和合规风险，现有系统无法完全解决这些挑战。

Method: VeriGuard采用双阶段架构：离线阶段包括明确用户意图、合成行为策略并进行测试与形式化验证；在线阶段则通过运行时监控验证每个代理动作是否符合预验证策略。

Result: VeriGuard能够为LLM代理的行为提供形式化的安全保证，确保其符合预定义的安全约束。

Conclusion: VeriGuard通过双阶段架构（离线验证和在线监控）为基于LLM的代理提供形式化的安全保障，显著提升了其可信度。

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [146] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 研究通过Bloom分类法评估LLM在测试生成中的认知能力，发现模型对语言变化部分鲁棒，但标识符突变导致性能骤降。技术元素比叙述描述更关键，少量示例可显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在自动化软件测试中的泛化能力，尤其是对自然语言缺陷报告的推理能力，明确其是否仅依赖记忆模式。

Method: 基于Bloom认知分类法（记忆、理解、应用、分析、评估、创造）的系统评估框架，利用LIBRO框架在Defects4J、GHRB及其变体上测试StarCoder和GPT-4o。

Result: 模型在记忆任务中表现稳定，对语言重述和翻译部分鲁棒，但在标识符突变时性能下降超过60%。提供少量示例可将成功率提升三倍，技术元素（如测试代码和方法名）对生成成功的影响远大于叙述描述。

Conclusion: 该研究揭示了LLM在测试生成中的认知过程，提出了改进性能的具体方向，并为此任务建立了稳健且现实的评估范式。

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [147] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: 研究通过数据驱动方法识别了七种RSE角色，帮助理解贡献者行为和仓库动态。


<details>
  <summary>Details</summary>
Motivation: 为研究软件工程（RSE）开发提供描述和识别常见及罕见模式的方法，帮助个人和团队理解贡献、影响和仓库动态。

Method: 结合软件仓库挖掘和数据驱动角色应用于研究软件（RS），评估了不同合作互动行为模式。

Result: 成功表征了来自1,284个RS仓库的115,174名贡献者，识别并命名了七种从低到高互动性的角色。

Conclusion: 通过分析大型数据集，尽管软件项目在管理因素、研究领域和贡献者背景上存在差异，研究仍成功识别并总结了七种不同的RSE角色。

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [148] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX 是一个开源的 AI 多代理系统，通过结合 AI 代理、形式化方法和 LLMs 自动生成遗留代码的单元测试，提升测试覆盖率和软件可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决遗留代码库中测试覆盖率低和关键价值测试不足的问题，同时提升代码的可读性和文档化。

Method: UnitTenX 利用 AI 代理、形式化方法和 LLMs 的组合来自动化测试生成，特别针对复杂和遗留代码库的挑战。

Result: 实验结果表明，UnitTenX 能够高效生成高质量测试用例，并识别潜在问题，同时提升代码的可读性和文档化。

Conclusion: UnitTenX 提供了一个强大的框架，通过结合 AI 代理、形式化方法和大型语言模型（LLMs），显著提升了遗留代码的测试覆盖率和关键价值测试，从而改善了软件的可靠性和可维护性。

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [149] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 研究LLM和人类代码审查评论的类型及其解决率，发现可读性、错误和维护性评论更易被采纳，LLM与人类审查员互补。


<details>
  <summary>Details</summary>
Motivation: 理解哪些类型的LLM生成的代码审查评论更可能触发代码变更，以识别可操作的评论。

Method: 开发了一个LLM-as-a-Judge工具，基于五个类别的分类法自动分类评论。

Result: 可读性、错误和维护性相关的评论比代码设计相关的评论有更高的解决率。

Conclusion: LLM和人类代码审查员在项目背景下展现出互补的优势和弱点，LLM生成的评论中有相当一部分是可操作的，可以被开发者解决。

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [150] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究分析了SECURITY.md文件在开源社区的漏洞报告过程中的挑战，发现大多数问题是请求添加该文件，且包含链接的报告关闭更快，为改进安全报告政策提供了见解。


<details>
  <summary>Details</summary>
Motivation: 虽然GitHub建议项目采用SECURITY.md文件来概述漏洞报告流程，但此类文件的有效性和操作挑战尚未完全理解。本研究旨在阐明SECURITY.md文件在开源社区漏洞报告过程中面临的挑战。

Method: 对711个随机抽样的与SECURITY.md相关的问题进行分类和内容分析，并对六个社区健康文件（包括SECURITY.md）的问题关闭时间和响应数量进行了定量比较分析。

Result: 分析显示，79.5%的SECURITY.md相关问题为添加该文件的请求，包含链接的报告关闭时间中位数缩短了2天。

Conclusion: 通过分析SECURITY.md文件在漏洞报告过程中的挑战，研究为改善安全报告政策和社区管理提供了实用见解，有助于构建更安全的开源生态系统。

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [151] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: The Software Observatory at OpenEBench helps analyze and improve research software FAIRness, offering trend insights and evaluation tools for better development practices.


<details>
  <summary>Details</summary>
Motivation: To address gaps in research software development by understanding current trends and promoting FAIR principles to ensure scientific progress.

Method: The platform consolidates software metadata from various sources, offers trend analysis, and evaluates software based on FAIR principles through the FAIRsoft Evaluator.

Result: The platform provides comprehensive insights into the Life Sciences research software ecosystem, enabling users to analyze trends and evaluate software FAIRness.

Conclusion: The Software Observatory at OpenEBench serves as a valuable resource for researchers and developers, promoting better adherence to FAIR principles and enhancing research software development practices.

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [152] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: 数字孪生技术有望优化软件工程过程，支持专家协作，但实现和部署仍需解决关键问题。


<details>
  <summary>Details</summary>
Motivation: 利用数字孪生技术更好地表示、理解和优化软件工程过程，以解决软件工程师短缺问题，并支持领域专家生产高质量软件。

Method: 通过探讨数字孪生的概念及其在软件工程中的潜在应用，提出了一个愿景框架。

Result: 提出了软件工程数字孪生的概念框架，并讨论了其潜在益处及实现挑战。

Conclusion: 本文概述了数字孪生在软件工程中的应用潜力，指出了实现和部署软件工程数字孪生仍需解决的问题。

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [153] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum是一个4B参数的开放权重代码补全模型家族，专为JetBrains IDE设计，通过严格数据治理和多阶段训练实现高效交互式补全。


<details>
  <summary>Details</summary>
Motivation: 开发一个紧凑、任务集中的模型，满足交互式代码补全的成本和延迟约束，同时提供高质量建议。

Method: 采用Llama风格架构，4B参数，预训练于4T多语言代码数据，包括填空训练和项目上下文监督微调，并通过直接偏好优化进行对齐。

Result: Mellum模型在离线基准测试和在线生产部署中表现优异，已发布在HuggingFace上，供实践者参考。

Conclusion: Mellum模型家族通过严格的数据治理、多阶段训练和实时反馈优化，成功实现了在JetBrains IDE中的高效交互式代码补全。

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [154] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: 研究发现远程入职降低员工留存率，混合工作模式结合导师制可有效提升知识密集型企业的员工留存。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情永久改变了职场结构，但完全远程工作对软件团队带来挑战，尤其是员工留存问题。

Method: 利用2016-2025年爱立信瑞典分公司的HR数据，分析不同工作模式（现场、远程和混合）对员工留存的影响。

Result: 研究发现，疫情期间远程入职的员工在头三年内离职率显著更高，而公司最终成功恢复到疫情前的留存率，证明了差异化工作政策的价值。

Conclusion: 研究表明，精心设计的混合工作模式，结合组织归属感和导师制，可以有效维持知识密集型企业的员工留存率。

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [155] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: 本文提出了LLM驱动报告系统的实现模式，解决了上下文窗口限制和未记录的架构问题，提供了双响应模式和多租户安全等实用指南。


<details>
  <summary>Details</summary>
Motivation: 由于上下文窗口限制，大型语言模型无法直接部署在需要处理完整数据集的报告系统中，而现有规范未记录可扩展报告架构的实现模式。

Method: 介绍了双响应模式，扩展ResourceLink以支持迭代查询优化和带外数据访问，同时提供了多租户安全和资源生命周期管理的模式。

Result: 提出了构建LLM驱动的报告系统的模式，将查询生成与数据检索解耦。

Conclusion: 本文提出的模式解决了LLM驱动报告应用中的基本挑战，并提供了实用的开发指南。

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [156] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究发现，开发者广泛使用GenAI进行代码生成，但熟练用户更倾向于将其用于复杂任务。多轮对话比单次提示更受欢迎，文档任务可靠性最高，而复杂代码生成和调试仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程已成为关键技能，但现有研究主要关注个体技术而非开发者更广泛的工作流程。本研究旨在系统调查软件工程师如何将GenAI工具集成到专业实践中。

Method: 通过对91名软件工程师（包括72名活跃的GenAI用户）进行大规模调查，研究了提示策略、对话模式和可靠性评估。

Result: 14项关键发现表明，代码生成几乎普遍，而熟练度与使用AI进行调试和代码审查等更复杂任务密切相关。开发者更喜欢迭代的多轮对话而非单次提示。文档任务被认为最可靠，而复杂代码生成和调试则面临较大挑战。

Conclusion: 该研究为当前开发者实践提供了经验基准，从简单的代码生成到更深层次的工作流程集成，并为未来改进提供了可操作的见解。

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [157] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 研究探讨LLM如何将故障预测指标转化为易懂的风险解释，帮助OSS贡献者更高效地规划和审查代码变更。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）贡献者面临理解复杂代码变更的挑战，现有静态分析和缺陷预测工具的指标难以解读。

Method: 研究探讨LLM是否能将故障预测指标转化为人类可读的风险解释和可操作指导，并计划通过基于任务的研究评估其有用性。

Result: 提出了LLM生成助手的解释类型（描述性、上下文性和可操作性解释），并计划进一步评估其实际效果。

Conclusion: LLMs有望将故障预测指标转化为清晰、可读的风险解释和可操作建议，帮助OSS贡献者更好地规划和审查代码修改。

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [158] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: 研究利用大型语言模型自动修复不可编译的学生代码，评估了不同模型在保留学生代码意图方面的表现，为学习建模提供更全面的数据。


<details>
  <summary>Details</summary>
Motivation: CS1学习环境中大量学生编程提交无法编译，影响了学生建模和知识追踪。传统方法常忽略这些案例，导致学习观察的损失。

Method: 研究评估了三种大型语言模型（GPT-5、Claude 3.5 Haiku和Gemini 2.5 Flash）在高语境和低语境提示条件下的修复能力，主要考察编译性、编辑距离以及对学生原始结构和逻辑的保留程度。

Result: 所有三种LLM均能生成可编译的修复代码，但在保留学生控制流和代码结构方面表现各异，影响了其教学实用性。

Conclusion: 本研究通过自动程序修复策略，成功恢复了不可编译的学生代码，同时保留了学生的结构意图，为学生学习建模提供了更丰富的数据支持。

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [159] [VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing](https://arxiv.org/abs/2510.05213)
*Yixiao Wang,Mingxiao Huo,Zhixuan Liang,Yushi Du,Lingfeng Sun,Haotian Lin,Jinghuan Shang,Chensheng Peng,Mohit Bansal,Mingyu Ding,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: VER通过动态选择预训练的视觉专家库中的任务相关专家，提升了机器人学习任务的性能，并在多个任务中达到最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型通常在特定领域表现优异，但跨任务通用性有限。直接蒸馏多个模型可能导致不灵活的任务特定特征选择，并需要昂贵的全模型重新训练以整合机器人领域知识。

Method: VER在预训练阶段将多个视觉基础模型蒸馏到一个视觉专家库中，然后仅微调一个轻量级路由网络（参数少于0.4%）来动态选择任务相关专家。此外，引入了Patchwise Expert Routing with Curriculum Top-K Annealing以提高动态专家选择的灵活性和精确性。

Result: VER在17种多样化机器人任务中实现了最先进的性能，减少了任务无关区域的大范数异常值，并集中关注任务关键区域。

Conclusion: VER通过动态选择任务相关专家库中的视觉专家，显著提升了机器人学习任务的性能，并在17种多样化任务中实现了最先进的性能。

Abstract: Pretrained vision foundation models (VFMs) advance robotic learning via rich
visual representations, yet individual VFMs typically excel only in specific
domains, limiting generality across tasks. Distilling multiple VFMs into a
unified representation for policy can mitigate this limitation but often yields
inflexible task-specific feature selection and requires costly full re-training
to incorporate robot-domain knowledge. We propose VER, a Vision Expert
transformer for Robot learning. During pretraining, VER distills multiple VFMs
into a vision expert library. It then fine-tunes only a lightweight routing
network (fewer than 0.4% of parameters) to dynamically select task-relevant
experts from the pretrained library for downstream robot tasks. We further
introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve
both flexibility and precision of dynamic expert selection. Moreover, VER
supports parameter-efficient finetuning for scalable expert utilization and
adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks
and multiple policy heads, VER achieves state-of-the-art performance. We find
that VER reduces large-norm outliers in task-irrelevant regions (e.g.,
background) and concentrates on task-critical regions. Visualizations and codes
can be found in https://yixiaowang7.github.io/ver_page/.

</details>


### [160] [Adaptive Dynamics Planning for Robot Navigation](https://arxiv.org/abs/2510.05330)
*Lu Yuanjie,Mao Mingyang,Xu Tong,Wang Linji,Lin Xiaomin,Xiao Xuesu*

Main category: cs.RO

TL;DR: ADP uses reinforcement learning to dynamically adjust robot dynamics, improving navigation performance across diverse environments.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of static dynamics settings in hierarchical planning systems, which fail to adapt to environmental complexity variations.

Method: Proposes Adaptive Dynamics Planning (ADP), a learning-augmented paradigm that uses reinforcement learning to dynamically adjust robot dynamics properties.

Result: ADP improves navigation success, safety, and efficiency when integrated into three different planners and a standalone navigation system.

Conclusion: ADP consistently improves navigation success, safety, and efficiency in both simulation and real-world tests.

Abstract: Autonomous robot navigation systems often rely on hierarchical planning,
where global planners compute collision-free paths without considering
dynamics, and local planners enforce dynamics constraints to produce executable
commands. This discontinuity in dynamics often leads to trajectory tracking
failure in highly constrained environments. Recent approaches integrate
dynamics within the entire planning process by gradually decreasing its
fidelity, e.g., increasing integration steps and reducing collision checking
resolution, for real-time planning efficiency. However, they assume that the
fidelity of the dynamics should decrease according to a manually designed
scheme. Such static settings fail to adapt to environmental complexity
variations, resulting in computational overhead in simple environments or
insufficient dynamics consideration in obstacle-rich scenarios. To overcome
this limitation, we propose Adaptive Dynamics Planning (ADP), a
learning-augmented paradigm that uses reinforcement learning to dynamically
adjust robot dynamics properties, enabling planners to adapt across diverse
environments. We integrate ADP into three different planners and further design
a standalone ADP-based navigation system, benchmarking them against other
baselines. Experiments in both simulation and real-world tests show that ADP
consistently improves navigation success, safety, and efficiency.

</details>


### [161] [A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation](https://arxiv.org/abs/2510.05382)
*Zhuowei Xu,Zilin Si,Kevin Zhang,Oliver Kroemer,Zeynep Temel*

Main category: cs.RO

TL;DR: 低成本多模态触觉指尖设计，结合应变计和麦克风传感器，提升机器人手在视觉遮挡下的操作性能。


<details>
  <summary>Details</summary>
Motivation: 解决触觉传感器在机器人手中应用受限的问题，如高成本、制造集成困难及信号提取不可靠。

Method: 使用应变计传感器捕捉静态力，接触式麦克风传感器测量接触时的高频振动，所有传感器集成在紧凑的指尖设计内部。

Result: 应变计传感器在0-5 N范围内提供可重复的2D平面力测量，接触式麦克风传感器能区分接触材料特性，设计在三种灵巧操作任务中表现优异。

Conclusion: 集成了多模态触觉传感器的低成本、易制作、适应性强的指尖设计，显著提高了机器人手在视觉遮挡情况下的操作性能。

Abstract: Tactile sensing holds great promise for enhancing manipulation precision and
versatility, but its adoption in robotic hands remains limited due to high
sensor costs, manufacturing and integration challenges, and difficulties in
extracting expressive and reliable information from signals. In this work, we
present a low-cost, easy-to-make, adaptable, and compact fingertip design for
robotic hands that integrates multi-modal tactile sensors. We use strain gauge
sensors to capture static forces and a contact microphone sensor to measure
high-frequency vibrations during contact. These tactile sensors are integrated
into a compact design with a minimal sensor footprint, and all sensors are
internal to the fingertip and therefore not susceptible to direct wear and tear
from interactions. From sensor characterization, we show that strain gauge
sensors provide repeatable 2D planar force measurements in the 0-5 N range and
the contact microphone sensor has the capability to distinguish contact
material properties. We apply our design to three dexterous manipulation tasks
that range from zero to full visual occlusion. Given the expressiveness and
reliability of tactile sensor readings, we show that different tactile sensing
modalities can be used flexibly in different stages of manipulation, solely or
together with visual observations to achieve improved task performance. For
instance, we can precisely count and unstack a desired number of paper cups
from a stack with 100\% success rate which is hard to achieve with vision only.

</details>


### [162] [Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios](https://arxiv.org/abs/2510.05425)
*Marta Lagomarsino,Francesco Tassi*

Main category: cs.RO

TL;DR: 本文提出了一种自适应人机交互框架，通过集成移动性模型和分层最优控制器，帮助上肢残疾用户在工作环境中更有效地参与。


<details>
  <summary>Details</summary>
Motivation: 工作环境通常不适合上肢残疾人士，缺乏包容性。本文旨在通过自适应人机交互框架解决这一问题，促进其积极参与工作。

Method: 通过将特定关节限制的移动性模型集成到分层最优控制器中，使机器人能够在线生成反应性、移动性感知行为，并引导用户受损肢体利用残余功能移动性。

Result: 初步结果表明，该框架能够根据用户受损的活动范围个性化交互，并根据其功能限制的严重程度鼓励关节使用。

Conclusion: 该论文提出的自适应人机交互框架能够有效适应上肢残疾用户的活动范围，鼓励其根据功能限制的严重程度使用关节，从而促进工作参与。

Abstract: Work environments are often inadequate and lack inclusivity for individuals
with upper-body disabilities. This paper presents a novel online framework for
adaptive human-robot interaction (HRI) that accommodates users' arm mobility
impairments, ultimately aiming to promote active work participation. Unlike
traditional human-robot collaboration approaches that assume able-bodied users,
our method integrates a mobility model for specific joint limitations into a
hierarchical optimal controller. This allows the robot to generate reactive,
mobility-aware behaviour online and guides the user's impaired limb to exploit
residual functional mobility. The framework was tested in handover tasks
involving different upper-limb mobility impairments (i.e., emulated elbow and
shoulder arthritis, and wrist blockage), under both standing and seated
configurations with task constraints using a mobile manipulator, and
complemented by quantitative and qualitative comparisons with state-of-the-art
ergonomic HRI approaches. Preliminary results indicated that the framework can
personalise the interaction to fit within the user's impaired range of motion
and encourage joint usage based on the severity of their functional
limitations.

</details>


### [163] [Active Semantic Perception](https://arxiv.org/abs/2510.05430)
*Huayi Tang,Pratik Chaudhari*

Main category: cs.RO

TL;DR: 提出了一种基于LLMs的主动语义感知方法，通过层次化场景图和信息增益计算，在复杂室内环境中实现了快速准确的语义识别。


<details>
  <summary>Details</summary>
Motivation: 旨在利用场景语义进行探索等任务，解决复杂室内环境中语义感知的挑战。

Method: 开发了一种紧凑的层次化多层场景图，结合LLMs生成未观察区域的合理场景图样本，用于计算潜在路径点的信息增益。

Result: 在模拟的复杂3D室内环境中，该方法在定性和定量实验中都表现出比基线方法更快、更准确地确定环境语义的能力。

Conclusion: 该研究提出了一种基于大型语言模型（LLMs）的主动语义感知方法，通过构建层次化的多层场景图来表示复杂室内环境，并在模拟环境中验证了其快速准确捕捉环境语义的能力。

Abstract: We develop an approach for active semantic perception which refers to using
the semantics of the scene for tasks such as exploration. We build a compact,
hierarchical multi-layer scene graph that can represent large, complex indoor
environments at various levels of abstraction, e.g., nodes corresponding to
rooms, objects, walls, windows etc. as well as fine-grained details of their
geometry. We develop a procedure based on large language models (LLMs) to
sample plausible scene graphs of unobserved regions that are consistent with
partial observations of the scene. These samples are used to compute an
information gain of a potential waypoint for sophisticated spatial reasoning,
e.g., the two doors in the living room can lead to either a kitchen or a
bedroom. We evaluate this approach in complex, realistic 3D indoor environments
in simulation. We show using qualitative and quantitative experiments that our
approach can pin down the semantics of the environment quicker and more
accurately than baseline approaches.

</details>


### [164] [AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control](https://arxiv.org/abs/2510.05443)
*Shao-Yi Yu,Jen-Wei Wang,Maya Horii,Vikas Garg,Tarek Zohdi*

Main category: cs.RO

TL;DR: 提出了一种自适应动力学模型，通过神经常微分方程和两阶段训练处理环境变化，实验验证了其在多种机器人平台上的有效性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在不确定环境中需要能够适应环境变化的动力学模型，尤其是在直接获取环境信息有限的情况下。

Method: 基于神经常微分方程的自适应动力学模型，采用两阶段训练程序学习潜在环境表示。

Result: 实验结果表明，该方法在三种不同复杂度的机器人平台上（二维差速轮式机器人、三维四旋翼飞行器和Sphero BOLT机器人）均能有效处理时空变化的环境变化。

Conclusion: 该方法通过自适应动力学模型成功处理了时空变化的环境变化，在仿真和现实系统中均表现出色。

Abstract: Mobile robots, such as ground vehicles and quadrotors, are becoming
increasingly important in various fields, from logistics to agriculture, where
they automate processes in environments that are difficult to access for
humans. However, to perform effectively in uncertain environments using
model-based controllers, these systems require dynamics models capable of
responding to environmental variations, especially when direct access to
environmental information is limited. To enable such adaptivity and facilitate
integration with model predictive control, we propose an adaptive dynamics
model which bypasses the need for direct environmental knowledge by inferring
operational environments from state-action history. The dynamics model is based
on neural ordinary equations, and a two-phase training procedure is used to
learn latent environment representations. We demonstrate the effectiveness of
our approach through goal-reaching and path-tracking tasks on three robotic
platforms of increasing complexity: a 2D differential wheeled robot with
changing wheel contact conditions, a 3D quadrotor in variational wind fields,
and the Sphero BOLT robot under two contact conditions for real-world
deployment. Empirical results corroborate that our method can handle temporally
and spatially varying environmental changes in both simulation and real-world
systems.

</details>


### [165] [Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation](https://arxiv.org/abs/2510.05536)
*Mahboubeh Zarei,Robin Chhabra,Farrokh Janabi-Sharifi*

Main category: cs.RO

TL;DR: 本文提出了一种基于双视角和自适应扩展卡尔曼滤波器的分散式姿态与速度估计方法，实验证明其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 精确的姿态和速度估计对于机器人空间任务规划至关重要，传统集中式传感器融合方法存在局限性，因此需要探索分散式融合方法。

Method: 使用双视角测量配置（眼在手和眼对手）以及两个独立的自适应扩展卡尔曼滤波器（基于矩阵李群构建），通过相关性感知融合规则在李群上进行状态融合。

Result: 在UFactory xArm 850机器人上进行的实验验证了所提方法的有效性和鲁棒性，相比现有方法有显著提升。

Conclusion: 本文提出了一种新颖的分散式融合方法，用于估计机器人的姿态和速度，实验结果表明该方法在准确性和鲁棒性上优于现有技术。

Abstract: Accurate pose and velocity estimation is essential for effective spatial task
planning in robotic manipulators. While centralized sensor fusion has
traditionally been used to improve pose estimation accuracy, this paper
presents a novel decentralized fusion approach to estimate both pose and
velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand
vision sensor configuration mounted on a manipulator to track a target object
whose motion is modeled as random walk (stochastic acceleration model). The
robot runs two independent adaptive extended Kalman filters formulated on a
matrix Lie group, developed as part of this work. These filters predict poses
and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times
\mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final
fused state comprising the fused pose and velocities of the target is obtained
using a correlation-aware fusion rule on Lie groups. The proposed method is
evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,
tracking a moving target. Experimental results validate the effectiveness and
robustness of the proposed decentralized dual-view estimation framework,
showing consistent improvements over state-of-the-art methods.

</details>


### [166] [ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation](https://arxiv.org/abs/2510.05547)
*Eugene Vorobiov,Ammar Jaleel Mahmood,Salim Rezvani,Robin Chhabra*

Main category: cs.RO

TL;DR: ARRC系统结合RAG与RGB-D感知，通过LLM生成动作计划并在机器人上执行，实验证明其在任务中高效且安全。


<details>
  <summary>Details</summary>
Motivation: 旨在将自然语言指令与安全的本地机器人控制连接起来，通过整合RAG、RGB-D感知和保护性执行，实现高效且安全的机器人任务执行。

Method: 系统通过向量数据库索引机器人知识（移动模式、任务模板和安全启发式），检索任务相关上下文，并利用大型语言模型（LLM）生成JSON结构化的动作计划。计划在配备Dynamixel驱动平行夹爪和Intel RealSense D435相机的UFactory xArm 850上执行。感知使用AprilTag检测与深度融合生成以物体为中心的度量姿态。

Result: 实验结果表明，该方法在桌面扫描、接近和拾取放置任务中有效，显著提高了计划的有效性和适应性。

Conclusion: ARRC系统通过结合检索增强生成（RAG）与RGB-D感知及保护性执行，显著提升了机器人任务规划的效力和适应性，同时保持感知和底层控制的本地化。

Abstract: We present ARRC (Advanced Reasoning Robot Control), a practical system that
connects natural-language instructions to safe local robotic control by
combining Retrieval-Augmented Generation (RAG) with RGB-D perception and
guarded execution on an affordable robot arm. The system indexes curated robot
knowledge (movement patterns, task templates, and safety heuristics) in a
vector database, retrieves task-relevant context for each instruction, and
conditions a large language model (LLM) to produce JSON-structured action
plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven
parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag
detections fused with depth to produce object-centric metric poses. Execution
is enforced via software safety gates: workspace bounds, speed and force caps,
timeouts, and bounded retries. We describe the architecture, knowledge design,
integration choices, and a reproducible evaluation protocol for tabletop scan,
approach, and pick-place tasks. Experimental results demonstrate the efficacy
of the proposed approach. Our design shows that RAG-based planning can
substantially improve plan validity and adaptability while keeping perception
and low-level control local to the robot.

</details>


### [167] [GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps](https://arxiv.org/abs/2510.05553)
*Yan Rui Tan,Wenqi Liu,Wai Lun Leong,John Guan Zhong Tan,Wayne Wen Huei Yong,Fan Shi,Rodney Swee Huat Teo*

Main category: cs.RO

TL;DR: GO-Flock是一种混合集群框架，结合规划与反应式APF控制，有效解决了传统APF方法在复杂环境中的问题，并在实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统APF方法在障碍物存在时容易陷入局部最小值和死锁，现有解决方案多为被动式，导致集群导航效率低下。

Method: GO-Flock采用混合框架，包括上游的感知模块（处理深度图以提取路径点和虚拟代理）和下游的集体导航模块（应用新型APF策略）。

Result: GO-Flock在障碍物密集环境和硬件在环实验中表现出色，成功实现了九架无人机（六架实体和三架虚拟）在森林环境中的集群飞行。

Conclusion: GO-Flock通过结合规划与反应式APF控制，有效解决了传统APF方法在复杂环境中的局限性，成功实现了无人机群在障碍物密集环境中的高效集群导航。

Abstract: Artificial Potential Field (APF) methods are widely used for reactive
flocking control, but they often suffer from challenges such as deadlocks and
local minima, especially in the presence of obstacles. Existing solutions to
address these issues are typically passive, leading to slow and inefficient
collective navigation. As a result, many APF approaches have only been
validated in obstacle-free environments or simplified, pseudo 3D simulations.
This paper presents GO-Flock, a hybrid flocking framework that integrates
planning with reactive APF-based control. GO-Flock consists of an upstream
Perception Module, which processes depth maps to extract waypoints and virtual
agents for obstacle avoidance, and a downstream Collective Navigation Module,
which applies a novel APF strategy to achieve effective flocking behavior in
cluttered environments. We evaluate GO-Flock against passive APF-based
approaches to demonstrate their respective merits, such as their flocking
behavior and the ability to overcome local minima. Finally, we validate
GO-Flock through obstacle-filled environment and also hardware-in-the-loop
experiments where we successfully flocked a team of nine drones, six physical
and three virtual, in a forest environment.

</details>


### [168] [DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation](https://arxiv.org/abs/2510.05662)
*Taeyeop Lee,Gyuree Kang,Bowen Wen,Youngho Kim,Seunghyeok Back,In So Kweon,David Hyunchul Shim,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: DeLTa是一种结合深度估计、6D位姿估计和视觉语言规划的新框架，通过单次演示实现对新透明物体的长时程精确操作，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前透明物体机器人操作研究多限于短时程任务和基础抓取能力，缺乏对新物体的泛化性和长时程精确操作的支持。

Method: DeLTa整合了深度估计、6D位姿估计和视觉语言规划，采用单次演示方法，无需类别先验或额外训练即可泛化到新透明物体。

Result: DeLTa在长时程透明物体操作任务中显著优于现有方法，尤其在需要精确操作的场景中表现突出。

Conclusion: DeLTa框架通过结合深度估计、6D位姿估计和视觉语言规划，显著提升了透明物体长时程精确操作的能力，尤其在面对新物体时表现出优越的泛化性。

Abstract: Despite the prevalence of transparent object interactions in human everyday
life, transparent robotic manipulation research remains limited to
short-horizon tasks and basic grasping capabilities.Although some methods have
partially addressed these issues, most of them have limitations in
generalizability to novel objects and are insufficient for precise long-horizon
robot manipulation. To address this limitation, we propose DeLTa (Demonstration
and Language-Guided Novel Transparent Object Manipulation), a novel framework
that integrates depth estimation, 6D pose estimation, and vision-language
planning for precise long-horizon manipulation of transparent objects guided by
natural task instructions. A key advantage of our method is its
single-demonstration approach, which generalizes 6D trajectories to novel
transparent objects without requiring category-level priors or additional
training. Additionally, we present a task planner that refines the
VLM-generated plan to account for the constraints of a single-arm, eye-in-hand
robot for long-horizon object manipulation tasks. Through comprehensive
evaluation, we demonstrate that our method significantly outperforms existing
transparent object manipulation approaches, particularly in long-horizon
scenarios requiring precise manipulation capabilities. Project page:
https://sites.google.com/view/DeLTa25/

</details>


### [169] [Verifier-free Test-Time Sampling for Vision Language Action Models](https://arxiv.org/abs/2510.05681)
*Suhyeok Jang,Dongyoung Kim,Changyeon Kim,Youngsuk Kim,Jinwoo Shin*

Main category: cs.RO

TL;DR: MG-Select是一种无需额外训练或外部模块的测试时扩展框架，通过KL散度选择最优动作，显著提升VLA模型在高精度任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在高精度任务中表现受限，测试时扩展方法需要额外训练且泛化能力不足。

Method: 提出了MG-Select框架，通过KL散度作为置信度指标，从多个候选动作中选择最优动作。引入随机掩码状态和语言条件生成的参考分布，并采用联合训练策略。

Result: MG-Select在真实世界任务中表现显著提升，包括28%/35%的性能改进，以及168%的相对增益。

Conclusion: MG-Select通过利用VLA模型的内部特性，无需额外训练或外部模块，显著提升了任务性能，特别是在高精度要求的任务中。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
in robot control. However, they remain fundamentally limited in tasks that
require high precision due to their single-inference paradigm. While test-time
scaling approaches using external verifiers have shown promise, they require
additional training and fail to generalize to unseen conditions. We propose
Masking Distribution Guided Selection (MG-Select), a novel test-time scaling
framework for VLAs that leverages the model's internal properties without
requiring additional training or external modules. Our approach utilizes KL
divergence from a reference action token distribution as a confidence metric
for selecting the optimal action from multiple candidates. We introduce a
reference distribution generated by the same VLA but with randomly masked
states and language conditions as inputs, ensuring maximum uncertainty while
remaining aligned with the target task distribution. Additionally, we propose a
joint training strategy that enables the model to learn both conditional and
unconditional distributions by applying dropout to state and language
conditions, thereby further improving the quality of the reference
distribution. Our experiments demonstrate that MG-Select achieves significant
performance improvements, including a 28%/35% improvement in real-world
in-distribution/out-of-distribution tasks, along with a 168% relative gain on
RoboCasa pick-and-place tasks trained with 30 demonstrations.

</details>


### [170] [Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies](https://arxiv.org/abs/2510.05692)
*Yuhang Zhang,Jiaping Xiao,Chao Yan,Mir Feroskhan*

Main category: cs.RO

TL;DR: OMC-RL通过分阶段学习和教师指导，提升了视觉运动策略学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维视觉输入与敏捷动作输出结合导致的样本效率低和模拟到现实差距大的问题。

Method: OMC-RL将学习过程分为上游表示学习阶段和下游策略学习阶段。上游阶段使用掩码Transformer模块进行时间建模和对比学习，提取任务相关表示；下游阶段通过教师策略提供早期指导并逐步减少。

Result: 实验表明，OMC-RL在模拟和真实环境中均表现出更高的样本效率和渐近策略性能，且泛化能力更强。

Conclusion: OMC-RL框架通过分阶段学习和教师策略指导，显著提高了视觉运动策略学习的样本效率和渐近性能，同时增强了在复杂场景中的泛化能力。

Abstract: A prevailing approach for learning visuomotor policies is to employ
reinforcement learning to map high-dimensional visual observations directly to
action commands. However, the combination of high-dimensional visual inputs and
agile maneuver outputs leads to long-standing challenges, including low sample
efficiency and significant sim-to-real gaps. To address these issues, we
propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a
novel framework designed to improve the sample efficiency and asymptotic
performance of visuomotor policy learning. OMC-RL explicitly decouples the
learning process into two stages: an upstream representation learning stage and
a downstream policy learning stage. In the upstream stage, a masked Transformer
module is trained with temporal modeling and contrastive learning to extract
temporally-aware and task-relevant representations from sequential visual
inputs. After training, the learned encoder is frozen and used to extract
visual representations from consecutive frames, while the Transformer module is
discarded. In the downstream stage, an oracle teacher policy with privileged
access to global state information supervises the agent during early training
to provide informative guidance and accelerate early policy learning. This
guidance is gradually reduced to allow independent exploration as training
progresses. Extensive experiments in simulated and real-world environments
demonstrate that OMC-RL achieves superior sample efficiency and asymptotic
policy performance, while also improving generalization across diverse and
perceptually complex scenarios.

</details>


### [171] [Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs](https://arxiv.org/abs/2510.05707)
*David Boetius,Abdelrahman Abdelnaby,Ashok Kumar,Stefan Leue,Abdalla Swikir,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 提出了一种在黎曼流形上学习稳定动力系统的神经ODE框架，通过Lyapunov稳定性保证，并在仿真和实际机器人实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决黎曼流形上轨迹稳定性保证的挑战，确保机器人运动规划的安全性和可靠性。

Method: 使用神经ODE投影神经向量场，严格满足Lyapunov稳定性准则，确保系统状态的稳定性。

Result: 在单位四元数和对称正定矩阵流形上成功解决了Riemannian LASA数据集，并在实际机器人实验中验证了方法的性能。

Conclusion: 该论文提出了一种在黎曼流形上学习稳定动力系统的通用框架，通过神经ODE和Lyapunov稳定性准则确保系统稳定性，并在实际机器人运动中验证了其性能。

Abstract: Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.

</details>


### [172] [Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions](https://arxiv.org/abs/2510.05713)
*Wanli Ni,Hui Tian,Shuai Wang,Chengyang Li,Lei Sun,Zhaohui Yang*

Main category: cs.RO

TL;DR: 本文研究了适用于工业物联网的联邦分割学习框架，比较了不同框架和融合策略，并提出了优化技术，仿真验证了其性能，最后讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网（IoT）系统中数据隐私、通信效率和设备异构性是关键问题，FedSL作为一种有前景的范式，适用于资源受限的工业机器人场景。

Method: 比较了同步、异步、分层和异构的FedSL框架，并系统地将令牌融合策略分为输入级（预融合）、中间级（内融合）和输出级（后融合）三种范式。

Result: 仿真结果验证了这些框架在工业检测场景下的性能。

Conclusion: 本文概述了联邦分割学习（FedSL）在未来智能制造系统中的开放问题和研究方向。

Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for
enabling collaborative intelligence in industrial Internet of Things (IoT)
systems, particularly in smart factories where data privacy, communication
efficiency, and device heterogeneity are critical concerns. In this article, we
present a comprehensive study of FedSL frameworks tailored for
resource-constrained robots in industrial scenarios. We compare synchronous,
asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of
workflow, scalability, adaptability, and limitations under dynamic industrial
conditions. Furthermore, we systematically categorize token fusion strategies
into three paradigms: input-level (pre-fusion), intermediate-level
(intra-fusion), and output-level (post-fusion), and summarize their respective
strengths in industrial applications. We also provide adaptive optimization
techniques to enhance the efficiency and feasibility of FedSL implementation,
including model compression, split layer selection, computing frequency
allocation, and wireless resource management. Simulation results validate the
performance of these frameworks under industrial detection scenarios. Finally,
we outline open issues and research directions of FedSL in future smart
manufacturing systems.

</details>


### [173] [Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving](https://arxiv.org/abs/2510.05729)
*Marc Kaufeld,Johannes Betz*

Main category: cs.RO

TL;DR: 该研究提出两种高效方法，用于估计自动驾驶规划轨迹在不确定条件下的碰撞概率，结合状态不确定性并实现实时计算，已开源。


<details>
  <summary>Details</summary>
Motivation: 确定性碰撞检查在不确定驾驶条件下往往不准确或过于保守，因为感知噪声、定位误差和其他交通参与者的预测不确定性会在规划过程中引入显著的不确定性。

Method: 第一种方法评估自动驾驶车辆与周围障碍物的空间重叠概率，第二种方法基于随机边界穿越估计碰撞概率。两种方法均考虑了完整的状态不确定性（位置、方向和速度）。

Result: 仿真研究表明，所提方法能够准确匹配蒙特卡洛结果，同时显著节省计算时间，适用于风险感知轨迹规划。

Conclusion: 本研究提出的两种半解析方法能够高效计算自动驾驶规划轨迹与任意凸障碍物的碰撞概率，适用于实时规划，并通过开源软件提供实现。

Abstract: This research introduces two efficient methods to estimate the collision risk
of planned trajectories in autonomous driving under uncertain driving
conditions. Deterministic collision checks of planned trajectories are often
inaccurate or overly conservative, as noisy perception, localization errors,
and uncertain predictions of other traffic participants introduce significant
uncertainty into the planning process. This paper presents two semi-analytic
methods to compute the collision probability of planned trajectories with
arbitrary convex obstacles. The first approach evaluates the probability of
spatial overlap between an autonomous vehicle and surrounding obstacles, while
the second estimates the collision probability based on stochastic boundary
crossings. Both formulations incorporate full state uncertainties, including
position, orientation, and velocity, and achieve high accuracy at computational
costs suitable for real-time planning. Simulation studies verify that the
proposed methods closely match Monte Carlo results while providing significant
runtime advantages, enabling their use in risk-aware trajectory planning. The
collision estimation methods are available as open-source software:
https://github.com/TUM-AVS/Collision-Probability-Estimation

</details>


### [174] [Human-in-the-loop Optimisation in Robot-assisted Gait Training](https://arxiv.org/abs/2510.05780)
*Andreas Christou,Andreas Sochopoulos,Elliot Lister,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文探讨了人机协同循环优化（HILO）在步态训练中提供个性化辅助的潜力，发现尽管优化算法收敛到个性化设置，但对实际表现无显著影响，突显了人机协同适应和人类行为变异性的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于步行模式的显著人际和个体内变异性，设计能够适应每个个体独特特征的机器人控制器至关重要。

Method: 采用协方差矩阵适应进化策略（CMA-ES）持续优化下肢外骨骼的按需辅助控制器。

Result: 结果显示，CMA-ES似乎为每个个体收敛到一组独特的刚度设置，但在验证试验中未观察到对受试者表现的明显影响。

Conclusion: 本文强调了人机协同适应和人类行为变异性的影响可能超过个性化基于规则的辅助控制器的潜在益处，并指出了当前个性化方法在外骨骼辅助步态康复中的局限性。

Abstract: Wearable robots offer a promising solution for quantitatively monitoring gait
and providing systematic, adaptive assistance to promote patient independence
and improve gait. However, due to significant interpersonal and intrapersonal
variability in walking patterns, it is important to design robot controllers
that can adapt to the unique characteristics of each individual. This paper
investigates the potential of human-in-the-loop optimisation (HILO) to deliver
personalised assistance in gait training. The Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) was employed to continuously optimise an
assist-as-needed controller of a lower-limb exoskeleton. Six healthy
individuals participated over a two-day experiment. Our results suggest that
while the CMA-ES appears to converge to a unique set of stiffnesses for each
individual, no measurable impact on the subjects' performance was observed
during the validation trials. These findings highlight the impact of
human-robot co-adaptation and human behaviour variability, whose effect may be
greater than potential benefits of personalising rule-based assistive
controllers. Our work contributes to understanding the limitations of current
personalisation approaches in exoskeleton-assisted gait rehabilitation and
identifies key challenges for effective implementation of human-in-the-loop
optimisation in this domain.

</details>


### [175] [VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation](https://arxiv.org/abs/2510.05827)
*Haoran Zhang,Shuanghao Bai,Wanqi Zhou,Yuedi Zhang,Qi Zhang,Pengxiang Ding,Cheng Chi,Donglin Wang,Badong Chen*

Main category: cs.RO

TL;DR: VCoT-Grasp通过视觉思维链推理增强抓取生成，在复杂环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏足够的推理和泛化能力，或依赖复杂的模块化流程，且过度强调对话和物体语义，导致性能不佳和仅限于单物体抓取。

Method: 提出VCoT-Grasp，一个端到端的抓取基础模型，采用多轮处理范式，动态聚焦视觉输入并提供可解释的推理轨迹。

Result: 在VCoT-GraspSet和真实机器人上的大量实验表明，该方法显著提升了抓取成功率。

Conclusion: VCoT-Grasp显著提高了抓取成功率，并能有效泛化到未见过的物体、背景和干扰物。

Abstract: Robotic grasping is one of the most fundamental tasks in robotic
manipulation, and grasp detection/generation has long been the subject of
extensive research. Recently, language-driven grasp generation has emerged as a
promising direction due to its practical interaction capabilities. However,
most existing approaches either lack sufficient reasoning and generalization
capabilities or depend on complex modular pipelines. Moreover, current grasp
foundation models tend to overemphasize dialog and object semantics, resulting
in inferior performance and restriction to single-object grasping. To maintain
strong reasoning ability and generalization in cluttered environments, we
propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates
visual chain-of-thought reasoning to enhance visual understanding for grasp
generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically
focuses on visual inputs while providing interpretable reasoning traces. For
training, we refine and introduce a large-scale dataset, VCoT-GraspSet,
comprising 167K synthetic images with over 1.36M grasps, as well as 400+
real-world images with more than 1.2K grasps, annotated with intermediate
bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot
demonstrate that our method significantly improves grasp success rates and
generalizes effectively to unseen objects, backgrounds, and distractors. More
details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.

</details>


### [176] [A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling](https://arxiv.org/abs/2510.05923)
*Aman Singh,Aastha Mishra,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的三阶段协同设计优化框架，联合优化机械和控制参数，显著降低能耗并提高跳跃高度，同时简化制造流程。


<details>
  <summary>Details</summary>
Motivation: 现有的协同设计框架通常仅优化最大高度或最小能耗，忽略了二者的权衡，且常省略变速箱参数优化和使用过于简化的执行器质量模型，导致设计难以实际复制。

Method: 引入了一种新颖的三阶段协同设计优化框架，联合优化机械设计（包括变速箱）和控制参数，并采用现实的执行器质量模型。

Result: 实验评估显示，与基线设计相比，机械能耗降低了50%，同时实现了0.8米的跳跃高度。

Conclusion: 本文提出的三阶段协同设计优化框架显著减少了机械能耗，并实现了较高的跳跃高度，同时通过自动生成参数化CAD模型简化了制造流程。

Abstract: A monoped's jump height and energy consumption depend on both, its mechanical
design and control strategy. Existing co-design frameworks typically optimize
for either maximum height or minimum energy, neglecting their trade-off. They
also often omit gearbox parameter optimization and use oversimplified actuator
mass models, producing designs difficult to replicate in practice. In this
work, we introduce a novel three-stage co-design optimization framework that
jointly maximizes jump height while minimizing mechanical energy consumption of
a monoped. The proposed method explicitly incorporates realistic actuator mass
models and optimizes mechanical design (including gearbox) and control
parameters within a unified framework. The resulting design outputs are then
used to automatically generate a parameterized CAD model suitable for direct
fabrication, significantly reducing manual design iterations. Our experimental
evaluations demonstrate a 50 percent reduction in mechanical energy consumption
compared to the baseline design, while achieving a jump height of 0.8m. Video
presentation is available at http://y2u.be/XW8IFRCcPgM

</details>


### [177] [Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion](https://arxiv.org/abs/2510.05957)
*Vaughn Gzenda,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型强化学习（MB-RL）的框架，通过从传感器推断潜在动力学来指导行动者-评论家算法优化运动策略，应用于软体机器人爬行器的运动控制。


<details>
  <summary>Details</summary>
Motivation: 软体机器人爬行器的运动控制设计面临模型不准确、传感器噪声和运动步态发现等挑战。

Method: 采用基于模型的强化学习框架，利用从传感器推断的潜在动力学作为预测模型，指导行动者-评论家算法优化运动策略。

Result: 在模拟环境中，学习到的潜在动力学实现了短时运动预测，行动者-评论家算法发现了有效的运动策略。

Conclusion: 该研究表明，潜在动力学MB-RL框架有潜力仅基于噪声传感器反馈实现软体机器人的自适应运动。

Abstract: Soft robotic crawlers are mobile robots that utilize soft body deformability
and compliance to achieve locomotion through surface contact. Designing control
strategies for such systems is challenging due to model inaccuracies, sensor
noise, and the need to discover locomotor gaits. In this work, we present a
model-based reinforcement learning (MB-RL) framework in which latent dynamics
inferred from onboard sensors serve as a predictive model that guides an
actor-critic algorithm to optimize locomotor policies. We evaluate the
framework on a minimal crawler model in simulation using inertial measurement
units and time-of-flight sensors as observations. The learned latent dynamics
enable short-horizon motion prediction while the actor-critic discovers
effective locomotor policies. This approach highlights the potential of
latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion
based solely on noisy sensor feedback.

</details>


### [178] [The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics](https://arxiv.org/abs/2510.05981)
*Cristina Luna,Alba Guerra,Almudena Moreno,Manuel Esquer,Willy Roa,Mateusz Krawczak,Robert Popela,Piotr Osica,Davide Nicolis*

Main category: cs.RO

TL;DR: DISTANT设计将牵引和转向执行器移至车内温暖箱，解决了极端环境下的移动问题，满足50公里穿越需求。


<details>
  <summary>Details</summary>
Motivation: 行星探测任务需要能在极端环境中长期运行的稳健移动系统，DISTANT设计旨在保护敏感组件免受热循环、灰尘污染和机械磨损的影响。

Method: 采用双叉臂悬架配置，配备卡登接头和卷扬驱动转向，经过全面权衡分析选为最优架构。

Result: 系统实现了独立车轮牵引、转向控制和悬架管理，同时将所有电机化部件保持在受保护环境中，并集成了灰尘防护和热管理解决方案。

Conclusion: DISTANT设计通过将牵引和转向执行器移至受热保护的温暖箱内，解决了长期探测任务中的关键挑战，满足了50公里无性能下降的穿越要求。

Abstract: Planetary exploration missions require robust locomotion systems capable of
operating in extreme environments over extended periods. This paper presents
the DISTANT (Distant Transmission and Steering Systems) design, a novel
approach for relocating rover traction and steering actuators from
wheel-mounted positions to a thermally protected warm box within the rover
body. The design addresses critical challenges in long-distance traversal
missions by protecting sensitive components from thermal cycling, dust
contamination, and mechanical wear. A double wishbone suspension configuration
with cardan joints and capstan drive steering has been selected as the optimal
architecture following comprehensive trade-off analysis. The system enables
independent wheel traction, steering control, and suspension management whilst
maintaining all motorisation within the protected environment. The design meets
a 50 km traverse requirement without performance degradation, with integrated
dust protection mechanisms and thermal management solutions. Testing and
validation activities are planned for Q1 2026 following breadboard
manufacturing at 1:3 scale.

</details>


### [179] [AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations](https://arxiv.org/abs/2510.05985)
*Cristina Luna,Robert Field,Steven Kay*

Main category: cs.RO

TL;DR: AI系统通过障碍物检测、多机器人协作和地形分类，将行星探测车速度提升至1米/秒，验证于火星模拟环境。


<details>
  <summary>Details</summary>
Motivation: 当前行星探测车的移动速度限制在约10厘米/秒，严重制约了探测效率。

Method: 结合FASTNAV远障碍物检测器（FOD）、CISRU多机器人协调框架以及ViBEKO和AIAXR深度学习地形分类研究，通过计算机视觉和深度学习技术实现。

Result: 在火星模拟环境中验证了这些系统，技术成熟度达到4级，显著提高了探测速度、分类准确性和操作安全性。

Conclusion: 集成的AI系统通过三个组件显著提升了行星探测车的自主性，包括高速障碍物检测、多机器人协作框架以及深度学习地形分类，为下一代行星任务提供了可测量的速度、准确性和安全性改进。

Abstract: Current planetary rovers operate at traverse speeds of approximately 10 cm/s,
fundamentally limiting exploration efficiency. This work presents integrated AI
systems which significantly improve autonomy through three components: (i) the
FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s
speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot
coordination framework enabling human-robot collaboration for in-situ resource
utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain
classification studies. Field validation in Mars analogue environments
demonstrated these systems at Technology Readiness Level 4, providing
measurable improvements in traverse speed, classification accuracy, and
operational safety for next-generation planetary missions.

</details>


### [180] [Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations](https://arxiv.org/abs/2510.05992)
*Tien-Dat Nguyen,Thien-Minh Nguyen,Vinh-Hao Nguyen*

Main category: cs.RO

TL;DR: 提出两阶段方法融合UWB和SLAM数据，实现坐标一致且精准的定位，实验验证单次校准可行，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM方法坐标原点重置和UWB定位锚点坐标精确分配的问题，以实现坐标一致且精准的定位。

Method: 提出了一种两阶段方法：第一阶段通过连续时间批量优化问题恢复锚点的3D位置；第二阶段采用滑动窗口优化方案融合UWB和SLAM数据。

Result: 在NTU VIRAL数据集上的实验表明，单次运行数据校准即可支持后续运行的精准定位。

Conclusion: 通过两阶段方法，成功实现了UWB和SLAM数据的校准与融合，确保了在同一环境中坐标一致且精准的定位。实验验证了单次运行数据校准的可行性，并开源了源代码以促进社区发展。

Abstract: Onboard simultaneous localization and mapping (SLAM) methods are commonly
used to provide accurate localization information for autonomous robots.
However, the coordinate origin of SLAM estimate often resets for each run. On
the other hand, UWB-based localization with fixed anchors can ensure a
consistent coordinate reference across sessions; however, it requires an
accurate assignment of the anchor nodes' coordinates. To this end, we propose a
two-stage approach that calibrates and fuses UWB data and SLAM data to achieve
coordinate-wise consistent and accurate localization in the same environment.
In the first stage, we solve a continuous-time batch optimization problem by
using the range and odometry data from one full run, incorporating height
priors and anchor-to-anchor distance factors to recover the anchors' 3D
positions. For the subsequent runs in the second stage, a sliding-window
optimization scheme fuses the UWB and SLAM data, which facilitates accurate
localization in the same coordinate system. Experiments are carried out on the
NTU VIRAL dataset with six scenarios of UAV flight, and we show that
calibration using data in one run is sufficient to enable accurate localization
in the remaining runs. We release our source code to benefit the community at
https://github.com/ntdathp/slam-uwb-calibration.

</details>


### [181] [Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning](https://arxiv.org/abs/2510.06068)
*Heng Zhang,Kevin Yuchen Ma,Mike Zheng Shou,Weisi Lin,Yan Wu*

Main category: cs.RO

TL;DR: 提出基于特征抓取的端到端框架，解决多指手跨形态抓取问题，模拟和真实世界实验均表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多指手灵巧抓取的高维关节问题，以及现有端到端方法对大规模数据集和特定手形态的依赖，提出跨形态抓取生成框架。

Method: 通过形态描述生成形态嵌入和特征抓取集，结合对象点云和手腕姿势，使用振幅预测器回归低维空间中的关节系数，并通过运动感知关节损失（KAL）监督学习。

Result: 在模拟中，对未见过的手和物体实现了91.9%的平均抓取成功率，推理时间低于0.4秒；在少量样本适应后，对未见过的手在模拟和真实世界中的成功率分别为85.6%和87%。

Conclusion: 该论文提出的基于特征抓取的端到端框架在模拟和真实世界中均表现出色，能够快速适应新手的形态，并在未见过的手上实现高成功率。

Abstract: Dexterous grasping with multi-fingered hands remains challenging due to
high-dimensional articulations and the cost of optimization-based pipelines.
Existing end-to-end methods require training on large-scale datasets for
specific hands, limiting their ability to generalize across different
embodiments. We propose an eigengrasp-based, end-to-end framework for
cross-embodiment grasp generation. From a hand's morphology description, we
derive a morphology embedding and an eigengrasp set. Conditioned on these,
together with the object point cloud and wrist pose, an amplitude predictor
regresses articulation coefficients in a low-dimensional space, which are
decoded into full joint articulations. Articulation learning is supervised with
a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant
motions and injects morphology-specific structure. In simulation on unseen
objects across three dexterous hands, our model attains a 91.9% average grasp
success rate with less than 0.4 seconds inference per grasp. With few-shot
adaptation to an unseen hand, it achieves 85.6% success on unseen objects in
simulation, and real-world experiments on this few-shot generalized hand
achieve an 87% success rate. The code and additional materials will be made
available upon publication on our project website
https://connor-zh.github.io/cross_embodiment_dexterous_grasping.

</details>


### [182] [Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor](https://arxiv.org/abs/2510.06085)
*Roman Ibrahimov,Jannik Matthias Heinen*

Main category: cs.RO

TL;DR: 生物启发式多机器人系统利用分布式优化实现高效环境探索与建图，实验验证其高覆盖率和精确地图构建能力。


<details>
  <summary>Details</summary>
Motivation: 受壁随行为的启发，旨在开发一种高效的多机器人系统，用于未知环境的探索与建图，应用于搜救、工业检测和环境监测。

Method: 每个机器人基于触觉传感器（类似蟑螂触角）自主探索周围环境，记录碰撞点而非避障，采用分散控制策略进行任务分配和探索。

Result: 在1.5 x 1.5米模拟环境中使用e-puck机器人进行实验，结果显示系统能高效覆盖环境、减少碰撞并构建精确的2D地图。

Conclusion: 该生物启发式多机器人系统通过分布式优化实现了对未知环境的高效探索与建图，验证了其在高覆盖率、低碰撞和精确2D地图构建方面的有效性。

Abstract: This project proposes a bioinspired multi-robot system using Distributed
Optimization for efficient exploration and mapping of unknown environments.
Each robot explores its environment and creates a map, which is afterwards put
together to form a global 2D map of the environment. Inspired by wall-following
behaviors, each robot autonomously explores its neighborhood based on a tactile
sensor, similar to the antenna of a cockroach, mounted on the surface of the
robot. Instead of avoiding obstacles, robots log collision points when they
touch obstacles. This decentralized control strategy ensures effective task
allocation and efficient exploration of unknown terrains, with applications in
search and rescue, industrial inspection, and environmental monitoring. The
approach was validated through experiments using e-puck robots in a simulated
1.5 x 1.5 m environment with three obstacles. The results demonstrated the
system's effectiveness in achieving high coverage, minimizing collisions, and
constructing accurate 2D maps.

</details>


### [183] [Towards Autonomous Tape Handling for Robotic Wound Redressing](https://arxiv.org/abs/2510.06127)
*Xiao Liang,Lu Shen,Peihan Zhang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 论文提出自主框架处理伤口敷料中的胶带操作，通过模仿学习和轨迹优化实现可靠性能，推动机器人伤口护理自动化。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口护理成本高且依赖人工操作，机器人自动化有望降低成本并改善患者预后。

Method: 采用力反馈模仿学习方法处理胶带初始分离（TID），并基于数值轨迹优化方法确保胶带平滑无皱地粘贴到不同解剖表面。

Result: 通过大量实验验证，所提方法在定量评估和集成伤口敷料流程中表现出可靠性能。

Conclusion: 该论文通过引入自主框架处理伤口敷料中的关键子任务——胶带操作，为机器人伤口护理自动化奠定了重要基础。

Abstract: Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over
6.5 million patients in the United States alone and generate an annual cost
exceeding \$25 billion. Despite this burden, chronic wound care remains a
routine yet manual process performed exclusively by trained clinicians due to
its critical safety demands. We envision a future in which robotics and
automation support wound care to lower costs and enhance patient outcomes. This
paper introduces an autonomous framework for one of the most fundamental yet
challenging subtasks in wound redressing: adhesive tape manipulation.
Specifically, we address two critical capabilities: tape initial detachment
(TID) and secure tape placement. To handle the complex adhesive dynamics of
detachment, we propose a force-feedback imitation learning approach trained
from human teleoperation demonstrations. For tape placement, we develop a
numerical trajectory optimization method based to ensure smooth adhesion and
wrinkle-free application across diverse anatomical surfaces. We validate these
methods through extensive experiments, demonstrating reliable performance in
both quantitative evaluations and integrated wound redressing pipelines. Our
results establish tape manipulation as an essential step toward practical
robotic wound care automation.

</details>


### [184] [Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments](https://arxiv.org/abs/2510.06146)
*Jaehwan Jeong,Tuan-Anh Vu,Radha Lahoti,Jiawen Wang,Vivek Alumootil,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 该研究开发了一个结合视觉引导抓取和振动模型的机器人系统，用于精准授粉，实验验证了其高效性和安全性。


<details>
  <summary>Details</summary>
Motivation: 在受控农业环境中，风力授粉不可行，且商业授粉者的使用受到法规限制，机器人授粉成为替代人工和熊蜂辅助方法的可行方案。

Method: 研究提出了一个视觉引导的机器人框架，结合了3D植物重建、目标抓取规划和基于物理的振动模型。首先进行3D植物重建并注册到机器人坐标系，识别无障碍抓取姿势；其次，使用离散弹性杆模型预测驱动参数与花动态的关系，指导最优授粉策略选择；最后，使用带有软夹具的机械臂抓取茎干并施加受控振动以诱导花粉释放。

Result: 实验显示主茎抓取成功率为92.5%，通过仿真引导的振动参数优化进一步验证了方法的可行性，确保机器人能安全高效地完成授粉而不损伤花朵。

Conclusion: 该研究首次整合了视觉引导抓取和振动建模，实现了自动化精准授粉，成功验证了机器人系统在无损伤花朵的情况下高效完成授粉的可行性。

Abstract: Robotic pollination offers a promising alternative to manual labor and
bumblebee-assisted methods in controlled agriculture, where wind-driven
pollination is absent and regulatory restrictions limit the use of commercial
pollinators. In this work, we present and validate a vision-guided robotic
framework that uses data from an end-effector mounted RGB-D sensor and combines
3D plant reconstruction, targeted grasp planning, and physics-based vibration
modeling to enable precise pollination. First, the plant is reconstructed in 3D
and registered to the robot coordinate frame to identify obstacle-free grasp
poses along the main stem. Second, a discrete elastic rod model predicts the
relationship between actuation parameters and flower dynamics, guiding the
selection of optimal pollination strategies. Finally, a manipulator with soft
grippers grasps the stem and applies controlled vibrations to induce pollen
release. End-to-end experiments demonstrate a 92.5\% main-stem grasping success
rate, and simulation-guided optimization of vibration parameters further
validates the feasibility of our approach, ensuring that the robot can safely
and effectively perform pollination without damaging the flower. To our
knowledge, this is the first robotic system to jointly integrate vision-based
grasping and vibration modeling for automated precision pollination.

</details>


### [185] [A Preview of HoloOcean 2.0](https://arxiv.org/abs/2510.06160)
*Blake Romrell,Abigail Austin,Braden Meyers,Ryan Anderson,Carter Noh,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: HoloOcean 2.0是一个先进的海洋机器人模拟器，集成了Unreal Engine 5.3、Fossen车辆动力学模型和ROS2支持，显著提升了模拟逼真度和功能，支持海洋机器人系统的开发和验证。


<details>
  <summary>Details</summary>
Motivation: 随着海洋机器人领域的快速发展，对高保真模拟器的需求日益增长。HoloOcean 2.0的开发旨在提供一个功能全面、逼真度高的模拟平台，以支持海洋机器人系统的开发和验证。

Method: HoloOcean 2.0采用了Unreal Engine 5.3作为基础平台，结合Fossen的车辆动力学模型和自定义的ROS2桥接器，实现了高保真的海洋传感器、物理和视觉渲染模拟。

Result: HoloOcean 2.0成功集成了多项先进特性，包括Unreal Engine 5.3、Fossen车辆动力学模型和ROS2支持，显著提升了模拟器的性能和功能。此外，还计划开发更多高效和高保真的传感器模拟和环境生成工具。

Conclusion: HoloOcean 2.0作为一个先进的海洋机器人模拟器，通过引入Unreal Engine 5.3、先进的车辆动力学模型和ROS2支持等新特性，显著提升了模拟的逼真度和功能，为海洋机器人系统的开发和验证提供了强有力的工具。

Abstract: Marine robotics simulators play a fundamental role in the development of
marine robotic systems. With increased focus on the marine robotics field in
recent years, there has been significant interest in developing higher
fidelitysimulation of marine sensors, physics, and visual rendering
capabilities to support autonomous marine robot development and validation.
HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art
features under a general marine simulator capable of supporting a variety of
tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)
5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2
using a custom bridge. Additional features are currently in development,
including significantly more efficient ray tracing-based sidescan,
forward-looking, and bathymetric sonar implementations; semantic sensors;
environment generation tools; volumetric environmental effects; and realistic
waves.

</details>


### [186] [DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation](https://arxiv.org/abs/2510.06199)
*Chengyang Zhao,Uksang Yoo,Arkadeep Narayan Chaudhury,Giljoo Nam,Jonathan Francis,Jeffrey Ichnowski,Jean Oh*

Main category: cs.RO

TL;DR: DYMO-Hair是一个基于模型的机器人护发系统，通过新颖的动态学习范式和3D潜空间提高泛化能力，实验显示其在模拟和真实环境中均优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 由于头发的细粒度物理结构和复杂动力学，护发对行动不便者和自主机器人系统具有挑战性。

Method: 引入了一种新颖的动态学习范式，结合动作条件潜状态编辑机制和紧凑的3D潜空间，通过预训练的大规模头发物理模拟器提高泛化能力，并采用模型预测路径积分（MPPI）规划器进行视觉目标条件发型设计。

Result: 在模拟实验中，DYMO-Hair的动态模型在捕捉多样未见发型的局部变形上优于基线，闭环发型任务中几何误差平均降低22%，成功率提高42%。真实实验中展示了对假发的零样本迁移能力。

Conclusion: DYMO-Hair为基于模型的机器人护发系统奠定了基础，提高了在非约束物理环境中的通用性、灵活性和可访问性。

Abstract: Hair care is an essential daily activity, yet it remains inaccessible to
individuals with limited mobility and challenging for autonomous robot systems
due to the fine-grained physical structure and complex dynamics of hair. In
this work, we present DYMO-Hair, a model-based robot hair care system. We
introduce a novel dynamics learning paradigm that is suited for volumetric
quantities such as hair, relying on an action-conditioned latent state editing
mechanism, coupled with a compact 3D latent space of diverse hairstyles to
improve generalizability. This latent space is pre-trained at scale using a
novel hair physics simulator, enabling generalization across previously unseen
hairstyles. Using the dynamics model with a Model Predictive Path Integral
(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair
styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model
outperforms baselines on capturing local deformation for diverse, unseen
hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling
tasks on unseen hairstyles, with an average of 22% lower final geometric error
and 42% higher success rate than the state-of-the-art system. Real-world
experiments exhibit zero-shot transferability of our system to wigs, achieving
consistent success on challenging unseen hairstyles where the state-of-the-art
system fails. Together, these results introduce a foundation for model-based
robot hair care, advancing toward more generalizable, flexible, and accessible
robot hair styling in unconstrained physical environments. More details are
available on our project page: https://chengyzhao.github.io/DYMOHair-web/.

</details>


### [187] [EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model](https://arxiv.org/abs/2510.06207)
*Zefu Lin,Rongxu Cui,Chen Hanning,Xiangyu Wang,Junjia Xu,Xiaojuan Jin,Chen Wenbo,Hui Zhou,Lue Fan,Wenling Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: EmbodiedCoder 是一个无需训练的编码框架，通过将指令转化为代码直接生成机器人轨迹，实现了透明且通用的感知与操作连接，实验证明其在新环境中的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人控制方法在多样化环境中扩展能力有限，依赖大量标注数据集且解释性不足。

Method: EmbodiedCoder 是一个无需训练的框架，利用编码模型直接生成可执行的机器人轨迹，通过将高级指令编码为代码，实现灵活的物体几何参数化和操作轨迹合成。

Result: 实验表明，EmbodiedCoder 在真实移动机器人上实现了跨多样长期任务的稳健性能，并能有效推广到新物体和环境。

Conclusion: EmbodiedCoder 提供了一种可解释的方法，将高级推理与低级控制相结合，超越了固定原语，朝着多功能机器人智能迈进。

Abstract: Recent advances in control robot methods, from end-to-end
vision-language-action frameworks to modular systems with predefined
primitives, have advanced robots' ability to follow natural language
instructions. Nonetheless, many approaches still struggle to scale to diverse
environments, as they often rely on large annotated datasets and offer limited
interpretability.In this work, we introduce EmbodiedCoder, a training-free
framework for open-world mobile robot manipulation that leverages coding models
to directly generate executable robot trajectories. By grounding high-level
instructions in code, EmbodiedCoder enables flexible object geometry
parameterization and manipulation trajectory synthesis without additional data
collection or fine-tuning.This coding-based paradigm provides a transparent and
generalizable way to connect perception with manipulation. Experiments on real
mobile robots show that EmbodiedCoder achieves robust performance across
diverse long-term tasks and generalizes effectively to novel objects and
environments.Our results demonstrate an interpretable approach for bridging
high-level reasoning and low-level control, moving beyond fixed primitives
toward versatile robot intelligence. See the project page at:
https://anonymous.4open.science/w/Embodied-Coder/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [188] [Toward Systems Foundations for Agentic Exploration](https://arxiv.org/abs/2510.05556)
*Jiakai Xu,Tianle Zhou,Eugene Wu,Kostis Kaffes*

Main category: cs.DC

TL;DR: 论文指出现有系统支持不足，无法满足LLM代理的探索需求，并提出了三个根本挑战：分叉语义、外部副作用和原生分叉。


<details>
  <summary>Details</summary>
Motivation: 现有的系统支持无法满足LLM驱动的代理在多执行路径中分支、回溯和搜索的需求。

Method: 通过基准测试六种快照/恢复机制，评估了通用工具（如CRIU或容器提交）在孤立测试环境和实际部署中的表现。

Result: 研究发现，通用工具在孤立测试环境中速度不足，在实际部署中完全失效，尤其是在代理与其他代理及人类用户共享文件、套接字和云API的情况下。

Conclusion: 论文指出了在LLM驱动的代理探索中，现有系统支持的不足，并提出了三个根本性挑战：分叉语义、外部副作用和原生分叉。

Abstract: Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

</details>


### [189] [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
*Yilong Li,Shuai Zhang,Yijing Zeng,Hao Zhang,Xinmiao Xiong,Jingyu Liu,Pan Hu,Suman Banerjee*

Main category: cs.DC

TL;DR: NANOMIND是一个硬件-软件协同设计的框架，通过模块化分解和动态调度，显著提升多模态模型在资源受限设备上的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型通常以整体方式运行，未能充分利用异构加速器的潜力，导致高延迟和资源浪费。

Method: 提出了NANOMIND框架，包括模块化分解、动态卸载调度、低比特计算优化和硬件定制设计。

Result: NANOMIND在能效和资源利用率上显著优于现有方案，能耗降低42.3%，GPU内存使用减少11.2%，支持设备本地运行LLaVA-OneVision和LLaMA-3-8B长达数小时。

Conclusion: NANOMIND框架通过硬件-软件协同设计，将大型多模态模型分解为模块化组件，并在异构加速器上动态调度，显著提升了能效和资源利用率，实现了在资源受限设备上的高效运行。

Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

</details>


### [190] [Agora: Bridging the GPU Cloud Resource-Price Disconnect](https://arxiv.org/abs/2510.05111)
*Ian McDougall,Noah Scott,Joon Huh,Kirthevasan Kandasamy,Karthikeyan Sankaralingam*

Main category: cs.DC

TL;DR: 针对GPU带宽限制工作负载的经济效率问题，本文提出基于特征的定价框架Agora，实证显示其高效且可行。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的FLOPs能力持续经济扩展，但内存带宽未能同步增长，导致价格-性能脱节，现有时间定价模型对带宽限制工作负载不经济。

Method: 提出了一种新颖的基于特征的定价框架，并通过Agora系统实现，包括经济与算法定义及原型实现。

Result: Agora系统在50us采样下仅损失5%收入，10us采样下仅2.4%，验证了其可行性和有效性。

Conclusion: 本文提出了一种基于特征的定价框架Agora，有效解决了GPU带宽限制工作负载的经济效率问题，并通过实证验证了其透明性和市场效率。

Abstract: The historic trend of Moore's Law, which predicted exponential growth in
computational performance per dollar, has diverged for modern Graphics
Processing Units (GPUs). While Floating Point Operations per Second (FLOPs)
capabilities have continued to scale economically, memory bandwidth has not,
creating a significant price-performance disconnect. This paper argues that the
prevailing time-based pricing models for cloud GPUs are economically
inefficient for bandwidth-bound workloads. These models fail to account for the
rising marginal cost of memory bandwidth, leading to market distortions and
suboptimal hardware allocation. To address this, we propose a novel
feature-based pricing framework that directly links cost to resource
consumption, including but not limited to memory bandwidth. We provide a robust
economic and algorithmic definition of this framework and introduce Agora, a
practical and secure system architecture for its implementation. Our
implementation of Agora shows that a 50us sampling provides nearly perfect
pricing as what ideal sampling would provide - losing only 5\% of revenue. 10us
sampling is even better result in 2.4\% loss. Modern telemetry systems can
already provide this rate of measurement, and our prototype implementation
shows the system design for feature-based pricing is buildable. Our evaluation
across diverse GPU applications and hardware generations empirically validates
the effectiveness of our approach in creating a more transparent and efficient
market for cloud GPU resources.

</details>


### [191] [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)
*Lijuan Jiang,Xingjian Qian,Zhenxiang Ma,Zan Zong,Hengjie Li,Chao Yang,Jidong Zhai*

Main category: cs.DC

TL;DR: FlexPipe是一个可编程的管道并行框架，通过DSL和自动化调度器提升性能，相比现有框架最高加速2.28倍。


<details>
  <summary>Details</summary>
Motivation: 现有的管道并行化方法依赖于预定义的调度策略，无法自动适应新兴的模型架构，且手动实现调度策略存在编码负担和灵活性不足的问题。

Method: FlexPipe框架包含一个简洁的领域特定语言（DSL）和一个自动化调度器，支持广泛的调度类型和自动化调度探索。

Result: FlexPipe在评估中相比Megtron-LM实现了最高2.28倍的性能加速，相比最先进的自动化管道并行框架实现了最高1.49倍的性能加速。

Conclusion: FlexPipe框架通过其领域特定语言（DSL）和自动化调度器，显著提升了管道并行化的生产力和性能，实现了比现有框架更高的性能加速。

Abstract: Pipeline parallelism is an essential distributed parallelism method.
Increasingly complex and diverse DNN models necessitate meticulously customized
pipeline schedules for performance. However, existing practices typically rely
on predefined schedules, each with strengths, but fail to adapt automatically
to the emerging model architectures. Exploring novel high-efficiency schedules
is daunting due to the enormous and varying schedule space. Besides, manually
implementing schedules can be challenging due to the onerous coding burdens and
constantly changing needs. Unfortunately, existing frameworks have limitations
in automated schedule exploration and lack flexibility and controllability.
  This paper presents FlexPipe, a programmable pipeline parallelism framework
with enhanced productivity, programmability, debuggability, and ease of tuning.
FlexPipe has two main components: a succinct domain-specific language (DSL) and
an automated scheduler. FlexPipe enables automated schedule exploration for
various parallel scenarios within a broad spectrum of schedule types at a small
search cost. Besides, users can swiftly develop and customize schedules using
the FlexPipe DSL, which embodies flexible controllability in the pipeline order
of micro-batch computations over stages. It also provides convenient mechanisms
to include new operations in schedules to meet changing demands. Our evaluation
results demonstrate that FlexPipe achieves up to 2.28X performance speedup
compared to the popular large-scale parallel framework Megtron-LM, and gains up
to 1.49X performance speedup compared to the state-of-the-art automated
pipeline parallelism framework.

</details>


### [192] [Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum](https://arxiv.org/abs/2510.05118)
*Cynthia Marcelino,Noah Krennmair,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Lumos 工具显示 AoT 编译的 Wasm 在边缘云连续体中性能优于容器，但解释型 Wasm 表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究 WebAssembly 在异构和资源受限的边缘云连续体中作为轻量级运行时执行无服务器函数的性能优势与权衡。

Method: 通过 Lumos 性能模型和基准测试工具，对容器和 Wasm 运行时（解释模式及 AoT 编译模式）进行性能比较。

Result: AoT 编译的 Wasm 镜像比容器小 30 倍，冷启动延迟降低 16%；解释型 Wasm 的热延迟高 55 倍，I/O 序列化开销高 10 倍。

Conclusion: Lumos 性能模型和基准测试工具揭示了在边缘云连续体中，AoT 编译的 WebAssembly 在冷启动延迟和镜像大小方面优于容器，而解释型 Wasm 在热延迟和 I/O 序列化方面表现较差。

Abstract: WebAssembly has emerged as a lightweight and portable runtime to execute
serverless functions, particularly in heterogeneous and resource-constrained
environments such as the Edge Cloud Continuum. However, the performance
benefits versus trade-offs remain insufficiently understood. This paper
presents Lumos, a performance model and benchmarking tool for characterizing
serverless runtimes. Lumos identifies workload, system, and environment-level
performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art
containers and the Wasm runtime in interpreted mode and with ahead-of-time
compilation. Our performance characterization shows that AoT-compiled Wasm
images are up to 30x smaller and decrease cold-start latency by up to 16%
compared to containers, while interpreted Wasm suffers up to 55x higher warm
latency and up to 10x I/O-serialization overhead.

</details>


### [193] [Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines](https://arxiv.org/abs/2510.05127)
*Harshit Goyal*

Main category: cs.DC

TL;DR: 该论文提出了一种基于随机森林回归的AI方法，用于预测大数据管道的资源利用，结果显示高准确性并适用于云环境中的成本感知自动扩展。


<details>
  <summary>Details</summary>
Motivation: 现代云计算中高效资源分配是关键挑战，过度配置会导致不必要的成本，而配置不足则可能引发性能下降和SLA违规。

Method: 采用随机森林回归方法对Google Borg集群跟踪数据进行预处理，包括清理、转换和提取相关特征（CPU、内存、使用分布）。

Result: 模型实现了高预测准确性（R Square = 0.99, MAE = 0.0048, RMSE = 0.137），能够捕捉工作负载特征与资源利用之间的非线性关系。

Conclusion: 该研究展示了AI驱动预测在云环境中成本感知自动扩展的潜力，既能减少不必要的资源配置，又能保障服务质量。

Abstract: Efficient resource allocation is a key challenge in modern cloud computing.
Over-provisioning leads to unnecessary costs, while under-provisioning risks
performance degradation and SLA violations. This work presents an artificial
intelligence approach to predict resource utilization in big data pipelines
using Random Forest regression. We preprocess the Google Borg cluster traces to
clean, transform, and extract relevant features (CPU, memory, usage
distributions). The model achieves high predictive accuracy (R Square = 0.99,
MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between
workload characteristics and resource utilization. Error analysis reveals
impressive performance on small-to-medium jobs, with higher variance in rare
large-scale jobs. These results demonstrate the potential of AI-driven
prediction for cost-aware autoscaling in cloud environments, reducing
unnecessary provisioning while safeguarding service quality.

</details>


### [194] [FlashResearch: Real-time Agent Orchestration for Efficient Deep Research](https://arxiv.org/abs/2510.05145)
*Lunyiu Nie,Nedim Lipka,Ryan A. Rossi,Swarat Chaudhuri*

Main category: cs.DC

TL;DR: FlashResearch通过并行化和动态资源分配显著提升深度研究代理的效率，实验显示最高可达到5倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决深度研究代理因顺序推理过程导致的高延迟、运行时适应性差和资源分配效率低的问题。

Method: 引入了FlashResearch框架，包括自适应规划器、实时协调层和多维并行化框架，将顺序处理转化为并行运行时协调。

Result: 实验表明，FlashResearch在固定时间预算内能持续提高最终报告质量，并最高可达到5倍加速。

Conclusion: FlashResearch通过并行化处理显著提升了深度研究代理的效率，实现了在固定时间预算内提高报告质量，并最高可达到5倍加速。

Abstract: Deep research agents, which synthesize information across diverse sources,
are significantly constrained by their sequential reasoning processes. This
architectural bottleneck results in high latency, poor runtime adaptability,
and inefficient resource allocation, making them impractical for interactive
applications. To overcome this, we introduce FlashResearch, a novel framework
for efficient deep research that transforms sequential processing into
parallel, runtime orchestration by dynamically decomposing complex queries into
tree-structured sub-tasks. Our core contributions are threefold: (1) an
adaptive planner that dynamically allocates computational resources by
determining research breadth and depth based on query complexity; (2) a
real-time orchestration layer that monitors research progress and prunes
redundant paths to reallocate resources and optimize efficiency; and (3) a
multi-dimensional parallelization framework that enables concurrency across
both research breadth and depth. Experiments show that FlashResearch
consistently improves final report quality within fixed time budgets, and can
deliver up to a 5x speedup while maintaining comparable quality.

</details>


### [195] [cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications](https://arxiv.org/abs/2510.05476)
*Xi Wang,Bin Ma,Jongryool Kim,Byungil Koh,Hoshik Kim,Dong Li*

Main category: cs.DC

TL;DR: cMPI利用CXL内存共享优化MPI通信，显著降低延迟并提升带宽，特别是在小消息传输中表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统的MPI库依赖于复杂的网络互连和协议栈进行跨节点通信，存在性能瓶颈。cMPI旨在通过CXL内存共享技术，优化MPI通信性能。

Method: cMPI将跨节点通信转化为CXL内存内的内存事务和数据拷贝，绕过了传统网络协议，并解决了CXL内存共享中的数据对象管理、缓存一致性和原子操作等挑战。

Result: cMPI在小型和中型集群中，比基于TCP的互连实现了7.2x-8.1x的延迟降低，并在小消息传输中，相较于标准以太网NIC和高性能SmartNIC，分别实现了49x和72x的延迟和带宽提升。

Conclusion: cMPI通过利用CXL内存共享技术，显著优化了MPI点对点通信的性能，特别是在小消息传输中，相较于传统网络协议，实现了显著的延迟和带宽提升。

Abstract: Message Passing Interface (MPI) is a foundational programming model for
high-performance computing. MPI libraries traditionally employ network
interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP
and RoCE) with complex software stacks for cross-node communication. We present
cMPI, the first work to optimize MPI point-to-point communication (both
one-sided and two-sided) using CXL memory sharing on a real CXL platform,
transforming cross-node communication into memory transactions and data copies
within CXL memory, bypassing traditional network protocols. We analyze
performance across various interconnects and find that CXL memory sharing
achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in
small- and medium-scale clusters. We address challenges of CXL memory sharing
for MPI communication, including data object management over the dax
representation [50], cache coherence, and atomic operations. Overall, cMPI
outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x
and 72x in latency and bandwidth, respectively, for small messages.

</details>


### [196] [Percepta: High Performance Stream Processing at the Edge](https://arxiv.org/abs/2510.05149)
*Clarisse Sousa,Tiago Fonseca,Luis Lino Ferreira,Ricardo Venâncio,Ricardo Severino*

Main category: cs.DC

TL;DR: Percepta 是一个边缘计算专用的轻量级数据流处理系统，支持 AI 特别是强化学习，解决了数据标准化、异构协议和缺失数据等问题。


<details>
  <summary>Details</summary>
Motivation: 实时数据和物联网设备的普及暴露了云中心解决方案在延迟、带宽和隐私方面的局限性，推动了边缘计算的发展。物联网还带来数据速率协调、协议转换、数据丢失和 AI 模型集成等挑战。

Method: Percepta 引入了奖励函数计算、模型重训练数据存储和实时数据准备等专门功能，支持连续决策。系统还包括数据标准化、异构协议和采样率协调以及鲁棒的数据缺失处理。

Result: Percepta 能够有效支持边缘 AI 部署，特别是在强化学习场景中，通过实时数据准备和专门功能提升了决策连续性和数据处理的鲁棒性。

Conclusion: Percepta 是一个专为边缘 AI 工作负载设计的轻量级数据流处理系统，特别适合强化学习等应用，能够有效解决边缘计算中的数据标准化、协议异构性和数据缺失等问题。

Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT)
devices have highlighted the limitations of cloud-centric solutions,
particularly regarding latency, bandwidth, and privacy. These challenges have
driven the growth of Edge Computing. Associated with IoT appears a set of other
problems, like: data rate harmonization between multiple sources, protocol
conversion, handling the loss of data and the integration with Artificial
Intelligence (AI) models. This paper presents Percepta, a lightweight Data
Stream Processing (DSP) system tailored to support AI workloads at the edge,
with a particular focus on such as Reinforcement Learning (RL). It introduces
specialized features such as reward function computation, data storage for
model retraining, and real-time data preparation to support continuous
decision-making. Additional functionalities include data normalization,
harmonization across heterogeneous protocols and sampling rates, and robust
handling of missing or incomplete data, making it well suited for the
challenges of edge-based AI deployment.

</details>


### [197] [SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading](https://arxiv.org/abs/2510.05164)
*Yuanzhe Shen,Yide Liu,Zisu Huang,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.DC

TL;DR: SATER是一种双模式兼容方法，通过优化和拒绝机制显著减少成本和延迟，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然性能卓越，但依赖昂贵的商业API或云服务；小型语言模型（SLMs）成本低但能力有限。现有路由策略（预生成路由和级联路由）各有局限，需要一种更优的方法来平衡性能与成本。

Method: SATER采用最短响应偏好优化和置信度感知拒绝机制，对模型进行微调。

Result: 在三个SLM和六个数据集上的实验表明，SATER在保持性能的同时，计算成本降低50%以上，级联延迟减少80%以上。

Conclusion: SATER作为一种双模式兼容方法，通过最短响应偏好优化和置信度感知拒绝机制，显著减少了冗余输出和响应时间，同时提升了预生成路由的性能和级联路由的效率。实验证明，SATER在保持可比性能的同时，持续降低计算成本50%以上，级联延迟80%以上。

Abstract: Large language models (LLMs) demonstrate remarkable performance across
diverse tasks, yet their effectiveness frequently depends on costly commercial
APIs or cloud services. Model selection thus entails a critical trade-off
between performance and cost: high-performing LLMs typically incur substantial
expenses, whereas budget-friendly small language models (SLMs) are constrained
by limited capabilities. Current research primarily proposes two routing
strategies: pre-generation routing and cascade routing. Both approaches have
distinct characteristics, with cascade routing typically offering superior
cost-effectiveness and accuracy despite its higher latency. To further address
the limitations of both approaches, we introduce SATER, a dual-mode compatible
approach that fine-tunes models through shortest-response preference
optimization and a confidence-aware rejection mechanism. SATER significantly
reduces redundant outputs and response times, while improving both the
performance of pre-generation routing and the efficiency of cascade routing.
Experiments across three SLMs and six datasets, varying in type and complexity,
demonstrate that SATER achieves comparable performance while consistently
reducing computational costs by over 50\% and cascade latency by over 80\%.

</details>


### [198] [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://arxiv.org/abs/2510.05186)
*Hongpei Li,Han Zhang,Huikang Liu,Dongdong Ge,Yinyu Ye*

Main category: cs.DC

TL;DR: 本文提出了一种优化视角的细粒度流水线调度方法，动态权衡内存与时间，实验证明可减少50%空闲时间并提升内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的流水线并行技术虽在减少内存消耗方面取得进展，但方法多为启发式且粗粒度，未能充分权衡内存、计算和调度延迟之间的细粒度关系。

Method: 本文将流水线调度问题建模为一个约束优化问题，综合考虑内存容量、激活重用和流水线气泡最小化。通过动态优化内存与时间的权衡，生成细粒度的调度方案。

Result: 实验结果显示，该方法能显著减少流水线气泡，提升吞吐量和内存利用率。在相同设备内存限制下，空闲流水线时间减少达50%，并支持更大模型的训练。

Conclusion: 本文提出了一种基于优化视角的细粒度流水线调度方法，能够在严格的内存预算下减少流水线气泡，提升吞吐量和内存利用率。实验结果表明，该方法在相同设备内存限制下可将空闲流水线时间减少达50%，并在某些情况下支持在有限内存预算下训练更大模型。

Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large
language model (LLM) training across multiple devices. However, despite recent
progress in reducing memory consumption through activation offloading, existing
approaches remain largely heuristic and coarse-grained, often overlooking the
fine-grained trade-offs between memory, computation, and scheduling latency. In
this work, we revisit the pipeline scheduling problem from a principled
optimization perspective. We observe that prevailing strategies either rely on
static rules or aggressively offload activations without fully leveraging the
interaction between memory constraints and scheduling efficiency. To address
this, we formulate scheduling as a constrained optimization problem that
jointly accounts for memory capacity, activation reuse, and pipeline bubble
minimization. Solving this model yields fine-grained schedules that reduce
pipeline bubbles while adhering to strict memory budgets. Our approach
complements existing offloading techniques: whereas prior approaches trade
memory for time in a fixed pattern, we dynamically optimize the tradeoff with
respect to model structure and hardware configuration. Experimental results
demonstrate that our method consistently improves both throughput and memory
utilization. In particular, we reduce idle pipeline time by up to 50% under the
same per-device memory limit, and in some cases, enable the training of larger
models within limited memory budgets.

</details>


### [199] [Performance of a high-order MPI-Kokkos accelerated fluid solver](https://arxiv.org/abs/2510.05254)
*Filipp Sporykhin,Holger Homann*

Main category: cs.DC

TL;DR: 高阶数值方案在GPU上表现更优，但现代GPU的能源效率提升可能被更大的计算需求抵消。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估现代高性能计算架构（如多核CPU和GPU）上高阶数值方案的性能及其环境影响。

Method: 本文采用空间节点不连续Galerkin方案，结合高达六阶的Runge-Kutta方法，通过Kokkos库和MPI接口在现代高性能计算架构上实现。

Result: 研究发现，八阶模拟在计算时间和精度上显著优于三阶或四阶模拟。GPU在大型模拟中表现优异，而CPU在小型网格上更快。能源效率方面，GPU在大型模拟中更节能，但在小型网格上消耗更多能源。

Conclusion: 本文得出结论，高阶数值方案在计算效率和精度上优于低阶方案，特别是在大规模网格模拟中。GPU在大型模拟中表现优于CPU，但在小型网格上CPU更为高效。然而，现代GPU的能源效率提升可能被更大的计算需求所抵消。

Abstract: This work discusses the performance of a modern numerical scheme for fluid
dynamical problems on modern high-performance computing architectures. Our code
implements a spatial nodal discontinuous Galerkin scheme that we test up to an
order of convergence of eight. It is temporally coupled to a set of Runge-Kutta
methods of orders up to six. The code integrates the linear advection equations
as well as the isothermal Euler equations in one, two, and three dimensions. In
order to target modern hardware involving many-core Central Processing Units
and accelerators such as Graphic Processing Units we use the Kokkos library in
conjunction with the Message Passing Interface to run our single source code on
various GPU systems. We find that the higher the order the faster is the code.
Eighth-order simulations attain a given global error with much less computing
time than third- or fourth-order simulations. The RK scheme has a smaller
impact on the code performance and a classical fourth-order scheme seems to
generally be a good choice. The code performs very well on all considered GPUs.
The many-CPU performance is also very good and perfect weak scaling is observed
up to many hundreds of CPU cores using MPI. We note that small grid-size
simulations are faster on CPUs than on GPUs while GPUs win significantly over
CPUs for simulations involving more than $10^7$ degrees of freedom ($\approx
3100^2$ grid points). When it comes to the environmental impact of numerical
simulations we estimate that GPUs consume less energy than CPUs for large
grid-size simulations but more energy on small grids. We observe a tendency
that the more modern is the GPU the larger needs to be the grid in order to use
it efficiently. This yields a rebound effect because larger simulations need
longer computing times and in turn more energy that is not compensated by the
energy efficiency gain of the newer GPUs.

</details>


### [200] [Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting](https://arxiv.org/abs/2510.05497)
*Zhongkai Yu,Yue Guan,Zihao Yu,Chenyang Zhou,Shuyi Pei,Yangwook Kang,Yufei Ding,Po-An Tsai*

Main category: cs.DC

TL;DR: 论文分析了MoE模型的数据移动瓶颈，提出六项优化见解，并在案例中实现6.3X和4.0X加速。


<details>
  <summary>Details</summary>
Motivation: MoE架构的LLMs因其随机专家选择机制导致数据移动成为多单元服务系统的主要瓶颈。论文旨在预测并优化这种数据移动模式。

Method: 论文对三种最先进的大规模MoE模型（200B-671B）进行了数据移动为中心的分析，覆盖24,000多个请求的多样化工作负载，生成150GB+的跟踪文件，并从时间和空间角度进行了系统分析。

Result: 通过分析提炼出六项关键见解，在晶圆级GPU案例中，通过微小的架构修改实现了显著性能提升（6.3X和4.0X平均加速）。

Conclusion: 该论文通过大规模MoE模型的数据移动分析，提出六项关键见解，并通过案例研究展示了显著的性能提升（DeepSeek V3和Qwen3分别达到6.3倍和4.0倍加速）。研究首次提供了全面的数据为中心的分析，并公开了分析结果和仿真框架。

Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

</details>


### [201] [Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems](https://arxiv.org/abs/2510.05621)
*Zhiyuan Ren,Tao Zhang,Wenchi Chen*

Main category: cs.DC

TL;DR: 本文提出了确定性因果结构（DCS），将正确性与策略解耦，证明了其作为异步计算边界原则的重要性，并确立了‘Correctness-as-a-Chassis’范式。


<details>
  <summary>Details</summary>
Motivation: 在分布式多智能体系统中，正确性常与操作策略（如调度、批处理或路由）纠缠，导致系统脆弱，因为性能驱动的策略演变可能破坏完整性保证。

Method: 开发了一个最小的公理理论，并证明了四个结果：存在性和唯一性、政策无关的不变性、观察等价性和公理最小性。

Result: DCS解决了因果模糊性，这是CRDT等值中心收敛模型无法解决的问题，并且移除任何公理都会将确定性转化为模糊性。

Conclusion: DCS作为一种边界原则，类似于CAP和FLP，为异步计算提供了政策无关的正确性保障，确立了‘Correctness-as-a-Chassis’范式。

Abstract: In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

</details>


### [202] [Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium](https://arxiv.org/abs/2510.05711)
*Ailiya Borjigin,Cong He*

Main category: cs.DC

TL;DR: 本文提出时间绑定稳定币和流动性溢价（TLP）概念，通过金融工程和实证分析量化TLP，并提出动态LTV调整机制来控制风险。


<details>
  <summary>Details</summary>
Motivation: 研究时间绑定稳定币如何通过暂时代币化传统证券在市场关闭期间提供连续跨市场流动性，并量化时间流动性溢价（TLP）。

Method: 结合金融工程（无套利条件、期权式定价）与实证金融（对交叉上市股票和期货的事件研究）来分析时间流动性溢价（TLP），并提出了一个动态风险控制机制来实时调整贷款价值比（LTV）。

Result: 结果显示TLP随市场关闭时间和波动性增加，但可以通过自适应LTV控制。

Conclusion: 时间绑定稳定币是一种能够减少时间性市场低效的工具，为未来的研究和部署提供了参考。

Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional
securities during market off-hours, enabling continuous cross-market liquidity.
We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of
providing liquidity when the primary market is closed. We build a no-arbitrage
pricing model that yields a band for fair values over different expiries, and a
dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real
time to keep TLP within a target range. Our analysis blends financial
engineering (no-arbitrage conditions, option-style pricing) with empirical
finance (event studies on cross-listed stocks and futures) to measure TLP under
time-zone frictions. We define TLP formally, derive closed-form expressions for
its term structure under idealized assumptions, and simulate scenarios that
vary volatility and collateralization. We then propose an LTV policy that
raises or lowers collateral to expand or curtail time-bound stablecoin supply,
analogous to a central bank adjusting rates to defend a peg. We outline
empirical proxies for TLP, including ADR premiums, overseas index futures
versus cash index divergence, and pre-market versus official close gaps.
Results show that TLP grows with closure length and volatility, yet can be
contained by adaptive LTV. We provide backtests and figures (term-structure
curves, capital-efficiency versus tail-risk trade-offs, time-liquidity
heatmaps) and discuss protocol design (vault structure, closing-price oracles,
on-chain auction liquidations). The findings position time-bound stablecoins as
a tool to reduce temporal market inefficiencies and inform future research and
deployment.

</details>


### [203] [A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications](https://arxiv.org/abs/2510.05738)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh,Sadhana Tiwari*

Main category: cs.DC

TL;DR: 本文综述了本体驱动语义数据管理在医疗大数据中的应用，分类分析了六类技术，并探讨了与大数据框架的整合潜力及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 异构医疗数据的快速增长导致数据湖和集中式架构的广泛采用，但缺乏有效治理可能使其退化为无序数据沼泽。本体驱动的语义数据管理提供了增强语义互操作性、提高数据可发现性和支持领域感知访问的解决方案。

Method: 采用系统性研究策略，制定关键研究问题，并在主要学术数据库中进行结构化文献检索，将选定研究分析并分类为六类本体驱动的医疗分析。

Result: 研究分析了六类本体驱动医疗分析技术，并探讨了本体技术与大数据框架（如Hadoop、Spark等）的整合潜力，以提供可扩展和智能的医疗分析。

Conclusion: 本文强调了本体驱动语义数据管理在构建可持续、高性能医疗数据生态系统中的关键作用，并指出了未来结合AI、机器学习、IoT等技术的趋势。

Abstract: Exponential growth in heterogeneous healthcare data arising from electronic
health records (EHRs), medical imaging, wearable sensors, and biomedical
research has accelerated the adoption of data lakes and centralized
architectures capable of handling the Volume, Variety, and Velocity of Big Data
for advanced analytics. However, without effective governance, these
repositories risk devolving into disorganized data swamps. Ontology-driven
semantic data management offers a robust solution by linking metadata to
healthcare knowledge graphs, thereby enhancing semantic interoperability,
improving data discoverability, and enabling expressive, domain-aware access.
This review adopts a systematic research strategy, formulating key research
questions and conducting a structured literature search across major academic
databases, with selected studies analyzed and classified into six categories of
ontology-driven healthcare analytics: (i) ontology-driven integration
frameworks, (ii) semantic modeling for metadata enrichment, (iii)
ontology-based data access (OBDA), (iv) basic semantic data management, (v)
ontology-based reasoning for decision support, and (vi) semantic annotation for
unstructured data. We further examine the integration of ontology technologies
with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting
their combined potential to deliver scalable and intelligent healthcare
analytics. For each category, recent techniques, representative case studies,
technical and organizational challenges, and emerging trends such as artificial
intelligence, machine learning, the Internet of Things (IoT), and real-time
analytics are reviewed to guide the development of sustainable, interoperable,
and high-performance healthcare data ecosystems.

</details>


### [204] [EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models](https://arxiv.org/abs/2510.05943)
*Zheyue Tan,Mustapha Abdullahi,Tuo Shi,Huining Yuan,Zelai Xu,Chao Yu,Boxun Li,Bo Zhao*

Main category: cs.DC

TL;DR: EARL通过动态并行化和分散式数据交换，解决了代理型RL训练中的内存和延迟瓶颈，提升了大规模训练的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决代理型强化学习在训练过程中因上下文长度增长导致的内存使用膨胀、延迟增加和跨设备数据移动瓶颈。

Method: 设计了并行选择器动态调整模型和训练并行度，以及数据分发器进行布局感知的分散式数据交换。

Result: EARL系统提高了吞吐量，减少了长上下文失败，并实现了无需硬性限制上下文长度的稳定大规模训练。

Conclusion: EARL系统通过动态并行选择器和数据分发器，显著提升了代理型强化学习系统的可扩展性和效率，解决了长上下文训练中的内存和延迟问题。

Abstract: Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

</details>
