{"id": "2510.16284", "categories": ["cs.DC", "cs.MS", "cs.NA", "math.NA", "stat.CO", "65Y05, 65C60, 62F40", "F.2.2; G.3; D.1.3"], "pdf": "https://arxiv.org/pdf/2510.16284", "abs": "https://arxiv.org/abs/2510.16284", "authors": ["Di Zhang"], "title": "Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI", "comment": "6 pages", "summary": "Bootstrapping is a powerful statistical resampling technique for estimating\nthe sampling distribution of an estimator. However, its computational cost\nbecomes prohibitive for large datasets or a high number of resamples. This\npaper presents a theoretical analysis and design of parallel bootstrapping\nalgorithms using the Message Passing Interface (MPI). We address two key\nchallenges: high communication overhead and memory constraints in distributed\nenvironments. We propose two novel strategies: 1) Local Statistic Aggregation,\nwhich drastically reduces communication by transmitting sufficient statistics\ninstead of full resampled datasets, and 2) Synchronized Pseudo-Random Number\nGeneration, which enables distributed resampling when the entire dataset cannot\nbe stored on a single process. We develop analytical models for communication\nand computation complexity, comparing our methods against naive baseline\napproaches. Our analysis demonstrates that the proposed methods offer\nsignificant reductions in communication volume and memory usage, facilitating\nscalable parallel bootstrapping on large-scale systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5e76\u884c\u5f15\u5bfc\u7b97\u6cd5\u7b56\u7565\uff0c\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f15\u5bfc\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u96c6\u6216\u9ad8\u91cd\u91c7\u6837\u6b21\u6570\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u89e3\u51b3\u901a\u4fe1\u5f00\u9500\u548c\u5185\u5b58\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6d88\u606f\u4f20\u9012\u63a5\u53e3\uff08MPI\uff09\u8bbe\u8ba1\u5e76\u884c\u5f15\u5bfc\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1) \u672c\u5730\u7edf\u8ba1\u805a\u5408\uff0c\u901a\u8fc7\u4f20\u8f93\u5145\u5206\u7edf\u8ba1\u91cf\u800c\u975e\u5b8c\u6574\u91cd\u91c7\u6837\u6570\u636e\u96c6\u51cf\u5c11\u901a\u4fe1\uff1b2) \u540c\u6b65\u4f2a\u968f\u673a\u6570\u751f\u6210\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u91cd\u91c7\u6837\u65f6\u6570\u636e\u96c6\u65e0\u6cd5\u5355\u8fdb\u7a0b\u5b58\u50a8\u7684\u95ee\u9898\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u91cf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u4e0a\u7684\u53ef\u6269\u5c55\u5e76\u884c\u5f15\u5bfc\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5e76\u884c\u5f15\u5bfc\u7b97\u6cd5\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u91cf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u5e76\u884c\u5f15\u5bfc\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16144", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16144", "abs": "https://arxiv.org/abs/2510.16144", "authors": ["Sukhdeep Singh", "Avinash Bhat", "Shweta M", "Subhash K Singh", "Moonki Hong", "Madhan Raj K", "Kandeepan Sithamparanathan", "Sunder A. Khowaja", "Kapal Dev"], "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance", "comment": null, "summary": "The increasing complexity of Beyond 5G and 6G networks necessitates new\nparadigms for autonomy and assur- ance. Traditional O-RAN control loops rely\nheavily on RIC- based orchestration, which centralizes intelligence and exposes\nthe system to risks such as policy conflicts, data drift, and unsafe actions\nunder unforeseen conditions. In this work, we argue that the future of\nautonomous networks lies in a multi-agentic architecture, where specialized\nagents collaborate to perform data collection, model training, prediction,\npolicy generation, verification, deployment, and assurance. By replacing\ntightly- coupled centralized RIC-based workflows with distributed agents, the\nframework achieves autonomy, resilience, explainability, and system-wide\nsafety. To substantiate this vision, we design and evaluate a traffic steering\nuse case under surge and drift conditions. Results across four KPIs: RRC\nconnected users, IP throughput, PRB utilization, and SINR, demonstrate that a\nnaive predictor-driven deployment improves local KPIs but destabilizes\nneighbors, whereas the agentic system blocks unsafe policies, preserving global\nnetwork health. This study highlights multi- agent architectures as a credible\nfoundation for trustworthy AI- driven autonomy in next-generation RANs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4f5c\u4e3a\u4e0b\u4e00\u4ee3RAN\u81ea\u4e3b\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4ee3\u7406\u5b9e\u73b0\u81ea\u6cbb\u3001\u5f39\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6d41\u91cf\u8f6c\u5411\u7528\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfO-RAN\u63a7\u5236\u5faa\u73af\u4f9d\u8d56\u57fa\u4e8eRIC\u7684\u7f16\u6392\uff0c\u96c6\u4e2d\u5316\u667a\u80fd\u5e76\u66b4\u9732\u7cfb\u7edf\u4e8e\u653f\u7b56\u51b2\u7a81\u3001\u6570\u636e\u6f02\u79fb\u548c\u4e0d\u53ef\u9884\u89c1\u6761\u4ef6\u4e0b\u7684\u4e0d\u5b89\u5168\u884c\u52a8\u7b49\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u5728\u6fc0\u589e\u548c\u6f02\u79fb\u6761\u4ef6\u4e0b\u7684\u6d41\u91cf\u8f6c\u5411\u7528\u4f8b\uff0c\u901a\u8fc7\u56db\u4e2aKPI\uff08RRC\u8fde\u63a5\u7528\u6237\u3001IP\u541e\u5410\u91cf\u3001PRB\u5229\u7528\u7387\u548cSINR\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u963b\u6b62\u4e0d\u5b89\u5168\u653f\u7b56\uff0c\u4fdd\u6301\u5168\u5c40\u7f51\u7edc\u5065\u5eb7\uff0c\u800c\u7b80\u5355\u7684\u9884\u6d4b\u9a71\u52a8\u90e8\u7f72\u867d\u7136\u6539\u5584\u4e86\u5c40\u90e8KPI\uff0c\u4f46\u4f1a\u7834\u574f\u90bb\u5c45\u7a33\u5b9a\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3RAN\u4e2d\u53ef\u4fe1\u8d56\u7684AI\u9a71\u52a8\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
{"id": "2510.16415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16415", "abs": "https://arxiv.org/abs/2510.16415", "authors": ["Rizhen Hu", "Yutong He", "Ran Yan", "Mou Sun", "Binghang Yuan", "Kun Yuan"], "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization", "comment": "NeurIPS 2025 poster", "summary": "As distributed optimization scales to meet the demands of Large Language\nModel (LLM) training, hardware failures become increasingly non-negligible.\nExisting fault-tolerant training methods often introduce significant\ncomputational or memory overhead, demanding additional resources. To address\nthis challenge, we propose Memory- and Computation-efficient Fault-tolerant\nOptimization (MeCeFO), a novel algorithm that ensures robust training with\nminimal overhead. When a computing node fails, MeCeFO seamlessly transfers its\ntraining task to a neighboring node while employing memory- and\ncomputation-efficient algorithmic optimizations to minimize the extra workload\nimposed on the neighboring node handling both tasks. MeCeFO leverages three key\nalgorithmic designs: (i) Skip-connection, which drops the multi-head attention\n(MHA) module during backpropagation for memory- and computation-efficient\napproximation; (ii) Recomputation, which reduces activation memory in\nfeedforward networks (FFNs); and (iii) Low-rank gradient approximation,\nenabling efficient estimation of FFN weight matrix gradients. Theoretically,\nMeCeFO matches the convergence rate of conventional distributed training, with\na rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and\nT is the number of iterations. Empirically, MeCeFO maintains robust performance\nunder high failure rates, incurring only a 4.18% drop in throughput,\ndemonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA\napproaches. Codes are available at https://github.com/pkumelon/MeCeFO.", "AI": {"tldr": "MeCeFO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bb9\u9519\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7Skip-connection\u3001Recomputation\u548cLow-rank\u68af\u5ea6\u8fd1\u4f3c\u6280\u672f\uff0c\u5728\u9ad8\u6545\u969c\u7387\u4e0b\u4fdd\u6301\u8bad\u7ec3\u7a33\u5065\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u4f18\u5316\u6269\u5c55\u5230\u6ee1\u8db3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u786c\u4ef6\u6545\u969c\u53d8\u5f97\u8d8a\u6765\u8d8a\u4e0d\u53ef\u5ffd\u89c6\u3002\u73b0\u6709\u7684\u5bb9\u9519\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u663e\u8457\u7684\u8ba1\u7b97\u6216\u5185\u5b58\u5f00\u9500\uff0c\u9700\u8981\u989d\u5916\u8d44\u6e90\u3002MeCeFO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "MeCeFO \u901a\u8fc7\u4e09\u79cd\u5173\u952e\u7b97\u6cd5\u8bbe\u8ba1\u5b9e\u73b0\u5bb9\u9519\u4f18\u5316\uff1a(i) Skip-connection\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u4e22\u5f03\u591a\u5934\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u8282\u7701\u5185\u5b58\u548c\u8ba1\u7b97\uff1b(ii) Recomputation\uff0c\u51cf\u5c11\u524d\u9988\u7f51\u7edc\u4e2d\u7684\u6fc0\u6d3b\u5185\u5b58\uff1b(iii) Low-rank\u68af\u5ea6\u8fd1\u4f3c\uff0c\u9ad8\u6548\u4f30\u8ba1\u524d\u9988\u7f51\u7edc\u6743\u91cd\u77e9\u9635\u7684\u68af\u5ea6\u3002", "result": "\u7406\u8bba\u4e0a\uff0cMeCeFO\u4e0e\u4f20\u7edf\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u6536\u655b\u901f\u5ea6\u4e00\u81f4\uff0c\u4e3aO(1/\u221anT)\u3002\u5b9e\u8bc1\u663e\u793a\uff0cMeCeFO\u5728\u9ad8\u6545\u969c\u7387\u4e0b\u4fdd\u6301\u7a33\u5065\u6027\u80fd\uff0c\u541e\u5410\u91cf\u4ec5\u4e0b\u964d4.18%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5f3a5.0\u500d\u81f36.7\u500d\u3002", "conclusion": "MeCeFO \u662f\u4e00\u79cd\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5bb9\u9519\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u9ad8\u6545\u969c\u7387\u4e0b\u4fdd\u6301\u7a33\u5065\u6027\u80fd\uff0c\u4ec5\u5bfc\u81f44.18%\u7684\u541e\u5410\u91cf\u4e0b\u964d\uff0c\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd55.0\u500d\u81f36.7\u500d\u7684\u66f4\u5f3a\u97e7\u6027\u3002"}}
{"id": "2510.17009", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17009", "abs": "https://arxiv.org/abs/2510.17009", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Indrakshi Dey"], "title": "Traffic Prioritization Mechanisms for Mission and Time Critical Applications in Industrial Internet of Things", "comment": null, "summary": "Industrial Internet of Things (IIoT) promises to revolutionize industrial\noperations and productions through utilizing Machine-to-Machine (M2M)\ncommunications. Since each node in such environments generates various types of\ndata with diverse service requirements, MAC protocol holds crucial importance\nto ensure efficient delivery. In this context, simple to complex MAC schemes\nare found in literature. This paper focuses on evaluating the performance of\ntwo major techniques \"slot stealing\" and \"packet fragmentation\" for the IIoT;\nrepresentative protocols SS-MAC and FROG-MAC have been chosen from each\ncategory respectively. We conducted realistic simulations for the two protocols\nusing Contiki. Delay and packet loss comparison for SS-MAC and FROG-MAC\nindicates the superiority of FROG-MAC due to reduction in the waiting time for\nurgent traffic. Thus, a simple fragmentation scheme could be deployed for\nefficient scheduling of heterogenous traffic in the industrial environments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86IIoT\u4e2dSS-MAC\u548cFROG-MAC\u4e24\u79cdMAC\u534f\u8bae\u7684\u6027\u80fd\uff0c\u53d1\u73b0FROG-MAC\u56e0\u51cf\u5c11\u7d27\u6025\u6d41\u91cf\u7b49\u5f85\u65f6\u95f4\u800c\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5f02\u6784\u6d41\u91cf\u8c03\u5ea6\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u901a\u8fc7\u673a\u5668\u5bf9\u673a\u5668(M2M)\u901a\u4fe1\u6539\u53d8\u5de5\u4e1a\u64cd\u4f5c\u548c\u751f\u4ea7\uff0c\u4f46\u4e0d\u540c\u8282\u70b9\u751f\u6210\u7684\u6570\u636e\u7c7b\u578b\u548c\u670d\u52a1\u9700\u6c42\u591a\u6837\uff0cMAC\u534f\u8bae\u5bf9\u786e\u4fdd\u9ad8\u6548\u4f20\u8f93\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7Contiki\u5bf9SS-MAC\u548cFROG-MAC\u4e24\u79cd\u534f\u8bae\u8fdb\u884c\u5b9e\u9645\u6a21\u62df\uff0c\u6bd4\u8f83\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u60c5\u51b5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793aFROG-MAC\u5728\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u65b9\u9762\u4f18\u4e8eSS-MAC\u3002", "conclusion": "FROG-MAC\u5728\u5de5\u4e1a\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u5176\u51cf\u5c11\u4e86\u7d27\u6025\u6d41\u91cf\u7684\u7b49\u5f85\u65f6\u95f4\uff0c\u7b80\u5355\u7684\u5206\u7247\u65b9\u6848\u9002\u7528\u4e8e\u5f02\u6784\u6d41\u91cf\u7684\u9ad8\u6548\u8c03\u5ea6\u3002"}}
{"id": "2510.16418", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16418", "abs": "https://arxiv.org/abs/2510.16418", "authors": ["Jian Ma", "Xinchen Lyu", "Jun Jiang", "Longhao Zou", "Chenshan Ren", "Qimei Cui", "Xiaofeng Tao"], "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference", "comment": null, "summary": "Collaborative large language model (LLM) inference enables real-time,\nprivacy-preserving AI services on resource-constrained edge devices by\npartitioning computational workloads between client devices and edge servers.\nHowever, this paradigm is severely hindered by communication bottlenecks caused\nby the transmission of high-dimensional intermediate activations, exacerbated\nby the autoregressive decoding structure of LLMs, where bandwidth consumption\nscales linearly with output length. Existing activation compression methods\nstruggle to simultaneously achieve high compression ratios, low reconstruction\nerror, and computational efficiency. This paper proposes FourierCompress, a\nnovel, layer-aware activation compression framework that exploits the\nfrequency-domain sparsity of LLM activations. We rigorously demonstrate that\nactivations from the first Transformer layer exhibit strong smoothness and\nenergy concentration in the low-frequency domain, making them highly amenable\nto near-lossless compression via the Fast Fourier Transform (FFT).\nFourierCompress transforms activations into the frequency domain, retains only\na compact block of low-frequency coefficients, and reconstructs the signal at\nthe server using conjugate symmetry, enabling seamless hardware acceleration on\nDSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10\ncommonsense reasoning datasets demonstrate that FourierCompress preserves\nperformance remarkably close to the uncompressed baseline, outperforming Top-k,\nQR, and SVD. FourierCompress bridges the gap between communication efficiency\n(an average 7.6x reduction in activation size), near-lossless inference (less\nthan 0.3% average accuracy loss), and significantly faster compression\n(achieving over 32x reduction in compression time compared to Top-k via\nhardware acceleration) for edge-device LLM inference.", "AI": {"tldr": "FourierCompress\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u7a00\u758f\u6027\u7684LLM\u6fc0\u6d3b\u538b\u7f29\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u534f\u4f5cLLM\u63a8\u7406\u4e2d\u7531\u9ad8\u7ef4\u4e2d\u95f4\u6fc0\u6d3b\u4f20\u8f93\u5f15\u8d77\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u3001\u4f4e\u91cd\u6784\u8bef\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "FourierCompress\u5c06\u6fc0\u6d3b\u8f6c\u6362\u5230\u9891\u57df\uff0c\u4ec5\u4fdd\u7559\u4f4e\u9891\u7cfb\u6570\uff0c\u5e76\u901a\u8fc7\u5171\u8f6d\u5bf9\u79f0\u6027\u5728\u670d\u52a1\u5668\u7aef\u91cd\u5efa\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u3002", "result": "\u5728Llama 3\u548cQwen2.5\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFourierCompress\u5728\u6fc0\u6d3b\u5927\u5c0f\u4e0a\u5e73\u5747\u51cf\u5c11\u4e867.6\u500d\uff0c\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e0.3%\uff0c\u538b\u7f29\u65f6\u95f4\u6bd4Top-k\u5feb32\u500d\u4ee5\u4e0a\u3002", "conclusion": "FourierCompress\u901a\u8fc7\u5229\u7528LLM\u6fc0\u6d3b\u7684\u9891\u7387\u57df\u7a00\u758f\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u65e0\u635f\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17147", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17147", "abs": "https://arxiv.org/abs/2510.17147", "authors": ["Linhan Xia", "Mingzhan Yang", "Jingjing Wang", "Ziwei Yan", "Yakun Ren", "Guo Yu", "Kai Lei"], "title": "Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking", "comment": null, "summary": "Transformer-based large language models (LLMs) are increasingly being adopted\nin networking research to address domain-specific challenges. However, their\nquadratic time complexity and substantial model sizes often result in\nsignificant computational overhead and memory constraints, particularly in\nresource-constrained environments. Drawing inspiration from the efficiency and\nperformance of the Deepseek-R1 model within the knowledge distillation\nparadigm, this paper introduces Mamba4Net, a novel cross-architecture\ndistillation framework. Mamba4Net transfers networking-specific knowledge from\ntransformer-based LLMs to student models built on the Mamba architecture, which\nfeatures linear time complexity. This design substantially enhances\ncomputational efficiency compared to the quadratic complexity of\ntransformer-based models, while the reduced model size further minimizes\ncomputational demands, improving overall performance and resource utilization.\nTo evaluate its effectiveness, Mamba4Net was tested across three diverse\nnetworking tasks: viewport prediction, adaptive bitrate streaming, and cluster\njob scheduling. Compared to existing methods that do not leverage LLMs,\nMamba4Net demonstrates superior task performance. Furthermore, relative to\ndirect applications of transformer-based LLMs, it achieves significant\nefficiency gains, including a throughput 3.96 times higher and a storage\nfootprint of only 5.48% of that required by previous LLM-based approaches.\nThese results highlight Mamba4Net's potential to enable the cost-effective\napplication of LLM-derived knowledge in networking contexts. The source code is\nopenly available to support further research and development.", "AI": {"tldr": "Mamba4Net\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06LLM\u7684\u7f51\u7edc\u77e5\u8bc6\u8f6c\u79fb\u5230Mamba\u67b6\u6784\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7f51\u7edc\u73af\u5883\u3002", "motivation": "\u5c3d\u7ba1Transformer-based LLMs\u5728\u7f51\u7edc\u7814\u7a76\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5927\u6a21\u578b\u5c3a\u5bf8\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMamba4Net\u7684\u8de8\u67b6\u6784\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u57fa\u4e8eTransformer\u7684LLMs\u7684\u7f51\u7edc\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\u5230\u57fa\u4e8eMamba\u67b6\u6784\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u540e\u8005\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "Mamba4Net\u5728\u4e09\u4e2a\u7f51\u7edc\u4efb\u52a1\uff08\u89c6\u53e3\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u6d41\u548c\u96c6\u7fa4\u4f5c\u4e1a\u8c03\u5ea6\uff09\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e863.96\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c\u4ec5\u97005.48%\u7684\u5b58\u50a8\u7a7a\u95f4\u3002", "conclusion": "Mamba4Net\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7f51\u7edc\u7279\u5b9a\u77e5\u8bc6\u8f6c\u79fb\u5230\u57fa\u4e8eMamba\u67b6\u6784\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u7f51\u7edc\u9886\u57df\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684LLM\u5e94\u7528\u65b9\u6848\u3002"}}
{"id": "2510.16055", "categories": ["cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16055", "abs": "https://arxiv.org/abs/2510.16055", "authors": ["Norman Zadeh"], "title": "Is Zadeh's Least-Entered Pivot Rule Exponential?", "comment": "8 pages, 1 figure", "summary": "In 2011, Friedmann [F 7] claimed to have proved that pathological linear\nprograms existed for which the Simplex method using Zadeh's least-entered rule\n[Z 14] would take an exponential number of pivots. In 2019, Disser and Hopp [DH\n5] argued that there were errors in Friedmann's 2011 construction. In 2020,\nDisser, Friedmann, and Hopp [DFH 3,4] again contended that the least-entered\nrule was exponential. We show that their arguments contain multiple flaws. In\nother words, the worst-case behavior of the least-entered rule has not been\nestablished. Neither [F 7] nor [DFH 3,4] provides pathological linear programs\nthat can be tested. Instead, the authors contend that their pathological linear\nprograms are of the form (P) as shown on page 12 of [DFH 3]. The authors\ncontend that the constraints of (P) ensure that the probability of entering a\nvertex u is equal to the probability of exiting u. In fact, we note that the\nauthors' constraints (P) are flawed in at least three ways: a) they require the\nprobability of exiting u to exceed the probability of entering u, b) they\nrequire the probability of exiting some nodes to exceed 1, and c) they overlook\nflows from decision nodes to decision nodes. At my request, in August of 2025,\nDisser, Friedmann, and Hopp provided me with their first ten purportedly\npathological LPs and the graph of their first purportedly pathological Markov\nDecision Process (MDP1). It is shown that: a) their first two pathological LPs\nare infeasible if the variables are supposed to be probabilities, as the\nauthors contend, and b) their first purportedly pathological LP does not match\nup with their first purportedly pathological MDP. In other words, the authors\nhave not come close to providing counterexamples to the least-entered rule.", "AI": {"tldr": "\u8bba\u6587\u53cd\u9a73\u4e86\u5173\u4e8e\u6700\u5c0f\u8fdb\u5165\u89c4\u5219\u6307\u6570\u590d\u6742\u5ea6\u7684\u9519\u8bef\u8bba\u8bc1\uff0c\u6307\u51fa\u5176\u63d0\u4f9b\u7684\u53cd\u4f8b\u5b58\u5728\u7f3a\u9677\u3002", "motivation": "\u53cd\u9a732011\u5e74Friedmann\u548c2019\u5e74Disser\u4e0eHopp\u5173\u4e8e\u6700\u5c0f\u8fdb\u5165\u89c4\u5219\u6307\u6570\u590d\u6742\u5ea6\u7684\u9519\u8bef\u7ed3\u8bba\u3002", "method": "\u901a\u8fc7\u5206\u6790Disser\u7b49\u4eba\u63d0\u4f9b\u7684\u75c5\u7406\u7ebf\u6027\u7a0b\u5e8f\u548cMDP\uff0c\u53d1\u73b0\u5176\u4e0d\u6ee1\u8db3\u6982\u7387\u7ea6\u675f\u4e14\u4e0eMDP\u4e0d\u5339\u914d\u3002", "result": "\u53d1\u73b0Disser\u7b49\u4eba\u63d0\u4f9b\u7684\u75c5\u7406\u7ebf\u6027\u7a0b\u5e8f\u4e0d\u53ef\u884c\u4e14\u4e0eMDP\u4e0d\u5339\u914d\uff0c\u672a\u80fd\u63d0\u4f9b\u6709\u6548\u53cd\u4f8b\u3002", "conclusion": "\u8bba\u6587\u6307\u51faDisser\u3001Friedmann\u548cHopp\u7684\u8bba\u8bc1\u5b58\u5728\u591a\u5904\u7f3a\u9677\uff0c\u672a\u80fd\u8bc1\u660e\u6700\u5c0f\u8fdb\u5165\u89c4\u5219\u7684\u6700\u574f\u60c5\u51b5\u884c\u4e3a\u3002"}}
{"id": "2510.16497", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16497", "abs": "https://arxiv.org/abs/2510.16497", "authors": ["Pacome Simon Mbonimpa", "Diane Tuyizere", "Azizuddin Ahmed Biyabani", "Ozan K. Tonguz"], "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages", "comment": null, "summary": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18-\u4e91\u5e76\u884c\u7684\u8bed\u97f3\u8f6c\u5f55\u4e0e\u5408\u6210\u6846\u67b6\uff0c\u663e\u8457\u4f18\u5316\u4e86Kinyarwanda\u548cSwahili\u8bed\u8a00\u7684\u5904\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u5730\u533a\u3002", "motivation": "\u9488\u5bf9\u4e1c\u975e\u5730\u533a\u5e7f\u6cdb\u4f7f\u7528\u7684Kinyarwanda\u548cSwahili\u8bed\u8a00\u7f3a\u4e4f\u5f3a\u5927\u8bed\u8a00\u5904\u7406\u5de5\u5177\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u80cc\u666f\u4e0b\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8fb9\u7f18-\u4e91\u5e76\u884c\u5904\u7406\u63d0\u5347\u8bed\u97f3\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002", "method": "\u6846\u67b6\u7ed3\u5408Whisper\u548cSpeechT5\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u91c7\u7528\u7ea7\u8054\u673a\u5236\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u4e91\u7aef\u5206\u914d\u6a21\u578b\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f18\u5316\u4e86\u5185\u5b58\u4f7f\u7528\uff08SpeechT5\u538b\u7f299.5%\uff0cWhisper\u538b\u7f2914%\uff09\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57281.7 GHz CPU\u8fb9\u7f18\u8bbe\u5907\u548c1 MB/s\u7f51\u7edc\u5e26\u5bbd\u4e0b\uff0c\u7cfb\u7edf\u80fd\u57281\u5206\u949f\u5185\u5904\u7406270\u5b57\u7b26\u7684\u6587\u672c\uff08STT\u548cTTS\uff09\u3002\u5b9e\u9645\u8c03\u67e5\u6570\u636e\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u5728\u51c6\u786e\u6027\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u8fb9\u7f18-\u4e91\u5e76\u884c\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86Kinyarwanda\u548cSwahili\u8bed\u8a00\u7684\u8bed\u97f3\u8f6c\u5f55\u4e0e\u5408\u6210\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17342", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17342", "abs": "https://arxiv.org/abs/2510.17342", "authors": ["Alberto Ceresoli", "Viola Bernazzoli", "Roberto Pegurri", "Ilario Filippini"], "title": "AoA Services in 5G Networks: A Framework for Real-World Implementation and Systematic Testing", "comment": null, "summary": "Accurate positioning is a key enabler for emerging 5G applications. While the\nstandardized Location Management Function (LMF) operates centrally within the\ncore network, its scalability and latency limitations hinder low-latency and\nfine-grained localization. A practical alternative is to shift positioning\nintelligence toward the radio access network (RAN), where uplink sounding\nreference signal (SRS)-based angle-of-arrival (AoA) estimation offers a\nlightweight, network-native solution. In this work, we present the first fully\nopen-source 5G testbed for AoA estimation, enabling systematic and repeatable\nexperimentation under realistic yet controllable channel conditions. The\nframework integrates the NVIDIA Sionna RT with a Keysight PROPSIM channel\nemulator and includes a novel phase calibration procedure for USRP N310\ndevices. Experimental results show sub-degree to few-degree accuracy,\nvalidating the feasibility of lightweight, single-anchor, network-native\nlocalization within next-generation 5G systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u76845G\u6d4b\u8bd5\u5e8a\uff0c\u7528\u4e8eAoA\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u3001\u5355\u951a\u70b9\u3001\u7f51\u7edc\u539f\u751f\u5b9a\u4f4d\u57285G\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u6807\u51c6\u5316\u7684\u4f4d\u7f6e\u7ba1\u7406\u529f\u80fd\uff08LMF\uff09\u5728\u6838\u5fc3\u7f51\u7edc\u4e2d\u96c6\u4e2d\u8fd0\u884c\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u9650\u5236\u963b\u788d\u4e86\u4f4e\u5ef6\u8fdf\u548c\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u3002\u5c06\u5b9a\u4f4d\u667a\u80fd\u8f6c\u5411\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u662f\u4e00\u79cd\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86NVIDIA Sionna RT\u4e0eKeysight PROPSIM\u4fe1\u9053\u6a21\u62df\u5668\uff0c\u5e76\u5305\u542b\u4e86\u4e00\u79cd\u9488\u5bf9USRP N310\u8bbe\u5907\u7684\u65b0\u578b\u76f8\u4f4d\u6821\u51c6\u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u4e0a\u884c\u63a2\u6d4b\u53c2\u8003\u4fe1\u53f7\uff08SRS\uff09\u7684\u5230\u8fbe\u89d2\uff08AoA\uff09\u4f30\u8ba1\u80fd\u591f\u5b9e\u73b0\u4e9a\u5ea6\u81f3\u51e0\u5ea6\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u3001\u5355\u951a\u70b9\u3001\u7f51\u7edc\u539f\u751f\u5b9a\u4f4d\u5728\u4e0b\u4e00\u4ee35G\u7cfb\u7edf\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u7cbe\u5ea6\u53ef\u8fbe\u4e9a\u5ea6\u81f3\u51e0\u5ea6\u3002"}}
{"id": "2510.16330", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.16330", "abs": "https://arxiv.org/abs/2510.16330", "authors": ["Daniel Paul-Pena", "C. Seshadhri"], "title": "Near-linear time subhypergraph counting in bounded degeneracy hypergraphs", "comment": null, "summary": "Counting small patterns in a large dataset is a fundamental algorithmic task.\nThe most common version of this task is subgraph/homomorphism counting, wherein\nwe count the number of occurrences of a small pattern graph $H$ in an input\ngraph $G$. The study of this problem is a field in and of itself. Recently,\nboth in theory and practice, there has been an interest in \\emph{hypergraph}\nalgorithms, where $G = (V,E)$ is a hypergraph. One can view $G$ as a set system\nwhere hyperedges are subsets of the universe $V$.\n  Counting patterns $H$ in hypergraphs is less studied, although there are many\napplications in network science and database algorithms. Inspired by advances\nin the graph literature, we study when linear time algorithms are possible.\n  We focus on input hypergraphs $G$ that have bounded \\emph{degeneracy}, a\nwell-studied concept for graph algorithms. We give a spectrum of definitions\nfor hypergraph degeneracy that cover all existing notions. For each such\ndefinition, we give a precise characterization of the patterns $H$ that can be\ncounted in (near) linear time. Specifically, we discover a set of ``obstruction\npatterns\". If $H$ does not contain an obstruction, then the number of\n$H$-subhypergraphs can be counted exactly in $O(n\\log n)$ time (where $n$ is\nthe number of vertices in $G$). If $H$ contains an obstruction, then (assuming\nhypergraph variants of fine-grained complexity conjectures), there is a\nconstant $\\gamma > 0$, such that there is no $o(n^{1+\\gamma})$ time algorithm\nfor counting $H$-subhypergraphs. These sets of obstructions can be defined for\nall notions of hypergraph degeneracy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d85\u56fe\u4e2d\u6a21\u5f0f\u8ba1\u6570\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9000\u5316\u6027\u5b9a\u4e49\u7684\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u6761\u4ef6\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u7ec4\u969c\u788d\u6a21\u5f0f\u51b3\u5b9a\u8ba1\u6570\u590d\u6742\u6027\u3002", "motivation": "\u8d85\u56fe\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u8d85\u56fe\u4e2d\u6a21\u5f0f\u8ba1\u6570\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u5c3d\u7ba1\u5728\u7f51\u7edc\u79d1\u5b66\u548c\u6570\u636e\u5e93\u7b97\u6cd5\u4e2d\u6709\u8bb8\u591a\u5e94\u7528\u3002\u53d7\u56fe\u7b97\u6cd5\u6587\u732e\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4f55\u65f6\u53ef\u4ee5\u8bbe\u8ba1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u6709\u754c\u9000\u5316\u6027\u7684\u8f93\u5165\u8d85\u56feG\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8986\u76d6\u6240\u6709\u73b0\u6709\u9000\u5316\u6027\u6982\u5ff5\u7684\u5b9a\u4e49\u3002\u5bf9\u4e8e\u6bcf\u79cd\u5b9a\u4e49\uff0c\u7ed9\u51fa\u4e86\u6a21\u5f0fH\u662f\u5426\u53ef\u4ee5\u5728\u7ebf\u6027\u6216\u8fd1\u7ebf\u6027\u65f6\u95f4\u5185\u8ba1\u6570\u7684\u7cbe\u786e\u523b\u753b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u7ec4\u201c\u969c\u788d\u6a21\u5f0f\u201d\uff0c\u5982\u679cH\u4e0d\u5305\u542b\u8fd9\u4e9b\u969c\u788d\uff0c\u5219\u53ef\u4ee5\u7cbe\u786e\u8ba1\u6570H-\u5b50\u8d85\u56fe\u7684\u6570\u91cf\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n log n)\uff1b\u5426\u5219\uff0c\u5728\u5047\u8bbe\u8d85\u56fe\u7248\u672c\u7684\u7ec6\u7c92\u5ea6\u590d\u6742\u6027\u731c\u60f3\u4e0b\uff0c\u4e0d\u5b58\u5728o(n^{1+\u03b3})\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u8d85\u56fe\u6a21\u5f0f\u8ba1\u6570\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u8d85\u56fe\u9000\u5316\u6027\u7684\u4e00\u7cfb\u5217\u6982\u5ff5\uff0c\u7cbe\u786e\u523b\u753b\u4e86\u54ea\u4e9b\u6a21\u5f0f\u53ef\u4ee5\u5728\u7ebf\u6027\u6216\u8fd1\u7ebf\u6027\u65f6\u95f4\u5185\u88ab\u8ba1\u6570\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u7ec4\u201c\u969c\u788d\u6a21\u5f0f\u201d\uff0c\u5982\u679c\u6a21\u5f0f\u4e0d\u5305\u542b\u8fd9\u4e9b\u969c\u788d\uff0c\u5219\u53ef\u4ee5\u5728O(n log n)\u65f6\u95f4\u5185\u7cbe\u786e\u8ba1\u6570\uff1b\u5426\u5219\uff0c\u5728\u5047\u8bbe\u8d85\u56fe\u7248\u672c\u7684\u7ec6\u7c92\u5ea6\u590d\u6742\u6027\u731c\u60f3\u4e0b\uff0c\u4e0d\u5b58\u5728o(n^{1+\u03b3})\u65f6\u95f4\u7b97\u6cd5\u3002"}}
{"id": "2510.16059", "categories": ["cs.SE", "cs.CL", "D.2.2; D.2.3"], "pdf": "https://arxiv.org/pdf/2510.16059", "abs": "https://arxiv.org/abs/2510.16059", "authors": ["Xin Cao", "Nan Yu"], "title": "SIADAFIX: issue description response for adaptive program repair", "comment": "20 pages, 3 figures", "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of\nlarge language model-based agents on complex tasks such as program repair. In\nparticular, we design an adaptive program repair method based on issue\ndescription response, called SIADAFIX. The proposed method utilizes slow\nthinking bug fix agent to complete complex program repair tasks, and employs\nfast thinking workflow decision components to optimize and classify issue\ndescriptions, using issue description response results to guide the\norchestration of bug fix agent workflows. SIADAFIX adaptively selects three\nrepair modes, i.e., easy, middle and hard mode, based on problem complexity. It\nemploys fast generalization for simple problems and test-time scaling\ntechniques for complex problems. Experimental results on the SWE-bench Lite\nshow that the proposed method achieves 60.67% pass@1 performance using the\nClaude-4 Sonnet model, reaching state-of-the-art levels among all open-source\nmethods. SIADAFIX effectively balances repair efficiency and accuracy,\nproviding new insights for automated program repair. Our code is available at\nhttps://github.com/liauto-siada/siada-cli.", "AI": {"tldr": "SIADAFIX\u662f\u4e00\u79cd\u7ed3\u5408\u5feb\u6162\u601d\u8003\u7684\u81ea\u9002\u5e94\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u4f5c\u6d41\u548c\u5206\u7c7b\u95ee\u9898\u63cf\u8ff0\uff0c\u6709\u6548\u63d0\u5347\u4fee\u590d\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u6027\u80fd\u8fbe60.67% pass@1\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u5feb\u6162\u601d\u8003\u7b56\u7565\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u4fee\u590d\uff09\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u63cf\u8ff0\u54cd\u5e94\u7684\u81ea\u9002\u5e94\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5SIADAFIX\uff0c\u5229\u7528\u6162\u601d\u8003\u7684bug\u4fee\u590d\u4ee3\u7406\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u5feb\u601d\u8003\u7684\u5de5\u4f5c\u6d41\u51b3\u7b56\u7ec4\u4ef6\u4f18\u5316\u548c\u5206\u7c7b\u95ee\u9898\u63cf\u8ff0\u3002\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u4e09\u79cd\u4fee\u590d\u6a21\u5f0f\uff08\u7b80\u5355\u3001\u4e2d\u7b49\u3001\u56f0\u96be\uff09\u3002", "result": "\u5728SWE-bench Lite\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSIADAFIX\u4f7f\u7528Claude-4 Sonnet\u6a21\u578b\u8fbe\u5230\u4e8660.67%\u7684pass@1\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u5f00\u6e90\u65b9\u6cd5\u4e2d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "SIADAFIX\u901a\u8fc7\u7ed3\u5408\u5feb\u6162\u601d\u8003\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u7a0b\u5e8f\u4fee\u590d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.15873", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.15873", "abs": "https://arxiv.org/abs/2510.15873", "authors": ["Hengyuan Chang", "Xiaoxuan Xie", "Syuhei Sato", "Haoran Xie"], "title": "Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function", "comment": "3 pages, 4 figures. SIGGRAPH 2025 Poster", "summary": "In this paper, we propose a two-stage sketch-based smoke illustration\ngeneration framework using stream function and latent diffusion models (LDM).\nThe user sketch is used to guide the generation of the stream function, which\nserves as the control condition for the velocity field generator. The generated\nvelocity field can be used to guide the smoke simulation to align with the\nintended flow. We adopt streamlines to encode global flow dynamics as sketch\nguidance during training. The stream function constitutes the intermediate\nrepresentation that captures continuous variation and rotational flow details\nabsent from sketches.", "AI": {"tldr": "\u7ed3\u5408\u6d41\u51fd\u6570\u548cLDM\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8349\u56fe\u7684\u70df\u96fe\u751f\u6210\uff0c\u8865\u5145\u8349\u56fe\u4e2d\u7f3a\u5931\u7684\u6d41\u52a8\u7ec6\u8282\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u70df\u96fe\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7528\u6237\u8349\u56fe\u610f\u56fe\u7684\u8fde\u7eed\u53d8\u5316\u548c\u65cb\u8f6c\u6d41\u52a8\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d41\u51fd\u6570\u548cLDM\u7684\u65b0\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5229\u7528\u7528\u6237\u8349\u56fe\u751f\u6210\u6d41\u51fd\u6570\uff0c\u4f5c\u4e3a\u901f\u5ea6\u573a\u751f\u6210\u5668\u7684\u63a7\u5236\u6761\u4ef6\uff1b\u5176\u6b21\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u751f\u6210\u70df\u96fe\u6a21\u62df\uff0c\u786e\u4fdd\u4e0e\u9884\u671f\u6d41\u52a8\u5bf9\u9f50\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6d41\u7ebf\u7f16\u7801\u5168\u5c40\u6d41\u52a8\u52a8\u529b\u5b66\u4f5c\u4e3a\u8349\u56fe\u6307\u5bfc\u3002", "result": "\u751f\u6210\u7684\u70df\u96fe\u6a21\u62df\u80fd\u591f\u51c6\u786e\u5bf9\u9f50\u7528\u6237\u8349\u56fe\u7684\u9884\u671f\u6d41\u52a8\uff0c\u6d41\u51fd\u6570\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6709\u6548\u8865\u5145\u4e86\u8349\u56fe\u4e2d\u7f3a\u5931\u7684\u7ec6\u8282\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6d41\u51fd\u6570\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7528\u6237\u8349\u56fe\u7684\u70df\u96fe\u751f\u6210\uff0c\u6709\u6548\u6355\u6349\u4e86\u8349\u56fe\u4e2d\u7f3a\u5931\u7684\u8fde\u7eed\u53d8\u5316\u548c\u65cb\u8f6c\u6d41\u52a8\u7ec6\u8282\u3002"}}
{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "ESCA\u6846\u67b6\u901a\u8fc7SGClip\u6a21\u578b\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u7ba1\u9053\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u89c6\u89c9\u5185\u5bb9\u4e0e\u6587\u672c\u8bed\u4e49\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u5177\u8eab\u4ee3\u7406\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86ESCA\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662fSGClip\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7ba1\u9053\u572887K+\u5f00\u653e\u57df\u89c6\u9891\u4e0a\u8bad\u7ec3\uff0c\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u76d1\u7763\u548c\u7ed3\u6784\u5316\u63a8\u7406\u3002", "result": "SGClip\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cESCA\u6846\u67b6\u5728\u4e24\u4e2a\u5177\u8eab\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ESCA\u6846\u67b6\u901a\u8fc7SGClip\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u611f\u77e5\u9519\u8bef\uff0c\u5e76\u4f7f\u5f00\u6e90\u6a21\u578b\u8d85\u8d8a\u4e13\u6709\u57fa\u7ebf\u3002"}}
{"id": "2510.16205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16205", "abs": "https://arxiv.org/abs/2510.16205", "authors": ["Jo\u00e3o Carlos Virgolino Soares", "Gabriel Fischer Abati", "Claudio Semini"], "title": "VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments", "comment": "Code available at https://github.com/iit-DLSLab/VAR-SLAM", "summary": "Visual SLAM in dynamic environments remains challenging, as several existing\nmethods rely on semantic filtering that only handles known object classes, or\nuse fixed robust kernels that cannot adapt to unknown moving objects, leading\nto degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual\nAdaptive and Robust SLAM), an ORB-SLAM3-based system that combines a\nlightweight semantic keypoint filter to deal with known moving objects, with\nBarron's adaptive robust loss to handle unknown ones. The shape parameter of\nthe robust kernel is estimated online from residuals, allowing the system to\nautomatically adjust between Gaussian and heavy-tailed behavior. We evaluate\nVAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which\ninclude both known and unknown moving objects. Results show improved trajectory\naccuracy and robustness over state-of-the-art baselines, achieving up to 25%\nlower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining\nperformance at 27 FPS on average.", "AI": {"tldr": "VAR-SLAM\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u52a8\u6001\u73af\u5883\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u7ed3\u5408\u8bed\u4e49\u8fc7\u6ee4\u548c\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bed\u4e49\u8fc7\u6ee4\u6216\u56fa\u5b9a\u9c81\u68d2\u6838\uff0c\u65e0\u6cd5\u9002\u5e94\u672a\u77e5\u79fb\u52a8\u7269\u4f53\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u57fa\u4e8eORB-SLAM3\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5173\u952e\u70b9\u8fc7\u6ee4\u5668\u548cBarron\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\uff0c\u5728\u7ebf\u4f30\u8ba1\u9c81\u68d2\u6838\u7684\u5f62\u72b6\u53c2\u6570\u3002", "result": "\u5728TUM RGB-D\u3001Bonn RGB-D Dynamic\u548cOpenLORIS\u6570\u636e\u96c6\u4e0a\uff0cVAR-SLAM\u7684\u8f68\u8ff9\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0cATE RMSE\u964d\u4f4e\u8fbe25%\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u574727 FPS\u7684\u6027\u80fd\u3002", "conclusion": "VAR-SLAM\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5173\u952e\u70b9\u8fc7\u6ee4\u5668\u548c\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u89c6\u89c9SLAM\u7684\u8f68\u8ff9\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16606", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16606", "abs": "https://arxiv.org/abs/2510.16606", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "Reimagining RDMA Through the Lens of ML", "comment": "4 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs\nconnected by ultra-high-speed inter-connects, tail latency in collective\ncommunication has emerged as a primary bottleneck. Prior RDMA designs, like\nRoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying\non retransmissions and packet sequencing to ensure correctness. While effective\nfor general-purpose workloads, these mechanisms introduce complexity and\nlatency that scale poorly, where even rare packet losses or delays can\nconsistently degrade system performance. We introduce Celeris, a\ndomain-specific RDMA transport that revisits traditional reliability guarantees\nbased on ML's tolerance for lost or partial data. Celeris removes\nretransmissions and in-order delivery from the RDMA NIC, enabling best-effort\ntransport that exploits the robustness of ML workloads. It retains congestion\ncontrol (e.g., DCQCN) and manages communication with software-level mechanisms\nsuch as adaptive timeouts and data prioritization, while shifting loss recovery\nto the ML pipeline (e.g., using the Hadamard Transform). Early results show\nthat Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by\n67%, and nearly doubles NIC resilience to faults -- delivering a resilient,\nscalable transport tailored for ML at cluster scale.", "AI": {"tldr": "Celeris\u662f\u4e00\u79cd\u9488\u5bf9ML\u4f18\u5316\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u79fb\u9664\u91cd\u4f20\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5c3e\u90e8\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0fML\u5de5\u4f5c\u8d1f\u8f7d\u6269\u5c55\u5230\u6570\u5343\u4e2aGPU\uff0c\u96c6\u4f53\u901a\u4fe1\u4e2d\u7684\u5c3e\u90e8\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edfRDMA\u8bbe\u8ba1\uff08\u5982RoCE\u3001IRN\u3001SRNIC\uff09\u7684\u4e25\u683c\u53ef\u9760\u6027\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\u867d\u7136\u9002\u7528\u4e8e\u901a\u7528\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46\u5bf9\u4e8eML\u5de5\u4f5c\u8d1f\u8f7d\u800c\u8a00\uff0c\u8fd9\u4e9b\u673a\u5236\u5f15\u5165\u4e86\u590d\u6742\u6027\u548c\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\u3002", "method": "Celeris\u662f\u4e00\u79cd\u9488\u5bf9ML\u4f18\u5316\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u79fb\u9664\u4e86\u4f20\u7edfRDMA NIC\u7684\u91cd\u4f20\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\uff0c\u91c7\u7528\u6700\u4f73\u52aa\u529b\u4f20\u8f93\u7b56\u7565\uff0c\u5e76\u5229\u7528\u8f6f\u4ef6\u7ea7\u673a\u5236\uff08\u5982\u81ea\u9002\u5e94\u8d85\u65f6\u548c\u6570\u636e\u4f18\u5148\u7ea7\uff09\u8fdb\u884c\u901a\u4fe1\u7ba1\u7406\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0cCeleris\u5c06\u7b2c99\u767e\u5206\u4f4d\u5ef6\u8fdf\u964d\u4f4e\u4e862.3\u500d\uff0cBRAM\u4f7f\u7528\u51cf\u5c11\u4e8667%\uff0cNIC\u5bf9\u6545\u969c\u7684\u6062\u590d\u80fd\u529b\u63d0\u9ad8\u4e86\u8fd1\u4e00\u500d\u3002", "conclusion": "Celeris\u901a\u8fc7\u79fb\u9664\u4f20\u7edfRDMA NIC\u7684\u91cd\u4f20\u548c\u6709\u5e8f\u4ea4\u4ed8\u673a\u5236\uff0c\u4e3aML\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f39\u6027\u3001\u53ef\u6269\u5c55\u7684\u4f20\u8f93\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5c3e\u90e8\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.17395", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17395", "abs": "https://arxiv.org/abs/2510.17395", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Aleksei Shashin", "Evgeny Khorov"], "title": "Enhancing 5G V2X Mode 2 for Sporadic Traffic", "comment": null, "summary": "The emerging road safety and autonomous vehicle applications require timely\nand reliable data delivery between vehicles and between vehicles and\ninfrastructure. To satisfy this demand, 3GPP develops a 5G\nVehicle-to-Everything (V2X) technology. Depending on the served traffic type,\n5G V2X specifications propose two channel access methods: (i) Mode 1, according\nto which a base station allocates resources to users, and (ii) Mode 2,\naccording to which users autonomously select resources for their transmissions.\nIn the paper, we consider a scenario with sporadic traffic, e.g., a vehicle\ngenerates a packet at a random time moment when it detects a dangerous\nsituation, which imposes strict requirements on delay and reliability. To\nsatisfy strict delay requirements, vehicles use Mode 2. We analyze the\nperformance of Mode 2 for sporadic traffic and propose several approaches to\nimprove it. Simulation results show that the proposed approaches can increase\nthe system capacity by up to 40% with a low impact on complexity.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865G V2X\u4e2dMode 2\u5728\u5076\u53d1\u6d41\u91cf\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\uff0c\u4eff\u771f\u663e\u793a\u7cfb\u7edf\u5bb9\u91cf\u53ef\u63d0\u534740%\uff0c\u4e14\u590d\u6742\u5ea6\u589e\u52a0\u6709\u9650\u3002", "motivation": "\u6ee1\u8db3\u8f66\u8f86\u95f4\u53ca\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u95f4\u5bf9\u53ca\u65f6\u53ef\u9760\u6570\u636e\u4f20\u8f93\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u5076\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\uff08\u5982\u8f66\u8f86\u68c0\u6d4b\u5230\u5371\u9669\u60c5\u51b5\u65f6\u968f\u673a\u751f\u6210\u6570\u636e\u5305\uff09\uff0c\u9700\u6ee1\u8db3\u4e25\u683c\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u8981\u6c42\u3002", "method": "\u5206\u6790\u4e86Mode 2\u5728\u5076\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6cd5\u53ef\u5c06\u7cfb\u7edf\u5bb9\u91cf\u63d0\u5347\u9ad8\u8fbe40%\uff0c\u540c\u65f6\u5bf9\u7cfb\u7edf\u7684\u590d\u6742\u5ea6\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e865G V2X\u7cfb\u7edf\u4e2dMode 2\u7684\u6027\u80fd\uff0c\u7cfb\u7edf\u5bb9\u91cf\u53ef\u589e\u52a0\u9ad8\u8fbe40%\uff0c\u4e14\u5bf9\u590d\u6742\u5ea6\u5f71\u54cd\u8f83\u4f4e\u3002"}}
{"id": "2510.16336", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16336", "abs": "https://arxiv.org/abs/2510.16336", "authors": ["Pachara Sawettamalya", "Huacheng Yu"], "title": "A (Very) Nearly Optimal Sketch for $k$-Edge Connectivity Certificates", "comment": null, "summary": "In this note, we present a simple algorithm for computing a\n\\emph{$k$-connectivity certificate} in dynamic graph streams. Our algorithm\nuses $O(n \\log^2 n \\cdot \\max\\{k, \\log n \\log k\\})$ bits of space which\nimproves upon the $O(kn \\log^3 n)$-space algorithm of Ahn, Guha, and McGregor\n(SODA'12). For the values of $k$ that are truly sublinear, our space usage\n\\emph{very nearly} matches the known lower bound $\\Omega(n \\log^2 n \\cdot\n\\max\\{k, \\log n\\})$ established by Nelson and Yu (SODA'19; implicit) and\nRobinson (DISC'24). In particular, our algorithm fully settles the space\ncomplexity at $\\Theta(kn \\log^2{n})$ for $k = \\Omega(\\log n \\log \\log n)$, and\nbridges the gap down to only a doubly-logarithmic factor of $O(\\log \\log n)$\nfor a smaller range of $k = o(\\log n \\log \\log n)$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u52a8\u6001\u56fe\u6d41$k$-\u8fde\u901a\u6027\u8bc1\u4e66\u7b97\u6cd5\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u63a5\u8fd1\u5df2\u77e5\u4e0b\u754c\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u5728\u7a7a\u95f4\u590d\u6742\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e$k$\u4e3a\u4e9a\u7ebf\u6027\u503c\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u56fe\u6d41\u4e2d\u8ba1\u7b97$k$-\u8fde\u901a\u6027\u8bc1\u4e66\u3002", "result": "\u7b97\u6cd5\u5728\u7a7a\u95f4\u4f7f\u7528\u4e0a\u63a5\u8fd1\u5df2\u77e5\u4e0b\u754c\uff0c\u7279\u522b\u662f\u5728$k = \\Omega(\\log n \\log \\log n)$\u65f6\u5b8c\u5168\u5339\u914d\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u52a8\u6001\u56fe\u6d41\u4e2d\u8ba1\u7b97$k$-\u8fde\u901a\u6027\u8bc1\u4e66\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a$O(n \\log^2 n \\cdot \\max\\{k, \\log n \\log k\\})$\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u5e76\u5728$k = \\Omega(\\log n \\log \\log n)$\u65f6\u5b8c\u5168\u5339\u914d\u5df2\u77e5\u4e0b\u754c\u3002"}}
{"id": "2510.16242", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.16242", "abs": "https://arxiv.org/abs/2510.16242", "authors": ["Eva Maxfield Brown", "Isaac Slaughter", "Nicholas Weber"], "title": "Code Contribution and Credit in Science", "comment": null, "summary": "Software development has become essential to scientific research, but its\nrelationship to traditional metrics of scholarly credit remains poorly\nunderstood. We develop a dataset of approximately 140,000 paired research\narticles and code repositories, as well as a predictive model that matches\nresearch article authors with software repository developer accounts. We use\nthis data to investigate how software development activities influence credit\nallocation in collaborative scientific settings. Our findings reveal\nsignificant patterns distinguishing software contributions from traditional\nauthorship credit. We find that nearly 30% of articles include non-author code\ncontributors- individuals who participated in software development but received\nno formal authorship recognition. While code-contributing authors show a modest\n$\\sim$4.2% increase in article citations, this effect becomes non-significant\nwhen controlling for domain, article type, and open access status. First\nauthors are significantly more likely to be code contributors than other author\npositions. Notably, we identify a negative relationship between coding\nfrequency and scholarly impact metrics. Authors who contribute code more\nfrequently exhibit progressively lower h-indices than non-coding colleagues,\neven when controlling for publication count, author position, domain, and\narticle type. These results suggest a disconnect between software contributions\nand credit, highlighting important implications for institutional reward\nstructures and science policy.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8f6f\u4ef6\u8d21\u732e\u4e0e\u4f20\u7edf\u5b66\u672f\u8ba4\u53ef\u8131\u8282\uff0c\u4ee3\u7801\u8d21\u732e\u8005\u5e38\u88ab\u5ffd\u89c6\uff0c\u4e14\u9891\u7e41\u4ee3\u7801\u8d21\u732e\u4e0e\u8f83\u4f4e\u5b66\u672f\u5f71\u54cd\u529b\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u8f6f\u4ef6\u5f00\u53d1\u6d3b\u52a8\u5982\u4f55\u5f71\u54cd\u534f\u4f5c\u79d1\u5b66\u73af\u5883\u4e2d\u7684\u5b66\u672f\u8ba4\u53ef\u5206\u914d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u7ea6140,000\u7bc7\u7814\u7a76\u6587\u7ae0\u4e0e\u4ee3\u7801\u5e93\u914d\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u6765\u5339\u914d\u6587\u7ae0\u4f5c\u8005\u548c\u8f6f\u4ef6\u4ed3\u5e93\u5f00\u53d1\u8005\u8d26\u6237\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd130%\u7684\u6587\u7ae0\u5305\u542b\u975e\u4f5c\u8005\u7684\u4ee3\u7801\u8d21\u732e\u8005\uff1b\u4ee3\u7801\u8d21\u732e\u4f5c\u8005\u7684\u6587\u7ae0\u5f15\u7528\u7387\u67094.2%\u7684\u5fae\u5c0f\u63d0\u5347\uff0c\u4f46\u5728\u63a7\u5236\u53d8\u91cf\u540e\u6548\u679c\u4e0d\u663e\u8457\uff1b\u7b2c\u4e00\u4f5c\u8005\u66f4\u53ef\u80fd\u662f\u4ee3\u7801\u8d21\u732e\u8005\uff1b\u4ee3\u7801\u8d21\u732e\u9891\u7387\u4e0e\u5b66\u672f\u5f71\u54cd\u529b\u6307\u6807\u5448\u8d1f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8f6f\u4ef6\u8d21\u732e\u4e0e\u4f20\u7edf\u5b66\u672f\u8ba4\u53ef\u4e4b\u95f4\u7684\u8131\u8282\uff0c\u5bf9\u673a\u6784\u5956\u52b1\u673a\u5236\u548c\u79d1\u5b66\u653f\u7b56\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.15874", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.15874", "abs": "https://arxiv.org/abs/2510.15874", "authors": ["Hao Jin", "Haoran Xie"], "title": "Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images", "comment": "2 pages, 5 figures. SIGGRAPH 2025 Poster", "summary": "Integrating motion into static images not only enhances visual expressiveness\nbut also creates a sense of immersion and temporal depth, establishing it as a\nlongstanding and impactful theme in artistic expression. Fluid elements such as\nwaterfall, river, and oceans are common features in landscape, but their\ncomplex dynamic characteristics pose significant challenges in modeling and\ncontrolling their motion within visual computing. Physics-based methods are\noften used in fluid animation to track particle movement. However, they are\neasily affected by boundary conditions. Recently, latent diffusion models have\nbeen applied to video generation tasks, demonstrating impressive capabilities\nin producing high-quality and temporally coherent results. However, it is\nchallenging for the existing methods to animate fluid smooth and temporally\nconsistent motion. To solve these issues, this paper introduces a framework for\ngenerating landscape videos by animating fluid in still images under the\nguidance of motion sketches. We propose a finetuned conditional latent\ndiffusion model for generating motion field from user-provided sketches, which\nare subsequently integrated into a latent video diffusion model via a motion\nadapter to precisely control the fluid movement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u8349\u56fe\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9759\u6001\u56fe\u50cf\u4e2d\u751f\u6210\u6d41\u7545\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u6d41\u4f53\u52a8\u753b\u3002", "motivation": "\u6d41\u4f53\u5143\u7d20\u7684\u52a8\u6001\u7279\u6027\u590d\u6742\uff0c\u4f20\u7edf\u7269\u7406\u65b9\u6cd5\u6613\u53d7\u8fb9\u754c\u6761\u4ef6\u5f71\u54cd\uff0c\u73b0\u6709\u6f5c\u5728\u6269\u6563\u6a21\u578b\u96be\u4ee5\u751f\u6210\u6d41\u7545\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u8fd0\u52a8\u3002", "method": "\u91c7\u7528\u5fae\u8c03\u7684\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u8349\u56fe\u4e2d\u751f\u6210\u8fd0\u52a8\u573a\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u9002\u914d\u5668\u5c06\u5176\u96c6\u6210\u5230\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u4ee5\u7cbe\u786e\u63a7\u5236\u6d41\u4f53\u8fd0\u52a8\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u666f\u89c2\u89c6\u9891\uff0c\u901a\u8fc7\u52a8\u753b\u5316\u9759\u6001\u56fe\u50cf\u4e2d\u7684\u6d41\u4f53\uff0c\u5b9e\u73b0\u6d41\u7545\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u8fd0\u52a8\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8fd0\u52a8\u8349\u56fe\u5f15\u5bfc\u9759\u6001\u56fe\u50cf\u4e2d\u6d41\u4f53\u52a8\u753b\u751f\u6210\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u6d41\u7545\u4e14\u65f6\u95f4\u4e00\u81f4\u8fd0\u52a8\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "CrossRay3D\u662f\u4e00\u79cd\u7a00\u758f\u591a\u6a21\u6001\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u6539\u8fdbtoken\u8868\u793a\u8d28\u91cf\uff0c\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u8fd0\u884c\u66f4\u5feb\u3001\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u68c0\u6d4b\u5668\u5ffd\u89c6\u4e86token\u8868\u793a\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u524d\u666f\u8d28\u91cf\u6b21\u4f18\u548c\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u53d1\u73b0\u51e0\u4f55\u7ed3\u6784\u4fdd\u7559\u548c\u7c7b\u522b\u5206\u5e03\u662f\u63d0\u5347\u7a00\u758f\u68c0\u6d4b\u5668\u6027\u80fd\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86Sparse Selector (SS)\uff0c\u5176\u6838\u5fc3\u6a21\u5757\u662fRay-Aware Supervision (RAS)\u548cClass-Balanced Supervision\uff0c\u4ee5\u53caRay Positional Encoding (Ray PE)\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u7aef\u5230\u7aef\u7684\u7a00\u758f\u591a\u6a21\u6001\u68c0\u6d4b\u5668CrossRay3D\u4e2d\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCrossRay3D\u8fbe\u523072.4 mAP\u548c74.7 NDS\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb1.84\u500d\uff0c\u4e14\u5728\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "CrossRay3D\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8672.4 mAP\u548c74.7 NDS\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u5176\u4ed6\u9886\u5148\u65b9\u6cd5\u5feb1.84\u500d\uff0c\u4e14\u5728LiDAR\u6216\u76f8\u673a\u6570\u636e\u90e8\u5206\u6216\u5b8c\u5168\u7f3a\u5931\u7684\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16231", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16231", "abs": "https://arxiv.org/abs/2510.16231", "authors": ["Bihao Zhang", "Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly", "comment": null, "summary": "Intelligent robotic disassembly of end-of-life (EOL) products has been a\nlong-standing challenge in robotics. While machine learning techniques have\nshown promise, the lack of specialized hardware limits their application in\nreal-world scenarios. We introduce DeGrip, a customized gripper designed for\nthe disassembly of EOL computer desktops. DeGrip provides three degrees of\nfreedom (DOF), enabling arbitrary configurations within the disassembly\nenvironment when mounted on a robotic manipulator. It employs a cable-driven\ntransmission mechanism that reduces its overall size and enables operation in\nconfined spaces. The wrist is designed to decouple the actuation of wrist and\njaw joints. We also developed an EOL desktop disassembly environment in Isaac\nSim to evaluate the effectiveness of DeGrip. The tasks were designed to\ndemonstrate its ability to operate in confined spaces and disassemble\ncomponents in arbitrary configurations. The evaluation results confirm the\ncapability of DeGrip for EOL desktop disassembly.", "AI": {"tldr": "DeGrip\u662f\u4e00\u79cd\u5b9a\u5236\u5939\u722a\uff0c\u4e13\u4e3aEOL\u53f0\u5f0f\u673a\u62c6\u5378\u8bbe\u8ba1\uff0c\u5177\u6709\u4e09\u81ea\u7531\u5ea6\u548c\u7535\u7f06\u9a71\u52a8\u673a\u5236\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u6709\u6548\u3002", "motivation": "\u89e3\u51b3EOL\u4ea7\u54c1\u667a\u80fd\u673a\u5668\u4eba\u62c6\u5378\u7684\u957f\u671f\u6311\u6218\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u4e13\u7528\u786c\u4ef6\u9650\u5236\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86DeGrip\u5b9a\u5236\u5939\u722a\uff0c\u91c7\u7528\u7535\u7f06\u9a71\u52a8\u4f20\u8f93\u673a\u5236\u548c\u4e09\u81ea\u7531\u5ea6\u8bbe\u8ba1\uff0c\u5e76\u5728Isaac Sim\u4e2d\u6784\u5efa\u4e86EOL\u53f0\u5f0f\u673a\u62c6\u5378\u73af\u5883\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8bc1\u5b9eDeGrip\u5728EOL\u53f0\u5f0f\u673a\u62c6\u5378\u4e2d\u7684\u80fd\u529b\u3002", "conclusion": "DeGrip\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u62c6\u5378EOL\u53f0\u5f0f\u673a\uff0c\u5c55\u793a\u4e86\u5728\u53d7\u9650\u7a7a\u95f4\u548c\u4efb\u610f\u914d\u7f6e\u4e0b\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2510.16890", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16890", "abs": "https://arxiv.org/abs/2510.16890", "authors": ["Ji\u0159\u00ed Klepl", "Martin Kruli\u0161", "Maty\u00e1\u0161 Brabec"], "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Recent Advances in the Message Passing Interface (EuroMPI 2025),\n  and is available online at https://doi.org/10.1007/978-3-032-07194-1_3", "summary": "Message Passing Interface (MPI) has been a well-established technology in the\ndomain of distributed high-performance computing for several decades. However,\none of its greatest drawbacks is a rather ancient pure-C interface. It lacks\nmany useful features of modern languages (namely C++), like basic type-checking\nor support for generic code design. In this paper, we propose a novel\nabstraction for MPI, which we implemented as an extension of the C++ Noarr\nlibrary. It follows Noarr paradigms (first-class layout and traversal\nabstraction) and offers layout-agnostic design of MPI applications. We also\nimplemented a layout-agnostic distributed GEMM kernel as a case study to\ndemonstrate the usability and syntax of the proposed abstraction. We show that\nthe abstraction achieves performance comparable to the state-of-the-art MPI C++\nbindings while allowing for a more flexible design of distributed applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eC++ Noarr\u5e93\u7684MPI\u65b0\u62bd\u8c61\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMPI\u63a5\u53e3\u7f3a\u4e4f\u73b0\u4ee3\u8bed\u8a00\u7279\u6027\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0fGEMM\u6838\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMPI\u63a5\u53e3\u57fa\u4e8e\u7eafC\u8bed\u8a00\uff0c\u7f3a\u4e4f\u73b0\u4ee3\u8bed\u8a00\uff08\u5982C++\uff09\u7684\u7c7b\u578b\u68c0\u67e5\u548c\u6cdb\u578b\u8bbe\u8ba1\u652f\u6301\uff0c\u9650\u5236\u4e86\u5206\u5e03\u5f0f\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u5f00\u53d1\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eC++ Noarr\u5e93\u7684\u65b0\u62bd\u8c61\u65b9\u6cd5\uff0c\u9075\u5faaNoarr\u8303\u5f0f\uff08\u5e03\u5c40\u548c\u904d\u5386\u7684\u4e00\u6d41\u62bd\u8c61\uff09\uff0c\u5b9e\u73b0\u4e86\u5e03\u5c40\u65e0\u5173\u7684MPI\u5e94\u7528\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u5206\u5e03\u5f0fGEMM\u6838\u6848\u4f8b\u8bc1\u660e\uff0c\u8be5\u62bd\u8c61\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684MPI C++\u7ed1\u5b9a\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5206\u5e03\u5f0f\u5e94\u7528\u8bbe\u8ba1\u3002", "conclusion": "\u65b0\u62bd\u8c61\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86MPI\u5e94\u7528\u7684\u5f00\u53d1\u7075\u6d3b\u6027\u548c\u73b0\u4ee3\u8bed\u8a00\u7279\u6027\u652f\u6301\u3002"}}
{"id": "2510.17410", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17410", "abs": "https://arxiv.org/abs/2510.17410", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Pavel Savlukovich", "Evgeny Khorov"], "title": "Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?", "comment": null, "summary": "5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to\nsupport inter-vehicle communication. In contrast to 4G V2X which allows only\nbroadcast communication, 5G V2X enables groupcast and unicast communication.\nSuch types of communication are needed for new V2X scenarios: platooning,\nextended sensors, remote driving, etc. To improve the data transmission\nreliability and assist in the selection of the transmission parameters in these\nscenarios, 5G V2X introduces a feedback channel that allows receivers to send\nacknowledgments in response to data packets. However, some part of the overall\nresource shall be allocated for the feedback channel, which reduces the amount\nof channel resources available for data transmission. In this paper, we\nconsider a scenario with a platoon, which generates groupcast traffic, and\nsurrounding vehicles, which generate legacy broadcast traffic. Using extensive\nsimulations in NS-3, we analyze how the usage of the feedback channel\ninfluences the overall system capacity. Our results show that depending on the\nplatoon size, groupcast, and broadcast traffic intensities, and their quality\nof service requirements, the usage of the feedback channel can in some cases\nsignificantly increase the system capacity (up to 2x), while in other cases it\nalmost halves the system capacity. We explain the reasons for such effects and\ndiscuss how to adaptively select the feedback channel parameters.", "AI": {"tldr": "5G V2X\u7684\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u5f71\u54cd\u663e\u8457\uff0c\u53ef\u80fd\u63d0\u5347\u6216\u964d\u4f4e\u5bb9\u91cf\uff0c\u9700\u6839\u636e\u573a\u666f\u81ea\u9002\u5e94\u9009\u62e9\u53c2\u6570\u3002", "motivation": "5G V2X\u5f15\u5165\u4e86\u53cd\u9988\u4fe1\u9053\u4ee5\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u53ef\u9760\u6027\u5e76\u534f\u52a9\u9009\u62e9\u4f20\u8f93\u53c2\u6570\uff0c\u4f46\u53cd\u9988\u4fe1\u9053\u4f1a\u5360\u7528\u90e8\u5206\u8d44\u6e90\uff0c\u53ef\u80fd\u5f71\u54cd\u6570\u636e\u4f20\u4f20\u8f93\u7684\u6574\u4f53\u5bb9\u91cf\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u53cd\u9988\u4fe1\u9053\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u901a\u8fc7NS-3\u4e2d\u7684\u5e7f\u6cdb\u6a21\u62df\u5206\u6790\u4e86\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u8003\u8651\u4e86\u8f66\u961f\u751f\u6210\u7ec4\u64ad\u6d41\u91cf\u548c\u5468\u56f4\u8f66\u8f86\u751f\u6210\u5e7f\u64ad\u6d41\u91cf\u7684\u573a\u666f\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u53cd\u9988\u4fe1\u9053\u7684\u4f7f\u7528\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u53ef\u80fd\u63d0\u5347\u6216\u964d\u4f4e\u5bb9\u91cf\u3002\u4f5c\u8005\u89e3\u91ca\u4e86\u8fd9\u4e9b\u6548\u5e94\u7684\u539f\u56e0\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u53c2\u6570\u9009\u62e9\u7684\u5efa\u8bae\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u53cd\u9988\u4fe1\u9053\u7684\u4f7f\u7528\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u591a\u79cd\u56e0\u7d20\uff0c\u5982\u8f66\u961f\u89c4\u6a21\u3001\u7ec4\u64ad\u548c\u5e7f\u64ad\u6d41\u91cf\u5f3a\u5ea6\u53ca\u5176\u670d\u52a1\u8d28\u91cf\u8981\u6c42\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53cd\u9988\u4fe1\u9053\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u5bb9\u91cf\uff08\u6700\u9ad8\u53ef\u8fbe2\u500d\uff09\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5219\u53ef\u80fd\u51e0\u4e4e\u51cf\u534a\u3002\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u5982\u4f55\u81ea\u9002\u5e94\u9009\u62e9\u53cd\u9988\u4fe1\u9053\u53c2\u6570\u4ee5\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.16346", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2510.16346", "abs": "https://arxiv.org/abs/2510.16346", "authors": ["Timothy M. Chan", "Hsien-Chih Chang", "Jie Gao", "S\u00e1ndor Kisfaludi-Bak", "Hung Le", "Da Wei Zheng"], "title": "Truly Subquadratic Time Algorithms for Diameter and Related Problems in Graphs of Bounded VC-dimension", "comment": "FOCS 2025", "summary": "We give the first truly subquadratic time algorithm, with $O^*(n^{2-1/18})$\nrunning time, for computing the diameter of an $n$-vertex unit-disk graph,\nresolving a central open problem in the literature. Our result is obtained as\nan instance of a general framework, applicable to different graph families and\ndistance problems. Surprisingly, our framework completely bypasses sublinear\nseparators (or $r$-divisions) which were used in all previous algorithms.\nInstead, we use low-diameter decompositions in their most elementary form. We\nalso exploit bounded VC-dimension of set systems associated with the input\ngraph, as well as new ideas on geometric data structures. Among the numerous\napplications of the general framework, we obtain:\n  1. An $\\tilde{O}(mn^{1-1/(2d)})$ time algorithm for computing the diameter of\n$m$-edge sparse unweighted graphs with constant VC-dimension $d$. The\npreviously known algorithms by Ducoffe, Habib, and Viennot [SODA 2019] and\nDuraj, Konieczny, and Pot\\c{e}pa [ESA 2024] are truly subquadratic only when\nthe diameter is a small polynomial. Our result thus generalizes truly\nsubquadratic time algorithms known for planar and minor-free graphs (in fact,\nit slightly improves the previous time bound for minor-free graphs).\n  2. An $\\tilde{O}(n^{2-1/12})$ time algorithm for computing the diameter of\nintersection graphs of axis-aligned squares with arbitrary size. The best-known\nalgorithm by Duraj, Konieczny, and Pot\\c{e}pa [ESA 2024] only works for unit\nsquares and is only truly subquadratic in the low-diameter regime.\n  3. The first algorithms with truly subquadratic complexity for other\ndistance-related problems, including all-vertex eccentricities, Wiener index,\nand exact distance oracles. (... truncated to meet the arXiv abstract\nrequirement.)", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5355\u4f4d\u5706\u76d8\u56fe\u76f4\u5f84\u7684\u4e9a\u4e8c\u6b21\u65f6\u95f4\u8ba1\u7b97\uff0c\u5e76\u6269\u5c55\u5230\u5176\u4ed6\u56fe\u65cf\u548c\u8ddd\u79bb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5355\u4f4d\u5706\u76d8\u56fe\u76f4\u5f84\u8ba1\u7b97\u4e2d\u7684\u4e9a\u4e8c\u6b21\u65f6\u95f4\u7b97\u6cd5\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u9002\u7528\u4e8e\u4e0d\u540c\u56fe\u65cf\u548c\u8ddd\u79bb\u95ee\u9898\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4f4e\u76f4\u5f84\u5206\u89e3\u7684\u57fa\u672c\u5f62\u5f0f\uff0c\u7ed3\u5408\u6709\u9650VC\u7ef4\u5ea6\u7684\u96c6\u5408\u7cfb\u7edf\u548c\u65b0\u7684\u51e0\u4f55\u6570\u636e\u7ed3\u6784\u601d\u60f3\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002", "result": "1. \u5bf9\u4e8e\u5177\u6709\u5e38\u6570VC\u7ef4\u5ea6\u7684\u7a00\u758f\u65e0\u6743\u56fe\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a$\tilde{O}(mn^{1-1/(2d)})$\u65f6\u95f4\u7b97\u6cd5\uff1b2. \u5bf9\u4e8e\u8f74\u5bf9\u9f50\u6b63\u65b9\u5f62\u4ea4\u96c6\u56fe\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a$\tilde{O}(n^{2-1/12})$\u65f6\u95f4\u7b97\u6cd5\uff1b3. \u9996\u6b21\u4e3a\u5176\u4ed6\u8ddd\u79bb\u76f8\u5173\u95ee\u9898\uff08\u5982\u6240\u6709\u9876\u70b9\u79bb\u5fc3\u7387\u3001Wiener\u6307\u6570\u548c\u7cbe\u786e\u8ddd\u79bb\u9884\u8a00\u673a\uff09\u63d0\u4f9b\u4e86\u4e9a\u4e8c\u6b21\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u79cd\u56fe\u65cf\u548c\u8ddd\u79bb\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e9a\u4e8c\u6b21\u65f6\u95f4\u7b97\u6cd5\uff0c\u7a81\u7834\u4e86\u4ee5\u5f80\u4f9d\u8d56\u5b50\u7ebf\u6027\u5206\u9694\u5668\u7684\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u4e2a\u5177\u4f53\u95ee\u9898\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.16357", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis.", "AI": {"tldr": "MLCPD\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u8bed\u8a00\u65e0\u5173\u7684\u6570\u636e\u96c6\uff0c\u7edf\u4e00\u4e86\u5341\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u8bed\u6cd5\u548c\u7ed3\u6784\u8868\u793a\uff0c\u4e3a\u8de8\u8bed\u8a00\u7814\u7a76\u548c\u7a0b\u5e8f\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u6599\u5e93\u4e3b\u8981\u96c6\u4e2d\u5728\u6807\u8bb0\u7ea7\u522b\u7684\u4ee3\u7801\u6216\u5b64\u7acb\u7684\u89e3\u6790\u5668\u4e0a\uff0c\u7f3a\u4e4f\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u7edf\u4e00\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u6a21\u5f0f\uff0c\u7edf\u4e00\u4e86\u5341\u79cd\u4e3b\u8981\u7f16\u7a0b\u8bed\u8a00\u7684\u8bed\u6cd5\u548c\u7ed3\u6784\u8868\u793a\uff0c\u5305\u542b\u8d85\u8fc7\u4e03\u767e\u4e07\u4e2a\u89e3\u6790\u540e\u7684\u6e90\u6587\u4ef6\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u7684\u5f3a\u7ed3\u6784\u89c4\u5f8b\u6027\uff0c\u8868\u660e\u4e0d\u540c\u8bed\u8a00\u7684\u8bed\u6cd5\u56fe\u53ef\u4ee5\u5728\u5171\u4eab\u6a21\u5f0f\u4e0b\u5bf9\u9f50\u3002", "conclusion": "MLCPD\u4e3a\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u548c\u7a0b\u5e8f\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2510.15876", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.15876", "abs": "https://arxiv.org/abs/2510.15876", "authors": ["Abhinav Dayal", "Cliff Woolley", "Benjamin Watson", "David Luebke"], "title": "Adaptive Frameless Rendering", "comment": null, "summary": "We propose an adaptive form of frameless rendering with the potential to\ndramatically increase rendering speed over conventional interactive rendering\napproaches. Without the rigid sampling patterns of framed renderers, sampling\nand reconstruction can adapt with very fine granularity to spatio-temporal\ncolor change. A sampler uses closed-loop feedback to guide sampling toward\nedges or motion in the image. Temporally deep buffers store all the samples\ncreated over a short time interval for use in reconstruction and as sampler\nfeedback. GPU-based reconstruction responds both to sampling density and\nspace-time color gradients. Where the displayed scene is static, spatial color\nchange dominates and older samples are given significant weight in\nreconstruction, resulting in sharper and eventually antialiased images. Where\nthe scene is dynamic, more recent samples are emphasized, resulting in less\nsharp but more up-to-date images. We also use sample reprojection to improve\nreconstruction and guide sampling toward occlusion edges, undersampled regions,\nand specular highlights. In simulation our frameless renderer requires an order\nof magnitude fewer samples than traditional rendering of similar visual quality\n(as measured by RMS error), while introducing overhead amounting to 15% of\ncomputation time.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u65e0\u5e27\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u548c\u52a8\u6001\u91cd\u5efa\u663e\u8457\u63d0\u5347\u6e32\u67d3\u6548\u7387\uff0c\u51cf\u5c11\u6837\u672c\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u4ea4\u4e92\u5f0f\u6e32\u67d3\u65b9\u6cd5\u56e0\u56fa\u5b9a\u7684\u91c7\u6837\u6a21\u5f0f\u6548\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u7075\u6d3b\u9002\u5e94\u65f6\u7a7a\u989c\u8272\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65e0\u5e27\u6e32\u67d3\u65b9\u6cd5\uff0c\u5229\u7528\u95ed\u73af\u53cd\u9988\u6307\u5bfc\u91c7\u6837\uff0c\u7ed3\u5408GPU\u91cd\u5efa\u6280\u672f\uff0c\u6839\u636e\u91c7\u6837\u5bc6\u5ea6\u548c\u65f6\u7a7a\u989c\u8272\u68af\u5ea6\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u8be5\u65e0\u5e27\u6e32\u67d3\u5668\u6240\u9700\u6837\u672c\u6570\u91cf\u6bd4\u4f20\u7edf\u6e32\u67d3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a15%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u91cd\u5efa\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u6e32\u67d3\u76f8\u4f3c\u7684\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528CCTV\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u548c\u603b\u7ed3\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\u7684\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u624b\u52a8\u68c0\u67e5\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u73b0\u6709\u81ea\u52a8\u7cfb\u7edf\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7f3a\u9677\u7c7b\u578b\u6216\u63d0\u4f9b\u975e\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u7ba1\u9053\uff0c\u5229\u7528\u8857\u9053CCTV\u6d41\u8fdb\u884c\u591a\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u4f7f\u7528YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5e76\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u9012\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u573a\u666f\u611f\u77e5\u603b\u7ed3\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u6355\u83b7\u7684CCTV\u7247\u6bb5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u7f3a\u9677\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u603b\u7ed3\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u5c06\u7cfb\u7edf\u6269\u5c55\u5230\u57ce\u5e02\u8303\u56f4\u90e8\u7f72\u7684\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2510.16240", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16240", "abs": "https://arxiv.org/abs/2510.16240", "authors": ["Lukas Zbinden", "Nigel Nelson", "Juo-Tung Chen", "Xinhao Chen", "Ji Woong", "Kim", "Mahdi Azizian", "Axel Krieger", "Sean Huver"], "title": "Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning", "comment": null, "summary": "The rise of surgical robots and vision-language-action models has accelerated\nthe development of autonomous surgical policies and efficient assessment\nstrategies. However, evaluating these policies directly on physical robotic\nplatforms such as the da Vinci Research Kit (dVRK) remains hindered by high\ncosts, time demands, reproducibility challenges, and variability in execution.\nWorld foundation models (WFM) for physical AI offer a transformative approach\nto simulate complex real-world surgical tasks, such as soft tissue deformation,\nwith high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune\nof the Cosmos WFM, which, together with a trained video classifier, enables\nfully automated online evaluation and benchmarking of surgical policies. We\nevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop\nsuture pad tasks, the automated pipeline achieves strong correlation between\nonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si\nplatform, as well as good agreement between human labelers and the V-JEPA\n2-derived video classifier. Additionally, preliminary experiments with ex-vivo\nporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising\nalignment with real-world evaluations, highlighting the platform's potential\nfor more complex surgical procedures.", "AI": {"tldr": "Cosmos-Surg-dVRK\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u624b\u672f\u7b56\u7565\u5728\u7269\u7406\u5e73\u53f0\u4e0a\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u624b\u672f\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u76f4\u63a5\u8bc4\u4f30\u624b\u672f\u7b56\u7565\u5728\u7269\u7406\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u96be\u4ee5\u590d\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u4fdd\u771f\u5ea6\u7684\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86Cosmos-Surg-dVRK\uff0c\u4e00\u4e2a\u57fa\u4e8eCosmos WFM\u7684\u624b\u672f\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86\u89c6\u9891\u5206\u7c7b\u5668\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u5728\u684c\u9762\u7f1d\u5408\u57ab\u4efb\u52a1\u4e2d\uff0c\u81ea\u52a8\u5316\u6d41\u7a0b\u5728Cosmos-Surg-dVRK\u4e2d\u7684\u5728\u7ebf\u63a8\u6f14\u4e0e\u771f\u5b9edVRK\u5e73\u53f0\u4e0a\u7684\u7b56\u7565\u7ed3\u679c\u5177\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u4e14\u89c6\u9891\u5206\u7c7b\u5668\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u7684\u4e00\u81f4\u6027\u826f\u597d\u3002\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u79bb\u4f53\u732a\u80c6\u56ca\u5207\u9664\u672f\u4efb\u52a1\u5728Cosmos-Surg-dVRK\u4e2d\u4e0e\u73b0\u5b9e\u8bc4\u4f30\u6709\u826f\u597d\u5bf9\u9f50\u3002", "conclusion": "Cosmos-Surg-dVRK\u7ed3\u5408\u8bad\u7ec3\u7684\u89c6\u9891\u5206\u7c7b\u5668\uff0c\u4e3a\u624b\u672f\u7b56\u7565\u63d0\u4f9b\u4e86\u5168\u81ea\u52a8\u5728\u7ebf\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u624b\u672f\u7a0b\u5e8f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16896", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16896", "abs": "https://arxiv.org/abs/2510.16896", "authors": ["Yiming Hu"], "title": "FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems", "comment": null, "summary": "Two-Phase Triple Modular Redundancy TMR divides redundancy operations into\ntwo stages, omitting part of the computation during fault-free operation to\nreduce energy consumption. However, it becomes ineffective under permanent\nfaults, limiting its reliability in critical systems. To address this,\nReactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty\ncores, tolerating both transient and permanent faults. Yet, its reliance on\nadditional hardware increases system complexity and reduces fault tolerance\nwhen multiple cores or auxiliary modules fail. This paper proposes an\nintegrated fault-tolerant architecture for interconnected multicore systems. By\nconstructing a stability metric to identify reliable machines and performing\nperiodic diagnostics, the method enables permanent fault isolation and adaptive\ntask scheduling without extra hardware. Experimental results show that it\nreduces task workload by approximately 30% compared to baseline TMR and\nachieves superior fault coverage and isolation accuracy, significantly\nimproving both reliability and energy efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u786c\u4ef6\u7684\u96c6\u6210\u5bb9\u9519\u67b6\u6784\uff0c\u901a\u8fc7\u7a33\u5b9a\u6027\u6307\u6807\u548c\u5468\u671f\u6027\u8bca\u65ad\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709Two-Phase TMR\u5728\u6c38\u4e45\u6545\u969c\u4e0b\u5931\u6548\u4ee5\u53caR-TMR\u56e0\u989d\u5916\u786c\u4ef6\u4f9d\u8d56\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u6027\u548c\u5bb9\u9519\u6027\u964d\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u7a33\u5b9a\u6027\u6307\u6807\u8bc6\u522b\u53ef\u9760\u673a\u5668\uff0c\u5e76\u8fdb\u884c\u5468\u671f\u6027\u8bca\u65ad\uff0c\u5b9e\u73b0\u4e86\u6c38\u4e45\u6545\u969c\u9694\u79bb\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u51c6TMR\u51cf\u5c11\u4e86\u7ea630%\u7684\u4efb\u52a1\u8d1f\u8f7d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6545\u969c\u8986\u76d6\u7387\u548c\u9694\u79bb\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5f0f\u5bb9\u9519\u67b6\u6784\uff0c\u901a\u8fc7\u6784\u5efa\u7a33\u5b9a\u6027\u6307\u6807\u548c\u5468\u671f\u6027\u8bca\u65ad\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u786c\u4ef6\u7684\u6c38\u4e45\u6545\u969c\u9694\u79bb\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u80fd\u6548\u3002"}}
{"id": "2510.17445", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17445", "abs": "https://arxiv.org/abs/2510.17445", "authors": ["Mohd Saif Ali Khan", "Karthik RM", "Samar Agnihotri"], "title": "Adaptive Local Combining with Decentralized Decoding for Distributed Massive MIMO", "comment": null, "summary": "A major bottleneck in uplink distributed massive multiple-input\nmultiple-output networks is the sub-optimal performance of local combining\nschemes, coupled with high fronthaul load and computational cost inherent in\ncentralized large scale fading decoding (LSFD) architectures. This paper\nintroduces a decentralized decoding architecture that fundamentally breaks from\nthe conventional LSFD, by allowing each AP calculates interference-suppressing\nlocal weights independently and applies them to its data estimates before\ntransmission. Furthermore, two generalized local zero-forcing (ZF) framework,\ngeneralized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF\n(G-PWPFZF), are introduced, where each access point (AP) adaptively and\nindependently determines its combining strategy through a local sum spectral\nefficiency optimization that classifies user equipments (UEs) as strong or weak\nusing only local information, eliminating the fixed thresholds used in PFZF and\nPWPFZF. To further enhance scalability, pilot-dependent combining vectors\ninstead of user-dependent ones are introduced and are shared among users with\nthe same pilot. The corresponding closed-form spectral efficiency expressions\nare derived. Numerical results show that the proposed generalized schemes\nconsistently outperform fixed-threshold counterparts, while the introduction of\nlocal weights yields lower overhead and computation costs with minimal\nperformance penalty compared to them.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u89e3\u7801\u67b6\u6784\u548c\u4e24\u79cd\u5e7f\u4e49\u5c40\u90e8ZF\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7f51\u7edc\u6027\u80fd\uff0c\u964d\u4f4e\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684LSFD\u67b6\u6784\u5b58\u5728\u5f00\u9500\u9ad8\u3001\u8ba1\u7b97\u6210\u672c\u5927\u4e14\u6027\u80fd\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u7801\u65b9\u6848\u6765\u63d0\u5347\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7f51\u7edc\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u89e3\u7801\u67b6\u6784\uff0c\u5141\u8bb8\u6bcf\u4e2aAP\u72ec\u7acb\u8ba1\u7b97\u5e72\u6270\u6291\u5236\u7684\u5c40\u90e8\u6743\u91cd\uff0c\u5e76\u5e94\u7528\u4e8e\u6570\u636e\u4f30\u8ba1\u3002\u63d0\u51fa\u4e86G-PFZF\u548cG-PWPFZF\u4e24\u79cd\u6846\u67b6\uff0cAP\u901a\u8fc7\u5c40\u90e8\u4fe1\u606f\u548c\u9891\u8c31\u6548\u7387\u4f18\u5316\u81ea\u9002\u5e94\u786e\u5b9a\u7ec4\u5408\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5bfc\u9891\u7684\u7ec4\u5408\u5411\u91cf\u4ee5\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5e7f\u4e49\u65b9\u6848\u4f18\u4e8e\u56fa\u5b9a\u9608\u503c\u65b9\u6848\uff0c\u5c40\u90e8\u6743\u91cd\u7684\u5f15\u5165\u964d\u4f4e\u4e86\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u7801\u67b6\u6784\u548c\u4e24\u79cd\u5e7f\u4e49\u5c40\u90e8\u96f6\u5e72\u6270\uff08ZF\uff09\u6846\u67b6\uff08G-PFZF\u548cG-PWPFZF\uff09\u663e\u8457\u63d0\u5347\u4e86\u4e0a\u884c\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u56fa\u5b9a\u9608\u503c\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16351", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16351", "abs": "https://arxiv.org/abs/2510.16351", "authors": ["Amir Azarmehr", "Soheil Behnezhad", "Mohammad Roghani", "Aviad Rubinstein"], "title": "Tight Pair Query Lower Bounds for Matching and Earth Mover's Distance", "comment": null, "summary": "How many adjacency matrix queries (also known as pair queries) are required\nto estimate the size of a maximum matching in an $n$-vertex graph $G$? We study\nthis fundamental question in this paper.\n  On the upper bound side, an algorithm of Bhattacharya, Kiss, and Saranurak\n[FOCS'23] gives an estimate that is within $\\epsilon n$ of the right bound with\n$n^{2-\\Omega_\\epsilon(1)}$ queries, which is subquadratic in $n$ (and thus\nsublinear in the matrix size) for any fixed $\\epsilon > 0$. On the lower bound\nside, while there has been a lot of progress in the adjacency list model, no\nnon-trivial lower bound has been established for algorithms with adjacency\nmatrix query access. In particular, the only known lower bound is a folklore\nbound of $\\Omega(n)$, leaving a huge gap.\n  In this paper, we present the first superlinear in $n$ lower bound for this\nproblem. In fact, we close the gap mentioned above entirely by showing that the\nalgorithm of [BKS'23] is optimal. Formally, we prove that for any fixed $\\delta\n> 0$, there is a fixed $\\epsilon > 0$ such that an estimate that is within\n$\\epsilon n$ of the true bound requires $\\Omega(n^{2-\\delta})$ adjacency matrix\nqueries.\n  Our lower bound also has strong implications for estimating the earth mover's\ndistance between distributions. For this problem, Beretta and Rubinstein\n[STOC'24] gave an $n^{2-\\Omega_\\epsilon(1)}$ time algorithm that obtains an\nadditive $\\epsilon$-approximation and works for any distance function. Whether\nthis can be improved generally, or even for metric spaces, had remained open.\nOur lower bound rules out the possibility of any improvements over this bound,\neven under the strong assumption that the underlying distances are in a (1,\n2)-metric.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86\u90bb\u63a5\u77e9\u9635\u67e5\u8be2\u5728\u4f30\u8ba1\u56fe\u6700\u5927\u5339\u914d\u5927\u5c0f\u65f6\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86[BKS'23]\u7b97\u6cd5\u7684\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76\u90bb\u63a5\u77e9\u9635\u67e5\u8be2\u5728\u4f30\u8ba1\u56fe\u7684\u6700\u5927\u5339\u914d\u5927\u5c0f\u65f6\u6240\u9700\u7684\u67e5\u8be2\u6b21\u6570\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86[BKS'23]\u7b97\u6cd5\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u662f\u6700\u4f18\u7684\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u4f55\u56fa\u5b9a\u7684\u03b4>0\uff0c\u5b58\u5728\u56fa\u5b9a\u7684\u03b5>0\uff0c\u4f7f\u5f97\u4f30\u8ba1\u503c\u5728\u771f\u5b9e\u503c\u7684\u03b5n\u8303\u56f4\u5185\u9700\u8981\u03a9(n^{2\u2212\u03b4})\u6b21\u90bb\u63a5\u77e9\u9635\u67e5\u8be2\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u4f30\u8ba1\u56fe\u7684\u6700\u5927\u5339\u914d\u5927\u5c0f\u65f6\uff0c\u4f7f\u7528\u90bb\u63a5\u77e9\u9635\u67e5\u8be2\u7684\u7b97\u6cd5\u9700\u8981\u03a9(n^{2\u2212\u03b4})\u6b21\u67e5\u8be2\uff0c\u8fd9\u4e00\u7ed3\u679c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility.", "AI": {"tldr": "SemOpt\u5229\u7528\u9759\u6001\u5206\u6790\u548cLLMs\u63d0\u5347\u4ee3\u7801\u4f18\u5316\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u56e0\u65e0\u6cd5\u8bc6\u522b\u8bed\u4e49\u7b49\u6548\u4f46\u8bed\u6cd5\u4e0d\u540c\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5bfc\u81f4\u4f18\u5316\u6548\u679c\u4e0d\u4f73\u3002SemOpt\u65e8\u5728\u901a\u8fc7\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u548cLLMs\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SemOpt\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7b56\u7565\u5e93\u6784\u5efa\u5668\u3001\u89c4\u5219\u751f\u6210\u5668\u548c\u4f18\u5316\u5668\uff0c\u5747\u4f9d\u8d56LLMs\u5b9e\u73b0\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u5de5\u4f5c\uff0c\u901a\u8fc7\u9759\u6001\u5206\u6790\u7cbe\u786e\u8bc6\u522b\u53ef\u4f18\u5316\u4ee3\u7801\u6bb5\uff0c\u5e76\u751f\u6210\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u5305\u542b151\u4e2a\u4f18\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemOpt\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u4f18\u5316\u6b21\u6570\u63d0\u9ad8\u4e861.38\u81f328\u500d\u3002\u5728\u5927\u578bC/C++\u9879\u76ee\u4e2d\uff0c\u6027\u80fd\u6307\u6807\u63d0\u5347\u4e865.04%\u81f3218.07%\u3002", "conclusion": "SemOpt\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u548cLLMs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u4f18\u5316\u7684\u6210\u529f\u7387\u548c\u6027\u80fd\u6539\u8fdb\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u8df5\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.15877", "categories": ["cs.GR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15877", "abs": "https://arxiv.org/abs/2510.15877", "authors": ["Thomas Lechner", "Ben Watson", "Uri Wilenski", "Seth Tisue", "Martin Felsen", "Andy Moddrell", "Pin Ren", "Craig Brozefsky"], "title": "Procedural modeling of urban land use", "comment": null, "summary": "Cities are important elements of content in digital productions, but their\ncomplexity and size make them very challenging to model. Few tools exist that\ncan help artists with this work, even as rapid improvements in graphics\nhardware create demand for richer content without matching increases in\nproduction cost. We propose a method for procedurally generating realistic\npatterns of land use in cities, automating placement of buildings and roads for\nartists.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a0b\u5e8f\u5316\u751f\u6210\u57ce\u5e02\u571f\u5730\u5229\u7528\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u5efa\u7b51\u7269\u548c\u9053\u8def\u7684\u5e03\u5c40\uff0c\u4ee5\u5e94\u5bf9\u56fe\u5f62\u786c\u4ef6\u5feb\u901f\u53d1\u5c55\u5bf9\u4e30\u5bcc\u5185\u5bb9\u7684\u9700\u6c42\uff0c\u540c\u65f6\u51cf\u8f7b\u827a\u672f\u5bb6\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u57ce\u5e02\u5728\u6570\u5b57\u4f5c\u54c1\u4e2d\u7684\u91cd\u8981\u6027\u4e0e\u5176\u5efa\u6a21\u7684\u590d\u6742\u6027\u548c\u6311\u6218\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u5de5\u5177\u5e2e\u52a9\u827a\u672f\u5bb6\u5e94\u5bf9\u56fe\u5f62\u786c\u4ef6\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7a0b\u5e8f\u5316\u751f\u6210\u6280\u672f\uff0c\u81ea\u52a8\u521b\u5efa\u57ce\u5e02\u4e2d\u571f\u5730\u5229\u7528\u7684\u73b0\u5b9e\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5efa\u7b51\u7269\u548c\u9053\u8def\u5e03\u5c40\u7684\u65b9\u6cd5\uff0c\u51cf\u8f7b\u4e86\u827a\u672f\u5bb6\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u5316\u751f\u6210\u57ce\u5e02\u4e2d\u5efa\u7b51\u7269\u548c\u9053\u8def\u7684\u5e03\u5c40\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u56fe\u5f62\u786c\u4ef6\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u5bf9\u4e30\u5bcc\u5185\u5bb9\u7684\u9700\u6c42\u3002"}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "IAD-GPT \u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548c\u56fe\u50cf\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u591a\u8f6e\u4eba\u673a\u5bf9\u8bdd\u548c\u8be6\u7ec6\u63cf\u8ff0\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u6fc0\u53d1\u5927\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7ed3\u5408\u4e30\u5bcc\u7684\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u4fe1\u606f\uff0c\u91c7\u7528\u5f02\u5e38\u63d0\u793a\u751f\u6210\u5668\uff08APG\uff09\u751f\u6210\u8be6\u7ec6\u5f02\u5e38\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u5668\u548c\u591a\u63a9\u7801\u878d\u5408\u6a21\u5757\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u3002", "result": "\u5728 MVTec-AD \u548c VisA \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IAD-GPT \u5728\u81ea\u76d1\u7763\u548c\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2510.16263", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16263", "abs": "https://arxiv.org/abs/2510.16263", "authors": ["Jierui Peng", "Yanyan Zhang", "Yicheng Duan", "Tuo Liang", "Vipin Chaudhary", "Yu Yin"], "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?", "comment": "Homepage: https://vulab-ai.github.io/NEBULA-Alpha/", "summary": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the\ncoarse, end-task success metric that fails to provide precise skill diagnosis\nor measure robustness to real-world perturbations. This challenge is\nexacerbated by a fragmented data landscape that impedes reproducible research\nand the development of generalist models. To address these limitations, we\nintroduce \\textbf{NEBULA}, a unified ecosystem for single-arm manipulation that\nenables diagnostic and reproducible evaluation. NEBULA features a novel\ndual-axis evaluation protocol that combines fine-grained \\textit{capability\ntests} for precise skill diagnosis with systematic \\textit{stress tests} that\nmeasure robustness. A standardized API and a large-scale, aggregated dataset\nare provided to reduce fragmentation and support cross-dataset training and\nfair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle\nwith key capabilities such as spatial reasoning and dynamic adaptation, which\nare consistently obscured by conventional end-task success metrics. By\nmeasuring both what an agent can do and when it does so reliably, NEBULA\nprovides a practical foundation for robust, general-purpose embodied agents.", "AI": {"tldr": "NEBULA\u662f\u4e00\u4e2a\u7edf\u4e00\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u8f74\u8bc4\u4f30\u534f\u8bae\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86VLA\u4ee3\u7406\u8bc4\u4f30\u4e2d\u7684\u7c97\u7c92\u5ea6\u6307\u6807\u548c\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6307\u6807\u63a9\u76d6\u7684\u5173\u952e\u80fd\u529b\u7f3a\u9677\u3002", "motivation": "\u89e3\u51b3VLA\u4ee3\u7406\u8bc4\u4f30\u4e2d\u7c97\u7c92\u5ea6\u7ec8\u7aef\u4efb\u52a1\u6210\u529f\u6307\u6807\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u6280\u80fd\u8bca\u65ad\u6216\u6d4b\u91cf\u5bf9\u73b0\u5b9e\u4e16\u754c\u6270\u52a8\u7684\u9c81\u68d2\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u6570\u636e\u788e\u7247\u5316\u963b\u788d\u53ef\u91cd\u590d\u7814\u7a76\u548c\u901a\u7528\u6a21\u578b\u5f00\u53d1\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165NEBULA\u7edf\u4e00\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u542b\u53cc\u8f74\u8bc4\u4f30\u534f\u8bae\uff08\u7ec6\u7c92\u5ea6\u80fd\u529b\u6d4b\u8bd5\u548c\u7cfb\u7edf\u538b\u529b\u6d4b\u8bd5\uff09\u3001\u6807\u51c6\u5316API\u548c\u5927\u89c4\u6a21\u805a\u5408\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0\u9876\u7ea7VLA\u4ee3\u7406\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u52a8\u6001\u9002\u5e94\u7b49\u5173\u952e\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u7f3a\u9677\u88ab\u4f20\u7edf\u7ec8\u7aef\u4efb\u52a1\u6210\u529f\u6307\u6807\u6240\u63a9\u76d6\u3002", "conclusion": "NEBULA\u4e3a\u7a33\u5065\u3001\u901a\u7528\u7684\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u901a\u8fc7\u6d4b\u91cf\u4ee3\u7406\u7684\u80fd\u529b\u53ca\u5176\u53ef\u9760\u6027\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u7ec8\u7aef\u4efb\u52a1\u6210\u529f\u6307\u6807\u6240\u63a9\u76d6\u7684\u5173\u952e\u80fd\u529b\u7f3a\u9677\u3002"}}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Maty\u00e1\u0161 Brabec", "Ji\u0159\u00ed Klepl", "Michal T\u00f6pfer", "Martin Kruli\u0161"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u4f18\u5316CUDA\u4ee3\u7801\uff0c\u4f46\u9700\u4e13\u5bb6\u6307\u5bfc\u624d\u80fd\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u6700\u65b0\u63a8\u7406\u6a21\u578b\u5728\u751f\u6210\u4f18\u5316CUDA\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u786e\u5b9aLLMs\u80fd\u72ec\u7acb\u5b8c\u6210\u7684\u4ee3\u7801\u4f18\u5316\u7c7b\u578b\u53ca\u901a\u8fc7\u63d0\u793a\u6539\u8fdb\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\uff08\u6b63\u786e\u6027\u548c\u52a0\u901f\u6bd4\uff09\u548c\u624b\u52a8\u4ee3\u7801\u5ba1\u67e5\uff0c\u8bc4\u4f30LLMs\u751f\u6210\u7684\u4f18\u5316CUDA\u4ee3\u7801\uff0c\u5e76\u5c1d\u8bd5\u4ea4\u4e92\u5f0f\u4fee\u6b63\u65b9\u6cd5\u3002", "result": "LLMs\u662f\u719f\u7ec3\u7684\u7f16\u7801\u8005\uff0c\u4f46\u5728\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u4e0a\u9700\u8981\u8be6\u7ec6\u63d0\u793a\u548c\u6307\u5bfc\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u4e13\u5bb6\u6307\u5bfc\u624d\u80fd\u8fbe\u5230\u5e76\u884c\u8ba1\u7b97\u4e13\u5bb6\u63d0\u4f9b\u7684\u4f18\u5316\u6c34\u5e73\u3002"}}
{"id": "2510.17647", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17647", "abs": "https://arxiv.org/abs/2510.17647", "authors": ["P. Brach del Prever", "P. Testolina", "A. Masihi", "S. Petrushkevich", "M. Polese", "T. Melodia", "J. M. Jornet"], "title": "Pointing-Error-Induced Fading in an Open-Loop THz Uplink with Hardware Impairments", "comment": "To be published in: Proceedings of 2nd IEEE International Workshop on\n  Terahertz Communications, Sensing, and Security at IEEE Military\n  Communications Conference (MILCOM) 2025. 6 pages, 6 figures", "summary": "We analyze the open-loop mechanical tracking performance of a sub-Terahertz\n(sub-THz) and Terahertz (THz) uplink communication system. These high-frequency\nbands enable multi-gigabit links through large bandwidths and narrow beams, but\nrequire precise pointing to overcome spreading loss. A tracking system can be\nused to orient horn antennas toward mobile targets. We develop a mathematical\nmodel that captures the mechanical dynamics of a real tracking system, which\nincludes motion latency and acceleration and velocity limits, to quantify\npointing errors during satellite passes and integrate these effects into the\nlink budget. We evaluate the trade-offs between beam directionality and\npointing tolerance across different Low Earth Orbit (LEO) satellite\ntrajectories and control strategies. The results link the hardware limitations\nto the communications performance, providing design guidelines for\nhigh-frequency Non-Terrestrial Network (NTN) uplink under practical mechanical\nconstraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e9a\u592a\u8d6b\u5179\u548c\u592a\u8d6b\u5179\u4e0a\u884c\u94fe\u8def\u901a\u4fe1\u7cfb\u7edf\u7684\u673a\u68b0\u8ddf\u8e2a\u6027\u80fd\uff0c\u5f00\u53d1\u4e86\u6570\u5b66\u6a21\u578b\u91cf\u5316\u6307\u5411\u8bef\u5dee\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u9891\u975e\u5730\u9762\u7f51\u7edc\u4e0a\u884c\u94fe\u8def\u7684\u8bbe\u8ba1\u6307\u5357\u3002", "motivation": "\u7814\u7a76\u4e9a\u592a\u8d6b\u5179\uff08sub-THz\uff09\u548c\u592a\u8d6b\u5179\uff08THz\uff09\u4e0a\u884c\u94fe\u8def\u901a\u4fe1\u7cfb\u7edf\u7684\u5f00\u73af\u673a\u68b0\u8ddf\u8e2a\u6027\u80fd\uff0c\u8fd9\u4e9b\u9ad8\u9891\u6bb5\u901a\u8fc7\u5927\u5e26\u5bbd\u548c\u7a84\u6ce2\u675f\u5b9e\u73b0\u591a\u5343\u5146\u4f4d\u94fe\u8def\uff0c\u4f46\u9700\u8981\u7cbe\u786e\u6307\u5411\u4ee5\u514b\u670d\u6269\u5c55\u635f\u8017\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u6355\u6349\u771f\u5b9e\u8ddf\u8e2a\u7cfb\u7edf\u7684\u673a\u68b0\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5305\u62ec\u8fd0\u52a8\u5ef6\u8fdf\u3001\u52a0\u901f\u5ea6\u548c\u901f\u5ea6\u9650\u5236\uff0c\u4ee5\u91cf\u5316\u536b\u661f\u901a\u8fc7\u65f6\u7684\u6307\u5411\u8bef\u5dee\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6548\u5e94\u7eb3\u5165\u94fe\u8def\u9884\u7b97\u3002", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u8f68\u8ff9\u548c\u63a7\u5236\u7b56\u7565\u4e0b\u6ce2\u675f\u65b9\u5411\u6027\u4e0e\u6307\u5411\u5bb9\u9650\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u9ad8\u9891\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u4e0a\u884c\u94fe\u8def\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5357\uff0c\u5c06\u786c\u4ef6\u9650\u5236\u4e0e\u901a\u4fe1\u6027\u80fd\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2510.16454", "categories": ["cs.DS", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.16454", "abs": "https://arxiv.org/abs/2510.16454", "authors": ["Gregory Kucherov", "Yakov Nekrich"], "title": "Online computation of normalized substring complexity", "comment": "15 pages, 1 figure", "summary": "The normalized substring complexity $\\delta$ of a string is defined as\n$\\max_k \\{c[k]/k\\}$, where $c[k]$ is the number of \\textit{distinct} substrings\nof length $k$. This simply defined measure has recently attracted attention due\nto its established relationship to popular string compression algorithms. We\nconsider the problem of computing $\\delta$ online, when the string is provided\nfrom a stream. We present two algorithms solving the problem: one working in\n$O(\\log n)$ amortized time per character, and the other in $O(\\log^3 n)$\nworst-case time per character. To our knowledge, this is the first polylog-time\nonline solution to this problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u7ebf\u8ba1\u7b97\u5f52\u4e00\u5316\u5b50\u4e32\u590d\u6742\u5ea6\u03b4\u7684\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\u3002", "motivation": "\u5f52\u4e00\u5316\u5b50\u4e32\u590d\u6742\u5ea6\u03b4\u4e0e\u6d41\u884c\u7684\u5b57\u7b26\u4e32\u538b\u7f29\u7b97\u6cd5\u6709\u5bc6\u5207\u5173\u7cfb\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u6548\u7684\u5728\u7ebf\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e00\u79cd\u5728O(log n)\u644a\u9500\u65f6\u95f4\u5185\u5904\u7406\u6bcf\u4e2a\u5b57\u7b26\uff0c\u53e6\u4e00\u79cd\u5728O(log\u00b3 n)\u6700\u574f\u60c5\u51b5\u4e0b\u5904\u7406\u6bcf\u4e2a\u5b57\u7b26\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u7ebf\u8ba1\u7b97\u5f52\u4e00\u5316\u5b50\u4e32\u590d\u6742\u5ea6\u03b4\u7684\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCode Digital Twin\u6846\u67b6\uff0c\u6574\u5408AI\u4e0e\u4f01\u4e1a\u7ba1\u7406\u8f6f\u4ef6\u5f00\u53d1\uff0c\u89e3\u51b3\u9690\u6027\u77e5\u8bc6\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u7cfb\u7edf\u7684\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e3b\u8981\u4f9d\u8d56\u589e\u91cf\u6f14\u8fdb\uff0c\u6d89\u53ca\u9690\u6027\u77e5\u8bc6\uff0c\u9700\u8981\u5c06AI\u80fd\u529b\u4e0e\u4f01\u4e1a\u5f00\u53d1\u73b0\u5b9e\u5bf9\u9f50\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Code Digital Twin\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u3001\u591a\u9636\u6bb5\u63d0\u53d6\u7ba1\u9053\u3001\u589e\u91cf\u66f4\u65b0\u3001LLM\u8d4b\u80fd\u5e94\u7528\u548c\u4eba\u673a\u4ea4\u4e92\u53cd\u9988\u3002", "result": "Code Digital Twin\u5c06\u788e\u7247\u5316\u77e5\u8bc6\u8f6c\u5316\u4e3a\u660e\u786e\u4e14\u53ef\u64cd\u4f5c\u7684\u8868\u793a\uff0c\u4e3a\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9bAI\u652f\u6301\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86Code Digital Twin\u4f5c\u4e3aAI\u4e0e\u4f01\u4e1a\u7ba1\u7406\u8f6f\u4ef6\u73b0\u5b9e\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u8d85\u590d\u6742\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u3001\u667a\u80fd\u548c\u5f39\u6027\u5f00\u53d1\u4e0e\u6f14\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u7ebf\u56fe\u3002"}}
{"id": "2510.15886", "categories": ["cs.GR", "cs.CG", "cs.HC", "68U05 (Primary) 05C85 (Secondary)", "I.3.5; G.2.2; I.2.1"], "pdf": "https://arxiv.org/pdf/2510.15886", "abs": "https://arxiv.org/abs/2510.15886", "authors": ["Diogo de Andrade", "Nuno Fachada"], "title": "Structural Tree Extraction from 3D Surfaces", "comment": null, "summary": "This paper introduces a method to extract a hierarchical tree representation\nfrom 3D unorganized polygonal data. The proposed approach first extracts a\ngraph representation of the surface, which serves as the foundation for\nstructural analysis. A Steiner tree is then generated to establish an optimized\nconnection between key terminal points, defined according to\napplication-specific criteria. The structure can be further refined by\nleveraging line-of-sight constraints, reducing redundancy while preserving\nessential connectivity. Unlike traditional skeletonization techniques, which\noften assume volumetric interpretations, this method operates directly on the\nsurface, ensuring that the resulting representation remains relevant for\nnavigation-aware geometric analysis. The method is validated through two use\ncases: extracting structural representations from tile-based elements for\nprocedural content generation, and identifying key points and structural\nmetrics for automated level analysis. Results demonstrate its ability to\nproduce simplified, coherent representations, supporting applications in\nprocedural generation, spatial reasoning, and map analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece3D\u975e\u7ec4\u7ec7\u591a\u8fb9\u5f62\u6570\u636e\u4e2d\u63d0\u53d6\u5c42\u6b21\u6811\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210Steiner\u6811\u548c\u5229\u7528\u89c6\u7ebf\u7ea6\u675f\u4f18\u5316\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u7a0b\u5e8f\u751f\u6210\u548c\u5730\u56fe\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u9aa8\u67b6\u5316\u6280\u672f\u901a\u5e38\u5047\u8bbe\u4f53\u79ef\u89e3\u91ca\uff0c\u800c\u8be5\u65b9\u6cd5\u76f4\u63a5\u5728\u8868\u9762\u4e0a\u64cd\u4f5c\uff0c\u786e\u4fdd\u7ed3\u679c\u8868\u793a\u5bf9\u5bfc\u822a\u611f\u77e5\u7684\u51e0\u4f55\u5206\u6790\u4fdd\u6301\u76f8\u5173\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4ece3D\u975e\u7ec4\u7ec7\u591a\u8fb9\u5f62\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u9762\u56fe\u8868\u793a\uff0c\u7136\u540e\u751f\u6210Steiner\u6811\u4ee5\u4f18\u5316\u5173\u952e\u7ec8\u7aef\u70b9\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u5e76\u5229\u7528\u89c6\u7ebf\u7ea6\u675f\u8fdb\u4e00\u6b65\u7ec6\u5316\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7b80\u5316\u7684\u3001\u8fde\u8d2f\u7684\u7ed3\u6784\u8868\u793a\uff0c\u652f\u6301\u7a0b\u5e8f\u751f\u6210\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u5730\u56fe\u5206\u6790\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u7b80\u5316\u7684\u3001\u8fde\u8d2f\u7684\u7ed3\u6784\u8868\u793a\uff0c\u9002\u7528\u4e8e\u7a0b\u5e8f\u751f\u6210\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u5730\u56fe\u5206\u6790\u7b49\u5e94\u7528\u3002"}}
{"id": "2510.16070", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.IV", "H.5.5; H.1.2; I.4.0"], "pdf": "https://arxiv.org/pdf/2510.16070", "abs": "https://arxiv.org/abs/2510.16070", "authors": ["Mahta Khoobi", "Marc Sebastian von der Stueck", "Felix Barajas Ordonez", "Anca-Maria Iancu", "Eric Corban", "Julia Nowak", "Aleksandar Kargaliev", "Valeria Perelygina", "Anna-Sophie Schott", "Daniel Pinto dos Santos", "Christiane Kuhl", "Daniel Truhn", "Sven Nebelung", "Robert Siepmann"], "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography", "comment": "Preprint version - Under second revision at Radiology (manuscript\n  RAD-25-1348)", "summary": "Structured reporting (SR) and artificial intelligence (AI) may transform how\nradiologists interact with imaging studies. This prospective study (July to\nDecember 2024) evaluated the impact of three reporting modes: free-text (FT),\nstructured reporting (SR), and AI-assisted structured reporting (AI-SR), on\nimage analysis behavior, diagnostic accuracy, efficiency, and user experience.\nFour novice and four non-novice readers (radiologists and medical students)\neach analyzed 35 bedside chest radiographs per session using a customized\nviewer and an eye-tracking system. Outcomes included diagnostic accuracy\n(compared with expert consensus using Cohen's $\\kappa$), reporting time per\nradiograph, eye-tracking metrics, and questionnaire-based user experience.\nStatistical analysis used generalized linear mixed models with Bonferroni\npost-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy\nwas similar in FT ($\\kappa = 0.58$) and SR ($\\kappa = 0.60$) but higher in\nAI-SR ($\\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$\ns (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade\ncounts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm\n58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s\n(FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <\n.001$ each). Novice readers shifted gaze towards the radiograph in SR, while\nnon-novice readers maintained their focus on the radiograph. AI-SR was the\npreferred mode. In conclusion, SR improves efficiency by guiding visual\nattention toward the image, and AI-prefilled SR further enhances diagnostic\naccuracy and user satisfaction.", "AI": {"tldr": "\u7ed3\u6784\u5316\u62a5\u544a\uff08SR\uff09\u548cAI\u8f85\u52a9SR\uff08AI-SR\uff09\u663e\u8457\u63d0\u5347\u653e\u5c04\u79d1\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0cAI-SR\u6548\u679c\u6700\u4f73\u4e14\u6700\u53d7\u7528\u6237\u9752\u7750\u3002", "motivation": "\u63a2\u7d22\u7ed3\u6784\u5316\u62a5\u544a\uff08SR\uff09\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5982\u4f55\u6539\u53d8\u653e\u5c04\u79d1\u533b\u751f\u4e0e\u5f71\u50cf\u7814\u7a76\u7684\u4e92\u52a8\u65b9\u5f0f\uff0c\u8bc4\u4f30\u5176\u5bf9\u8bca\u65ad\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u4e3a\u524d\u77bb\u6027\u7814\u7a76\uff082024\u5e747\u6708\u81f312\u6708\uff09\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u62a5\u544a\u6a21\u5f0f\uff08\u81ea\u7531\u6587\u672cFT\u3001\u7ed3\u6784\u5316\u62a5\u544aSR\u3001AI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544aAI-SR\uff09\u5bf9\u56fe\u50cf\u5206\u6790\u884c\u4e3a\u3001\u8bca\u65ad\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002\u53c2\u4e0e\u8005\u5305\u62ec\u56db\u4f4d\u65b0\u624b\u548c\u56db\u4f4d\u975e\u65b0\u624b\u8bfb\u8005\uff08\u653e\u5c04\u79d1\u533b\u751f\u548c\u533b\u5b66\u751f\uff09\uff0c\u6bcf\u4eba\u4f7f\u7528\u5b9a\u5236\u5316\u67e5\u770b\u5668\u548c\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u5206\u679035\u5f20\u5e8a\u65c1\u80f8\u7247\u3002\u7ed3\u679c\u6307\u6807\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u6027\uff08\u4e0e\u4e13\u5bb6\u5171\u8bc6\u6bd4\u8f83\uff0c\u4f7f\u7528Cohen's \u03ba\uff09\u3001\u6bcf\u5f20\u80f8\u7247\u62a5\u544a\u65f6\u95f4\u3001\u773c\u52a8\u8ffd\u8e2a\u6307\u6807\u548c\u95ee\u5377\u8c03\u67e5\u7684\u7528\u6237\u4f53\u9a8c\u3002\u7edf\u8ba1\u5206\u6790\u91c7\u7528\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u548cBonferroni\u4e8b\u540e\u68c0\u9a8c\uff0c\u663e\u8457\u6027\u6c34\u5e73\u4e3aP \u2264 0.01\u3002", "result": "\u8bca\u65ad\u51c6\u786e\u6027\u5728FT\uff08\u03ba = 0.58\uff09\u548cSR\uff08\u03ba = 0.60\uff09\u4e2d\u76f8\u4f3c\uff0c\u4f46\u5728AI-SR\u4e2d\u66f4\u9ad8\uff08\u03ba = 0.71, P < 0.001\uff09\u3002\u62a5\u544a\u65f6\u95f4\u4eceFT\u768488 \u00b1 38\u79d2\u51cf\u5c11\u5230SR\u768437 \u00b1 18\u79d2\u548cAI-SR\u768425 \u00b1 9\u79d2\uff08P < 0.001\uff09\u3002SR\u548cAI-SR\u964d\u4f4e\u4e86\u653e\u5c04\u56fe\u533a\u57df\u7684\u626b\u89c6\u6b21\u6570\u548c\u62a5\u544a\u533a\u57df\u7684\u603b\u6ce8\u89c6\u65f6\u95f4\uff08P < 0.001\uff09\u3002\u65b0\u624b\u8bfb\u8005\u5728SR\u4e2d\u89c6\u7ebf\u8f6c\u5411\u653e\u5c04\u56fe\uff0c\u800c\u975e\u65b0\u624b\u8bfb\u8005\u4fdd\u6301\u5bf9\u653e\u5c04\u56fe\u7684\u5173\u6ce8\u3002AI-SR\u662f\u6700\u53d7\u6b22\u8fce\u7684\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u6784\u5316\u62a5\u544a\uff08SR\uff09\u901a\u8fc7\u5f15\u5bfc\u89c6\u89c9\u6ce8\u610f\u529b\u671d\u5411\u56fe\u50cf\u63d0\u9ad8\u6548\u7387\uff0c\u800cAI\u9884\u586b\u5145\u7684SR\uff08AI-SR\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2510.16281", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16281", "abs": "https://arxiv.org/abs/2510.16281", "authors": ["Yilin Wu", "Anqi Li", "Tucker Hermans", "Fabio Ramos", "Andrea Bajcsy", "Claudia P'erez-D'Arpino"], "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification", "comment": null, "summary": "Reasoning Vision Language Action (VLA) models improve robotic\ninstruction-following by generating step-by-step textual plans before low-level\nactions, an approach inspired by Chain-of-Thought (CoT) reasoning in language\nmodels. Yet even with a correct textual plan, the generated actions can still\nmiss the intended outcomes in the plan, especially in out-of-distribution (OOD)\nscenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,\nand introduce a training-free, runtime policy steering method for\nreasoning-action alignment. Given a reasoning VLA's intermediate textual plan,\nour framework samples multiple candidate action sequences from the same model,\npredicts their outcomes via simulation, and uses a pre-trained Vision-Language\nModel (VLM) to select the sequence whose outcome best aligns with the VLA's own\ntextual plan. Only executing action sequences that align with the textual\nreasoning turns our base VLA's natural action diversity from a source of error\ninto a strength, boosting robustness to semantic and visual OOD perturbations\nand enabling novel behavior composition without costly re-training. We also\ncontribute a reasoning-annotated extension of LIBERO-100, environment\nvariations tailored for OOD evaluation, and demonstrate up to 15% performance\ngain over prior work on behavior composition tasks and scales with compute and\ndata diversity. Project Website at:\nhttps://yilin-wu98.github.io/steering-reasoning-vla/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u63d0\u5347VLA\u6a21\u578b\u6307\u4ee4\u8ddf\u968f\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u548cVLM\u9009\u62e9\u5bf9\u9f50\u52a8\u4f5c\u5e8f\u5217\uff0c\u5728OOD\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u751f\u6210\u6b63\u786e\u7684\u6587\u672c\u8ba1\u5212\u540e\uff0c\u4ecd\u53ef\u80fd\u56e0\u52a8\u4f5c\u6267\u884c\u4e0e\u8ba1\u5212\u4e0d\u4e00\u81f4\u800c\u5931\u8d25\uff0c\u5c24\u5176\u662f\u5728OOD\u573a\u666f\u4e2d\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5fe0\u5b9e\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e8f\u5217\u3001\u9884\u6d4b\u5176\u6a21\u62df\u7ed3\u679c\uff0c\u5e76\u9009\u62e9\u4e0e\u6587\u672c\u8ba1\u5212\u6700\u5bf9\u9f50\u7684\u5e8f\u5217\u6765\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u884c\u4e3a\u7ec4\u5408\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u5de5\u4f5c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe15%\uff0c\u4e14\u80fd\u968f\u8ba1\u7b97\u548c\u6570\u636e\u591a\u6837\u6027\u6269\u5c55\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u8fd0\u884c\u65f6\u7b56\u7565\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u884c\u4e3a\u7ec4\u5408\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u3002"}}
{"id": "2510.16946", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16946", "abs": "https://arxiv.org/abs/2510.16946", "authors": ["Erfan Darzi", "Aldo Pareja", "Shreeanant Bharadwaj"], "title": "Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure", "comment": null, "summary": "Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is\ncritical for maintaining performance predictability and resource utilization,\nyet existing monitoring tools lack the granularity for root cause analysis in\nshared computing environments. We introduce an eBPF-based telemetry system that\nprovides unified host-side monitoring of GPU workloads, correlating\neBPF-derived host metrics with GPU-internal events for holistic system\nobservability. The system achieves 81--88\\% diagnostic accuracy, detects spikes\nwithin 5 seconds, and completes root cause analysis in 6--8 seconds, operating\nwith 1.21\\% CPU overhead at 100Hz sampling. Evaluated on distributed learning\nworkloads, the system identifies root causes including NIC contention, PCIe\npressure, and CPU interference, enabling operational debugging for multi-tenant\nGPU infrastructure without requiring cluster-wide instrumentation.", "AI": {"tldr": "\u63d0\u51faeBPF\u9065\u6d4b\u7cfb\u7edf\uff0c\u89e3\u51b3GPU\u5c3e\u5ef6\u8fdf\u8bca\u65ad\u95ee\u9898\uff0c\u9ad8\u7cbe\u5ea6\u4f4e\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u76d1\u63a7\u5de5\u5177\u5728\u5171\u4eab\u8ba1\u7b97\u73af\u5883\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u6790\u80fd\u529b\uff0c\u65e0\u6cd5\u8fdb\u884c\u6839\u56e0\u5206\u6790\u3002", "method": "\u91c7\u7528eBPF\u6280\u672f\uff0c\u7ed3\u5408\u4e3b\u673a\u6307\u6807\u4e0eGPU\u5185\u90e8\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u5168\u7cfb\u7edf\u89c2\u6d4b\u3002", "result": "\u7cfb\u7edf\u8bca\u65ad\u51c6\u786e\u7387\u8fbe81-88%\uff0c5\u79d2\u5185\u68c0\u6d4b\u5c16\u5cf0\uff0c6-8\u79d2\u5b8c\u6210\u6839\u56e0\u5206\u6790\uff0cCPU\u5f00\u9500\u4e3a1.21%\uff08100Hz\u91c7\u6837\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eeBPF\u7684\u9065\u6d4b\u7cfb\u7edf\uff0c\u6709\u6548\u8bca\u65ad\u4e86GPU\u5c3e\u5ef6\u8fdf\u5c16\u5cf0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5171\u4eab\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6027\u80fd\u53ef\u9884\u6d4b\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2510.16516", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.16516", "abs": "https://arxiv.org/abs/2510.16516", "authors": ["Yossi Azar", "Niv Buchbinder", "Roie Levin", "Or Vardi"], "title": "Trading Prophets with Initial Capital", "comment": null, "summary": "Correa et al. [EC' 2023] introduced the following trading prophets problem. A\ntrader observes a sequence of stochastic prices for a stock, each drawn from a\nknown distribution, and at each time must decide whether to buy or sell.\nUnfortunately, they observed that in this setting it is impossible to compete\nwith a prophet who knows all future stock prices.\n  In this paper, we explore the trading prophets problem when we are given\ninitial capital with which to start trading. We show that initial capital is\nenough to bypass the impossibility result and obtain a competitive ratio of $3$\nwith respect to a prophet who knows all future prices (and who also starts with\ncapital), and we show that this competitive ratio is best possible. We further\nstudy a more realistic model in which the trader must pay multiplicative and/or\nadditive transaction costs for trading which model dynamics such as bid-ask\nspreads and broker fees.", "AI": {"tldr": "\u521d\u59cb\u8d44\u672c\u4f7f\u4ea4\u6613\u8005\u80fd\u4e0e\u5148\u77e5\u7ade\u4e89\uff0c\u7ade\u4e89\u6bd4\u4e3a3\u4e14\u6700\u4f18\u3002\u7814\u7a76\u8fd8\u6269\u5c55\u81f3\u5305\u542b\u4ea4\u6613\u6210\u672c\u7684\u73b0\u5b9e\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5728\u521d\u59cb\u8d44\u672c\u4e0b\u5982\u4f55\u7ed5\u8fc7\u4ea4\u6613\u5148\u77e5\u95ee\u9898\u4e2d\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u5e76\u7814\u7a76\u66f4\u73b0\u5b9e\u7684\u4ea4\u6613\u6210\u672c\u6a21\u578b\u3002", "method": "\u7814\u7a76\u4e86\u5728\u7ed9\u5b9a\u521d\u59cb\u8d44\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u4ea4\u6613\u8005\u5982\u4f55\u901a\u8fc7\u51b3\u7b56\u4e70\u5356\u80a1\u7968\u6765\u7ade\u4e89\u3002\u5206\u6790\u4e86\u5305\u542b\u4e58\u6027\u548c\u52a0\u6027\u4ea4\u6613\u6210\u672c\u7684\u66f4\u590d\u6742\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u4e0e\u5148\u77e5\u76f8\u6bd4\u7684\u7ade\u4e89\u6bd4\u4e3a3\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e00\u6bd4\u4f8b\u662f\u6700\u4f18\u7684\u3002\u5728\u5305\u542b\u4ea4\u6613\u6210\u672c\u7684\u6a21\u578b\u4e2d\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u521d\u59cb\u8d44\u672c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u521d\u59cb\u8d44\u672c\u53ef\u4ee5\u7ed5\u8fc7\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e0e\u77e5\u6653\u672a\u6765\u4ef7\u683c\u7684\u5148\u77e5\u76f8\u6bd4\u7684\u7ade\u4e89\u6bd4\u4e3a3\uff0c\u4e14\u8fd9\u4e00\u7ade\u4e89\u6bd4\u662f\u6700\u4f18\u7684\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u5305\u542b\u4ea4\u6613\u6210\u672c\u7684\u66f4\u73b0\u5b9e\u6a21\u578b\u3002"}}
{"id": "2510.16433", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16433", "abs": "https://arxiv.org/abs/2510.16433", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Yasutaka Kamei", "Hajimu Iida"], "title": "Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions", "comment": null, "summary": "Software vulnerabilities are constantly being reported and exploited in\nsoftware products, causing significant impacts on society. In recent years, the\nmain approach to vulnerability detection, fuzzing, has been integrated into the\ncontinuous integration process to run in short and frequent cycles. This\ncontinuous fuzzing allows for fast identification and remediation of\nvulnerabilities during the development process. Despite adoption by thousands\nof projects, however, it is unclear how continuous fuzzing contributes to\nvulnerability detection. This study aims to elucidate the role of continuous\nfuzzing in vulnerability detection. Specifically, we investigate the coverage\nand the total number of fuzzing sessions when fuzzing bugs are discovered. We\ncollect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an\nonline service provided by Google that performs fuzzing during continuous\nintegration. Through an empirical study of a total of approximately 1.12\nmillion fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal\nthat (i) a substantial number of fuzzing bugs exist prior to the integration of\ncontinuous fuzzing, leading to a high detection rate in the early stages; (ii)\ncode coverage continues to increase as continuous fuzzing progresses; and (iii)\nchanges in coverage contribute to the detection of fuzzing bugs. This study\nprovides empirical insights into how continuous fuzzing contributes to fuzzing\nbug detection, offering practical implications for future strategies and tool\ndevelopment in continuous fuzzing.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790OSS-Fuzz\u6570\u636e\uff0c\u63ed\u793a\u4e86\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u8d21\u732e\uff0c\u5305\u62ec\u65e9\u671f\u9ad8\u68c0\u6d4b\u7387\u3001\u8986\u76d6\u7387\u589e\u52a0\u53ca\u5176\u5bf9\u6f0f\u6d1e\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5df2\u88ab\u6570\u5343\u4e2a\u9879\u76ee\u91c7\u7528\uff0c\u4f46\u5176\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u5177\u4f53\u8d21\u732e\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u9610\u660e\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6536\u96c6\u4e86\u6765\u81eaOSS-Fuzz\u7684\u7ea6112\u4e07\u6b21\u6a21\u7cca\u6d4b\u8bd5\u4f1a\u8bdd\u7684\u6570\u636e\uff0c\u5305\u62ec\u95ee\u9898\u62a5\u544a\u3001\u8986\u76d6\u7387\u62a5\u544a\u548c\u6a21\u7cca\u6d4b\u8bd5\u65e5\u5fd7\uff0c\u5e76\u5bf9878\u4e2a\u9879\u76ee\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i)\u5927\u91cf\u6a21\u7cca\u6d4b\u8bd5\u6f0f\u6d1e\u5728\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u96c6\u6210\u524d\u5df2\u5b58\u5728\uff0c\u5bfc\u81f4\u65e9\u671f\u68c0\u6d4b\u7387\u9ad8\uff1b(ii)\u968f\u7740\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u7684\u8fdb\u884c\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u6301\u7eed\u589e\u52a0\uff1b(iii)\u8986\u76d6\u7387\u7684\u53d8\u5316\u6709\u52a9\u4e8e\u6a21\u7cca\u6d4b\u8bd5\u6f0f\u6d1e\u7684\u68c0\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u8d21\u732e\uff0c\u4e3a\u672a\u6765\u6301\u7eed\u6a21\u7cca\u6d4b\u8bd5\u7684\u7b56\u7565\u548c\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2510.16147", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16147", "abs": "https://arxiv.org/abs/2510.16147", "authors": ["Maxim Gumin", "Do Heon Han", "Seung Jean Yoo", "Aditya Ganeshan", "R. Kenny Jones", "Kailiang Fu", "Rio Aguina-Kang", "Stewart Morris", "Daniel Ritchie"], "title": "Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search", "comment": "To appear in SIGGRAPH Asia 2025", "summary": "Synthesizing 3D scenes from open-vocabulary text descriptions is a\nchallenging, important, and recently-popular application. One of its critical\nsubproblems is layout generation: given a set of objects, lay them out to\nproduce a scene matching the input description. Nearly all recent work adopts a\ndeclarative paradigm for this problem: using an LLM to generate a specification\nof constraints between objects, then solving those constraints to produce the\nfinal layout. In contrast, we explore an alternative imperative paradigm, in\nwhich an LLM iteratively places objects, with each object's position and\norientation computed as a function of previously-placed objects. The imperative\napproach allows for a simpler scene specification language while also handling\na wider variety and larger complexity of scenes. We further improve the\nrobustness of our imperative scheme by developing an error correction mechanism\nthat iteratively improves the scene's validity while staying as close as\npossible to the original layout generated by the LLM. In forced-choice\nperceptual studies, participants preferred layouts generated by our imperative\napproach 82% and 94% of the time when compared against two declarative layout\ngeneration methods. We also present a simple, automated evaluation metric for\n3D scene layout generation that aligns well with human preferences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u547d\u4ee4\u5f0f\u4e09\u7ef4\u573a\u666f\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u8fed\u4ee3\u653e\u7f6e\u5bf9\u8c61\u5e76\u5f15\u5165\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u58f0\u660e\u5f0f\u65b9\u6cd5\uff0c\u4e14\u65b0\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u91c7\u7528\u58f0\u660e\u5f0f\u8303\u5f0f\uff08\u901a\u8fc7LLM\u751f\u6210\u5bf9\u8c61\u95f4\u7ea6\u675f\u89c4\u8303\u518d\u6c42\u89e3\uff09\uff0c\u4f46\u5b58\u5728\u573a\u666f\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u5904\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u547d\u4ee4\u5f0f\u8303\u5f0f\uff0c\u5373\u901a\u8fc7LLM\u8fed\u4ee3\u653e\u7f6e\u5bf9\u8c61\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u57fa\u4e8e\u5148\u524d\u653e\u7f6e\u7684\u5bf9\u8c61\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u5e03\u5c40\u7684\u9c81\u68d2\u6027\u3002", "result": "\u547d\u4ee4\u5f0f\u65b9\u6cd5\u5728\u611f\u77e5\u7814\u7a76\u4e2d\u663e\u8457\u4f18\u4e8e\u58f0\u660e\u5f0f\u65b9\u6cd5\uff0c\u4e14\u63d0\u51fa\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002", "conclusion": "\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u672c\u6587\u63d0\u51fa\u7684\u547d\u4ee4\u5f0f\u65b9\u6cd5\u751f\u6210\u7684\u5e03\u5c40\uff0c\u5176\u5728\u611f\u77e5\u7814\u7a76\u4e2d\u5206\u522b\u4ee582%\u548c94%\u7684\u504f\u597d\u7387\u4f18\u4e8e\u4e24\u79cd\u58f0\u660e\u5f0f\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u7684\u4e09\u7ef4\u573a\u666f\u5e03\u5c40\u751f\u6210\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2510.16072", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16072", "abs": "https://arxiv.org/abs/2510.16072", "authors": ["Farjana Yesmin"], "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation", "comment": "18 pages", "summary": "Machine learning models trained on imbalanced datasets often exhibit\nintersectional biases-systematic errors arising from the interaction of\nmultiple attributes such as object class and environmental conditions. This\npaper presents a data-driven framework for analyzing and mitigating such biases\nin image classification. We introduce the Intersectional Fairness Evaluation\nFramework (IFEF), which combines quantitative fairness metrics with\ninterpretability tools to systematically identify bias patterns in model\npredictions. Building on this analysis, we propose Bias-Weighted Augmentation\n(BWA), a novel data augmentation strategy that adapts transformation\nintensities based on subgroup distribution statistics. Experiments on the Open\nImages V7 dataset with five object classes demonstrate that BWA improves\naccuracy for underrepresented class-environment intersections by up to 24\npercentage points while reducing fairness metric disparities by 35%.\nStatistical analysis across multiple independent runs confirms the significance\nof improvements (p < 0.05). Our methodology provides a replicable approach for\nanalyzing and addressing intersectional biases in image classification systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIFEF\u548cBWA\u6846\u67b6\uff0c\u6709\u6548\u51cf\u5c11\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u4ea4\u53c9\u504f\u89c1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u5e73\u8861\u65f6\u51fa\u73b0\u7684\u4ea4\u53c9\u504f\u89c1\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4ea4\u53c9\u516c\u5e73\u8bc4\u4f30\u6846\u67b6\uff08IFEF\uff09\u548c\u504f\u89c1\u52a0\u6743\u589e\u5f3a\uff08BWA\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u5b9a\u91cf\u516c\u5e73\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u3002", "result": "\u5728Open Images V7\u6570\u636e\u96c6\u4e0a\uff0cBWA\u5c06\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u522b-\u73af\u5883\u4ea4\u53c9\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8624\u4e2a\u767e\u5206\u70b9\uff0c\u516c\u5e73\u6027\u6307\u6807\u5dee\u5f02\u51cf\u5c11\u4e8635%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u6765\u5206\u6790\u548c\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\u4e2d\u7684\u4ea4\u53c9\u504f\u89c1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16308", "abs": "https://arxiv.org/abs/2510.16308", "authors": ["Chi Zhang", "Xian Huang", "Wei Dong"], "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling", "comment": null, "summary": "UAVs equipped with a single depth camera encounter significant challenges in\ndynamic obstacle avoidance due to limited field of view and inevitable blind\nspots. While active vision strategies that steer onboard cameras have been\nproposed to expand sensing coverage, most existing methods separate motion\nplanning from sensing considerations, resulting in less effective and delayed\nobstacle response. To address this limitation, we introduce SPOT\n(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning\nframework for observation-aware trajectory planning that explicitly\nincorporates sensing objectives into motion optimization. At the core of our\nmethod is a Gaussian Process-based obstacle belief map, which establishes a\nunified probabilistic representation of both recognized (previously observed)\nand potential obstacles. This belief is further processed through a\ncollision-aware inference mechanism that transforms spatial uncertainty and\ntrajectory proximity into a time-varying observation urgency map. By\nintegrating urgency values within the current field of view, we define\ndifferentiable objectives that enable real-time, observation-aware trajectory\nplanning with computation times under 10 ms. Simulation and real-world\nexperiments in dynamic, cluttered, and occluded environments show that our\nmethod detects potential dynamic obstacles 2.8 seconds earlier than baseline\napproaches, increasing dynamic obstacle visibility by over 500\\%, and enabling\nsafe navigation through cluttered, occluded environments.", "AI": {"tldr": "SPOT\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u611f\u77e5\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8fd0\u52a8\u89c4\u5212\u4e0e\u611f\u77e5\u8003\u8651\u5206\u79bb\uff0c\u5bfc\u81f4\u969c\u788d\u7269\u54cd\u5e94\u6548\u679c\u4e0d\u4f73\u4e14\u5ef6\u8fdf\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u89c4\u5212\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u57fa\u7840\u7684\u969c\u788d\u7269\u4fe1\u5ff5\u5730\u56fe\uff0c\u7ed3\u5408\u78b0\u649e\u611f\u77e5\u63a8\u7406\u673a\u5236\uff0c\u751f\u6210\u65f6\u53d8\u89c2\u6d4b\u7d27\u6025\u5ea6\u5730\u56fe\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u89c2\u6d4b\u611f\u77e5\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u8868\u660e\uff0cSPOT\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u524d2.8\u79d2\u68c0\u6d4b\u5230\u6f5c\u5728\u52a8\u6001\u969c\u788d\u7269\uff0c\u52a8\u6001\u969c\u788d\u7269\u53ef\u89c1\u6027\u63d0\u9ad8500%\u4ee5\u4e0a\u3002", "conclusion": "SPOT\u6846\u67b6\u901a\u8fc7\u5c06\u611f\u77e5\u76ee\u6807\u660e\u786e\u7eb3\u5165\u8fd0\u52a8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u3001\u6742\u4e71\u548c\u906e\u6321\u73af\u5883\u4e2d\u7684\u969c\u788d\u7269\u68c0\u6d4b\u548c\u907f\u969c\u80fd\u529b\u3002"}}
{"id": "2510.17158", "categories": ["cs.DC", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17158", "abs": "https://arxiv.org/abs/2510.17158", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Charles Jekel", "Abhinav Bhatele", "Harshitha Menon"], "title": "Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization", "comment": null, "summary": "Language models are now prevalent in software engineering with many\ndevelopers using them to automate tasks and accelerate their development. While\nlanguage models have been tremendous at accomplishing complex software\nengineering tasks, there are still many areas where they fail to deliver\ndesirable results, for instance code performance related tasks. Tasks like\noptimization depend on many complex data from the environment, hardware, etc.\nthat are not directly represented in source code. Recent efforts have seen\nlarge improvements in general code modeling tasks using chain-of-thought style\nreasoning, but these models still fail to comprehend how the environment\ninteracts with code performance. In this paper we propose a methodology to\ntrain language models that can interact with performance tools during their\nreasoning process. We then demonstrate how this methodology can be used to\ntrain a state-of-the-art GPU kernel optimization model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0e\u6027\u80fd\u5de5\u5177\u4e92\u52a8\uff0c\u4ece\u800c\u63d0\u5347\u4ee3\u7801\u6027\u80fd\u4efb\u52a1\u7684\u6267\u884c\u6548\u679c\uff0c\u5e76\u6210\u529f\u8bad\u7ec3\u51fa\u5148\u8fdb\u7684GPU\u5185\u6838\u4f18\u5316\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4ee3\u7801\u6027\u80fd\u76f8\u5173\u4efb\u52a1\uff08\u5982\u4f18\u5316\uff09\u4e2d\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u4f9d\u8d56\u4e8e\u73af\u5883\u3001\u786c\u4ef6\u7b49\u590d\u6742\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u4e0e\u6027\u80fd\u5de5\u5177\u4e92\u52a8\u3002", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u8bad\u7ec3\u51fa\u4e86\u4e00\u4e2a\u5148\u8fdb\u7684GPU\u5185\u6838\u4f18\u5316\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u4e0e\u6027\u80fd\u5de5\u5177\u4e92\u52a8\uff0c\u4ece\u800c\u63d0\u5347\u4ee3\u7801\u6027\u80fd\u76f8\u5173\u4efb\u52a1\u7684\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2510.16663", "categories": ["cs.DS", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16663", "abs": "https://arxiv.org/abs/2510.16663", "authors": ["Yiding Feng", "Vahideh Manshadi", "Rad Niazadeh", "Saba Neyshabouri"], "title": "Robust Dynamic Staffing with Predictions", "comment": null, "summary": "We consider a natural dynamic staffing problem in which a decision-maker\nsequentially hires workers over a finite horizon to meet an unknown demand\nrevealed at the end. Predictions about demand arrive over time and become\nincreasingly accurate, while worker availability decreases. This creates a\nfundamental trade-off between hiring early to avoid understaffing (when workers\nare more available but forecasts are less reliable) and hiring late to avoid\noverstaffing (when forecasts are more accurate but availability is lower). This\nproblem is motivated by last-mile delivery operations, where companies such as\nAmazon rely on gig-economy workers whose availability declines closer to the\noperating day.\n  To address practical limitations of Bayesian models (in particular, to remain\nagnostic to the underlying forecasting method), we study this problem under\nadversarial predictions. In this model, sequential predictions are\nadversarially chosen uncertainty intervals that (approximately) contain the\ntrue demand. The objective is to minimize worst-case staffing imbalance cost.\nOur main result is a simple and computationally efficient online algorithm that\nis minimax optimal. We first characterize the minimax cost against a restricted\nadversary via a polynomial-size linear program, then show how to emulate this\nsolution in the general case. While our base model focuses on a single demand,\nwe extend the framework to multiple demands (with egalitarian/utilitarian\nobjectives), to settings with costly reversals of hiring decisions, and to\ninconsistent prediction intervals. We also introduce a practical \"re-solving\"\nvariant of our algorithm, which we prove is also minimax optimal. Finally we\nconduct numerical experiments showing that our algorithms outperform Bayesian\nheuristics in both cost and speed, and are competitive with (approximate or\nexact) Bayesian-optimal policies when those can be computed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u5c0f\u6781\u5927\u6700\u4f18\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u89e3\u51b3\u52a8\u6001\u4eba\u5458\u914d\u7f6e\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6269\u5c55\u573a\u666f\u4e2d\u4fdd\u6301\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u8d1d\u53f6\u65af\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u8be5\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u8fd0\u8425\u4e2d\u7684\u52a8\u6001\u4eba\u5458\u914d\u7f6e\u95ee\u9898\uff0c\u5176\u4e2d\u516c\u53f8\u9700\u8981\u5728\u5de5\u4eba\u53ef\u7528\u6027\u4e0b\u964d\u548c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e4b\u95f4\u6743\u8861\u3002\u4e3a\u4e86\u907f\u514d\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5bf9\u6297\u6027\u9884\u6d4b\u4e0b\u7684\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u591a\u9879\u5f0f\u5927\u5c0f\u7684\u7ebf\u6027\u89c4\u5212\u63cf\u8ff0\u4e86\u4e00\u4e2a\u53d7\u9650\u5bf9\u624b\u7684\u6781\u5c0f\u6781\u5927\u6210\u672c\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u6a21\u62df\u8fd9\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u6269\u5c55\u4e86\u6846\u67b6\uff0c\u5904\u7406\u591a\u9700\u6c42\u3001\u96c7\u4f63\u51b3\u7b56\u7684\u6602\u8d35\u9006\u8f6c\u4ee5\u53ca\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u201c\u91cd\u65b0\u6c42\u89e3\u201d\u7b97\u6cd5\u53d8\u4f53\u3002", "result": "\u4e3b\u8981\u6210\u679c\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u6781\u5c0f\u6781\u5927\u610f\u4e49\u4e0b\u662f\u6700\u4f18\u7684\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u6269\u5c55\u4e86\u6846\u67b6\u4ee5\u5904\u7406\u591a\u79cd\u9700\u6c42\u548c\u5176\u4ed6\u590d\u6742\u60c5\u51b5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u7b97\u6cd5\u53d8\u4f53\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u6781\u5c0f\u6781\u5927\u610f\u4e49\u4e0b\u662f\u6700\u4f18\u7684\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u52a8\u6001\u4eba\u5458\u914d\u7f6e\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6269\u5c55\u573a\u666f\u4e2d\u4fdd\u6301\u6027\u80fd\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u6210\u672c\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u8d1d\u53f6\u65af\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e0e\u8d1d\u53f6\u65af\u6700\u4f18\u7b56\u7565\u76f8\u5f53\u3002"}}
{"id": "2510.16502", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16502", "abs": "https://arxiv.org/abs/2510.16502", "authors": ["Sebasti\u00e1n Pizard", "Ramiro Moreira", "Federico Galiano", "Ignacio Sastre", "Lorena Etcheverry"], "title": "On the Use of Large Language Models for Qualitative Synthesis", "comment": null, "summary": "Large language models (LLMs) show promise for supporting systematic reviews\n(SR), even complex tasks such as qualitative synthesis (QS). However, applying\nthem to a stage that is unevenly reported and variably conducted carries\nimportant risks: misuse can amplify existing weaknesses and erode confidence in\nthe SR findings. To examine the challenges of using LLMs for QS, we conducted a\ncollaborative autoethnography involving two trials. We evaluated each trial for\nmethodological rigor and practical usefulness, and interpreted the results\nthrough a technical lens informed by how LLMs are built and their current\nlimitations.", "AI": {"tldr": "LLMs\u5728\u7cfb\u7edf\u8bc4\u4ef7\u7684\u5b9a\u6027\u5408\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8c28\u614e\u4f7f\u7528\u4ee5\u907f\u514d\u98ce\u9669\uff0c\u7814\u7a76\u901a\u8fc7\u81ea\u6c11\u65cf\u5fd7\u65b9\u6cd5\u8bc4\u4f30\u4e86\u5176\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u7cfb\u7edf\u8bc4\u4ef7\u4e2d\uff0c\u5c24\u5176\u662f\u5b9a\u6027\u5408\u6210\u9636\u6bb5\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u98ce\u9669\uff0c\u4ee5\u907f\u514d\u56e0\u6ee5\u7528\u800c\u52a0\u5267\u73b0\u6709\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5408\u4f5c\u81ea\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u4e24\u4e2a\u8bd5\u9a8c\u7684\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u5b9e\u7528\u6027\uff0c\u5e76\u7ed3\u5408LLMs\u7684\u6280\u672f\u7279\u6027\u548c\u5f53\u524d\u5c40\u9650\u6027\u8fdb\u884c\u89e3\u8bfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cLLMs\u5728QS\u9636\u6bb5\u7684\u5e94\u7528\u9700\u4e25\u683c\u8bc4\u4f30\u5176\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u8003\u8651\u5176\u6280\u672f\u9650\u5236\u3002", "conclusion": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5b9a\u6027\u5408\u6210\uff08QS\uff09\u5b58\u5728\u6311\u6218\uff0c\u9700\u8c28\u614e\u4ee5\u907f\u514d\u653e\u5927\u73b0\u6709\u5f31\u70b9\u5e76\u5f71\u54cd\u7cfb\u7edf\u8bc4\u4ef7\uff08SR\uff09\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.16486", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16486", "abs": "https://arxiv.org/abs/2510.16486", "authors": ["Mathieu Pont", "Christoph Garth"], "title": "Region-Aware Wasserstein Distances of Persistence Diagrams and Merge Trees", "comment": null, "summary": "This paper presents a generalization of the Wasserstein distance for both\npersistence diagrams and merge trees [20], [66] that takes advantage of the\nregions of their topological features in the input domain. Specifically, we\nredefine the comparison of topological features as a distance between the\nvalues of their extrema-aligned regions. It results in a more discriminative\nmetric than the classical Wasserstein distance and generalizes it through an\ninput parameter adjusting the impact of the region properties in the distance.\nWe present two strategies to control both computation time and memory storage\nof our method by respectively enabling the use of subsets of the regions in the\ncomputation, and by compressing the regions' properties to obtain low-memory\nrepresentations. Extensive experiments on openly available ensemble data\ndemonstrate the efficiency of our method, with running times on the orders of\nminutes on average. We show the utility of our contributions with two\napplications. First, we use the assignments between topological features\nprovided by our method to track their evolution in time-varying ensembles and\npropose the temporal persistence curves to facilitate the understanding of how\nthese features appear, disappear and change over time. Second, our method\nallows to compute a distance matrix of an ensemble that can be used for\ndimensionality reduction purposes and visually represent in 2D all its members,\nwe show that such distance matrices also allow to detect key phases in the\nensemble. Finally, we provide a C++ implementation that can be used to\nreproduce our results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684Wasserstein\u8ddd\u79bb\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u6301\u4e45\u6027\u56fe\u548c\u5408\u5e76\u6811\u7684\u6bd4\u8f83\uff0c\u63d0\u9ad8\u4e86\u533a\u5206\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edf\u7684Wasserstein\u8ddd\u79bb\u5728\u6bd4\u8f83\u62d3\u6251\u7279\u5f81\u65f6\u7f3a\u4e4f\u533a\u5206\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8f93\u5165\u57df\u4e2d\u7684\u533a\u57df\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4 discriminative \u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u91cd\u65b0\u5b9a\u4e49\u62d3\u6251\u7279\u5f81\u7684\u6bd4\u8f83\u65b9\u5f0f\uff0c\u57fa\u4e8e\u6781\u503c\u5bf9\u9f50\u533a\u57df\u7684\u503c\u8ba1\u7b97\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u8f93\u5165\u53c2\u6570\u8c03\u6574\u533a\u57df\u5c5e\u6027\u5728\u8ddd\u79bb\u4e2d\u7684\u5f71\u54cd\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\u6765\u63a7\u5236\u8ba1\u7b97\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u4e3a\u5206\u949f\u7ea7\u522b\uff0c\u4e14\u5728\u65f6\u95f4\u53d8\u5316\u96c6\u5408\u7684\u7279\u5f81\u8ddf\u8e2a\u548c\u964d\u7ef4\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684Wasserstein\u8ddd\u79bb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6301\u4e45\u6027\u56fe\u548c\u5408\u5e76\u6811\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u65f6\u95f4\u548c\u5185\u5b58\u5b58\u50a8\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u6548\u7387\u9ad8\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u65f6\u95f4\u53d8\u5316\u96c6\u5408\u548c\u964d\u7ef4\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.16088", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16088", "abs": "https://arxiv.org/abs/2510.16088", "authors": ["Zia Badar"], "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch", "comment": null, "summary": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6bd4\u7279\u91cf\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u5728ImageNet\u4e0a\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u91cf\u5316\u65b9\u6cd5\u4e2d\u975e\u53ef\u5fae\u5206\u6027\u548c\u6fc0\u6d3b\u91cf\u5316\u4e0e\u6743\u91cd\u91cf\u5316\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u91cf\u5316\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u53ef\u5fae\u5206\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6bd4\u7279\u91cf\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u8bc1\u660e\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u5bf9\u6743\u91cd\u8fdb\u884c\u91cf\u5316\u65f6\uff0c\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e1%\uff1b\u540c\u65f6\u5bf9\u6743\u91cd\u548c\u6fc0\u6d3b\u8fdb\u884c\u91cf\u5316\u65f6\uff0c\u6027\u80fd\u4e0eSOTA\u65b9\u6cd5\u76f8\u5f53\uff0c\u8bad\u7ec3\u4ec5\u970015\u4e2aepoch\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u53ef\u5fae\u5206\u91cf\u5316\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2510.16344", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16344", "abs": "https://arxiv.org/abs/2510.16344", "authors": ["Chenrui Tie", "Shengxiang Sun", "Yudi Lin", "Yanbo Wang", "Zhongrui Li", "Zhouhan Zhong", "Jinxuan Zhu", "Yiman Pang", "Haonan Chen", "Junting Chen", "Ruihai Wu", "Lin Shao"], "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models", "comment": null, "summary": "Assembly hinges on reliably forming connections between parts; yet most\nrobotic approaches plan assembly sequences and part poses while treating\nconnectors as an afterthought. Connections represent the critical \"last mile\"\nof assembly execution, while task planning may sequence operations and motion\nplan may position parts, the precise establishment of physical connections\nultimately determines assembly success or failure. In this paper, we consider\nconnections as first-class primitives in assembly representation, including\nconnector types, specifications, quantities, and placement locations. Drawing\ninspiration from how humans learn assembly tasks through step-by-step\ninstruction manuals, we present Manual2Skill++, a vision-language framework\nthat automatically extracts structured connection information from assembly\nmanuals. We encode assembly tasks as hierarchical graphs where nodes represent\nparts and sub-assemblies, and edges explicitly model connection relationships\nbetween components. A large-scale vision-language model parses symbolic\ndiagrams and annotations in manuals to instantiate these graphs, leveraging the\nrich connection knowledge embedded in human-designed instructions. We curate a\ndataset containing over 20 assembly tasks with diverse connector types to\nvalidate our representation extraction approach, and evaluate the complete task\nunderstanding-to-execution pipeline across four complex assembly scenarios in\nsimulation, spanning furniture, toys, and manufacturing components with\nreal-world correspondence.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faManual2Skill++\u6846\u67b6\uff0c\u5c06\u8fde\u63a5\u4f5c\u4e3a\u7ec4\u88c5\u7684\u9996\u8981\u5143\u7d20\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4ece\u624b\u518c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u9a8c\u8bc1\u4e86\u5728\u590d\u6742\u7ec4\u88c5\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7ec4\u88c5\u7684\u6210\u529f\u4e0e\u5426\u5f80\u5f80\u53d6\u51b3\u4e8e\u8fde\u63a5\u7684\u7cbe\u786e\u5efa\u7acb\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u4eba\u7ec4\u88c5\u65b9\u6cd5\u901a\u5e38\u5c06\u8fde\u63a5\u89c6\u4e3a\u6b21\u8981\u8003\u8651\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u8fde\u63a5\u4f5c\u4e3a\u7ec4\u88c5\u8868\u793a\u4e2d\u7684\u9996\u8981\u5143\u7d20\uff0c\u4ee5\u66f4\u53ef\u9760\u5730\u5b8c\u6210\u7ec4\u88c5\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86Manual2Skill++\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7ec4\u88c5\u624b\u518c\u4e2d\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u8fde\u63a5\u4fe1\u606f\uff0c\u5e76\u5c06\u7ec4\u88c5\u4efb\u52a1\u7f16\u7801\u4e3a\u5c42\u6b21\u5316\u56fe\u7ed3\u6784\uff0c\u5176\u4e2d\u8282\u70b9\u8868\u793a\u90e8\u4ef6\u548c\u5b50\u7ec4\u88c5\u4f53\uff0c\u8fb9\u660e\u786e\u5efa\u6a21\u7ec4\u4ef6\u95f4\u7684\u8fde\u63a5\u5173\u7cfb\u3002", "result": "\u5728\u5305\u542b20\u591a\u79cd\u7ec4\u88c5\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8868\u793a\u63d0\u53d6\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u56db\u4e2a\u590d\u6742\u7684\u7ec4\u88c5\u573a\u666f\uff08\u5bb6\u5177\u3001\u73a9\u5177\u548c\u5236\u9020\u7ec4\u4ef6\uff09\u4e2d\u8bc4\u4f30\u4e86\u4ece\u4efb\u52a1\u7406\u89e3\u5230\u6267\u884c\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8fde\u63a5\u4f5c\u4e3a\u7ec4\u88c5\u8868\u793a\u4e2d\u7684\u9996\u8981\u5143\u7d20\uff0c\u5e76\u5229\u7528Manual2Skill++\u6846\u67b6\u4ece\u7ec4\u88c5\u624b\u518c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8fde\u63a5\u4fe1\u606f\uff0c\u7814\u7a76\u6210\u529f\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u5728\u590d\u6742\u7ec4\u88c5\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17639", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17639", "abs": "https://arxiv.org/abs/2510.17639", "authors": ["Alkida Balliu", "Sebastian Brandt", "Ole Gabsdil", "Dennis Olivetti", "Jukka Suomela"], "title": "On the Universality of Round Elimination Fixed Points", "comment": null, "summary": "Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC\n2020] has drawn attention to the following open question: are round elimination\nfixed points a universal technique for proving lower bounds? That is, given a\nlocally checkable problem $\\Pi$ that requires at least $\\Omega(\\log n)$ rounds\nin the deterministic LOCAL model, can we always find a relaxation $\\Pi'$ of\n$\\Pi$ that is a nontrivial fixed point for the round elimination technique [see\nSTOC 2016, PODC 2019]? If yes, then a key part of distributed computational\ncomplexity would be also decidable.\n  The key obstacle so far has been a certain family of homomorphism problems\n[ITCS 2022], which require $\\Omega(\\log n)$ rounds, but the only known proof is\nbased on Marks' technique [J.AMS 2016].\n  We develop a new technique for constructing round elimination lower bounds\nsystematically. Using so-called tripotent inputs we show that the\naforementioned homomorphism problems indeed admit a lower bound proof that is\nbased on round elimination fixed points. Hence we eliminate the only known\nobstacle for the universality of round elimination.\n  Yet we also present a new obstacle: we show that there are some problems with\ninputs that require $\\Omega(\\log n)$ rounds, yet there is no proof that is\nbased on relaxations to nontrivial round elimination fixed points. Hence round\nelimination cannot be a universal technique for problems with inputs (but it\nmight be universal for problems without inputs).\n  We also prove the first fully general lower bound theorem that is applicable\nto any problem, with or without inputs, that is a fixed point in round\nelimination. Prior results of this form were only able to handle certain very\nrestricted inputs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8f6e\u6b21\u6d88\u9664\u6280\u672f\u662f\u5426\u9002\u7528\u4e8e\u6240\u6709\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u4e0b\u754c\u8bc1\u660e\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u4f46\u5bf9\u4e8e\u6709\u8f93\u5165\u7684\u95ee\u9898\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u901a\u7528\u4e0b\u754c\u5b9a\u7406\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u8f6e\u6b21\u6d88\u9664\u56fa\u5b9a\u70b9\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u8bc1\u660e\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u4e0b\u754c\u7684\u901a\u7528\u6280\u672f\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u52bf\u8f93\u5165\u7684\u65b0\u6280\u672f\uff0c\u7cfb\u7edf\u5730\u6784\u5efa\u8f6e\u6b21\u6d88\u9664\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u4e86\u67d0\u4e9b\u540c\u6001\u95ee\u9898\u786e\u5b9e\u53ef\u4ee5\u901a\u8fc7\u8f6e\u6b21\u6d88\u9664\u56fa\u5b9a\u70b9\u8bc1\u660e\u4e0b\u754c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8f6e\u6b21\u6d88\u9664\u6280\u672f\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u662f\u6709\u6548\u7684\uff0c\u4f46\u5bf9\u4e8e\u6709\u8f93\u5165\u7684\u95ee\u9898\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u9002\u7528\u4e8e\u4efb\u4f55\u95ee\u9898\u7684\u901a\u7528\u4e0b\u754c\u5b9a\u7406\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u8f6e\u6b21\u6d88\u9664\u6280\u672f\u867d\u7136\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8bc1\u660e\u4e0b\u754c\uff0c\u4f46\u5e76\u975e\u666e\u904d\u9002\u7528\u4e8e\u6240\u6709\u95ee\u9898\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u6709\u8f93\u5165\u7684\u95ee\u9898\uff0c\u5b58\u5728\u65e0\u6cd5\u901a\u8fc7\u8f6e\u6b21\u6d88\u9664\u56fa\u5b9a\u70b9\u8bc1\u660e\u4e0b\u754c\u7684\u60c5\u51b5\u3002"}}
{"id": "2510.16678", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16678", "abs": "https://arxiv.org/abs/2510.16678", "authors": ["Feyza Duman Keles", "Lisa Hellerstein", "Kunal Marwaha", "Christopher Musco", "Xinchen Yang"], "title": "An Exact Algorithm for the Unanimous Vote Problem", "comment": "1+23+31 pages, 5 figures", "summary": "Consider $n$ independent, biased coins, each with a known probability of\nheads. Presented with an ordering of these coins, flip (i.e., toss) each coin\nonce, in that order, until we have observed both a *head* and a *tail*, or\nflipped all coins. The Unanimous Vote problem asks us to find the ordering that\nminimizes the expected number of flips. Gkenosis et al. [arXiv:1806.10660] gave\na polynomial-time $\\phi$-approximation algorithm for this problem, where $\\phi\n\\approx 1.618$ is the golden ratio. They left open whether the problem was\nNP-hard. We answer this question by giving an exact algorithm that runs in time\n$O(n \\log n)$. The Unanimous Vote problem is an instance of the more general\nStochastic Boolean Function Evaluation problem: it thus becomes one of the only\nsuch problems known to be solvable in polynomial time. Our proof uses simple\ninterchange arguments to show that the optimal ordering must be close to the\nordering produced by a natural greedy algorithm. Beyond our main result, we\ncompare the optimal ordering with the best adaptive strategy, proving a tight\nadaptivity gap of $1.2\\pm o(1)$ for the Unanimous Vote problem.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86Unanimous Vote\u95ee\u9898\u7684NP\u96be\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2aO(n log n)\u65f6\u95f4\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4e0e\u81ea\u9002\u5e94\u7b56\u7565\u7684\u9002\u5e94\u6027\u5dee\u8ddd\u4e3a1.2\u00b1o(1)\u3002", "motivation": "\u89e3\u51b3Gkenosis\u7b49\u4eba\u63d0\u51fa\u7684Unanimous Vote\u95ee\u9898\u662f\u5426NP\u96be\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5176\u5728Stochastic Boolean Function Evaluation\u95ee\u9898\u4e2d\u7684\u4f4d\u7f6e\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u4ea4\u6362\u53c2\u6570\u8bc1\u660e\u6700\u4f18\u6392\u5e8f\u5fc5\u987b\u63a5\u8fd1\u81ea\u7136\u8d2a\u5a6a\u7b97\u6cd5\u751f\u6210\u7684\u6392\u5e8f\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cbe\u786e\u7684O(n log n)\u65f6\u95f4\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cbe\u786e\u7684O(n log n)\u65f6\u95f4\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86Unanimous Vote\u95ee\u9898\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u6700\u4f18\u6392\u5e8f\u4e0e\u6700\u4f73\u81ea\u9002\u5e94\u7b56\u7565\u4e4b\u95f4\u7684\u9002\u5e94\u6027\u5dee\u8ddd\u4e3a1.2\u00b1o(1)\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86Unanimous Vote\u95ee\u9898\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u7cbe\u786e\u7684O(n log n)\u65f6\u95f4\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u95ee\u9898\u5728Stochastic Boolean Function Evaluation\u95ee\u9898\u4e2d\u7684\u7279\u6b8a\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u6700\u4f18\u6392\u5e8f\u4e0e\u6700\u4f73\u81ea\u9002\u5e94\u7b56\u7565\u4e4b\u95f4\u7684\u7d27\u5bc6\u9002\u5e94\u6027\u5dee\u8ddd\u4e3a1.2\u00b1o(1)\u3002"}}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendk\u00fbuni C. Ou\u00e9draogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers.", "AI": {"tldr": "CoReEval\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f00\u53d1\u8005\u5f15\u5bfc\u63d0\u793a\u63d0\u9ad8\u4e86\u5bf9\u9f50\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6307\u6807\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u5224\u65ad\u7684\u4e3b\u89c2\u6027\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0cLLM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u884c\u4e3a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86CoReEval\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\uff0c\u5305\u62ec\u8d85\u8fc7140\u4e07\u4e2a\u6a21\u578b-\u7247\u6bb5-\u63d0\u793a\u8bc4\u4f30\uff0c\u8986\u76d610\u79cd\u6700\u5148\u8fdb\u7684LLM\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u4eba\u7c7b\u5b9a\u4e49\u7684\u53ef\u8bfb\u6027\u7ef4\u5ea6\u7684\u5f00\u53d1\u8005\u5f15\u5bfc\u63d0\u793a\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u5bf9\u9f50\u6027\uff0c\u589e\u5f3a\u4e86\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u89d2\u8272\u6846\u67b6\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u4e2a\u6027\u5316\u3002", "conclusion": "CoReEval\u4e3aLLM\u5728\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u6559\u80b2\u3001\u5165\u804c\u548cCI/CD\u7ba1\u9053\u4e2d\uff0cLLM\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u89e3\u91ca\u3001\u9002\u5e94\u6027\u5f3a\u7684\u8bc4\u5ba1\u8005\u3002"}}
{"id": "2510.16684", "categories": ["cs.GR", "cs.CV", "I.3"], "pdf": "https://arxiv.org/pdf/2510.16684", "abs": "https://arxiv.org/abs/2510.16684", "authors": ["Devin Zhao", "Rephael Wenger"], "title": "Filtering of Small Components for Isosurface Generation", "comment": "8 pages, 6 figures, 5 tables", "summary": "Let $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$ be a scalar field. An isosurface\nis a piecewise linear approximation of a level set $f^{-1}(\\sigma)$ for some\n$\\sigma \\in \\mathbb{R}$ built from some regular grid sampling of $f$.\nIsosurfaces constructed from scanned data such as CT scans or MRIs often\ncontain extremely small components that distract from the visualization and do\nnot form part of any geometric model produced from the data. Simple\nprefiltering of the data can remove such small components while having no\neffect on the large components that form the body of the visualization. We\npresent experimental results on such filtering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u9884\u5904\u7406\u8fc7\u6ee4\u53bb\u9664\u626b\u63cf\u6570\u636e\u7b49\u503c\u9762\u4e2d\u7684\u5fae\u5c0f\u6210\u5206\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u626b\u63cf\u6570\u636e\uff08\u5982CT\u6216MRI\uff09\u751f\u6210\u7684\u7b49\u503c\u9762\u5e38\u5305\u542b\u5fae\u5c0f\u6210\u5206\uff0c\u5e72\u6270\u53ef\u89c6\u5316\u4e14\u65e0\u52a9\u4e8e\u51e0\u4f55\u6a21\u578b\u7684\u6784\u5efa\u3002", "method": "\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u9884\u5904\u7406\u8fc7\u6ee4\u65b9\u6cd5\u53bb\u9664\u5fae\u5c0f\u7b49\u503c\u9762\u6210\u5206\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u9884\u5904\u7406\u8fc7\u6ee4\u80fd\u6709\u6548\u53bb\u9664\u5fae\u5c0f\u6210\u5206\uff0c\u4fdd\u7559\u4e3b\u8981\u53ef\u89c6\u5316\u90e8\u5206\u3002", "conclusion": "\u9884\u5904\u7406\u8fc7\u6ee4\u80fd\u6709\u6548\u53bb\u9664\u626b\u63cf\u6570\u636e\u4e2d\u7684\u5fae\u5c0f\u7b49\u503c\u9762\u6210\u5206\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u4e3b\u8981\u53ef\u89c6\u5316\u90e8\u5206\u3002"}}
{"id": "2510.16115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16115", "abs": "https://arxiv.org/abs/2510.16115", "authors": ["Jianhan Lin", "Yuchu Qin", "Shuai Gao", "Yikang Rui", "Jie Liu", "Yanjie Lv"], "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection", "comment": null, "summary": "Well-maintained road networks are crucial for achieving Sustainable\nDevelopment Goal (SDG) 11. Road surface damage not only threatens traffic\nsafety but also hinders sustainable urban development. Accurate detection,\nhowever, remains challenging due to the diverse shapes of damages, the\ndifficulty of capturing slender cracks with high aspect ratios, and the high\nerror rates in small-scale damage recognition. To address these issues, we\npropose StripRFNet, a novel deep neural network comprising three modules: (1) a\nShape Perception Module (SPM) that enhances shape discrimination via large\nseparable kernel attention (LSKA) in multi-scale feature aggregation; (2) a\nStrip Receptive Field Module (SRFM) that employs large strip convolutions and\npooling to capture features of slender cracks; and (3) a Small-Scale\nEnhancement Module (SSEM) that leverages a high-resolution P2 feature map, a\ndedicated detection head, and dynamic upsampling to improve small-object\ndetection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses\nexisting methods. On the Chinese subset, it improves F1-score, mAP50, and\nmAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,\nrespectively. On the full dataset, it achieves the highest F1-score of 80.33%\ncompared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while\nmaintaining competitive inference speed. These results demonstrate that\nStripRFNet achieves state-of-the-art accuracy and real-time efficiency,\noffering a promising tool for intelligent road maintenance and sustainable\ninfrastructure management.", "AI": {"tldr": "StripRFNet\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u89e3\u51b3\u4e86\u9053\u8def\u635f\u4f24\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u5728RDD2022\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6548\u7387\u3002", "motivation": "\u9053\u8def\u8868\u9762\u635f\u4f24\u5a01\u80c1\u4ea4\u901a\u5b89\u5168\u5e76\u963b\u788d\u53ef\u6301\u7eed\u57ce\u5e02\u53d1\u5c55\uff0c\u4f46\u51c6\u786e\u68c0\u6d4b\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5982\u635f\u4f24\u5f62\u72b6\u591a\u6837\u3001\u7ec6\u957f\u88c2\u7f1d\u96be\u4ee5\u6355\u6349\u548c\u5c0f\u89c4\u6a21\u635f\u4f24\u8bc6\u522b\u9519\u8bef\u7387\u9ad8\u3002", "method": "StripRFNet\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u5f62\u72b6\u611f\u77e5\u6a21\u5757\uff08SPM\uff09\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u589e\u5f3a\u5f62\u72b6\u533a\u5206\uff1b\u6761\u72b6\u611f\u53d7\u91ce\u6a21\u5757\uff08SRFM\uff09\u6355\u83b7\u7ec6\u957f\u88c2\u7f1d\u7279\u5f81\uff1b\u5c0f\u89c4\u6a21\u589e\u5f3a\u6a21\u5757\uff08SSEM\uff09\u63d0\u9ad8\u5c0f\u7269\u4f53\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728RDD2022\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStripRFNet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e2d\u56fd\u5b50\u96c6\u4e0aF1\u5206\u6570\u3001mAP50\u548cmAP50:95\u5206\u522b\u63d0\u9ad8\u4e864.4\u30012.9\u548c3.4\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5168\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u8fbe\u523080.33%\u3002", "conclusion": "StripRFNet\u5728\u9053\u8def\u8868\u9762\u635f\u4f24\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u9053\u8def\u7ef4\u62a4\u548c\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2510.16424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16424", "abs": "https://arxiv.org/abs/2510.16424", "authors": ["Dan Guo", "Xibin Jin", "Shuai Wang", "Zhigang Wen", "Miaowen Wen", "Chengzhong Xu"], "title": "Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach", "comment": null, "summary": "Edge robotics involves frequent exchanges of large-volume multi-modal data.\nExisting methods ignore the interdependency between robotic functionalities and\ncommunication conditions, leading to excessive communication overhead. This\npaper revolutionizes edge robotics systems through integrated perception,\nmotion, and communication (IPMC). As such, robots can dynamically adapt their\ncommunication strategies (i.e., compression ratio, transmission frequency,\ntransmit power) by leveraging the knowledge of robotic perception and motion\ndynamics, thus reducing the need for excessive sensor data uploads.\nFurthermore, by leveraging the learning to optimize (LTO) paradigm, an\nimitation learning neural network is designed and implemented, which reduces\nthe computational complexity by over 10x compared to state-of-the art\noptimization solvers. Experiments demonstrate the superiority of the proposed\nIPMC and the real-time execution capability of LTO.", "AI": {"tldr": "IPMC\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u901a\u4fe1\u7b56\u7565\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0cLTO\u964d\u4f4e\u4e8610\u500d\u4ee5\u4e0a\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u673a\u5668\u4eba\u529f\u80fd\u4e0e\u901a\u4fe1\u6761\u4ef6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u3001\u8fd0\u52a8\u548c\u901a\u4fe1\uff08IPMC\uff09\uff0c\u52a8\u6001\u8c03\u6574\u901a\u4fe1\u7b56\u7565\uff08\u5982\u538b\u7f29\u6bd4\u3001\u4f20\u8f93\u9891\u7387\u3001\u53d1\u5c04\u529f\u7387\uff09\uff0c\u5e76\u5229\u7528\u5b66\u4e60\u4f18\u5316\uff08LTO\uff09\u8303\u5f0f\u8bbe\u8ba1\u6a21\u4eff\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIPMC\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cLTO\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u6bd4\u6700\u5148\u8fdb\u7684\u4f18\u5316\u6c42\u89e3\u5668\u964d\u4f4e\u4e8610\u500d\u4ee5\u4e0a\u3002", "conclusion": "IPMC\u548cLTO\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2510.16741", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16741", "abs": "https://arxiv.org/abs/2510.16741", "authors": ["Yotam Kenneth-Mordoch", "Robert Krauthgamer"], "title": "All-Pairs Minimum Cut using $\\tilde{O}(n^{7/4})$ Cut Queries", "comment": null, "summary": "We present the first non-trivial algorithm for the all-pairs minimum cut\nproblem in the cut-query model. Given cut-query access to an unweighted graph\n$G=(V,E)$ with $n$ vertices, our randomized algorithm constructs a Gomory-Hu\ntree of $G$, and thus solves the all-pairs minimum cut problem, using\n$\\tilde{O}(n^{7/4})$ cut queries.", "AI": {"tldr": "\u9996\u6b21\u5728\u5272\u67e5\u8be2\u6a21\u578b\u4e2d\u63d0\u51fa\u5168\u5bf9\u6700\u5c0f\u5272\u7684\u975e\u5e73\u51e1\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaGomory-Hu\u6811\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u3002", "motivation": "\u89e3\u51b3\u5728\u5272\u67e5\u8be2\u6a21\u578b\u4e2d\u5168\u5bf9\u6700\u5c0f\u5272\u95ee\u9898\u7684\u975e\u5e73\u51e1\u7b97\u6cd5\u7f3a\u5931\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u968f\u673a\u7b97\u6cd5\u6784\u5efaGomory-Hu\u6811\uff0c\u5e76\u5728\u5272\u67e5\u8be2\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86$\tilde{O}(n^{7/4})$\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5272\u67e5\u8be2\u6a21\u578b\u4e2d\u6210\u529f\u6784\u5efa\u4e86Gomory-Hu\u6811\uff0c\u89e3\u51b3\u4e86\u5168\u5bf9\u6700\u5c0f\u5272\u95ee\u9898\uff0c\u67e5\u8be2\u590d\u6742\u5ea6\u4e3a$\tilde{O}(n^{7/4})$\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5272\u67e5\u8be2\u6a21\u578b\u4e2d\u7684\u975e\u5e73\u51e1\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5168\u5bf9\u6700\u5c0f\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efaGomory-Hu\u6811\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5272\u67e5\u8be2\u3002"}}
{"id": "2510.16665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16665", "abs": "https://arxiv.org/abs/2510.16665", "authors": ["Mohamed Sami Rakha", "Andriy Miranskyy", "Daniel Alencar da Costa"], "title": "Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios", "comment": "Accepted to IEEE Transactions on Software Engineering", "summary": "Software defect prediction (SDP) is crucial for delivering high-quality\nsoftware products. Recent research has indicated that prediction performance\nimprovements in SDP are achievable by applying hyperparameter tuning to a\nparticular SDP scenario. However, the positive impact resulting from the\nhyperparameter tuning step may differ based on the targeted SDP scenario.\nComparing the impact of hyperparameter tuning across SDP scenarios is necessary\nto provide comprehensive insights and enhance the robustness, generalizability,\nand, eventually, the practicality of SDP modeling for quality assurance.\n  Therefore, in this study, we contrast the impact of hyperparameter tuning\nacross two pivotal and consecutive SDP scenarios: (1) Inner Version Defect\nPrediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main\ndistinctions between the two scenarios lie in the scope of defect prediction\nand the selected evaluation setups. This study's experiments use common\nevaluation setups, 28 machine learning (ML) algorithms, 53 post-release\nsoftware datasets, two tuning algorithms, and five optimization metrics. We\napply statistical analytics to compare the SDP performance impact differences\nby investigating the overall impact, the single ML algorithm impact, and\nvariations across different software dataset sizes.\n  The results indicate that the SDP gains within the IVDP scenario are\nsignificantly larger than those within the CVDP scenario. The results reveal\nthat asserting performance gains for up to 24 out of 28 ML algorithms may not\nhold across multiple SDP scenarios. Furthermore, we found that small software\ndatasets are more susceptible to larger differences in performance impacts.\nOverall, the study findings recommend software engineering researchers and\npractitioners to consider the effect of the selected SDP scenario when\nexpecting performance gains from hyperparameter tuning.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86IVDP\u548cCVDP\u4e24\u79cdSDP\u573a\u666f\u4e0b\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5f71\u54cd\uff0c\u53d1\u73b0IVDP\u7684\u6027\u80fd\u589e\u76ca\u66f4\u5927\uff0c\u4e14\u5c0f\u578b\u6570\u636e\u96c6\u5bf9\u6027\u80fd\u5dee\u5f02\u66f4\u654f\u611f\u3002\u5efa\u8bae\u5728\u9009\u62e9SDP\u573a\u666f\u65f6\u8003\u8651\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5f71\u54cd\u3002", "motivation": "\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u7279\u5b9aSDP\u573a\u666f\u7684\u9884\u6d4b\u6027\u80fd\u6709\u63d0\u5347\u4f5c\u7528\uff0c\u4f46\u5176\u79ef\u6781\u5f71\u54cd\u53ef\u80fd\u56e0\u76ee\u6807\u573a\u666f\u800c\u5f02\u3002\u6bd4\u8f83\u4e0d\u540c\u573a\u666f\u4e0b\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5f71\u54cd\u6709\u52a9\u4e8e\u63d0\u4f9b\u5168\u9762\u89c1\u89e3\uff0c\u589e\u5f3aSDP\u5efa\u6a21\u7684\u7a33\u5065\u6027\u3001\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\u6bd4\u8f83\u4e86\u4e24\u79cdSDP\u573a\u666f\uff08IVDP\u548cCVDP\uff09\u4e0b\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u4e8628\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u300153\u4e2a\u53d1\u5e03\u540e\u8f6f\u4ef6\u6570\u636e\u96c6\u3001\u4e24\u79cd\u8c03\u4f18\u7b97\u6cd5\u548c\u4e94\u79cd\u4f18\u5316\u6307\u6807\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cIVDP\u573a\u666f\u4e0b\u7684SDP\u589e\u76ca\u663e\u8457\u5927\u4e8eCVDP\u573a\u666f\u3002\u6b64\u5916\uff0c24/28\u7684ML\u7b97\u6cd5\u5728\u591a\u4e2aSDP\u573a\u666f\u4e2d\u53ef\u80fd\u65e0\u6cd5\u4fdd\u6301\u6027\u80fd\u589e\u76ca\uff0c\u4e14\u5c0f\u578b\u8f6f\u4ef6\u6570\u636e\u96c6\u66f4\u5bb9\u6613\u53d7\u5230\u6027\u80fd\u5f71\u54cd\u5dee\u5f02\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5efa\u8bae\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u5728\u671f\u5f85\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u65f6\uff0c\u5e94\u8003\u8651\u6240\u9009\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\uff08SDP\uff09\u573a\u666f\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.16966", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16966", "abs": "https://arxiv.org/abs/2510.16966", "authors": ["Paascal Grosset", "James Ahrens"], "title": "A Scalable In Transit Solution for Comprehensive Exploration of Simulation Data", "comment": null, "summary": "As simulations produce more data than available disk space on supercomputers,\nmany simulations are employing in situ analysis and visualization to reduce the\namount of data that needs to be stored. While in situ visualization offers\npotential for substantial data reduction, its efficacy is hindered by the need\nfor a priori knowledge. First, we need to know what visualization parameters to\nuse to highlight features of interest. Second, we do not know ahead of time how\nmuch resources will be needed to run the in situ workflows, e.g. how many\ncompute nodes will be needed for in situ work. In this work, we present SeerX,\na lightweight, scalable in-transit in situ service that supports dynamic\nresource allocation and lossy compression of 3D simulation data. SeerX enables\nmultiple simulations to offload analysis to a shared, elastic service\ninfrastructure without MPI synchronization.", "AI": {"tldr": "SeerX\u662f\u4e00\u79cd\u65b0\u578b\u7684\u4f20\u8f93\u4e2d\u73b0\u573a\u670d\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u573a\u5206\u6790\u4e2d\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u652f\u6301\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u6570\u636e\u538b\u7f29\u3002", "motivation": "\u968f\u7740\u6a21\u62df\u4ea7\u751f\u7684\u6570\u636e\u91cf\u8d85\u8fc7\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u53ef\u7528\u78c1\u76d8\u7a7a\u95f4\uff0c\u73b0\u573a\u5206\u6790\u548c\u53ef\u89c6\u5316\u6210\u4e3a\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u4f46\u5176\u6548\u679c\u53d7\u9650\u4e8e\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SeerX\uff0c\u4e00\u79cd\u652f\u6301\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c3D\u6570\u636e\u6709\u635f\u538b\u7f29\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u4f20\u8f93\u4e2d\u73b0\u573a\u670d\u52a1\u3002", "result": "SeerX\u80fd\u591f\u5728\u4e0d\u8fdb\u884cMPI\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\uff0c\u652f\u6301\u591a\u4e2a\u6a21\u62df\u5c06\u5206\u6790\u4efb\u52a1\u5378\u8f7d\u5230\u5171\u4eab\u7684\u5f39\u6027\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u4e0a\u3002", "conclusion": "SeerX\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u4f20\u8f93\u4e2d\u73b0\u573a\u670d\u52a1\uff0c\u652f\u6301\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c3D\u6a21\u62df\u6570\u636e\u7684\u6709\u635f\u538b\u7f29\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u73b0\u573a\u5206\u6790\u4e2d\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\u3002"}}
{"id": "2510.16118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16118", "abs": "https://arxiv.org/abs/2510.16118", "authors": ["Nishad Sahu", "Shounak Sural", "Aditya Satish Patil", "Ragunathan", "Rajkumar"], "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025\n  Workshops", "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.", "AI": {"tldr": "ObjectTransforms\u901a\u8fc7\u5bf9\u8c61\u7279\u5b9a\u53d8\u6362\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u91cf\u5316\u5e76\u51cf\u5c11\u89c6\u89c9\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u6613\u53d7\u6570\u636e\u504f\u5dee\u548c\u5206\u5e03\u504f\u79fb\u7b49\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\uff0c\u5f71\u54cd\u5b89\u5168\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "method": "\u5728\u8bad\u7ec3\u65f6\uff0cObjectTransforms\u901a\u8fc7\u5bf9\u5355\u4e2a\u5bf9\u8c61\u8fdb\u884c\u8272\u5f69\u7a7a\u95f4\u6270\u52a8\uff0c\u589e\u5f3a\u5bf9\u5149\u7167\u548c\u989c\u8272\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff1b\u540c\u65f6\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u884c\u4eba\u5b9e\u4f8b\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u5e94\u7528\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u68c0\u6d4b\u5206\u6570\u7684\u65b9\u5dee\u5b9e\u65f6\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728NuImages 10K\u6570\u636e\u96c6\u4e0a\u4f7f\u7528YOLOv8\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u76ee\u6807\u7c7b\u522b\u4e0a\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u5047\u9633\u6027\u9884\u6d4b\u4e86\u66f4\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\u503c\u3002", "conclusion": "ObjectTransforms\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5206\u522b\u51cf\u5c11\u548c\u91cf\u5316\u89c6\u89c9\u611f\u77e5\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2510.16435", "categories": ["cs.RO", "cs.CL", "cs.HC", "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16435", "abs": "https://arxiv.org/abs/2510.16435", "authors": ["Lennart Wachowiak", "Andrew Coles", "Gerard Canal", "Oya Celiktutan"], "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics", "comment": null, "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b1,893\u4e2a\u7528\u6237\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5e2e\u52a9\u673a\u5668\u4eba\u66f4\u597d\u5730\u56de\u7b54\u591a\u6837\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u590d\u6742\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002\u6570\u636e\u96c6\u8fd8\u63ed\u793a\u4e86\u65b0\u624b\u548c\u7ecf\u9a8c\u7528\u6237\u5728\u63d0\u95ee\u7c7b\u578b\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8bdd\u754c\u9762\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u673a\u5668\u4eba\u56de\u7b54\u7528\u6237\u95ee\u9898\u7684\u80fd\u529b\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u53ef\u89e3\u91ca\u673a\u5668\u4eba\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u2018\u4e3a\u4ec0\u4e48\u2019\u95ee\u9898\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u95ee\u9898\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u521b\u5efa15\u4e2a\u89c6\u9891\u523a\u6fc0\u548c7\u4e2a\u6587\u672c\u523a\u6fc0\uff0c\u63cf\u7ed8\u673a\u5668\u4eba\u6267\u884c\u5404\u79cd\u5bb6\u52a1\u4efb\u52a1\uff0c\u7136\u540e\u5728Prolific\u5e73\u53f0\u4e0a\u6536\u96c6100\u540d\u53c2\u4e0e\u8005\u5bf9\u6bcf\u4e2a\u60c5\u5883\u4e2d\u53ef\u80fd\u63d0\u51fa\u7684\u95ee\u9898\u3002\u6700\u7ec8\u6570\u636e\u96c6\u5305\u542b1,893\u4e2a\u7528\u6237\u95ee\u9898\uff0c\u5206\u4e3a12\u4e2a\u7c7b\u522b\u548c70\u4e2a\u5b50\u7c7b\u522b\u3002", "result": "\u6570\u636e\u96c6\u4e2d\u6700\u5e38\u89c1\u7684\u7c7b\u522b\u662f\u5173\u4e8e\u4efb\u52a1\u6267\u884c\u7ec6\u8282\uff0822.5%\uff09\u3001\u673a\u5668\u4eba\u80fd\u529b\uff0812.7%\uff09\u548c\u6027\u80fd\u8bc4\u4f30\uff0811.3%\uff09\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5173\u4e8e\u673a\u5668\u4eba\u5982\u4f55\u5904\u7406\u590d\u6742\u573a\u666f\u7684\u95ee\u9898\u8f83\u5c11\uff0c\u4f46\u7528\u6237\u8ba4\u4e3a\u8fd9\u4e9b\u662f\u6700\u91cd\u8981\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u65b0\u624b\u548c\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7528\u6237\u5728\u63d0\u95ee\u7c7b\u578b\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u7840\uff0c\u5e2e\u52a9\u8bc6\u522b\u673a\u5668\u4eba\u9700\u8981\u8bb0\u5f55\u548c\u66b4\u9732\u7ed9\u5bf9\u8bdd\u754c\u9762\u7684\u4fe1\u606f\uff0c\u8bc4\u4f30\u95ee\u7b54\u6a21\u5757\u7684\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u89e3\u91ca\u7b56\u7565\u3002"}}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "VisuoAlign\u662f\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u5b9e\u73b0\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LVLMs\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u591a\u6a21\u6001\u611f\u77e5\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u5bf9\u9f50\u4ecd\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u6613\u53d7\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\uff0c\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u4e14\u5bf9\u9f50\u5728\u591a\u6a21\u6001\u878d\u5408\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86VisuoAlign\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5d4c\u5165\u5b89\u5168\u7ea6\u675f\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7cfb\u7edf\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u548c\u5408\u89c4\u54cd\u5e94\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVisuoAlign\u80fd\u4e3b\u52a8\u66b4\u9732\u98ce\u9669\u3001\u751f\u6210\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u5e76\u663e\u8457\u63d0\u5347LVLMs\u5bf9\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VisuoAlign\u901a\u8fc7\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\u3001\u5229\u7528MCTS\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86LVLMs\u5bf9\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17182", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17182", "abs": "https://arxiv.org/abs/2510.17182", "authors": ["Aaron Bernstein", "Joakim Blikstad", "Jason Li", "Thatchaphol Saranurak", "Ta-Wei Tu"], "title": "Combinatorial Maximum Flow via Weighted Push-Relabel on Shortcut Graphs", "comment": null, "summary": "We give a combinatorial algorithm for computing exact maximum flows in\ndirected graphs with $n$ vertices and edge capacities from $\\{1,\\dots,U\\}$ in\n$\\tilde{O}(n^{2}\\log U)$ time, which is near-optimal on dense graphs. This\nshaves an $n^{o(1)}$ factor from the recent result of\n[Bernstein-Blikstad-Saranurak-Tu FOCS'24] and, more importantly, greatly\nsimplifies their algorithm. We believe that ours is by a significant margin the\nsimplest of all algorithms that go beyond $\\tilde{O}(m\\sqrt{n})$ time in\ngeneral graphs. To highlight this relative simplicity, we provide a full\nimplementation of the algorithm in C++.\n  The only randomized component of our work is the cut-matching game. Via\nexisting tools, we show how to derandomize it for vertex-capacitated max flow\nand obtain a deterministic $\\tilde{O}(n^2)$ time algorithm. This marks the\nfirst deterministic near-linear time algorithm for this problem (or even for\nthe special case of bipartite matching) in any density regime.", "AI": {"conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7a20\u5bc6\u56fe\u4e2d\u8fd1\u4e4e\u6700\u4f18\u7684\u7ec4\u5408\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u5177\u6709\u7279\u5b9a\u8fb9\u5bb9\u91cf\u7684\u6709\u5411\u56fe\u4e2d\u7684\u7cbe\u786e\u6700\u5927\u6d41\uff0c\u663e\u8457\u7b80\u5316\u4e86\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684C++\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u53bb\u968f\u673a\u5316\u6280\u672f\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u7b97\u6cd5\uff0c\u7ed3\u5408\u5207\u5272\u5339\u914d\u6e38\u620f\uff08cut-matching game\uff09\u7684\u968f\u673a\u5316\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u53bb\u968f\u673a\u5316\u6280\u672f\u5904\u7406\u9876\u70b9\u5bb9\u91cf\u6700\u5927\u6d41\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u73b0\u6709\u7b97\u6cd5\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7a20\u5bc6\u56fe\u4e2d\uff0c\u540c\u65f6\u63a2\u7d22\u786e\u5b9a\u6027\u7b97\u6cd5\u7684\u53ef\u80fd\u6027\u3002", "result": "\u5b9e\u73b0\u4e86$\\tilde{O}(n^{2}\\log U)$\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u7b97\u6cd5\uff0c\u7b80\u5316\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4e86\u786e\u5b9a\u6027$\\tilde{O}(n^2)$\u65f6\u95f4\u7b97\u6cd5\u3002", "tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u4e14\u9ad8\u6548\u7684\u7ec4\u5408\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u6709\u5411\u56fe\u4e2d\u7684\u6700\u5927\u6d41\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002"}}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs.", "AI": {"tldr": "QuanBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u4f4e\u4e14\u9519\u8bef\u591a\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u901a\u7528\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86QuanBench\uff0c\u4e00\u4e2a\u5305\u542b44\u4e2a\u7f16\u7a0b\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u91cf\u5b50\u7b97\u6cd5\u3001\u72b6\u6001\u51c6\u5907\u3001\u95e8\u5206\u89e3\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u3002\u6bcf\u4e2a\u4efb\u52a1\u901a\u8fc7\u529f\u80fd\u6b63\u786e\u6027\uff08Pass@K\uff09\u548c\u91cf\u5b50\u8bed\u4e49\u7b49\u4ef7\u6027\uff08Process Fidelity\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524dLLM\u751f\u6210\u6b63\u786e\u91cf\u5b50\u4ee3\u7801\u7684\u80fd\u529b\u6709\u9650\uff0c\u603b\u4f53\u51c6\u786e\u7387\u4f4e\u4e8e40%\uff0c\u4e14\u5e38\u89c1\u8bed\u4e49\u9519\u8bef\u3002", "conclusion": "QuanBench\u4e3a\u672a\u6765\u6539\u8fdbLLM\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17101", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17101", "abs": "https://arxiv.org/abs/2510.17101", "authors": ["Lu Yin", "Ziying Shi", "Yinghao Wu", "Xinyu Yi", "Feng Xu", "Shihui Guo"], "title": "Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors", "comment": "Accepted by SIGGRAPH Asia 2025 (TOG)", "summary": "Human motion capture with sparse inertial sensors has gained significant\nattention recently. However, existing methods almost exclusively rely on a\ntemplate adult body shape to model the training data, which poses challenges\nwhen generalizing to individuals with largely different body shapes (such as a\nchild). This is primarily due to the variation in IMU-measured acceleration\ncaused by changes in body shape. To fill this gap, we propose Shape-aware\nInertial Poser (SAIP), the first solution considering body shape differences in\nsparse inertial-based motion capture. Specifically, we decompose the sensor\nmeasurements related to shape and pose in order to effectively model their\njoint correlations. Firstly, we train a regression model to transfer the\nIMU-measured accelerations of a real body to match the template adult body\nmodel, compensating for the shape-related sensor measurements. Then, we can\neasily follow the state-of-the-art methods to estimate the full body motions of\nthe template-shaped body. Finally, we utilize a second regression model to map\nthe joint velocities back to the real body, combined with a shape-aware\nphysical optimization strategy to calculate global motions on the subject.\nFurthermore, our method relies on body shape awareness, introducing the first\ninertial shape estimation scheme. This is accomplished by modeling the\nshape-conditioned IMU-pose correlation using an MLP-based network. To validate\nthe effectiveness of SAIP, we also present the first IMU motion capture dataset\ncontaining individuals of different body sizes. This dataset features 10\nchildren and 10 adults, with heights ranging from 110 cm to 190 cm, and a total\nof 400 minutes of paired IMU-Motion samples. Extensive experimental results\ndemonstrate that SAIP can effectively handle motion capture tasks for diverse\nbody shapes. The code and dataset are available at\nhttps://github.com/yinlu5942/SAIP.", "AI": {"tldr": "SAIP\u662f\u4e00\u79cd\u8003\u8651\u4f53\u578b\u5dee\u5f02\u7684\u7a00\u758f\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5f62\u72b6\u548c\u59ff\u52bf\u76f8\u5173\u4f20\u611f\u5668\u6d4b\u91cf\uff0c\u5e76\u5229\u7528\u56de\u5f52\u6a21\u578b\u548c\u7269\u7406\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u4f53\u578b\u4e2a\u4f53\u7684\u8fd0\u52a8\u6355\u6349\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u677f\u6210\u4eba\u8eab\u4f53\u5f62\u72b6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4f53\u578b\u5dee\u5f02\u8f83\u5927\u7684\u4e2a\u4f53\uff08\u5982\u513f\u7ae5\uff09\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u8eab\u4f53\u5f62\u72b6\u53d8\u5316\u5bfc\u81f4\u7684IMU\u6d4b\u91cf\u52a0\u901f\u5ea6\u5dee\u5f02\u3002", "method": "SAIP\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u56de\u5f52\u6a21\u578b\uff0c\u5c06\u771f\u5b9e\u8eab\u4f53\u7684IMU\u6d4b\u91cf\u52a0\u901f\u5ea6\u8f6c\u6362\u4e3a\u6a21\u677f\u6210\u4eba\u8eab\u4f53\u6a21\u578b\uff0c\u8865\u507f\u5f62\u72b6\u76f8\u5173\u7684\u4f20\u611f\u5668\u6d4b\u91cf\u3002\u968f\u540e\uff0c\u5229\u7528\u73b0\u6709\u65b9\u6cd5\u4f30\u8ba1\u6a21\u677f\u5f62\u72b6\u8eab\u4f53\u7684\u5168\u8eab\u8fd0\u52a8\u3002\u6700\u540e\uff0c\u901a\u8fc7\u7b2c\u4e8c\u4e2a\u56de\u5f52\u6a21\u578b\u5c06\u5173\u8282\u901f\u5ea6\u6620\u5c04\u56de\u771f\u5b9e\u8eab\u4f53\uff0c\u5e76\u7ed3\u5408\u5f62\u72b6\u611f\u77e5\u7684\u7269\u7406\u4f18\u5316\u7b56\u7565\u8ba1\u7b97\u5168\u5c40\u8fd0\u52a8\u3002", "result": "SAIP\u5728\u5305\u542b\u4e0d\u540c\u4f53\u578b\u4e2a\u4f53\u7684IMU\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u5904\u7406\u4e86\u591a\u6837\u4f53\u578b\u7684\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u3002", "conclusion": "SAIP\u901a\u8fc7\u5206\u89e3\u4f20\u611f\u5668\u6d4b\u91cf\u4e2d\u4e0e\u5f62\u72b6\u548c\u59ff\u52bf\u76f8\u5173\u7684\u90e8\u5206\uff0c\u5e76\u5229\u7528\u56de\u5f52\u6a21\u578b\u548c\u7269\u7406\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u4f53\u578b\u4e2a\u4f53\u7684\u8fd0\u52a8\u6355\u6349\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u9996\u6b21\u5f15\u5165\u4e86\u60ef\u6027\u5f62\u72b6\u4f30\u8ba1\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "A2PD\u662f\u4e00\u4e2a\u901a\u8fc7Aria Gen 2\u773c\u955c\u6355\u6349\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e94\u79cd\u65e5\u5e38\u6d3b\u52a8\u573a\u666f\uff0c\u516c\u5f00\u63d0\u4f9b\u6570\u636e\u53ca\u5de5\u5177\uff0c\u652f\u6301\u591a\u6a21\u6001\u611f\u77e5\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u591a\u6a21\u6001\u611f\u77e5\u7814\u7a76\uff0c\u63d0\u4f9b\u53ca\u65f6\u7684\u6570\u636e\u8bbf\u95ee\uff0c\u5e76\u5c55\u793a\u8bbe\u5907\u5728\u4e0d\u540c\u7528\u6237\u548c\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u80fd\u3002", "method": "\u4f7f\u7528Aria Gen 2\u773c\u955c\u6355\u6349\u591a\u6a21\u6001\u6570\u636e\uff0c\u6db5\u76d6\u4e94\u79cd\u4e3b\u8981\u573a\u666f\uff08\u6e05\u6d01\u3001\u70f9\u996a\u3001\u8fdb\u98df\u3001\u73a9\u800d\u548c\u6237\u5916\u6b65\u884c\uff09\uff0c\u5e76\u63d0\u4f9b\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u53ca\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u8f93\u51fa\u6570\u636e\u3002", "result": "A2PD\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u573a\u666f\u6570\u636e\u548c\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u8f93\u51fa\uff0c\u5c55\u793a\u4e86\u8bbe\u5907\u5728\u611f\u77e5\u4f69\u6234\u8005\u3001\u73af\u5883\u548c\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "A2PD\u4f5c\u4e3a\u4e00\u4e2a\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u901a\u8fc7Aria Gen 2\u773c\u955c\u6355\u6349\u4e86\u4e30\u5bcc\u7684\u65e5\u5e38\u6d3b\u52a8\u573a\u666f\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5de5\u5177\u548c\u793a\u4f8b\uff0c\u652f\u6301\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u4f7f\u7528\u3002"}}
{"id": "2510.16500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16500", "abs": "https://arxiv.org/abs/2510.16500", "authors": ["Chen Min", "Jilin Mei", "Heng Zhai", "Shuai Wang", "Tong Sun", "Fanjie Kong", "Haoyang Li", "Fangyuan Mao", "Fuyang Liu", "Shuo Wang", "Yiming Nie", "Qi Zhu", "Liang Xiao", "Dawei Zhao", "Yu Hu"], "title": "Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks", "comment": "Off-road robotics", "summary": "A major bottleneck in off-road autonomous driving research lies in the\nscarcity of large-scale, high-quality datasets and benchmarks. To bridge this\ngap, we present ORAD-3D, which, to the best of our knowledge, is the largest\ndataset specifically curated for off-road autonomous driving. ORAD-3D covers a\nwide spectrum of terrains, including woodlands, farmlands, grasslands,\nriversides, gravel roads, cement roads, and rural areas, while capturing\ndiverse environmental variations across weather conditions (sunny, rainy,\nfoggy, and snowy) and illumination levels (bright daylight, daytime, twilight,\nand nighttime). Building upon this dataset, we establish a comprehensive suite\nof benchmark evaluations spanning five fundamental tasks: 2D free-space\ndetection, 3D occupancy prediction, rough GPS-guided path planning,\nvision-language model-driven autonomous driving, and world model for off-road\nenvironments. Together, the dataset and benchmarks provide a unified and robust\nresource for advancing perception and planning in challenging off-road\nscenarios. The dataset and code will be made publicly available at\nhttps://github.com/chaytonmin/ORAD-3D.", "AI": {"tldr": "ORAD-3D\u662f\u6700\u5927\u7684\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u6837\u5730\u5f62\u548c\u5929\u6c14\uff0c\u63d0\u4f9b\u4e94\u9879\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u7814\u7a76\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ORAD-3D\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u79cd\u5730\u5f62\u548c\u73af\u5883\u53d8\u5316\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b\u4e94\u9879\u57fa\u672c\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "ORAD-3D\u662f\u76ee\u524d\u6700\u5927\u7684\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u4e13\u7528\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u652f\u6301\u3002", "conclusion": "ORAD-3D\u6570\u636e\u96c6\u53ca\u5176\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u5f3a\u5927\u7684\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u611f\u77e5\u548c\u89c4\u5212\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "SCL\u662f\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u54f2\u5b66\u7406\u8bba\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u52a8\u6001\u7684\u8ba4\u77e5\u91cd\u5efa\u8fc7\u7a0b\uff0c\u4e3aAI\u548c\u54f2\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u867d\u8868\u73b0\u51fa\u667a\u80fd\u4f46\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\uff0c\u51f8\u663e\u4e86\u8ba4\u77e5\u67b6\u6784\u7684\u7f3a\u5931\u3002SCL\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u8ba4\u77e5\u6d8c\u73b0\u7684\u6761\u4ef6\u800c\u975e\u4ec5\u8ffd\u95ee\u667a\u80fd\u7684\u672c\u8d28\u3002", "method": "\u57fa\u4e8e\u5fc3\u667a\u54f2\u5b66\u548c\u8ba4\u77e5\u73b0\u8c61\u5b66\uff0c\u7ed3\u5408\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u5b9a\u4e49\u4e3a\u4e00\u79cd\u6267\u884c\u8fc7\u7a0b\u800c\u975e\u9759\u6001\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "SCL\u5b9e\u73b0\u4e86\u54f2\u5b66\u6d1e\u89c1\u7684\u53ef\u8ba1\u7b97\u5316\uff0c\u5c55\u793a\u4e86\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u6bd4\u5355\u4e00\u63d0\u793a\u7cfb\u7edf\u66f4\u5177\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e86\u667a\u80fd\u4e3a\u5bf9\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u610f\u5411\u6027\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "SCL\u6846\u67b6\u901a\u8fc7\u5c06\u54f2\u5b66\u6d1e\u89c1\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u4e0d\u4ec5\u4e3aAI\u63d0\u4f9b\u4e86\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u8fd8\u91cd\u65b0\u5b9a\u4e49\u4e86\u667a\u80fd\u7684\u672c\u8d28\u2014\u2014\u4e0d\u662f\u8868\u5f81\u51c6\u786e\u6027\uff0c\u800c\u662f\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002\u8fd9\u4e00\u6846\u67b6\u5bf9\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548cAI\u9886\u57df\u5747\u6709\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2510.17262", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17262", "abs": "https://arxiv.org/abs/2510.17262", "authors": ["Chuhan Qi"], "title": "Finding 4-Additive Spanners: Faster, Stronger, and Simpler", "comment": null, "summary": "Additive spanners are fundamental graph structures with wide applications in\nnetwork design, graph sparsification, and distance approximation. In\nparticular, a $4$-additive spanner is a subgraph that preserves all pairwise\ndistances up to an additive error of $4$. In this paper, we present a new\ndeterministic algorithm for constructing $4$-additive spanners that matches the\nbest known edge bound of $\\tilde{O}(n^{7/5})$ (up to polylogarithmic factors),\nwhile improving the running time to $\\tilde{O}(\\min\\{mn^{3/5}, n^{11/5}\\})$,\ncompared to the previous $\\tilde{O}(mn^{3/5})$ randomized construction. Our\nalgorithm is not only faster in the dense regime but also fully deterministic,\nconceptually simpler, and easier to implement and analyze.", "AI": {"conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa4-\u52a0\u6cd5\u6027\u751f\u6210\u5b50\u56fe\uff0c\u5176\u8fb9\u7f18\u754c\u9650\u4e0e\u5df2\u77e5\u6700\u4f73\u7ed3\u679c\u4e00\u81f4\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4e14\u5728\u6982\u5ff5\u4e0a\u66f4\u7b80\u5355\u3001\u66f4\u6613\u4e8e\u5b9e\u73b0\u548c\u5206\u6790\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa4-\u52a0\u6cd5\u6027\u751f\u6210\u5b50\u56fe\uff0c\u5176\u8fb9\u7f18\u754c\u9650\u4e3a$\\tilde{O}(n^{7/5})$\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a$\\tilde{O}(\\min\\{mn^{3/5}, n^{11/5}\\})$\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u968f\u673a\u5316\u6784\u9020\u3002", "motivation": "\u52a0\u6cd5\u6027\u751f\u6210\u5b50\u56fe\u5728\u7f51\u7edc\u8bbe\u8ba1\u3001\u56fe\u7a00\u758f\u5316\u548c\u8ddd\u79bb\u8fd1\u4f3c\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u76844-\u52a0\u6cd5\u6027\u751f\u6210\u5b50\u56fe\u6784\u9020\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u4e14\u786e\u5b9a\u6027\u66f4\u5f3a\u7684\u7b97\u6cd5\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u8fb9\u7f18\u754c\u9650\u4e0a\u5339\u914d\u5df2\u77e5\u6700\u4f73\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4e14\u5728\u6982\u5ff5\u4e0a\u66f4\u7b80\u5355\u3001\u66f4\u6613\u4e8e\u5b9e\u73b0\u548c\u5206\u6790\u3002", "tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa4-\u52a0\u6cd5\u6027\u751f\u6210\u5b50\u56fe\uff0c\u5176\u8fb9\u7f18\u754c\u9650\u4e0e\u5df2\u77e5\u6700\u4f73\u7ed3\u679c\u4e00\u81f4\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4e14\u5728\u6982\u5ff5\u4e0a\u66f4\u7b80\u5355\u3001\u66f4\u6613\u4e8e\u5b9e\u73b0\u548c\u5206\u6790\u3002"}}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.", "AI": {"tldr": "\u52a8\u6001\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4eLLM\u7f16\u7801\u4ee3\u7406\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u4e0d\u53ef\u9884\u6d4b\u7684\u9ad8\u6210\u672c\uff08\u5982\u8f6e\u6b21\u589e\u957f\u5bfc\u81f4\u7684\u4ee4\u724c\u6570\u6fc0\u589e\uff09\u800c\u53d7\u9650\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u8f6e\u4f18\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u603b\u8f6e\u6b21\u7684\u6218\u7565\u63a7\u5236\u3002", "method": "\u5728SWE-bench\u4e0a\u4f7f\u7528\u4e09\u79cd\u5148\u8fdb\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff1a\u65e0\u9650\u5236\u57fa\u7ebf\u3001\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\uff08\u5e26\u63d0\u9192\uff09\u548c\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\uff08\u6309\u9700\u6269\u5c55\uff09\u3002", "result": "\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u89e3\u51b3\u7387\u7684\u540c\u65f6\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e12%-24%\u7684\u6210\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u9650\u5236\u7b56\u7565\u3002", "conclusion": "\u52a8\u6001\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff08\u7279\u522b\u662f\u6309\u9700\u6269\u5c55\u7684\u52a8\u6001\u7b56\u7565\uff09\u5728\u5e73\u8861\u6210\u672c\u4e0e\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u8d44\u6e90\u5206\u914d\u6307\u5357\u3002"}}
{"id": "2510.16136", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16136", "abs": "https://arxiv.org/abs/2510.16136", "authors": ["Sayan Deb Sarkar", "Sinisa Stekovic", "Vincent Lepetit", "Iro Armeni"], "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer", "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/", "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u7528\u5f15\u5bfc\u6210\u529f\u5c06\u5916\u89c2\u7ec6\u8282\u8f6c\u79fb\u52303D\u8d44\u4ea7\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528GPT\u7cfb\u7edf\u8fdb\u884c\u5ba2\u89c2\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5728\u8f93\u5165\u4e0e\u5916\u89c2\u5bf9\u8c61\u7684\u51e0\u4f55\u5f62\u72b6\u5dee\u5f02\u8f83\u5927\u65f6\u4ecd\u5b58\u5728\u5931\u8d25\u3002\u76f4\u63a5\u5e94\u75283D\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u4ea7\u751f\u5438\u5f15\u4eba\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6821\u6b63\u6d41\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6dfb\u52a0\u5f15\u5bfc\u6765\u4e0e\u91c7\u6837\u8fc7\u7a0b\u4ea4\u4e92\u3002\u5f15\u5bfc\u53ef\u5efa\u6a21\u4e3a\u53ef\u5fae\u5206\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e24\u79cd\u5f15\u5bfc\u7c7b\u578b\uff1a\u5916\u89c2\u7684\u90e8\u5206\u611f\u77e5\u635f\u5931\u548c\u81ea\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u7eb9\u7406\u548c\u51e0\u4f55\u7ec6\u8282\u8f6c\u79fb\u5230\u8f93\u51653D\u8d44\u4ea7\u4e0a\uff0c\u4e14\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u4e0d\u9002\u7528\u4e8e\u6b64\u4efb\u52a1\uff0c\u56e0\u6b64\u91c7\u7528\u57fa\u4e8eGPT\u7684\u5ba2\u89c2\u6392\u540d\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u5f15\u5bfc\u539f\u5219\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u5916\u89c2\u7ec6\u8282\u8f6c\u79fb\u52303D\u8d44\u4ea7\u4e0a\uff0c\u5e76\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u4e0d\u540c\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\u548c\u5f15\u5bfc\u51fd\u6570\u3002"}}
{"id": "2510.16517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16517", "abs": "https://arxiv.org/abs/2510.16517", "authors": ["Haokai Ding", "Wenzeng Zhang"], "title": "A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "Accepted author manuscript (AAM) for IEEE/RSJ IROS 2025. 6 pages, 10\n  figures", "summary": "This paper introduces a novel robotic gripper, named as the SPD gripper. It\nfeatures a palm and two mechanically identical and symmetrically arranged\nfingers, which can be driven independently or by a single motor. The fingertips\nof the fingers follow a linear motion trajectory, facilitating the grasping of\nobjects of various sizes on a tabletop without the need to adjust the overall\nheight of the gripper. Traditional industrial grippers with parallel gripping\ncapabilities often exhibit an arcuate motion at the fingertips, requiring the\nentire robotic arm to adjust its height to avoid collisions with the tabletop.\nThe SPD gripper, with its linear parallel gripping mechanism, effectively\naddresses this issue. Furthermore, the SPD gripper possesses adaptive\ncapabilities, accommodating objects of different shapes and sizes. This paper\npresents the design philosophy, fundamental composition principles, and\noptimization analysis theory of the SPD gripper. Based on the design theory, a\nrobotic gripper prototype was developed and tested. The experimental results\ndemonstrate that the robotic gripper successfully achieves linear parallel\ngripping functionality and exhibits good adaptability. In the context of the\nongoing development of embodied intelligence technologies, this robotic gripper\ncan assist various robots in achieving effective grasping, laying a solid\nfoundation for collecting data to enhance deep learning training.", "AI": {"tldr": "SPD\u673a\u68b0\u624b\u901a\u8fc7\u7ebf\u6027\u5e73\u884c\u5939\u6301\u673a\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u5939\u6301\u5668\u9700\u8c03\u6574\u9ad8\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u5177\u5907\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7269\u4f53\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u5939\u6301\u5668\u5728\u5e73\u884c\u5939\u6301\u65f6\u6307\u5c16\u5448\u5f27\u5f62\u8fd0\u52a8\uff0c\u9700\u8c03\u6574\u673a\u68b0\u81c2\u9ad8\u5ea6\u4ee5\u907f\u514d\u4e0e\u684c\u9762\u78b0\u649e\uff0cSPD\u673a\u68b0\u624b\u901a\u8fc7\u7ebf\u6027\u5e73\u884c\u5939\u6301\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86SPD\u673a\u68b0\u624b\u7684\u8bbe\u8ba1\u7406\u5ff5\u3001\u57fa\u672c\u7ec4\u6210\u539f\u7406\u53ca\u4f18\u5316\u5206\u6790\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u8bbe\u8ba1\u7406\u8bba\u5f00\u53d1\u4e86\u539f\u578b\u673a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u673a\u68b0\u624b\u6210\u529f\u5b9e\u73b0\u4e86\u7ebf\u6027\u5e73\u884c\u5939\u6301\u529f\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "SPD\u673a\u68b0\u624b\u6210\u529f\u5b9e\u73b0\u4e86\u7ebf\u6027\u5e73\u884c\u5939\u6301\u529f\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\uff0c\u4e3a\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528citiverses\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u7684\u865a\u62df\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e13\u5bb6\u54a8\u8be2\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\uff0c\u5f3a\u8c03\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u591a\u7ef4\u5ea6\u8003\u91cf\u3002", "motivation": "\u63a2\u7d22citiverses\u4f5c\u4e3a\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\uff0c\u652f\u6301\u76d1\u7ba1\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4ee5\u5e94\u5bf9\u4ea4\u901a\u3001\u57ce\u5e02\u89c4\u5212\u548c\u73af\u5883/\u6c14\u5019\u5371\u673a\u7b49\u9886\u57df\u7684\u653f\u7b56\u6311\u6218\u3002", "method": "\u8bba\u6587\u57fa\u4e8e\u4e0e\u9ad8\u7ea7\u4e13\u5bb6\u5c0f\u7ec4\u7684\u54a8\u8be2\uff0c\u5305\u62ec\u6b27\u6d32\u59d4\u5458\u4f1a\u7684\u653f\u7b56\u5236\u5b9a\u8005\u3001\u56fd\u5bb6\u653f\u5e9c\u79d1\u5b66\u987e\u95ee\u4ee5\u53ca\u6570\u5b57\u76d1\u7ba1\u548c\u865a\u62df\u4e16\u754c\u9886\u57df\u7684\u9886\u5148\u7814\u7a76\u4eba\u5458\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u4e3b\u9898\u3002", "result": "\u8bc6\u522b\u4e86\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u53cd\u9988\u3001\u590d\u6742\u6027\u5efa\u6a21\u3001\u8de8\u5883\u534f\u4f5c\u3001\u98ce\u9669\u964d\u4f4e\u3001\u516c\u6c11\u53c2\u4e0e\u3001\u4f26\u7406\u8003\u8651\u548c\u65b0\u5174\u6280\u672f\u6574\u5408\u7b49\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u5e76\u5206\u6790\u4e86\u53ef\u5728citiverse\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u4e3b\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u79d1\u5b66\u653f\u7b56\u8bae\u7a0b\uff0c\u65e8\u5728\u63a2\u7d22citiverses\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u5b9e\u9a8c\u7a7a\u95f4\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u5728\u5f00\u53d1\u548c\u4f7f\u7528citiverses\u65f6\u9700\u91c7\u53d6\u8d1f\u8d23\u4efb\u7684\u65b9\u6cd5\uff0c\u7efc\u5408\u8003\u8651\u4f26\u7406\u3001\u7ecf\u6d4e\u3001\u751f\u6001\u548c\u793e\u4f1a\u7b49\u591a\u7ef4\u5ea6\u56e0\u7d20\u3002"}}
{"id": "2510.17344", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17344", "abs": "https://arxiv.org/abs/2510.17344", "authors": ["Nicolas Bousquet", "Amer E. Mouawad", "Stephanie Maaz", "Naomi Nishimura", "Sebastian Siebertz"], "title": "On Algorithmic Meta-Theorems for Solution Discovery: Tractability and Barriers", "comment": null, "summary": "Solution discovery asks whether a given (infeasible) starting configuration\nto a problem can be transformed into a feasible solution using a limited number\nof transformation steps. This paper investigates meta-theorems for solution\ndiscovery for graph problems definable in monadic second-order logic (MSO$_1$\nand MSO$_2$) and first-order logic (FO) where the transformation step is to\nslide a token to an adjacent vertex, focusing on parameterized complexity and\nstructural graph parameters that do not involve the transformation budget $b$.\nWe present both positive and negative results. On the algorithmic side, we\nprove that MSO$_2$-Discovery is in XP when parameterized by treewidth and that\nMSO$_1$-Discovery is fixed-parameter tractable when parameterized by\nneighborhood diversity. On the hardness side, we establish that FO-Discovery is\nW[1]-hard when parameterized by modulator to stars, modulator to paths, as well\nas twin cover, numbers. Additionally, we prove that MSO$_1$-Discovery is\nW[1]-hard when parameterized by bandwidth. These results complement the\nstraightforward observation that solution discovery for the studied problems is\nfixed-parameter tractable when the budget $b$ is included in the parameter (in\nparticular, parameterized by cliquewidth$+b$, where the cliquewidth of a graph\nis at most any of the studied parameters), and provide a near-complete\n(fixed-parameter tractability) meta-theorems investigation for solution\ndiscovery problems for MSO- and FO-definable graph problems and structural\nparameters larger than cliquewidth.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86MSO\u548cFO\u53ef\u5b9a\u4e49\u7684\u56fe\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u7b97\u6cd5\u548c\u786c\u5ea6\u7ed3\u679c\uff0c\u8865\u5145\u4e86\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u6027\u5143\u5b9a\u7406\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\u95ee\u9898\uff0c\u5373\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6709\u9650\u7684\u8f6c\u6362\u6b65\u9aa4\u5c06\u521d\u59cb\u914d\u7f6e\u8f6c\u6362\u4e3a\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u9488\u5bf9MSO\u548cFO\u53ef\u5b9a\u4e49\u7684\u56fe\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u8f6c\u6362\u6b65\u9aa4\u5c06\u4e0d\u53ef\u884c\u7684\u521d\u59cb\u914d\u7f6e\u8f6c\u6362\u4e3a\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u7684\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u53c2\u6570\u5316\u590d\u6742\u6027\u548c\u4e0d\u6d89\u53ca\u8f6c\u6362\u9884\u7b97$b$\u7684\u7ed3\u6784\u56fe\u53c2\u6570\u3002", "result": "\u8bc1\u660e\u4e86MSO$_2$-Discovery\u5728\u6811\u5bbd\u53c2\u6570\u5316\u4e0b\u5c5e\u4e8eXP\uff0cMSO$_1$-Discovery\u5728\u90bb\u57df\u591a\u6837\u6027\u53c2\u6570\u5316\u4e0b\u662f\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7684\uff1b\u540c\u65f6\uff0cFO-Discovery\u5728\u661f\u5f62\u8c03\u5236\u5668\u3001\u8def\u5f84\u8c03\u5236\u5668\u548c\u53cc\u8986\u76d6\u6570\u53c2\u6570\u5316\u4e0b\u662fW[1]-\u96be\u7684\uff0cMSO$_1$-Discovery\u5728\u5e26\u5bbd\u53c2\u6570\u5316\u4e0b\u4e5f\u662fW[1]-\u96be\u7684\u3002", "conclusion": "\u672c\u6587\u4e3aMSO\u548cFO\u53ef\u5b9a\u4e49\u7684\u56fe\u95ee\u9898\u53ca\u7ed3\u6784\u53c2\u6570\u5927\u4e8ecliquewidth\u7684\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u8fd1\u5b8c\u6574\u7684\uff08\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u6027\uff09\u5143\u5b9a\u7406\u7814\u7a76\u3002"}}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u5c11\u91cf\u7cbe\u9009\u793a\u4f8b\u6bd4\u5927\u91cf\u793a\u4f8b\u66f4\u80fd\u63d0\u5347\u529f\u80fd\u6b63\u786e\u6027\uff0c\u6311\u6218'\u8d8a\u591a\u8d8a\u597d'\u7684\u5047\u8bbe\u3002", "motivation": "\u63a2\u7a76\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u589e\u52a0\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\uff08'many-shot'\u63d0\u793a\uff09\u662f\u5426\u603b\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u4ece\u96f6\u6837\u672c\u5230\u591a\u8fbe625\u4e2a\u6837\u672c\u7684\u4e0d\u540c\u63d0\u793a\u914d\u7f6e\u5bf9\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0'\u591a\u6837\u672c\u6096\u8bba'\uff1a\u9759\u6001\u76f8\u4f3c\u6027\u6307\u6807\u53ef\u80fd\u7565\u6709\u6539\u5584\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u6027\u5728\u5c11\u91cf\u6837\u672c\uff085-25\u4e2a\u793a\u4f8b\uff09\u65f6\u8fbe\u5230\u5cf0\u503c\uff0c\u66f4\u591a\u6837\u672c\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\uff0c\u5c11\u91cf\u7cbe\u5fc3\u6311\u9009\u7684\u793a\u4f8b\u6bd4\u5927\u91cf\u793a\u4f8b\u66f4\u80fd\u63d0\u5347\u529f\u80fd\u6b63\u786e\u6027\uff0c\u6311\u6218\u4e86'\u8d8a\u591a\u8d8a\u597d'\u7684\u666e\u904d\u5047\u8bbe\u3002"}}
{"id": "2510.16833", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16833", "abs": "https://arxiv.org/abs/2510.16833", "authors": ["Xiangyu Mu", "Dongliang Zhou", "Jie Hou", "Haijun Zhang", "Weili Guan"], "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display", "comment": null, "summary": "Mannequin-based clothing displays offer a cost-effective alternative to\nreal-model showcases for online fashion presentation, but lack realism and\nexpressive detail. To overcome this limitation, we introduce a new task called\nmannequin-to-human (M2H) video generation, which aims to synthesize\nidentity-controllable, photorealistic human videos from footage of mannequins.\nWe propose M2HVideo, a pose-aware and identity-preserving video generation\nframework that addresses two key challenges: the misalignment between head and\nbody motion, and identity drift caused by temporal modeling. In particular,\nM2HVideo incorporates a dynamic pose-aware head encoder that fuses facial\nsemantics with body pose to produce consistent identity embeddings across\nframes. To address the loss of fine facial details due to latent space\ncompression, we introduce a mirror loss applied in pixel space through a\ndenoising diffusion implicit model (DDIM)-based one-step denoising.\nAdditionally, we design a distribution-aware adapter that aligns statistical\ndistributions of identity and clothing features to enhance temporal coherence.\nExtensive experiments on the UBC fashion dataset, our self-constructed ASOS\ndataset, and the newly collected MannequinVideos dataset captured on-site\ndemonstrate that M2HVideo achieves superior performance in terms of clothing\nconsistency, identity preservation, and video fidelity in comparison to\nstate-of-the-art methods.", "AI": {"tldr": "M2HVideo\u662f\u4e00\u79cd\u4ece\u4eba\u4f53\u6a21\u578b\u751f\u6210\u771f\u5b9e\u4eba\u7c7b\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5934\u90e8\u7f16\u7801\u548c\u955c\u50cf\u635f\u5931\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u4e0d\u5bf9\u9f50\u548c\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u4eba\u4f53\u6a21\u578b\u7684\u670d\u88c5\u5c55\u793a\u7f3a\u4e4f\u771f\u5b9e\u611f\u548c\u8868\u8fbe\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4ece\u4eba\u4f53\u6a21\u578b\u5230\u771f\u5b9e\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u7684\u65b0\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86M2HVideo\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u59ff\u6001\u611f\u77e5\u5934\u90e8\u7f16\u7801\u5668\u3001\u57fa\u4e8eDDIM\u7684\u4e00\u6b65\u53bb\u566a\u955c\u50cf\u635f\u5931\u548c\u5206\u5e03\u611f\u77e5\u9002\u914d\u5668\uff0c\u4ee5\u89e3\u51b3\u8eab\u4efd\u6f02\u79fb\u548c\u8fd0\u52a8\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u5728UBC\u65f6\u5c1a\u6570\u636e\u96c6\u3001\u81ea\u5efa\u7684ASOS\u6570\u636e\u96c6\u548c\u73b0\u573a\u91c7\u96c6\u7684MannequinVideos\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM2HVideo\u5728\u670d\u88c5\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "M2HVideo\u6846\u67b6\u5728\u670d\u88c5\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u59ff\u6001\u611f\u77e5\u5934\u90e8\u7f16\u7801\u5668\u548c\u955c\u50cf\u635f\u5931\u7b49\u521b\u65b0\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5934\u90e8\u4e0e\u8eab\u4f53\u8fd0\u52a8\u4e0d\u5bf9\u9f50\u53ca\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2510.16145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16145", "abs": "https://arxiv.org/abs/2510.16145", "authors": ["Ahmad Arrabi", "Jay hwasung Jung", "J Le", "A Nguyen", "J Reed", "E Stahl", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy", "comment": null, "summary": "Thrombectomy is one of the most effective treatments for ischemic stroke, but\nit is resource and personnel-intensive. We propose employing deep learning to\nautomate critical aspects of thrombectomy, thereby enhancing efficiency and\nsafety. In this work, we introduce a self-supervised framework that classifies\nvarious skeletal landmarks using a regression-based pretext task. Our\nexperiments demonstrate that our model outperforms existing methods in both\nregression and classification tasks. Notably, our results indicate that the\npositional pretext task significantly enhances downstream classification\nperformance. Future work will focus on extending this framework toward fully\nautonomous C-arm control, aiming to optimize trajectories from the pelvis to\nthe head during stroke thrombectomy procedures. All code used is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u5f52\u4efb\u52a1\u5206\u7c7b\u9aa8\u9abc\u6807\u5fd7\u70b9\uff0c\u63d0\u5347\u8840\u6813\u5207\u9664\u672f\u6548\u7387\uff0c\u672a\u6765\u76ee\u6807\u4e3a\u5b9e\u73b0\u81ea\u4e3bC\u81c2\u63a7\u5236\u3002", "motivation": "\u8840\u6813\u5207\u9664\u672f\u662f\u6cbb\u7597\u7f3a\u8840\u6027\u4e2d\u98ce\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u8d44\u6e90\u5bc6\u96c6\u4e14\u4f9d\u8d56\u4e13\u4e1a\u4eba\u5458\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5316\u5173\u952e\u6b65\u9aa4\u53ef\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u56de\u5f52\u4efb\u52a1\u4f5c\u4e3a\u524d\u7f6e\u4efb\u52a1\u6765\u5206\u7c7b\u9aa8\u9abc\u6807\u5fd7\u70b9\u3002", "result": "\u6a21\u578b\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f4d\u7f6e\u524d\u7f6e\u4efb\u52a1\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u56de\u5f52\u7684\u524d\u7f6e\u4efb\u52a1\u5206\u7c7b\u9aa8\u9abc\u6807\u5fd7\u70b9\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u6b64\u6846\u67b6\u4ee5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684C\u81c2\u63a7\u5236\uff0c\u4f18\u5316\u4e2d\u98ce\u8840\u6813\u5207\u9664\u672f\u4e2d\u7684\u8f68\u8ff9\u3002"}}
{"id": "2510.16518", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16518", "abs": "https://arxiv.org/abs/2510.16518", "authors": ["Jes\u00fas Ortega-Peimbert", "Finn Lukas Busch", "Timon Homberger", "Quantao Yang", "Olov Andersson"], "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation", "comment": null, "summary": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/", "AI": {"tldr": "DIV-Nav\u662f\u4e00\u4e2a\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u67e5\u8be2\u3001\u8ba1\u7b97\u8bed\u4e49\u56fe\u4ea4\u96c6\u548c\u9a8c\u8bc1\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u5e26\u6709\u7a7a\u95f4\u5173\u7cfb\u7684\u81ea\u7531\u6587\u672c\u67e5\u8be2\u5bfc\u822a\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u96f6-shot\u5bf9\u8c61\u5bfc\u822a\u901a\u5e38\u4ec5\u652f\u6301\u7b80\u5355\u67e5\u8be2\uff08\u5982\u201c\u7535\u89c6\u201d\u6216\u201c\u84dd\u8272\u5730\u6bef\u201d\uff09\uff0c\u800c\u65e0\u6cd5\u5904\u7406\u5e26\u6709\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u81ea\u7531\u6587\u672c\u67e5\u8be2\uff08\u5982\u201c\u627e\u5230\u684c\u5b50\u4e0a\u7684\u9065\u63a7\u5668\u201d\uff09\u3002DIV-Nav\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u6620\u5c04\u7684\u9c81\u68d2\u6027\u3002", "method": "DIV-Nav\u7cfb\u7edf\u91c7\u7528\u4e09\u6b65\u653e\u677e\u7b56\u7565\uff1a\u5206\u89e3\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e3a\u7b80\u5355\u5bf9\u8c61\u7ea7\u67e5\u8be2\uff0c\u8ba1\u7b97\u4e2a\u4f53\u8bed\u4e49\u4fe1\u5ff5\u56fe\u7684\u4ea4\u96c6\u4ee5\u8bc6\u522b\u5171\u5b58\u533a\u57df\uff0c\u4ee5\u53ca\u901a\u8fc7LVLM\u9a8c\u8bc1\u5bf9\u8c61\u4e0e\u539f\u59cb\u7ea6\u675f\u7684\u5339\u914d\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u8fd8\u63a2\u7d22\u4e86\u5982\u4f55\u8c03\u6574\u5728\u7ebf\u8bed\u4e49\u6620\u5c04\u7684\u524d\u6cbf\u63a2\u7d22\u76ee\u6807\u4ee5\u9002\u5e94\u7a7a\u95f4\u641c\u7d22\u67e5\u8be2\u3002", "result": "DIV-Nav\u5728MultiON\u57fa\u51c6\u6d4b\u8bd5\u548c\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\uff08\u642d\u8f7dJetson Orin AGX\uff09\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DIV-Nav\u7cfb\u7edf\u901a\u8fc7\u5206\u89e3\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u8ba1\u7b97\u8bed\u4e49\u4fe1\u5ff5\u56fe\u7684\u4ea4\u96c6\u4ee5\u53ca\u9a8c\u8bc1\u5bf9\u8c61\u4e0e\u539f\u59cb\u7ea6\u675f\u7684\u5339\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u81ea\u7531\u6587\u672c\u67e5\u8be2\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u5e76\u5728MultiON\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "PISA\u662f\u4e00\u79cd\u53d7\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u6a21\u6001\u9002\u5e94\u548c\u6df7\u5408\u68c0\u7d22\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u7684\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4e14\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u6784\u5efa\u6027\u548c\u4efb\u52a1\u5bfc\u5411\u6027\u3002", "method": "PISA \u5f15\u5165\u4e86\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\uff08\u6a21\u5f0f\u66f4\u65b0\u3001\u6a21\u5f0f\u6f14\u5316\u548c\u6a21\u5f0f\u521b\u5efa\uff09\uff0c\u5e76\u7ed3\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u68c0\u7d22\u7684\u6df7\u5408\u5185\u5b58\u8bbf\u95ee\u67b6\u6784\u3002", "result": "\u5728LOCOMO\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684AggQA\u57fa\u51c6\u4e0a\uff0cPISA\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6807\u6746\u3002", "conclusion": "PISA \u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u3001\u53d7\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u7edf\u4e00\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u6784\u5efa\u6027\u548c\u9002\u5e94\u6027\u7684\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2510.17595", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17595", "abs": "https://arxiv.org/abs/2510.17595", "authors": ["Manuel Christalla", "Luise Puhlmann", "Vera Traub"], "title": "Approximating Asymmetric A Priori TSP beyond the Adaptivity Gap", "comment": null, "summary": "In Asymmetric A Priori TSP (with independent activation probabilities) we are\ngiven an instance of the Asymmetric Traveling Salesman Problem together with an\nactivation probability for each vertex. The task is to compute a tour that\nminimizes the expected length after short-cutting to the randomly sampled set\nof active vertices.\n  We prove a polynomial lower bound on the adaptivity gap for Asymmetric A\nPriori TSP. Moreover, we show that a poly-logarithmic approximation ratio, and\nhence an approximation ratio below the adaptivity gap, can be achieved by a\nrandomized algorithm with quasi-polynomial running time.\n  To achieve this, we provide a series of polynomial-time reductions. First we\nreduce to a novel generalization of the Asymmetric Traveling Salesman Problem,\ncalled Hop-ATSP. Next, we use directed low-diameter decompositions to obtain\nstructured instances, for which we then provide a reduction to a covering\nproblem. Eventually, we obtain a polynomial-time reduction of Asymmetric A\nPriori TSP to a problem of finding a path in an acyclic digraph minimizing a\nparticular objective function, for which we give an O(log n)-approximation\nalgorithm in quasi-polynomial time.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u5bf9\u79f0\u5148\u9a8cTSP\u7684\u81ea\u9002\u5e94\u95f4\u9699\u548c\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u591a\u9879\u5f0f\u4e0b\u754c\u5e76\u63d0\u51fa\u4e86\u62df\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u7684O(log n)\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u975e\u5bf9\u79f0\u5148\u9a8cTSP\uff08\u9876\u70b9\u5177\u6709\u72ec\u7acb\u6fc0\u6d3b\u6982\u7387\uff09\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u65e8\u5728\u8ba1\u7b97\u4e00\u4e2a\u8def\u5f84\uff0c\u4f7f\u5f97\u5728\u968f\u673a\u91c7\u6837\u7684\u6fc0\u6d3b\u9876\u70b9\u96c6\u4e0a\u77ed\u622a\u540e\u7684\u671f\u671b\u957f\u5ea6\u6700\u5c0f\u5316\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u591a\u9879\u5f0f\u65f6\u95f4\u5f52\u7ea6\uff0c\u9996\u5148\u5f52\u7ea6\u5230\u4e00\u4e2a\u65b0\u7684\u5e7f\u4e49\u975e\u5bf9\u79f0\u65c5\u884c\u5546\u95ee\u9898\uff08Hop-ATSP\uff09\uff0c\u7136\u540e\u5229\u7528\u6709\u5411\u4f4e\u76f4\u5f84\u5206\u89e3\u83b7\u5f97\u7ed3\u6784\u5316\u5b9e\u4f8b\uff0c\u8fdb\u4e00\u6b65\u5f52\u7ea6\u5230\u4e00\u4e2a\u8986\u76d6\u95ee\u9898\uff0c\u6700\u7ec8\u5c06\u975e\u5bf9\u79f0\u5148\u9a8cTSP\u5f52\u7ea6\u4e3a\u5728\u65e0\u73af\u6709\u5411\u56fe\u4e2d\u5bfb\u627e\u8def\u5f84\u7684\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u8be5\u95ee\u9898\u63d0\u51fa\u4e86\u4e00\u4e2a\u62df\u591a\u9879\u5f0f\u65f6\u95f4\u5185O(log n)\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u95f4\u9699\u5b58\u5728\u591a\u9879\u5f0f\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u62df\u591a\u9879\u5f0f\u65f6\u95f4\u5185O(log n)\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u975e\u5bf9\u79f0\u5148\u9a8cTSP\u7684\u81ea\u9002\u5e94\u95f4\u9699\u5b58\u5728\u591a\u9879\u5f0f\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u62df\u591a\u9879\u5f0f\u8fd0\u884c\u65f6\u95f4\u7684\u968f\u673a\u7b97\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u6570\u591a\u9879\u5f0f\u8fd1\u4f3c\u6bd4\uff0c\u4ece\u800c\u4f4e\u4e8e\u81ea\u9002\u5e94\u95f4\u9699\u3002"}}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684Chrome\u6269\u5c55\u7a0b\u5e8f\u5b58\u5728\u9ad8\u6bd4\u4f8b\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c24\u5176\u5728\u654f\u611f\u6570\u636e\u5904\u7406\u573a\u666f\uff0c\u9ad8\u7ea7\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f00\u53d1\u8005\u53ef\u80fd\u5ffd\u89c6\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLM\u751f\u6210\u7684\u6846\u67b6\u7ea6\u675f\u7a0b\u5e8f\u7684\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86ChromeSecBench\u6570\u636e\u96c6\uff0c\u5305\u542b140\u4e2a\u57fa\u4e8e\u5df2\u77e5\u6f0f\u6d1e\u6269\u5c55\u7684\u63d0\u793a\uff0c\u5e76\u6307\u5bfc\u4e5d\u79cd\u5148\u8fdbLLM\u751f\u6210\u5b8c\u6574\u7684Chrome\u6269\u5c55\u7a0b\u5e8f\uff0c\u968f\u540e\u4ece\u573a\u666f\u7c7b\u578b\u3001\u6a21\u578b\u5dee\u5f02\u548c\u6f0f\u6d1e\u7c7b\u522b\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u5176\u5b89\u5168\u6027\u3002", "result": "LLM\u751f\u6210\u7684\u7a0b\u5e8f\u5b58\u5728\u9ad8\u8fbe18%-50%\u7684\u6f0f\u6d1e\u7387\uff0c\u8eab\u4efd\u8ba4\u8bc1\u548cCookie\u7ba1\u7406\u573a\u666f\u7684\u6f0f\u6d1e\u7387\u5206\u522b\u9ad8\u8fbe83%\u548c78%\uff0c\u4e14\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u6846\u67b6\u7ea6\u675f\u7a0b\u5e8f\u5b58\u5728\u9ad8\u6bd4\u4f8b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u8eab\u4efd\u8ba4\u8bc1\u548cCookie\u7ba1\u7406\u573a\u666f\u4e2d\uff0c\u8fd9\u8868\u660eLLM\u5728\u7f16\u5199\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002"}}
{"id": "2510.16146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16146", "abs": "https://arxiv.org/abs/2510.16146", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Vi Vu", "Ba-Thinh Lam", "Phat Huynh", "Tianyang Wang", "Xingjian Li", "Ulas Bagci", "Min Xu"], "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization", "comment": "The paper is under review at CMIG", "summary": "The limited availability of annotated data in medical imaging makes\nsemi-supervised learning increasingly appealing for its ability to learn from\nimperfect supervision. Recently, teacher-student frameworks have gained\npopularity for their training benefits and robust performance. However, jointly\noptimizing the entire network can hinder convergence and stability, especially\nin challenging scenarios. To address this for medical image segmentation, we\npropose DuetMatch, a novel dual-branch semi-supervised framework with\nasynchronous optimization, where each branch optimizes either the encoder or\ndecoder while keeping the other frozen. To improve consistency under noisy\nconditions, we introduce Decoupled Dropout Perturbation, enforcing\nregularization across branches. We also design Pair-wise CutMix Cross-Guidance\nto enhance model diversity by exchanging pseudo-labels through augmented input\npairs. To mitigate confirmation bias from noisy pseudo-labels, we propose\nConsistency Matching, refining labels using stable predictions from frozen\nteacher models. Extensive experiments on benchmark brain MRI segmentation\ndatasets, including ISLES2022 and BraTS, show that DuetMatch consistently\noutperforms state-of-the-art methods, demonstrating its effectiveness and\nrobustness across diverse semi-supervised segmentation scenarios.", "AI": {"tldr": "DuetMatch\u662f\u4e00\u79cd\u65b0\u578b\u53cc\u5206\u652f\u534a\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u4f18\u5316\u548c\u591a\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u5728\u8054\u5408\u4f18\u5316\u65f6\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faDuetMatch\uff0c\u4e00\u79cd\u5177\u6709\u5f02\u6b65\u4f18\u5316\u7684\u53cc\u5206\u652f\u534a\u76d1\u7763\u6846\u67b6\uff0c\u91c7\u7528\u89e3\u8026\u4e22\u5f03\u6270\u52a8\u3001\u6210\u5bf9CutMix\u4ea4\u53c9\u5f15\u5bfc\u548c\u4e00\u81f4\u6027\u5339\u914d\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728ISLES2022\u548cBraTS\u7b49\u8111MRI\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDuetMatch\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DuetMatch\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u591a\u79cd\u534a\u76d1\u7763\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16524", "abs": "https://arxiv.org/abs/2510.16524", "authors": ["Haokai Ding", "Zhaohan Chen", "Tao Yang", "Wenzeng Zhang"], "title": "Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "6 pages, 9 figures, Accepted author manuscript for IEEE CASE 2025", "summary": "This paper presents the SP-Diff parallel gripper system, addressing the\nlimited adaptability of conventional end-effectors in intelligent industrial\nautomation. The proposed design employs an innovative differential linkage\nmechanism with a modular symmetric dual-finger configuration to achieve\nlinear-parallel grasping. By integrating a planetary gear transmission, the\nsystem enables synchronized linear motion and independent finger pose\nadjustment while maintaining structural rigidity, reducing Z-axis recalibration\nrequirements by 30% compared to arc-trajectory grippers. The compact palm\narchitecture incorporates a kinematically optimized parallelogram linkage and\nDifferential mechanism, demonstrating adaptive grasping capabilities for\ndiverse industrial workpieces and deformable objects such as citrus fruits.\nFuture-ready interfaces are embedded for potential force/vision sensor\nintegration to facilitate multimodal data acquisition (e.g., trajectory\nplanning and object deformation) in digital twin frameworks. Designed as a\nflexible manufacturing solution, SP-Diff advances robotic end-effector\nintelligence through its adaptive architecture, showing promising applications\nin collaborative robotics, logistics automation, and specialized operational\nscenarios.", "AI": {"tldr": "SP-Diff\u5e73\u884c\u5939\u722a\u7cfb\u7edf\u901a\u8fc7\u5dee\u5206\u8fde\u6746\u548c\u884c\u661f\u9f7f\u8f6e\u4f20\u52a8\u63d0\u5347\u6293\u53d6\u9002\u5e94\u6027\uff0c\u51cf\u5c11\u6821\u51c6\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5de5\u4ef6\u548c\u534f\u4f5c\u673a\u5668\u4eba\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u672b\u7aef\u6267\u884c\u5668\u5728\u667a\u80fd\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0d\u540c\u5de5\u4e1a\u5de5\u4ef6\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u6293\u53d6\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5dee\u5206\u8fde\u6746\u673a\u5236\u548c\u6a21\u5757\u5316\u5bf9\u79f0\u53cc\u6307\u914d\u7f6e\uff0c\u96c6\u6210\u884c\u661f\u9f7f\u8f6e\u4f20\u52a8\uff0c\u5b9e\u73b0\u7ebf\u6027\u5e73\u884c\u6293\u53d6\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u5b66\u4f18\u5316\u7684\u5e73\u884c\u56db\u8fb9\u5f62\u8fde\u6746\u548c\u5dee\u5206\u673a\u5236\uff0c\u4fdd\u6301\u7ed3\u6784\u521a\u6027\u3002", "result": "\u7cfb\u7edf\u51cf\u5c11\u4e8630%\u7684Z\u8f74\u91cd\u65b0\u6821\u51c6\u9700\u6c42\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u540c\u6b65\u7ebf\u6027\u8fd0\u52a8\u548c\u72ec\u7acb\u624b\u6307\u59ff\u6001\u8c03\u6574\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5de5\u4ef6\u548c\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u5982\u67d1\u6a58\u7c7b\u6c34\u679c\uff09\u3002", "conclusion": "SP-Diff\u5e73\u884c\u5939\u722a\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u5dee\u5206\u8fde\u6746\u673a\u5236\u548c\u6a21\u5757\u5316\u5bf9\u79f0\u53cc\u6307\u914d\u7f6e\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u672b\u7aef\u6267\u884c\u5668\u7684\u9002\u5e94\u6027\uff0c\u5c55\u793a\u4e86\u5728\u534f\u4f5c\u673a\u5668\u4eba\u3001\u7269\u6d41\u81ea\u52a8\u5316\u548c\u7279\u6b8a\u64cd\u4f5c\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u590d\u6742\u8c1c\u9898\u4e2d\u6027\u80fd\u5d29\u6e83\uff0c\u73af\u5883\u63a5\u53e3\u65e0\u6548\uff0c\u6a21\u5f0f\u5d29\u6e83\u662f\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u6027\u80fd\u5d29\u6e83\u7684\u539f\u56e0\uff0c\u4ee5\u53ca\u73af\u5883\u63a5\u53e3\u662f\u5426\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u5176\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u64cd\u4f5c\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\u5e76\u81ea\u6211\u63d0\u793a\u4e0b\u4e00\u6b65\u884c\u52a8\u3002", "result": "\u73af\u5883\u63a5\u53e3\u5e76\u672a\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\uff0c\u6a21\u578b\u5728\u590d\u6742\u5ea6\u7684\u6bcf\u4e00\u5c42\u7ea7\u90fd\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6a21\u5f0f\u662f\u5426\u53cd\u6620\u95ee\u9898\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u89e3\u51b3\u7279\u5b9a\u590d\u6742\u5ea6\u4ee5\u4e0a\u7684\u8c1c\u9898\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u53ef\u80fd\u4e0e\u6a21\u578b\u81ea\u8eab\u7684\u72b6\u6001\u7a7a\u95f4\u8ddf\u8e2a\u80fd\u529b\u6709\u5173\u3002"}}
{"id": "2510.17645", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17645", "abs": "https://arxiv.org/abs/2510.17645", "authors": ["Ce Jin", "Tomasz Kociumaka"], "title": "Near-Optimal Property Testers for Pattern Matching", "comment": "To appear at FOCS 2025. Abstract shortened to meet arXiv requirements", "summary": "The classic exact pattern matching problem, given two strings -- a pattern\n$P$ of length $m$ and a text $T$ of length $n$ -- asks whether $P$ occurs as a\nsubstring of $T$. A property tester for the problem needs to distinguish (with\nhigh probability) the following two cases for some threshold $k$: the YES case,\nwhere $P$ occurs as a substring of $T$, and the NO case, where $P$ has Hamming\ndistance greater than $k$ from every substring of $T$, that is, $P$ has no\n$k$-mismatch occurrence in $T$.\n  In this work, we provide adaptive and non-adaptive property testers for the\nexact pattern matching problem, jointly covering the whole spectrum of\nparameters. We further establish unconditional lower bounds demonstrating that\nthe time and query complexities of our algorithms are optimal, up to\n$\\mathrm{polylog}\\, n$ factors hidden within the $\\tilde O(\\cdot)$ notation\nbelow.\n  In the most studied regime of $n=m+\\Theta(m)$, our non-adaptive property\ntester has the time complexity of $\\tilde O(n/\\sqrt{k})$, and a matching lower\nbound remains valid for the query complexity of adaptive algorithms. This\nimproves both upon a folklore solution that attains the optimal query\ncomplexity but requires $\\Omega(n)$ time, and upon the only previously known\nsublinear-time property tester, by Chan, Golan, Kociumaka, Kopelowitz, and\nPorat [STOC 2020], with time complexity $\\tilde O(n/\\sqrt[3]{k})$. The\naforementioned results remain valid for $n=m+\\Omega(m)$, where our optimal\nrunning time $\\tilde O(\\sqrt{nm/k}+n/k)$ improves upon the previously best time\ncomplexity of $\\tilde O(\\sqrt[3]{n^2m/k}+n/k)$. In the regime of $n=m+o(m)$,\nwhich has not been targeted in any previous work, we establish a surprising\nseparation between adaptive and non-adaptive algorithms, whose optimal time and\nquery complexities are $\\tilde O(\\sqrt{(n-m+1)m/k}+n/k)$ and $\\tilde\nO(\\min(n\\sqrt{n-m+1}/k,\\sqrt{nm/k}+n/k))$, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6700\u4f18\u7684\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u5c5e\u6027\u6d4b\u8bd5\u5668\uff0c\u7528\u4e8e\u7cbe\u786e\u6a21\u5f0f\u5339\u914d\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u8303\u56f4\u5185\u5c55\u793a\u4e86\u5176\u590d\u6742\u5ea6\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u7ecf\u5178\u7cbe\u786e\u6a21\u5f0f\u5339\u914d\u95ee\u9898\u4e2d\u7684\u5c5e\u6027\u6d4b\u8bd5\u9700\u6c42\uff0c\u533a\u5206\u6a21\u5f0f\u662f\u5426\u4f5c\u4e3a\u5b50\u4e32\u51fa\u73b0\u6216\u4e0e\u6240\u6709\u5b50\u4e32\u7684\u6c49\u660e\u8ddd\u79bb\u8d85\u8fc7\u9608\u503c\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u7684\u5c5e\u6027\u6d4b\u8bd5\u5668\uff0c\u901a\u8fc7\u8986\u76d6\u6240\u6709\u53c2\u6570\u8303\u56f4\uff0c\u5e76\u5229\u7528\u65e0\u6761\u4ef6\u4e0b\u9650\u8bc1\u660e\u5176\u590d\u6742\u5ea6\u6700\u4f18\u3002", "result": "\u5728\u53c2\u6570\u8303\u56f4\u5185\uff0c\u975e\u81ea\u9002\u5e94\u5c5e\u6027\u6d4b\u8bd5\u5668\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO\u0303(n/\u221ak)\uff0c\u4e14\u81ea\u9002\u5e94\u7b97\u6cd5\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u9650\u5339\u914d\u8be5\u7ed3\u679c\u3002\u5728n=m+o(m)\u8303\u56f4\u5185\uff0c\u81ea\u9002\u5e94\u4e0e\u975e\u81ea\u9002\u5e94\u7b97\u6cd5\u5c55\u73b0\u51fa\u65f6\u95f4\u4e0e\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u7684\u5c5e\u6027\u6d4b\u8bd5\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u7cbe\u786e\u6a21\u5f0f\u5339\u914d\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u67e5\u8be2\u590d\u6742\u5ea6\u4e0a\u7684\u6700\u4f18\u6027\u3002"}}
{"id": "2510.17056", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17056", "abs": "https://arxiv.org/abs/2510.17056", "authors": ["Luis F. G. Campos", "Leonardo C. Marques", "Walter T. Nakamura"], "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection", "comment": "Accepted and to be published in SBQS25 - Brazilian Symposium on\n  Software Quality 2025", "summary": "Usability inspection is a well-established technique for identifying\ninteraction issues in software interfaces, thereby contributing to improved\nproduct quality. However, it is a costly process that requires time and\nspecialized knowledge from inspectors. With advances in Artificial Intelligence\n(AI), new opportunities have emerged to support this task, particularly through\ngenerative models capable of interpreting interfaces and performing inspections\nmore efficiently. This study examines the performance of generative AIs in\nidentifying usability problems, comparing them to those of experienced human\ninspectors. A software prototype was evaluated by four specialists and two AI\nmodels (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,\nand F1-score. While inspectors achieved the highest levels of precision and\noverall coverage, the AIs demonstrated high individual performance and\ndiscovered many novel defects, but with a higher rate of false positives and\nredundant reports. The combination of AIs and human inspectors produced the\nbest results, revealing their complementarity. These findings suggest that AI,\nin its current stage, cannot replace human inspectors but can serve as a\nvaluable augmentation tool to improve efficiency and expand defect coverage.\nThe results provide evidence based on quantitative analysis to inform the\ndiscussion on the role of AI in usability inspections, pointing to viable paths\nfor its complementary use in software quality assessment contexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u5728\u53ef\u7528\u6027\u68c0\u67e5\u4e2d\u867d\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\uff0c\u4f46\u53ef\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u63d0\u5347\u6548\u7387\uff0cAI\u4e0e\u4eba\u7c7b\u7ed3\u5408\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u4e3a\u652f\u6301\u53ef\u7528\u6027\u68c0\u67e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8AI\u5728\u8bc6\u522b\u53ef\u7528\u6027\u95ee\u9898\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u56db\u4f4d\u4e13\u5bb6\u548c\u4e24\u79cdAI\u6a21\u578b\uff08GPT-4o\u548cGemini 2.5 Flash\uff09\u5bf9\u8f6f\u4ef6\u539f\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4eba\u7c7b\u68c0\u67e5\u5458\u5728\u7cbe\u786e\u5ea6\u548c\u6574\u4f53\u8986\u76d6\u7387\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cAI\u5728\u4e2a\u4f53\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\u5e76\u53d1\u73b0\u8bb8\u591a\u65b0\u7f3a\u9677\uff0c\u4f46\u5047\u9633\u6027\u548c\u5197\u4f59\u62a5\u544a\u7387\u8f83\u9ad8\u3002AI\u4e0e\u4eba\u7c7b\u68c0\u67e5\u5458\u7684\u7ed3\u5408\u4ea7\u751f\u4e86\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u5f53\u524d\u7684\u9636\u6bb5\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u68c0\u67e5\u5458\uff0c\u4f46\u53ef\u4ee5\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u6269\u5927\u7f3a\u9677\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2510.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16160", "abs": "https://arxiv.org/abs/2510.16160", "authors": ["Ahmad Arrabi", "Jay Hwasung Jung", "Jax Luo", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "Automated C-Arm Positioning via Conformal Landmark Localization", "comment": null, "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided\ninterventions. However, clinical workflows rely on manual alignment that\nincreases radiation exposure and procedural delays. In this work, we present a\npipeline that autonomously navigates the C-arm to predefined anatomical\nlandmarks utilizing X-ray images. Given an input X-ray image from an arbitrary\nstarting location on the operating table, the model predicts a 3D displacement\nvector toward each target landmark along the body. To ensure reliable\ndeployment, we capture both aleatoric and epistemic uncertainties in the\nmodel's predictions and further calibrate them using conformal prediction. The\nderived prediction regions are interpreted as 3D confidence regions around the\npredicted landmark locations. The training framework combines a probabilistic\nloss with skeletal pose regularization to encourage anatomically plausible\noutputs. We validate our approach on a synthetic X-ray dataset generated from\nDeepDRR. Results show not only strong localization accuracy across multiple\narchitectures but also well-calibrated prediction bounds. These findings\nhighlight the pipeline's potential as a component in safe and reliable\nautonomous C-arm systems. Code is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance_APAH", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u5bfc\u822aC\u81c2\u5230\u89e3\u5256\u6807\u5fd7\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6821\u51c6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b89\u5168\u548c\u53ef\u9760\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4f9d\u8d56\u624b\u52a8\u5bf9\u9f50C\u81c2\uff0c\u589e\u52a0\u4e86\u8f90\u5c04\u66b4\u9732\u548c\u624b\u672f\u5ef6\u8fdf\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6982\u7387\u635f\u5931\u548c\u9aa8\u9abc\u59ff\u6001\u6b63\u5219\u5316\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528X\u5c04\u7ebf\u56fe\u50cf\u9884\u6d4b3D\u4f4d\u79fb\u5411\u91cf\uff0c\u5e76\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9a\u4f4d\u51c6\u786e\uff0c\u800c\u4e14\u9884\u6d4b\u8fb9\u754c\u6821\u51c6\u826f\u597d\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u81ea\u4e3b\u5bfc\u822aC\u81c2\u5230\u9884\u5b9a\u4e49\u89e3\u5256\u6807\u5fd7\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6821\u51c6\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b89\u5168\u548c\u53ef\u9760\u81ea\u4e3bC\u81c2\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16617", "abs": "https://arxiv.org/abs/2510.16617", "authors": ["Ruihan Zhao", "Tyler Ingebrand", "Sandeep Chinchali", "Ufuk Topcu"], "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation", "comment": null, "summary": "Vision-Language-Action (VLA) models trained on large robot datasets promise\ngeneral-purpose, robust control across diverse domains and embodiments.\nHowever, existing approaches often fail out-of-the-box when deployed in novel\nenvironments, embodiments, or tasks. We introduce Mixture of Skills VLA\n(MoS-VLA), a framework that represents robot manipulation policies as linear\ncombinations of a finite set of learned basis functions. During pretraining,\nMoS-VLA jointly learns these basis functions across datasets from the Open\nX-Embodiment project, producing a structured skill space. At test time,\nadapting to a new task requires only a single expert demonstration. The\ncorresponding skill representation is then inferred via a lightweight convex\noptimization problem that minimizes the L1 action error, without requiring\ngradient updates. This gradient-free adaptation incurs minimal overhead while\nenabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower\naction-prediction error on five out of five unseen datasets and succeeds in\nboth simulation and real-robot tasks where a pretrained VLA model fails\noutright. Project page: mos-vla.github.io/", "AI": {"tldr": "MoS-VLA\u901a\u8fc7\u9884\u8bad\u7ec3\u8054\u5408\u5b66\u4e60\u57fa\u7840\u6280\u80fd\uff0c\u6d4b\u8bd5\u65f6\u4ec5\u9700\u5355\u4e2a\u6f14\u793a\u5373\u53ef\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u672a\u89c1\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u65b0\u73af\u5883\u3001\u65b0\u4efb\u52a1\u6216\u65b0\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u4e14\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u7684\u65b9\u6cd5\u3002", "method": "MoS-VLA\u5c06\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8868\u793a\u4e3a\u6709\u9650\u5b66\u4e60\u57fa\u7840\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5728Open X-Embodiment\u9879\u76ee\u4e2d\u5b66\u4e60\u8fd9\u4e9b\u57fa\u7840\u51fd\u6570\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u6280\u80fd\u7a7a\u95f4\u3002\u6d4b\u8bd5\u65f6\uff0c\u4ec5\u9700\u5355\u4e2a\u4e13\u5bb6\u6f14\u793a\uff0c\u901a\u8fc7L1\u52a8\u4f5c\u8bef\u5dee\u6700\u5c0f\u5316\u7684\u51f8\u4f18\u5316\u95ee\u9898\u63a8\u65ad\u6280\u80fd\u8868\u793a\u3002", "result": "MoS-VLA\u5728\u4e94\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u52a8\u4f5c\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9884\u8bad\u7ec3VLA\u6a21\u578b\u3002", "conclusion": "MoS-VLA\u6846\u67b6\u901a\u8fc7\u9884\u8bad\u7ec3\u8054\u5408\u5b66\u4e60\u57fa\u7840\u6280\u80fd\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51f8\u4f18\u5316\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.", "AI": {"tldr": "CLTs\u662f\u4e00\u79cd\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u80fd\u9884\u6d4b\u9519\u8bef\u3001\u63ed\u793a\u7b56\u7565\u5e76\u63d0\u5347\u63a8\u7406\u6548\u738715-30%\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u8ba4\u77e5\u8d1f\u8f7d\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u6df1\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5185\u90e8\u7684\u8d44\u6e90\u5206\u914d\u548c\u63a8\u7406\u52a8\u6001\u3002", "method": "\u63d0\u51faCLTs\u4f5c\u4e3a\u7b26\u53f7\u5316\u3001\u968f\u65f6\u95f4\u53d8\u5316\u7684\u51fd\u6570\uff0c\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u5177\u4f53\u8868\u793a\u4e3a\u4e09\u7ec4\u5206\u968f\u673a\u8fc7\u7a0b\uff08IL_t, EL_t, GL_t\uff09\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u3001KV\u7f13\u5b58\u7f3a\u5931\u7387\u7b49\u53ef\u6d4b\u91cf\u4ee3\u7406\u5b9e\u4f8b\u5316\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u7b26\u53f7\u5316\u516c\u5f0f\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\uff08\u8d1f\u8f7d\u66f2\u7ebf\u3001\u5355\u7eaf\u5f62\u56fe\uff09\u3002", "result": "\u5728\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLTs\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1f\u8f7d\u5f15\u5bfc\u5e72\u9884\u63d0\u5347\u63a8\u7406\u6548\u738715-30%\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "CLTs \u4f5c\u4e3a\u4e00\u79cd\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u4e0d\u4ec5\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\uff0c\u8fd8\u80fd\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1f\u8f7d\u5f15\u5bfc\u5e72\u9884\u63d0\u5347\u63a8\u7406\u6548\u738715-30%\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2510.17714", "categories": ["cs.DS", "cs.LG", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.17714", "abs": "https://arxiv.org/abs/2510.17714", "authors": ["Atticus McWhorter", "Daryl DeFord"], "title": "The Marked Edge Walk: A Novel MCMC Algorithm for Sampling of Graph Partitions", "comment": null, "summary": "Novel Markov Chain Monte Carlo (MCMC) methods have enabled the generation of\nlarge ensembles of redistricting plans through graph partitioning. However,\nexisting algorithms such as Reversible Recombination (RevReCom) and\nMetropolized Forest Recombination (MFR) are constrained to sampling from\ndistributions related to spanning trees. We introduce the marked edge walk\n(MEW), a novel MCMC algorithm for sampling from the space of graph partitions\nunder a tunable distribution. The walk operates on the space of spanning trees\nwith marked edges, allowing for calculable transition probabilities for use in\nthe Metropolis-Hastings algorithm. Empirical results on real-world dual graphs\nshow convergence under target distributions unrelated to spanning trees. For\nthis reason, MEW represents an advancement in flexible ensemble generation.", "AI": {"tldr": "MEW\u662f\u4e00\u79cd\u65b0\u578bMCMC\u7b97\u6cd5\uff0c\u901a\u8fc7\u6807\u8bb0\u8fb9\u884c\u8d70\u5b9e\u73b0\u5bf9\u56fe\u5206\u533a\u7684\u7075\u6d3b\u91c7\u6837\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u751f\u6210\u6811\u76f8\u5173\u5206\u5e03\u7684\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u7684MCMC\u65b9\u6cd5\uff08\u5982RevReCom\u548cMFR\uff09\u5c40\u9650\u4e8e\u751f\u6210\u6811\u76f8\u5173\u7684\u5206\u5e03\u91c7\u6837\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u56fe\u5206\u533a\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "MEW\u662f\u4e00\u79cd\u65b0\u578b\u7684MCMC\u7b97\u6cd5\uff0c\u57fa\u4e8e\u751f\u6210\u6811\u548c\u6807\u8bb0\u8fb9\u7684\u7a7a\u95f4\u8fdb\u884c\u64cd\u4f5c\uff0c\u901a\u8fc7Metropolis-Hastings\u7b97\u6cd5\u8ba1\u7b97\u8f6c\u79fb\u6982\u7387\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u53cc\u56fe\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMEW\u80fd\u591f\u5728\u4e0e\u751f\u6210\u6811\u65e0\u5173\u7684\u76ee\u6807\u5206\u5e03\u4e0b\u5b9e\u73b0\u6536\u655b\u3002", "conclusion": "MEW\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u6807\u8bb0\u8fb9\u884c\u8d70\uff08MEW\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u8c03\u7684\u56fe\u5206\u533a\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MCMC\u5728\u975e\u751f\u6210\u6811\u76f8\u5173\u5206\u5e03\u4e0b\u7684\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2510.17110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17110", "abs": "https://arxiv.org/abs/2510.17110", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs", "comment": "This paper was accepted by ASE2025", "summary": "With the growing interest in quantum computing, the emergence of quantum\nsupremacy has marked a pivotal milestone in the field. As a result, numerous\nquantum programming languages (QPLs) have been introduced to support the\ndevelopment of quantum algorithms. However, the application of Model-Driven\nDevelopment (MDD) in quantum system engineering remains largely underexplored.\nThis paper presents an MDD-based approach to support the structured design and\nimplementation of quantum systems. Our framework enables the automatic\ngeneration of quantum code for multiple QPLs, thereby enhancing development\nefficiency and consistency across heterogeneous quantum platforms. The\neffectiveness and practicality of our approach have been demonstrated through\nmultiple case studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMDD\u7684\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u591a\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\uff0c\u63d0\u5347\u91cf\u5b50\u7cfb\u7edf\u5f00\u53d1\u6548\u7387\uff0c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u91cf\u5b50\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e9f\u9700\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\uff08MDD\uff09\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u591a\u79cd\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\uff08QPLs\uff09\u7684\u4ee3\u7801\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eMDD\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u7cfb\u7edf\u5f00\u53d1\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u79cd\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\uff0c\u652f\u6301\u5f02\u6784\u91cf\u5b50\u5e73\u53f0\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.16179", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16179", "abs": "https://arxiv.org/abs/2510.16179", "authors": ["Xavier Giro-i-Nieto", "Nefeli Andreou", "Anqi Liang", "Manel Baradad", "Francesc Moreno-Noguer", "Aleix Martinez"], "title": "Cost Savings from Automatic Quality Assessment of Generated Images", "comment": null, "summary": "Deep generative models have shown impressive progress in recent years, making\nit possible to produce high quality images with a simple text prompt or a\nreference image. However, state of the art technology does not yet meet the\nquality standards offered by traditional photographic methods. For this reason,\nproduction pipelines that use generated images often include a manual stage of\nimage quality assessment (IQA). This process is slow and expensive, especially\nbecause of the low yield of automatically generated images that pass the\nquality bar. The IQA workload can be reduced by introducing an automatic\npre-filtering stage, that will increase the overall quality of the images sent\nto review and, therefore, reduce the average cost required to obtain a high\nquality image. We present a formula that estimates the cost savings depending\non the precision and pass yield of a generic IQA engine. This formula is\napplied in a use case of background inpainting, showcasing a significant cost\nsaving of 51.61% obtained with a simple AutoML solution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9884\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u901a\u8fc7\u516c\u5f0f\u4f30\u8ba1\u6210\u672c\u8282\u7ea6\uff0c\u5e76\u5728\u80cc\u666f\u4fee\u590d\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e8651.61%\u7684\u6210\u672c\u8282\u7ea6\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u5c1a\u672a\u8fbe\u5230\u4f20\u7edf\u6444\u5f71\u65b9\u6cd5\u7684\u6807\u51c6\uff0c\u624b\u52a8\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u8fc7\u7a0b\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5f0f\uff0c\u7528\u4e8e\u4f30\u8ba1\u901a\u7528IQA\u5f15\u64ce\u7684\u7cbe\u5ea6\u548c\u901a\u8fc7\u7387\u5bf9\u6210\u672c\u8282\u7ea6\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u80cc\u666f\u4fee\u590d\u7684\u7528\u4f8b\u4e2d\u5e94\u7528\u8be5\u516c\u5f0f\u3002", "result": "\u5728\u80cc\u666f\u4fee\u590d\u7684\u7528\u4f8b\u4e2d\uff0c\u901a\u8fc7\u81ea\u52a8\u9884\u8fc7\u6ee4\u9636\u6bb5\u5b9e\u73b0\u4e8651.61%\u7684\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9884\u8fc7\u6ee4\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u83b7\u53d6\u7684\u5e73\u5747\u6210\u672c\uff0c\u901a\u8fc7\u7b80\u5355\u7684AutoML\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e8651.61%\u7684\u6210\u672c\u8282\u7ea6\u3002"}}
{"id": "2510.16692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16692", "abs": "https://arxiv.org/abs/2510.16692", "authors": ["Tianshu Ruan", "Zoe Betta", "Georgios Tzoumas", "Rustam Stolkin", "Manolis Chiou"], "title": "First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response", "comment": null, "summary": "This study investigates First Responders' (FRs) attitudes toward the use of\nsemantic information and Situational Awareness (SA) in robotic systems during\nemergency operations. A structured questionnaire was administered to 22 FRs\nacross eight countries, capturing their demographic profiles, general attitudes\ntoward robots, and experiences with semantics-enhanced SA. Results show that\nmost FRs expressed positive attitudes toward robots, and rated the usefulness\nof semantic information for building SA at an average of 3.6 out of 5. Semantic\ninformation was also valued for its role in predicting unforeseen emergencies\n(mean 3.9). Participants reported requiring an average of 74.6\\% accuracy to\ntrust semantic outputs and 67.8\\% for them to be considered useful, revealing a\nwillingness to use imperfect but informative AI support tools.\n  To the best of our knowledge, this study offers novel insights by being one\nof the first to directly survey FRs on semantic-based SA in a cross-national\ncontext. It reveals the types of semantic information most valued in the field,\nsuch as object identity, spatial relationships, and risk context-and connects\nthese preferences to the respondents' roles, experience, and education levels.\nThe findings also expose a critical gap between lab-based robotics capabilities\nand the realities of field deployment, highlighting the need for more\nmeaningful collaboration between FRs and robotics researchers. These insights\ncontribute to the development of more user-aligned and situationally aware\nrobotic systems for emergency response.", "AI": {"tldr": "\u7b2c\u4e00\u54cd\u5e94\u8005\u5bf9\u8bed\u4e49\u589e\u5f3a\u60c5\u5883\u610f\u8bc6\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u4f46\u9700\u8981\u5728\u51c6\u786e\u7387\u4e0a\u8fbe\u5230\u4e00\u5b9a\u6807\u51c6\u624d\u80fd\u4fe1\u4efb\u548c\u4f7f\u7528\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5b9e\u9a8c\u5ba4\u4e0e\u73b0\u573a\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u8c03\u67e5\u7b2c\u4e00\u54cd\u5e94\u8005\u5bf9\u5728\u7d27\u6025\u884c\u52a8\u4e2d\u4f7f\u7528\u8bed\u4e49\u4fe1\u606f\u548c\u60c5\u5883\u610f\u8bc6\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6001\u5ea6\uff0c\u586b\u8865\u4e86\u76f4\u63a5\u5728\u7b2c\u4e00\u54cd\u5e94\u8005\u4e2d\u8c03\u67e5\u8bed\u4e49\u57fa\u7840\u60c5\u5883\u610f\u8bc6\u7684\u8de8\u56fd\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u95ee\u5377\u5bf9\u6765\u81ea\u516b\u4e2a\u56fd\u5bb6\u768422\u540d\u7b2c\u4e00\u54cd\u5e94\u8005\u8fdb\u884c\u8c03\u67e5\uff0c\u6536\u96c6\u4e86\u4ed6\u4eec\u7684\u4eba\u53e3\u7edf\u8ba1\u8d44\u6599\u3001\u5bf9\u673a\u5668\u4eba\u7684\u4e00\u822c\u6001\u5ea6\u4ee5\u53ca\u4f7f\u7528\u8bed\u4e49\u589e\u5f3a\u60c5\u5883\u610f\u8bc6\u7684\u7ecf\u9a8c\u3002", "result": "\u5927\u591a\u6570\u7b2c\u4e00\u54cd\u5e94\u8005\u5bf9\u673a\u5668\u4eba\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u8bed\u4e49\u4fe1\u606f\u5728\u6784\u5efa\u60c5\u5883\u610f\u8bc6\u4e2d\u7684\u6709\u7528\u6027\u5e73\u5747\u8bc4\u5206\u4e3a3.6/5\uff0c\u5bf9\u9884\u6d4b\u672a\u9884\u89c1\u7d27\u6025\u60c5\u51b5\u7684\u5e73\u5747\u8bc4\u5206\u4e3a3.9\u3002\u53c2\u4e0e\u8005\u8868\u793a\u9700\u898174.6%\u7684\u51c6\u786e\u7387\u624d\u80fd\u4fe1\u4efb\u8bed\u4e49\u8f93\u51fa\uff0c67.8%\u7684\u51c6\u786e\u7387\u8ba4\u4e3a\u5176\u6709\u7528\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b9e\u9a8c\u5ba4\u673a\u5668\u4eba\u80fd\u529b\u4e0e\u73b0\u573a\u90e8\u7f72\u73b0\u5b9e\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u7b2c\u4e00\u54cd\u5e94\u8005\u4e0e\u673a\u5668\u4eba\u7814\u7a76\u4eba\u5458\u4e4b\u95f4\u9700\u8981\u66f4\u6709\u610f\u4e49\u7684\u5408\u4f5c\u3002\u8fd9\u4e9b\u89c1\u89e3\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u4e14\u5177\u6709\u60c5\u5883\u610f\u8bc6\u7684\u5e94\u6025\u54cd\u5e94\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u901a\u8fc7DAG\u548c\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\uff0c\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7ed3\u6784\u4fdd\u771f\u548c\u6027\u80fd\uff0cProofScore\u8fbe0.545\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f46\u5e38\u65e0\u6cd5\u4fdd\u7559\u539f\u59cb\u8bba\u8bc1\u7684\u8bed\u4e49\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6ce8\u91cd\u7ed3\u6784\u4fdd\u771f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "ProofFlow\u9996\u5148\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u9010\u6b65\u5f62\u5f0f\u5316\u6bcf\u4e2a\u6b65\u9aa4\u4e3a\u4e2d\u95f4\u5f15\u7406\u3002", "result": "ProofFlow\u5728184\u4e2a\u672c\u79d1\u7ea7\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0cProofScore\u4e3a0.545\uff0c\u663e\u8457\u4f18\u4e8e\u5b8c\u6574\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.123\uff09\u548c\u9010\u6b65\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.072\uff09\u3002", "conclusion": "ProofFlow\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u4fdd\u771f\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6027\u80fd\uff0c\u5176ProofScore\u8fbe\u52300.545\uff0c\u8fdc\u8d85\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.17740", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17740", "abs": "https://arxiv.org/abs/2510.17740", "authors": ["Shunhua Jiang", "Michael Kapralov", "Lawrence Li", "Aaron Sidford"], "title": "Generalized Flow in Nearly-linear Time on Moderately Dense Graphs", "comment": "65 pages. FOCS 2025", "summary": "In this paper we consider generalized flow problems where there is an\n$m$-edge $n$-node directed graph $G = (V,E)$ and each edge $e \\in E$ has a loss\nfactor $\\gamma_e >0$ governing whether the flow is increased or decreased as it\ncrosses edge $e$. We provide a randomized $\\tilde{O}( (m + n^{1.5}) \\cdot\n\\mathrm{polylog}(\\frac{W}{\\delta}))$ time algorithm for solving the generalized\nmaximum flow and generalized minimum cost flow problems in this setting where\n$\\delta$ is the target accuracy and $W$ is the maximum of all costs,\ncapacities, and loss factors and their inverses. This improves upon the\nprevious state-of-the-art $\\tilde{O}(m \\sqrt{n} \\cdot \\log^2(\\frac{W}{\\delta})\n)$ time algorithm, obtained by combining the algorithm of [Daitch-Spielman,\n2008] with techniques from [Lee-Sidford, 2014]. To obtain this result we\nprovide new dynamic data structures and spectral results regarding the matrices\nassociated to generalized flows and apply them through the interior point\nmethod framework of [Brand-Lee-Liu-Saranurak-Sidford-Song-Wang, 2021].", "AI": {"conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e7f\u4e49\u6700\u5927\u6d41\u548c\u5e7f\u4e49\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\uff0c\u5176\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\\tilde{O}( (m + n^{1.5}) \\cdot \\mathrm{polylog}(\\frac{W}{\\delta}))$\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u65b0\u7684\u52a8\u6001\u6570\u636e\u7ed3\u6784\u548c\u5173\u4e8e\u5e7f\u4e49\u6d41\u77e9\u9635\u7684\u8c31\u7ed3\u679c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e[BLLSSSW21]\u7684\u5185\u70b9\u6cd5\u6846\u67b6\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u5e7f\u4e49\u6d41\u95ee\u9898\u7b97\u6cd5\u65f6\u95f4\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e7f\u4e49\u6d41\u95ee\u9898\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems.", "AI": {"tldr": "SEER\u662f\u4e00\u4e2a\u81ea\u63a2\u7d22\u6df1\u5ea6\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u8def\u5f84\u63a2\u7d22\u548c\u8d28\u91cf\u611f\u77e5\u8bad\u7ec3\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709CoT\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e2d\u95f4\u6b65\u9aa4\u8d28\u91cf\u8bc4\u4f30\u548c\u2018\u8fc7\u5ea6\u601d\u8003\u2019\u95ee\u9898\uff0cSEER\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "SEER\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c(2) \u63a8\u7406\u8d28\u91cf\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\uff0c(3) \u81ea\u9002\u5e94CoT\u63a8\u7406\u3002", "result": "SEER\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u4ee3\u7801\uff0c\u5c24\u5176\u5728\u591a\u6837\u5316\u7f16\u7a0b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SEER\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u3001\u63a8\u7406\u8d28\u91cf\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\u548c\u81ea\u9002\u5e94CoT\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709CoT\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.16196", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16196", "abs": "https://arxiv.org/abs/2510.16196", "authors": ["Zheng Huang", "Enpei Zhang", "Yinghao Cai", "Weikang Qiu", "Carl Yang", "Elynn Chen", "Xiang Zhang", "Rex Ying", "Dawei Zhou", "Yujun Yan"], "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "comment": null, "summary": "Understanding how the brain encodes visual information is a central challenge\nin neuroscience and machine learning. A promising approach is to reconstruct\nvisual stimuli, essentially images, from functional Magnetic Resonance Imaging\n(fMRI) signals. This involves two stages: transforming fMRI signals into a\nlatent space and then using a pretrained generative model to reconstruct\nimages. The reconstruction quality depends on how similar the latent space is\nto the structure of neural activity and how well the generative model produces\nimages from that space. Yet, it remains unclear which type of latent space best\nsupports this transformation and how it should be organized to represent visual\nstimuli effectively. We present two key findings. First, fMRI signals are more\nsimilar to the text space of a language model than to either a vision based\nspace or a joint text image space. Second, text representations and the\ngenerative model should be adapted to capture the compositional nature of\nvisual stimuli, including objects, their detailed attributes, and\nrelationships. Building on these insights, we propose PRISM, a model that\nProjects fMRI sIgnals into a Structured text space as an interMediate\nrepresentation for visual stimuli reconstruction. It includes an object centric\ndiffusion module that generates images by composing individual objects to\nreduce object detection errors, and an attribute relationship search module\nthat automatically identifies key attributes and relationships that best align\nwith the neural activity. Extensive experiments on real world datasets\ndemonstrate that our framework outperforms existing methods, achieving up to an\n8% reduction in perceptual loss. These results highlight the importance of\nusing structured text as the intermediate space to bridge fMRI signals and\nimage reconstruction.", "AI": {"tldr": "PRISM\u6a21\u578b\u901a\u8fc7\u5c06fMRI\u4fe1\u53f7\u6295\u5c04\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u5e76\u7ed3\u5408\u5bf9\u8c61\u7ec4\u5408\u548c\u5c5e\u6027\u5173\u7cfb\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u5982\u4f55\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002\u4ecefMRI\u4fe1\u53f7\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\uff08\u56fe\u50cf\uff09\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u54ea\u79cd\u6f5c\u5728\u7a7a\u95f4\u6700\u9002\u5408\u8fd9\u79cd\u8f6c\u6362\u53ca\u5176\u7ec4\u7ec7\u65b9\u5f0f\u5c1a\u4e0d\u660e\u786e\u3002", "method": "PRISM\u6a21\u578b\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u6a21\u5757\uff0c\u901a\u8fc7\u7ec4\u5408\u5355\u4e2a\u5bf9\u8c61\u751f\u6210\u56fe\u50cf\u4ee5\u51cf\u5c11\u5bf9\u8c61\u68c0\u6d4b\u9519\u8bef\uff1b2) \u5c5e\u6027\u5173\u7cfb\u641c\u7d22\u6a21\u5757\uff0c\u81ea\u52a8\u8bc6\u522b\u4e0e\u795e\u7ecf\u6d3b\u52a8\u6700\u5339\u914d\u7684\u5173\u952e\u5c5e\u6027\u548c\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u6846\u67b6\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe8%\u7684\u611f\u77e5\u635f\u5931\u51cf\u5c11\u3002fMRI\u4fe1\u53f7\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u7a7a\u95f4\u76f8\u4f3c\u5ea6\u9ad8\u4e8e\u89c6\u89c9\u6216\u6587\u672c\u56fe\u50cf\u8054\u5408\u7a7a\u95f4\u3002", "conclusion": "\u4f7f\u7528\u7ed3\u6784\u5316\u6587\u672c\u4f5c\u4e3a\u4e2d\u95f4\u7a7a\u95f4\u6765\u8fde\u63a5fMRI\u4fe1\u53f7\u548c\u56fe\u50cf\u91cd\u5efa\u7684\u91cd\u8981\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002PRISM\u6a21\u578b\u901a\u8fc7\u5c06fMRI\u4fe1\u53f7\u6295\u5c04\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u6a21\u5757\u548c\u5c5e\u6027\u5173\u7cfb\u641c\u7d22\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.16738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16738", "abs": "https://arxiv.org/abs/2510.16738", "authors": ["Matteo El-Hariry", "Vittorio Franzese", "Miguel Olivares-Mendez"], "title": "Towards Active Excitation-Based Dynamic Inertia Identification in Satellites", "comment": null, "summary": "This paper presents a comprehensive analysis of how excitation design\ninfluences the identification of the inertia properties of rigid nano- and\nmicro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel\ncoupling, actuator limits, and external disturbances, and excite the system\nusing eight torque profiles of varying spectral richness. Two estimators are\ncompared, a batch Least Squares method and an Extended Kalman Filter, across\nthree satellite configurations and time-varying inertia scenarios. Results show\nthat excitation frequency content and estimator assumptions jointly determine\nestimation accuracy and robustness, offering practical guidance for in-orbit\nadaptive inertia identification by outlining the conditions under which each\nmethod performs best. The code is provided as open-source .", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6fc0\u52b1\u8bbe\u8ba1\u5bf9\u536b\u661f\u60ef\u6027\u8bc6\u522b\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u4f30\u8ba1\u5668\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u5728\u8f68\u8bc6\u522b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u7814\u7a76\u6fc0\u52b1\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u521a\u4f53\u7eb3\u7c73\u548c\u5fae\u578b\u536b\u661f\u7684\u60ef\u6027\u5c5e\u6027\u8bc6\u522b\uff0c\u4ee5\u63d0\u4f9b\u5728\u8f68\u81ea\u9002\u5e94\u60ef\u6027\u8bc6\u522b\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "method": "\u6a21\u62df\u975e\u7ebf\u6027\u59ff\u6001\u52a8\u529b\u5b66\uff0c\u5305\u62ec\u53cd\u5e94\u8f6e\u8026\u5408\u3001\u6267\u884c\u5668\u9650\u5236\u548c\u5916\u90e8\u5e72\u6270\uff0c\u4f7f\u7528\u516b\u79cd\u4e0d\u540c\u9891\u8c31\u4e30\u5bcc\u5ea6\u7684\u626d\u77e9\u5256\u9762\u6fc0\u52b1\u7cfb\u7edf\uff0c\u5e76\u6bd4\u8f83\u6279\u91cf\u6700\u5c0f\u4e8c\u4e58\u6cd5\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e24\u79cd\u4f30\u8ba1\u5668\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6fc0\u52b1\u9891\u7387\u5185\u5bb9\u548c\u4f30\u8ba1\u5668\u5047\u8bbe\u5171\u540c\u51b3\u5b9a\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u6fc0\u52b1\u8bbe\u8ba1\u548c\u4f30\u8ba1\u5668\u5047\u8bbe\u5bf9\u60ef\u6027\u5c5e\u6027\u8bc6\u522b\u7684\u5f71\u54cd\uff0c\u672c\u6587\u4e3a\u5728\u8f68\u81ea\u9002\u5e94\u60ef\u6027\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u660e\u786e\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6700\u4f73\u9002\u7528\u6761\u4ef6\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u57fa\u4e8eMO|RE\u6570\u636e\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u8bed\u4e49\u6a21\u578b\u6807\u51c6\u5316\u8fd0\u52a8\u8868\u73b0\u6570\u636e\uff0c\u4fc3\u8fdb\u4f53\u80b2\u79d1\u5b66\u7814\u7a76\u7684\u6570\u5b57\u5316\u8f6c\u578b\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u4eba\u7fa4\u7684\u751f\u7406\u548c\u8ba4\u77e5\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u7684\u65b9\u6cd5\u6765\u6d4b\u8bd5\u548c\u5206\u6790\u8fd0\u52a8\u8868\u73b0\u6570\u636e\uff0cMO|RE\u6570\u636e\u4ed3\u5e93\u4e3a\u6b64\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u8bba\uff08Basic Formal Ontology\uff09\u7684\u8bed\u4e49\u6a21\u578b\uff0c\u5c06\u8ba1\u5212\u89c4\u8303\u3001\u5177\u4f53\u8fc7\u7a0b\u53ca\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u5f62\u5f0f\u5316\u8868\u793a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06MO|RE\u6570\u636e\u8f6c\u5316\u4e3a\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u6027\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u57fa\u4e8eMO|RE\u6570\u636e\u77e5\u8bc6\u56fe\u8c31\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u7684\u65b9\u5f0f\u5efa\u6a21\u548c\u5171\u4eab\u8fd0\u52a8\u8868\u73b0\u6570\u636e\uff0c\u63a8\u52a8\u4f53\u80b2\u79d1\u5b66\u7814\u7a76\u7684\u6570\u5b57\u5316\u8f6c\u578b\u3002"}}
{"id": "2510.17752", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17752", "abs": "https://arxiv.org/abs/2510.17752", "authors": ["Panagiotis Charalampopoulos", "Tomasz Kociumaka", "Philip Wellnitz"], "title": "Pattern Matching under Weighted Edit Distance", "comment": "96 pages + bibliography + index of results, 8 figures. Sections 7 and\n  8 of this article generalize and heavily draw from our earlier works\n  arXiv:2004.08350 and arXiv:2204.03087", "summary": "In Pattern Matching with Weighted Edits (PMWED), we are given a pattern $P$\nof length $m$, a text $T$ of length $n$, a positive threshold $k$, and oracle\naccess to a weight function that specifies the costs of edits (depending on the\ninvolved characters, and normalized so that the cost of each edit is at least\n$1$). The goal is to compute the starting positions of all fragments of $T$\nthat can be obtained from $P$ with edits of total cost at most $k$. PMWED\ncaptures typical real-world applications more accurately than its unweighted\nvariant (PMED), where all edits have unit costs.\n  We obtain three main results:\n  (a) a conceptually simple $\\tilde{O}(nk)$-time algorithm for PMWED, very\ndifferent from that of Landau and Vishkin for PMED;\n  (b) a significantly more complicated $\\tilde{O}(n+k^{3.5} \\cdot W^4\\cdot\nn/m)$-time algorithm for PMWED under the assumption that the weight function is\na metric with integer values between $0$ and $W$; and\n  (c) an $\\tilde{O}(n+k^4 \\cdot n/m)$-time algorithm for PMWED for the case of\narbitrary weights.\n  In the setting of metrics with small integer values, we nearly match the\nstate of the art for PMED where $W=1$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u9488\u5bf9\u52a0\u6743\u7f16\u8f91\u8ddd\u79bb\u6a21\u5f0f\u5339\u914d\u7684\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u4e0d\u540c\u6743\u91cd\u60c5\u51b5\u4e0b\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u7279\u522b\u5728\u6574\u6570\u5ea6\u91cf\u6743\u91cd\u60c5\u51b5\u4e0b\u63a5\u8fd1\u672a\u52a0\u6743\u53d8\u4f53\u7684\u6700\u4f18\u89e3\u3002", "motivation": "PMWED\u6bd4\u672a\u52a0\u6743\u7684\u53d8\u4f53\uff08PMED\uff09\u66f4\u80fd\u51c6\u786e\u53cd\u6620\u73b0\u5b9e\u5e94\u7528\uff0c\u56e0\u4e3a\u5176\u8003\u8651\u4e86\u4e0d\u540c\u7f16\u8f91\u64cd\u4f5c\u7684\u5b9e\u9645\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3PMWED\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\uff08a\uff09\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u7b80\u5355\u7684\u7b97\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\\tilde{O}(nk)$\uff1b\uff08b\uff09\u5728\u6743\u91cd\u51fd\u6570\u4e3a\u6574\u6570\u5ea6\u91cf\u4e14\u53d6\u503c\u8303\u56f4\u5728$0$\u5230$W$\u4e4b\u95f4\u7684\u5047\u8bbe\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u590d\u6742\u7684\u7b97\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\\tilde{O}(n+k^{3.5} \\cdot W^4\\cdot n/m)$\uff1b\uff08c\uff09\u9488\u5bf9\u4efb\u610f\u6743\u91cd\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\\tilde{O}(n+k^4 \\cdot n/m)$\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u6574\u6570\u5ea6\u91cf\u6743\u91cd\u60c5\u51b5\u4e0b\uff0c\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u63a5\u8fd1PMED\u7684\u6700\u4f18\u89e3\uff08\u5f53$W=1$\u65f6\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u9488\u5bf9\u52a0\u6743\u7f16\u8f91\u8ddd\u79bb\u6a21\u5f0f\u5339\u914d\uff08PMWED\uff09\u7684\u7b97\u6cd5\uff0c\u5206\u522b\u5728\u4e00\u822c\u6743\u91cd\u3001\u6574\u6570\u5ea6\u91cf\u6743\u91cd\u548c\u4efb\u610f\u6743\u91cd\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u5316\u3002"}}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.", "AI": {"tldr": "Peace\u662f\u4e00\u4e2a\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4ee3\u7801\u7f16\u8f91\u5b9e\u73b0\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4ee3\u7801\u6548\u7387\u4f18\u5316\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u51fd\u6570\u7ea7\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u51fd\u6570\u95f4\u4ea4\u4e92\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u3002", "method": "Peace\u6846\u67b6\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u4f9d\u8d56\u611f\u77e5\u7684\u4f18\u5316\u51fd\u6570\u5e8f\u5217\u6784\u5efa\u3001\u6709\u6548\u5173\u8054\u7f16\u8f91\u8bc6\u522b\u548c\u6548\u7387\u4f18\u5316\u7f16\u8f91\u8fed\u4ee3\u3002", "result": "Peace\u5728146\u4e2a\u771f\u5b9e\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6b63\u786e\u7387\u8fbe69.2%\uff08pass@1\uff09\uff0c\u4f18\u5316\u7387\u63d0\u534746.9%\uff0c\u6267\u884c\u6548\u7387\u52a0\u901f0.840\u500d\u3002", "conclusion": "Peace\u6846\u67b6\u5728\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u591a\u51fd\u6570\u7684\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2510.16207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16207", "abs": "https://arxiv.org/abs/2510.16207", "authors": ["Mateus Pinto da Silva", "Sabrina P. L. P. Correa", "Hugo N. Oliveira", "Ian M. Nunes", "Jefersson A. dos Santos"], "title": "Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions", "comment": "5 pages, 1 figure", "summary": "Mapping agriculture in tropical areas through remote sensing presents unique\nchallenges, including the lack of high-quality annotated data, the elevated\ncosts of labeling, data variability, and regional generalisation. This paper\nadvocates a Data-Centric Artificial Intelligence (DCAI) perspective and\npipeline, emphasizing data quality and curation as key drivers for model\nrobustness and scalability. It reviews and prioritizes techniques such as\nconfident learning, core-set selection, data augmentation, and active learning.\nThe paper highlights the readiness and suitability of 25 distinct strategies in\nlarge-scale agricultural mapping pipelines. The tropical context is of high\ninterest, since high cloudiness, diverse crop calendars, and limited datasets\nlimit traditional model-centric approaches. This tutorial outlines practical\nsolutions as a data-centric approach for curating and training AI models better\nsuited to the dynamic realities of tropical agriculture. Finally, we propose a\npractical pipeline using the 9 most mature and straightforward methods that can\nbe applied to a large-scale tropical agricultural mapping project.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3AI\u65b9\u6cd5\u89e3\u51b3\u70ed\u5e26\u519c\u4e1a\u9065\u611f\u5236\u56fe\u6311\u6218\uff0c\u63a8\u83509\u79cd\u5b9e\u7528\u6280\u672f\u3002", "motivation": "\u70ed\u5e26\u519c\u4e1a\u9065\u611f\u5236\u56fe\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u6570\u636e\u53d8\u5f02\u6027\u548c\u533a\u57df\u6cdb\u5316\u6027\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u4e2d\u5fc3\u65b9\u6cd5\u53d7\u9650\u3002", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684AI\u89c6\u89d2\u548c\u6d41\u7a0b\uff0c\u91cd\u70b9\u5305\u62ec\u6570\u636e\u8d28\u91cf\u548c\u6574\u7406\uff0c\u8bc4\u4f30\u4e86\u5982\u7f6e\u4fe1\u5b66\u4e60\u3001\u6838\u5fc3\u96c6\u9009\u62e9\u3001\u6570\u636e\u589e\u5f3a\u548c\u4e3b\u52a8\u5b66\u4e60\u7b49\u6280\u672f\u3002", "result": "\u786e\u5b9a\u4e8625\u79cd\u7b56\u7565\u5728\u5927\u89c4\u6a21\u519c\u4e1a\u5236\u56fe\u6d41\u7a0b\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a8\u8350\u4e86\u5176\u4e2d9\u79cd\u6700\u6210\u719f\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684AI\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u70ed\u5e26\u519c\u4e1a\u9065\u611f\u5236\u56fe\uff0c\u63a8\u8350\u4e869\u79cd\u6210\u719f\u4e14\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9879\u76ee\u3002"}}
{"id": "2510.16755", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16755", "abs": "https://arxiv.org/abs/2510.16755", "authors": ["Kyung-Hwan Kim", "DongHyun Ahn", "Dong-hyun Lee", "JuYoung Yoon", "Dong Jin Hyun"], "title": "Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation", "comment": "6 pages, accepted to IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "State estimation is crucial for legged robots as it directly affects control\nperformance and locomotion stability. In this paper, we propose an Adaptive\nInvariant Extended Kalman Filter to improve proprioceptive state estimation for\nlegged robots. The proposed method adaptively adjusts the noise level of the\ncontact foot model based on online covariance estimation, leading to improved\nstate estimation under varying contact conditions. It effectively handles small\nslips that traditional slip rejection fails to address, as overly sensitive\nslip rejection settings risk causing filter divergence. Our approach employs a\ncontact detection algorithm instead of contact sensors, reducing the reliance\non additional hardware. The proposed method is validated through real-world\nexperiments on the quadruped robot LeoQuad, demonstrating enhanced state\nestimation performance in dynamic locomotion scenarios.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u6574\u566a\u58f0\u6c34\u5e73\u548c\u63a5\u89e6\u68c0\u6d4b\u7b97\u6cd5\uff0c\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\u76f4\u63a5\u5f71\u54cd\u63a7\u5236\u6027\u80fd\u548c\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u6ed1\u79fb\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u4e14\u53ef\u80fd\u5f15\u53d1\u6ee4\u6ce2\u5668\u53d1\u6563\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u57fa\u4e8e\u5728\u7ebf\u534f\u65b9\u5dee\u4f30\u8ba1\u8c03\u6574\u63a5\u89e6\u811a\u6a21\u578b\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u5e76\u91c7\u7528\u63a5\u89e6\u68c0\u6d4b\u7b97\u6cd5\u66ff\u4ee3\u63a5\u89e6\u4f20\u611f\u5668\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4ebaLeoQuad\u4e0a\u7684\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u63a5\u89e6\u811a\u6a21\u578b\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u6709\u6548\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u5bf9\u989d\u5916\u786c\u4ef6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u4eceRFS\u548cDST\u89c6\u89d2\u5206\u6790\u4e86RPS\u4e2d\u7684\u51b2\u7a81\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u91cd\u53e0\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5177\u6709\u9876\u90e8\u52a0\u6743\u6027\u8d28\uff0c\u5e76\u63d0\u4f9b\u4e86\u53c2\u6570\u7075\u6d3b\u9009\u62e9\u3002", "motivation": "\u6d4b\u91cf\u7531\u6392\u5217\u8d28\u91cf\u51fd\u6570\u8868\u793a\u7684\u4e24\u4e2a\u8bc1\u636e\u4e4b\u95f4\u7684\u51b2\u7a81\u662f\u987a\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u4e2d\u7684\u4e00\u4e2a\u7d27\u8feb\u7814\u7a76\u8bfe\u9898\u3002", "method": "\u672c\u6587\u4ece\u968f\u673a\u6709\u9650\u96c6\uff08RFS\uff09\u548cDempster-Shafer\u7406\u8bba\uff08DST\uff09\u4e24\u4e2a\u4e0d\u540c\u89c6\u89d2\u5bf9RPS\u4e2d\u7684\u51b2\u7a81\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002\u9996\u5148\u57fa\u4e8e\u6392\u5217\u7684\u89c2\u5bdf\uff0c\u5b9a\u4e49\u4e86\u6392\u5217\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86RPS\u7684\u975e\u91cd\u53e0\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u51b2\u7a81\u5ea6\u91cf\u7684\u884c\u4e3a\u548c\u6027\u8d28\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u6027\u8d28\uff0c\u80fd\u591f\u4eceDST\u89c6\u89d2\u6709\u6548\u8861\u91cfRPS\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u8fd8\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\u3002"}}
{"id": "2510.17799", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17799", "abs": "https://arxiv.org/abs/2510.17799", "authors": ["Debarati Das", "Jacob Gilbert", "MohammadTaghi Hajiaghayi", "Tomasz Kociumaka", "Barna Saha"], "title": "Dynamic Dyck and Tree Edit Distance: Decompositions and Reductions to String Edit Distance", "comment": "Full version of a FOCS 2025 paper", "summary": "We present the first dynamic algorithms for Dyck and tree edit distances with\nsubpolynomial update times. Dyck edit distance measures how far a parenthesis\nstring is from a well-parenthesized expression, while tree edit distance\nquantifies the minimum number of node insertions, deletions, and substitutions\nrequired to transform one rooted, ordered, labeled tree into another. Despite\nextensive study, no prior work has addressed efficient dynamic algorithms for\nthese problems, which naturally arise in evolving structured data such as LaTeX\ndocuments, JSON or XML files, and RNA secondary structures.\n  Our main contribution is a set of reductions and decompositions that\ntransform Dyck and tree edit distance instances into efficiently maintainable\nstring edit distance instances, which can be approximated within a $n^{o(1)}$\nfactor in $n^{o(1)}$ update time. For Dyck edit distance, our reduction incurs\nonly polylogarithmic overheads in approximation and update time, yielding an\n$n^{o(1)}$-approximation with $n^{o(1)}$ updates. For tree edit distance, we\nintroduce a new static reduction that improves the best-known approximation\nratio from $n^{3/4}$ to $\\tilde{O}(\\sqrt{n})$ and removes the restriction to\nconstant-degree trees. Extending this reduction dynamically achieves\n$n^{1/2+o(1)}$ approximation with $n^{o(1)}$ update time.\n  A key component is a dynamic maintenance algorithm for history-independent\nheavy-light decompositions, of independent interest. We also provide a novel\nstatic and dynamic decomposition achieving an $O(k \\log n)$-approximation when\nthe tree edit distance is at most $k$. Combined with the trivial bound $k \\le\nn$, this yields a dynamic deterministic $O(\\sqrt{n \\log n})$-approximation. In\nthe static setting, our algorithm runs in near-linear time; dynamically, it\nrequires only polylogarithmic updates, improving on prior linear-time static\n$O(\\sqrt{n})$-approximation.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u63d0\u51faDyck\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u52a8\u6001\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f52\u7ea6\u548c\u5206\u89e3\u6280\u672f\u5b9e\u73b0\u4e9a\u591a\u9879\u5f0f\u66f4\u65b0\u65f6\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u4f3c\u6bd4\u548c\u6548\u7387\u3002", "motivation": "Dyck\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\u5728\u52a8\u6001\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982LaTeX\u6587\u6863\u3001JSON/XML\u6587\u4ef6\u548cRNA\u4e8c\u7ea7\u7ed3\u6784\uff09\u4e2d\u81ea\u7136\u51fa\u73b0\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u9ad8\u6548\u7684\u52a8\u6001\u7b97\u6cd5\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u5f52\u7ea6\u548c\u5206\u89e3\u6280\u672f\uff0c\u5c06Dyck\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u52a8\u6001\u7ef4\u62a4\u7684\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\u95ee\u9898\u3002\u5bf9\u4e8eDyck\u7f16\u8f91\u8ddd\u79bb\uff0c\u5f52\u7ea6\u4ec5\u5f15\u5165\u591a\u5bf9\u6570\u7ea7\u5f00\u9500\uff1b\u5bf9\u4e8e\u6811\u7f16\u8f91\u8ddd\u79bb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9759\u6001\u5f52\u7ea6\uff0c\u5e76\u6269\u5c55\u4e3a\u52a8\u6001\u7248\u672c\u3002", "result": "\u8bba\u6587\u5b9e\u73b0\u4e86Dyck\u7f16\u8f91\u8ddd\u79bb\u7684n^{o(1)}\u8fd1\u4f3c\u6bd4\u548c\u66f4\u65b0\u65f6\u95f4\uff1b\u5bf9\u4e8e\u6811\u7f16\u8f91\u8ddd\u79bb\uff0c\u9759\u6001\u5f52\u7ea6\u5c06\u8fd1\u4f3c\u6bd4\u4ecen^{3/4}\u63d0\u5347\u81f3O\u0303(\u221an)\uff0c\u52a8\u6001\u7248\u672c\u5219\u8fbe\u5230n^{1/2+o(1)}\u8fd1\u4f3c\u6bd4\u548cn^{o(1)}\u66f4\u65b0\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9759\u6001\u548c\u52a8\u6001\u5206\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7b97\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86Dyck\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u4e9a\u591a\u9879\u5f0f\u66f4\u65b0\u65f6\u95f4\u3002\u901a\u8fc7\u5c06Dyck\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u5b9e\u4f8b\u8f6c\u6362\u4e3a\u53ef\u9ad8\u6548\u7ef4\u62a4\u7684\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\u5b9e\u4f8b\uff0c\u8bba\u6587\u5728\u8fd1\u4f3c\u6bd4\u548c\u66f4\u65b0\u65f6\u95f4\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\u3002"}}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;", "AI": {"tldr": "TREAT\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801LLM\u53ef\u4fe1\u8d56\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u3001\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u4efb\u52a1\u8303\u56f4\u548c\u8bc4\u4f30\u65b9\u9762\uff08\u5982\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff09\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86TREAT\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u591a\u4efb\u52a1\u8bc4\u4f30\u3001\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bc4\u4f30\u3001\u9c81\u68d2\u6027\u8bc4\u4f30\u4ee5\u53ca\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u4e8626\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u591a\u6a21\u6001\u6a21\u578b\u5728UI\u4ee3\u7801\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "TREAT\u6846\u67b6\u4e3a\u8bc4\u4f30\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u8d56\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u548c\u591a\u6a21\u6001\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16209", "abs": "https://arxiv.org/abs/2510.16209", "authors": ["Nyle Siddiqui", "Rohit Gupta", "Sirnam Swetha", "Mubarak Shah"], "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "comment": null, "summary": "State space models (SSMs) have emerged as a competitive alternative to\ntransformers in various tasks. Their linear complexity and hidden-state\nrecurrence make them particularly attractive for modeling long sequences,\nwhereas attention becomes quadratically expensive. However, current training\nmethods for video understanding are tailored towards transformers and fail to\nfully leverage the unique attributes of SSMs. For example, video models are\noften trained at a fixed resolution and video length to balance the quadratic\nscaling of attention cost against performance. Consequently, these models\nsuffer from degraded performance when evaluated on videos with spatial and\ntemporal resolutions unseen during training; a property we call spatio-temporal\ninflexibility. In the context of action recognition, this severely limits a\nmodel's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves\nthe inherent adaptability of SSMs. Our method samples videos at varying\ntemporal and spatial resolutions during training and dynamically interpolates\nmodel weights to accommodate any spatio-temporal scale. This instills our SSM,\nwhich we call StretchySnake, with spatio-temporal flexibility and enables it to\nseamlessly handle videos ranging from short, fine-grained clips to long,\ncomplex activities. We introduce and compare five different variants of\nflexible training, and identify the most effective strategy for video SSMs. On\nshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,\nStretchySnake outperforms transformer and SSM baselines alike by up to 28%,\nwith strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,\nour method provides a simple drop-in training recipe that makes video SSMs more\nrobust, resolution-agnostic, and efficient across diverse action recognition\nscenarios.", "AI": {"tldr": "\u63d0\u51faStretchySnake\uff0c\u4e00\u79cd\u7075\u6d3b\u8bad\u7ec3SSMs\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u89c6\u9891\u548c\u77ed\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9Transformer\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528SSMs\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u9690\u85cf\u72b6\u6001\u9012\u5f52\u7279\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u4e0d\u540c\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u89c6\u9891\u5e76\u52a8\u6001\u63d2\u503c\u6a21\u578b\u6743\u91cd\uff0c\u4f7fSSMs\u9002\u5e94\u5404\u79cd\u65f6\u7a7a\u5c3a\u5ea6\u3002", "result": "\u5728\u77ed\u52a8\u4f5c\uff08UCF-101, HMDB-51\uff09\u548c\u957f\u52a8\u4f5c\uff08COIN, Breakfast\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStretchySnake\u6bd4Transformer\u548cSSM\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe28%\uff0c\u4e14\u5bf9\u7ec6\u7c92\u5ea6\u52a8\u4f5c\uff08SSV2, Diving-48\uff09\u6709\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684SSMs\u8bad\u7ec3\u65b9\u6cd5StretchySnake\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u957f\u89c6\u9891\u548c\u77ed\u89c6\u9891\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16767", "abs": "https://arxiv.org/abs/2510.16767", "authors": ["Jia Li", "Guoxiang Zhao"], "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic", "comment": null, "summary": "Translating natural language instructions into executable motion plans is a\nfundamental challenge in robotics. Traditional approaches are typically\nconstrained by their reliance on domain-specific expertise to customize\nplanners, and often struggle with spatio-temporal couplings that usually lead\nto infeasible motions or discrepancies between task planning and motion\nexecution. Despite the proficiency of Large Language Models (LLMs) in\nhigh-level semantic reasoning, hallucination could result in infeasible motion\nplans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic\nmotion planning framework that self-corrects it output with formal methods. The\nframework decomposes spatio-temporal task constraints via three cascaded\nmodules, each of which stimulates an LLM to generate candidate trajectory\nsequences and examines their feasibility via a Signal Temporal Logic (STL)\nverifier until one that satisfies complex spatial, temporal, and logical\nconstraints is found.Experiments across different scenarios show that T3\nPlanner significantly outperforms the baselines. The required reasoning can be\ndistilled into a lightweight Qwen3-4B model that enables efficient deployment.\nAll supplementary materials are accessible at\nhttps://github.com/leeejia/T3_Planner.", "AI": {"tldr": "T3 Planner\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u81ea\u6211\u4fee\u6b63\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e14\u96be\u4ee5\u5904\u7406\u65f6\u7a7a\u8026\u5408\uff0c\u800cLLM\u867d\u64c5\u957f\u8bed\u4e49\u63a8\u7406\u4f46\u53ef\u80fd\u751f\u6210\u4e0d\u53ef\u884c\u8fd0\u52a8\u8ba1\u5212\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u6211\u4fee\u6b63\u7684\u6846\u67b6\u3002", "method": "T3 Planner\u91c7\u7528\u4e09\u4e2a\u7ea7\u8054\u6a21\u5757\u5206\u89e3\u65f6\u7a7a\u4efb\u52a1\u7ea6\u675f\uff0c\u5229\u7528LLM\u751f\u6210\u5019\u9009\u8f68\u8ff9\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7STL\u9a8c\u8bc1\u5668\u68c0\u67e5\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT3 Planner\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5176\u63a8\u7406\u80fd\u529b\u53ef\u84b8\u998f\u81f3\u8f7b\u91cf\u7ea7Qwen3-4B\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "T3 Planner\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5230\u53ef\u6267\u884c\u8fd0\u52a8\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u53ef\u884c\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "AI": {"tldr": "PAINT\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u5b6a\u751f\u4f53\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u52a8\u6001\u7cfb\u7edf\u4e2d\u4fdd\u6301\u8f68\u8ff9\u5e76\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u9ad8\u4fdd\u771f\u9884\u6d4b\u72b6\u6001\u3002", "motivation": "\u63d0\u51fa\u795e\u7ecf\u5b6a\u751f\u4f53\u4f5c\u4e3a\u795e\u7ecf\u4ee3\u7406\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u65e8\u5728\u521b\u5efa\u771f\u5b9e\u7cfb\u7edf\u7684\u6570\u5b57\u526f\u672c\uff0c\u4ee5\u652f\u6301\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u51b3\u7b56\u3002", "method": "PAINT\u662f\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u65b9\u6cd5\u5bb6\u65cf\uff0c\u901a\u8fc7\u8bad\u7ec3\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u5e76\u884c\u65f6\u95f4\u72b6\u6001\u5206\u5e03\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u4ee5\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u4ece\u6d4b\u91cf\u4e2d\u9884\u6d4b\u72b6\u6001\u3002", "result": "PAINT\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4e8c\u7ef4\u6e4d\u6d41\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4fdd\u6301\u8f68\u8ff9\u5e76\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u9ad8\u4fdd\u771f\u5730\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u3002", "conclusion": "PAINT\u5c55\u793a\u4e86\u5728\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u8f68\u8ff9\u7684\u795e\u7ecf\u5b6a\u751f\u4f53\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u51b3\u7b56\u3002"}}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u4e13\u4e1a\u4eba\u5458\u5982\u4f55\u5b9e\u9645\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u63d0\u51fa\u521d\u6b65\u6307\u5357\u4ee5\u652f\u6301\u5176\u96c6\u6210\u5230\u6d4b\u8bd5\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\u53d1\u73b0\uff0c\u6d4b\u8bd5\u8fc7\u7a0b\u6d89\u53ca\u8fed\u4ee3\u3001\u53cd\u601d\u548c\u4eba\u7c7b\u76d1\u7763\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u4f7f\u7528LLM\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u8fc5\u901f\u589e\u957f\uff0c\u652f\u6301\u4ece\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5230\u81ea\u52a8\u5316\u548c\u6587\u6863\u7684\u591a\u79cd\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5176\u91c7\u7528\u5f80\u5f80\u4f9d\u8d56\u4e8e\u975e\u6b63\u5f0f\u7684\u5b9e\u9a8c\uff0c\u800c\u975e\u7ed3\u6784\u5316\u6307\u5bfc\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u8f6f\u4ef6\u6d4b\u8bd5\u4e13\u4e1a\u4eba\u5458\u5982\u4f55\u5b9e\u9645\u4f7f\u7528LLM\uff0c\u4ee5\u63d0\u51fa\u521d\u6b65\u7684\u3001\u7531\u4ece\u4e1a\u8005\u6307\u5bfc\u7684\u6307\u5357\uff0c\u652f\u6301\u5176\u96c6\u6210\u5230\u6d4b\u8bd5\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u5b9a\u6027\u7814\u7a76\uff0c\u6d89\u53ca15\u540d\u6765\u81ea\u4e0d\u540c\u89d2\u8272\u548c\u9886\u57df\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u4eba\u5458\u3002\u6570\u636e\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6536\u96c6\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u7684\u4e3b\u9898\u5206\u6790\u8fc7\u7a0b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6d4b\u8bd5\u4eba\u5458\u63cf\u8ff0\u4e86\u4e00\u4e2a\u8fed\u4ee3\u548c\u53cd\u601d\u7684\u8fc7\u7a0b\uff0c\u5305\u62ec\u5b9a\u4e49\u6d4b\u8bd5\u76ee\u6807\u3001\u5e94\u7528\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3001\u4f18\u5316\u63d0\u793a\u3001\u8bc4\u4f30\u8f93\u51fa\u4ee5\u53ca\u968f\u65f6\u95f4\u5b66\u4e60\u3002\u4ed6\u4eec\u5f3a\u8c03\u4e86\u4eba\u7c7b\u76d1\u7763\u548c\u4ed4\u7ec6\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\uff0c\u7279\u522b\u662f\u7531\u4e8eLLM\u7684\u5df2\u77e5\u9650\u5236\uff0c\u5982\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u7684\u63a8\u7406\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u6b63\u5728\u589e\u957f\uff0c\u4f46\u4ecd\u53d7\u5236\u4e8e\u4e0d\u65ad\u53d1\u5c55\u7684\u5b9e\u8df5\u548c\u5bf9\u98ce\u9669\u7684\u8c28\u614e\u6001\u5ea6\u3002\u672c\u7814\u7a76\u4e3a\u5728\u6d4b\u8bd5\u73af\u5883\u4e2d\u7ed3\u6784\u5316\u4f7f\u7528LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d77\u70b9\uff0c\u5e76\u9080\u8bf7\u672a\u6765\u7814\u7a76\u5728\u56e2\u961f\u3001\u5de5\u5177\u548c\u4efb\u52a1\u4e2d\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u5b9e\u8df5\u3002"}}
{"id": "2510.16220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16220", "abs": "https://arxiv.org/abs/2510.16220", "authors": ["Djamel Eddine Boukhari"], "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction", "comment": null, "summary": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision\ntask, aiming to model the subjective and intricate nature of human aesthetic\nperception. While deep learning models, particularly Convolutional Neural\nNetworks (CNNs), have made significant strides, they often struggle to capture\nthe global, holistic facial features that are critical to human judgment.\nVision Transformers (ViT) address this by effectively modeling long-range\nspatial relationships, but their quadratic complexity can be a bottleneck. This\npaper introduces a novel, heterogeneous ensemble architecture,\n\\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths\nof a Vision Transformer and a Mamba-based Vision model, a recent advancement in\nState-Space Models (SSMs). The ViT backbone excels at capturing global facial\nstructure and symmetry, while the Mamba backbone efficiently models long-range\ndependencies with linear complexity, focusing on sequential features and\ntextures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our\nproposed VM-BeautyNet achieves state-of-the-art performance, with a\n\\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error\n(MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}.\nFurthermore, through Grad-CAM visualizations, we provide interpretability\nanalysis that confirms the complementary feature extraction of the two\nbackbones, offering new insights into the model's decision-making process and\npresenting a powerful new architectural paradigm for computational aesthetics.", "AI": {"tldr": "VM-BeautyNet\u7ed3\u5408ViT\u548cMamba\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u9762\u90e8\u7f8e\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982CNN\uff09\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u5ba1\u7f8e\u5224\u65ad\u4e2d\u5173\u952e\u7684\u5168\u5c40\u3001\u6574\u4f53\u9762\u90e8\u7279\u5f81\uff0c\u800cVision Transformers\u867d\u80fd\u6709\u6548\u5efa\u6a21\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\uff0c\u4f46\u5176\u4e8c\u6b21\u590d\u6742\u5ea6\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u6784\u96c6\u6210\u67b6\u6784VM-BeautyNet\uff0c\u878d\u5408\u4e86Vision Transformer\u548cMamba-based Vision\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u524d\u8005\u64c5\u957f\u6355\u6349\u5168\u5c40\u9762\u90e8\u7ed3\u6784\uff0c\u540e\u8005\u9ad8\u6548\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\uff0cVM-BeautyNet\u53d6\u5f97\u4e86Pearson Correlation (PC)\u4e3a0.9212\u3001Mean Absolute Error (MAE)\u4e3a0.2085\u3001Root Mean Square Error (RMSE)\u4e3a0.2698\u7684\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "VM-BeautyNet\u901a\u8fc7\u7ed3\u5408Vision Transformer\u548cMamba-based Vision\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u8ba1\u7b97\u7f8e\u5b66\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u67b6\u6784\u8303\u5f0f\u3002"}}
{"id": "2510.16771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16771", "abs": "https://arxiv.org/abs/2510.16771", "authors": ["Xu He", "Xiaolin Meng", "Wenxuan Yin", "Youdong Zhang", "Lingfei Mo", "Xiangdong An", "Fangwen Yu", "Shuguo Pan", "Yufeng Liu", "Jingnan Liu", "Yujia Zhang", "Wang Gao"], "title": "A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT", "comment": null, "summary": "Developing universal Positioning, Navigation, and Timing (PNT) is our\nenduring goal. Today's complex environments demand PNT that is more resilient,\nenergy-efficient and cognitively capable. This paper asks how we can endow\nunmanned systems with brain-inspired spatial cognition navigation while\nexploiting the high precision of machine PNT to advance universal PNT. We\nprovide a new perspective and roadmap for shifting PNT from \"tool-oriented\" to\n\"cognition-driven\". Contributions: (1) multi-level dissection of differences\namong traditional PNT, biological brain PNT and brain-inspired PNT; (2) a\nfour-layer (observation-capability-decision-hardware) fusion framework that\nunites numerical precision and brain-inspired intelligence; (3) forward-looking\nrecommendations for future development of brain-inspired PNT.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u8111\u542f\u53d1\u7a7a\u95f4\u8ba4\u77e5\u5bfc\u822a\u63d0\u5347\u65e0\u4eba\u673a\u7cfb\u7edf\u7684PNT\u80fd\u529b\uff0c\u63d0\u51fa\u56db\u5c42\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u503c\u7cbe\u5ea6\u4e0e\u8111\u542f\u53d1\u667a\u80fd\uff0c\u5e76\u5c55\u671b\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f39\u6027\u3001\u8282\u80fd\u4e14\u5177\u5907\u8ba4\u77e5\u80fd\u529b\u7684\u901a\u7528PNT\u7cfb\u7edf\uff0c\u63a8\u52a8PNT\u4ece\u201c\u5de5\u5177\u5bfc\u5411\u201d\u8f6c\u5411\u201c\u8ba4\u77e5\u9a71\u52a8\u201d\u3002", "method": "\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u6bd4\u8f83\u5206\u6790\uff08\u4f20\u7edfPNT\u3001\u751f\u7269\u5927\u8111PNT\u4e0e\u8111\u542f\u53d1PNT\uff09\u548c\u56db\u5c42\u878d\u5408\u6846\u67b6\uff08\u89c2\u6d4b-\u80fd\u529b-\u51b3\u7b56-\u786c\u4ef6\uff09\uff0c\u7ed3\u5408\u6570\u503c\u7cbe\u5ea6\u4e0e\u8111\u542f\u53d1\u667a\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u6570\u503c\u7cbe\u5ea6\u4e0e\u8111\u542f\u53d1\u667a\u80fd\u7684\u56db\u5c42\u878d\u5408\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u8111\u542f\u53d1PNT\u53d1\u5c55\u7684\u524d\u77bb\u6027\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4f20\u7edfPNT\u3001\u751f\u7269\u5927\u8111PNT\u4e0e\u8111\u542f\u53d1PNT\u76f8\u7ed3\u5408\u7684\u521b\u65b0\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u8111\u542f\u53d1PNT\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN", "AI": {"tldr": "ISGFAN\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u5206\u79bb\u548c\u5bf9\u6297\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u566a\u58f0\u548c\u9886\u57df\u504f\u79fb\u4e0b\u7684\u6545\u969c\u8bca\u65ad\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u8fc1\u79fb\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6570\u636e\u5e72\u51c0\u6216\u9886\u57df\u76f8\u4f3c\uff0c\u4f46\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u566a\u58f0\u5e72\u6270\u548c\u9886\u57df\u504f\u79fb\u5171\u5b58\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "ISGFAN\u6846\u67b6\u57fa\u4e8e\u4fe1\u606f\u5206\u79bb\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6297\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6b63\u4ea4\u635f\u5931\uff0c\u89e3\u8026\u9886\u57df\u4e0d\u53d8\u7684\u6545\u969c\u8868\u793a\uff0c\u540c\u65f6\u91c7\u7528\u5168\u5c40-\u5c40\u90e8\u5bf9\u6297\u65b9\u6848\u7ea6\u675f\u6a21\u578b\u7684\u6761\u4ef6\u548c\u8fb9\u7f18\u5206\u5e03\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cISGFAN\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ISGFAN\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u5206\u79bb\u548c\u5168\u5c40-\u5c40\u90e8\u5bf9\u6297\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u548c\u9886\u57df\u504f\u79fb\u5171\u5b58\u4e0b\u7684\u8de8\u9886\u57df\u6545\u969c\u8bca\u65ad\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.17184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17184", "abs": "https://arxiv.org/abs/2510.17184", "authors": ["Nicolas Robert", "Fabien Gandon", "Maxime Lefran\u00e7ois"], "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development", "comment": null, "summary": "Agile and collaborative approaches to ontologies design are crucial because\nthey contribute to making them userdriven, up-to-date, and able to evolve\nalongside the systems they support, hence proper continuous validation tooling\nis required to ensure ontologies match developers' requirements all along their\ndevelopment. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV\nWorkflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C\nStandards to assist the development of modular ontologies through GitHub\nComposite Actions, pre-commit hooks, or a command line interface. OLIVAW was\ntested on several ontology projects to ensure its usefulness, genericity and\nreusability. A template repository is available for a quick start. OLIVAW is", "AI": {"tldr": "OLIVAW\u662f\u4e00\u4e2a\u57fa\u4e8eGitHub\u7684\u5de5\u5177\uff0c\u652f\u6301ACIMOV\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7W3C\u6807\u51c6\u548c\u591a\u79cd\u63a5\u53e3\u8f85\u52a9\u6a21\u5757\u5316\u672c\u4f53\u7684\u5f00\u53d1\u548c\u6301\u7eed\u9a8c\u8bc1\u3002", "motivation": "\u654f\u6377\u548c\u534f\u4f5c\u7684\u672c\u4f53\u8bbe\u8ba1\u65b9\u6cd5\u5bf9\u4e8e\u786e\u4fdd\u672c\u4f53\u7528\u6237\u9a71\u52a8\u3001\u53ca\u65f6\u66f4\u65b0\u5e76\u4e0e\u652f\u6301\u7cfb\u7edf\u540c\u6b65\u6f14\u5316\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u6301\u7eed\u9a8c\u8bc1\u5de5\u5177\u6765\u5339\u914d\u5f00\u53d1\u8005\u7684\u9700\u6c42\u3002", "method": "OLIVAW\u5229\u7528W3C\u6807\u51c6\u548cGitHub Composite Actions\u3001\u9884\u63d0\u4ea4\u94a9\u5b50\u6216\u547d\u4ee4\u884c\u754c\u9762\u6765\u8f85\u52a9\u6a21\u5757\u5316\u672c\u4f53\u7684\u5f00\u53d1\u3002", "result": "OLIVAW\u5728\u591a\u4e2a\u672c\u4f53\u9879\u76ee\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\u3001\u901a\u7528\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u677f\u4ed3\u5e93\u4ee5\u4fbf\u5feb\u901f\u542f\u52a8\u3002", "conclusion": "OLIVAW\u901a\u8fc7\u652f\u6301ACIMOV\u65b9\u6cd5\u8bba\u5728GitHub\u4e0a\u7684\u5b9e\u73b0\uff0c\u4e3a\u6a21\u5757\u5316\u672c\u4f53\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u786e\u4fdd\u4e86\u672c\u4f53\u7684\u6301\u7eed\u9a8c\u8bc1\u548c\u66f4\u65b0\u3002"}}
{"id": "2510.16235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16235", "abs": "https://arxiv.org/abs/2510.16235", "authors": ["Vishal Manikanden", "Aniketh Bandlamudi", "Daniel Haehn"], "title": "Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection", "comment": null, "summary": "Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head\nand neck cancer. Due to the subtle nature of its early stages, deep and hidden\nareas of development, and slow growth, OCSCC often goes undetected, leading to\npreventable deaths. However, properly trained Convolutional Neural Networks\n(CNNs), with their precise image segmentation techniques and ability to apply\nkernel matrices to modify the RGB values of images for accurate image pattern\nrecognition, would be an effective means for early detection of OCSCC. Pairing\nthis neural network with image capturing and processing hardware would allow\nincreased efficacy in OCSCC detection. The aim of our project is to develop a\nConvolutional Neural Network trained to recognize OCSCC, as well as to design a\nphysical hardware system to capture and process detailed images, in order to\ndetermine the image quality required for accurate predictions. A CNN was\ntrained on 4293 training images consisting of benign and malignant tumors, as\nwell as negative samples, and was evaluated for its precision, recall, and Mean\nAverage Precision (mAP) in its predictions of OCSCC. A testing dataset of\nrandomly assorted images of cancerous, non-cancerous, and negative images was\nchosen, and each image was altered to represent 5 common resolutions. This test\ndata set was thoroughly analyzed by the CNN and predictions were scored on the\nbasis of accuracy. The designed enhancement hardware was used to capture\ndetailed images, and its impact was scored. An application was developed to\nfacilitate the testing process and bring open access to the CNN. Images of\nincreasing resolution resulted in higher-accuracy predictions on a logarithmic\nscale, demonstrating the diminishing returns of higher pixel counts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7CNN\u548c\u56fe\u50cf\u786c\u4ef6\u7cfb\u7edf\u63d0\u5347\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u65e9\u671f\u68c0\u6d4b\u6548\u7387\uff0c\u53d1\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\u4f46\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002", "motivation": "\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\uff08OCSCC\uff09\u662f\u5934\u9888\u90e8\u6700\u5e38\u89c1\u7684\u764c\u75c7\uff0c\u65e9\u671f\u75c7\u72b6\u4e0d\u660e\u663e\u4e14\u751f\u957f\u7f13\u6162\uff0c\u5e38\u88ab\u5ffd\u89c6\u800c\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u6b7b\u4ea1\u3002\u901a\u8fc7\u8bad\u7ec3CNN\u7ed3\u5408\u56fe\u50cf\u91c7\u96c6\u548c\u5904\u7406\u786c\u4ef6\uff0c\u53ef\u4ee5\u63d0\u9ad8OCSCC\u7684\u65e9\u671f\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u7814\u7a76\u56e2\u961f\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u4f7f\u75284293\u5f20\u5305\u542b\u826f\u6027\u548c\u6076\u6027\u80bf\u7624\u4ee5\u53ca\u9634\u6027\u6837\u672c\u7684\u8bad\u7ec3\u56fe\u50cf\u3002\u6d4b\u8bd5\u6570\u636e\u96c6\u5305\u62ec\u968f\u673a\u9009\u62e9\u7684\u764c\u75c7\u3001\u975e\u764c\u75c7\u548c\u9634\u6027\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u88ab\u8c03\u6574\u4e3a5\u79cd\u5e38\u89c1\u5206\u8fa8\u7387\u3002CNN\u5bf9\u8fd9\u4e9b\u56fe\u50cf\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5e76\u6839\u636e\u51c6\u786e\u6027\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u56fe\u50cf\u589e\u5f3a\u786c\u4ef6\u4ee5\u6355\u83b7\u8be6\u7ec6\u56fe\u50cf\uff0c\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "CNN\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u589e\u52a0\u4ee5\u5bf9\u6570\u6bd4\u4f8b\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u9ad8\u50cf\u7d20\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002\u5f00\u53d1\u7684\u786c\u4ef6\u7cfb\u7edf\u548c\u5e94\u7528\u7a0b\u5e8f\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u63d0\u9ad8\u53ef\u4ee5\u663e\u8457\u63d0\u5347CNN\u5bf9\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\uff08OCSCC\uff09\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u9ad8\u50cf\u7d20\u5e26\u6765\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002\u5f00\u53d1\u7684\u786c\u4ef6\u7cfb\u7edf\u548c\u5e94\u7528\u7a0b\u5e8f\u4e3aOCSCC\u7684\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f00\u653e\u4e14\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2510.16905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16905", "abs": "https://arxiv.org/abs/2510.16905", "authors": ["Yukang Cao", "Rahul Moorthy", "O. Goktug Poyrazoglu", "Volkan Isler"], "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control", "comment": "Submitted to the 2026 IEEE International Conference on Robotics and\n  Automation (ICRA). 8 pages, 4 figures", "summary": "Trajectory sampling is a key component of sampling-based control mechanisms.\nTrajectory samplers rely on control input samplers, which generate control\ninputs u from a distribution p(u | x) where x is the current state. We\nintroduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for\nshort) which has two key features: (i) it generates a control input\ndistribution so as to uniformly sample the free configuration space, and (ii)\nin contrast to previously introduced trajectory sampling mechanisms where the\ndistribution p(u | x) is independent of the environment, C-Free-Uniform is\nexplicitly conditioned on the current local map. Next, we integrate this\nsampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.\nExperiments show that CFU-MPPI outperforms existing methods in terms of success\nrate in challenging navigation tasks in cluttered polygonal environments while\nrequiring a much smaller sampling budget.", "AI": {"tldr": "\u63d0\u51faC-Free-Uniform\u6982\u5ff5\uff0c\u901a\u8fc7\u8003\u8651\u73af\u5883\u4fe1\u606f\u6539\u8fdb\u8f68\u8ff9\u91c7\u6837\uff0c\u96c6\u6210\u5230CFU-MPPI\u63a7\u5236\u5668\u540e\uff0c\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u91c7\u6837\u673a\u5236\u751f\u6210\u7684\u5206\u5e03\u72ec\u7acb\u4e8e\u73af\u5883\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5f53\u524d\u5c40\u90e8\u5730\u56fe\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "method": "\u5f15\u5165\u4e86\u81ea\u7531\u914d\u7f6e\u7a7a\u95f4\u5747\u5300\u6027\uff08C-Free-Uniform\uff09\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u65b0\u7684\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\uff08MPPI\uff09\u63a7\u5236\u5668\u4e2d\u3002", "result": "CFU-MPPI\u5728\u590d\u6742\u591a\u8fb9\u5f62\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u91c7\u6837\u9884\u7b97\u4f4e\u3002", "conclusion": "CFU-MPPI\u63a7\u5236\u5668\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u91c7\u6837\u9884\u7b97\u3002"}}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebfCP\u4f18\u5316\u4e0e\u5728\u7ebfSTNU\u6267\u884c\u7684\u6df7\u5408\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5728\u6700\u574f\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u4fdd\u8bc1\u53ef\u884c\u6027\uff0c\u663e\u8457\u51cf\u5c11\u622a\u6b62\u671f\u9650\u8fdd\u53cd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u9700\u8981\u5728\u5e94\u5bf9\u968f\u673a\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u7684\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u4ea4\u4ed8\u671f\u9650\uff0c\u4f20\u7edf\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u73b0\u5b9e\u504f\u79bb\u540d\u4e49\u8ba1\u5212\u65f6\u5d29\u6e83\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u6700\u540e\u4fee\u590d\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u7075\u6d3b\u4f5c\u4e1a\u8f66\u95f4\u7684CP\u6a21\u578b\uff0c\u63d2\u5165\u6700\u4f18\u7f13\u51b2\u0394*\u4ee5\u83b7\u5f97\u5b8c\u5168\u4e3b\u52a8\u7684\u57fa\u7ebf\uff0c\u7136\u540e\u5c06\u8ba1\u5212\u8f6c\u5316\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7b80\u5355\u65f6\u5e8f\u7f51\u7edc\uff08STNU\uff09\u5e76\u9a8c\u8bc1\u52a8\u6001\u53ef\u63a7\u6027\u3002", "result": "\u5728Kacem 1-4\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0c\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86100%\u7684\u622a\u6b62\u671f\u9650\u8fdd\u53cd\uff0c\u540c\u65f6\u4ec5\u589e\u52a03-5%\u7684\u603b\u5de5\u671f\u5f00\u9500\uff0cCP\u6c42\u89e3\u65f6\u95f4\u548cSTNU\u68c0\u67e5\u5728\u4e2d\u7b49\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u4fdd\u6301\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u7ea6\u675f\u7f16\u7a0b\u4f18\u5316\u4e0e\u5728\u7ebf\u65f6\u5e8f\u7f51\u7edc\u6267\u884c\uff0c\u521b\u5efa\u4e86\u5728\u6700\u574f\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u53ef\u884c\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u4e3a\u771f\u6b63\u6570\u5b57\u5316\u3001\u81ea\u6211\u7ea0\u6b63\u7684\u5de5\u5382\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.", "AI": {"tldr": "AdapTrack\u901a\u8fc7\u56de\u6eaf\u673a\u5236\u907f\u514d\u7ea6\u675f\u89e3\u7801\u626d\u66f2\u6a21\u578b\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7ea6\u675f\u89e3\u7801\u6280\u672f\u867d\u80fd\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u4ee3\u7801\uff0c\u4f46\u53ef\u80fd\u626d\u66f2\u6a21\u578b\u8f93\u51fa\u610f\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u4e0d\u7b26\u5408\u5f00\u53d1\u610f\u56fe\u3002", "method": "AdapTrack\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u56de\u6eaf\u673a\u5236\uff0c\u907f\u514d\u5f3a\u5236\u6ee1\u8db3\u7ea6\u675f\u800c\u626d\u66f2\u6a21\u578b\u8f93\u51fa\u610f\u56fe\u3002", "result": "\u5728\u5408\u6210API\u8865\u5168\u6570\u636e\u96c6\u4e0a\u63d0\u5347360.87%\uff0c\u771f\u5b9e\u4e16\u754cAPI\u8865\u5168\u6570\u636e\u96c6\u63d0\u534738.93%\uff0cHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u5206\u522b\u63d0\u53477.84%\u548c6.42%\u3002", "conclusion": "AdapTrack\u901a\u8fc7\u5f15\u5165\u56de\u6eaf\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ea6\u675f\u89e3\u7801\u6280\u672f\u626d\u66f2\u6a21\u578b\u8f93\u51fa\u610f\u56fe\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16258", "abs": "https://arxiv.org/abs/2510.16258", "authors": ["Claire McLean", "Makenzie Meendering", "Tristan Swartz", "Orri Gabbay", "Alexandra Olsen", "Rachel Jacobs", "Nicholas Rosen", "Philippe de Bree", "Tony Garcia", "Gadsden Merrill", "Jake Sandakly", "Julia Buffalini", "Neham Jain", "Steven Krenn", "Moneish Kumar", "Dejan Markovic", "Evonne Ng", "Fabian Prada", "Andrew Saba", "Siwei Zhang", "Vasu Agrawal", "Tim Godisart", "Alexander Richard", "Michael Zollhoefer"], "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset", "comment": null, "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.", "AI": {"tldr": "Meta\u7684Codec Avatars Lab\u53d1\u5e03Embody 3D\u6570\u636e\u96c6\uff0c\u5305\u542b500\u5c0f\u65f6439\u4eba\u76843D\u8fd0\u52a8\u6570\u636e\uff0c\u6db5\u76d6\u624b\u52bf\u3001\u60c5\u611f\u5bf9\u8bdd\u53ca\u534f\u4f5c\u6d3b\u52a8\u3002", "motivation": "\u4e3a\u7814\u7a763D\u4eba\u4f53\u8fd0\u52a8\u3001\u624b\u52bf\u8bc6\u522b\u3001\u60c5\u611f\u5bf9\u8bdd\u53ca\u591a\u4eba\u534f\u4f5c\u884c\u4e3a\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u591a\u6444\u50cf\u5934\u91c7\u96c6439\u540d\u53c2\u4e0e\u8005\u7684500\u5c0f\u65f63D\u8fd0\u52a8\u6570\u636e\uff0c\u5305\u62ec\u5355\u4eba\u52a8\u4f5c\u3001\u624b\u52bf\u3001\u591a\u4eba\u5bf9\u8bdd\u53ca\u534f\u4f5c\u6d3b\u52a8\u3002", "result": "\u6536\u96c6\u4e86\u8d85\u8fc75400\u4e07\u5e27\u76843D\u8fd0\u52a8\u6570\u636e\uff0c\u5305\u62ec\u624b\u90e8\u8ffd\u8e2a\u3001\u8eab\u4f53\u5f62\u72b6\u3001\u6587\u672c\u6ce8\u91ca\u53ca\u72ec\u7acb\u97f3\u9891\u8f68\u9053\u3002", "conclusion": "Embody 3D\u6570\u636e\u96c6\u4e3a\u7814\u7a763D\u8fd0\u52a8\u3001\u624b\u52bf\u3001\u60c5\u611f\u72b6\u6001\u4e0b\u7684\u5bf9\u8bdd\u53ca\u534f\u4f5c\u6d3b\u52a8\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.16931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16931", "abs": "https://arxiv.org/abs/2510.16931", "authors": ["Zhaoliang Wan", "Zida Zhou", "Zetong Bi", "Zehui Yang", "Hao Ding", "Hui Cheng"], "title": "Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems", "comment": "Accepted by IROS2025", "summary": "This paper addresses the scarcity of affordable, fully-actuated five-fingered\nhands for dexterous teleoperation, which is crucial for collecting large-scale\nreal-robot data within the \"Learning from Demonstrations\" paradigm. We\nintroduce the prototype version of the RAPID Hand, the first low-cost,\n20-degree-of-actuation (DoA) dexterous hand that integrates a novel\nanthropomorphic actuation and transmission scheme with an optimized motor\nlayout and structural design to enhance dexterity. Specifically, the RAPID Hand\nfeatures a universal phalangeal transmission scheme for the non-thumb fingers\nand an omnidirectional thumb actuation mechanism. Prioritizing affordability,\nthe hand employs 3D-printed parts combined with custom gears for easier\nreplacement and repair. We assess the RAPID Hand's performance through\nquantitative metrics and qualitative testing in a dexterous teleoperation\nsystem, which is evaluated on three challenging tasks: multi-finger retrieval,\nladle handling, and human-like piano playing. The results indicate that the\nRAPID Hand's fully actuated 20-DoF design holds significant promise for\ndexterous teleoperation.", "AI": {"tldr": "RAPID Hand\u662f\u9996\u4e2a\u4f4e\u6210\u672c\u300120\u81ea\u7531\u5ea6\u7684\u7075\u5de7\u624b\u539f\u578b\uff0c\u91c7\u7528\u65b0\u578b\u62df\u4eba\u5316\u9a71\u52a8\u65b9\u6848\u548c3D\u6253\u5370\u90e8\u4ef6\uff0c\u6d4b\u8bd5\u663e\u793a\u5176\u5728\u7075\u5de7\u9065\u64cd\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u9065\u64cd\u4f5c\u4e2d\u4f4e\u6210\u672c\u3001\u5168\u9a71\u52a8\u4e94\u6307\u624b\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301'\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60'\u8303\u5f0f\u4e0b\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u3002", "method": "\u91c7\u7528\u65b0\u578b\u62df\u4eba\u5316\u9a71\u52a8\u4e0e\u4f20\u52a8\u65b9\u6848\uff0c\u7ed3\u5408\u4f18\u5316\u7684\u7535\u673a\u5e03\u5c40\u548c\u7ed3\u6784\u8bbe\u8ba1\uff0c\u63d0\u5347\u7075\u5de7\u6027\u3002\u5177\u4f53\u5305\u62ec\u975e\u62c7\u6307\u624b\u6307\u7684\u901a\u7528\u6307\u8282\u4f20\u52a8\u65b9\u6848\u548c\u5168\u5411\u62c7\u6307\u9a71\u52a8\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u6d4b\u8bd5\u8bc4\u4f30RAPID Hand\u5728\u7075\u5de7\u9065\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u591a\u6307\u6293\u53d6\u3001\u52fa\u67c4\u64cd\u4f5c\u548c\u7c7b\u4eba\u94a2\u7434\u6f14\u594f\u7b49\u4efb\u52a1\u3002", "conclusion": "RAPID Hand\u768420\u81ea\u7531\u5ea6\u5168\u9a71\u52a8\u8bbe\u8ba1\u5728\u7075\u5de7\u9065\u64cd\u4f5c\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u9009\u62e9\u6027\u63d0\u793a\u7b56\u7565\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4e34\u5e8aCoTs\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86'\u53cc\u539f\u5219'\u6846\u67b6\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e13\u5bb6\u5728\u4e34\u5e8aAI\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u4e34\u5e8aCoTs\u751f\u6210\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1LLM\u751f\u6210\u7684CoTs\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u76f2\u6cd5\u6bd4\u8f83\u7814\u7a76\uff0c\u8d44\u6df1\u8f85\u52a9\u751f\u6b96\u6280\u672f\uff08ART\uff09\u4e34\u5e8a\u4e13\u5bb6\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e0d\u540c\u7b56\u7565\u751f\u6210\u7684CoTs\uff1a\u96f6\u6837\u672c\u3001\u968f\u673a\u5c11\u6837\u672c\uff08\u4f7f\u7528\u6d45\u5c42\u793a\u4f8b\uff09\u548c\u9009\u62e9\u6027\u5c11\u6837\u672c\uff08\u4f7f\u7528\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u793a\u4f8b\uff09\u3002\u8fd9\u4e9b\u4e13\u5bb6\u8bc4\u5206\u4e0e\u6700\u5148\u8fdb\u7684AI\u6a21\u578b\uff08GPT-4o\uff09\u7684\u8bc4\u4f30\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff08p < .001\uff09\uff0c\u800c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\u4e0e\u96f6\u6837\u672c\u57fa\u7ebf\u76f8\u6bd4\u65e0\u663e\u8457\u6539\u8fdb\u3002AI\u8bc4\u4f30\u5668\u672a\u80fd\u8bc6\u522b\u8fd9\u4e9b\u5173\u952e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e'\u53cc\u539f\u5219'\u6846\u67b6\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u89c4\u6a21\u5316\u751f\u6210\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aCoTs\u6570\u636e\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u4e13\u5bb6\u5728\u9ad8\u98ce\u9669\u4e34\u5e8aAI\u8bc4\u4f30\u4e2d\u7684\u4e0d\u53ef\u66ff\u4ee3\u4f5c\u7528\u3002"}}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65e5\u672c2025\u5e74\u60ac\u5d16\u95ee\u9898\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u6269\u5c55CI/CD\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408GitHub\u3001Jenkins\u3001AWS\u548cDocker\uff0c\u964d\u4f4e\u4e86\u7ef4\u62a4\u6210\u672c\u5e76\u4fc3\u8fdb\u4e86\u6570\u5b57\u5316\u8f6c\u578b\u3002", "motivation": "\u65e5\u672c2025\u5e74\u60ac\u5d16\u95ee\u9898\u5bfc\u81f4\u4f20\u7edf\u6838\u5fc3IT\u7cfb\u7edf\u7684\u7ef4\u62a4\u6210\u672c\u6fc0\u589e\u4e14\u96be\u4ee5\u66f4\u65b0\u6216\u66ff\u6362\uff0c\u963b\u788d\u4e86\u6570\u5b57\u5316\u8f6c\u578b\u3002Asahi\u516c\u53f8\u4e5f\u9762\u4e34\u7c7b\u4f3c\u6311\u6218\uff0c\u624b\u52a8\u7ef4\u62a4\u6d41\u7a0b\u548c\u6709\u9650\u7684QA\u73af\u5883\u4f7f\u5173\u952e\u7cfb\u7edf\u8fc7\u65f6\u4e14\u96be\u4ee5\u66f4\u65b0\u3002", "method": "\u91c7\u7528GitHub\u8fdb\u884c\u6e90\u4ee3\u7801\u63a7\u5236\u548c\u5206\u652f\u7ba1\u7406\uff0cJenkins\u5b9e\u73b0\u6d41\u6c34\u7ebf\u81ea\u52a8\u5316\uff0cAWS\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0cDocker\u5b9e\u73b0\u73af\u5883\u5bb9\u5668\u5316\u3002", "result": "\u901a\u8fc7\u53ef\u6269\u5c55CI/CD\u6d41\u6c34\u7ebf\uff0c\u5f00\u53d1\u8005\u80fd\u81ea\u7531\u5b89\u5168\u5730\u6d4b\u8bd5\u7ef4\u62a4\u7a0b\u5e8f\u5e76\u5c1d\u8bd5\u65b0\u6280\u672f\uff0c\u964d\u4f4e\u4e86\u7ef4\u62a4\u6210\u672c\u5e76\u63a8\u52a8\u4e86\u6570\u5b57\u5316\u8f6c\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684CI/CD\u6d41\u6c34\u7ebf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e5\u672c2025\u5e74\u60ac\u5d16\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u548c\u5220\u9664\u9694\u79bb\u7684\u5f00\u53d1\u73af\u5883\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7ef4\u62a4\u6210\u672c\u5e76\u63a8\u52a8\u4e86\u6570\u5b57\u5316\u8f6c\u578b\u3002"}}
{"id": "2510.16272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16272", "abs": "https://arxiv.org/abs/2510.16272", "authors": ["Baicheng Li", "Zike Yan", "Dong Wu", "Hongbin Zha"], "title": "Proactive Scene Decomposition and Reconstruction", "comment": null, "summary": "Human behaviors are the major causes of scene dynamics and inherently contain\nrich cues regarding the dynamics. This paper formalizes a new task of proactive\nscene decomposition and reconstruction, an online approach that leverages\nhuman-object interactions to iteratively disassemble and reconstruct the\nenvironment. By observing these intentional interactions, we can dynamically\nrefine the decomposition and reconstruction process, addressing inherent\nambiguities in static object-level reconstruction. The proposed system\neffectively integrates multiple tasks in dynamic environments such as accurate\ncamera and object pose estimation, instance decomposition, and online map\nupdating, capitalizing on cues from human-object interactions in egocentric\nlive streams for a flexible, progressive alternative to conventional\nobject-level reconstruction methods. Aided by the Gaussian splatting technique,\naccurate and consistent dynamic scene modeling is achieved with photorealistic\nand efficient rendering. The efficacy is validated in multiple real-world\nscenarios with promising advantages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\u7684\u52a8\u6001\u573a\u666f\u5206\u89e3\u4e0e\u91cd\u5efa\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u7269\u4f53\u7ea7\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u800c\u4eba\u7c7b\u884c\u4e3a\u662f\u573a\u666f\u52a8\u6001\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u8574\u542b\u4e30\u5bcc\u52a8\u6001\u7ebf\u7d22\u3002", "method": "\u5229\u7528\u4eba\u673a\u4ea4\u4e92\u7ebf\u7d22\uff0c\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u5728\u7ebf\u5206\u89e3\u4e0e\u91cd\u5efa\uff0c\u540c\u65f6\u6574\u5408\u76f8\u673a\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3001\u5b9e\u4f8b\u5206\u89e3\u53ca\u5728\u7ebf\u5730\u56fe\u66f4\u65b0\u7b49\u591a\u4efb\u52a1\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6e10\u8fdb\u5f0f\u4f18\u52bf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\u7684\u52a8\u6001\u573a\u666f\u5206\u89e3\u4e0e\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u903c\u771f\u7684\u6e32\u67d3\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17038", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17038", "abs": "https://arxiv.org/abs/2510.17038", "authors": ["Pedram Fekri", "Majid Roshanfar", "Samuel Barbeau", "Seyedfarzad Famouri", "Thomas Looi", "Dale Podolsky", "Mehrdad Zadeh", "Javad Dargahi"], "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation", "comment": null, "summary": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies.", "AI": {"tldr": "DINO-CVA\u662f\u4e00\u79cd\u591a\u6a21\u6001\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u514b\u9686\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u8fd0\u52a8\u5b66\u6570\u636e\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u7ba1\u5bfc\u822a\uff0c\u51cf\u5c11\u64cd\u4f5c\u4f9d\u8d56\u5e76\u63d0\u9ad8\u6cbb\u7597\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5bfc\u7ba1\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u5bfc\u81f4\u64cd\u4f5c\u75b2\u52b3\u3001\u8f90\u5c04\u66b4\u9732\u589e\u52a0\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u667a\u80fd\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86DINO-CVA\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u514b\u9686\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u89c2\u5bdf\u548c\u64cd\u7eb5\u6746\u8fd0\u52a8\u5b66\u5230\u4e00\u4e2a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u81ea\u56de\u5f52\u9884\u6d4b\u52a8\u4f5c\u3002", "result": "DINO-CVA\u5728\u52a8\u4f5c\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u4e0e\u4ec5\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u5c06\u9884\u6d4b\u57fa\u4e8e\u89e3\u5256\u73af\u5883\u3002", "conclusion": "DINO-CVA\u6846\u67b6\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u3001\u76ee\u6807\u5bfc\u5411\u67b6\u6784\u5728\u5bfc\u7ba1\u5bfc\u822a\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u51cf\u5c11\u64cd\u4f5c\u4f9d\u8d56\u6027\u548c\u63d0\u9ad8\u5bfc\u7ba1\u6cbb\u7597\u7684\u53ef\u9760\u6027\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u7b97\u6cd5\u65f6\u4ee3\u5982\u4f55\u901a\u8fc7\u52a8\u6001\u80fd\u529b\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u6a21\u578b\u6765\u8861\u91cf\u4f01\u4e1a\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u6cd5\u5f8b\u6807\u51c6\u4e0a\uff0c\u4ee5\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u6027\u548c\u95ee\u8d23\u5236\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u65e5\u76ca\u4ecb\u5165\uff0c\u4f20\u7edf\u57fa\u4e8e\u4eba\u7c7b\u4ee3\u7406\u7684\u4f01\u4e1a\u8d23\u4efb\u5047\u8bbe\u53d7\u5230\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\u4ee5\u9002\u5e94\u7b97\u6cd5\u65f6\u4ee3\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u8ba4\u77e5\u7406\u8bba\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u6a21\u578b\uff0c\u6355\u6349\u90e8\u7f72\u590d\u6742AI\u6216\u4fe1\u606f\u7cfb\u7edf\u7684\u4f01\u4e1a\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5f15\u5165\u4e86\u8fde\u7eed\u7684\u7ec4\u7ec7\u77e5\u8bc6\u5ea6\u91cf$S_S(\\varphi)$\uff0c\u6574\u5408\u4e86\u7ba1\u9053\u7684\u8ba1\u7b97\u6210\u672c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u7684\u9519\u8bef\u7387\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u6a21\u578b\uff0c\u5305\u62ec\u77e5\u8bc6\u5ea6\u91cf$S_S(\\varphi)$\u3001\u77e5\u8bc6\u8c13\u8bcd$\\mathsf{K}_S$\u548c\u4f01\u4e1a\u8303\u56f4\u7684\u8ba4\u77e5\u80fd\u529b\u6307\u6570$\\mathcal{K}_{S,t}$\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5b9a\u91cf\u6307\u6807\u6620\u5c04\u5230\u6cd5\u5f8b\u6807\u51c6\u4e0a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u80fd\u529b\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u8fde\u7eed\u7684\u7ec4\u7ec7\u77e5\u8bc6\u5ea6\u91cf$S_S(\\varphi)$\u548c\u9608\u503c\u77e5\u8bc6\u8c13\u8bcd$\\mathsf{K}_S$\uff0c\u4e3a\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684AI\u548c\u4fe1\u606f\u7cfb\u7edf\u7684\u53ef\u5ba1\u8ba1\u6027\u548c\u95ee\u8d23\u5236\u63d0\u4f9b\u4e86\u53ef\u8861\u91cf\u7684\u6cd5\u5f8b\u6807\u51c6\u3002"}}
{"id": "2510.16290", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16290", "abs": "https://arxiv.org/abs/2510.16290", "authors": ["Yue Zheng", "Xiufang Shi", "Jiming Chen", "Yuanchao Shu"], "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models", "comment": null, "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.", "AI": {"tldr": "Cerberus\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u8fd0\u52a8\u63a9\u7801\u63d0\u793a\u3001\u89c4\u5219\u504f\u5dee\u68c0\u6d4b\u7b49\u521b\u65b0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u901f\u5ea6\u63d0\u5347151.79\u500d\uff0c\u7cbe\u5ea6\u8fbe97.2%\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u7a33\u5b9a\u7684\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u9650\u5236\u4e86\u5b9e\u65f6\u90e8\u7f72\u3002Cerberus\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "Cerberus\u91c7\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u7cfb\u7edf\uff0c\u79bb\u7ebf\u5b66\u4e60\u6b63\u5e38\u884c\u4e3a\u89c4\u5219\uff0c\u5728\u7ebf\u63a8\u7406\u65f6\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u548c\u7ec6\u7c92\u5ea6VLM\u63a8\u7406\u3002\u5173\u952e\u521b\u65b0\u5305\u62ec\u8fd0\u52a8\u63a9\u7801\u63d0\u793a\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u504f\u5dee\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCerberus\u5e73\u5747\u8fbe\u523057.68 fps\uff08151.79\u500d\u52a0\u901f\uff09\uff0c97.2%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u6700\u5148\u8fdb\u7684VLM-based VAD\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Cerberus\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7ea7\u8054\u8bbe\u8ba1\u548c\u5173\u952e\u521b\u65b0\uff08\u8fd0\u52a8\u63a9\u7801\u63d0\u793a\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u504f\u5dee\u68c0\u6d4b\uff09\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.17086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17086", "abs": "https://arxiv.org/abs/2510.17086", "authors": ["Xueqian Bai", "Nicklas Hansen", "Adabhav Singh", "Michael T. Tolley", "Yan Duan", "Pieter Abbeel", "Xiaolong Wang", "Sha Yi"], "title": "Learning to Design Soft Hands using Reward Models", "comment": null, "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects.", "AI": {"tldr": "\u63d0\u51faCEM-RM\u6846\u67b6\uff0c\u901a\u8fc7\u9065\u64cd\u4f5c\u6570\u636e\u4f18\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u6293\u53d6\u6210\u529f\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u8bbe\u8ba1\u65e2\u5177\u6709\u9ad8\u987a\u5e94\u6027\u53c8\u80fd\u5728\u591a\u6837\u5316\u4f7f\u7528\u573a\u666f\u4e2d\u4fdd\u6301\u529f\u80fd\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u4ecd\u5177\u6311\u6218\u6027\u3002\u786c\u4ef6\u4e0e\u63a7\u5236\u7684\u534f\u540c\u8bbe\u8ba1\u867d\u80fd\u66f4\u597d\u8026\u5408\u5f62\u6001\u4e0e\u884c\u4e3a\uff0c\u4f46\u5176\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u548c\u4eff\u771f\u8bc4\u4f30\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u71b5\u65b9\u6cd5\u548c\u5956\u52b1\u6a21\u578b\uff08CEM-RM\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6536\u96c6\u7684\u9065\u64cd\u4f5c\u6570\u636e\u4f18\u5316\u808c\u8171\u9a71\u52a8\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\uff0c\u51cf\u5c11\u4e86\u8bbe\u8ba1\u8bc4\u4f30\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u4f18\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u591a\u6837\u5316\u6311\u6218\u6027\u7269\u4f53\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u8bbe\u8ba1\u8bc4\u4f30\u6b21\u6570\u51cf\u5c11\u4e86\u4e00\u534a\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7CEM-RM\u6846\u67b6\u4f18\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u5728\u6293\u53d6\u6210\u529f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u786c\u4ef6\u548c\u63a7\u5236\u534f\u540c\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI \u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528 LLM \u81ea\u52a8\u8bc4\u4f30 PHI \u53bb\u6807\u8bc6\u5316\u8d28\u91cf\u5e76\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "motivation": "PHI \u53bb\u6807\u8bc6\u5316\u5bf9\u4e8e\u4e34\u5e8a\u7b14\u8bb0\u7684\u5b89\u5168\u91cd\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bc4\u4f30\u548c\u6bd4\u8f83\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6ce8\u91ca\u3002", "method": "TEAM-PHI \u90e8\u7f72\u4e86\u591a\u4e2a\u8bc4\u4f30\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4ee3\u7406\u72ec\u7acb\u5224\u65ad PHI \u63d0\u53d6\u7684\u6b63\u786e\u6027\u5e76\u8f93\u51fa\u7ed3\u6784\u5316\u6307\u6807\u3002\u7136\u540e\u901a\u8fc7\u57fa\u4e8e LLM \u7684\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI \u751f\u6210\u4e86\u4e00\u81f4\u4e14\u51c6\u786e\u7684\u6392\u540d\uff0c\u4e0e\u771f\u5b9e\u6ce8\u91ca\u548c\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "TEAM-PHI \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728 PHI \u53bb\u6807\u8bc6\u5316\u4e2d\u81ea\u52a8\u8bc4\u4f30\u548c\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5373\u4f7f\u771f\u5b9e\u6807\u7b7e\u6709\u9650\u3002"}}
{"id": "2510.16295", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16295", "abs": "https://arxiv.org/abs/2510.16295", "authors": ["Ryoto Miyamoto", "Xin Fan", "Fuyuko Kido", "Tsuneo Matsumoto", "Hayato Yamana"], "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "comment": null, "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.", "AI": {"tldr": "OpenLVLM-MIA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5e73\u8861\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u62a5\u544a\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u5f80\u5f80\u6e90\u4e8e\u68c0\u6d4b\u6570\u636e\u96c6\u6784\u5efa\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u5206\u5e03\u504f\u5dee\uff0c\u800c\u975e\u771f\u5b9e\u7684\u6210\u5458\u72b6\u6001\u8bc6\u522b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b6,000\u5f20\u56fe\u50cf\u7684\u53d7\u63a7\u57fa\u51c6\uff0c\u5176\u4e2d\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u7684\u5206\u5e03\u7ecf\u8fc7\u7cbe\u5fc3\u5e73\u8861\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u63d0\u4f9b\u4e86\u771f\u5b9e\u6210\u5458\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6700\u5148\u8fdb\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\uff0c\u5176\u6027\u80fd\u6536\u655b\u4e8e\u968f\u673a\u731c\u6d4b\u6c34\u5e73\u3002", "conclusion": "OpenLVLM-MIA\u4f5c\u4e3a\u4e00\u4e2a\u900f\u660e\u4e14\u65e0\u504f\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17111", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17111", "abs": "https://arxiv.org/abs/2510.17111", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "comment": null, "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u5347VLA\u6a21\u578b\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5206\u7c7b\u4e3a\u6a21\u578b\u67b6\u6784\u3001\u611f\u77e5\u7279\u5f81\u3001\u52a8\u4f5c\u751f\u6210\u548c\u8bad\u7ec3/\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u6311\u6218\u3002", "motivation": "VLA\u6a21\u578b\u5728\u5177\u8eab\u63a7\u5236\u4e2d\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4e0e\u8fb9\u7f18\u5e73\u53f0\u5b9e\u65f6\u6027\u8981\u6c42\u7684\u51b2\u7a81\uff0c\u4e9f\u9700\u63d0\u5347\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5bf9\u73b0\u6709\u7814\u7a76\u7684\u7cfb\u7edf\u56de\u987e\uff0c\u5c06\u89e3\u51b3\u65b9\u6848\u5206\u4e3a\u56db\u4e2a\u7ef4\u5ea6\uff1a\u6a21\u578b\u67b6\u6784\u3001\u611f\u77e5\u7279\u5f81\u3001\u52a8\u4f5c\u751f\u6210\u548c\u8bad\u7ec3/\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u603b\u7ed3\u4e86\u6bcf\u7c7b\u4e2d\u7684\u4ee3\u8868\u6027\u6280\u672f\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u5206\u7c7b\u5e76\u603b\u7ed3\u4e86\u63d0\u5347VLA\u6548\u7387\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u6548\u7387\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u9ad8\u6548\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8LLMs\u53ef\u80fd\u5bfc\u81f4\u7684\u4fe1\u606f\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51fa\u2018\u88ab\u8bb0\u4f4f\u7684\u6743\u5229\u2019\uff08RTBR\uff09\u4ee5\u786e\u4fddAI\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5bf9\u4fe1\u606f\u68c0\u7d22\u7684\u5f71\u54cd\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u89c2\u70b9\u88ab\u8fc7\u5ea6\u653e\u5927\u6216\u5ffd\u7565\uff0c\u4ece\u800c\u5a01\u80c1\u96c6\u4f53\u8bb0\u5fc6\u7684\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4fe1\u606f\u68c0\u7d22\u7684\u5f71\u54cd\uff0c\u63d0\u51faRTBR\u6982\u5ff5\u4ee5\u5e94\u5bf9\u6f5c\u5728\u7684\u4fe1\u606f\u504f\u89c1\u548c\u9057\u6f0f\u95ee\u9898\u3002", "result": "\u63d0\u51faRTBR\u6982\u5ff5\uff0c\u5f3a\u8c03\u5728AI\u751f\u6210\u5185\u5bb9\u4e2d\u9700\u5e73\u8861\u4fe1\u606f\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u548c\u771f\u5b9e\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u2018\u88ab\u8bb0\u4f4f\u7684\u6743\u5229\u2019\uff08RTBR\uff09\u6982\u5ff5\uff0c\u65e8\u5728\u51cf\u5c11AI\u9a71\u52a8\u7684\u4fe1\u606f\u9057\u6f0f\u98ce\u9669\uff0c\u786e\u4fdd\u516c\u5e73\u5bf9\u5f85\uff0c\u5e76\u6700\u5927\u5316\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002"}}
{"id": "2510.16319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16319", "abs": "https://arxiv.org/abs/2510.16319", "authors": ["Rui Yang", "Huining Li", "Yiyi Long", "Xiaojun Wu", "Shengfeng He"], "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation", "comment": "ICCV 2025", "summary": "Generating sketches guided by reference styles requires precise transfer of\nstroke attributes, such as line thickness, deformation, and texture sparsity,\nwhile preserving semantic structure and content fidelity. To this end, we\npropose Stroke2Sketch, a novel training-free framework that introduces\ncross-image stroke attention, a mechanism embedded within self-attention layers\nto establish fine-grained semantic correspondences and enable accurate stroke\nattribute transfer. This allows our method to adaptively integrate reference\nstroke characteristics into content images while maintaining structural\nintegrity. Additionally, we develop adaptive contrast enhancement and\nsemantic-focused attention to reinforce content preservation and foreground\nemphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches\nthat closely resemble handcrafted results, outperforming existing methods in\nexpressive stroke control and semantic coherence. Codes are available at\nhttps://github.com/rane7/Stroke2Sketch.", "AI": {"tldr": "Stroke2Sketch\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u7b14\u753b\u6ce8\u610f\u673a\u5236\u5b9e\u73b0\u98ce\u683c\u5316\u8349\u56fe\u751f\u6210\uff0c\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u53c2\u8003\u98ce\u683c\u5f15\u5bfc\u7684\u8349\u56fe\u9700\u8981\u7cbe\u786e\u4f20\u9012\u7b14\u753b\u5c5e\u6027\uff08\u5982\u7ebf\u6761\u7c97\u7ec6\u3001\u53d8\u5f62\u548c\u7eb9\u7406\u7a00\u758f\u6027\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u548c\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faStroke2Sketch\u6846\u67b6\uff0c\u5f15\u5165\u8de8\u56fe\u50cf\u7b14\u753b\u6ce8\u610f\u673a\u5236\uff0c\u5d4c\u5165\u81ea\u6ce8\u610f\u529b\u5c42\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u5e94\u548c\u7b14\u753b\u5c5e\u6027\u4f20\u9012\uff1b\u5f00\u53d1\u81ea\u9002\u5e94\u5bf9\u6bd4\u589e\u5f3a\u548c\u8bed\u4e49\u805a\u7126\u6ce8\u610f\u4ee5\u589e\u5f3a\u5185\u5bb9\u4fdd\u7559\u548c\u524d\u666f\u5f3a\u8c03\u3002", "result": "Stroke2Sketch\u80fd\u591f\u5408\u6210\u98ce\u683c\u5fe0\u5b9e\u4e14\u63a5\u8fd1\u624b\u5de5\u7ed8\u5236\u7ed3\u679c\u7684\u8349\u56fe\uff0c\u5728\u8868\u8fbe\u6027\u7b14\u753b\u63a7\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Stroke2Sketch\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u7b14\u753b\u6ce8\u610f\u673a\u5236\uff0c\u80fd\u591f\u51c6\u786e\u4f20\u9012\u7b14\u753b\u5c5e\u6027\u5e76\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5728\u8868\u8fbe\u6027\u7b14\u753b\u63a7\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.17143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17143", "abs": "https://arxiv.org/abs/2510.17143", "authors": ["Shantnav Agarwal", "Javier Alonso-Mora", "Sihao Sun"], "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning", "comment": "Accepted by IEEE MRS 2025", "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u52a8\u529b\u5b66\u89c4\u5212\u65b9\u6cd5\uff0c\u65e0\u9700\u4ee3\u7406\u95f4\u901a\u4fe1\uff0c\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u63a7\u5236\u67b6\u6784\u6216\u53ef\u9760\u7684\u4ee3\u7406\u95f4\u901a\u4fe1\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u65e0\u901a\u4fe1\u60c5\u51b5\u4e0b\u7684\u8d1f\u8f7d\u8fd0\u8f93\u95ee\u9898\u3002", "method": "\u5229\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6bcf\u4e2a\u65e0\u4eba\u673a\u7684\u5206\u6563\u5b66\u751f\u7b56\u7565\uff0c\u6a21\u4eff\u5177\u6709\u5168\u5c40\u89c2\u5bdf\u6743\u9650\u7684\u96c6\u4e2d\u5f0f\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u5668\u3002\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u5e73\u6ed1\u8f68\u8ff9\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8ddf\u968f\u654f\u6377\u53c2\u8003\u8f68\u8ff9\uff0c\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u52a8\u529b\u5b66\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4ee3\u7406\u95f4\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u64cd\u4f5c\uff0c\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "ScholarEval\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\u7684\u5065\u5168\u6027\u548c\u8d21\u732e\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u5de5\u5177\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u7814\u7a76\u6784\u601d\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u751f\u6210\u60f3\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165\u4e86ScholarEval\uff0c\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86ScholarIdeas\u6570\u636e\u96c6\uff08117\u4e2a\u8de8\u5b66\u79d1\u7814\u7a76\u60f3\u6cd5\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "ScholarEval\u5728\u8986\u76d6\u4e13\u5bb6\u6807\u6ce8\u7684\u8bc4\u5206\u70b9\u3001\u8bc4\u4f30\u53ef\u64cd\u4f5c\u6027\u3001\u6df1\u5ea6\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7528\u6237\u7814\u7a76\u4e5f\u8bc1\u5b9e\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "ScholarEval\u6846\u67b6\u5728\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\u7684\u5065\u5168\u6027\u548c\u8d21\u732e\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u6587\u732e\u53c2\u4e0e\u3001\u60f3\u6cd5\u7cbe\u70bc\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.16320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16320", "abs": "https://arxiv.org/abs/2510.16320", "authors": ["Wenhao Wang", "Longqi Cai", "Taihong Xiao", "Yuxiao Wang", "Ming-Hsuan Yang"], "title": "Scaling Laws for Deepfake Detection", "comment": null, "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u6700\u5927\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6570\u636e\u96c6ScaleDF\uff0c\u53d1\u73b0\u68c0\u6d4b\u8bef\u5dee\u968f\u6570\u636e\u89c4\u6a21\u589e\u52a0\u5448\u5e42\u5f8b\u8870\u51cf\uff0c\u4e3a\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u548c\u6570\u636e\u5bf9\u7b56\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\u7684\u7814\u7a76\u9700\u6c42\uff0c\u56e0\u6b64\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6ScaleDF\uff0c\u5e76\u7814\u7a76\u5176\u7f29\u653e\u89c4\u5f8b\u3002", "method": "\u901a\u8fc7\u6784\u5efaScaleDF\u6570\u636e\u96c6\uff08\u5305\u542b580\u4e07\u771f\u5b9e\u56fe\u50cf\u548c880\u4e07\u4f2a\u9020\u56fe\u50cf\uff09\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u771f\u5b9e\u56fe\u50cf\u57df\u6570\u91cf\u3001\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u65b9\u6cd5\u53ca\u8bad\u7ec3\u56fe\u50cf\u6570\u91cf\u7684\u5173\u7cfb\u3002", "result": "\u89c2\u5bdf\u5230\u7c7b\u4f3c\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e42\u5f8b\u7f29\u653e\u89c4\u5f8b\uff0c\u5373\u5e73\u5747\u68c0\u6d4b\u8bef\u5dee\u968f\u771f\u5b9e\u57df\u6216\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u6570\u91cf\u7684\u589e\u52a0\u800c\u5448\u5e42\u5f8b\u8870\u51cf\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u7740\u771f\u5b9e\u56fe\u50cf\u57df\u6216\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5e73\u5747\u68c0\u6d4b\u8bef\u5dee\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\u8870\u51cf\u3002\u8fd9\u4e00\u5173\u952e\u89c2\u5bdf\u4e0d\u4ec5\u6709\u52a9\u4e8e\u9884\u6d4b\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u6240\u9700\u7684\u989d\u5916\u8d44\u6e90\uff0c\u8fd8\u542f\u53d1\u4e86\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5bf9\u7b56\u6765\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u3002"}}
{"id": "2510.17148", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17148", "abs": "https://arxiv.org/abs/2510.17148", "authors": ["Yu Gao", "Yiru Wang", "Anqing Jiang", "Heng Yuwen", "Wang Shuo", "Sun Hao", "Wang Jijun"], "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "comment": null, "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.", "AI": {"tldr": "DiffVLA++\u662f\u4e00\u4e2a\u7ed3\u5408\u8ba4\u77e5\u63a8\u7406\u548c\u7aef\u5230\u7aef\u89c4\u5212\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5ea6\u91cf\u5f15\u5bfc\u7684\u5bf9\u9f50\u63d0\u5347\u4e86\u957f\u5c3e\u573a\u666f\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u9a7e\u9a76\u6a21\u578b\u7f3a\u4e4f\u5fc5\u8981\u7684\u4e16\u754c\u77e5\u8bc6\u6765\u7406\u89e3\u548c\u63a8\u7406\u5468\u56f4\u73af\u5883\uff0c\u800cVLA\u6a21\u578b\u867d\u7136\u80fd\u5904\u7406\u6311\u6218\u6027\u6848\u4f8b\uff0c\u4f46\u5176\u6709\u9650\u76843D\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u5bfc\u81f4\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u76f4\u63a5\u751f\u6210\u8bed\u4e49\u57fa\u7840\u9a7e\u9a76\u8f68\u8ff9\u7684VLA\u6a21\u5757\uff0c\u5176\u6b21\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u7684\u7aef\u5230\u7aef\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u4e86\u5ea6\u91cf\u5f15\u5bfc\u7684\u8f68\u8ff9\u8bc4\u5206\u5668\u6765\u5bf9\u9f50\u4e24\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u3002", "result": "\u5728ICCV 2025 Autonomous Grand Challenge\u6392\u884c\u699c\u4e0a\uff0cDiffVLA++\u5b9e\u73b0\u4e8649.12\u7684EPDMS\u3002", "conclusion": "DiffVLA++\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u63a8\u7406\u548c\u7aef\u5230\u7aef\u89c4\u5212\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5bb9\u6613\u88ab\u6076\u610f\u5206\u5fc3\u4efb\u52a1\u5e72\u6270\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408SFT\u548cRL\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u88ab\u6076\u610f\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\u7684\u6f0f\u6d1e\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u7814\u7a76\u4e0d\u540c\u6a21\u578b\u548c\u57fa\u51c6\uff0c\u8bc6\u522b\u63a8\u7406\u5206\u5fc3\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LRMs\u5bf9\u63a8\u7406\u5206\u5fc3\u9ad8\u5ea6\u654f\u611f\uff0c\u5206\u5fc3\u6ce8\u5165\u53ef\u964d\u4f4e\u4efb\u52a1\u51c6\u786e\u7387\u9ad8\u8fbe60%\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u6311\u6218\u6027\u5206\u5fc3\u653b\u51fb\u4e0a\u63d0\u9ad8\u4e8650%\u4ee5\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u5206\u5fc3\u662f\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u53ef\u9760\u6027\u7684\u4e00\u4e2a\u72ec\u7279\u4e14\u7d27\u8feb\u7684\u5a01\u80c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u5206\u5fc3\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16325", "abs": "https://arxiv.org/abs/2510.16325", "authors": ["Yuyao Zhang", "Yu-Wing Tai"], "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention", "comment": "22 pages", "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.", "AI": {"tldr": "Scale-DiT\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u5168\u5c40\u5f15\u5bfc\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\uff08\u59824K\uff09\u56fe\u50cf\u751f\u6210\u4e2d\u56e0\u6ce8\u610f\u529b\u673a\u5236\u590d\u6742\u5ea6\u9ad8\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faScale-DiT\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f4e\u5206\u8fa8\u7387\u6f5c\u5728\u7a7a\u95f4\u5168\u5c40\u5f15\u5bfc\uff0c\u7ed3\u5408LoRA\u9002\u914d\u548cHilbert\u66f2\u7ebf\u6392\u5e8f\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u3002", "result": "Scale-DiT\u57284K\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Scale-DiT\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u7684\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.17150", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17150", "abs": "https://arxiv.org/abs/2510.17150", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Gokhan Solak", "Arash Ajoudani"], "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation", "comment": "Code, video and RAG dataset are available at\n  \\url{https://sites.google.com/view/omni-vic}", "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC).", "AI": {"tldr": "OmniVIC \u662f\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u6211\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u901a\u7528\u53ef\u53d8\u963b\u6297\u63a7\u5236\u5668\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u53ef\u53d8\u963b\u6297\u63a7\u5236\u5668\uff08VIC\uff09\u5728\u673a\u5668\u4eba\u7269\u7406\u4ea4\u4e92\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u672a\u77e5\u3001\u590d\u6742\u548c\u975e\u7ed3\u6784\u5316\u7684\u4efb\u52a1\u573a\u666f\u4e2d\u7f3a\u4e4f\u666e\u9002\u6027\uff0cOmniVIC\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u548c\u4f4e\u7ea7\u522b\u5408\u89c4\u63a7\u5236\u7684\u7ed3\u5408\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "OmniVIC \u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u81ea\u6211\u6539\u8fdb\u7684RAG\u548cICL\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u7ed3\u6784\u5316\u8bb0\u5fc6\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u5148\u9a8c\u7ecf\u9a8c\uff0c\u5e76\u5229\u7528VLM\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u81ea\u9002\u5e94\u963b\u6297\u53c2\u6570\uff0c\u540c\u65f6\u7ed3\u5408\u5b9e\u65f6\u529b/\u529b\u77e9\u53cd\u9988\u786e\u4fdd\u4ea4\u4e92\u529b\u5728\u5b89\u5168\u9608\u503c\u5185\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cOmniVIC\u5728\u590d\u6742\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u548c\u529b\u8fdd\u89c4\u7387\u5747\u663e\u8457\u6539\u5584\u3002", "conclusion": "OmniVIC \u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u81ea\u6211\u6539\u8fdb\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u53ca\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\uff0c\u6210\u529f\u5c06\u5e73\u5747\u6210\u529f\u7387\u4ece27%\u63d0\u5347\u81f361.4%\u3002"}}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u5ef6\u8fdf\u5360\u4ee3\u7406\u7cfb\u7edf\u603b\u5ef6\u8fdf\u768453.7%\uff0c\u63d0\u51fa\u7684SpecCache\u6846\u67b6\u901a\u8fc7\u7f13\u5b58\u548c\u63a8\u6d4b\u6027\u6267\u884c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7f51\u7edc\u4ea4\u4e92\u5e26\u6765\u7684\u5ef6\u8fdf\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u5206\u89e3\u7aef\u5230\u7aef\u5ef6\u8fdf\u4e3aLLM API\u5ef6\u8fdf\u548c\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u4e24\u90e8\u5206\uff0c\u5e76\u63d0\u51faSpecCache\u6846\u67b6\uff0c\u7ed3\u5408\u63a8\u6d4b\u6027\u6267\u884c\u548c\u7f13\u5b58\u7b56\u7565\u4f18\u5316\u6548\u7387\u3002", "result": "SpecCache\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u5347\u9ad8\u8fbe58\u500d\uff0c\u7f51\u7edc\u73af\u5883\u5f00\u9500\u964d\u4f4e3.2\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u4ee3\u7406\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "SpecCache\u6846\u67b6\u901a\u8fc7\u63a8\u6d4b\u6027\u6267\u884c\u548c\u7f13\u5b58\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u7f51\u7edc\u7684\u4ee3\u7406\u7cfb\u7edf\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.16326", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16326", "abs": "https://arxiv.org/abs/2510.16326", "authors": ["Yi Wei", "Shunpu Tang", "Liang Zhao", "Qiangian Yang"], "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution", "comment": null, "summary": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.", "AI": {"tldr": "DiffusionX\u901a\u8fc7\u4e91\u8fb9\u534f\u540c\u548c\u52a8\u6001\u8d1f\u8f7d\u5e73\u8861\uff0c\u663e\u8457\u964d\u4f4e\u6269\u6563\u6a21\u578b\u751f\u6210\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u8ba1\u7b97\u5bc6\u96c6\u3001\u7528\u6237\u9700\u591a\u6b21\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u5bfc\u81f4\u5ef6\u8fdf\u9ad8\u548c\u4e91\u8d44\u6e90\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DiffusionX\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bbe\u5907\u7aef\u6269\u6563\u6a21\u578b\u548c\u9ad8\u5bb9\u91cf\u4e91\u7aef\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u566a\u58f0\u6c34\u5e73\u9884\u6d4b\u5668\u52a8\u6001\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffusionX\u6bd4Stable Diffusion v1.5\u5e73\u5747\u751f\u6210\u65f6\u95f4\u51cf\u5c1115.8%\uff0c\u56fe\u50cf\u8d28\u91cf\u76f8\u8fd1\uff1b\u6bd4Tiny-SD\u4ec5\u61620.9%\u4f46\u56fe\u50cf\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DiffusionX\u901a\u8fc7\u4e91\u8fb9\u534f\u540c\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u751f\u6210\u5ef6\u8fdf\u548c\u4e91\u8d44\u6e90\u8d1f\u62c5\uff0c\u540c\u65f6\u5728\u4fdd\u8bc1\u56fe\u50cf\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2510.17191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.", "AI": {"tldr": "SimpleVSF\u7ed3\u5408VLM\u548c\u8f68\u8ff9\u878d\u5408\u6280\u672f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u8d28\u91cf\uff0c\u5728ICCV 2025\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u51b3\u7b56\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSimpleVSF\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edf\u8bc4\u5206\u5668\u548cVLM\u589e\u5f3a\u8bc4\u5206\u5668\uff0c\u901a\u8fc7\u5b9a\u91cf\u805a\u5408\u548c\u5b9a\u6027\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u8fdb\u884c\u8f68\u8ff9\u878d\u5408\u3002", "result": "\u4f5c\u4e3aICCV 2025 NAVSIM v2\u6311\u6218\u8d5b\u7684\u9886\u5148\u65b9\u6cd5\uff0cSimpleVSF\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SimpleVSF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5148\u8fdb\u8f68\u8ff9\u878d\u5408\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u8212\u9002\u548c\u6548\u7387\u7684\u4f18\u5f02\u5e73\u8861\u3002"}}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.", "AI": {"tldr": "DTKG\u53cc\u8f68KG\u9a8c\u8bc1\u548c\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u8df3QA\u4efb\u52a1\u4e2d\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8df3\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u53cc\u8f68KG\u9a8c\u8bc1\u548c\u63a8\u7406\u6846\u67b6DTKG\uff0c\u5305\u62ec\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5\u3002", "result": "DTKG\u6846\u67b6\u80fd\u591f\u540c\u65f6\u4f18\u5316\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "DTKG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53cc\u8f68KG\u9a8c\u8bc1\u548c\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u8df3QA\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.16332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16332", "abs": "https://arxiv.org/abs/2510.16332", "authors": ["Haiyue Sun", "Qingdong He", "Jinlong Peng", "Peng Tang", "Jiangning Zhang", "Junwei Zhu", "Xiaobin Hu", "Shuicheng Yan"], "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement", "comment": null, "summary": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR", "AI": {"tldr": "TokenAR\u901a\u8fc7\u4ee4\u724c\u7ea7\u589e\u5f3a\u673a\u5236\u89e3\u51b3\u4e86\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86InstructAR\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5728\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u53c2\u8003\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u673a\u5236\u6765\u89e3\u8026\u4e0d\u540c\u53c2\u8003\u8eab\u4efd\u3002", "method": "TokenAR\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a\u4ee4\u724c\u7d22\u5f15\u5d4c\u5165\uff08Token Index Embedding\uff09\u3001\u6307\u4ee4\u4ee4\u724c\u6ce8\u5165\uff08Instruct Token Injection\uff09\u548c\u8eab\u4efd\u4ee4\u724c\u89e3\u8026\u7b56\u7565\uff08ITD\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86InstructAR\u6570\u636e\u96c6\uff0c\u5305\u542b28K\u8bad\u7ec3\u5bf9\uff0c\u7528\u4e8e\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cTokenAR\u5728\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "TokenAR\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u7684\u4ee4\u724c\u7ea7\u589e\u5f3a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u53c2\u8003\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u5728\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17203", "abs": "https://arxiv.org/abs/2510.17203", "authors": ["Ryota Soga", "Masataka Kobayashi", "Tsukasa Shimizu", "Shintaro Shiba", "Quan Kong", "Shan Lu", "Takaya Yamazato"], "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera", "comment": "7pages, APCC2025", "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684VLC-VLP\u96c6\u6210\u7cfb\u7edf\uff0c\u7528\u4e8eGPS\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u5b9a\u4f4d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3GPS\u5728\u96a7\u9053\u7b49\u53d7\u9650\u73af\u5883\u4e2d\u5931\u6548\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u53ef\u89c1\u5149\u6280\u672f\u7684\u7ed3\u5408\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u901a\u8fc7Walsh-Hadamard\u7801\u5206\u914d\u552f\u4e00\u5bfc\u9891\u5e8f\u5217\u7ed9\u591a\u4e2aLED\u53d1\u5c04\u5668\uff0c\u5e76\u4f7f\u7528\u76f8\u4f4d\u76f8\u5173(POC)\u8fdb\u884c\u8ddd\u79bb\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8f66\u8f86\u4ee530 km/h\u901f\u5ea6\u884c\u9a76\u65f6\uff0c\u8ddd\u79bb\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\u5728100\u7c73\u8303\u56f4\u5185\u5c0f\u4e8e0.75\u7c73\uff0c\u8bef\u7801\u7387(BER)\u4f4e\u4e8e0.01\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65b0\u578b\u81ea\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u7ed3\u5408\u53ef\u89c1\u5149\u901a\u4fe1(VLC)\u548c\u53ef\u89c1\u5149\u5b9a\u4f4d(VLP)\uff0c\u5728GPS\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u8f66\u8f86\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002"}}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.", "AI": {"tldr": "MedRule-KG\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u5668\u63d0\u5347LLMs\u7684\u6570\u5b66\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5b9e\u73b0100%\u7cbe\u786e\u5339\u914d\u5e76\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6d41\u7545\u63a8\u7406\u6b65\u9aa4\u65f6\u5e38\u8fdd\u53cd\u7b80\u5355\u6570\u5b66\u6216\u903b\u8f91\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fdd\u8bc1\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u6b63\u786e\u6027\u3002", "method": "\u5f15\u5165MedRule-KG\uff0c\u4e00\u4e2a\u7d27\u51d1\u7684\u7c7b\u578b\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u7528\u4e8e\u5f3a\u5236\u6267\u884c\u6570\u5b66\u53ef\u89e3\u91ca\u89c4\u5219\u3002", "result": "\u572890\u4e2aFDA\u884d\u751f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedRule-KG\u5c06\u7cbe\u786e\u5339\u914d\u7387\u4ece0.767\u63d0\u5347\u81f30.900\uff0c\u52a0\u5165\u9a8c\u8bc1\u5668\u540e\u8fbe\u52301.000\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u89c4\u5219\u8fdd\u53cd\u3002", "conclusion": "MedRule-KG\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u6d88\u9664\u4e86\u89c4\u5219\u8fdd\u53cd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b89\u5168\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u901a\u7528\u652f\u67b6\u4f5c\u7528\u3002"}}
{"id": "2510.16333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16333", "abs": "https://arxiv.org/abs/2510.16333", "authors": ["Junha Song", "Sangdoo Yun", "Dongyoon Han", "Jaegul Choo", "Byeongho Heo"], "title": "RL makes MLLMs see better than SFT", "comment": null, "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0RL\u8bad\u7ec3\u4f18\u4e8eSFT\uff0c\u80fd\u63d0\u5347\u89c6\u89c9\u7f16\u7801\u5668\u7684\u80fd\u529b\uff1b\u63d0\u51fa\u7684PIVOT\u65b9\u6cd5\u9ad8\u6548\u4e14\u6027\u80fd\u5353\u8d8a\u3002", "motivation": "\u5f53\u524dMLLM\u7814\u7a76\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u4eceSFT\u8f6c\u5411RL\u8bad\u7ec3\u8303\u5f0f\u65f6\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5982\u4f55\u91cd\u5851\u89c6\u89c9\u7f16\u7801\u5668\u53caMLLM\u7684\u5206\u6790\u3002", "method": "\u901a\u8fc7\u591a\u6837\u5316\u7684\u5b9e\u9a8c\uff08\u5982ImageNet\u5206\u7c7b\u3001\u5206\u5272\u548c\u68af\u5ea6\u53ef\u89c6\u5316\uff09\u5206\u6790MLLM\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\uff08SFT\u548cRL\uff09\u5bf9\u89c6\u89c9\u8868\u793a\u7684\u5f71\u54cd\u3002", "result": "RL\u8bad\u7ec3\u7b56\u7565\u5728\u5f3a\u89c6\u89c9\u76f8\u5173\u7684VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eSFT\uff0c\u4e14\u80fd\u4ea7\u751f\u66f4\u7cbe\u786e\u7684\u89c6\u89c9\u8868\u793a\u3002PIVOT\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f8\u8f83\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u80fd\u4ea7\u751f\u66f4\u5f3a\u4e14\u5b9a\u4f4d\u66f4\u7cbe\u786e\u7684\u89c6\u89c9\u8868\u793a\uff0c\u63d0\u5347\u89c6\u89c9\u7f16\u7801\u5668\u7684\u80fd\u529b\u3002\u63d0\u51fa\u7684PIVOT\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4e3aMLLM\u7684\u89c6\u89c9\u4e3b\u5e72\u63d0\u4f9b\u4e86\u9ad8\u6548\u63a8\u8fdb\u8def\u5f84\u3002"}}
{"id": "2510.17237", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17237", "abs": "https://arxiv.org/abs/2510.17237", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance", "comment": "4 pages, technical report", "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201cPole-Image\u201d\u65b9\u6cd5\uff0c\u5229\u7528\u6746\u72b6\u5730\u6807\u53ca\u5176\u5468\u56f4\u73af\u5883\u751f\u62102D\u6781\u5750\u6807\u56fe\u50cf\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u9c81\u68d2\u81ea\u6211\u5b9a\u4f4d\u548c\u9ad8\u7075\u654f\u5ea6\u5730\u56fe\u7ef4\u62a4\u3002", "motivation": "\u957f\u671f\u81ea\u4e3b\u6027\u5bf9\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u9c81\u68d2\u7684\u81ea\u6211\u5b9a\u4f4d\u548c\u53ef\u9760\u7684\u5730\u56fe\u7ef4\u62a4\u3002\u4f20\u7edf\u5730\u6807\u65b9\u6cd5\u5728\u9ad8\u53ef\u68c0\u6d4b\u6027\u4f46\u4f4e\u72ec\u7279\u6027\uff08\u5982\u6746\u72b6\u7269\uff09\u4e0e\u9ad8\u72ec\u7279\u6027\u4f46\u96be\u4ee5\u7a33\u5b9a\u68c0\u6d4b\uff08\u5982\u5c40\u90e8\u70b9\u4e91\u7ed3\u6784\uff09\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u6746\u72b6\u5730\u6807\u7684\u9ad8\u68c0\u6d4b\u6027\u548c\u5468\u56f4\u70b9\u4e91\u7684\u72ec\u7279\u6027\uff0c\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c4\u8303\u8868\u793a\u65b9\u6cd5\u201cPole-Image\u201d\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6746\u72b6\u5730\u6807\u4f5c\u4e3a\u951a\u70b9\uff0c\u4ece\u5468\u56f4\u76843D\u7ed3\u6784\u4e2d\u751f\u6210\u7b7e\u540d\u3002Pole-Image\u5c06\u6746\u72b6\u5730\u6807\u53ca\u5176\u5468\u56f4\u73af\u5883\u8868\u793a\u4e3a\u4ee5\u6746\u4e3a\u539f\u70b9\u76842D\u6781\u5750\u6807\u56fe\u50cf\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u7a33\u5b9a\u6746\u4e0e\u53ef\u53d8\u5468\u56f4\u70b9\u4e91\u4e4b\u95f4\u7684\u201c\u76f8\u5bf9\u51e0\u4f55\u5173\u7cfb\u201d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u53c2\u8003\u70b9\u3002", "result": "\u901a\u8fc7Pole-Image\u8868\u793a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6a21\u578b\u5b66\u4e60\u5230\u4e86\u89c6\u89d2\u4e0d\u53d8\u4e14\u9ad8\u5ea6\u533a\u5206\u7684\u63cf\u8ff0\u7b26\u3002\u8be5\u63cf\u8ff0\u7b26\u514b\u670d\u4e86\u611f\u77e5\u6df7\u6dc6\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u81ea\u6211\u5b9a\u4f4d\uff0c\u540c\u65f6\u9ad8\u7cbe\u5ea6\u7f16\u7801\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u7684\u53d8\u5316\u68c0\u6d4b\uff0c\u4e3a\u5730\u56fe\u7ef4\u62a4\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPole-Image\u201d\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6746\u72b6\u5730\u6807\u53ca\u5176\u5468\u56f4\u73af\u5883\u8868\u793a\u4e3a2D\u6781\u5750\u6807\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u957f\u671f\u81ea\u4e3b\u6027\u4e2d\u9762\u4e34\u7684\u81ea\u6211\u5b9a\u4f4d\u548c\u5730\u56fe\u7ef4\u62a4\u7684\u6311\u6218\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u6746\u72b6\u5730\u6807\u7684\u9ad8\u68c0\u6d4b\u6027\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u81ea\u6211\u5b9a\u4f4d\u548c\u9ad8\u7075\u654f\u5ea6\u7684\u53d8\u5316\u68c0\u6d4b\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "SELECT\u662f\u4e00\u4e2a\u52a8\u6001\u951a\u5b9a\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u951a\u70b9\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u951a\u5b9a\u7b56\u7565\u5bfc\u81f4\u7684\u6982\u5ff5\u91cd\u65b0\u51fa\u73b0\u548c\u4fb5\u8680\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u951a\u5b9a\u7b56\u7565\uff0c\u8fd9\u5f80\u5f80\u5bfc\u81f4\u6982\u5ff5\u91cd\u65b0\u51fa\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8fdb\u884c\u4e86\u56e0\u679c\u8ffd\u8e2a\u4ee5\u63ed\u793a\u64e6\u9664\u5bf9\u951a\u5b9a\u9009\u62e9\u7684\u56fa\u6709\u654f\u611f\u6027\uff0c\u5e76\u5b9a\u4e49\u4e86Sibling Exclusive Concepts\u4f5c\u4e3a\u66f4\u4f18\u7684\u951a\u5b9a\u7c7b\u522b\u3002", "method": "\u63d0\u51fa\u4e86SELECT\uff08Sibling-Exclusive Evaluation for Contextual Targeting\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u52a8\u6001\u951a\u5b9a\u9009\u62e9\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\uff0c\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u951a\u70b9\u4ee5\u8fdb\u884c\u7cbe\u786e\u64e6\u9664\uff0c\u540c\u65f6\u8bc6\u522b\u5173\u952e\u8fb9\u754c\u951a\u70b9\u4ee5\u4fdd\u7559\u76f8\u5173\u6982\u5ff5\u3002", "result": "\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0cSELECT\u4e0d\u4ec5\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u8fd8\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e73\u5747\u6bcf\u4e2a\u6982\u5ff5\u7684\u951a\u5b9a\u6316\u6398\u4ec5\u97004\u79d2\u3002", "conclusion": "SELECT\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u951a\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u8fd8\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e73\u5747\u6bcf\u4e2a\u6982\u5ff5\u7684\u951a\u5b9a\u6316\u6398\u4ec5\u97004\u79d2\u3002"}}
{"id": "2510.16335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16335", "abs": "https://arxiv.org/abs/2510.16335", "authors": ["Bo Peng", "Jie Lu", "Guangquan Zhang", "Zhen Fang"], "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "comment": "revised and extended version of ICCV2025", "summary": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa GradNorm\uff0c\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u4e2d\u7684\u6b63\u540d\u8bcd\u8fc7\u6ee4\uff0c\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u4e14\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3 Language-assisted Image Clustering (LaIC) \u4e2d\u6b63\u540d\u8bcd\u8fc7\u6ee4\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6846\u67b6 GradNorm\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u68af\u5ea6\u5927\u5c0f\u6765\u8861\u91cf\u540d\u8bcd\u7684\u79ef\u6781\u6027\u3002", "result": "GradNorm \u5728\u7406\u8bba\u4e0a\u6709\u4e25\u683c\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "GradNorm \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u6b63\u540d\u8bcd\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u805a\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2510.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17249", "abs": "https://arxiv.org/abs/2510.17249", "authors": ["Franek Stark", "Rohit Kumar", "Shubham Vyas", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u57fa\u7840\u52a8\u6001\u63a7\u5236\u548c\u5728\u7ebf\u9002\u5e94\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u5728\u672a\u77e5\u5730\u5f62\u4e2d\u9ad8\u6548\u5bfc\u822a\uff0c\u5b9e\u5730\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u6781\u7aef\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5730\u5f62\u548c\u673a\u5668\u4eba\u53c2\u6570\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u73af\u5883\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6a21\u578b\u57fa\u7840\u52a8\u6001\u63a7\u5236\u3001\u5728\u7ebf\u6a21\u578b\u9002\u5e94\u548c\u81ea\u9002\u5e94\u811a\u6b65\u89c4\u5212\uff0c\u652f\u6301\u72b6\u6001\u4f30\u8ba1\uff08\u6709\u65e0\u63a5\u89e6\u4f20\u611f\u5747\u53ef\uff09\uff0c\u5e76\u80fd\u5728\u8fd0\u884c\u65f6\u91cd\u65b0\u914d\u7f6e\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e24\u79cd\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u3001\u591a\u79cd\u786c\u4ef6\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5728\u706b\u5c71\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u6210\u529f\u884c\u8d70\u8d85\u8fc7700\u7c73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u5757\u5316\u63a7\u5236\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u6781\u7aef\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u57fa\u7840\u52a8\u6001\u63a7\u5236\u548c\u5728\u7ebf\u6a21\u578b\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u5730\u5f62\u548c\u673a\u5668\u4eba\u53c2\u6570\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.", "AI": {"tldr": "\u7528\u6237\u901a\u8fc7\u8fdc\u89c1\u6216\u5c0f\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u53ef\u8c03\u6574\u7b97\u6cd5\u5bf9\u9f50\u5174\u8da3\uff0c\u4f46\u9700\u9762\u5bf9\u91cd\u5927\u8d1f\u62c5\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u5982\u4f55\u5728\u591a\u6b65\u4ea4\u4e92\u4e2d\u901a\u8fc7\u9009\u62e9\u6027\u53c2\u4e0e\u5185\u5bb9\u6765\u5f15\u5bfc\u7b97\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u4e0e\u5176\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u504f\u597d\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u5c06\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7406\u6027\u7cfb\u7edf2\u548c\u51b2\u52a8\u7cfb\u7edf1\u7684\u5206\u88c2\uff0c\u5e76\u7814\u7a76\u4e00\u4e2a\u591a\u9886\u5bfc\u8005\u3001\u5355\u8ffd\u968f\u8005\u7684\u6269\u5c55Stackelberg\u535a\u5f08\uff0c\u5176\u4e2d\u7528\u6237\uff08\u7cfb\u7edf2\uff09\u901a\u8fc7\u627f\u8bfa\u53c2\u4e0e\u7b56\u7565\u6765\u5f15\u5bfc\u7b97\u6cd5\u3002", "result": "\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u65f6\u95f4\u8303\u56f4\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u9f50\uff0c\u800c\u7f3a\u4e4f\u8fdc\u89c1\u7684\u7528\u6237\u5219\u4f1a\u88ab\u7b97\u6cd5\u7684\u76ee\u6807\u6240\u5bf9\u9f50\u3002\u5c0f\u800c\u6709\u6210\u672c\u7684\u4fe1\u53f7\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8fd9\u4e00\u5173\u952e\u65f6\u95f4\u8303\u56f4\u3002", "conclusion": "\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u8db3\u591f\u7684\u8fdc\u89c1\u6216\u5c0f\u800c\u6709\u6210\u672c\u7684\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u6765\u8c03\u6574\u7b97\u6cd5\uff0c\u4f7f\u5176\u4e0e\u81ea\u8eab\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff0c\u5c3d\u7ba1\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u80fd\u9762\u4e34\u91cd\u5927\u8d1f\u62c5\u3002"}}
{"id": "2510.16370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16370", "abs": "https://arxiv.org/abs/2510.16370", "authors": ["Pulin Li", "Guocheng Wu", "Li Yin", "Yuxin Zheng", "Wei Zhang", "Yanjie Zhou"], "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization", "comment": "https://github.com/wu33learn/MIRAD", "summary": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.", "AI": {"tldr": "MIRAD\u662f\u9996\u4e2a\u9488\u5bf9\u793e\u4f1a\u5236\u9020\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8986\u76d6\u4e86\u4e2a\u4f53\u5316\u4ea7\u54c1\u7684\u591a\u6837\u6027\u3001\u5730\u7406\u5206\u6563\u7684\u5236\u9020\u8282\u70b9\u548c\u6210\u50cf\u5f02\u8d28\u6027\u3002\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3a\u5de5\u4e1a5.0\u7684\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u73b0\u5b9e\u57fa\u7840\u3002", "motivation": "\u793e\u4f1a\u5236\u9020\u6a21\u5f0f\u4e0b\u7684\u8d28\u91cf\u63a7\u5236\u5728\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u9ad8\u5ea6\u5b9a\u5236\u5316\u7684\u4ea7\u54c1\u914d\u7f6e\u3001\u5c0f\u6279\u91cf\u788e\u7247\u5316\u8ba2\u5355\u4ee5\u53ca\u5206\u5e03\u5f0f\u7ad9\u70b9\u95f4\u6210\u50cf\u73af\u5883\u7684\u663e\u8457\u5dee\u5f02\u3002", "method": "\u5f15\u5165\u4e86Mass Individualization Robust Anomaly Detection (MIRAD)\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6db5\u76d6\u5355\u7c7b\u3001\u591a\u7c7b\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728MIRAD\u4e0a\u7684\u6027\u80fd\u5747\u663e\u8457\u4e0b\u964d\uff0c\u51f8\u663e\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2a\u4f53\u5316\u751f\u4ea7\u4e2d\u7f3a\u9677\u68c0\u6d4b\u7684\u672a\u89e3\u51b3\u590d\u6742\u6027\u3002", "conclusion": "MIRAD\u6570\u636e\u96c6\u4e3a\u5de5\u4e1a5.0\u65f6\u4ee3\u63d0\u4f9b\u4e86\u5f00\u53d1\u9c81\u68d2\u8d28\u91cf\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u7684\u73b0\u5b9e\u57fa\u7840\uff0c\u586b\u8865\u4e86\u5de5\u4e1a\u9700\u6c42\u4e0e\u5b66\u672f\u7814\u7a76\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2510.17261", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17261", "abs": "https://arxiv.org/abs/2510.17261", "authors": ["Fernando Salanova", "Jes\u00fas Roche", "Cristian Mahuela", "Eduardo Montijano"], "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection", "comment": "6 pages,3 figures, Iberian Robotics Conference 2025", "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eNets-within-Nets\u548cTransformer\u7684\u65b9\u6cd5\uff0c\u9ad8\u6548\u68c0\u6d4b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5b9e\u9a8c\u663e\u793a\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5f02\u6784\u4ee3\u7406\u6267\u884c\u9ad8\u7ea7\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u53ef\u9760\u7684\u65b9\u6cd5\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\u4ee5\u786e\u4fdd\u4efb\u52a1\u987a\u5229\u5b8c\u6210\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u6846\u67b6\uff08\u57fa\u4e8eNets-within-Nets\u8303\u5f0f\uff09\u534f\u8c03\u673a\u5668\u4eba\u884c\u52a8\u4e0eLTL\u5168\u5c40\u4efb\u52a1\u89c4\u8303\uff0c\u5e76\u91c7\u7528Transformer\u5f02\u5e38\u68c0\u6d4b\u6d41\u7a0b\u5206\u7c7b\u673a\u5668\u4eba\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6267\u884c\u6548\u7387\u8bc6\u522b\u4e0a\u8fbe\u523091.3%\u7684\u51c6\u786e\u7387\uff0c\u6838\u5fc3\u4efb\u52a1\u8fdd\u89c4\u68c0\u6d4b\u7387\u4e3a88.3%\uff0c\u7ea6\u675f\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u7387\u4e3a66.8%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eNets-within-Nets\u8303\u5f0f\u7684\u65b9\u6cd5\u548cTransformer\u5f02\u5e38\u68c0\u6d4b\u6d41\u7a0b\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\u5e76\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u63a7\u4e0e\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728GSM8K\u4e0a\u7684\u63a8\u7406\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5c1d\u8bd5\u6b21\u6570\u3002", "motivation": "\u5f53\u524d\u589e\u5f3aLLM\u63a8\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u79cd\u5b64\u7acb\u8303\u5f0f\uff1aMonitor-Generate\u65b9\u6cd5\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\uff0c\u800cGenerate-Verify\u65b9\u6cd5\u5219\u76f2\u76ee\u5f00\u59cb\u751f\u6210\u3002\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9e\u73b0Flavell\u7684\u8ba4\u77e5\u76d1\u63a7\u6a21\u578b\uff081979\uff09\uff0c\u5e76\u5c06\u5176\u64cd\u4f5c\u5316\u4e3a\u4e00\u4e2a\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\u3002", "result": "\u5728GSM8K\u4e0a\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a75.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSELF-REFINE\uff0868.44%\uff09\u548cSelf-Verification\uff0867.07%\uff09\uff0c\u4e14\u5c1d\u8bd5\u6b21\u6570\u66f4\u5c11\uff081.3 vs 2.0\uff09\uff0c\u63a8\u7406\u6210\u672c\u589e\u52a027-37%\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u524d\u7f6e\u76d1\u63a7\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u51cf\u5c11\u7ec6\u5316\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u7b97\u672f\u63a8\u7406\u4e4b\u5916\u8fdb\u884c\u8bc4\u4f30\u4ee5\u786e\u5b9a\u5176\u666e\u9002\u6027\u3002"}}
{"id": "2510.16371", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16371", "abs": "https://arxiv.org/abs/2510.16371", "authors": ["Mohammad Javad Ahmadi", "Iman Gandomi", "Parisa Abdi", "Seyed-Farzad Mohammadi", "Amirhossein Taslimi", "Mehdi Khodaparast", "Hassan Hashemi", "Mahdi Tavakoli", "Hamid D. Taghirad"], "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis", "comment": "20 pages, 11 figures, 11 tables. Data descriptor for the Cataract-LMM\n  benchmark dataset. Source code and dataset are available", "summary": "The development of computer-assisted surgery systems depends on large-scale,\nannotated datasets. Current resources for cataract surgery often lack the\ndiversity and annotation depth needed to train generalizable deep-learning\nmodels. To address this gap, we present a dataset of 3,000 phacoemulsification\ncataract surgery videos from two surgical centers, performed by surgeons with a\nrange of experience levels. This resource is enriched with four annotation\nlayers: temporal surgical phases, instance segmentation of instruments and\nanatomical structures, instrument-tissue interaction tracking, and quantitative\nskill scores based on the established competency rubrics like the ICO-OSCAR.\nThe technical quality of the dataset is supported by a series of benchmarking\nexperiments for key surgical AI tasks, including workflow recognition, scene\nsegmentation, and automated skill assessment. Furthermore, we establish a\ndomain adaptation baseline for the phase recognition task by training a model\non a subset of surgical centers and evaluating its performance on a held-out\ncenter. The dataset and annotations are available in Google Form\n(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b3,000\u4e2a\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5177\u6709\u591a\u5c42\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5bf9\u624b\u672fAI\u4efb\u52a1\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u767d\u5185\u969c\u624b\u672f\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u6ce8\u91ca\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6536\u96c6\u4e86\u6765\u81ea\u4e24\u4e2a\u624b\u672f\u4e2d\u5fc3\u76843,000\u4e2a\u8d85\u58f0\u4e73\u5316\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\uff0c\u5e76\u6dfb\u52a0\u4e86\u56db\u79cd\u6ce8\u91ca\u5c42\uff1a\u624b\u672f\u9636\u6bb5\u3001\u5de5\u5177\u548c\u7ed3\u6784\u7684\u5b9e\u4f8b\u5206\u5272\u3001\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u8ddf\u8e2a\u4ee5\u53ca\u57fa\u4e8eICO-OSCAR\u7684\u6280\u80fd\u8bc4\u5206\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u5173\u952e\u624b\u672fAI\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u5de5\u4f5c\u6d41\u8bc6\u522b\u3001\u573a\u666f\u5206\u5272\u548c\u81ea\u52a8\u6280\u80fd\u8bc4\u4f30\uff0c\u5e76\u5efa\u7acb\u4e86\u9636\u6bb5\u8bc6\u522b\u4efb\u52a1\u7684\u9886\u57df\u9002\u5e94\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u767d\u5185\u969c\u624b\u672fAI\u4efb\u52a1\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u7684\u6ce8\u91ca\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6280\u672f\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u7cfb\u7edf\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17270", "abs": "https://arxiv.org/abs/2510.17270", "authors": ["Lucas Schulze", "Juliano Decico Negri", "Victor Barasuol", "Vivian Suzano Medeiros", "Marcelo Becker", "Jan Peters", "Oleg Arenz"], "title": "Floating-Base Deep Lagrangian Networks", "comment": null, "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u7ea6\u675f\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bFeLaN\uff0c\u7528\u4e8e\u6d6e\u52a8\u57fa\u7cfb\u7edf\u8bc6\u522b\uff0c\u8868\u73b0\u4f18\u5f02\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "motivation": "\u5f53\u524d\u7070\u76d2\u6a21\u578b\u5ffd\u7565\u4e86\u6d6e\u52a8\u57fa\u7cfb\u7edf\uff08\u5982\u4eba\u5f62\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\uff09\u7684\u7279\u5b9a\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u7269\u7406\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u7684\u60ef\u6027\u77e9\u9635\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7269\u7406\u4e0a\u5408\u7406\u7684\u60ef\u6027\u77e9\u9635\u3002", "result": "FeLaN \u5728\u591a\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u7269\u7406\u4e00\u81f4\u6027\u66f4\u5f3a\u3002", "conclusion": "Floating-Base Deep Lagrangian Networks (FeLaN) \u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.16382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16382", "abs": "https://arxiv.org/abs/2510.16382", "authors": ["Ze Tao", "Jian Zhang", "Haowei Li", "Xianshuai Li", "Yifei Peng", "Xiyao Liu", "Senzhang Wang", "Chao Liu", "Sheng Ren", "Shichao Zhang"], "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization", "comment": null, "summary": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM.", "AI": {"tldr": "HSCM\u662f\u4e00\u79cd\u53d7\u4eba\u7c7b\u667a\u80fd\u542f\u53d1\u7684\u56e0\u679c\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u56fe\u50cf\u5c5e\u6027\u63d0\u5347\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9886\u57df\u6cdb\u5316\u6a21\u578b\u4f9d\u8d56\u7edf\u8ba1\u6570\u636e\u6355\u6349\u6570\u636e-\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002HSCM\u53d7\u4eba\u7c7b\u667a\u80fd\u542f\u53d1\uff0c\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "HSCM\u901a\u8fc7\u89e3\u8026\u548c\u91cd\u65b0\u52a0\u6743\u5173\u952e\u56fe\u50cf\u5c5e\u6027\uff08\u5982\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\uff09\uff0c\u5efa\u6a21\u7ec6\u7c92\u5ea6\u56e0\u679c\u673a\u5236\u3002", "result": "HSCM\u5728\u591a\u6837\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "HSCM\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5c42\u6b21\u5904\u7406\u548c\u591a\u7ea7\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2510.16375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16375", "abs": "https://arxiv.org/abs/2510.16375", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance", "comment": "Under review", "summary": "Road potholes pose significant safety hazards and maintenance challenges,\nparticularly on India's diverse and under-maintained road networks. This paper\npresents iWatchRoadv2, a fully automated end-to-end platform for real-time\npothole detection, GPS-based geotagging, and dynamic road health visualization\nusing OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000\ndashcam frames capturing diverse Indian road conditions, weather patterns, and\nlighting scenarios, which we used to fine-tune the Ultralytics YOLO model for\naccurate pothole detection. The system synchronizes OCR-extracted video\ntimestamps with external GPS logs to precisely geolocate each detected pothole,\nenriching detections with comprehensive metadata, including road segment\nattribution and contractor information managed through an optimized backend\ndatabase. iWatchRoadv2 introduces intelligent governance features that enable\nauthorities to link road segments with contract metadata through a secure login\ninterface. The system automatically sends alerts to contractors and officials\nwhen road health deteriorates, supporting automated accountability and warranty\nenforcement. The intuitive web interface delivers actionable analytics to\nstakeholders and the public, facilitating evidence-driven repair planning,\nbudget allocation, and quality assessment. Our cost-effective and scalable\nsolution streamlines frame processing and storage while supporting seamless\npublic engagement for urban and rural deployments. By automating the complete\npothole monitoring lifecycle, from detection to repair verification,\niWatchRoadv2 enables data-driven smart city management, transparent governance,\nand sustainable improvements in road infrastructure maintenance. The platform\nand live demonstration are accessible at\nhttps://smlab.niser.ac.in/project/iwatchroad.", "AI": {"tldr": "iWatchRoadv2\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u9053\u8def\u5751\u6d1e\u3001\u5730\u7406\u6807\u8bb0\u548c\u9053\u8def\u5065\u5eb7\u53ef\u89c6\u5316\uff0c\u652f\u6301\u667a\u80fd\u6cbb\u7406\u548c\u6570\u636e\u9a71\u52a8\u7684\u9053\u8def\u7ef4\u62a4\u3002", "motivation": "\u5370\u5ea6\u591a\u6837\u5316\u4e14\u7ef4\u62a4\u4e0d\u8db3\u7684\u9053\u8def\u7f51\u7edc\u4e2d\uff0c\u9053\u8def\u5751\u6d1e\u5e26\u6765\u4e86\u663e\u8457\u7684\u5b89\u5168\u9690\u60a3\u548c\u7ef4\u62a4\u6311\u6218\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528\u81ea\u6807\u6ce8\u76847,000\u591a\u5e27\u5370\u5ea6\u9053\u8def\u591a\u6837\u5316\u6761\u4ef6\u7684\u4eea\u8868\u76d8\u6444\u50cf\u5934\u6570\u636e\u96c6\uff0c\u5fae\u8c03Ultralytics YOLO\u6a21\u578b\u8fdb\u884c\u5751\u6d1e\u68c0\u6d4b\uff0c\u5e76\u7ed3\u5408OCR\u63d0\u53d6\u7684\u89c6\u9891\u65f6\u95f4\u6233\u4e0e\u5916\u90e8GPS\u65e5\u5fd7\u540c\u6b65\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5730\u7406\u5b9a\u4f4d\u3002\u7cfb\u7edf\u8fd8\u5305\u62ec\u4f18\u5316\u7684\u540e\u7aef\u6570\u636e\u5e93\u7ba1\u7406\u9053\u8def\u6bb5\u5c5e\u6027\u548c\u627f\u5305\u5546\u4fe1\u606f\uff0c\u4ee5\u53ca\u667a\u80fd\u6cbb\u7406\u529f\u80fd\u3002", "result": "iWatchRoadv2\u5e73\u53f0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5751\u6d1e\u68c0\u6d4b\u3001GPS\u5730\u7406\u6807\u8bb0\u548c\u52a8\u6001\u9053\u8def\u5065\u5eb7\u53ef\u89c6\u5316\uff0c\u652f\u6301\u81ea\u52a8\u5316\u95ee\u8d23\u548c\u4fdd\u4fee\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u76f4\u89c2\u7684Web\u754c\u9762\u4e3a\u5229\u76ca\u76f8\u5173\u8005\u548c\u516c\u4f17\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684 analytics\u3002", "conclusion": "iWatchRoadv2\u901a\u8fc7\u81ea\u52a8\u5316\u6574\u4e2a\u5751\u6d1e\u76d1\u6d4b\u751f\u547d\u5468\u671f\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u9a71\u52a8\u7684\u667a\u6167\u57ce\u5e02\u7ba1\u7406\u3001\u900f\u660e\u6cbb\u7406\u548c\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u7684\u53ef\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2510.17315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17315", "abs": "https://arxiv.org/abs/2510.17315", "authors": ["Po-Chen Ko", "Jiayuan Mao", "Yu-Hsiang Fu", "Hsien-Jeng Yeh", "Chu-Rong Chen", "Wei-Chiu Ma", "Yilun Du", "Shao-Hua Sun"], "title": "Implicit State Estimation via Video Replanning", "comment": null, "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4ea4\u4e92\u65f6\u95f4\u6570\u636e\u7684\u89c6\u9891\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u66f4\u65b0\u548c\u8fc7\u6ee4\u5931\u8d25\u8ba1\u5212\uff0c\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u548c\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\uff0c\u63d0\u5347\u4e86\u91cd\u65b0\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u89c4\u5212\u6846\u67b6\u96be\u4ee5\u9002\u5e94\u4ea4\u4e92\u65f6\u7684\u5931\u8d25\uff0c\u4e3b\u8981\u56e0\u4e3a\u65e0\u6cd5\u5904\u7406\u90e8\u5206\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u4ea4\u4e92\u65f6\u95f4\u6570\u636e\u96c6\u6210\u5230\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u5728\u7ebf\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u5e76\u8fc7\u6ee4\u5931\u8d25\u8ba1\u5212\uff0c\u5b9e\u73b0\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u5728\u65b0\u6a21\u62df\u64cd\u4f5c\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u5347\u91cd\u65b0\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u63a8\u52a8\u89c6\u9891\u51b3\u7b56\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u548c\u8fc7\u6ee4\u5931\u8d25\u8ba1\u5212\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u9002\u5e94\u548c\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u89c6\u9891\u89c4\u5212\u7684\u91cd\u65b0\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.", "AI": {"tldr": "RGMem\u662f\u4e00\u4e2a\u53d7\u7269\u7406\u5b66\u542f\u53d1\u7684\u81ea\u6f14\u5316\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4fe1\u606f\u5904\u7406\u89e3\u51b3LLM\u4f1a\u8bdd\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u7528\u6237\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4f1a\u8bdd\u7cfb\u7edf\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u9759\u6001\u53c2\u6570\u8bb0\u5fc6\u7684\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u5efa\u6a21\u8de8\u4f1a\u8bdd\u7684\u957f\u671f\u7528\u6237\u72b6\u6001\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u5982RAG\u548c\u663e\u5f0f\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u7ea7\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u7f3a\u4e4f\u4ece\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u63d0\u70bc\u6f5c\u5728\u504f\u597d\u548c\u6df1\u5c42\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u8bb0\u5fc6\u6846\u67b6RGMem\uff0c\u53d7\u7269\u7406\u5b66\u4e2d\u7684\u91cd\u6574\u5316\u7fa4\uff08RG\uff09\u601d\u60f3\u542f\u53d1\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u7c97\u7c92\u5316\u548c\u91cd\u6807\u5ea6\u64cd\u4f5c\uff0c\u4ece\u5bf9\u8bdd\u5386\u53f2\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7528\u6237\u6d1e\u5bdf\uff0c\u9010\u6b65\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf\u3002", "result": "RGMem\u6846\u67b6\u80fd\u591f\u4ece\u5bf9\u8bdd\u5386\u53f2\u7684\u591a\u4e2a\u5c3a\u5ea6\u4e2d\u7ec4\u7ec7\u548c\u63d0\u53d6\u4fe1\u606f\uff0c\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "conclusion": "RGMem\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u4fe1\u606f\u538b\u7f29\u548c\u6d8c\u73b0\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u4ece\u566a\u58f0\u548c\u5fae\u89c2\u4ea4\u4e92\u4e2d\u6784\u5efa\u9ad8\u5c42\u6b21\u3001\u51c6\u786e\u7684\u7528\u6237\u753b\u50cf\uff0c\u4e3aLLM\u65f6\u4ee3\u7684\u8bed\u8a00\u4ee3\u7406\u63d0\u4f9b\u4e86\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16377", "abs": "https://arxiv.org/abs/2510.16377", "authors": ["Tianhang Cheng", "Albert J. Zhai", "Evan Z. Chen", "Rui Zhou", "Yawen Deng", "Zitong Li", "Kejie Zhao", "Janice Shiu", "Qianyu Zhao", "Yide Xu", "Xinlei Wang", "Yuan Shen", "Sheng Wang", "Lisa Ainsworth", "Kaiyu Guan", "Shenlong Wang"], "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World", "comment": "ICCV 2025", "summary": "Learning 3D parametric shape models of objects has gained popularity in\nvision and graphics and has showed broad utility in 3D reconstruction,\ngeneration, understanding, and simulation. While powerful models exist for\nhumans and animals, equally expressive approaches for modeling plants are\nlacking. In this work, we present Demeter, a data-driven parametric model that\nencodes key factors of a plant morphology, including topology, shape,\narticulation, and deformation into a compact learned representation. Unlike\nprevious parametric models, Demeter handles varying shape topology across\nvarious species and models three sources of shape variation: articulation,\nsubcomponent shape variation, and non-rigid deformation. To advance crop plant\nmodeling, we collected a large-scale, ground-truthed dataset from a soybean\nfarm as a testbed. Experiments show that Demeter effectively synthesizes\nshapes, reconstructs structures, and simulates biophysical processes. Code and\ndata is available at https://tianhang-cheng.github.io/Demeter/.", "AI": {"tldr": "Demeter\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u690d\u7269\u5f62\u6001\u53c2\u6570\u6a21\u578b\uff0c\u80fd\u5904\u7406\u591a\u79cd\u5f62\u72b6\u53d8\u5316\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u867d\u7136\u4eba\u7c7b\u548c\u52a8\u7269\u76843D\u53c2\u6570\u5f62\u72b6\u6a21\u578b\u5df2\u7ecf\u975e\u5e38\u5f3a\u5927\uff0c\u4f46\u690d\u7269\u5efa\u6a21\u65b9\u9762\u7f3a\u4e4f\u540c\u6837\u8868\u8fbe\u529b\u7684\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Demeter\u901a\u8fc7\u7f16\u7801\u690d\u7269\u7684\u62d3\u6251\u3001\u5f62\u72b6\u3001\u5173\u8282\u548c\u53d8\u5f62\u7b49\u5173\u952e\u56e0\u7d20\u5230\u4e00\u4e2a\u7d27\u51d1\u7684\u5b66\u4e60\u8868\u793a\u4e2d\uff0c\u5904\u7406\u4e0d\u540c\u7269\u79cd\u7684\u5f62\u72b6\u62d3\u6251\u53d8\u5316\uff0c\u5e76\u5efa\u6a21\u4e09\u79cd\u5f62\u72b6\u53d8\u5316\u6e90\uff1a\u5173\u8282\u3001\u5b50\u7ec4\u4ef6\u5f62\u72b6\u53d8\u5316\u548c\u975e\u521a\u6027\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDemeter\u5728\u5f62\u72b6\u5408\u6210\u3001\u7ed3\u6784\u91cd\u5efa\u548c\u751f\u7269\u7269\u7406\u8fc7\u7a0b\u6a21\u62df\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Demeter\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6570\u636e\u9a71\u52a8\u53c2\u6570\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5408\u6210\u690d\u7269\u5f62\u6001\u3001\u91cd\u5efa\u7ed3\u6784\u5e76\u6a21\u62df\u751f\u7269\u7269\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.17335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17335", "abs": "https://arxiv.org/abs/2510.17335", "authors": ["Xintong Yang", "Minglun Wei", "Ze Ji", "Yu-Kun Lai"], "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials", "comment": "Accepted as a regular paper by the IEEE Transactions on Robotics", "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks.", "AI": {"tldr": "DDBot \u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u5fae\u5206\u7269\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u9897\u7c92\u6750\u6599\u6316\u6398\u4efb\u52a1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5feb\u901f\u6536\u655b\u548c\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u3002", "motivation": "\u9897\u7c92\u6750\u6599\u64cd\u4f5c\u9762\u4e34\u590d\u6742\u63a5\u89e6\u52a8\u529b\u5b66\u3001\u4e0d\u53ef\u9884\u6d4b\u6750\u6599\u7279\u6027\u548c\u7cfb\u7edf\u72b6\u6001\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "DDBot \u914d\u5907\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53ef\u5fae\u5206\u7269\u7406\u7684\u6a21\u62df\u5668\uff0c\u652f\u6301 GPU \u52a0\u901f\u5e76\u884c\u8ba1\u7b97\u548c\u81ea\u52a8\u5fae\u5206\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7cfb\u7edf\u8bc6\u522b\u548c\u9ad8\u7cbe\u5ea6\u6316\u6398\u6280\u80fd\u4f18\u5316\u5b9e\u73b0\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a DDBot \u80fd\u5728 5 \u81f3 20 \u5206\u949f\u5185\u9ad8\u6548\u8bc6\u522b\u672a\u77e5\u9897\u7c92\u6750\u6599\u52a8\u6001\u5e76\u4f18\u5316\u6316\u6398\u6280\u80fd\uff0c\u96f6\u6837\u672c\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "DDBot \u6846\u67b6\u5728\u672a\u77e5\u7269\u7406\u7279\u6027\u7684\u9897\u7c92\u6750\u6599\u6316\u6398\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReviewSense\u6846\u67b6\uff0c\u5229\u7528LLM\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u4e1a\u52a1\u5efa\u8bae\uff0c\u6574\u5408\u805a\u7c7b\u548c\u4e13\u5bb6\u8bc4\u4f30\uff0c\u521d\u6b65\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u64c5\u957f\u9884\u6d4b\u7528\u6237\u504f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e1a\u52a1\u5efa\u8bae\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u805a\u7c7b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9002\u914d\u548c\u4e13\u5bb6\u9a71\u52a8\u8bc4\u4f30\u7684\u7edf\u4e00\u6d41\u7a0b\u3002", "result": "\u521d\u6b65\u624b\u52a8\u8bc4\u4f30\u8868\u660e\uff0c\u6a21\u578b\u63a8\u8350\u4e0e\u4e1a\u52a1\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u6f5c\u529b\u3002", "conclusion": "ReviewSense\u6846\u67b6\u901a\u8fc7\u6574\u5408\u805a\u7c7b\u3001LLM\u9002\u914d\u548c\u4e13\u5bb6\u8bc4\u4f30\uff0c\u4e3a\u4e1a\u52a1\u51b3\u7b56\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5ba2\u6237\u53cd\u9988\u7684\u6df1\u5ea6\u6d1e\u5bdf\uff0c\u5c55\u793a\u4e86AI\u9a71\u52a8\u60c5\u611f\u5206\u6790\u5728\u4f18\u5316\u5546\u4e1a\u7b56\u7565\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.16396", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16396", "abs": "https://arxiv.org/abs/2510.16396", "authors": ["Yeh Keng Hao", "Hsu Tzu Wei", "Sun Min"], "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation", "comment": "Accepted to AICCC 2025", "summary": "With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5377\u79ef\u548cSPLite\u89e3\u7801\u5668\u63d0\u5347AR/VR\u8fb9\u7f18\u8bbe\u5907\u7684\u6548\u7387\uff0c\u901f\u5ea6\u63d0\u53472.98\u500d\uff0c\u7cbe\u5ea6\u4e0d\u53d8\u3002", "motivation": "\u968f\u7740AR/VR\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u5b9e\u65f6\u63a8\u7406\u3001\u4f4e\u529f\u8017\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u7a00\u758f\u5377\u79ef\u548cSPLite\u89e3\u7801\u5668\uff0c\u5e76\u5e94\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u3002", "result": "\u5728Raspberry Pi 5\u4e0a\u5b9e\u73b0\u7aef\u5230\u7aef\u6548\u7387\u63d0\u534742%\uff0c\u89e3\u7801\u5e27\u7387\u63d0\u53473.1\u500d\uff0c\u6574\u4f53\u901f\u5ea6\u63d0\u53472.98\u500d\uff0c\u7cbe\u5ea6\u4e0e\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8eAR/VR\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u63a8\u7406\u3002"}}
{"id": "2510.17341", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17341", "abs": "https://arxiv.org/abs/2510.17341", "authors": ["Fan Shao", "Satoshi Endo", "Sandra Hirche", "Fanny Ficuciello"], "title": "Interactive Force-Impedance Control", "comment": null, "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ea4\u4e92\u529b-\u963b\u6297\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u63a5\u89e6\u5bc6\u96c6\u73af\u5883\u4e2d\u5b89\u5168\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u4e0e\u4e3b\u52a8\u4eba\u7c7b\u6216\u975e\u88ab\u52a8\u73af\u5883\u7269\u7406\u4ea4\u4e92\u65f6\u53ef\u80fd\u5931\u53bb\u88ab\u52a8\u6027\u4ece\u800c\u5371\u53ca\u5b89\u5168\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4ea4\u4e92\u529b-\u963b\u6297\u63a7\u5236\uff08IFIC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u4efb\u52a1\u63a7\u5236\u7aef\u53e3\u5b9e\u73b0\u7cfb\u7edf\u88ab\u52a8\u6027\u3002", "result": "IFIC\u6846\u67b6\u80fd\u591f\u9002\u5e94\u4ea4\u4e92\u529f\u7387\u6d41\uff0c\u786e\u4fdd\u5728\u63a5\u89e6\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u8f7b\u677e\u548c\u5b89\u5168\u4ea4\u4e92\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ea4\u4e92\u529b-\u963b\u6297\u63a7\u5236\uff08IFIC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u5e94\u4ea4\u4e92\u529f\u7387\u6d41\uff0c\u786e\u4fdd\u5728\u63a5\u89e6\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u8f7b\u677e\u548c\u5b89\u5168\u4ea4\u4e92\u3002\u63a7\u5236\u67b6\u6784\u5728\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u5185\u5236\u5b9a\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u4efb\u52a1\u63a7\u5236\u7aef\u53e3\u4fdd\u8bc1\u7cfb\u7edf\u88ab\u52a8\u6027\u3002"}}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.", "AI": {"tldr": "NP-ENGINE\u6846\u67b6\u9996\u6b21\u5168\u9762\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u89e3\u51b3NP-hard\u95ee\u9898\uff0c\u63a8\u51fa\u7684QWEN2.5-7B-NP\u6a21\u578b\u5728NP-BENCH\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u5e76\u5c55\u793a\u4e86RLVR\u8bad\u7ec3\u7684\u6cdb\u5316\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u89e3\u51b3\u590d\u6742\u4f18\u5316\u95ee\u9898\uff08\u5982NP-hard\u95ee\u9898\uff09\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86NP-ENGINE\u6846\u67b6\uff0c\u5305\u542b\u53ef\u63a7\u5236\u7684\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u652f\u6301\u5206\u5c42\u96be\u5ea6\u7684RLVR\u8bad\u7ec3\u3002\u540c\u65f6\u63a8\u51fa\u4e86NP-BENCH\u57fa\u51c6\u548cQWEN2.5-7B-NP\u6a21\u578b\u3002", "result": "QWEN2.5-7B-NP\u5728NP-BENCH\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5c55\u793a\u4e86RLVR\u8bad\u7ec3\u5bf9\u8de8\u9886\u57df\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RLVR\u8bad\u7ec3\u5728NP-ENGINE-DATA\u4e0a\u4e0d\u4ec5\u80fd\u63d0\u5347LLM\u89e3\u51b3NP-hard\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd8\u80fd\u663e\u8457\u589e\u5f3a\u5176\u5728\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u548c\u975e\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86RLVR\u6269\u5c55\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.16410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16410", "abs": "https://arxiv.org/abs/2510.16410", "authors": ["Changyue Shi", "Minghao Chen", "Yiping Mao", "Chuxiao Yang", "Xinyuan Hu", "Jiajun Ding", "Zhou Yu"], "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting", "comment": null, "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.", "AI": {"tldr": "REALM\u662f\u4e00\u79cd\u521b\u65b0\u7684MLLM\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u7b56\u7565\u5b9e\u73b03D\u5bf9\u8c61\u7684\u9ad8\u7cbe\u5ea6\u5206\u5272\uff0c\u652f\u6301\u590d\u6742\u6307\u4ee4\u548c\u591a\u79cd\u4ea4\u4e92\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u6307\u4ee4\uff0c\u4ee5\u53ca2D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u7684\u5c40\u9650\u6027\u3002", "method": "REALM\u91c7\u7528\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684\u5168\u5c40\u5230\u5c40\u90e8\u7a7a\u95f4\u5b9a\u4f4d\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u5e76\u884c\u5904\u7406\u5b9e\u73b0\u7c97\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u518d\u901a\u8fc7\u5408\u6210\u7279\u5199\u89c6\u56fe\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u5272\u3002", "result": "\u5728LERF\u30013D-OVS\u548cREALM3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5bf9\u8c61\u79fb\u9664\u3001\u66ff\u6362\u548c\u98ce\u683c\u8f6c\u6362\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "REALM\u6846\u67b6\u5728\u7406\u89e3\u548c\u6267\u884c\u590d\u6742\u7684\u4eba\u7c7b\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u591a\u79cd3D\u4ea4\u4e92\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2510.17369", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17369", "abs": "https://arxiv.org/abs/2510.17369", "authors": ["Haochen Su", "Cristian Meo", "Francesco Stella", "Andrea Peirone", "Kai Junge", "Josie Hughes"], "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots", "comment": "Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main\n  paper, excluding references and supplements)", "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86VLA\u6a21\u578b\u5728\u8f6f\u4f53\u8fde\u7eed\u673a\u68b0\u81c2\u4e0a\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u5b9e\u73b0\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\uff0c\u6027\u80fd\u4e0e\u521a\u6027\u673a\u68b0\u81c2\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4f53\u8fde\u7eed\u673a\u68b0\u81c2\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u9700\u6c42\uff0c\u586b\u8865VLA\u6a21\u578b\u5728\u975e\u4f20\u7edf\u673a\u68b0\u81c2\u4e0a\u90e8\u7f72\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u7684\u5fae\u8c03\u548c\u90e8\u7f72\u6d41\u7a0b\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cd\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\uff08OpenVLA-OFT\u548c\u03c00\uff09\u5728\u4ee3\u8868\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\uff0c\u8f6f\u4f53\u673a\u5668\u4eba\u8868\u73b0\u4e0e\u521a\u6027\u673a\u68b0\u81c2\u76f8\u5f53\uff0c\u8bc1\u5b9e\u5fae\u8c03\u5bf9\u4e8e\u5f25\u8865\u6784\u578b\u5dee\u5f02\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u7ed3\u5408VLA\u6a21\u578b\u4e0e\u8f6f\u4f53\u673a\u5668\u4eba\u53ef\u4ee5\u5b9e\u73b0\u5b89\u5168\u3001\u7075\u6d3b\u7684\u5177\u8eabAI\u5728\u4eba\u673a\u5171\u4eab\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.16533", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16533", "abs": "https://arxiv.org/abs/2510.16533", "authors": ["Eilene Tomkins-Flanagan", "Connor Hanley", "Mary A. Kelly"], "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination", "comment": null, "summary": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned.", "AI": {"tldr": "Doug\u662f\u4e00\u79cd\u57fa\u4e8eVSA\u7684\u8ba1\u7b97\u673a\u8bed\u8a00\uff0c\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0\u6280\u80fd\u83b7\u53d6\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7c7b\u578b\uff0c\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u8868\u5f81\u3002", "motivation": "\u5f53\u524d\u7684\u6280\u80fd\u83b7\u53d6\u65b9\u6cd5\u6548\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u6280\u80fd\u83b7\u53d6\u7684\u901f\u5ea6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Doug\u8bed\u8a00\uff0c\u5b9e\u73b0\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6280\u80fd\u83b7\u53d6\u65b9\u5f0f\uff0c\u5e76\u6a21\u62df\u4eba\u7c7b\u5927\u8111\u4e2d\u7684\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "Doug\u57fa\u4e8e\u69fd\u503c\u7f16\u7801\u65b9\u6848\u548c\u5168\u606f\u58f0\u660e\u6027\u8bb0\u5fc6\uff08HDM\uff09\u7f16\u7801\u7c7b\u578b\uff0c\u5e76\u4f7f\u7528Lisp VSA\u7684\u53d8\u4f53\u7f16\u7801\u9879\u3002\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7684\u5d4c\u5165\u7a7a\u95f4\uff0cDoug\u80fd\u591f\u5c06\u7c7b\u578b\u8868\u793a\u4e3a\u53ef\u5b66\u4e60\u7684\u70b9\uff0c\u5e76\u5b9e\u73b0\u7c7b\u578b\u7ed3\u6784\u548c\u5185\u5bb9\u7684\u76f8\u4f3c\u6027\u3002", "result": "Doug\u8bed\u8a00\u80fd\u591f\u9ad8\u6548\u5730\u5b9e\u73b0\u6280\u80fd\u83b7\u53d6\uff0c\u5176\u901f\u5ea6\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u66b4\u529b\u641c\u7d22\uff09\uff0c\u5e76\u80fd\u591f\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7c7b\u578b\uff0c\u4ece\u800c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5fc3\u7406\u8868\u5f81\u7684\u5b9e\u9645\u5b58\u5728\u548c\u5b66\u4e60\u65b9\u5f0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDoug\u7684\u8ba1\u7b97\u673a\u8bed\u8a00\uff0c\u901a\u8fc7\u5411\u91cf\u7b26\u53f7\u67b6\u6784\uff08VSA\uff09\u7f16\u7801\uff0c\u4f7f\u5f97\u6240\u6709\u7c7b\u578b\u5316\u7a0b\u5e8f\u90fd\u80fd\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u505c\u6b62\u8fd0\u884c\u3002Doug\u4e0d\u4ec5\u80fd\u591f\u7f16\u7801\u8f7b\u91cf\u7ebf\u6027\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\uff08LLFPL\uff09\uff0c\u8fd8\u80fd\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7684\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u7c7b\u578b\uff0c\u4ece\u800c\u6a21\u62df\u4eba\u7c7b\u6280\u80fd\u83b7\u53d6\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2510.16416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16416", "abs": "https://arxiv.org/abs/2510.16416", "authors": ["Xiaojun Guo", "Runyu Zhou", "Yifei Wang", "Qi Zhang", "Chenheng Zhang", "Stefanie Jegelka", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Yisen Wang"], "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "comment": null, "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.", "AI": {"tldr": "SSL4RL \u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u6216\u5728\u63a8\u7406\u4e2d\u4f7f\u7528\u6587\u672c\u6377\u5f84\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u5956\u52b1\u673a\u5236\u6765\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51fa SSL4RL \u6846\u67b6\uff0c\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u65cb\u8f6c\u9884\u6d4b\u6216\u63a9\u7801\u8865\u4e01\u91cd\u5efa\uff09\u8f6c\u5316\u4e3a\u5bc6\u96c6\u7684\u81ea\u52a8\u5956\u52b1\u4fe1\u53f7\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSL4RL \u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u56fe\u5b66\u4e60\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "conclusion": "SSL4RL \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u81ea\u76d1\u7763\u76ee\u6807\u6765\u5bf9\u9f50\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2510.17408", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17408", "abs": "https://arxiv.org/abs/2510.17408", "authors": ["Halima I. Kure", "Jishna Retnakumari", "Augustine O. Nwajana", "Umar M. Ismail", "Bilyaminu A. Romo", "Ehigiator Egho-Promise"], "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting", "comment": "5 pages, 2 figures", "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings.", "AI": {"tldr": "\u7ed3\u5408\u53ef\u4fe1AI\u548c\u8282\u80fd\u673a\u68b0\u81c2\u7684\u667a\u80fd\u5e9f\u7269\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7CNN\u548c\u8fc1\u79fb\u5b66\u4e60\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u5206\u7c7b\uff0c\u673a\u68b0\u81c2\u6a21\u62df\u5668\u4f18\u5316\u80fd\u8017\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u57ce\u5e02\u5e9f\u7269\u7ba1\u7406\u7684\u667a\u80fd\u5316\u9700\u6c42\uff0c\u7ed3\u5408\u53ef\u4fe1AI\u548c\u8282\u80fd\u6280\u672f\uff0c\u63d0\u5347\u5206\u7c7b\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u4f7f\u7528MobileNetV2\u589e\u5f3a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u7cfb\u7edf\u5c06\u5e9f\u7269\u51c6\u786e\u5206\u7c7b\u4e3a\u516d\u7c7b\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u673a\u68b0\u81c2\u6a21\u62df\u5668\u8fdb\u884c\u865a\u62df\u5206\u62e3\uff0c\u5e76\u901a\u8fc7\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8ba1\u7b97\u6bcf\u4e2a\u52a8\u4f5c\u7684\u80fd\u8017\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u8fbe\u523099.8%\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a80.5%\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002\u673a\u68b0\u81c2\u6a21\u62df\u5668\u786e\u4fdd\u4e86\u9ad8\u6548\u8282\u80fd\u7684\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\u4e0e\u8282\u80fd\u673a\u68b0\u81c2\u7684\u667a\u80fd\u5e9f\u7269\u5206\u7c7b\u548c\u5206\u62e3\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u57ce\u5e02\u667a\u80fd\u5e9f\u7269\u7ba1\u7406\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.", "AI": {"tldr": "Urban-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u57ce\u5e02\u667a\u80fd\u6a21\u578b\u7684\u5730\u7406\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u52a0\u5267\u4e86\u5bf9\u57ce\u5e02\u901a\u7528\u667a\u80fd(UGI)\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u76d1\u7763\u5fae\u8c03(SFT)\u6a21\u578b\u5b58\u5728\u5730\u7406\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u533a\u57df\u504f\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faUrban-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528Group Relative Policy Optimization (GRPO)\u4f18\u5316\u5730\u7406\u7fa4\u4f53\u95f4\u7684\u63a8\u7406\uff0c\u5e76\u5229\u7528\u57ce\u5e02\u533a\u57df\u5206\u6790\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u5956\u52b1\u3002", "result": "\u8de8\u533a\u57df\u548c\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUrban-R1\u6709\u6548\u7f13\u89e3\u5730\u7406\u504f\u89c1\u5e76\u63d0\u5347\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8eSFT\u8bad\u7ec3\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u662f\u5b9e\u73b0\u516c\u5e73\u53ef\u4fe1\u57ce\u5e02\u667a\u80fd\u7684\u6709\u524d\u666f\u9014\u5f84\u3002"}}
{"id": "2510.16438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16438", "abs": "https://arxiv.org/abs/2510.16438", "authors": ["Aidyn Ubingazhibov", "R\u00e9mi Pautrat", "Iago Su\u00e1rez", "Shaohui Liu", "Marc Pollefeys", "Viktor Larsson"], "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching", "comment": "Accepted at ICCVW 2025", "summary": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick.", "AI": {"tldr": "LightGlueStick\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u70b9\u7ebf\u5339\u914d\u5668\uff0c\u901a\u8fc7ALMP\u63d0\u5347\u6548\u7387\uff0c\u6210\u4e3a\u65b0\u7684SOTA\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u70b9\u7ebf\u5339\u914d\u72ec\u7acb\u5904\u7406\u4e14\u8ba1\u7b97\u590d\u6742\uff0cGlueStick\u867d\u8054\u5408\u5339\u914d\u4f46\u67b6\u6784\u7b28\u91cd\uff0c\u65e0\u6cd5\u5b9e\u65f6\u6216\u8fb9\u7f18\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u5339\u914d\u5668LightGlueStick\uff0c\u6838\u5fc3\u662f\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012\uff08ALMP\uff09\u7ec4\u4ef6\uff0c\u663e\u5f0f\u66b4\u9732\u7ebf\u6761\u8fde\u63a5\u6027\u3002", "result": "LightGlueStick\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "LightGlueStick\u901a\u8fc7ALMP\u7ec4\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u70b9\u7ebf\u5339\u914d\u7684\u6548\u7387\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.17439", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17439", "abs": "https://arxiv.org/abs/2510.17439", "authors": ["Zhengshen Zhang", "Hao Li", "Yalun Dai", "Zhengbang Zhu", "Lei Zhou", "Chenchen Liu", "Dong Wang", "Francis E. H. Tay", "Sijin Chen", "Ziwei Liu", "Yuxiao Liu", "Xinghang Li", "Pan Zhou"], "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors", "comment": "Project page: https://falcon-vla.github.io/", "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.", "AI": {"tldr": "FALCON\u901a\u8fc7\u6ce8\u51653D\u7a7a\u95f4\u6807\u8bb0\u548c\u589e\u5f3a\u52a8\u4f5c\u5934\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u6a21\u6001\u8fc1\u79fb\u95ee\u9898\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e2D\u7f16\u7801\u5668\u7684VLA\u6a21\u578b\u57283D\u771f\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u7a7a\u95f4\u63a8\u7406\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "method": "FALCON\u5229\u7528\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u4eceRGB\u4e2d\u63d0\u53d6\u5f3a\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u53ef\u9009\u7684Embodied Spatial Model\u4ee5\u878d\u5408\u6df1\u5ea6\u6216\u59ff\u6001\u4fe1\u606f\uff0c\u540c\u65f6\u901a\u8fc7Spatial-Enhanced Action Head\u4fdd\u6301\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u57fa\u51c6\u548c\u5341\u4e00\u4e2a\u73b0\u5b9e\u4efb\u52a1\u4e2d\uff0cFALCON\u8868\u73b0\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u5e76\u5728\u6742\u4e71\u3001\u7a7a\u95f4\u63d0\u793a\u6761\u4ef6\u548c\u5bf9\u8c61\u5c3a\u5ea6\u53d8\u5316\u7b49\u60c5\u51b5\u4e0b\u4fdd\u6301\u7a33\u5065\u3002", "conclusion": "FALCON\u901a\u8fc7\u5f15\u5165\u4e30\u5bcc\u76843D\u7a7a\u95f4\u6807\u8bb0\u5230\u52a8\u4f5c\u5934\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u6a21\u6001\u8fc1\u79fb\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16559", "abs": "https://arxiv.org/abs/2510.16559", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "comment": "33 pages, 10 figures", "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.", "AI": {"tldr": "BuildArena\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u5de5\u7a0b\u5efa\u8bbe\u80fd\u529b\u7684\u4ea4\u4e92\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u73b0\u4ee3LLMs\u5728\u5de5\u7a0b\u5efa\u8bbe\u81ea\u52a8\u5316\u9886\u57df\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u8bc4\u4f30\uff0c\u9700\u8981\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u590d\u6742\u96c6\u6210\u63a8\u7406\u3002", "method": "\u5f15\u5165BuildArena\uff0c\u5305\u62ec\u53ef\u5b9a\u5236\u7684\u57fa\u51c6\u6846\u67b6\u3001\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u8bbe\u8ba1\u7b56\u7565\u30013D\u7a7a\u95f4\u51e0\u4f55\u8ba1\u7b97\u5e93\u548c\u57fa\u7ebfLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u516b\u4e2a\u524d\u6cbfLLMs\u4e0a\u5168\u9762\u8bc4\u4f30\u4e86\u5176\u8bed\u8a00\u9a71\u52a8\u548c\u7269\u7406\u57fa\u7840\u7684\u5de5\u7a0b\u5efa\u8bbe\u81ea\u52a8\u5316\u80fd\u529b\u3002", "conclusion": "BuildArena\u662f\u9996\u4e2a\u9488\u5bf9\u8bed\u8a00\u9a71\u52a8\u5de5\u7a0b\u5efa\u8bbe\u7684\u7269\u7406\u5bf9\u9f50\u4ea4\u4e92\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86LLMs\u5728\u5de5\u7a0b\u5efa\u8bbe\u80fd\u529b\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.16442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16442", "abs": "https://arxiv.org/abs/2510.16442", "authors": ["Haoran Sun", "Chen Cai", "Huiping Zhuang", "Kong Aik Lee", "Lap-Pui Chau", "Yi Wang"], "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning", "comment": null, "summary": "The rapid development of deepfake video technology has not only facilitated\nartistic creation but also made it easier to spread misinformation. Traditional\ndeepfake video detection (DVD) methods face issues such as a lack of\ntransparency in their principles and insufficient generalization capabilities\nto cope with evolving forgery techniques. This highlights an urgent need for\ndetectors that can identify forged content and provide verifiable reasoning\nexplanations. This paper proposes the explainable deepfake video detection\n(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model\n(MLLM) reasoning framework, which provides traceable reasoning processes\nalongside accurate detection results and trustworthy explanations. Our approach\nfirst incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)\nto extract and fuse global and local cross-frame deepfake features, providing\nrich spatio-temporal semantic information input for MLLM reasoning. Second, we\nconstruct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which\nintroduces facial feature data as hard constraints during the reasoning process\nto achieve pixel-level spatio-temporal video localization, suppress\nhallucinated outputs, and enhance the reliability of the chain of thought. In\naddition, we build an Explainable Reasoning FF++ benchmark dataset\n(ER-FF++set), leveraging structured data to annotate videos and ensure quality\ncontrol, thereby supporting dual supervision for reasoning and detection.\nExtensive experiments demonstrate that EDVD-LLaMA achieves outstanding\nperformance and robustness in terms of detection accuracy, explainability, and\nits ability to handle cross-forgery methods and cross-dataset scenarios.\nCompared to previous DVD methods, it provides a more explainable and superior\nsolution. The source code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53ef\u89e3\u91ca\u6df1\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u4efb\u52a1 EDVD\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6 EDVD-LLaMA\uff0c\u901a\u8fc7 ST-SIT \u548c Fg-MCoT \u673a\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u4e0e\u53ef\u8ffd\u6eaf\u63a8\u7406\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e9f\u9700\u80fd\u591f\u8bc6\u522b\u4f2a\u9020\u5185\u5bb9\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u89e3\u91ca\u7684\u68c0\u6d4b\u5668\u3002", "method": "\u91c7\u7528 Spatio-Temporal Subtle Information Tokenization (ST-SIT) \u63d0\u53d6\u548c\u878d\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u7684\u8de8\u5e27\u6df1\u4f2a\u9020\u7279\u5f81\uff0c\u5e76\u6784\u5efa Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) \u673a\u5236\uff0c\u5f15\u5165\u9762\u90e8\u7279\u5f81\u6570\u636e\u4f5c\u4e3a\u786c\u7ea6\u675f\u4ee5\u5b9e\u73b0\u50cf\u7d20\u7ea7\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u3002", "result": "EDVD-LLaMA \u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u8de8\u4f2a\u9020\u65b9\u6cd5\u548c\u8de8\u6570\u636e\u96c6\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EDVD-LLaMA \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u89e3\u91ca\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u5e76\u89e3\u91ca\u6df1\u4f2a\u9020\u89c6\u9891\u5185\u5bb9\uff0c\u5176\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.17448", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.17448", "abs": "https://arxiv.org/abs/2510.17448", "authors": ["Mirko Mizzoni", "Pieter van Goor", "Barbara Bazzana", "Antonio Franchi"], "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions", "comment": null, "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5207\u6362\u53cd\u9988\u7ebf\u6027\u5316\u8f93\u51fa\u96c6\uff0c\u901a\u8fc7\u2018meld\u2019\u6982\u5ff5\u548c\u6b63\u5f0f\u8bc1\u660e\uff0c\u786e\u4fdd\u5207\u6362\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u65e0\u7f1d\u8ddf\u8e2a\u3002", "motivation": "\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u63a7\u5236\u901a\u5e38\u9700\u8981\u5904\u7406\u591a\u4e2a\u8f93\u51fa\u96c6\u4e4b\u95f4\u7684\u5207\u6362\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7406\u8bba\u652f\u6301\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u786e\u4fdd\u5728\u5207\u6362\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u5f15\u5165\u2018meld\u2019\u6982\u5ff5\u6765\u5b9a\u4e49\u6709\u6548\u7684\u53cd\u9988\u7ebf\u6027\u5316\u8f93\u51fa\u5b50\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6b63\u5f0f\u7684\u6570\u5b66\u8bc1\u660e\uff0c\u786e\u4fdd\u5728\u5207\u6362\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u72b6\u6001\u7684\u5747\u5300\u6709\u754c\u6027\u548c\u8bef\u5dee\u52a8\u6001\u7684\u6307\u6570\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u9002\u5f53\u7684\u505c\u7559\u65f6\u95f4\u548c\u517c\u5bb9\u6027\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u53ef\u4ee5\u5728\u4e0d\u540cmeld\u4e4b\u95f4\u5207\u6362\uff0c\u540c\u65f6\u4fdd\u8bc1\u72b6\u6001\u7684\u5747\u5300\u6709\u754c\u6027\u548c\u8bef\u5dee\u52a8\u6001\u7684\u6307\u6570\u7a33\u5b9a\u6027\u3002\u8fde\u7eedmeld\u5171\u6709\u7684\u8f93\u51fa\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u65e0\u7f1d\u8ddf\u8e2a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u53cd\u9988\u7ebf\u6027\u5316\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4e0d\u540c\u8f93\u51fa\u96c6\u4e4b\u95f4\u5207\u6362\u3002\u901a\u8fc7\u5f15\u5165\u2018meld\u2019\u6982\u5ff5\uff0c\u6b63\u5f0f\u5b9a\u4e49\u4e86\u53ef\u4ee5\u4ece\u66f4\u5927\u8f93\u51fa\u96c6\u5408\u4e2d\u9009\u62e9\u7684\u6709\u6548\u3001\u53ef\u53cd\u9988\u7ebf\u6027\u5316\u8f93\u51fa\u5b50\u96c6\u3002\u4e3b\u8981\u8d21\u732e\u662f\u6b63\u5f0f\u8bc1\u660e\uff0c\u5728\u9002\u5f53\u7684\u505c\u7559\u65f6\u95f4\u548c\u517c\u5bb9\u6027\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u5728\u4e0d\u540cmeld\u4e4b\u95f4\u5207\u6362\uff0c\u540c\u65f6\u4fdd\u8bc1\u7cfb\u7edf\u72b6\u6001\u7684\u5747\u5300\u6709\u754c\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u5728\u6bcf\u6b21\u5207\u6362\u95f4\u9694\u5185\uff0c\u6d3b\u52a8\u8f93\u51fa\u7684\u8bef\u5dee\u52a8\u6001\u4fdd\u6301\u6307\u6570\u7a33\u5b9a\uff0c\u4e14\u8fde\u7eedmeld\u5171\u6709\u7684\u8f93\u51fa\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u65e0\u7f1d\u8ddf\u8e2a\u3002\u8be5\u7406\u8bba\u9002\u7528\u4e8e\u4efb\u4f55\u53ef\u53cd\u9988\u7ebf\u6027\u5316\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5982\u673a\u5668\u4eba\u3001\u7a7a\u4e2d\u548c\u5730\u9762\u8f66\u8f86\u7b49\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u673a\u68b0\u81c2\u6570\u503c\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents", "AI": {"tldr": "REP\u534f\u8bae\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u5171\u4eab\u8f7b\u91cf\u7ea7\u654f\u611f\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u901a\u4fe1\u534f\u8baeA2A\u3002", "motivation": "\u73b0\u4ee3AI\u667a\u80fd\u4f53\u901a\u8fc7A2A\u548cACP\u7b49\u534f\u8bae\u4ea4\u6362\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u4fa7\u91cd\u4e8e\u901a\u4fe1\u800c\u975e\u534f\u8c03\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u7fa4\u4f53\u884c\u4e3a\u8106\u5f31\uff0c\u4e2a\u4f53\u667a\u80fd\u4f46\u7fa4\u4f53\u7ed3\u679c\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86Ripple Effect Protocol (REP)\uff0c\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u667a\u80fd\u4f53\u4e0d\u4ec5\u5171\u4eab\u51b3\u7b56\uff0c\u8fd8\u5171\u4eab\u8f7b\u91cf\u7ea7\u654f\u611f\u4fe1\u53f7\u3002\u8fd9\u4e9b\u4fe1\u53f7\u901a\u8fc7\u672c\u5730\u7f51\u7edc\u4f20\u64ad\uff0c\u5e2e\u52a9\u7fa4\u4f53\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u5730\u8fbe\u6210\u4e00\u81f4\u3002\u7814\u7a76\u8fd8\u5f62\u5f0f\u5316\u4e86REP\u7684\u534f\u8bae\u89c4\u8303\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u4e0d\u540c\u6fc0\u52b1\u548c\u7f51\u7edc\u62d3\u6251\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u4f9b\u5e94\u94fe\u7ea7\u8054\u3001\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u504f\u597d\u805a\u5408\u3001\u53ef\u6301\u7eed\u8d44\u6e90\u5206\u914d\uff09\uff0cREP\u6bd4A2A\u63d0\u9ad8\u4e8641%\u81f3100%\u7684\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u80fd\u7075\u6d3b\u5904\u7406\u6765\u81eaLLM\u7684\u591a\u6a21\u6001\u654f\u611f\u4fe1\u53f7\u3002", "conclusion": "REP\u534f\u8bae\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u654f\u611f\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.16444", "categories": ["cs.CV", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16444", "abs": "https://arxiv.org/abs/2510.16444", "authors": ["Kunyu Peng", "Di Wen", "Jia Fu", "Jiamin Wu", "Kailun Yang", "Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Yuqian Fu", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba", "comment": "Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and\n  code are released at https://github.com/KPeng9510/refAVA2", "summary": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.", "AI": {"tldr": "RAVAR\u4efb\u52a1\u901a\u8fc7RefAtomNet++\u6846\u67b6\u63d0\u5347\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\uff0cRefAVA++\u6570\u636e\u96c6\u6269\u5c55\u81f3290\u4e07\u5e27\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u548c\u76ee\u6807\u4eba\u7269\u5b9a\u4f4d\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faRefAtomNet++\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\uff0c\u52a8\u6001\u9009\u62e9\u89c6\u89c9\u7a7a\u95f4token\u8fdb\u884c\u8de8\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u3002", "result": "RefAtomNet++\u5728RefAVA++\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RefAtomNet++\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728RefAVA++\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2510.17525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17525", "abs": "https://arxiv.org/abs/2510.17525", "authors": ["Simon Schaefer", "Helen Oleynikova", "Sandra Hirche", "Stefan Leutenegger"], "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans", "comment": null, "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability.", "AI": {"tldr": "HumanMPC\u662f\u4e00\u4e2a\u7ed3\u5408\u5b89\u5168\u4fdd\u8bc1\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u76843D\u65e0\u4eba\u673a\u5bfc\u822a\u6846\u67b6\uff0c\u6709\u6548\u4e14\u5b89\u5168\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u7b80\u5316\u76842D\u4eba\u7fa4\u5bfc\u822a\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u4eba\u4f53\u52a8\u529b\u5b66\u7684\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\u4e0e\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u53ef\u8fbe\u6027\u7684\u5b89\u5168\u7ea6\u675f\u65b9\u6cd5\uff0c\u4ec5\u7ea6\u675f\u521d\u59cb\u63a7\u5236\u8f93\u5165\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u9a8c\u8bc1\u4e86HumanMPC\u7684\u6709\u6548\u6027\uff0c\u6db5\u76d6\u76ee\u6807\u5bfc\u822a\u548c\u89c6\u89c9\u4f3a\u670d\u8ddf\u8e2a\u7b49\u4efb\u52a1\u3002", "conclusion": "HumanMPC\u6846\u67b6\u5728\u786e\u4fdd\u5b89\u5168\u7684\u540c\u65f6\u907f\u514d\u4e86\u8fc7\u5ea6\u4fdd\u5b88\uff0c\u4e14\u5728\u6548\u7387\u548c\u53ef\u9760\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e73\u53f0\u3002"}}
{"id": "2510.16582", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16582", "abs": "https://arxiv.org/abs/2510.16582", "authors": ["Junchi Yu", "Yujie Liu", "Jindong Gu", "Philip Torr", "Dongzhan Zhou"], "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness.", "AI": {"tldr": "GraphFlow \u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684 RAG \u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76ee\u6807\u4f18\u5316\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684 RAG \u65b9\u6cd5\u96be\u4ee5\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u4e14\u591a\u6837\u5316\u7684\u4fe1\u606f\uff0c\u800c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\u7684\u8fc7\u7a0b\u7ea7\u76d1\u7763\u4fe1\u53f7\u3002", "method": "GraphFlow \u91c7\u7528\u57fa\u4e8e\u8f6c\u79fb\u7684\u6d41\u5339\u914d\u76ee\u6807\uff0c\u8054\u5408\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u548c\u6d41\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5956\u52b1\u5206\u89e3\u6307\u5bfc\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "GraphFlow \u5728 STaRK \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff08\u5305\u62ec GPT-4o\uff09\uff0c\u5e73\u5747\u547d\u4e2d\u7387\u548c\u53ec\u56de\u7387\u63d0\u5347 10%\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u77e5\u8bc6\u56fe\u8c31\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GraphFlow \u5728 STaRK \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u547d\u4e2d\u7387\u548c\u53ec\u56de\u7387\u6bd4\u57fa\u7ebf\u9ad8\u51fa 10%\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u77e5\u8bc6\u56fe\u8c31\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16445", "abs": "https://arxiv.org/abs/2510.16445", "authors": ["Chien Thai", "Mai Xuan Trang", "Huong Ninh", "Hoang Hiep Ly", "Anh Son Le"], "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance", "comment": "Neurocomputing", "summary": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u9ad8\u65af\u8868\u793a\u548cBhattacharyya\u8ddd\u79bb\u63d0\u5347\u65cb\u8f6c\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\u5728\u65cb\u8f6c\u7269\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u96be\u4ee5\u6355\u6349\u65b9\u5411\u53d8\u5316\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u9ad8\u65af\u8fb9\u754c\u6846\u8868\u793a\u548cBhattacharyya\u8ddd\u79bb\uff0c\u5e76\u7ed3\u5408\u5404\u5411\u5f02\u6027\u9ad8\u65af\u8868\u793a\u6765\u89e3\u51b3\u65b9\u5f62\u7269\u4f53\u5404\u5411\u540c\u6027\u65b9\u5dee\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6539\u8fdb\u635f\u5931\u51fd\u6570\u901a\u8fc7\u9ad8\u65af\u8fb9\u754c\u6846\u8868\u793a\u548cBhattacharyya\u8ddd\u79bb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65cb\u8f6c\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9700\u8981\u7cbe\u786e\u7269\u4f53\u5b9a\u4f4d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.17541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17541", "abs": "https://arxiv.org/abs/2510.17541", "authors": ["Xiaobo Zheng", "Pan Tang", "Defu Lin", "Shaoming He"], "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm", "comment": null, "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ADMM\u548cPDDP\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5D-PDDP\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7fa4\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u9700\u8981\u9884\u5148\u8bbe\u5b9a\u4ee3\u7406\u7684\u6700\u7ec8\u65f6\u95f4\u4e14\u8fed\u4ee3\u6b21\u6570\u591a\u3001\u8017\u65f6\u957f\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7fa4\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e24\u5c42\u67b6\u6784\uff1a\u4f7f\u7528PDDP\u4f5c\u4e3a\u5355\u4e2a\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u4f18\u5316\u5668\uff0cADMM\u7528\u4e8e\u6ee1\u8db3\u5c40\u90e8\u7ea6\u675f\u5e76\u5b9e\u73b0\u6240\u6709\u65e0\u4eba\u673a\u95f4\u7684\u65f6\u7a7a\u53c2\u6570\u5171\u8bc6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u8c31\u68af\u5ea6\u6cd5\u7684\u60e9\u7f5a\u53c2\u6570\u81ea\u9002\u5e94\u8c03\u6574\u51c6\u5219\u4ee5\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u63d0\u51fa\u7684D-PDDP\u7b97\u6cd5\u5728\u591a\u4e2a\u4eff\u771f\u4f8b\u5b50\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u5171\u8bc6\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD-PDDP\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408ADMM\u548cPDDP\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7fa4\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16601", "abs": "https://arxiv.org/abs/2510.16601", "authors": ["Tianxing Wu", "Shutong Zhu", "Jingting Wang", "Ning Xu", "Guilin Qi", "Haofen Wang"], "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning", "comment": "13 pages, accepted by NeurIPS 2025 (spotlight)", "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics.", "AI": {"tldr": "ssCDL\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u6539\u8fdbUKG\u8865\u5168\uff0c\u5229\u7528\u7f6e\u4fe1\u5206\u5e03\u548c\u5143\u5b66\u4e60\u589e\u5f3a\u6570\u636e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u5d4c\u5165\u5b66\u4e60\u5ffd\u89c6\u7f6e\u4fe1\u5ea6\u6781\u7aef\u4e0d\u5e73\u8861\u5206\u5e03\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8865\u5168\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u7f6e\u4fe1\u5206\u5e03\u5b66\u4e60\uff08ssCDL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u5206\u5e03\u4ee5\u5f15\u5165\u66f4\u591a\u76d1\u7763\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u5143\u5b66\u4e60\u9884\u6d4b\u4f2a\u6807\u7b7e\u6570\u636e\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2aUKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cssCDL\u5728\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ssCDL\u65b9\u6cd5\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.16446", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16446", "abs": "https://arxiv.org/abs/2510.16446", "authors": ["Jaekyun Park", "Hye Won Chung"], "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "comment": "NeurIPS 2025", "summary": "In the era of large-scale foundation models, fully fine-tuning pretrained\nnetworks for each downstream task is often prohibitively resource-intensive.\nPrompt tuning offers a lightweight alternative by introducing tunable prompts\nwhile keeping the backbone frozen. However, existing visual prompt tuning\nmethods often fail to specialize the prompts or enrich the representation\nspace--especially when applied to self-supervised backbones. We show that these\nlimitations become especially pronounced in challenging tasks and data-scarce\nsettings, where effective adaptation is most critical. In this work, we\nintroduce VIPAMIN, a visual prompt initialization strategy that enhances\nadaptation of self-supervised models by (1) aligning prompts with semantically\ninformative regions in the embedding space, and (2) injecting novel\nrepresentational directions beyond the pretrained subspace. Despite its\nsimplicity--requiring only a single forward pass and lightweight\noperations--VIPAMIN consistently improves performance across diverse tasks and\ndataset sizes, setting a new state of the art in visual prompt tuning. Our code\nis available at https://github.com/iamjaekyun/vipamin.", "AI": {"tldr": "VIPAMIN\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9\u63d0\u793a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u8868\u793a\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u81ea\u76d1\u7763\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u573a\u666f\u4e0b\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\uff0c\u5b8c\u5168\u5fae\u8c03\u9884\u8bad\u7ec3\u7f51\u7edc\u5bf9\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u6765\u8bf4\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3002\u73b0\u6709\u7684\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u81ea\u76d1\u7763\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u4e13\u4e1a\u5316\u63d0\u793a\u6216\u4e30\u5bcc\u8868\u793a\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u6311\u6218\u6027\u4efb\u52a1\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u3002", "method": "VIPAMIN\u662f\u4e00\u79cd\u89c6\u89c9\u63d0\u793a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\uff081\uff09\u5c06\u63d0\u793a\u4e0e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u533a\u57df\u5bf9\u9f50\uff0c\uff082\uff09\u5728\u9884\u8bad\u7ec3\u5b50\u7a7a\u95f4\u4e4b\u5916\u6ce8\u5165\u65b0\u7684\u8868\u793a\u65b9\u5411\uff0c\u6765\u589e\u5f3a\u81ea\u76d1\u7763\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "result": "VIPAMIN\u5728\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u89c4\u6a21\u4e0a\u5747\u80fd\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u6210\u4e3a\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u9886\u57df\u7684\u65b0\u6807\u6746\u3002", "conclusion": "VIPAMIN\u901a\u8fc7\u7b80\u5355\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u8f7b\u91cf\u7ea7\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7684\u6027\u80fd\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2510.17576", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17576", "abs": "https://arxiv.org/abs/2510.17576", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries", "comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review", "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u610f\u56fe\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u611f\u77e5\u3001LLM\u751f\u6210\u4e0e\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u534f\u4f5c\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u89c4\u5212\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u673a\u5668\u4eba\u5177\u6709\u4e0d\u540c\u7684\u672b\u7aef\u6267\u884c\u5668\u548c\u80fd\u529b\uff0c\u9700\u8981\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u7b80\u5355\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u534f\u4f5c\u3002", "method": "\u7ba1\u9053\u6574\u5408\u4e86\u611f\u77e5\u5230\u6587\u672c\u7684\u573a\u666f\u7f16\u7801\u3001\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5019\u9009\u79fb\u9664\u5e8f\u5217\u751f\u6210\u3001LLM\u9a8c\u8bc1\u5668\u5f3a\u5236\u6267\u884c\u683c\u5f0f\u548c\u4f18\u5148\u7ea7\u7ea6\u675f\uff0c\u4ee5\u53ca\u786e\u5b9a\u6027\u4e00\u81f4\u6027\u8fc7\u6ee4\u5668\u62d2\u7edd\u5e7b\u89c9\u5bf9\u8c61\u3002", "result": "\u5728200\u4e2a\u771f\u5b9e\u573a\u666f\u548c600\u4e2a\u64cd\u4f5c\u5458\u63d0\u793a\u7684\u8bc4\u4f30\u4e2d\uff0c\u4f7f\u7528\u5168\u5e8f\u5217\u6b63\u786e\u6027\u548c\u4e0b\u4e00\u4efb\u52a1\u6b63\u786e\u6027\u6307\u6807\uff0c\u9a8c\u8bc1\u4e86\u7ba1\u9053\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002LLM\u4eba\u673a\u754c\u9762\u5728\u6267\u884c\u65f6\u95f4\u548cNASA TLX\u65b9\u9762\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u610f\u56fe\u9a71\u52a8\u89c4\u5212\u7ba1\u9053\u80fd\u591f\u53ef\u9760\u5730\u5c06\u64cd\u4f5c\u5458\u610f\u56fe\u6620\u5c04\u4e3a\u5b89\u5168\u3001\u53ef\u6267\u884c\u7684\u591a\u673a\u5668\u4eba\u8ba1\u5212\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u7528\u6237\u52aa\u529b\u3002"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u65b0\u578bRL\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u589e\u5f3a\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f9d\u8d56\u4e8e\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u7684\u63a2\u7d22\uff0c\u5bfc\u81f4LLMs\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u91cd\u590d\u548c\u6b21\u4f18\u7684\u6a21\u5f0f\u3002", "method": "MERCI\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u7684Coin Flipping Network\uff08CFN\uff09\u6765\u4f30\u8ba1\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u4e0e\u4efb\u52a1\u5956\u52b1\u7ed3\u5408\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMERCI\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u4e86\u66f4\u4e30\u5bcc\u548c\u591a\u6837\u5316\u7684\u601d\u7ef4\u94fe\u3002", "conclusion": "MERCI\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e2e\u52a9\u6a21\u578b\u6446\u8131\u5c40\u90e8\u6700\u4f18\uff0c\u53d1\u73b0\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16450", "abs": "https://arxiv.org/abs/2510.16450", "authors": ["Shan Xiong", "Jiabao Chen", "Ye Wang", "Jialin Peng"], "title": "Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy", "comment": null, "summary": "Annotation-efficient segmentation of the numerous mitochondria instances from\nvarious electron microscopy (EM) images is highly valuable for biological and\nneuroscience research. Although unsupervised domain adaptation (UDA) methods\ncan help mitigate domain shifts and reduce the high costs of annotating each\ndomain, they typically have relatively low performance in practical\napplications. Thus, we investigate weakly supervised domain adaptation (WDA)\nthat utilizes additional sparse point labels on the target domain, which\nrequire minimal annotation effort and minimal expert knowledge. To take full\nuse of the incomplete and imprecise point annotations, we introduce a multitask\nlearning framework that jointly conducts segmentation and center detection with\na novel cross-teaching mechanism and class-focused cross-domain contrastive\nlearning. While leveraging unlabeled image regions is essential, we introduce\nsegmentation self-training with a novel instance-aware pseudo-label (IPL)\nselection strategy. Unlike existing methods that typically rely on pixel-wise\npseudo-label filtering, the IPL semantically selects reliable and diverse\npseudo-labels with the help of the detection task. Comprehensive validations\nand comparisons on challenging datasets demonstrate that our method outperforms\nexisting UDA and WDA methods, significantly narrowing the performance gap with\nthe supervised upper bound. Furthermore, under the UDA setting, our method also\nachieves substantial improvements over other UDA techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5b9e\u4f8b\u611f\u77e5\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\u7684\u5f31\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u7c92\u4f53\u5206\u5272\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4e0e\u76d1\u7763\u4e0a\u9650\u7684\u5dee\u8ddd\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u955c\uff08EM\uff09\u56fe\u50cf\u4e2d\u5927\u91cf\u7ebf\u7c92\u4f53\u5b9e\u4f8b\u7684\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5f31\u76d1\u7763\u57df\u9002\u5e94\uff08WDA\uff09\u4ee5\u5229\u7528\u76ee\u6807\u57df\u4e0a\u7684\u7a00\u758f\u70b9\u6807\u6ce8\uff0c\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u548c\u4e13\u5bb6\u77e5\u8bc6\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5272\u548c\u4e2d\u5fc3\u68c0\u6d4b\u4efb\u52a1\uff0c\u91c7\u7528\u4ea4\u53c9\u6559\u5b66\u673a\u5236\u548c\u7c7b\u805a\u7126\u8de8\u57df\u5bf9\u6bd4\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5b9e\u4f8b\u611f\u77e5\u4f2a\u6807\u7b7e\uff08IPL\uff09\u9009\u62e9\u7b56\u7565\u7684\u81ea\u8bad\u7ec3\u5206\u5272\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u548c\u6bd4\u8f83\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684UDA\u548cWDA\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u76d1\u7763\u4e0a\u9650\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5728UDA\u8bbe\u7f6e\u4e0b\uff0c\u4e5f\u5927\u5e45\u4f18\u4e8e\u5176\u4ed6UDA\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u57df\u9002\u5e94\uff08WDA\uff09\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u76d1\u7763\u4e0a\u9650\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2510.17604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17604", "abs": "https://arxiv.org/abs/2510.17604", "authors": ["Hao Qiao", "Yan Wang", "Shuo Yang", "Xiaoyao Yu", "Jian kuang", "Xiaoji Niu"], "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm", "comment": null, "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684MoE\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u81ea\u884c\u8f66\u5b9a\u4f4d\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u6210\u672c\uff0c\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u4f20\u7edf\u7684GNSS\u65b9\u6cd5\u548c\u60ef\u6027\u5bfc\u822a\u65b9\u6cd5\u5728\u81ea\u884c\u8f66\u5b9a\u4f4d\u4e2d\u5b58\u5728\u591a\u8def\u5f84\u6548\u5e94\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cTLIO\u867d\u80fd\u51cf\u5c11\u4f4d\u7f6e\u6f02\u79fb\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u5c06TLIO\u6269\u5c55\u5230\u81ea\u884c\u8f66\u5b9a\u4f4d\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684Mixture-of-Experts\uff08MoE\uff09\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u76f8\u6bd4LLIO\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u53c2\u6570\u51cf\u5c11\u4e8664.7%\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8681.8%\u3002", "conclusion": "\u6539\u8fdb\u7684Mixture-of-Experts\uff08MoE\uff09\u6a21\u578b\u5728\u81ea\u884c\u8f66\u5b9a\u4f4d\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u5927\u89c4\u6a21AI\u6a21\u578b\u6b63\u5728\u6539\u53d8\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7b49\u6311\u6218\uff0c\u5e76\u4fc3\u8fdb\u4e86\u4e34\u5e8a\u8f6c\u5316\uff0c\u4f46\u9700\u6ce8\u610f\u8bc4\u4f30\u6846\u67b6\u548c\u4f26\u7406\u6307\u5357\u3002", "motivation": "\u63a2\u8ba8\u5927\u89c4\u6a21AI\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4fc3\u8fdb\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u7684\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5e26\u6765\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u5305\u62ec\u795e\u7ecf\u5f71\u50cf\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u3001\u4e34\u5e8a\u8f85\u52a9\u53ca\u75be\u75c5\u7279\u5f02\u6027\u5e94\u7528\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7684\u4e3b\u8981\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u89c4\u6a21AI\u6a21\u578b\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u3001\u89e3\u91ca\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e34\u5e8a\u5e94\u7528\u7684\u8f6c\u5316\u6846\u67b6\uff0c\u540c\u65f6\u751f\u7269\u542f\u53d1\u7684\u67b6\u6784\u7ea6\u675f\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u96c6\u5217\u8868\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u65bd\u65f6\u9700\u6ce8\u610f\u7684\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u3001\u9886\u57df\u77e5\u8bc6\u6574\u5408\u53ca\u4f26\u7406\u6307\u5357\u3002"}}
{"id": "2510.16457", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16457", "abs": "https://arxiv.org/abs/2510.16457", "authors": ["Peiran Xu", "Xicheng Gong", "Yadong MU"], "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation", "comment": "ICCV 2025", "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eQ\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u672a\u6765\u7f16\u7801\u5668\u7684\u524d\u77bb\u6027VLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u4fe1\u606f\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5386\u53f2\u4fe1\u606f\u505a\u51fa\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u52a8\u4f5c\u7684\u672a\u6765\u5f71\u54cd\u548c\u957f\u671f\u7ed3\u679c\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5177\u6709\u524d\u77bb\u6027\u7684\u667a\u80fd\u4f53\u3002", "method": "\u5229\u7528Q\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2aQ\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u8f68\u8ff9\u6570\u636e\u5b66\u4e60\u5ba4\u5185\u573a\u666f\u5e03\u5c40\u548c\u7269\u4f53\u5173\u7cfb\u7684\u901a\u7528\u77e5\u8bc6\u3002\u8be5\u6a21\u578b\u751f\u6210\u7c7b\u4f3c\u4f20\u7edfQ\u7f51\u7edc\u4e2dQ\u503c\u7684Q\u7279\u5f81\uff0c\u63cf\u8ff0\u91c7\u53d6\u7279\u5b9a\u52a8\u4f5c\u540e\u53ef\u80fd\u89c2\u5bdf\u5230\u7684\u6f5c\u5728\u672a\u6765\u4fe1\u606f\u3002\u968f\u540e\uff0c\u8de8\u6a21\u6001\u672a\u6765\u7f16\u7801\u5668\u5c06\u8fd9\u4e9b\u4efb\u52a1\u65e0\u5173\u7684Q\u7279\u5f81\u4e0e\u5bfc\u822a\u6307\u4ee4\u7ed3\u5408\uff0c\u751f\u6210\u53cd\u6620\u672a\u6765\u524d\u666f\u7684\u52a8\u4f5c\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408Q\u7279\u5f81\u548c\u5bfc\u822a\u6307\u4ee4\u7684A*\u641c\u7d22\u7b56\u7565\u80fd\u6709\u6548\u63a2\u7d22\u66f4\u53ef\u80fd\u5bfc\u5411\u76ee\u7684\u5730\u7684\u533a\u57df\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u76ee\u6807\u5bfc\u5411VLN\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8868\u660e\u7ed3\u5408\u672a\u6765\u4fe1\u606f\u7684A*\u641c\u7d22\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2510.17640", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17640", "abs": "https://arxiv.org/abs/2510.17640", "authors": ["Yuquan Xue", "Guanxing Lu", "Zhenyu Wu", "Chuanrui Zhang", "Bofang Jia", "Zhengyi Gu", "Yansong Tang", "Ziwei Wang"], "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation", "comment": "9 pages,7 figures, submitted to ICRA2026", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models.", "AI": {"tldr": "RESample\u901a\u8fc7\u63a2\u7d22\u6027\u91c7\u6837\u589e\u5f3aVLA\u6a21\u578b\u5728OOD\u72b6\u6001\u4e0b\u7684\u6062\u590d\u80fd\u529b\uff0c\u63d0\u5347\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u96c6\u4ec5\u5305\u542b\u6210\u529f\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u5931\u8d25\u6216\u6062\u590d\u6570\u636e\uff0c\u5bfc\u81f4VLA\u6a21\u578b\u5728\u504f\u79bb\u8bad\u7ec3\u5206\u5e03\u7684OOD\u72b6\u6001\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u83b7\u53d6\u52a8\u4f5c\u4ef7\u503c\u7f51\u7edc\uff0c\u8bc6\u522b\u6b21\u4f18\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7rollout\u91c7\u6837\u6f5c\u5728OOD\u72b6\u6001\uff0c\u8bbe\u8ba1\u63a2\u7d22\u6027\u91c7\u6837\u673a\u5236\u5c06\u8fd9\u4e9b\u52a8\u4f5c\u4ee3\u7406\u7eb3\u5165\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u548c\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cRESample\u6301\u7eed\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RESample\u6846\u67b6\u901a\u8fc7\u63a2\u7d22\u6027\u91c7\u6837\u589e\u5f3a\u4e86VLA\u6a21\u578b\u5728OOD\u72b6\u6001\u4e0b\u7684\u6062\u590d\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "AI": {"tldr": "AFL\u6846\u67b6\u5229\u7528LLMs\u5b9e\u73b0\u4e86\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u5168\u81ea\u52a8\u5316\u89e3\u51b3\uff0c\u65e0\u9700\u5916\u90e8\u5e72\u9884\uff0c\u6027\u80fd\u63a5\u8fd1100%\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRPs\uff09\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u52aa\u529b\uff0c\u73b0\u6709LLMs\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5e72\u9884\uff0c\u5bfc\u81f4\u81ea\u4e3b\u6027\u53d7\u9650\u3001\u6267\u884c\u9519\u8bef\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLMs\u7684\u4ee3\u7406\u6846\u67b6\uff08AFL\uff09\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f93\u5165\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u5b9e\u73b0\u81ea\u5305\u542b\u7684\u4ee3\u7801\u751f\u6210\uff0c\u65e0\u9700\u624b\u5de5\u6a21\u5757\u6216\u5916\u90e8\u6c42\u89e3\u5668\u3002\u6846\u67b6\u5c06\u6574\u4f53\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u56db\u4e2a\u4e13\u4e1a\u4ee3\u7406\u4ee5\u786e\u4fdd\u8de8\u529f\u80fd\u4e00\u81f4\u6027\u548c\u903b\u8f91\u5408\u7406\u6027\u3002", "result": "\u572860\u4e2a\u590d\u6742VRPs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAFL\u6846\u67b6\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u3002", "conclusion": "AFL\u6846\u67b6\u901a\u8fc7LLMs\u5b9e\u73b0\u4e86\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u5168\u81ea\u52a8\u5316\u89e3\u51b3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\uff0c\u6027\u80fd\u63a5\u8fd1100%\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.16463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16463", "abs": "https://arxiv.org/abs/2510.16463", "authors": ["Haocheng Tang", "Ruoke Yan", "Xinhui Yin", "Qi Zhang", "Xinfeng Zhang", "Siwei Ma", "Wen Gao", "Chuanmin Jia"], "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars", "comment": "ACM International Conference on Multimedia 2025", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,\nphotorealistic rendering of dynamic 3D scenes, showing strong potential in\nimmersive communication. However, in digital human encoding and transmission,\nthe compression methods based on general 3DGS representations are limited by\nthe lack of human priors, resulting in suboptimal bitrate efficiency and\nreconstruction quality at the decoder side, which hinders their application in\nstreamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical\nGaussian Compression framework designed for efficient transmission and\nhigh-quality rendering of dynamic avatars. Our method disentangles the Gaussian\nrepresentation into a structural layer, which maps poses to Gaussians via a\nStyleUNet-based generator, and a motion layer, which leverages the SMPL-X model\nto represent temporal pose variations compactly and semantically. This\nhierarchical design supports layer-wise compression, progressive decoding, and\ncontrollable rendering from diverse pose inputs such as video sequences or\ntext. Since people are most concerned with facial realism, we incorporate a\nfacial attention mechanism during StyleUNet training to preserve identity and\nexpression details under low-bitrate constraints. Experimental results\ndemonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar\nrendering, while significantly outperforming prior methods in both visual\nquality and compression efficiency.", "AI": {"tldr": "HGC-Avatar\u901a\u8fc7\u5206\u5c42\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u7ed3\u5408StyleUNet\u548cSMPL-X\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f20\u8f93\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u52a8\u60013D\u5316\u8eab\uff0c\u7279\u522b\u5173\u6ce8\u9762\u90e8\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3DGS\u7684\u538b\u7f29\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6bd4\u7279\u7387\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5728\u53ef\u6d41\u5f0f3D\u5316\u8eab\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u9ad8\u65af\u8868\u793a\u5206\u89e3\u4e3a\u7ed3\u6784\u5c42\u548c\u8fd0\u52a8\u5c42\uff0c\u5206\u522b\u901a\u8fc7StyleUNet\u751f\u6210\u5668\u548cSMPL-X\u6a21\u578b\u5904\u7406\uff0c\u5e76\u5f15\u5165\u9762\u90e8\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u4fdd\u6301\u9762\u90e8\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGC-Avatar\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u538b\u7f29\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HGC-Avatar\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6d41\u5f0f\u4f20\u8f93\u7684\u52a8\u60013D\u5316\u8eab\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2510.17783", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17783", "abs": "https://arxiv.org/abs/2510.17783", "authors": ["Simeon Adebola", "Chung Min Kim", "Justin Kerr", "Shuangyu Xie", "Prithvi Akella", "Jose Luis Susa Rincon", "Eugen Solowjow", "Ken Goldberg"], "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.", "AI": {"tldr": "Botany-Bot\u5229\u7528\u591a\u786c\u4ef6\u548c\u7b97\u6cd5\u6784\u5efa\u690d\u7269\u6570\u5b57\u5b6a\u751f\uff0c\u89e3\u51b3\u4e86\u53f6\u7247\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u56fa\u5b9a\u76f8\u673a\u7cfb\u7edf\u56e0\u53f6\u7247\u906e\u6321\u65e0\u6cd5\u6355\u6349\u690d\u7269\u7ec6\u8282\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7acb\u4f53\u76f8\u673a\u3001\u6570\u5b57\u8f6c\u53f0\u3001\u5de5\u4e1a\u673a\u68b0\u81c2\u548c3D\u5206\u5272\u9ad8\u65af\u6a21\u578b\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u7b97\u6cd5\u64cd\u7eb5\u53f6\u7247\u4ee5\u62cd\u6444\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBotany-Bot\u5728\u53f6\u7247\u5206\u5272\u3001\u68c0\u6d4b\u3001\u64cd\u7eb5\u53ca\u62cd\u6444\u7ec6\u8282\u56fe\u50cf\u65b9\u9762\u5747\u8fbe\u5230\u8f83\u9ad8\u51c6\u786e\u7387\uff0890.8%\u300186.2%\u300177.9%\u300177.3%\uff09\u3002", "conclusion": "Botany-Bot\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u786c\u4ef6\u548c\u7b97\u6cd5\uff0c\u6210\u529f\u6784\u5efa\u4e86\u8be6\u7ec6\u7684\u690d\u7269\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u4ee3\u7406AI\u4ecePipeline-based\u7cfb\u7edf\u5230Model-native\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u5f3a\u8c03\u4e86RL\u5728\u7edf\u4e00LLM\u3001RL\u548c\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u80fd\u529b\u5185\u5316\u5bf9\u672a\u6765\u4ee3\u7406AI\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4ee3\u7406AI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u4ecePipeline-based\u7cfb\u7edf\u5230Model-native\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4ee5\u53caRL\u4f5c\u4e3a\u8fd9\u4e00\u8f6c\u53d8\u7684\u7b97\u6cd5\u5f15\u64ce\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u7b49\u80fd\u529b\u7684\u6f14\u53d8\uff0c\u4ece\u5916\u90e8\u811a\u672c\u6a21\u5757\u5230\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\uff0c\u4ee5\u53ca\u8003\u5bdf\u8fd9\u4e00\u8303\u5f0f\u8f6c\u53d8\u5982\u4f55\u91cd\u5851\u4e3b\u8981\u4ee3\u7406\u5e94\u7528\uff08\u5982Deep Research\u4ee3\u7406\u548cGUI\u4ee3\u7406\uff09\u3002", "result": "\u5c55\u793a\u4e86\u4ece\u5916\u90e8\u903b\u8f91\u5230\u6a21\u578b\u5185\u90e8\u5316\u7684\u80fd\u529b\u6f14\u53d8\uff0c\u4ee5\u53ca\u8fd9\u4e00\u8f6c\u53d8\u5982\u4f55\u5f71\u54cd\u4ee3\u7406\u5e94\u7528\u7684\u5f00\u53d1\uff0c\u5982\u5f3a\u8c03\u957f\u671f\u63a8\u7406\u7684Deep Research\u4ee3\u7406\u548c\u5f3a\u8c03\u5b9e\u4f53\u4ea4\u4e92\u7684GUI\u4ee3\u7406\u3002", "conclusion": "\u8bba\u6587\u6982\u8ff0\u4e86AI\u4ece\u5916\u90e8\u903b\u8f91\u7cfb\u7edf\u5230\u6a21\u578b\u539f\u751f\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u5f3a\u8c03\u4e86LLM\u4e0eRL\u7684\u7ed3\u5408\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u5b9e\u4f53\u9886\u57df\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\uff0c\u4ee3\u7406\u80fd\u529b\u7684\u5185\u5316\uff08\u5982\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u53cd\u601d\uff09\u5c06\u7ee7\u7eed\u53d1\u5c55\uff0c\u6807\u5fd7\u7740\u4ece\u6784\u5efa\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u5230\u5f00\u53d1\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\u7684\u8f6c\u53d8\u3002"}}
{"id": "2510.16505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16505", "abs": "https://arxiv.org/abs/2510.16505", "authors": ["Lukas Selch", "Yufang Hou", "M. Jehanzeb Mirza", "Sivan Doveh", "James Glass", "Rogerio Feris", "Wei Lin"], "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies", "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.", "AI": {"tldr": "PRISMM-Bench\u662f\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5ba1\u7a3f\u610f\u89c1\u7684\u591a\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u663e\u793a\u5f53\u524dLMM\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u79d1\u5b66\u8bba\u6587\u4e2d\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u901a\u5e38\u5fae\u5999\u4e14\u9886\u57df\u7279\u5b9a\uff0c\u5f71\u54cd\u6e05\u6670\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\uff08\u5ba1\u9605\u6316\u6398\u3001LLM\u8f85\u52a9\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\uff09\u4ece242\u7bc7\u8bba\u6587\u4e2d\u7b5b\u9009\u51fa262\u4e2a\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e09\u9879\u4efb\u52a1\uff08\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u4fee\u6b63\u548c\u914d\u5bf9\u5339\u914d\uff09\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "21\u4e2a\u9886\u5148LMM\u6a21\u578b\u7684\u6027\u80fd\u663e\u8457\u4f4e\u4e0b\uff0826.1-54.2%\uff09\uff0c\u7a81\u663e\u4e86\u591a\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u7684\u6311\u6218\u3002", "conclusion": "PRISMM-Bench\u662f\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u8bba\u6587\u4e2d\u5ba1\u7a3f\u4eba\u6307\u51fa\u7684\u4e0d\u4e00\u81f4\u6027\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4f4e\u6027\u80fd\uff0826.1-54.2%\uff09\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u53ef\u4fe1\u79d1\u5b66\u52a9\u624b\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2510.17792", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17792", "abs": "https://arxiv.org/abs/2510.17792", "authors": ["Gabriel B. Margolis", "Michelle Wang", "Nolan Fey", "Pulkit Agrawal"], "title": "SoftMimic: Learning Compliant Whole-body Control from Examples", "comment": "Website: https://gmargo11.github.io/softmimic/", "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment.", "AI": {"tldr": "SoftMimic\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u793a\u4f8b\u52a8\u4f5c\u4e2d\u5b66\u4e60\u67d4\u987a\u63a7\u5236\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5b89\u5168\u5e94\u5bf9\u610f\u5916\u63a5\u89e6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u503e\u5411\u4e8e\u521a\u6027\u63a7\u5236\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u9047\u5230\u610f\u5916\u63a5\u89e6\u65f6\u884c\u4e3a\u8106\u5f31\u4e14\u4e0d\u5b89\u5168\u3002", "method": "\u5229\u7528\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u751f\u6210\u589e\u5f3a\u7684\u67d4\u987a\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5956\u52b1\u7b56\u7565\u5339\u914d\u67d4\u987a\u54cd\u5e94\u800c\u975e\u521a\u6027\u8ddf\u8e2a\u53c2\u8003\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SoftMimic\u80fd\u591f\u5438\u6536\u5e72\u6270\u5e76\u4ece\u5355\u4e00\u52a8\u4f5c\u526a\u8f91\u6cdb\u5316\u5230\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "SoftMimic\u6846\u67b6\u901a\u8fc7\u4ece\u793a\u4f8b\u52a8\u4f5c\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u67d4\u987a\u5168\u8eab\u63a7\u5236\uff0c\u80fd\u591f\u5b89\u5168\u6709\u6548\u5730\u4e0e\u73af\u5883\u4e92\u52a8\u3002"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7406\u641c\u7d22\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u6311\u6218\uff0c\u65e8\u5728\u542f\u53d1\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f00\u653e\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e2d\u6539\u53d8\u4e86\u4fe1\u606f\u8bbf\u95ee\u548c\u63a8\u7406\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u9759\u6001\u77e5\u8bc6\u3001\u4e8b\u5b9e\u5e7b\u89c9\u548c\u65e0\u6cd5\u68c0\u7d22\u5b9e\u65f6\u6216\u7279\u5b9a\u9886\u57df\u4fe1\u606f\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5916\u90e8\u8bc1\u636e\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u4f20\u7edfRAG\u7ba1\u9053\u901a\u5e38\u662f\u5355\u8f6e\u548c\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u548c\u63a8\u7406\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "method": "\u7efc\u8ff0\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7\u65b0\u5174\u9886\u57df\uff1aRL\u7684\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u4f18\u5316\u8303\u56f4\u3002\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u57fa\u4e8eRL\u7684\u4ee3\u7406\u641c\u7d22\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u63d0\u4f9b\u4e86\u5bf9\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7406\u641c\u7d22\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u8ba8\u8bba\u4e86\u6784\u5efa\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7cfb\u7edf\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2510.16508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16508", "abs": "https://arxiv.org/abs/2510.16508", "authors": ["Franko \u0160iki\u0107", "Sven Lon\u010dari\u0107"], "title": "OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks", "comment": null, "summary": "Out-of-stock (OOS) detection is a very important retail verification process\nthat aims to infer the unavailability of products in their designated areas on\nthe shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based\nmethod that advances OOS detection through auxiliary learning. In particular,\nwe extend a well-established YOLOv8 object detection architecture with\nadditional convolutional branches to simultaneously detect OOS, segment\nproducts, and estimate scene depth. While OOS detection and product\nsegmentation branches are trained using ground truth data, the depth estimation\nbranch is trained using pseudo-labeled annotations produced by the\nstate-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,\nsince the aforementioned pseudo-labeled depth estimates display relative depth,\nwe propose an appropriate depth normalization procedure that stabilizes the\ntraining process. The experimental results show that the proposed method\nsurpassed the performance of the SOTA OOS detection methods by 1.8% of the mean\naverage precision (mAP). In addition, ablation studies confirm the\neffectiveness of auxiliary learning and the proposed depth normalization\nprocedure, with the former increasing mAP by 3.7% and the latter by 4.2%.", "AI": {"tldr": "OOS-DSD\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u4efb\u52a1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u5b66\u4e60\u548c\u6df1\u5ea6\u5f52\u4e00\u5316\u663e\u8457\u63d0\u5347\u7f3a\u8d27\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u96f6\u552e\u9a8c\u8bc1\u4e2d\u4ea7\u54c1\u7f3a\u8d27\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6269\u5c55YOLOv8\u67b6\u6784\uff0c\u589e\u52a0\u5377\u79ef\u5206\u652f\u4ee5\u540c\u65f6\u68c0\u6d4bOOS\u3001\u5206\u5272\u4ea7\u54c1\u548c\u4f30\u8ba1\u573a\u666f\u6df1\u5ea6\uff0c\u4f7f\u7528\u4f2a\u6807\u7b7e\u6df1\u5ea6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "OOS-DSD\u5728mAP\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e861.8%\uff0c\u8f85\u52a9\u5b66\u4e60\u548c\u6df1\u5ea6\u5f52\u4e00\u5316\u5206\u522b\u8d21\u732e\u4e863.7%\u548c4.2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "OOS-DSD\u901a\u8fc7\u8f85\u52a9\u5b66\u4e60\u548c\u6df1\u5ea6\u5f52\u4e00\u5316\u7a0b\u5e8f\u663e\u8457\u63d0\u5347\u4e86OOS\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2510.17801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17801", "abs": "https://arxiv.org/abs/2510.17801", "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain", "comment": null, "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io.", "AI": {"tldr": "RoboBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5177\u8eab\u5927\u8111\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u5f3a\u8c03\u6267\u884c\u6210\u529f\u6216\u5728\u9ad8\u5c42\u6b21\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e94\u4e2a\u7ef4\u5ea6\uff08\u6307\u4ee4\u7406\u89e3\u3001\u611f\u77e5\u63a8\u7406\u3001\u6cdb\u5316\u89c4\u5212\u3001\u529f\u80fd\u9884\u6d4b\u548c\u5931\u8d25\u5206\u6790\uff09\u548c14\u79cd\u80fd\u529b\u300125\u4e2a\u4efb\u52a1\u53ca6092\u4e2aQA\u5bf9\uff0cRoboBench\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86MLLMs\u4f5c\u4e3a\u5177\u8eab\u5927\u8111\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86MLLM-as-world-simulator\u6846\u67b6\u6765\u8bc4\u4f30\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e8614\u4e2aMLLMs\u5728\u9690\u5f0f\u6307\u4ee4\u7406\u89e3\u3001\u65f6\u7a7a\u63a8\u7406\u3001\u8de8\u573a\u666f\u89c4\u5212\u3001\u7ec6\u7c92\u5ea6\u529f\u80fd\u7406\u89e3\u548c\u6267\u884c\u5931\u8d25\u8bca\u65ad\u7b49\u65b9\u9762\u7684\u6839\u672c\u6027\u5c40\u9650\u3002", "conclusion": "RoboBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u91cf\u5316\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u6307\u5bfc\u4e0b\u4e00\u4ee3\u5177\u8eabMLLMs\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.16742", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16742", "abs": "https://arxiv.org/abs/2510.16742", "authors": ["Paul Saves", "Pramudita Satria Palar", "Muhammad Daffa Robani", "Nicolas Verstaevel", "Moncef Garouani", "Julien Aligon", "Benoit Gaudou", "Koji Shimoyama", "Joseph Morlier"], "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration", "comment": null, "summary": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u548cXAI\u7684\u5de5\u4f5c\u6d41\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u7cfb\u7edf\u4eff\u771f\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u9002\u7528\u4e8e\u5de5\u7a0b\u8bbe\u8ba1\u548c\u653f\u7b56\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u7cfb\u7edf\u4eff\u771f\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9ed1\u7bb1\u7ec4\u4ef6\u5e26\u6765\u7684\u900f\u660e\u5ea6\u4e0e\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6a21\u578b\u7684\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u7d27\u51d1\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\uff0c\u652f\u6301\u5168\u5c40\u548c\u5c40\u90e8\u7684XAI\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u89e3\u91ca\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u79d2\u7ea7\u7684\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u4ea4\u4e92\u548c\u6d8c\u73b0\u884c\u4e3a\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u548c\u653f\u7b56\u6760\u6746\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u548c\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u7cfb\u7edf\u4eff\u771f\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u5de5\u7a0b\u8bbe\u8ba1\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2510.16514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16514", "abs": "https://arxiv.org/abs/2510.16514", "authors": ["Duygu Sap", "Martin Lotz", "Connor Mattinson"], "title": "Image Categorization and Search via a GAT Autoencoder and Representative Models", "comment": "10 pages, 22 figures, Under review", "summary": "We propose a method for image categorization and retrieval that leverages\ngraphs and a graph attention network (GAT)-based autoencoder. Our approach is\nrepresentative-centric, that is, we execute the categorization and retrieval\nprocess via the representative models we construct for the images and image\ncategories. We utilize a graph where nodes represent images (or their\nrepresentatives) and edges capture similarity relationships. GAT highlights\nimportant features and relationships between images, enabling the autoencoder\nto construct context-aware latent representations that capture the key features\nof each image relative to its neighbors. We obtain category representatives\nfrom these embeddings and categorize a query image by comparing its\nrepresentative to the category representatives. We then retrieve the most\nsimilar image to the query image within its identified category. We demonstrate\nthe effectiveness of our representative-centric approach through experiments\nwith both the GAT autoencoders and standard feature-based techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAT\u81ea\u7f16\u7801\u5668\u7684\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u4ee3\u8868\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\u4e0e\u68c0\u7d22\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u4ee3\u8868\u6a21\u578b\u5b9e\u73b0\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u7d22\uff0c\u4ee5\u6355\u6349\u56fe\u50cf\u95f4\u7684\u5173\u952e\u7279\u5f81\u4e0e\u76f8\u4f3c\u6027\u3002", "method": "\u5229\u7528\u56fe\u7ed3\u6784\u8868\u793a\u56fe\u50cf\u53ca\u5176\u76f8\u4f3c\u6027\u5173\u7cfb\uff0c\u901a\u8fc7GAT\u81ea\u7f16\u7801\u5668\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u751f\u6210\u7c7b\u522b\u4ee3\u8868\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u4e0e\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u7279\u5f81\u7684\u6280\u672f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u81ea\u7f16\u7801\u5668\u7684\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "ELMM \u662f\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u5316\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u538b\u7f29\u56fe\u50cf\u4ee4\u724c\u548c\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MKGs\uff09\u7684\u4e0d\u5b8c\u6574\u6027\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\u9762\u4e34\u8bed\u4e49\u566a\u58f0\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\u3002", "method": "ELMM \u5f15\u5165\u4e86\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u89c6\u56fe\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\uff08MVTC\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728 FB15k-237-IMG \u548c WN18-IMG \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELMM \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ELMM \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u5316\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.16540", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16540", "abs": "https://arxiv.org/abs/2510.16540", "authors": ["Jihoon Kwon", "Kyle Min", "Jy-yong Sohn"], "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "comment": "Accepted at NeurIPS 2025 (poster). This is the camera-ready version", "summary": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.", "AI": {"tldr": "READ\u65b9\u6cd5\u901a\u8fc7\u91cd\u5efa\u548c\u5bf9\u9f50\u76ee\u6807\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0cREAD-CLIP\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6587\u672c\u7f16\u7801\u5668\u503e\u5411\u4e8e\u5173\u6ce8\u5355\u4e2a\u5355\u8bcd\u800c\u975e\u5176\u5173\u7cfb\u3002", "method": "READ\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\u6765\u589e\u5f3a\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff1a\uff081\uff09\u4ee4\u724c\u7ea7\u91cd\u5efa\u76ee\u6807\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u91cd\u5efa\u66ff\u4ee3\u6807\u9898\uff1b\uff082\uff09\u53e5\u5b50\u7ea7\u5bf9\u9f50\u76ee\u6807\uff0c\u660e\u786e\u5bf9\u9f50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8f6c\u8ff0\u53e5\u5b50\u3002", "result": "READ-CLIP\u5728\u4e94\u4e2a\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14READ\u65b9\u6cd5\u5bf9\u73b0\u6709CLIP\u53d8\u4f53\u4e5f\u6709\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "READ-CLIP\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u7684\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u4f20\u7edf\u5fae\u8c03\u57fa\u7ebf\u63d0\u9ad8\u4e864.1%\u3002\u6b64\u5916\uff0cREAD\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u73b0\u6709\u7684CLIP\u53d8\u4f53\uff08\u5982NegCLIP\u548cFSC-CLIP\uff09\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "ELLSA\u662f\u9996\u4e2a\u5168\u53cc\u5de5\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7SA-MoE\u67b6\u6784\u5b9e\u73b0\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u751f\u6210\uff0c\u652f\u6301\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u884c\u4e3a\u3002", "motivation": "\u4eba\u7c7b\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u7684\uff0c\u6a21\u62df\u8fd9\u4e9b\u80fd\u529b\u5bf9\u4e8e\u6784\u5efa\u4eba\u7c7b\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528SA-MoE\u67b6\u6784\uff08\u81ea\u6ce8\u610f\u529b\u6df7\u5408\u4e13\u5bb6\uff09\uff0c\u5c06\u6bcf\u79cd\u6a21\u6001\u8def\u7531\u5230\u4e13\u95e8\u4e13\u5bb6\u5e76\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u9aa8\u5e72\u878d\u5408\u3002", "result": "\u5728\u8bed\u97f3\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELLSA\u4e0e\u7279\u5b9a\u6a21\u6001\u57fa\u7ebf\u76f8\u5339\u914d\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u884c\u4e3a\u3002", "conclusion": "ELLSA\u6807\u5fd7\u7740\u5411\u66f4\u81ea\u7136\u548c\u901a\u7528\u4ea4\u4e92\u667a\u80fd\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u4e3a\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u5e7f\u6cdb\u8ffd\u6c42\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.16541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16541", "abs": "https://arxiv.org/abs/2510.16541", "authors": ["Binyuan Huang", "Yongdong Luo", "Xianda Guo", "Xiawu Zheng", "Zheng Zhu", "Jiahui Pan", "Chengju Zhou"], "title": "Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition", "comment": null, "summary": "Deep learning-based gait recognition has achieved great success in various\napplications. The key to accurate gait recognition lies in considering the\nunique and diverse behavior patterns in different motion regions, especially\nwhen covariates affect visual appearance. However, existing methods typically\nuse predefined regions for temporal modeling, with fixed or equivalent temporal\nscales assigned to different types of regions, which makes it difficult to\nmodel motion regions that change dynamically over time and adapt to their\nspecific patterns. To tackle this problem, we introduce a Region-aware Dynamic\nAggregation and Excitation framework (GaitRDAE) that automatically searches for\nmotion regions, assigns adaptive temporal scales and applies corresponding\nattention. Specifically, the framework includes two core modules: the\nRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches the\noptimal temporal receptive field for each region, and the Region-aware Dynamic\nExcitation (RDE) module, which emphasizes the learning of motion regions\ncontaining more stable behavior patterns while suppressing attention to static\nregions that are more susceptible to covariates. Experimental results show that\nGaitRDAE achieves state-of-the-art performance on several benchmark datasets.", "AI": {"tldr": "GaitRDAE\u901a\u8fc7\u52a8\u6001\u533a\u57df\u611f\u77e5\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5c3a\u5ea6\u5206\u914d\uff0c\u63d0\u5347\u4e86\u6b65\u6001\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9884\u5b9a\u4e49\u533a\u57df\u548c\u56fa\u5b9a\u65f6\u95f4\u5c3a\u5ea6\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u8fd0\u52a8\u533a\u57df\u548c\u7279\u5b9a\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u4e86Region-aware Dynamic Aggregation\u548cExcitation\u6a21\u5757\uff0c\u52a8\u6001\u641c\u7d22\u6700\u4f18\u65f6\u95f4\u611f\u53d7\u91ce\u5e76\u5e94\u7528\u76f8\u5e94\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GaitRDAE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5c3a\u5ea6\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b65\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u52a8\u6001\u53d8\u5316\u7684\u8fd0\u52a8\u533a\u57df\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista\u901a\u8fc7\u5206\u5c42\u4fe1\u606f\u7ec4\u7ec7\u548c\u6a21\u6001\u89c4\u5212\u4ee3\u7406\uff0c\u63d0\u5347\u56fe\u7406\u89e3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u56e0\u8f93\u5165\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\u548c\u6a21\u6001\u534f\u8c03\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "GraphVista\u91c7\u7528\u8f7b\u91cf\u7ea7GraphRAG\u57fa\u7840\u7ed3\u6784\uff0c\u6309\u9700\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u5b50\u56fe\uff0c\u5e76\u901a\u8fc7\u89c4\u5212\u4ee3\u7406\u52a8\u6001\u8def\u7531\u4efb\u52a1\u81f3\u6700\u9002\u5408\u7684\u6a21\u6001\u3002", "result": "GraphVista\u53ef\u6269\u5c55\u5230\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u56fe\uff0c\u5e76\u5728\u8d28\u91cf\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u53474.4\u500d\u3002", "conclusion": "GraphVista\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u56fe\u4fe1\u606f\u548c\u5f15\u5165\u89c4\u5212\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u7406\u89e3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5927\u578b\u56fe\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.16556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16556", "abs": "https://arxiv.org/abs/2510.16556", "authors": ["Guangyu Lin", "Li Lin", "Christina P. Walker", "Daniel S. Schiff", "Shu Hu"], "title": "Fit for Purpose? Deepfake Detection in the Real World", "comment": null, "summary": "The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u5e93\u8bc4\u4f30\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\uff0c\u53d1\u73b0\u5176\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5177\u653f\u6cbb\u80cc\u666f\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u591a\u6570\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u6cdb\u5316\u81f3\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u4f20\u64ad\u7684\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u4e9f\u9700\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4e8b\u4ef6\u6570\u636e\u5e93\uff08Political Deepfakes Incident Database\uff09\u6784\u5efa\u7cfb\u7edf\u5316\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u5b66\u672f\u754c\u3001\u653f\u5e9c\u53ca\u5de5\u4e1a\u754c\u7684\u6700\u5148\u8fdb\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5de5\u5177\u3002", "result": "\u5b66\u672f\u754c\u548c\u653f\u5e9c\u5f00\u53d1\u7684\u68c0\u6d4b\u5668\u8868\u73b0\u8f83\u5dee\uff0c\u4ed8\u8d39\u5de5\u5177\u6027\u80fd\u76f8\u5bf9\u8f83\u9ad8\u4f46\u6240\u6709\u68c0\u6d4b\u5668\u5747\u96be\u4ee5\u6709\u6548\u6cdb\u5316\u81f3\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u4e14\u6613\u53d7\u7b80\u5355\u89c6\u9891\u64cd\u4f5c\u5f71\u54cd\u3002", "conclusion": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5de5\u5177\u5728\u771f\u5b9e\u4e16\u754c\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u5177\u653f\u6cbb\u80cc\u666f\u7684\u68c0\u6d4b\u6846\u67b6\u4ee5\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u516c\u4f17\u4fdd\u62a4\u3002"}}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.", "AI": {"tldr": "CDC\u662f\u4e00\u79cd\u65b0\u578b\u77e5\u8bc6\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9886\u57df\u5206\u7c7b\u548cC-D-C\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u7c7b\u6bd4\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u53d7\u9650\u4e8e\u56fa\u5b9a\u672c\u4f53\uff0c\u65e0\u6cd5\u52a8\u6001\u5904\u7406\u9886\u57df\u4e0a\u4e0b\u6587\u3002CDC\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9886\u57df\uff0c\u63d0\u5347\u77e5\u8bc6\u8868\u793a\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "CDC\u91c7\u7528C-D-C\u4e09\u5143\u7ec4\u7ed3\u6784\uff08<\u6982\u5ff5, \u5173\u7cfb@\u9886\u57df, \u6982\u5ff5'>\uff09\uff0c\u5e76\u57fa\u4e8e\u8ba4\u77e5-\u8bed\u8a00\u540c\u6784\u6620\u5c04\u539f\u5219\uff0c\u5b9a\u4e49\u4e8620\u591a\u79cd\u6807\u51c6\u5316\u5173\u7cfb\u8c13\u8bcd\uff0c\u5e76\u5728Prolog\u4e2d\u5b9e\u73b0\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cCDC\u5728\u6559\u80b2\u3001\u4f01\u4e1a\u77e5\u8bc6\u7cfb\u7edf\u548c\u6280\u672f\u6587\u6863\u7b49\u9886\u57df\u4e2d\u5b9e\u73b0\u4e86\u4f20\u7edf\u6846\u67b6\u65e0\u6cd5\u8fbe\u5230\u7684\u80fd\u529b\u3002", "conclusion": "CDC\u6846\u67b6\u901a\u8fc7\u5c06\u9886\u57df\u63d0\u5347\u4e3a\u6982\u5ff5\u8868\u793a\u7684\u4e00\u6d41\u5143\u7d20\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u8de8\u9886\u57df\u7c7b\u6bd4\u548c\u4e2a\u6027\u5316\u77e5\u8bc6\u5efa\u6a21\u3002"}}
{"id": "2510.16596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16596", "abs": "https://arxiv.org/abs/2510.16596", "authors": ["Yiyang Huang", "Liang Shi", "Yitian Zhang", "Yi Xu", "Yun Fu"], "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense", "comment": null, "summary": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.", "AI": {"tldr": "SHIELD\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u89e3\u51b3LVLM\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff08\u6a21\u578b\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e0d\u51c6\u786e\u7684\u7269\u4f53\u63cf\u8ff0\uff09\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002\u672c\u6587\u9996\u6b21\u5c06LVLM\u5e7b\u89c9\u8ffd\u6eaf\u5230\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u8bc6\u522b\u51fa\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7edf\u8ba1\u504f\u5dee\u3001\u56fa\u6709\u504f\u5dee\u548c\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faSHIELD\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\uff1a\u91cd\u65b0\u52a0\u6743\u89c6\u89c9\u6807\u8bb0\u4ee5\u51cf\u5c11\u7edf\u8ba1\u504f\u5dee\u3001\u5f15\u5165\u566a\u58f0\u884d\u751f\u6807\u8bb0\u4ee5\u5bf9\u6297\u56fa\u6709\u504f\u5dee\u3001\u5e94\u7528\u5bf9\u6297\u6027\u653b\u51fb\u4e0e\u5bf9\u6bd4\u89e3\u7801\u4ee5\u89e3\u51b3\u8106\u5f31\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSHIELD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548cLVLM\u5bb6\u65cf\u4e2d\u6709\u6548\u7f13\u89e3\u7269\u4f53\u5e7b\u89c9\uff0c\u5e76\u5728\u901a\u7528LVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SHIELD\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86LVLMs\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u901a\u7528LVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.16624", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16624", "abs": "https://arxiv.org/abs/2510.16624", "authors": ["Sebastian Mocanu", "Emil Slusanschi", "Marius Leordeanu"], "title": "Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs", "comment": null, "summary": "This paper presents a vision-only autonomous flight system for small UAVs\noperating in controlled indoor environments. The system combines semantic\nsegmentation with monocular depth estimation to enable obstacle avoidance,\nscene exploration, and autonomous safe landing operations without requiring GPS\nor expensive sensors such as LiDAR. A key innovation is an adaptive scale\nfactor algorithm that converts non-metric monocular depth predictions into\naccurate metric distance measurements by leveraging semantic ground plane\ndetection and camera intrinsic parameters, achieving a mean distance error of\n14.4 cm. The approach uses a knowledge distillation framework where a\ncolor-based Support Vector Machine (SVM) teacher generates training data for a\nlightweight U-Net student network (1.6M parameters) capable of real-time\nsemantic segmentation. For more complex environments, the SVM teacher can be\nreplaced with a state-of-the-art segmentation model. Testing was conducted in a\ncontrolled 5x4 meter laboratory environment with eight cardboard obstacles\nsimulating urban structures. Extensive validation across 30 flight tests in a\nreal-world environment and 100 flight tests in a digital-twin environment\ndemonstrates that the combined segmentation and depth approach increases the\ndistance traveled during surveillance and reduces mission time while\nmaintaining 100% success rates. The system is further optimized through\nend-to-end learning, where a compact student neural network learns complete\nflight policies from demonstration data generated by our best-performing\nmethod, achieving an 87.5% autonomous mission success rate. This work advances\npractical vision-based drone navigation in structured environments,\ndemonstrating solutions for metric depth estimation and computational\nefficiency challenges that enable deployment on resource-constrained platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5c0f\u578b\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u969c\u788d\u7269\u907f\u969c\u548c\u81ea\u4e3b\u964d\u843d\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u6d4b\u8bd5\u663e\u793a\u9ad8\u6548\u4e14\u53ef\u9760\u3002", "motivation": "\u9488\u5bf9\u5c0f\u578b\u65e0\u4eba\u673a\u5728\u53d7\u63a7\u5ba4\u5185\u73af\u5883\u4e2d\u65e0\u9700GPS\u6216\u6602\u8d35\u4f20\u611f\u5668\uff08\u5982LiDAR\uff09\u7684\u81ea\u4e3b\u98de\u884c\u9700\u6c42\uff0c\u5f00\u53d1\u4e00\u79cd\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u989c\u8272\u7684SVM\u6559\u5e08\u7f51\u7edc\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7U-Net\u5b66\u751f\u7f51\u7edc\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6bd4\u4f8b\u56e0\u5b50\u7b97\u6cd5\u5c06\u975e\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u7cbe\u786e\u5ea6\u91cf\u8ddd\u79bb\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u548c\u6570\u5b57\u5b6a\u751f\u73af\u5883\u4e2d\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u4fdd\u6301100%\u6210\u529f\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u52a0\u4e86\u76d1\u63a7\u8ddd\u79bb\u5e76\u51cf\u5c11\u4e86\u4efb\u52a1\u65f6\u95f4\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e8687.5%\u7684\u81ea\u4e3b\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u9ad8\u6548\u89c6\u89c9\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6df1\u5ea6\u4f30\u8ba1\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218\u3002"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u4ee3\u7406LLM\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u5b9e\u73b0\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "motivation": "\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\uff0c\u514b\u670d\u73b0\u6709\u57fa\u4e8e\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u4ee3\u7406\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u4ee3\u7406\u8bad\u7ec3\u8303\u5f0f\u548c\u6570\u636e\u9a71\u52a8\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec58B\u53c2\u6570\u7684DeepAnalyze\u5728\u591a\u79cd\u6570\u636e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze-8B\u7684\u5f00\u6e90\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u4e3a\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u6570\u636e\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.16598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16598", "abs": "https://arxiv.org/abs/2510.16598", "authors": ["Jiaying Zhu", "Yurui Zhu", "Xin Lu", "Wenrui Yan", "Dong Li", "Kunlin Liu", "Xueyang Fu", "Zheng-Jun Zha"], "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs", "comment": "22 pages, 8 figures", "summary": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .", "AI": {"tldr": "VisionSelector\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4ee4\u724c\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u673a\u5236\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u591a\u56fe\u50cf\u8f93\u5165\u4ea7\u751f\u7684\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u538b\u7f29\u6280\u672f\u56e0\u542f\u53d1\u5f0f\u89c4\u5219\u53ef\u80fd\u4e22\u5f03\u5173\u952e\u4fe1\u606f\u6216\u5b58\u5728\u504f\u89c1\u7684\u5c40\u9650\u3002", "method": "\u63d0\u51faVisionSelector\uff0c\u4e00\u4e2a\u4e0eMLLM\u4e3b\u5e72\u89e3\u8026\u7684\u8bc4\u5206\u6a21\u5757\uff0c\u7ed3\u5408\u53ef\u5fae\u5206Top-K\u673a\u5236\u548c\u8bfe\u7a0b\u9000\u706b\u7b56\u7565\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53ef\u5b66\u4e60\u7684\u4ee4\u724c\u538b\u7f29\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "VisionSelector\u5728\u591a\u79cd\u538b\u7f29\u7387\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5982\u572830%\u4fdd\u7559\u9884\u7b97\u4e0b\u4fdd\u6301MME 100%\u51c6\u786e\u7387\uff0c10%\u4fdd\u7559\u9884\u7b97\u4e0b\u4f18\u4e8e\u5148\u524d\u65b9\u6cd512.14%\uff0c\u5e76\u5b9e\u73b0\u9884\u586b\u5145\u901f\u5ea6\u7ffb\u500d\u3002", "conclusion": "VisionSelector\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206Top-K\u673a\u5236\u548c\u8bfe\u7a0b\u9000\u706b\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.16643", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.9; I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16643", "abs": "https://arxiv.org/abs/2510.16643", "authors": ["Aaron Ray", "Jacob Arkin", "Harel Biggie", "Chuchu Fan", "Luca Carlone", "Nicholas Roy"], "title": "Structured Interfaces for Automated Reasoning with 3D Scene Graphs", "comment": "25 pages, 3 figures", "summary": "In order to provide a robot with the ability to understand and react to a\nuser's natural language inputs, the natural language must be connected to the\nrobot's underlying representations of the world. Recently, large language\nmodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for\ngrounding natural language and representing the world. In this work, we address\nthe challenge of using LLMs with 3DSGs to ground natural language. Existing\nmethods encode the scene graph as serialized text within the LLM's context\nwindow, but this encoding does not scale to large or rich 3DSGs. Instead, we\npropose to use a form of Retrieval Augmented Generation to select a subset of\nthe 3DSG relevant to the task. We encode a 3DSG in a graph database and provide\na query language interface (Cypher) as a tool to the LLM with which it can\nretrieve relevant data for language grounding. We evaluate our approach on\ninstruction following and scene question-answering tasks and compare against\nbaseline context window and code generation methods. Our results show that\nusing Cypher as an interface to 3D scene graphs scales significantly better to\nlarge, rich graphs on both local and cloud-based models. This leads to large\nperformance improvements in grounded language tasks while also substantially\nreducing the token count of the scene graph content. A video supplement is\navailable at https://www.youtube.com/watch?v=zY_YI9giZSA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7Cypher\u67e5\u8be2\u8bed\u8a00\u63a5\u53e3\u8fde\u63a5LLM\u4e0e3D\u573a\u666f\u56fe\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u63a5\u5730\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u578b\u6216\u4e30\u5bcc\u76843D\u573a\u666f\u56fe\u4e2d\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u4e0e\u673a\u5668\u4eba\u7684\u4e16\u754c\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Retrieval Augmented Generation\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u6570\u636e\u5e93\u7f16\u78013D\u573a\u666f\u56fe\uff0c\u5e76\u5229\u7528\u67e5\u8be2\u8bed\u8a00\u63a5\u53e3\uff08Cypher\uff09\u4f5c\u4e3aLLM\u7684\u5de5\u5177\uff0c\u4ee5\u68c0\u7d22\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u3002", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u548c\u573a\u666f\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u4e0e\u57fa\u7ebf\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528Cypher\u63a5\u53e3\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u4ee4\u724c\u8ba1\u6570\u3002", "conclusion": "\u4f7f\u7528Cypher\u4f5c\u4e3a3D\u573a\u666f\u56fe\u7684\u63a5\u53e3\uff0c\u5728\u5927\u578b\u3001\u4e30\u5bcc\u7684\u56fe\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u573a\u666f\u56fe\u5185\u5bb9\u7684\u4ee4\u724c\u8ba1\u6570\uff0c\u4ece\u800c\u5728\u8bed\u8a00\u63a5\u5730\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5927\u5e45\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4e16\u754c\u5efa\u6a21\u5956\u52b1\uff0c\u4f7f3B\u53c2\u6570\u7684VLM\u4ee3\u7406\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u672a\u8bad\u7ec3\u6a21\u578b\u548c\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee3\u7406\u662f\u5426\u80fd\u591f\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u4ece\u6587\u672c\u72b6\u6001\u5230\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\u7684\u8f6c\u6362\u5e26\u6765\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u548c\u4e16\u754c\u5efa\u6a21\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u548c\u53cc\u5c42\u6b21\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1\uff08Bi-Level GAE\uff09\u8fdb\u884c\u5bc6\u96c6\u76d1\u7763\u548c\u4fe1\u7528\u5206\u914d\u3002", "result": "3B\u53c2\u6570\u7684VLM\u6a21\u578b\u5728\u4e94\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e860.82\u7684\u5206\u6570\uff0c\u6bd4\u672a\u8bad\u7ec3\u7684\u6a21\u578b\uff080.21\uff09\u63d0\u9ad8\u4e863\u500d\uff0c\u5e76\u8d85\u8d8a\u4e86\u4e13\u6709\u63a8\u7406\u6a21\u578b\uff08\u5982GPT-5\u3001Gemini 2.5 Pro\u548cClaude 4.5\uff09\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u67b6\u6784\u548c\u4e16\u754c\u5efa\u6a21\u5956\u52b1\uff0c3B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e94\u4e2a\u591a\u6837\u5316\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e860.82\u7684\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u672a\u8bad\u7ec3\u7684\u6a21\u578b\uff080.21\uff09\u548c\u4e13\u6709\u63a8\u7406\u6a21\u578b\uff08\u5982GPT-5\u3001Gemini 2.5 Pro\u548cClaude 4.5\uff09\u3002"}}
{"id": "2510.16611", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16611", "abs": "https://arxiv.org/abs/2510.16611", "authors": ["Melika Filvantorkaman", "Maral Filvan Torkaman"], "title": "A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications", "comment": "20 pages, 4 figures", "summary": "Medical imaging plays a vital role in modern diagnostics; however,\ninterpreting high-resolution radiological data remains time-consuming and\nsusceptible to variability among clinicians. Traditional image processing\ntechniques often lack the precision, robustness, and speed required for\nreal-time clinical use. To overcome these limitations, this paper introduces a\ndeep learning framework for real-time medical image analysis designed to\nenhance diagnostic accuracy and computational efficiency across multiple\nimaging modalities, including X-ray, CT, and MRI. The proposed system\nintegrates advanced neural network architectures such as U-Net, EfficientNet,\nand Transformer-based models with real-time optimization strategies including\nmodel pruning, quantization, and GPU acceleration. The framework enables\nflexible deployment on edge devices, local servers, and cloud infrastructures,\nensuring seamless interoperability with clinical systems such as PACS and EHR.\nExperimental evaluations on public benchmark datasets demonstrate\nstate-of-the-art performance, achieving classification accuracies above 92%,\nsegmentation Dice scores exceeding 91%, and inference times below 80\nmilliseconds. Furthermore, visual explanation tools such as Grad-CAM and\nsegmentation overlays enhance transparency and clinical interpretability. These\nresults indicate that the proposed framework can substantially accelerate\ndiagnostic workflows, reduce clinician workload, and support trustworthy AI\nintegration in time-critical healthcare environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u6280\u672f\u7f3a\u4e4f\u5b9e\u65f6\u4e34\u5e8a\u4f7f\u7528\u6240\u9700\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u901f\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u4e86U-Net\u3001EfficientNet\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u5b9e\u65f6\u4f18\u5316\u7b56\u7565\uff08\u5982\u6a21\u578b\u526a\u679d\u3001\u91cf\u5316\u548cGPU\u52a0\u901f\uff09\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u3001\u672c\u5730\u670d\u52a1\u5668\u548c\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u7075\u6d3b\u90e8\u7f72\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc792%\uff0c\u5206\u5272Dice\u5206\u6570\u8d85\u8fc791%\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e80\u6beb\u79d2\uff0c\u4e14\u53ef\u89c6\u5316\u89e3\u91ca\u5de5\u5177\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u663e\u8457\u52a0\u901f\u8bca\u65ad\u6d41\u7a0b\uff0c\u51cf\u5c11\u4e34\u5e8a\u533b\u751f\u5de5\u4f5c\u91cf\uff0c\u5e76\u652f\u6301\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u533b\u7597\u73af\u5883\u4e2d\u53ef\u4fe1\u8d56\u7684AI\u96c6\u6210\u3002"}}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cdXRL\u7b97\u6cd5\u5728\u8c03\u8bd5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6548\u679c\u6709\u9650\u4e14\u7528\u6237\u81ea\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u51c6\u786e\u7387\u4e0d\u76f8\u5173\u3002", "motivation": "\u5f53\u524d\u5bf9\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\uff08XRL\uff09\u7b97\u6cd5\u7684\u6bd4\u8f83\u8bc4\u4f30\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u8c03\u8bd5\u8fd9\u4e00\u6838\u5fc3\u5e94\u7528\u4e0a\u7684\u8868\u73b0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7Atari\u7684Ms. Pacman\u73af\u5883\u548c\u56db\u79cdXRL\u7b97\u6cd5\uff0c\u6d4b\u8bd5\u7528\u6237\u662f\u5426\u80fd\u4ece\u51b3\u7b56\u89e3\u91ca\u4e2d\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u56db\u79cdXRL\u7b97\u6cd5\u4e2d\u4ec5\u6709\u4e00\u79cd\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u7684\u51c6\u786e\u7387\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u7528\u6237\u666e\u904d\u9ad8\u4f30\u4e86\u81ea\u5df1\u7684\u5224\u65ad\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684XRL\u7b97\u6cd5\u5728\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\u65b9\u9762\u7684\u8868\u73b0\u666e\u904d\u4e0d\u4f73\uff0c\u4e14\u7528\u6237\u7684\u81ea\u4fe1\u7a0b\u5ea6\u4e0e\u5b9e\u9645\u51c6\u786e\u7387\u4e0d\u5339\u914d\u3002\u6b64\u5916\uff0c\u7528\u6237\u81ea\u6211\u62a5\u544a\u7684\u6613\u7406\u89e3\u6027\u4e0e\u5b9e\u9645\u51c6\u786e\u7387\u65e0\u5173\u3002"}}
{"id": "2510.16800", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16800", "abs": "https://arxiv.org/abs/2510.16800", "authors": ["Zhenpeng Zhang", "Yi Wang", "Shanglei Chai", "Yingying Liu", "Zekai Xie", "Wenhao Huang", "Pengyu Li", "Zipei Luo", "Dajiang Lu", "Yibin Tian"], "title": "An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting", "comment": null, "summary": "Lychee is a high-value subtropical fruit. The adoption of vision-based\nharvesting robots can significantly improve productivity while reduce reliance\non labor. High-quality data are essential for developing such harvesting\nrobots. However, there are currently no consistently and comprehensively\nannotated open-source lychee datasets featuring fruits in natural growing\nenvironments. To address this, we constructed a dataset to facilitate lychee\ndetection and maturity classification. Color (RGB) images were acquired under\ndiverse weather conditions, and at different times of the day, across multiple\nlychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset\nencompasses three different ripeness stages and contains 11,414 images,\nconsisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth\nimages. The images are annotated with 9,658 pairs of lables for lychee\ndetection and maturity classification. To improve annotation consistency, three\nindividuals independently labeled the data, and their results were then\naggregated and verified by a fourth reviewer. Detailed statistical analyses\nwere done to examine the dataset. Finally, we performed experiments using three\nrepresentative deep learning models to evaluate the dataset. It is publicly\navailable for academic", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u7684\u8354\u679d\u6570\u636e\u96c6\uff0c\u652f\u6301\u68c0\u6d4b\u548c\u6210\u719f\u5ea6\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5168\u9762\u6807\u6ce8\u7684\u8354\u679d\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u57fa\u4e8e\u89c6\u89c9\u7684\u91c7\u6458\u673a\u5668\u4eba\u5f00\u53d1\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e0d\u540c\u54c1\u79cd\u3001\u6210\u719f\u5ea6\u548c\u73af\u5883\u6761\u4ef6\u7684\u8354\u679d\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u7edf\u8ba1\u5206\u6790\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u9a8c\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b11,414\u5f20\u56fe\u50cf\uff0c\u6807\u6ce8\u4e869,658\u5bf9\u6807\u7b7e\uff0c\u901a\u8fc7\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u8354\u679d\u6570\u636e\u96c6\uff0c\u652f\u6301\u8354\u679d\u68c0\u6d4b\u548c\u6210\u719f\u5ea6\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316GPU\u5185\u6838\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eGPU\u5185\u6838\u4f18\u5316\u6d89\u53ca\u590d\u6742\u7684\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u3001\u7ebf\u7a0b\u8c03\u5ea6\u548c\u786c\u4ef6\u7279\u5b9a\u7279\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u73b0\u6709LLM\u65b9\u6cd5\u591a\u4e3a\u5355\u6b21\u751f\u6210\u6216\u7b80\u5355\u4f18\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2aLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u57fa\u7840\u6307\u4ee4\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\u6765\u7cfb\u7edf\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6b63\u786e\u6027\uff0c\u8fd8\u5b9e\u73b0\u4e86\u9ad8\u8fbe16\u500d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5728GPU\u5185\u6838\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u5b9e\u73b0\u81ea\u52a8\u5316\u4f18\u5316\u3002"}}
{"id": "2510.16641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16641", "abs": "https://arxiv.org/abs/2510.16641", "authors": ["Young-Jun Lee", "Byung-Kwan Lee", "Jianshu Zhang", "Yechan Hwang", "Byungsoo Ko", "Han-Gyu Kim", "Dongyu Yao", "Xuankun Rong", "Eojin Joo", "Seung-Ho Han", "Bowon Ko", "Ho-Jin Choi"], "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models", "comment": "Project website:\n  https://passing2961.github.io/multiverse-project-page/", "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.", "AI": {"tldr": "MultiVerse \u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u5e7f\u6cdb\u4e3b\u9898\uff0c\u901a\u8fc7 GPT-4o \u8bc4\u4f30 VLM \u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u590d\u6742\u5bf9\u8bdd\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u4ec5\u90e8\u5206\u8986\u76d6\u4e86\u7528\u6237\u5b9e\u9645\u9047\u5230\u7684\u5bf9\u8bdd\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u548c\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30 VLMs \u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6e05\u5355\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528 GPT-4o \u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u6d4b\u91cf\u4e86 37 \u4e2a\u5173\u952e\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u4e86 18 \u4e2a VLMs\uff0c\u53d1\u73b0\u5373\u4f7f\u662f GPT-4o \u8fd9\u6837\u7684\u6700\u5f3a\u6a21\u578b\u5728\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e5f\u4ec5\u8fbe\u5230 50% \u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u53d1\u73b0\u63d0\u4f9b\u5b8c\u6574\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5bf9\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MultiVerse \u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5f3a\u5927\u7684 VLM \u6a21\u578b\u5728\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17363", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17363", "abs": "https://arxiv.org/abs/2510.17363", "authors": ["U. V. B. L Udugama", "George Vosselman", "Francesco Nex"], "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). 8 pages, 7 figures", "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.", "AI": {"tldr": "M2H\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u8bed\u4e49\u5206\u5272\u548c\u6df1\u5ea6\u7b49\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u591a\u4efb\u52a1\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u5e73\u8861\u95ee\u9898\u3002", "method": "M2H\u91c7\u7528\u57fa\u4e8e\u7a97\u53e3\u7684\u8de8\u4efb\u52a1\u6ce8\u610f\u529b\u6a21\u5757\uff08Window-Based Cross-Task Attention Module\uff09\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7ViT-based DINOv2\u9aa8\u5e72\u7f51\u7edc\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u7279\u5f81\u4ea4\u6362\u4e0e\u4efb\u52a1\u7279\u5b9a\u7ec6\u8282\u4fdd\u7559\u3002", "result": "M2H\u5728NYUDv2\u3001Hypersim\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "M2H\u6846\u67b6\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u53cd\u9988\u5de5\u5177\u8c03\u7528\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u5f71\u54cd\u5176\u53ef\u9760\u6027\uff0c\u9700\u4e00\u79cd\u8bca\u65ad\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u516b\u79cd\u7279\u5b9a\u9519\u8bef\u7c7b\u578b\u5e76\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u6765\u8bad\u7ec3ToolCritic\uff0c\u5229\u7528\u5176\u5bf9\u4e3bLLM\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u4e3bLLM\u57fa\u4e8e\u53cd\u9988\u4fee\u6b63\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\uff08SGD\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToolCritic\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u6700\u9ad8\u63d0\u534713%\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6821\u6b63\u6280\u672f\u3002", "conclusion": "ToolCritic\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2dLLM\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u7a33\u5065\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.", "AI": {"tldr": "TL;DR: LaGAT\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u542f\u53d1\u5f0f\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5\uff0c\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u4f18\u4e8e\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u8bba\u6587\u52a8\u673a\u662f\u89e3\u51b3\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u95ee\u9898\u4e2d\u5b9e\u65f6\u5bfb\u627e\u8fd1\u4f18\u89e3\u7684\u6311\u6218\uff0c\u5373\u4f7f\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u89c4\u5212\u8005\u4e5f\u662f\u5982\u6b64\u3002", "method": "\u8bba\u6587\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u6765\u81eaMAGAT\u7684\u5b66\u4e60\u542f\u53d1\u5f0f\uff08\u4e00\u79cd\u5177\u6709\u56fe\u6ce8\u610f\u529b\u65b9\u6848\u7684\u795e\u7ecfMAPF\u7b56\u7565\uff09\u96c6\u6210\u5230\u9886\u5148\u7684\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5LaCAM\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u62ec\u589e\u5f3a\u7684MAGAT\u67b6\u6784\u3001\u5728\u611f\u5174\u8da3\u7684\u5730\u56fe\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u7684\u7b56\u7565\uff0c\u4ee5\u53ca\u7528\u4e8e\u5904\u7406\u4e0d\u5b8c\u7f8e\u795e\u7ecf\u6307\u5bfc\u7684\u6b7b\u9501\u68c0\u6d4b\u65b9\u6848\u3002", "result": "\u8bba\u6587\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\uff08\u79f0\u4e3aLaGAT\uff09\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u4f18\u4e8e\u7eaf\u57fa\u4e8e\u641c\u7d22\u548c\u7eaf\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u8868\u660e\uff0c\u5f53\u7cbe\u5fc3\u8bbe\u8ba1\u65f6\uff0c\u6df7\u5408\u641c\u7d22\u4e3a\u7d27\u5bc6\u8026\u5408\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17064", "abs": "https://arxiv.org/abs/2510.17064", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation", "comment": "22 pages, 6 figures, 2 tables", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation.", "AI": {"tldr": "BRAINCELL-AID\u901a\u8fc7\u591a\u4ee3\u7406AI\u7cfb\u7edf\u6574\u5408\u6587\u672c\u4e0e\u672c\u4f53\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u7684\u57fa\u56e0\u96c6\u6ce8\u91ca\u51c6\u786e\u6027\uff0c\u652f\u6301\u8111\u7ec6\u80de\u529f\u80fd\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982GSEA\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u6ce8\u91ca\uff0c\u5728\u6d89\u53ca\u672a\u5145\u5206\u8868\u5f81\u7684\u57fa\u56e0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cLLMs\u96be\u4ee5\u5728\u7ed3\u6784\u5316\u672c\u4f53\u4e2d\u8868\u793a\u590d\u6742\u751f\u7269\u77e5\u8bc6\u3002", "method": "\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u4e0e\u672c\u4f53\u6807\u7b7e\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u591a\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u901a\u8fc7PubMed\u6587\u732e\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u5c0f\u9f20\u57fa\u56e0\u96c6\u4e2d\u5b9e\u73b0\u4e8677%\u7684\u51c6\u786e\u6ce8\u91ca\uff0c\u5e76\u5e94\u7528\u4e8e5,322\u4e2a\u8111\u7ec6\u80de\u7c07\u7684\u6ce8\u91ca\uff0c\u63ed\u793a\u4e86\u533a\u57df\u7279\u5f02\u6027\u57fa\u56e0\u5171\u8868\u8fbe\u6a21\u5f0f\u3002", "conclusion": "BRAINCELL-AID\u4e3a\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u7684\u57fa\u56e0\u96c6\u6ce8\u91ca\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u793e\u533a\u9a71\u52a8\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.16660", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16660", "abs": "https://arxiv.org/abs/2510.16660", "authors": ["Yuntian Wang", "Xilin Yang", "Che-Yung Shen", "Nir Pillar", "Aydogan Ozcan"], "title": "Universal and Transferable Attacks on Pathology Foundation Models", "comment": "38 Pages, 8 Figures", "summary": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology.", "AI": {"tldr": "UTAP\u662f\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u8fc1\u79fb\u7684\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u591a\u79cd\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8106\u5f31\u6027\u5e76\u63a8\u52a8\u9632\u5fa1\u673a\u5236\u7814\u7a76\u3002", "motivation": "\u63ed\u793a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u8bbe\u7acb\u9ad8\u6807\u51c6\u57fa\u51c6\uff0c\u5e76\u63a8\u52a8\u9632\u5fa1\u673a\u5236\u7684\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7684UTAP\uff0c\u662f\u4e00\u79cd\u56fa\u5b9a\u4e14\u5fae\u5f31\u7684\u566a\u58f0\u6a21\u5f0f\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u7834\u574f\u591a\u4e2a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002", "result": "UTAP\u5728\u591a\u79cd\u6700\u5148\u8fdb\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u4e0a\u5f15\u8d77\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5176\u6270\u52a8\u5177\u6709\u666e\u904d\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "UTAP\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u8fc1\u79fb\u7684\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u63ed\u793a\u4e86\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u673a\u5236\u6539\u8fdb\u7684\u9700\u6c42\u3002"}}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7NAS\u548cKPD-MADS\u4e24\u79cdLLM\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u975e\u8d22\u52a1\u5b9a\u6027\u6307\u6807\u7684\u81ea\u52a8\u5316\u63a8\u7406\u95ee\u9898\uff0cKPD-MADS\u5728\u591a\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u867d\u7136\u91d1\u878dAI\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u81ea\u52a8\u5316\u4ecd\u672a\u89e3\u51b3\uff0c\u5c24\u5176\u662f\u975e\u8d22\u52a1\u5b9a\u6027\u6307\u6807\u7684\u89c4\u8303\u5316\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff1a\u975e\u5bf9\u6297\u6027\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08NAS\uff09\u548c\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08KPD-MADS\uff09\u3002", "result": "\u4e24\u79cd\u7cfb\u7edf\u5747\u663e\u8457\u63d0\u5347\u4e86\u751f\u4ea7\u7387\uff08NAS: 11.55\u79d2/\u6848\u4f8b\uff1bKPD-MADS: 91.97\u79d2\uff1b\u4eba\u5de5\u57fa\u51c6: 1920\u79d2\uff09\u3002KPD-MADS\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u89e3\u91ca\u5145\u5206\u6027\u3001\u5b9e\u7528\u9002\u7528\u6027\u548c\u53ef\u7528\u6027\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff08\u5982KPD-MADS\uff09\u80fd\u663e\u8457\u63d0\u5347\u91d1\u878dAI\u4e2d\u7684\u63a8\u7406\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u548c\u53ef\u8fa9\u62a4\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.16664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16664", "abs": "https://arxiv.org/abs/2510.16664", "authors": ["Christopher Thirgood", "Oscar Mendez", "Erin Ling", "Jon Storey", "Simon Hadfield"], "title": "HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications", "comment": null, "summary": "Hyperspectral images (HSI) promise to support a range of new applications in\ncomputer vision. Recent research has explored the feasibility of generalizable\nSpectral Reconstruction (SR), the problem of recovering a HSI from a natural\nthree-channel color image in unseen scenarios.\n  However, previous Multi-Scale Attention (MSA) works have only demonstrated\nsufficient generalizable results for very sparse spectra, while modern HSI\nsensors contain hundreds of channels.\n  This paper introduces a novel approach to spectral reconstruction via our\nHYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).\n  Using a Teacher model that encapsulates latent hyperspectral image data and a\nStudent model that learns mappings from natural images to the Teacher's encoded\ndomain, alongside a novel training method, we achieve high-quality spectral\nreconstruction.\n  This addresses key limitations of prior SR models, providing SOTA performance\nacross all metrics, including an 18\\% boost in accuracy, and faster inference\ntimes than current SOTA models at various channel depths.", "AI": {"tldr": "HYDRA\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7684\u5149\u8c31\u91cd\u5efa\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u7a00\u758f\u5149\u8c31\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u9002\u5e94\u73b0\u4ee3\u9ad8\u5149\u8c31\u4f20\u611f\u5668\u6570\u767e\u901a\u9053\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5149\u8c31\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u67b6\u6784\uff0c\u6559\u5e08\u6a21\u578b\u5c01\u88c5\u6f5c\u5728\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u6570\u636e\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u4ece\u81ea\u7136\u56fe\u50cf\u5230\u6559\u5e08\u6a21\u578b\u7f16\u7801\u57df\u7684\u6620\u5c04\uff0c\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "HYDRA\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5305\u62ec\u51c6\u786e\u7387\u63d0\u534718%\uff0c\u5e76\u5728\u4e0d\u540c\u901a\u9053\u6df1\u5ea6\u4e0b\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHYDRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u5149\u8c31\u91cd\u5efa\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u8c31\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u514b\u670d\u4e86\u5148\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17145", "categories": ["cs.AI", "68T05, 62H30"], "pdf": "https://arxiv.org/pdf/2510.17145", "abs": "https://arxiv.org/abs/2510.17145", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion", "comment": "35 pages, 6 figures and 11 tables", "summary": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u7279\u5f81\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u4e3a\u98df\u54c1\u8d28\u91cf\u76d1\u63a7\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u8bc4\u4f30\u662f\u98df\u54c1\u884c\u4e1a\u7684\u4e3b\u8981\u6311\u6218\uff0c\u76f4\u63a5\u5f71\u54cd\u4ea7\u54c1\u8d28\u91cf\u3001\u5e02\u573a\u4ef7\u503c\u548c\u6d88\u8d39\u8005\u5065\u5eb7\u3002\u4f20\u7edf\u7684\u611f\u5b98\u8bc4\u4f30\u4e3b\u89c2\u6027\u5f3a\u3001\u4e0d\u4e00\u81f4\u4e14\u96be\u4ee5\u6807\u51c6\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u63d0\u53d6\u5e76\u9010\u6b65\u878d\u5408\u4e92\u8865\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u4ee5\u53ca\u7eb9\u7406\u7279\u5f81\uff08\u5982\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0fLBP\u548c\u7070\u5ea6\u5171\u751f\u77e9\u9635GLCM\uff09\uff0c\u4ece\u9c7c\u773c\u56fe\u50cf\u4e2d\u6355\u83b7\u5168\u5c40\u8272\u5ea6\u53d8\u5316\u548c\u5c40\u90e8\u964d\u89e3\u3002", "result": "\u5728\u6807\u51c6\u8bad\u7ec3-\u6d4b\u8bd5\u8bbe\u7f6e\u4e2d\uff0cLightGBM\u5206\u7c7b\u5668\u8fbe\u523077.56%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf63.21%\u63d0\u9ad8\u4e8614.35%\u3002\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u540e\uff0c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u8fbe\u523097.16%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f7377.3%\u63d0\u9ad8\u4e8619.86%\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u624b\u5de5\u7279\u5f81\u5728\u6218\u7565\u6027\u5904\u7406\u540e\uff0c\u4e3a\u81ea\u52a8\u5316\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u98df\u54c1\u8d28\u91cf\u76d1\u63a7\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2510.16688", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16688", "abs": "https://arxiv.org/abs/2510.16688", "authors": ["Yejie Guo", "Yunzhong Hou", "Wufei Ma", "Meng Tang", "Ming-Hsuan Yang"], "title": "Pursuing Minimal Sufficiency in Spatial Reasoning", "comment": null, "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.", "AI": {"tldr": "MSSR\u662f\u4e00\u4e2a\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u6700\u5c0f\u5145\u5206\u4fe1\u606f\u96c6\uff08MSS\uff09\u4f18\u5316\u7a7a\u95f4\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a2D\u9884\u8bad\u7ec3\u5bfc\u81f4\u76843D\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5197\u4f593D\u4fe1\u606f\u5f15\u53d1\u7684\u63a8\u7406\u5931\u8d25\u3002", "method": "MSSR\u91c7\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u611f\u77e5\u4ee3\u7406\u548c\u4e00\u4e2a\u63a8\u7406\u4ee3\u7406\u3002\u611f\u77e5\u4ee3\u7406\u901a\u8fc7\u7a0b\u5e8f\u5316\u67e5\u8be23D\u573a\u666f\u63d0\u53d6\u8db3\u591f\u4fe1\u606f\uff08\u5305\u62ec\u65b0\u9896\u7684SOG\u6a21\u5757\uff09\uff0c\u63a8\u7406\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u4fe1\u606f\u4ee5\u5b9e\u73b0\u6700\u5c0f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMSSR\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6027\u80fd\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "MSSR\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8ffd\u6c42\u4fe1\u606f\u7684\u5145\u5206\u6027\u548c\u6700\u5c0f\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u95f4\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u4e24\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u751f\u6210\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6e90\u3002"}}
{"id": "2510.17146", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17146", "abs": "https://arxiv.org/abs/2510.17146", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation", "comment": "NeurIPS 2025 Workshop of UrbanAI (Oral)", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems.", "AI": {"tldr": "PILLM\u7ed3\u5408\u7269\u7406\u539f\u7406\u4e0eLLM\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u4f18\u5316HVAC\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "HVAC\u7cfb\u7edf\u80fd\u8017\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9002\u5e94\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u7ed3\u5408\u7269\u7406\u539f\u7406\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51faPILLM\u6846\u67b6\uff0c\u5229\u7528\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\uff0c\u5f15\u5165\u7269\u7406\u4fe1\u606f\u53cd\u5c04\u548c\u4ea4\u53c9\u7b97\u5b50\u5d4c\u5165\u70ed\u529b\u5b66\u548c\u63a7\u5236\u7406\u8bba\u7ea6\u675f\u3002", "result": "\u5728\u516c\u5171\u5efa\u7b51\u6545\u969c\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cPILLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u89c4\u5219\u3002", "conclusion": "PILLM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u539f\u7406\u548cLLM\uff0c\u5728HVAC\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u5efa\u7b51\u7cfb\u7edf\u4e2d\u53ef\u4fe1\u8d56AI\u7684\u5e94\u7528\u3002"}}
{"id": "2510.16702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16702", "abs": "https://arxiv.org/abs/2510.16702", "authors": ["Huy Minh Nhat Nguyen", "Triet Hoang Minh Dao", "Chau Vinh Hoang Truong", "Cuong Tuan Nguyen"], "title": "SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation", "comment": "2025 IEEE Conference on Computational Intelligence in Bioinformatics\n  and Computational Biology (CIBCB)", "summary": "Optical Coherence Tomography (OCT) is a widely used non-invasive imaging\ntechnique that provides detailed three-dimensional views of the retina, which\nare essential for the early and accurate diagnosis of ocular diseases.\nConsequently, OCT image analysis and processing have emerged as key research\nareas in biomedical imaging. However, acquiring paired datasets of clean and\nreal-world noisy OCT images for supervised denoising models remains a\nformidable challenge due to intrinsic speckle noise and practical constraints\nin clinical imaging environments. To address these issues, we propose SDPA++: A\nGeneral Framework for Self-Supervised Denoising with Patch Aggregation. Our\nnovel approach leverages only noisy OCT images by first generating\npseudo-ground-truth images through self-fusion and self-supervised denoising.\nThese refined images then serve as targets to train an ensemble of denoising\nmodels using a patch-based strategy that effectively enhances image clarity.\nPerformance improvements are validated via metrics such as Contrast-to-Noise\nRatio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge\nPreservation (EP) on the real-world dataset from the IEEE SPS Video and Image\nProcessing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT\nimages without clean references, highlighting our method's potential for\nimproving image quality and diagnostic outcomes in clinical practice.", "AI": {"tldr": "SDPA++\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u53bb\u566a\u6846\u67b6\uff0c\u65e0\u9700\u5e72\u51c0\u56fe\u50cf\u5373\u53ef\u63d0\u5347OCT\u56fe\u50cf\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u3002", "motivation": "OCT\u56fe\u50cf\u5206\u6790\u5bf9\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u5e72\u51c0\u7684\u914d\u5bf9\u6570\u636e\u96c6\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u566a\u58f0\u3002", "method": "\u63d0\u51faSDPA++\u6846\u67b6\uff0c\u5229\u7528\u81ea\u878d\u5408\u548c\u81ea\u76d1\u7763\u53bb\u566a\u751f\u6210\u4f2a\u5730\u9762\u771f\u5b9e\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u8865\u4e01\u7684\u7b56\u7565\u8bad\u7ec3\u53bb\u566a\u6a21\u578b\u3002", "result": "\u5728VIP Cup\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u6307\u6807\uff08CNR\u3001MSR\u3001TP\u3001EP\uff09\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SDPA++\u6846\u67b6\u901a\u8fc7\u81ea\u76d1\u7763\u53bb\u566a\u548c\u8865\u4e01\u805a\u5408\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86OCT\u56fe\u50cf\u8d28\u91cf\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u56fe\u50cf\u652f\u6301\u3002"}}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.", "AI": {"tldr": "ProtocolBench\u7cfb\u7edf\u5316\u8bc4\u4f30\u591a\u4ee3\u7406\u534f\u8bae\u6027\u80fd\uff0cProtocolRouter\u52a8\u6001\u9009\u62e9\u534f\u8bae\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u6545\u969c\u6062\u590d\u65f6\u95f4\u51cf\u5c1118.1%\u3002", "motivation": "\u5927\u89c4\u6a21\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u534f\u8bae\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6307\u5bfc\u3002", "method": "\u5f15\u5165ProtocolBench\u57fa\u51c6\uff0c\u4ece\u4efb\u52a1\u6210\u529f\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u6d88\u606f/\u5b57\u8282\u5f00\u9500\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u534f\u8bae\u6027\u80fd\uff1b\u5e76\u5f00\u53d1ProtocolRouter\uff0c\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u534f\u8bae\u8def\u7531\u5668\uff0c\u6839\u636e\u573a\u666f\u9700\u6c42\u548c\u8fd0\u884c\u65f6\u4fe1\u53f7\u52a8\u6001\u9009\u62e9\u534f\u8bae\u3002", "result": "ProtocolBench\u663e\u793a\u4e0d\u540c\u534f\u8bae\u5728\u7cfb\u7edf\u884c\u4e3a\u4e0a\u6709\u663e\u8457\u5dee\u5f02\uff08\u5982\u5b8c\u6210\u65f6\u95f4\u5dee\u5f02\u8fbe36.5%\uff09\u3002ProtocolRouter\u5728\u6545\u969c\u6062\u590d\u65f6\u95f4\u4e0a\u6bd4\u6700\u4f73\u5355\u534f\u8bae\u57fa\u7ebf\u51cf\u5c1118.1%\uff0c\u5e76\u5728\u7279\u5b9a\u573a\u666f\uff08\u5982GAIA\uff09\u4e2d\u53d6\u5f97\u66f4\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86ProtocolBench\u548cProtocolRouter\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u52a8\u6001\u9009\u62e9\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16704", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16704", "abs": "https://arxiv.org/abs/2510.16704", "authors": ["Tianxin Wei", "Yifan Chen", "Xinrui He", "Wenxuan Bao", "Jingrui He"], "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization", "comment": "Accepted by KDD 2025", "summary": "Distribution shifts between training and testing samples frequently occur in\npractice and impede model generalization performance. This crucial challenge\nthereby motivates studies on domain generalization (DG), which aim to predict\nthe label on unseen target domain data by solely using data from source\ndomains. It is intuitive to conceive the class-separated representations\nlearned in contrastive learning (CL) are able to improve DG, while the reality\nis quite the opposite: users observe directly applying CL deteriorates the\nperformance. We analyze the phenomenon with the insights from CL theory and\ndiscover lack of intra-class connectivity in the DG setting causes the\ndeficiency. We thus propose a new paradigm, domain-connecting contrastive\nlearning (DCCL), to enhance the conceptual connectivity across domains and\nobtain generalizable representations for DG. On the data side, more aggressive\ndata augmentation and cross-domain positive samples are introduced to improve\nintra-class connectivity. On the model side, to better embed the unseen test\ndomains, we propose model anchoring to exploit the intra-class connectivity in\npre-trained representations and complement the anchoring with generative\ntransformation loss. Extensive experiments on five standard DG benchmarks are\nperformed. The results verify that DCCL outperforms state-of-the-art baselines\neven without domain supervision. The detailed model implementation and the code\nare provided through https://github.com/weitianxin/DCCL", "AI": {"tldr": "DCCL\u901a\u8fc7\u589e\u5f3a\u8de8\u9886\u57df\u7c7b\u5185\u8fde\u63a5\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9886\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u4e2d\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u76f4\u63a5\u5e94\u7528\u6548\u679c\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7c7b\u5185\u8fde\u63a5\u6027\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u63a5\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u9886\u57df\u8fde\u63a5\u5bf9\u6bd4\u5b66\u4e60\uff08DCCL\uff09\uff0c\u5305\u62ec\u6570\u636e\u4fa7\uff08\u66f4\u6fc0\u8fdb\u7684\u6570\u636e\u589e\u5f3a\u548c\u8de8\u9886\u57df\u6b63\u6837\u672c\uff09\u548c\u6a21\u578b\u4fa7\uff08\u6a21\u578b\u951a\u5b9a\u548c\u751f\u6210\u53d8\u6362\u635f\u5931\uff09\u7684\u6539\u8fdb\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6DG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCCL\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DCCL\u901a\u8fc7\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u63a5\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9886\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u5728\u65e0\u9886\u57df\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.", "AI": {"tldr": "\u7ed3\u5408ECGFounder\u4e0eXGBoost\u7684\u6df7\u5408\u6a21\u578b\u5728VT/VF\u98ce\u9669\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u987e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aAI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\uff08VT/VF\uff09\u662f\u6025\u6027\u5fc3\u808c\u6897\u6b7b\uff08AMI\uff09\u540e\u9662\u5185\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u65e9\u671f\u8bc6\u522b\u4ecd\u662f\u4e34\u5e8a\u6311\u6218\u3002\u4f20\u7edf\u98ce\u9669\u8bc4\u5206\u6027\u80fd\u6709\u9650\uff0c\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u4e86\u5927\u89c4\u6a21\u5fc3\u7535\u56fe\u57fa\u7840\u6a21\u578b\uff08ECGFounder\uff09\u4e0e\u53ef\u89e3\u91ca\u7684XGBoost\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548cSHAP\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6df7\u5408\u6a21\u578bAUC\u4e3a0.801\uff0c\u4f18\u4e8eKNN\uff080.677\uff09\u3001RNN\uff080.676\uff09\u548c1D-CNN\uff080.720\uff09\u3002SHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u8bc6\u522b\u7684\u5173\u952e\u7279\u5f81\uff08\u5982\u201c\u5ba4\u6027\u65e9\u640f\u201d\u548c\u201c\u6b63\u5e38\u7aa6\u6027\u5fc3\u5f8b\u201d\uff09\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3aVT/VF\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u53ef\u4f5c\u4e3a\u6709\u6548\u3001\u81ea\u52a8\u5316\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684AI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2510.16709", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16709", "abs": "https://arxiv.org/abs/2510.16709", "authors": ["Liu Haojie", "Gao Suixiang"], "title": "HumanCM: One Step Human Motion Prediction", "comment": "6 pages, 2 figures, 2 tables", "summary": "We present HumanCM, a one-step human motion prediction framework built upon\nconsistency models. Instead of relying on multi-step denoising as in\ndiffusion-based methods, HumanCM performs efficient single-step generation by\nlearning a self-consistent mapping between noisy and clean motion states. The\nframework adopts a Transformer-based spatiotemporal architecture with temporal\nembeddings to model long-range dependencies and preserve motion coherence.\nExperiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves\ncomparable or superior accuracy to state-of-the-art diffusion models while\nreducing inference steps by up to two orders of magnitude.", "AI": {"tldr": "HumanCM\u662f\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u578b\u7684\u9ad8\u6548\u5355\u6b65\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u6027\u80fd\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u4e14\u63a8\u7406\u6b65\u9aa4\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u65b9\u6cd5\u7684\u591a\u6b65\u53bb\u566a\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u65f6\u7a7a\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u5d4c\u5165\u6765\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u5e76\u4fdd\u6301\u8fd0\u52a8\u8fde\u8d2f\u6027\u3002", "result": "\u5728Human3.6M\u548cHumanEva-I\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHumanCM\u5728\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\u3002", "conclusion": "HumanCM\u901a\u8fc7\u4e00\u81f4\u6027\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5355\u6b65\u751f\u6210\uff0c\u5728\u4fdd\u6301\u8fd0\u52a8\u8fde\u8d2f\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u6b65\u9aa4\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u548c\u6a21\u62df\u5668\u5b9e\u9a8c\uff0c\u53d1\u73b0\u7edf\u4e00\u91cd\u5de5\u5177\u7b56\u7565\u5bf9\u7279\u5b9a\u5b50\u7fa4\u4f53\u6709\u5bb3\uff0c\u800c\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u5efa\u8bae\u51bb\u7ed3\u751f\u6210\u5668\u5e76\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u51b3\u7b56\u5934\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5065\u5eb7\u6559\u7ec3\u4e2d\u5b9e\u73b0\u4e2a\u6027\u5316\u8bc4\u4f30\uff0c\u5e76\u8bc6\u522b\u4e0d\u540c\u5b50\u7fa4\u4f53\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u548c\u5206\u89e3\u51b3\u7b56\u5934\uff08\u5de5\u5177/\u98ce\u683c\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u548c\u9690\u85cf\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u867d\u7136\u63d0\u9ad8\u4e86\u65e5\u5fd7\u7684\u5e73\u5747\u503c\uff0c\u4f46\u5bf9\u7279\u5b9a\u5b50\u7fa4\u4f53\uff08\u5982\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\uff09\u9020\u6210\u8d1f\u9762\u5f71\u54cd\u3002\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u663e\u793a\uff0c\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u80fd\u7f29\u77ed\u7279\u8d28\u8bc6\u522b\u65f6\u95f4\u5e76\u63d0\u9ad8\u76ee\u6807\u6210\u529f\u7387\u548cpass@3\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u51bb\u7ed3\u751f\u6210\u5668\u3001\u5b66\u4e60\u57fa\u4e8e\u5b50\u7fa4\u4f53\u7684\u51b3\u7b56\u5934\uff0c\u5e76\u62a5\u544a\u6bcf\u4e2a\u539f\u578b\u7684\u6307\u6807\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0\u4e2a\u6027\u5316\u8bc4\u4f30\uff0c\u540c\u65f6\u63ed\u793a\u88ab\u5e73\u5747\u503c\u63a9\u76d6\u7684\u5b50\u7fa4\u4f53\u95ee\u9898\u3002"}}
{"id": "2510.16714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16714", "abs": "https://arxiv.org/abs/2510.16714", "authors": ["Xiongkun Linghu", "Jiangyong Huang", "Ziyu Zhu", "Baoxiong Jia", "Siyuan Huang"], "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes", "comment": "Project page: https://scenecot.github.io/", "summary": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SCENECOT\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06CoT\u63a8\u7406\u5e94\u7528\u4e8e3D\u573a\u666f\u7406\u89e3\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u5f0f\u9010\u6b65\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u67093D\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a5\u5730\u95ee\u9898\u56de\u7b54\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u5bf9\u4eba\u7c7b\u5f0f\u573a\u666f-\u5bf9\u8c61\u63a5\u5730\u63a8\u7406\u673a\u5236\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6SCENECOT\uff0c\u5c06\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u4e14\u53ef\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u591a\u6a21\u6001\u4e13\u5bb6\u6a21\u5757\u6784\u5efa\u76f8\u5e94\u7684\u89c6\u89c9\u7ebf\u7d22\u3002\u540c\u65f6\u5f00\u53d1\u4e86SCENECOT-185K\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u590d\u67423D\u573a\u666f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u6846\u67b6\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u63a5\u5730-QA\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u9996\u6b21\u6210\u529f\u5c06CoT\u63a8\u7406\u5e94\u7528\u4e8e3D\u573a\u666f\u7406\u89e3\uff0c\u5b9e\u73b0\u4e86\u9010\u6b65\u7684\u4eba\u7c7b\u5f0f\u63a8\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb3D\u573a\u666f\u7406\u89e3\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.", "AI": {"tldr": "TD-HNODE\u662f\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u57282\u578b\u7cd6\u5c3f\u75c5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u6570\u636e\u6216\u6355\u6349\u590d\u6742\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTD-HNODE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0cTD-HNODE\u5728\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TD-HNODE\u6a21\u578b\u5728\u6a21\u62df2\u578b\u7cd6\u5c3f\u75c5\u53ca\u76f8\u5173\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.16729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16729", "abs": "https://arxiv.org/abs/2510.16729", "authors": ["Jianbiao Mei", "Yu Yang", "Xuemeng Yang", "Licheng Wen", "Jiajun Lv", "Botian Shi", "Yong Liu"], "title": "Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models", "comment": null, "summary": "End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.", "AI": {"tldr": "IR-WM\u901a\u8fc7\u4ec5\u9884\u6d4b\u573a\u666f\u53d8\u5316\u7684\u6b8b\u5dee\uff0c\u51cf\u5c11\u4e86\u5bf9\u9759\u6001\u80cc\u666f\u7684\u5197\u4f59\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9884\u6d4b\u548c\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u89c6\u89c9\u4e2d\u5fc3\u4e16\u754c\u6a21\u578b\u4e2d\u5b58\u5728\u5bf9\u9759\u6001\u80cc\u666f\u5197\u4f59\u5efa\u6a21\u7684\u95ee\u9898\uff0cIR-WM\u65e8\u5728\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5f53\u524d\u72b6\u6001\u548c\u4e16\u754c\u6f14\u53d8\u7684\u5efa\u6a21\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "IR-WM\u9996\u5148\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u5efa\u7acb\u5f53\u524d\u72b6\u6001\u7684\u9e1f\u77b0\u56fe\u8868\u793a\uff0c\u7136\u540e\u5229\u7528\u524d\u4e00\u65f6\u523b\u7684BEV\u7279\u5f81\u4f5c\u4e3a\u65f6\u95f4\u5148\u9a8c\uff0c\u4ec5\u9884\u6d4b\u57fa\u4e8e\u81ea\u8f66\u52a8\u4f5c\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u201c\u6b8b\u5dee\u201d\u53d8\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u4e86\u5bf9\u9f50\u6a21\u5757\u6765\u6821\u51c6\u8bed\u4e49\u548c\u52a8\u6001\u4e0d\u5bf9\u9f50\u3002", "result": "IR-WM\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e864D\u5360\u7528\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u7684\u9886\u5148\u6027\u80fd\u3002", "conclusion": "IR-WM\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57284D\u5360\u7528\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.17235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17235", "abs": "https://arxiv.org/abs/2510.17235", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis", "comment": null, "summary": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5).", "AI": {"tldr": "Coinvisor\u662f\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u52a9\u624b\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u63d0\u5347\u5206\u6790\u80fd\u529b\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u9ad8\u6ce2\u52a8\u6027\u548c\u4fe1\u606f\u788e\u7247\u5316\u5e26\u6765\u7684\u6295\u8d44\u51b3\u7b56\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faCoinvisor\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u4ee3\u7406\u6846\u67b6\u804a\u5929\u673a\u5668\u4eba\uff0c\u96c6\u6210\u591a\u6837\u5316\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u5b9e\u65f6\u6570\u636e\u548c\u591a\u6b65\u63a8\u7406\u3002", "result": "\u5728\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u4e0a\u53ec\u56de\u7387\u63d0\u534740.7%\uff0cF1\u5206\u6570\u63d0\u534726.6%\uff1b\u7528\u6237\u6ee1\u610f\u5ea6\u8fbe4.64/5\u3002", "conclusion": "Coinvisor\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u5de5\u5177\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u7684\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2510.16730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16730", "abs": "https://arxiv.org/abs/2510.16730", "authors": ["Tianyang Dou", "Ming Li", "Jiangying Qin", "Xuan Liao", "Jiageng Zhong", "Armin Gruen", "Mengyi Deng"], "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid", "comment": null, "summary": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce.", "AI": {"tldr": "UKANFormer\u662f\u4e00\u79cd\u65b0\u578b\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u53d8\u6362\u5668\u5757\u63d0\u5347\u73ca\u745a\u7901\u6620\u5c04\u7cbe\u5ea6\uff0c\u5373\u4f7f\u4f7f\u7528\u566a\u58f0\u6807\u7b7e\u8bad\u7ec3\u4e5f\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73ca\u745a\u7901\u662f\u91cd\u8981\u4f46\u8106\u5f31\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u9700\u8981\u7cbe\u786e\u7684\u5927\u89c4\u6a21\u6620\u5c04\u4ee5\u8fdb\u884c\u6709\u6548\u4fdd\u62a4\u3002\u73b0\u6709\u7684\u5168\u7403\u4ea7\u54c1\u5982Allen Coral Atlas\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u8fb9\u754c\u5212\u5206\u7684\u533a\u57df\u3002", "method": "UKANFormer\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u57fa\u4e8eUKAN\u67b6\u6784\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u52a0\u5165\u4e86\u5168\u5c40-\u5c40\u90e8\u53d8\u6362\u5668\uff08GL-Trans\uff09\u5757\uff0c\u4ee5\u63d0\u53d6\u5168\u5c40\u8bed\u4e49\u7ed3\u6784\u548c\u5c40\u90e8\u8fb9\u754c\u7ec6\u8282\u3002", "result": "UKANFormer\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e8667.00%\u7684\u73ca\u745a\u7c7bIoU\u548c83.98%\u7684\u50cf\u7d20\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u76f8\u540c\u566a\u58f0\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u7684\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "UKANFormer\u6311\u6218\u4e86\u6570\u636e\u8d28\u91cf\u76f4\u63a5\u9650\u5236\u6a21\u578b\u6027\u80fd\u7684\u89c2\u70b9\uff0c\u8868\u660e\u67b6\u6784\u8bbe\u8ba1\u53ef\u4ee5\u7f13\u89e3\u6807\u7b7e\u566a\u58f0\uff0c\u5e76\u5728\u4e0d\u5b8c\u7f8e\u7684\u76d1\u7763\u4e0b\u652f\u6301\u53ef\u6269\u5c55\u7684\u6620\u5c04\u3002"}}
{"id": "2510.17309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17309", "abs": "https://arxiv.org/abs/2510.17309", "authors": ["Thorsten Fr\u00f6hlich", "Tim Schlippe"], "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "comment": null, "summary": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation.", "AI": {"tldr": "RubiSCoT\u662f\u4e00\u4e2aAI\u652f\u6301\u7684\u8bba\u6587\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528NLP\u6280\u672f\u63d0\u4f9b\u4e00\u81f4\u3001\u9ad8\u6548\u7684\u8bc4\u4f30\uff0c\u4f18\u5316\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8bba\u6587\u8bc4\u4f30\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u8017\u65f6\u4e14\u53d7\u8bc4\u4f30\u8005\u4e3b\u89c2\u6027\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u7ed3\u5408\u521d\u6b65\u8bc4\u4f30\u3001\u591a\u7ef4\u8bc4\u4f30\u3001\u5185\u5bb9\u63d0\u53d6\u3001\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u5206\u548c\u8be6\u7ec6\u62a5\u544a\u3002", "result": "RubiSCoT\u6846\u67b6\u8bbe\u8ba1\u5e76\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u5b66\u672f\u8bc4\u4f30\u6d41\u7a0b\u4e00\u81f4\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "RubiSCoT\u6846\u67b6\u901a\u8fc7AI\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u5b66\u672f\u8bba\u6587\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u671b\u4f18\u5316\u4f20\u7edf\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2510.16732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16732", "abs": "https://arxiv.org/abs/2510.16732", "authors": ["Xinqing Li", "Xin He", "Le Zhang", "Yun Liu"], "title": "A Comprehensive Survey on World Models for Embodied AI", "comment": "https://github.com/Li-Zn-H/AwesomeWorldModels", "summary": "Embodied AI requires agents that perceive, act, and anticipate how actions\nreshape future world states. World models serve as internal simulators that\ncapture environment dynamics, enabling forward and counterfactual rollouts to\nsupport perception, prediction, and decision making. This survey presents a\nunified framework for world models in embodied AI. Specifically, we formalize\nthe problem setting and learning objectives, and propose a three-axis taxonomy\nencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)\nTemporal Modeling, Sequential Simulation and Inference vs. Global Difference\nPrediction; (3) Spatial Representation, Global Latent Vector, Token Feature\nSequence, Spatial Latent Grid, and Decomposed Rendering Representation. We\nsystematize data resources and metrics across robotics, autonomous driving, and\ngeneral video settings, covering pixel prediction quality, state-level\nunderstanding, and task performance. Furthermore, we offer a quantitative\ncomparison of state-of-the-art models and distill key open challenges,\nincluding the scarcity of unified datasets and the need for evaluation metrics\nthat assess physical consistency over pixel fidelity, the trade-off between\nmodel performance and the computational efficiency required for real-time\ncontrol, and the core modeling difficulty of achieving long-horizon temporal\nconsistency while mitigating error accumulation. Finally, we maintain a curated\nbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u4e86\u95ee\u9898\u8bbe\u7f6e\u548c\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u8f74\u5206\u7c7b\u6cd5\u3002\u540c\u65f6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u5177\u8eabAI\u4e2d\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u51b3\u7b56\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u73af\u5883\u52a8\u6001\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5f62\u5f0f\u5316\u95ee\u9898\u8bbe\u7f6e\u548c\u5b66\u4e60\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u8f74\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u529f\u80fd\u3001\u65f6\u95f4\u5efa\u6a21\u548c\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u5316\u4e86\u6570\u636e\u8d44\u6e90\u548c\u6307\u6807\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9a\u91cf\u6bd4\u8f83\uff0c\u6307\u51fa\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u96c6\u7f3a\u4e4f\u7edf\u4e00\u6027\u548c\u8bc4\u4f30\u6307\u6807\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.16751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16751", "abs": "https://arxiv.org/abs/2510.16751", "authors": ["Erik Riise", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling", "comment": null, "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.", "AI": {"tldr": "\u79bb\u6563\u81ea\u56de\u5f52\u6a21\u578b\u7ed3\u5408\u675f\u641c\u7d22\u7b56\u7565\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c2B\u53c2\u6570\u6a21\u578b\u8d85\u8d8a12B\u6269\u6563\u6a21\u578b\uff0c\u8bc1\u660e\u67b6\u6784\u8bbe\u8ba1\u5bf9\u63a8\u7406\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5c3d\u7ba1\u641c\u7d22\u7b56\u7565\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u79bb\u6563\u81ea\u56de\u5f52\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u641c\u7d22\u7b56\u7565\u63d0\u5347\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "method": "\u672c\u6587\u91c7\u7528\u79bb\u6563\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u675f\u641c\u7d22\u7b56\u7565\u8fdb\u884c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u79bb\u6563\u4ee4\u724c\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u65e9\u671f\u526a\u679d\u548c\u8ba1\u7b97\u91cd\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c2B\u53c2\u6570\u7684\u79bb\u6563\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u675f\u641c\u7d22\u7b56\u7565\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e12B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002\u9a8c\u8bc1\u5668\u5206\u6790\u63ed\u793a\u4e86\u901f\u5ea6\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u67b6\u6784\uff08\u4e0d\u4ec5\u662f\u89c4\u6a21\uff09\u5bf9\u4e8e\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u63a8\u7406\u65f6\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u79bb\u6563\u7684\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u6709\u6548\u7684\u641c\u7d22\u7b56\u7565\uff08\u5982\u675f\u641c\u7d22\uff09\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa$\\texttt{FBI}_\\texttt{LTL}$\uff0c\u4e00\u79cd\u57fa\u4e8eLTL\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u751f\u6210\u8bed\u4e49\u591a\u6837\u7684\u8ba1\u5212\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u57fa\u4e8e\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u751f\u6210\u7684\u5355\u4e00\u8ba1\u5212\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u7684\u504f\u597d\uff0c\u4e14\u73b0\u6709\u591a\u6837\u5316\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8bed\u4e49\u591a\u6837\u6027\u9700\u6c42\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86$\\texttt{FBI}_\\texttt{LTL}$\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u57fa\u4e8e\u6a21\u62df\u7684\u89c4\u5212\u95ee\u9898\u8bbe\u8ba1\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\u3002\u5b83\u5229\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u57fa\u4e8eLTL\u7684\u591a\u6837\u6027\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u641c\u7d22\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$\\texttt{FBI}_\\texttt{LTL}$\u751f\u6210\u7684\u8ba1\u5212\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5177\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u786e\u7acb\u4e86\u5728\u57fa\u4e8e\u6a21\u62df\u7684\u73af\u5883\u4e2d\u8bed\u4e49\u5f15\u5bfc\u591a\u6837\u5316\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5931\u6548\u7684\u73b0\u5b9e\u3001\u975e\u7b26\u53f7\u9886\u57df\u4e2d\u521b\u65b0\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.16752", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16752", "abs": "https://arxiv.org/abs/2510.16752", "authors": ["Ivan Molodetskikh", "Kirill Malyshev", "Mark Mirgaleev", "Nikita Zagainov", "Evgeney Bogatyrev", "Dmitriy Vatolin"], "title": "Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution", "comment": null, "summary": "Generative image super-resolution (SR) is rapidly advancing in visual quality\nand detail restoration. As the capacity of SR models expands, however, so does\ntheir tendency to produce artifacts: incorrect, visually disturbing details\nthat reduce perceived quality. Crucially, their perceptual impact varies: some\nartifacts are barely noticeable while others strongly degrade the image. We\nargue that artifacts should be characterized by their prominence to human\nobservers rather than treated as uniform binary defects. Motivated by this, we\npresent a novel dataset of 1302 artifact examples from 11 contemporary image-SR\nmethods, where each artifact is paired with a crowdsourced prominence score.\nBuilding on this dataset, we train a lightweight regressor that produces\nspatial prominence heatmaps and outperforms existing methods at detecting\nprominent artifacts. We release the dataset and code to facilitate\nprominence-aware evaluation and mitigation of SR artifacts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u8bc4\u4f30SR\u4f2a\u5f71\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u56de\u5f52\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u5f71\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6a21\u578b\u7684\u5bb9\u91cf\u6269\u5927\uff0c\u5176\u4ea7\u751f\u4f2a\u5f71\u7684\u8d8b\u52bf\u4e5f\u968f\u4e4b\u589e\u52a0\u3002\u8fd9\u4e9b\u4f2a\u5f71\u7684\u611f\u77e5\u5f71\u54cd\u5404\u4e0d\u76f8\u540c\uff0c\u56e0\u6b64\u4f5c\u8005\u8ba4\u4e3a\u5e94\u6839\u636e\u5176\u5bf9\u4eba\u773c\u89c2\u5bdf\u8005\u7684\u663e\u8457\u6027\u6765\u8868\u5f81\u4f2a\u5f71\uff0c\u800c\u975e\u5c06\u5176\u89c6\u4e3a\u7edf\u4e00\u7684\u4e8c\u8fdb\u5236\u7f3a\u9677\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1302\u4e2a\u4f2a\u5f71\u793a\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4f2a\u5f71\u90fd\u4e0e\u4f17\u5305\u663e\u8457\u6027\u8bc4\u5206\u914d\u5bf9\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u4ed6\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u56de\u5f52\u5668\u6765\u751f\u6210\u7a7a\u95f4\u663e\u8457\u6027\u70ed\u56fe\u3002", "result": "\u8bad\u7ec3\u7684\u56de\u5f52\u5668\u5728\u68c0\u6d4b\u663e\u8457\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u4e86\u7a7a\u95f4\u663e\u8457\u6027\u70ed\u56fe\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u56de\u5f52\u5668\uff0c\u7528\u4e8e\u751f\u6210\u7a7a\u95f4\u663e\u8457\u6027\u70ed\u56fe\uff0c\u5e76\u5728\u68c0\u6d4b\u663e\u8457\u4f2a\u5f71\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u4ed6\u4eec\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u57fa\u4e8e\u663e\u8457\u6027\u7684SR\u4f2a\u5f71\u8bc4\u4f30\u548c\u7f13\u89e3\u3002"}}
{"id": "2510.17450", "categories": ["cs.AI", "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"], "pdf": "https://arxiv.org/pdf/2510.17450", "abs": "https://arxiv.org/abs/2510.17450", "authors": ["Johan Schubert", "Farzad Kamrani", "Tove Gustavi"], "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions", "comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada", "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u5b9e\u73b0\u667a\u80fd\u4ee3\u7406\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u5e73\u8861\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u5728\u63a2\u7d22\u5e7f\u9614\u5730\u7406\u533a\u57df\u4e0e\u8ddf\u8e2a\u5df2\u8bc6\u522b\u76ee\u6807\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u548c\u9ad8\u65af\u4f20\u611f\u5668\u6a21\u578b\u7684\u8bc1\u636e\u5730\u56fe\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u65b9\u6cd5\u66f4\u65b0\u540e\u9a8c\u6982\u7387\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u81ea\u7531\u80fd\u8ba1\u7b97\u6307\u5bfc\u4ee3\u7406\u79fb\u52a8\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5f15\u5bfc\u4ee3\u7406\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u548c\u53d8\u5206\u81ea\u7531\u80fd\u6700\u5c0f\u5316\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u667a\u80fd\u4ee3\u7406\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u6709\u6548\u7ef4\u62a4\u4e86\u5730\u7406\u533a\u57df\u7684\u5171\u540c\u4f5c\u6218\u56fe\u666f\u3002"}}
{"id": "2510.16765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16765", "abs": "https://arxiv.org/abs/2510.16765", "authors": ["Shengyu Zhu", "Fan", "Fuxuan Zhang"], "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement", "comment": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral", "summary": "Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.", "AI": {"tldr": "WaMaIR\u901a\u8fc7GMWTConvs\u3001MCAM\u548cMTELoss\u63d0\u5347\u4e86\u7eb9\u7406\u7ec6\u8282\u6062\u590d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CNN-based\u65b9\u6cd5\u56e0\u611f\u53d7\u91ce\u5c0f\u548c\u7f3a\u4e4f\u901a\u9053\u7279\u5f81\u5efa\u6a21\uff0c\u96be\u4ee5\u5145\u5206\u6062\u590d\u7eb9\u7406\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86WaMaIR\u6846\u67b6\uff0c\u5305\u62ecGlobal Multiscale Wavelet Transform Convolutions (GMWTConvs)\u7528\u4e8e\u6269\u5927\u611f\u53d7\u91ce\uff0cMamba-Based Channel-Aware Module (MCAM)\u7528\u4e8e\u6355\u6349\u7279\u5f81\u901a\u9053\u7684\u957f\u7a0b\u4f9d\u8d56\uff0c\u4ee5\u53caMultiscale Texture Enhancement Loss (MTELoss)\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u4fdd\u7559\u7eb9\u7406\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660eWaMaIR\u5728\u56fe\u50cf\u6062\u590d\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WaMaIR\u6846\u67b6\u901a\u8fc7\u5f15\u5165GMWTConvs\u3001MCAM\u548cMTELoss\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u7684\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u6548\u679c\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegie\u0144"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.", "AI": {"tldr": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662f\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u7814\u7a76\u901a\u8fc7\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u901a\u5e38\u5c06\u8fc7\u53bb\u7684\u6848\u4f8b\u7ed3\u679c\u89c6\u4e3a\u7edd\u5bf9\u771f\u7406\uff0c\u4f46\u5ffd\u7565\u4e86\u4eba\u4e3a\u5e72\u9884\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u7684\u6848\u4f8b\uff0c\u7814\u7a76\u4e86\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6807\u7b7e\u7684\u6784\u5efa\u65b9\u5f0f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u3002", "conclusion": "\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u88ab\u7eb3\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8003\u8651\u8303\u56f4\u3002"}}
{"id": "2510.16772", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16772", "abs": "https://arxiv.org/abs/2510.16772", "authors": ["Thuy Phuong Vu", "Dinh-Cuong Hoang", "Minhhuy Le", "Phan Xuan Tan"], "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning", "comment": null, "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git", "AI": {"tldr": "\u63d0\u51faRegion in Context\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5b64\u7acb\u5904\u7406\u533a\u57df\u5bfc\u81f4\u7684\u8fde\u8d2f\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u50cf\u533a\u57df\u65f6\u5b64\u7acb\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7f16\u8f91\u4e0d\u4e00\u81f4\u3001\u8fc7\u6e21\u4e0d\u81ea\u7136\u6216\u6574\u4f53\u8fde\u8d2f\u6027\u4e27\u5931\u3002", "method": "\u5f15\u5165\u53cc\u7ea7\u5f15\u5bfc\u673a\u5236\uff0c\u7ed3\u5408\u5168\u56fe\u50cf\u4e0a\u4e0b\u6587\u548c\u8be6\u7ec6\u533a\u57df\u63cf\u8ff0\uff0c\u540c\u65f6\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u573a\u666f\u7ea7\u63cf\u8ff0\uff0c\u6307\u5bfc\u5c40\u90e8\u4fee\u6539\u548c\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u8fde\u8d2f\u4e14\u7b26\u5408\u6307\u4ee4\u7684\u7f16\u8f91\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684Region in Context\u6846\u67b6\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u534f\u8c03\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7ed3\u679c\u7684\u8fde\u8d2f\u6027\u548c\u6307\u4ee4\u5bf9\u9f50\u6027\u3002"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "MIRAGE\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u591a\u6a21\u6001\u9a8c\u8bc1\u4efb\u52a1\u5e76\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u7f51\u7edc\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6027\u80fd\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u5e73\u53f0\u4e0a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u901f\u5ea6\u5feb\u3001\u624b\u52a8\u6838\u67e5\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u76d1\u7763\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "MIRAGE\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u3001\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u68c0\u67e5\u4ee5\u53ca\u6821\u51c6\u5224\u65ad\u6a21\u5757\u3002", "result": "\u5728MMFakeBench\u9a8c\u8bc1\u96c6\u4e0a\uff0cMIRAGE\u4e0eGPT-4o-mini\u7ec4\u5408\u8fbe\u523081.65% F1\u548c75.1%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u7ebf\uff08GPT-4V\u4e0eMMD-Agent\u7ec4\u5408\u768474.0% F1\uff09\u3002", "conclusion": "\u5206\u89e3\u7684\u4ee3\u7406\u63a8\u7406\u4e0e\u7f51\u7edc\u68c0\u7d22\u53ef\u4ee5\u5728\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3002"}}
{"id": "2510.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16776", "abs": "https://arxiv.org/abs/2510.16776", "authors": ["Mingzheng Zhang", "Jinfeng Gao", "Dan Xu", "Jiangrui Yu", "Yuhan Qiao", "Lan Chen", "Jin Tang", "Xiao Wang"], "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation", "comment": null, "summary": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "EMRRG\u6846\u67b6\u5229\u7528\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3Mamba\u7f51\u7edc\uff0c\u7ed3\u5408SSM\u89c6\u89c9\u9aa8\u5e72\u548c\u6df7\u5408\u89e3\u7801\u5668LLM\uff0c\u663e\u8457\u63d0\u5347X\u5c04\u7ebf\u62a5\u544a\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709MRG\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56LLM\uff0c\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u9ad8\u7ea7\u5fae\u8c03\u6280\u672f\u63a2\u7d22\u6709\u9650\uff0c\u4e14\u5ffd\u89c6\u4e86\u975eTransformer\u67b6\u6784\uff08\u5982Mamba\u7f51\u7edc\uff09\u7684\u6f5c\u529b\u3002", "method": "EMRRG\u6846\u67b6\u5c06X\u5c04\u7ebf\u56fe\u50cf\u5206\u5757\u5e76\u6807\u8bb0\u5316\uff0c\u901a\u8fc7\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u91c7\u7528Partial LoRA\u8fdb\u884c\u4f18\u5316\uff0c\u7ed3\u5408\u6df7\u5408\u89e3\u7801\u5668\u7684LLM\u751f\u6210\u62a5\u544a\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684EMRRG\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Mamba\u7f51\u7edc\uff0c\u5728X\u5c04\u7ebf\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u9700\u8981\u7406\u89e3\u95ee\u9898\u5b9a\u4e49\u4e0e\u89e3\u51b3\u65b9\u6848\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u51c6\u786e\u7684\u8bcd\u6c47\u9884\u6d4b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u8fd9\u79cd\u80fd\u529b\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5176\u80fd\u529b\u84b8\u998f\u5230\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5c0f\u6a21\u578b\u6a21\u62df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "result": "\u5728MBPP\u3001MBPP Plus\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728pass@1\u3001\u5e73\u5747\u6570\u636e\u6d41\u548c\u5e73\u5747\u8bed\u6cd5\u5339\u914d\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16777", "abs": "https://arxiv.org/abs/2510.16777", "authors": ["Junbo Li", "Weimin Yuan", "Yinuo Wang", "Yue Zeng", "Shihao Shu", "Cai Meng", "Xiangzhi Bai"], "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation", "comment": null, "summary": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer\nvision, and current research typically predicts the 6D pose by establishing\ncorrespondences between 2D image features and 3D model features. However, these\nmethods often face difficulties with textureless objects and varying\nillumination conditions. To overcome these limitations, we propose GS2POSE, a\nnovel approach for 6D object pose estimation. GS2POSE formulates a pose\nregression algorithm inspired by the principles of Bundle Adjustment (BA). By\nleveraging Lie algebra, we extend the capabilities of 3DGS to develop a\npose-differentiable rendering pipeline, which iteratively optimizes the pose by\ncomparing the input image to the rendered image. Additionally, GS2POSE updates\ncolor parameters within the 3DGS model, enhancing its adaptability to changes\nin illumination. Compared to previous models, GS2POSE demonstrates accuracy\nimprovements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and\nLineMod datasets, respectively.", "AI": {"tldr": "GS2POSE\u662f\u4e00\u79cd\u65b0\u578b6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7Bundle Adjustment\u548cLie\u4ee3\u6570\u4f18\u5316\u59ff\u6001\uff0c\u9002\u5e94\u5149\u7167\u53d8\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e2D\u56fe\u50cf\u7279\u5f81\u4e0e3D\u6a21\u578b\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u9762\u5bf9\u7eb9\u7406less\u7269\u4f53\u548c\u53d8\u5316\u7684\u5149\u7167\u6761\u4ef6\u65f6\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86GS2POSE\u3002", "method": "GS2POSE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBundle Adjustment\uff08BA\uff09\u539f\u7406\u7684\u59ff\u6001\u56de\u5f52\u7b97\u6cd5\uff0c\u5229\u7528Lie\u4ee3\u6570\u6269\u5c553DGS\u7684\u80fd\u529b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u59ff\u6001\u53ef\u5fae\u6e32\u67d3\u7ba1\u9053\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u56fe\u50cf\u4e0e\u6e32\u67d3\u56fe\u50cf\u7684\u6bd4\u8f83\u6765\u4f18\u5316\u59ff\u6001\u3002\u6b64\u5916\uff0cGS2POSE\u8fd8\u66f4\u65b0\u4e863DGS\u6a21\u578b\u4e2d\u7684\u989c\u8272\u53c2\u6570\uff0c\u589e\u5f3a\u4e86\u5176\u5bf9\u5149\u7167\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002", "result": "GS2POSE\u5728T-LESS\u3001LineMod-Occlusion\u548cLineMod\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u5ea6\u5206\u522b\u63d0\u5347\u4e861.4%\u30012.8%\u548c2.5%\u3002", "conclusion": "GS2POSE\u5728T-LESS\u3001LineMod-Occlusion\u548cLineMod\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e861.4%\u30012.8%\u548c2.5%\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17614", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17614", "abs": "https://arxiv.org/abs/2510.17614", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration", "comment": null, "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.", "AI": {"tldr": "OG-Rank\u662f\u4e00\u4e2a\u4f4e\u5ef6\u8fdf\u7684\u4e34\u5e8a\u8ba2\u5355\u6392\u540d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u89e3\u7801\u5668\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u7075\u6d3b\u7684\u6392\u540d\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u5b9e\u65f6\u5de5\u4f5c\u5e76\u80fd\u8bc1\u660e\u5176\u9009\u62e9\u7684\u6392\u540d\u7cfb\u7edf\u3002", "method": "OG-Rank\u91c7\u7528\u5355\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408\u7b2c\u4e00\u4ee4\u724c\u8bc4\u5206\u4fe1\u53f7\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u5bf9\u6240\u6709\u5019\u9009\u8005\u8fdb\u884c\u4e00\u6b21\u8bc4\u5206\uff0c\u5e76\u5728\u5217\u8868\u771f\u6b63\u6a21\u7cca\u65f6\u751f\u6210\u7b80\u77ed\u7684\u7ed3\u6784\u5316\u7406\u7531\u3002", "result": "OG-Rank\u5728\u5feb\u901f\u8def\u5f84\u4e0b\u8868\u73b0\u826f\u597d\uff08Recall@1~0.45\uff0cnDCG@20~0.625\uff09\uff0c\u5e76\u5728\u95e8\u6fc0\u6d3b\u65f6\u8fdb\u4e00\u6b65\u6539\u5584\uff08Recall@1~0.56\uff0cnDCG@20~0.699\uff0c\u95e8\u6fc0\u6d3b\u7387\u4e3a45%\uff09\u3002", "conclusion": "OG-Rank\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u4f4e\u5ef6\u8fdf\u3001\u89e3\u7801\u5668\u57fa\u7840\u7684\u91cd\u65b0\u6392\u5e8f\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\u548c\u96c6\u4e2d\u8bad\u7ec3\u4e8e\u56f0\u96be\u6848\u4f8b\u7684\u8bfe\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5728\u4e34\u5e8a\u8ba2\u5355\u9009\u62e9\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u8868\u73b0\u3002"}}
{"id": "2510.16781", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16781", "abs": "https://arxiv.org/abs/2510.16781", "authors": ["Shihao Ji", "Zihui Song"], "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "comment": null, "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3VLM\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u89c6\u9891\u7ed3\u6784\u5206\u6790\uff0c\u5e76\u751f\u6210\u591a\u6a21\u6001\u6458\u8981\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u6846\u67b6\u5c06\u89c6\u9891\u7406\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9ad8\u7ef4\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u81ea\u76d1\u7763\u65f6\u7a7a\u805a\u7c7b\u95ee\u9898\u3002\u9996\u5148\u4f7f\u7528\u9884\u8bad\u7ec3VLM\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5c06\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u8bed\u4e49\u7279\u5f81\u8f68\u8ff9\uff0c\u7136\u540e\u91c7\u7528Kernel Temporal Segmentation\uff08KTS\uff09\u6280\u672f\u5206\u5272\u7279\u5f81\u6d41\uff0c\u6700\u540e\u901a\u8fc7\u65e0\u76d1\u7763\u5bc6\u5ea6\u805a\u7c7b\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\u5b8f\u89c2\u573a\u666f\u548c\u4e3b\u9898\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u3001\u591a\u6a21\u6001\u7684\u89c6\u9891\u5185\u5bb9\u6458\u8981\uff0c\u5c55\u793a\u4e86\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u9ad8\u6548\u89c6\u9891\u5206\u6790\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u5185\u5bb9\u7684\u96f6\u6837\u672c\u7ed3\u6784\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u8def\u5f84\u3002"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "LLMs\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4fe1\u606f\u5904\u7406\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u672a\u6765\u4e8b\u4ef6\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efaProphet Arena\u8bc4\u4f30\u57fa\u51c6\uff0c\u5c06\u9884\u6d4b\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u9636\u6bb5\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\u3002", "result": "\u8bb8\u591aLLMs\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5982\u6821\u51c6\u8bef\u5dee\u5c0f\u3001\u9884\u6d4b\u4fe1\u5fc3\u4e00\u81f4\u4e14\u6709\u5e02\u573a\u56de\u62a5\u6f5c\u529b\u3002", "conclusion": "LLMs\u5c55\u73b0\u51fa\u663e\u8457\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5173\u952e\u74f6\u9888\uff0c\u5982\u4e8b\u4ef6\u56de\u5fc6\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u53ca\u4fe1\u606f\u805a\u5408\u901f\u5ea6\u8f83\u6162\u3002"}}
{"id": "2510.16785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16785", "abs": "https://arxiv.org/abs/2510.16785", "authors": ["Jiazhen Liu", "Long Chen"], "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs", "comment": null, "summary": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.", "AI": {"tldr": "LENS\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u51bb\u7ed3\u7684MLLM\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u5934\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728MLLMs\u4e2d\u96c6\u6210\u50cf\u7d20\u7ea7\u5206\u5272\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u6a21\u578b\u4ee5\u4ea7\u751f\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u517c\u5bb9\u7684\u8f93\u51fa\uff0c\u8fd9\u4f1a\u6539\u53d8\u6a21\u578b\u7684\u8f93\u51fa\u7a7a\u95f4\u5e76\u635f\u5bb3\u5176\u5185\u5728\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "LENS\u901a\u8fc7\u5728\u5b8c\u5168\u51bb\u7ed3\u7684MLLM\u4e0a\u9644\u52a0\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u8bad\u7ec3\u7684\u5934\uff0c\u5229\u7528\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u5e76\u5c06\u5176\u63cf\u8ff0\u4e3a\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u76f4\u63a5\u517c\u5bb9\u7684\u70b9\u72b6\u7279\u5f81\u3002", "result": "LENS\u5728\u5206\u5272\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u4f18\u4e8e\u57fa\u4e8e\u91cd\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u4e86MLLM\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LENS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u9644\u52a0\u8bbe\u8ba1\u6269\u5c55MLLMs\u7684\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u771f\u6b63\u591a\u529f\u80fd\u7684\u7edf\u4e00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u901a\u8fc7MAIDs\u6846\u67b6\u63d0\u51fa\u76ee\u6807\u5e72\u9884\u8303\u5f0f\uff0c\u5229\u7528PSI\u6280\u672f\u5b9e\u73b0\u56e0\u679c\u6548\u5e94\u6700\u5927\u5316\uff0c\u6709\u6548\u89e3\u51b3MARL\u4e2d\u7684\u5168\u5c40\u6307\u5bfc\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u5168\u5c40\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\u53ca\u7f3a\u4e4f\u6613\u7528\u7814\u7a76\u5de5\u5177\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe\uff08MAIDs\uff09\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\uff0c\u8bbe\u8ba1\u76ee\u6807\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7PSI\u6280\u672f\u5b9e\u73b0\u56e0\u679c\u6548\u5e94\u6700\u5927\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u76ee\u6807\u5e72\u9884\u8303\u5f0f\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5173\u8054\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe\uff08MAIDs\uff09\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u76ee\u6807\u5e72\u9884\u7684\u65b0\u4ea4\u4e92\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u9884\u7b56\u7565\u5e72\u9884\uff08PSI\uff09\u6280\u672f\u5b9e\u73b0\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u76ee\u6807\u5e72\u9884\u7684\u6709\u6548\u6027\u53ca\u5173\u8054\u56fe\u5206\u6790\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2510.16790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16790", "abs": "https://arxiv.org/abs/2510.16790", "authors": ["Sara Hatami Rostami", "Behrooz Nasihatkon"], "title": "Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry", "comment": "7 pages, 3 figures", "summary": "This paper presents a fully unsupervised approach for binary road\nsegmentation (road vs. non-road), eliminating the reliance on costly manually\nlabeled datasets. The method leverages scene geometry and temporal cues to\ndistinguish road from non-road regions. Weak labels are first generated from\ngeometric priors, marking pixels above the horizon as non-road and a predefined\nquadrilateral in front of the vehicle as road. In a refinement stage, temporal\nconsistency is enforced by tracking local feature points across frames and\npenalizing inconsistent label assignments using mutual information\nmaximization. This enhances both precision and temporal stability. On the\nCityscapes dataset, the model achieves an Intersection-over-Union (IoU) of\n0.82, demonstrating high accuracy with a simple design. These findings\ndemonstrate the potential of combining geometric constraints and temporal\nconsistency for scalable unsupervised road segmentation in autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u9053\u8def\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u51e0\u4f55\u548c\u65f6\u5e8f\u7ebf\u7d22\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.82 IoU\u3002", "motivation": "\u6d88\u9664\u5bf9\u6602\u8d35\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u4e8c\u8fdb\u5236\u9053\u8def\u5206\u5272\uff08\u9053\u8def\u4e0e\u975e\u9053\u8def\uff09\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u573a\u666f\u51e0\u4f55\u548c\u65f6\u5e8f\u7ebf\u7d22\u6765\u533a\u5206\u9053\u8def\u548c\u975e\u9053\u8def\u533a\u57df\u3002\u9996\u5148\u4ece\u51e0\u4f55\u5148\u9a8c\u751f\u6210\u5f31\u6807\u7b7e\uff0c\u6807\u8bb0\u5730\u5e73\u7ebf\u4ee5\u4e0a\u50cf\u7d20\u4e3a\u975e\u9053\u8def\uff0c\u8f66\u8f86\u524d\u65b9\u9884\u5b9a\u4e49\u56db\u8fb9\u5f62\u4e3a\u9053\u8def\u3002\u5728\u7ec6\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u8de8\u5e27\u8ddf\u8e2a\u5c40\u90e8\u7279\u5f81\u70b9\u5e76\u5229\u7528\u4e92\u4fe1\u606f\u6700\u5927\u5316\u60e9\u7f5a\u4e0d\u4e00\u81f4\u7684\u6807\u7b7e\u5206\u914d\uff0c\u589e\u5f3a\u7cbe\u5ea6\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e860.82\u7684Intersection-over-Union (IoU)\uff0c\u5c55\u793a\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u7b80\u5355\u8bbe\u8ba1\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u9053\u8def\u5206\u5272\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "CAM\u548cHyCAM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u63d0\u5347LLM\u591a\u4efb\u52a1\u9002\u5e94\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u53473.65%\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u77e5\u8bc6\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u7684\u540c\u65f6\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86Contextual Attention Modulation (CAM)\u673a\u5236\u548cHybrid Contextual Attention Modulation (HyCAM)\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u7684\u5168\u53c2\u6570CAM\u6a21\u5757\u548c\u591a\u4e2a\u8f7b\u91cf\u7ea7CAM\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u7b56\u7565\u5b9e\u73b0\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u7b49\u5f02\u6784\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%\u3002", "conclusion": "CAM\u548cHyCAM\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6027\u80fd\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%\uff0c\u540c\u65f6\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002"}}
{"id": "2510.16791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16791", "abs": "https://arxiv.org/abs/2510.16791", "authors": ["Chengxuan Zhu", "Shuchen Weng", "Jiacong Fang", "Peixuan Zhang", "Si Li", "Chao Xu", "Boxin Shi"], "title": "Personalized Image Filter: Mastering Your Photographic Style", "comment": null, "summary": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/", "AI": {"tldr": "PIF\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u6587\u672c\u53cd\u8f6c\u6280\u672f\uff0c\u6210\u529f\u5b66\u4e60\u548c\u8f6c\u79fb\u6444\u5f71\u98ce\u683c\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u5de5\u4f5c\u65e0\u6cd5\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6444\u5f71\u6982\u5ff5\u6216\u65e0\u6cd5\u4fdd\u7559\u5185\u5bb9\u56fe\u50cf\u5185\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0cPIF\u901a\u8fc7\u5b66\u4e60\u6444\u5f71\u6982\u5ff5\u7684\u5e73\u5747\u5916\u89c2\u53ca\u5982\u4f55\u6839\u636e\u6587\u672c\u63d0\u793a\u8c03\u6574\u5b83\u4eec\uff0c\u7ed3\u5408\u6587\u672c\u53cd\u8f6c\u6280\u672f\u4f18\u5316\u6444\u5f71\u6982\u5ff5\u7684\u63d0\u793a\u3002", "result": "PIF\u80fd\u591f\u6709\u6548\u5b66\u4e60\u548c\u8f6c\u79fb\u53c2\u8003\u56fe\u50cf\u7684\u6444\u5f71\u98ce\u683c\u3002", "conclusion": "PIF\u5728\u63d0\u53d6\u548c\u8f6c\u79fb\u5404\u79cd\u6444\u5f71\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0VLM\u5b58\u5728\u2018\u770b\u89c1\u4f46\u4e0d\u76f8\u4fe1\u2019\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u8c03\u6df1\u5c42\u8bc1\u636e\u533a\u57df\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7a76VLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u95ee\u7b54\uff09\u4e2d\u5931\u8d25\u7684\u539f\u56e0\uff0c\u662f\u672a\u611f\u77e5\u8bc1\u636e\u8fd8\u662f\u672a\u6709\u6548\u5229\u7528\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u9010\u5c42\u6ce8\u610f\u529b\u52a8\u6001\u5206\u6790\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u800c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\u3002", "result": "\u5e72\u9884\u65b9\u6cd5\u5728\u591a\u4e2aVLM\u5bb6\u65cf\uff08\u5982LLaVA\u3001Qwen\u3001Gemma\u548cInternVL\uff09\u4e2d\u4e00\u81f4\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86VLM\u5185\u90e8\u7f16\u7801\u4f46\u672a\u5145\u5206\u5229\u7528\u8bc1\u636e\u7684\u73b0\u8c61\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u4f46\u672a\u5145\u5206\u5229\u7528\uff0c\u901a\u8fc7\u663e\u5f0f\u5316\u8fd9\u4e9b\u4fe1\u53f7\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347VLM\u7684\u8bca\u65ad\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16822", "abs": "https://arxiv.org/abs/2510.16822", "authors": ["Yahia Battach", "Abdulwahab Felemban", "Faizan Farooq Khan", "Yousef A. Radwan", "Xiang Li", "Fabio Marchese", "Sara Beery", "Burton H. Jones", "Francesca Benzoni", "Mohamed Elhoseiny"], "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification", "comment": null, "summary": "Coral reefs are rapidly declining due to anthropogenic pressures such as\nclimate change, underscoring the urgent need for scalable, automated\nmonitoring. We introduce ReefNet, a large public coral reef image dataset with\npoint-label annotations mapped to the World Register of Marine Species (WoRMS).\nReefNet aggregates imagery from 76 curated CoralNet sources and an additional\nsite from Al Wajh in the Red Sea, totaling approximately 925000 genus-level\nhard coral annotations with expert-verified labels. Unlike prior datasets,\nwhich are often limited by size, geography, or coarse labels and are not\nML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global\nscale to WoRMS. We propose two evaluation settings: (i) a within-source\nbenchmark that partitions each source's images for localized evaluation, and\n(ii) a cross-source benchmark that withholds entire sources to test domain\ngeneralization. We analyze both supervised and zero-shot classification\nperformance on ReefNet and find that while supervised within-source performance\nis promising, supervised performance drops sharply across domains, and\nperformance is low across the board for zero-shot models, especially for rare\nand visually similar genera. This provides a challenging benchmark intended to\ncatalyze advances in domain generalization and fine-grained coral\nclassification. We will release our dataset, benchmarking code, and pretrained\nmodels to advance robust, domain-adaptive, global coral reef monitoring and\nconservation.", "AI": {"tldr": "ReefNet\u662f\u4e00\u4e2a\u5168\u7403\u89c4\u6a21\u7684\u73ca\u745a\u7901\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u65e8\u5728\u63a8\u52a8\u9886\u57df\u6cdb\u5316\u548c\u73ca\u745a\u5206\u7c7b\u7814\u7a76\uff0c\u5c3d\u7ba1\u5f53\u524d\u6a21\u578b\u5728\u8de8\u57df\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73ca\u745a\u7901\u56e0\u6c14\u5019\u53d8\u5316\u7b49\u4eba\u4e3a\u538b\u529b\u8fc5\u901f\u8870\u9000\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "ReefNet\u6574\u5408\u4e8676\u4e2aCoralNet\u6765\u6e90\u548c\u4e00\u4e2a\u7ea2\u6d77Al Wajh\u7ad9\u70b9\u7684\u56fe\u50cf\uff0c\u63d0\u4f9b\u4e86\u7ea6925000\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5c5e\u7ea7\u786c\u73ca\u745a\u6807\u6ce8\uff0c\u5e76\u91c7\u7528\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u540c\u6e90\u57fa\u51c6\u548c\u8de8\u6e90\u57fa\u51c6\u3002", "result": "\u76d1\u7763\u5b66\u4e60\u5728\u540c\u6e90\u57fa\u51c6\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u6e90\u57fa\u51c6\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u96f6\u6837\u672c\u6a21\u578b\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u8868\u73b0\u5747\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u7a00\u6709\u548c\u89c6\u89c9\u76f8\u4f3c\u7684\u5c5e\u3002", "conclusion": "ReefNet\u6570\u636e\u96c6\u53ca\u5176\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u63a8\u52a8\u9886\u57df\u6cdb\u5316\u548c\u7ec6\u7c92\u5ea6\u73ca\u745a\u5206\u7c7b\u7684\u8fdb\u5c55\uff0c\u4e3a\u5168\u7403\u73ca\u745a\u7901\u76d1\u6d4b\u548c\u4fdd\u62a4\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2510.16832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16832", "abs": "https://arxiv.org/abs/2510.16832", "authors": ["Abdur Rahman", "Mohammad Marufuzzaman", "Jason Street", "Haifeng Wang", "Veera G. Gude", "Randy Buchanan"], "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction", "comment": null, "summary": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faAdaptMoist\u65b9\u6cd5\uff0c\u901a\u8fc7\u7eb9\u7406\u7279\u5f81\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6728\u6750\u788e\u7247\u6c34\u5206\u542b\u91cf\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u76f4\u63a5\u65b9\u6cd5\uff08\u5982\u70d8\u7bb1\u5e72\u71e5\uff09\u5904\u7406\u65f6\u95f4\u957f\u4e14\u7834\u574f\u6837\u672c\uff0c\u800c\u95f4\u63a5\u65b9\u6cd5\uff08\u5982\u8fd1\u7ea2\u5916\u5149\u8c31\u3001\u7535\u5bb9\u548c\u56fe\u50cf\u5206\u6790\uff09\u5728\u9762\u5bf9\u4e0d\u540c\u6765\u6e90\u7684\u6728\u6750\u788e\u7247\u65f6\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u51cf\u8f7b\u6e90\u53d8\u5f02\u6027\u5f71\u54cd\u7684\u7a33\u5065\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4e94\u79cd\u4e0d\u540c\u7684\u7eb9\u7406\u7279\u5f81\u7c7b\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptMoist\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u7eb9\u7406\u7279\u5f81\u5c06\u77e5\u8bc6\u4ece\u4e00\u4e2a\u6728\u6750\u788e\u7247\u6570\u636e\u6e90\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\u6570\u636e\u6e90\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u8c03\u6574\u4e92\u4fe1\u606f\u7684\u6a21\u578b\u4fdd\u5b58\u6807\u51c6\u3002", "result": "\u7ed3\u5408\u4e94\u79cd\u7eb9\u7406\u7279\u5f81\u7684\u7ec4\u5408\u5b9e\u73b0\u4e8695%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u7279\u5f81\u3002AdaptMoist\u65b9\u6cd5\u5c06\u8de8\u9886\u57df\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8623%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523080%\uff0c\u800c\u975e\u9002\u5e94\u6a21\u578b\u4ec5\u4e3a57%\u3002", "conclusion": "AdaptMoist\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u8de8\u9886\u57df\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6728\u6750\u788e\u7247\u6c34\u5206\u542b\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4f9d\u8d56\u6728\u6750\u788e\u7247\u7684\u884c\u4e1a\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16837", "abs": "https://arxiv.org/abs/2510.16837", "authors": ["Haofan Ren", "Qingsong Yan", "Ming Lu", "Rongfeng Lu", "Zunjie Zhu"], "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced\nneural fields, as it enables high-fidelity rendering with impressive visual\nquality. However, 3DGS has difficulty accurately representing surfaces. In\ncontrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian\ndisks. Despite advancements in geometric fidelity, rendering quality remains\ncompromised, highlighting the challenge of achieving both high-quality\nrendering and precise geometric structures. This indicates that optimizing both\ngeometric and rendering quality in a single training stage is currently\nunfeasible. To overcome this limitation, we present 2DGS-R, a new method that\nuses a hierarchical training approach to improve rendering quality while\nmaintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians\nwith the normal consistency regularization. Then 2DGS-R selects the 2D\nGaussians with inadequate rendering quality and applies a novel in-place\ncloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R\nmodel with opacity frozen. Experimental results show that compared to the\noriginal 2DGS, our method requires only 1\\% more storage and minimal additional\ntraining time. Despite this negligible overhead, it achieves high-quality\nrendering results while preserving fine geometric structures. These findings\nindicate that our approach effectively balances efficiency with performance,\nleading to improvements in both visual fidelity and geometric reconstruction\naccuracy.", "AI": {"tldr": "2DGS-R\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u548c\u539f\u4f4d\u514b\u9686\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\uff0c\u6548\u7387\u4e0e\u6027\u80fd\u517c\u987e\u3002", "motivation": "3DGS\u5728\u6e32\u67d3\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u96be\u4ee5\u51c6\u786e\u8868\u793a\u8868\u9762\uff0c\u800c2DGS\u867d\u7136\u63d0\u5347\u4e86\u51e0\u4f55\u4fdd\u771f\u5ea6\u4f46\u727a\u7272\u4e86\u6e32\u67d3\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4f18\u5316\u51e0\u4f55\u548c\u6e32\u67d3\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "2DGS-R\u91c7\u7528\u5206\u5c42\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u5bf9\u539f\u59cb2D\u9ad8\u65af\u8fdb\u884c\u6cd5\u7ebf\u4e00\u81f4\u6027\u6b63\u5219\u5316\u8bad\u7ec3\uff0c\u968f\u540e\u9009\u62e9\u6e32\u67d3\u8d28\u91cf\u4e0d\u8db3\u76842D\u9ad8\u65af\u8fdb\u884c\u539f\u4f4d\u514b\u9686\u64cd\u4f5c\u589e\u5f3a\uff0c\u6700\u540e\u51bb\u7ed3\u4e0d\u900f\u660e\u5ea6\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c2DGS-R\u4ec5\u589e\u52a01%\u7684\u5b58\u50a8\u548c\u5c11\u91cf\u8bad\u7ec3\u65f6\u95f4\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e32\u67d3\u5e76\u4fdd\u7559\u7cbe\u7ec6\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "2DGS-R\u65b9\u6cd5\u5728\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u51e0\u4f55\u91cd\u5efa\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.16854", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.5.4; I.4.6"], "pdf": "https://arxiv.org/pdf/2510.16854", "abs": "https://arxiv.org/abs/2510.16854", "authors": ["Akhila Kambhatla", "Taminul Islam", "Khaled R Ahmed"], "title": "ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification", "comment": "9 pages with 4 figures and 5 tables. This is a preprint submitted to\n  arXiv", "summary": "The escalating threat of weapon-related violence necessitates automated\ndetection systems capable of pixel-level precision for accurate threat\nassessment in real-time security applications. Traditional weapon detection\napproaches rely on object detection frameworks that provide only coarse\nbounding box localizations, lacking the fine-grained segmentation required for\ncomprehensive threat analysis. Furthermore, existing semantic segmentation\nmodels either sacrifice accuracy for computational efficiency or require\nexcessive computational resources incompatible with edge deployment scenarios.\nThis paper presents ArmFormer, a lightweight transformer-based semantic\nsegmentation framework that strategically integrates Convolutional Block\nAttention Module (CBAM) with MixVisionTransformer architecture to achieve\nsuperior accuracy while maintaining computational efficiency suitable for\nresource-constrained edge devices. Our approach combines CBAM-enhanced encoder\nbackbone with attention-integrated hamburger decoder to enable multi-class\nweapon segmentation across five categories: handgun, rifle, knife, revolver,\nand human. Comprehensive experiments demonstrate that ArmFormer achieves\nstate-of-the-art performance with 80.64% mIoU and 89.13% mFscore while\nmaintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M\nparameters, ArmFormer outperforms heavyweight models requiring up to 48x more\ncomputation, establishing it as the optimal solution for deployment on portable\nsecurity cameras, surveillance drones, and embedded AI accelerators in\ndistributed security infrastructure.", "AI": {"tldr": "ArmFormer\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408CBAM\u4e0eMixVisionTransformer\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u6b66\u5668\u68c0\u6d4b\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u6b66\u5668\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u7c97\u7565\u7684\u8fb9\u754c\u6846\u5b9a\u4f4d\uff0c\u4e14\u73b0\u6709\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fb9\u7f18\u90e8\u7f72\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u53c8\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ArmFormer\u7ed3\u5408\u4e86Convolutional Block Attention Module (CBAM)\u4e0eMixVisionTransformer\u67b6\u6784\uff0c\u901a\u8fc7CBAM\u589e\u5f3a\u7684\u7f16\u7801\u5668\u4e3b\u5e72\u548c\u6ce8\u610f\u529b\u96c6\u6210\u7684\u89e3\u7801\u5668\u5b9e\u73b0\u591a\u7c7b\u6b66\u5668\u5206\u5272\u3002", "result": "ArmFormer\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e8680.64%\u7684mIoU\u548c89.13%\u7684mFscore\uff0c\u540c\u65f6\u4fdd\u630182.26 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\uff0c\u4ec5\u97004.886G FLOPs\u548c3.66M\u53c2\u6570\u3002", "conclusion": "ArmFormer\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u7c7b\u591a\u79cd\u6b66\u5668\uff0c\u4e3a\u5b9e\u65f6\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16863", "abs": "https://arxiv.org/abs/2510.16863", "authors": ["Shujian Gao", "Yuan Wang", "Zekuan Yu"], "title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation", "comment": "14 pages, 5 figures", "summary": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully\nsupervised performance while sharply reducing annotation cost. Mainstream SSMIS\nmethods rely on \\emph{label-space consistency}, yet they overlook the equally\ncritical \\emph{representation-space alignment}. Without harmonizing latent\nfeatures, models struggle to learn representations that are both discriminative\nand spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment\nin Representation and Label spaces (BARL)}, a unified framework that couples\ntwo collaborative branches and enforces alignment in both spaces. For\nlabel-space alignment, inspired by co-training and multi-scale decoding, we\ndevise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively\nCognitive Bias Correction (PCBC)} to impose fine-grained cross-branch\nconsistency while mitigating error accumulation from coarse to fine scales. For\nrepresentation-space alignment, we conduct region-level and lesion-instance\nmatching between branches, explicitly capturing the fragmented, complex\npathological patterns common in medical imagery. Extensive experiments on four\npublic benchmarks and a proprietary CBCT dataset demonstrate that BARL\nconsistently surpasses state-of-the-art SSMIS methods. Ablative studies further\nvalidate the contribution of each component. Code will be released soon.", "AI": {"tldr": "BARL\u6846\u67b6\u901a\u8fc7\u53cc\u8fb9\u5bf9\u9f50\uff08\u8868\u5f81\u548c\u6807\u7b7e\u7a7a\u95f4\uff09\u63d0\u5347\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6807\u7b7e\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u8868\u5f81\u7a7a\u95f4\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u5b66\u4e60\u5230\u5224\u522b\u6027\u5f3a\u4e14\u7a7a\u95f4\u8fde\u8d2f\u7684\u8868\u793a\u3002", "method": "\u5f15\u5165BARL\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8def\u5f84\u6b63\u5219\u5316\uff08DPR\uff09\u548c\u6e10\u8fdb\u8ba4\u77e5\u504f\u5dee\u6821\u6b63\uff08PCBC\uff09\u7528\u4e8e\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ee5\u53ca\u533a\u57df\u7ea7\u548c\u75c5\u53d8\u5b9e\u4f8b\u5339\u914d\u7528\u4e8e\u8868\u5f81\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u548c\u4e00\u4e2a\u79c1\u6709CBCT\u6570\u636e\u96c6\u4e0a\uff0cBARL consistently surpasses state-of-the-art SSMIS methods\u3002", "conclusion": "BARL\u6846\u67b6\u901a\u8fc7\u53cc\u8fb9\u5bf9\u9f50\uff08\u8868\u5f81\u7a7a\u95f4\u548c\u6807\u7b7e\u7a7a\u95f4\uff09\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16865", "abs": "https://arxiv.org/abs/2510.16865", "authors": ["Yuyang Yu", "Zhengwei Chen", "Xuemiao Xu", "Lei Zhang", "Haoxin Yang", "Yongwei Nie", "Shengfeng He"], "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection", "comment": null, "summary": "3D anomaly detection in point-cloud data is critical for industrial quality\ncontrol, aiming to identify structural defects with high reliability. However,\ncurrent memory bank-based methods often suffer from inconsistent feature\ntransformations and limited discriminative capacity, particularly in capturing\nlocal geometric details and achieving rotation invariance. These limitations\nbecome more pronounced when registration fails, leading to unreliable detection\nresults. We argue that point-cloud registration plays an essential role not\nonly in aligning geometric structures but also in guiding feature extraction\ntoward rotation-invariant and locally discriminative representations. To this\nend, we propose a registration-induced, rotation-invariant feature extraction\nframework that integrates the objectives of point-cloud registration and\nmemory-based anomaly detection. Our key insight is that both tasks rely on\nmodeling local geometric structures and leveraging feature similarity across\nsamples. By embedding feature extraction into the registration learning\nprocess, our framework jointly optimizes alignment and representation learning.\nThis integration enables the network to acquire features that are both robust\nto rotations and highly effective for anomaly detection. Extensive experiments\non the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method\nconsistently outperforms existing approaches in effectiveness and\ngeneralizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u914d\u51c6\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u65b9\u6cd5\u5728\u7279\u5f81\u8f6c\u6362\u7684\u4e00\u81f4\u6027\u548c\u533a\u5206\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\u548c\u5b9e\u73b0\u65cb\u8f6c\u4e0d\u53d8\u6027\u65b9\u9762\u3002\u70b9\u4e91\u914d\u51c6\u4e0d\u4ec5\u5bf9\u9f50\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd8\u80fd\u5f15\u5bfc\u7279\u5f81\u63d0\u53d6\u671d\u5411\u65cb\u8f6c\u4e0d\u53d8\u548c\u5c40\u90e8\u533a\u5206\u6027\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u70b9\u4e91\u914d\u51c6\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u63d0\u53d6\u5d4c\u5165\u914d\u51c6\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u8054\u5408\u4f18\u5316\u5bf9\u9f50\u548c\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728Anomaly-ShapeNet\u548cReal3D-AD\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u914d\u51c6\u7684\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u6846\u67b6\u57283D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16870", "abs": "https://arxiv.org/abs/2510.16870", "authors": ["Yudan Ren", "Xinlong Wang", "Kexin Wang", "Tian Xia", "Zihan Ma", "Zhaowei Li", "Xiangrong Bi", "Xiao Li", "Xiaowei He"], "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding", "comment": "14 pages, 7 figures", "summary": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u6846\u67b6\uff0c\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5143\u6c34\u5e73\u4e0a\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u8111\u7684\u5206\u5c42\u5904\u7406\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86ANNs\u4e0e\u5927\u8111\u5904\u7406\u7684\u76f8\u4f3c\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u5728\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u4e0e\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4eba\u8111\u7684\u591a\u6a21\u6001\u5904\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u4eba\u5de5\u795e\u7ecf\u5143\uff08AN\uff09\u5206\u6790\u548c\u57fa\u4e8efMRI\u7684\u4f53\u7d20\u7f16\u7801\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u67b6\u6784\u4e0d\u540c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08CLIP\u548cMETER\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09ANs\u80fd\u9884\u6d4b\u751f\u7269\u795e\u7ecf\u5143\uff08BNs\uff09\u6d3b\u52a8\uff1b\uff082\uff09ANs\u548cBNs\u5747\u8868\u73b0\u51fa\u529f\u80fd\u5197\u4f59\uff1b\uff083\uff09ANs\u7684\u6781\u6027\u6a21\u5f0f\u4e0eBNs\u76f8\u4f3c\uff1b\uff084\uff09CLIP\u548cMETER\u7684\u67b6\u6784\u5bf9BNs\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u795e\u7ecf\u5143\u6c34\u5e73\u4e0a\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u8111\u7684\u5206\u5c42\u5904\u7406\u673a\u5236\uff0c\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u4e0e\u4eba\u7c7b\u5927\u8111\u5904\u7406\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\u3002"}}
{"id": "2510.16887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16887", "abs": "https://arxiv.org/abs/2510.16887", "authors": ["Nusrat Munia", "Abdullah Imran"], "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis", "comment": "EMBC 2025", "summary": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.", "AI": {"tldr": "Class-N-Diff\u662f\u4e00\u79cd\u7ed3\u5408\u5206\u7c7b\u5668\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u548c\u5206\u7c7b\u76ae\u80a4\u955c\u56fe\u50cf\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u7279\u5b9a\u533b\u5b66\u7c7b\u522b\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff08Class-N-Diff\uff09\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u57fa\u4e8e\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u56fe\u50cf\u3002", "result": "Class-N-Diff\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5316\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u5206\u7c7b\u5668\u6027\u80fd\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "Class-N-Diff\u6a21\u578b\u901a\u8fc7\u5c06\u5206\u7c7b\u5668\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ae\u80a4\u955c\u56fe\u50cf\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5206\u7c7b\u5668\u7684\u6027\u80fd\uff0c\u4e3a\u76ae\u80a4\u764c\u8bca\u65ad\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u5de5\u5177\u3002"}}
{"id": "2510.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16888", "abs": "https://arxiv.org/abs/2510.16888", "authors": ["Zongjian Li", "Zheyuan Liu", "Qihui Zhang", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Yang Ye", "Wangbo Yu", "Yuwei Niu", "Li Yuan"], "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback", "comment": null, "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.", "AI": {"tldr": "Edit-R1\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u548cMLLM\u5956\u52b1\u6a21\u578b\u63d0\u5347\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6027\u80fd\uff0cUniWorld-V2\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9886\u5148\u3002", "motivation": "\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u5728\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u4e2d\u8fc7\u62df\u5408\u8bad\u7ec3\u5206\u5e03\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a2\u7d22\u6027\u3002", "method": "\u91c7\u7528Diffusion Negative-aware Finetuning (DiffusionNFT)\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408MLLM\u4f5c\u4e3a\u8bad\u7ec3\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4f4e\u65b9\u5dee\u7ec4\u8fc7\u6ee4\u673a\u5236\u4ee5\u51cf\u5c11\u566a\u58f0\u3002", "result": "UniWorld-V2\u5728ImgEdit\u548cGEdit-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u53d6\u5f974.49\u548c7.83\u7684\u9ad8\u5206\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "Edit-R1\u6846\u67b6\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u548cMLLM\u4f5c\u4e3a\u7edf\u4e00\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.16891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16891", "abs": "https://arxiv.org/abs/2510.16891", "authors": ["Ramon Dalmau", "Gabriel Jarry", "Philippe Very"], "title": "Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data", "comment": null, "summary": "Aviation's non-CO2 effects, particularly contrails, are a significant\ncontributor to its climate impact. Persistent contrails can evolve into\ncirrus-like clouds that trap outgoing infrared radiation, with radiative\nforcing potentially comparable to or exceeding that of aviation's CO2\nemissions. While physical models simulate contrail formation, evolution and\ndissipation, validating and calibrating these models requires linking observed\ncontrails to the flights that generated them, a process known as\ncontrail-to-flight attribution. Satellite-based attribution is challenging due\nto limited spatial and temporal resolution, as contrails often drift and deform\nbefore detection. In this paper, we evaluate an alternative approach using\nground-based cameras, which capture contrails shortly after formation at high\nspatial and temporal resolution, when they remain thin, linear, and visually\ndistinct. Leveraging the ground visible camera contrail sequences (GVCCS)\ndataset, we introduce a modular framework for attributing contrails observed\nusing ground-based cameras to theoretical contrails derived from aircraft\nsurveillance and meteorological data. The framework accommodates multiple\ngeometric representations and distance metrics, incorporates temporal\nsmoothing, and enables flexible probability-based assignment strategies. This\nwork establishes a strong baseline and provides a modular framework for future\nresearch in linking contrails to their source flight.", "AI": {"tldr": "\u5730\u9762\u76f8\u673a\u9ad8\u5206\u8fa8\u7387\u6355\u6349\u822a\u8ff9\u4e91\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u6846\u67b6\u5173\u8054\u89c2\u6d4b\u4e0e\u7406\u8bba\u822a\u8ff9\u4e91\uff0c\u89e3\u51b3\u536b\u661f\u65b9\u6cd5\u5206\u8fa8\u7387\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u822a\u8ff9\u4e91\u5bf9\u6c14\u5019\u7684\u5f71\u54cd\u53ef\u80fd\u8d85\u8fc7\u822a\u7a7a\u4e1a\u7684CO2\u6392\u653e\uff0c\u4f46\u73b0\u6709\u536b\u661f\u65b9\u6cd5\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u6709\u9650\uff0c\u96be\u4ee5\u51c6\u786e\u5173\u8054\u822a\u8ff9\u4e91\u4e0e\u6e90\u822a\u73ed\u3002\u5730\u9762\u76f8\u673a\u80fd\u9ad8\u5206\u8fa8\u7387\u6355\u6349\u822a\u8ff9\u4e91\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u5229\u7528\u5730\u9762\u53ef\u89c1\u76f8\u673a\u822a\u8ff9\u4e91\u5e8f\u5217\uff08GVCCS\uff09\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u51e0\u4f55\u8868\u793a\u3001\u8ddd\u79bb\u5ea6\u91cf\u3001\u65f6\u95f4\u5e73\u6ed1\u548c\u6982\u7387\u5206\u914d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u822a\u8ff9\u4e91\u4e0e\u6e90\u822a\u73ed\u7684\u5173\u8054\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5173\u8054\u5730\u9762\u76f8\u673a\u89c2\u6d4b\u7684\u822a\u8ff9\u4e91\u4e0e\u7406\u8bba\u822a\u8ff9\u4e91\uff0c\u4e3a\u822a\u8ff9\u4e91-\u822a\u73ed\u5173\u8054\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u6a21\u5757\u5316\u5de5\u5177\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5730\u9762\u76f8\u673a\u6355\u6349\u822a\u8ff9\u4e91\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u89c2\u6d4b\u5230\u7684\u822a\u8ff9\u4e91\u4e0e\u7406\u8bba\u822a\u8ff9\u4e91\u8fdb\u884c\u5173\u8054\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7840\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16913", "categories": ["cs.CV", "68T07, 68U10, 68U35", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16913", "abs": "https://arxiv.org/abs/2510.16913", "authors": ["Akhila Kambhatla", "Ahmed R Khaled"], "title": "Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation", "comment": "9 Images with 1 figure and 3 Tables. This is a preprint submitted to\n  arXiv", "summary": "Thermal weapon segmentation is crucial for surveillance and security\napplications, enabling robust detection under lowlight and visually obscured\nconditions where RGB-based systems fail. While convolutional neural networks\n(CNNs) dominate thermal segmentation literature, their ability to capture\nlong-range dependencies and fine structural details is limited. Vision\nTransformers (ViTs), with their global context modeling capabilities, have\nachieved state-of-the-art results in RGB segmentation tasks, yet their\npotential in thermal weapon segmentation remains underexplored. This work\nadapts and evaluates four transformer-based architectures SegFormer,\nDeepLabV3\\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a\ncustom thermal dataset comprising 9,711 images collected from real world\nsurveillance videos and automatically annotated using SAM2. We employ standard\naugmentation strategies within the MMSegmentation framework to ensure robust\nmodel training and fair architectural comparison. Experimental results\ndemonstrate significant improvements in segmentation performance: SegFormer-b5\nachieves the highest mIoU (94.15\\%) and Pixel Accuracy (97.04\\%), while\nSegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive\nmIoU (90.84\\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and\n92.24\\% mIoU, and DeepLabV3\\+ R101-D8 reaches 92.76\\% mIoU at 29.86 FPS. The\ntransformer architectures demonstrate robust generalization capabilities for\nweapon detection in low-light and occluded thermal environments, with flexible\naccuracy-speed trade-offs suitable for diverse real-time security applications.", "AI": {"tldr": "Transformer-based models excel in thermal weapon segmentation, with SegFormer-b5 leading in accuracy and SegFormer-b0 in speed, suitable for real-time security applications.", "motivation": "Thermal weapon segmentation is critical for surveillance under low-light and obscured conditions where RGB systems fail. While CNNs are limited in capturing long-range dependencies, ViTs show promise but remain underexplored in thermal segmentation.", "method": "The study adapts and evaluates four transformer-based architectures (SegFormer, DeepLabV3+, SegNeXt, and Swin Transformer) on a custom thermal dataset using standard augmentation strategies within the MMSegmentation framework.", "result": "SegFormer-b5 achieves the highest mIoU (94.15%) and Pixel Accuracy (97.04%), while SegFormer-b0 offers the fastest inference speed (98.32 FPS) with competitive mIoU (90.84%). Other models like SegNeXt-mscans and DeepLabV3+ R101-D8 also show balanced performance.", "conclusion": "Transformer-based architectures, particularly SegFormer-b5, demonstrate superior performance in thermal weapon segmentation, offering robust generalization and flexible accuracy-speed trade-offs for real-time security applications."}}
{"id": "2510.16926", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16926", "abs": "https://arxiv.org/abs/2510.16926", "authors": ["Chenxu Li", "Zhicai Wang", "Yuan Sheng", "Xingyu Zhu", "Yanbin Hao", "Xiang Wang"], "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input", "comment": "23 pages,19 figures", "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faRes-Bench\u57fa\u51c6\u548c\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u91cf\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5206\u8fa8\u7387\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4e0d\u8db3\uff0c\u5e76\u63a2\u7d22\u63d0\u5347\u7a33\u5b9a\u6027\u7684\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u5206\u8fa8\u7387\u9c81\u68d2\u6027\uff08\u5373\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u7a33\u5b9a\u6027\uff09\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86Res-Bench\u57fa\u51c6\uff0c\u5305\u542b14,400\u4e2a\u6837\u672c\uff0c\u8986\u76d612\u79cd\u5206\u8fa8\u7387\u7ea7\u522b\u548c6\u4e2a\u6838\u5fc3\u80fd\u529b\u7ef4\u5ea6\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165Spearman\u76f8\u5173\u6027\u3001ACE\u548cRCE\u7b49\u6307\u6807\u6765\u91cf\u5316\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "result": "\u5927\u89c4\u6a21\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u9884\u5904\u7406\u7b56\u7565\uff08\u5982\u586b\u5145\u548c\u8d85\u5206\u8fa8\u7387\uff09\u548c\u5fae\u8c03\u5bf9\u63d0\u5347\u7a33\u5b9a\u6027\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86Res-Bench\u57fa\u51c6\u548c\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u5f15\u5165Spearman\u76f8\u5173\u6027\u548cACE/RCE\u7b49\u65b0\u6307\u6807\uff0c\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u63a2\u7d22\u4e86\u9884\u5904\u7406\u548c\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.16973", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16973", "abs": "https://arxiv.org/abs/2510.16973", "authors": ["Praveenbalaji Rajendran", "Mojtaba Safari", "Wenfeng He", "Mingzhe Hu", "Shansong Wang", "Jun Zhou", "Xiaofeng Yang"], "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis", "comment": null, "summary": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u5206\u6790\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5206\u7c7b\u4e86\u7814\u7a76\u5e76\u8fdb\u884c\u4e86\u5143\u5206\u6790\uff0c\u8ba8\u8bba\u4e86\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7814\u7a76\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u9886\u57df\u4ecd\u7f3a\u4e4f\u5bf9\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u4e34\u5e8a\u5e94\u7528\u7684\u7edf\u4e00\u7efc\u5408\u3002", "method": "\u6587\u7ae0\u7cfb\u7edf\u5730\u5c06\u7814\u7a76\u5206\u4e3a\u57fa\u4e8e\u89c6\u89c9\u548c\u89c6\u89c9\u8bed\u8a00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9a\u91cf\u5143\u5206\u6790\u4ee5\u63cf\u8ff0\u6570\u636e\u96c6\u5229\u7528\u548c\u5e94\u7528\u9886\u57df\u7684\u65f6\u95f4\u8d8b\u52bf\u3002", "result": "\u6587\u7ae0\u8bc6\u522b\u4e86\u6301\u7eed\u7684\u6311\u6218\uff08\u5982\u9886\u57df\u9002\u5e94\u3001\u9ad8\u6548\u5fae\u8c03\uff09\u53ca\u65b0\u5174\u89e3\u51b3\u65b9\u6848\uff08\u5982\u8054\u90a6\u5b66\u4e60\u3001\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u6587\u7ae0\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7ed3\u6784\u5316\u5206\u6790\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u589e\u5f3a\u5176\u7a33\u5065\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u6574\u5408\u3002"}}
{"id": "2510.16983", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16983", "abs": "https://arxiv.org/abs/2510.16983", "authors": ["Yuanzhi Zhu", "Eleftherios Tsonis", "Lucas Degeorge", "Vicky Kalogeiton"], "title": "One-step Diffusion Models with Bregman Density Ratio Matching", "comment": "work in progress", "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.", "AI": {"tldr": "Di-Bregman\u901a\u8fc7Bregman\u6563\u5ea6\u5bc6\u5ea6\u6bd4\u5339\u914d\uff0c\u7edf\u4e00\u6269\u6563\u84b8\u998f\u7406\u8bba\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e00\u6b65\u751f\u6210\u4e2d\u9ad8\u6548\u4e14\u4fdd\u771f\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u751f\u6210\u8d28\u91cf\u9ad8\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51faDi-Bregman\u6846\u67b6\uff0c\u5c06\u6269\u6563\u84b8\u998f\u95ee\u9898\u5efa\u6a21\u4e3aBregman\u6563\u5ea6\u5bc6\u5ea6\u6bd4\u5339\u914d\uff0c\u901a\u8fc7\u51f8\u5206\u6790\u89c6\u89d2\u7edf\u4e00\u591a\u4e2a\u76ee\u6807\u3002", "result": "\u5728CIFAR-10\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDi-Bregman\u5728\u4e00\u6b65FID\u4e0a\u4f18\u4e8e\u53cd\u5411KL\u84b8\u998f\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "Di-Bregman\u6846\u67b6\u901a\u8fc7Bregman\u6563\u5ea6\u5bc6\u5ea6\u6bd4\u5339\u914d\uff0c\u4e3a\u6269\u6563\u84b8\u998f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e00\u6b65\u751f\u6210\u4e2d\u4f18\u4e8e\u53cd\u5411KL\u84b8\u998f\uff0c\u5e76\u4fdd\u6301\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.16988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16988", "abs": "https://arxiv.org/abs/2510.16988", "authors": ["Junhao Zhao", "Zishuai Liu", "Ruili Fang", "Jin Lu", "Linghan Zhang", "Fei Dou"], "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams", "comment": null, "summary": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.", "AI": {"tldr": "CARE\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50\u548c\u8054\u5408\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347ADL\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u793a\u5c42\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e8f\u5217\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u610f\u8bc6\uff0c\u56fe\u50cf\u65b9\u6cd5\u538b\u7f29\u65f6\u95f4\u52a8\u6001\u5e76\u626d\u66f2\u4f20\u611f\u5668\u5e03\u5c40\uff0c\u7b80\u5355\u878d\u5408\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u63d0\u51faCARE\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u3001\u6297\u566a\u58f0\u7684\u5e8f\u5217\u7f16\u7801\u548c\u7a7a\u95f4\u611f\u77e5\u3001\u9891\u7387\u654f\u611f\u7684\u56fe\u50cf\u8868\u793a\uff0c\u901a\u8fc7\u8054\u5408\u5bf9\u6bd4\u5206\u7c7b\u76ee\u6807\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2aCASAS\u6570\u636e\u96c6\u4e0a\uff0cCARE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff08Milan 89.8%\uff0cCairo 88.9%\uff0cKyoto7 73.3%\uff09\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u4f20\u611f\u5668\u6545\u969c\u548c\u5e03\u5c40\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CARE\u6846\u67b6\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50\uff08SICA\uff09\u548c\u4ea4\u53c9\u71b5\u5206\u7c7b\u7684\u8054\u5408\u4f18\u5316\uff0c\u5728ADL\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5bf9\u4f20\u611f\u5668\u6545\u969c\u548c\u5e03\u5c40\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16989", "abs": "https://arxiv.org/abs/2510.16989", "authors": ["Luca Zanella", "Massimiliano Mancini", "Yiming Wang", "Alessio Tonioni", "Elisa Ricci"], "title": "Training-free Online Video Step Grounding", "comment": "NeurIPS 2025. Project website at https://lucazanella.github.io/baglm/", "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.", "AI": {"tldr": "BaGLM\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5728\u7ebf\u8fdb\u884c\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u6807\u8bb0\u8bad\u7ec3\u96c6\u4e14\u53ea\u80fd\u79bb\u7ebf\u5904\u7406\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ebf\u51b3\u7b56\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u4e14\u80fd\u5728\u7ebf\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u9884\u6d4b\u4e0e\u6709\u9650\u5e27\u96c6\u76f8\u5173\u7684\u6b65\u9aa4\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u6ee4\u6ce2\u539f\u5219\uff0c\u901a\u8fc7\u4f9d\u8d56\u77e9\u9635\u548c\u6b65\u9aa4\u8fdb\u5ea6\u4f30\u8ba1\u6765\u6539\u8fdb\u9884\u6d4b\u3002", "result": "BaGLM\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BaGLM\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u6ee4\u6ce2\u539f\u5219\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u8bad\u7ec3\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.17007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17007", "abs": "https://arxiv.org/abs/2510.17007", "authors": ["Ignacio M. De la Jara", "Cristian Rodriguez-Opazo", "Edison Marrese-Taylor", "Felipe Bravo-Marquez"], "title": "An empirical study of the effect of video encoders on Temporal Video Grounding", "comment": null, "summary": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u89c6\u9891\u7f16\u7801\u5668\u5bf9\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6697\u793a\u7279\u5f81\u4e92\u8865\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u96c6\u4e2d\u5728\u5c11\u6570\u89c6\u9891\u8868\u793a\u4e0a\uff0c\u53ef\u80fd\u5bfc\u81f4\u957f\u671f\u67b6\u6784\u8fc7\u62df\u5408\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eCNN\u3001\u65f6\u5e8f\u63a8\u7406\u548cTransformer\u7684\u89c6\u9891\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08Charades-STA\u3001ActivityNet-Captions\u548cYouCookII\uff09\u7684\u7279\u5f81\uff0c\u5e76\u5728\u7ecf\u5178\u67b6\u6784\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4ec5\u66f4\u6362\u89c6\u9891\u7f16\u7801\u5668\u5373\u53ef\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5b9a\u7279\u5f81\u5e26\u6765\u7684\u6a21\u5f0f\u548c\u9519\u8bef\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u89c6\u9891\u7f16\u7801\u5668\u5bf9\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5b9a\u7279\u5f81\u5e26\u6765\u7684\u6a21\u5f0f\u548c\u9519\u8bef\uff0c\u6697\u793a\u4e86\u7279\u5f81\u4e92\u8865\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17014", "abs": "https://arxiv.org/abs/2510.17014", "authors": ["Ani Vanyan", "Alvard Barseghyan", "Hakob Tamazyan", "Tigran Galstyan", "Vahan Huroyan", "Naira Hovakimyan", "Hrant Khachatrian"], "title": "Do Satellite Tasks Need Special Pretraining?", "comment": null, "summary": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5c0f\u89c4\u6a21\u4e0b\uff0c\u7279\u5b9a\u9886\u57df\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u5e76\u4e0d\u6bd4\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "motivation": "\u7814\u7a76\u9065\u611f\u5f71\u50cf\u7684\u72ec\u7279\u7279\u5f81\u3001\u7279\u5b9a\u5e94\u7528\u53ca\u536b\u661f\u56fe\u50cf\u5206\u6790\u6240\u9700\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u8ba8\u7279\u5b9a\u9886\u57df\u57fa\u7840\u6a21\u578b\u662f\u5426\u6bd4\u901a\u7528\u6a21\u578b\u66f4\u6709\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8861\u91cf\u9065\u611f\u6a21\u578b\u5728\u4e24\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728MillionAID\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86iBOT\uff08\u4e00\u79cd\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\uff09\uff0c\u5e76\u9488\u5bf9\u9065\u611f\u8fdb\u884c\u4e86\u7279\u5b9a\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u5728ViT-B\u89c4\u6a21\u4e0a\u5747\u672a\u5bf9\u901a\u7528\u57fa\u7ebf\u5e26\u6765\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u7279\u5b9a\u9886\u57df\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u4e0a\u5e76\u672a\u663e\u793a\u51fa\u6bd4\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u66f4\u663e\u8457\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.17023", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17023", "abs": "https://arxiv.org/abs/2510.17023", "authors": ["Shraman Pramanick", "Effrosyni Mavroudi", "Yale Song", "Rama Chellappa", "Lorenzo Torresani", "Triantafyllos Afouras"], "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs", "comment": "ICCV 2025 (Highlights)", "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.", "AI": {"tldr": "ED-VTG\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8054\u5408\u5904\u7406\u6587\u672c\u548c\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u8054\u5408\u5904\u7406\u6587\u672c\u548c\u89c6\u9891\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5c06\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u5305\u542b\u7f3a\u5931\u7ec6\u8282\u548c\u7ebf\u7d22\u7684\u4e30\u5bcc\u53e5\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u57fa\u4e8e\u8fd9\u4e9b\u4e30\u5bcc\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u9884\u6d4b\u51c6\u786e\u8fb9\u754c\u3002\u8bad\u7ec3\u65f6\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\u76ee\u6807\u4ee5\u51cf\u5c11\u566a\u58f0\u548c\u5e7b\u89c9\u5f71\u54cd\u3002", "result": "\u5728\u5404\u79cd\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\u548c\u6bb5\u843d\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u4e4b\u524d\u6240\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "ED-VTG\u5728\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u548c\u6bb5\u843d\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u573a\u666f\u4e2d\u4fdd\u6301\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2510.17034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17034", "abs": "https://arxiv.org/abs/2510.17034", "authors": ["Yutong Zhong"], "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding", "comment": null, "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.", "AI": {"tldr": "W2R2\u6846\u67b6\u901a\u8fc7\u89e3\u80262D\u8bed\u4e49\u548c3D\u7a7a\u95f4\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u7684'2D\u8bed\u4e49\u504f\u5dee'\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u8fc7\u5ea6\u4f9d\u8d562D\u56fe\u50cf\u7279\u5f81\uff0c\u5bfc\u81f4'2D\u8bed\u4e49\u504f\u5dee'\uff0c\u5ffd\u89c6\u4e863D\u51e0\u4f55\u8f93\u5165\uff0c\u5bfc\u81f4\u878d\u5408\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aWhat-Where Representation Re-Forming (W2R2)\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6291\u5236\u6377\u5f84\u6765\u91cd\u5851\u6a21\u578b\u7684\u5185\u90e8\u7a7a\u95f4\u30022D\u7279\u5f81\u7528\u4e8e\u8bed\u4e49\u8bc6\u522b\uff08What\uff09\uff0c3D\u7279\u5f81\u7528\u4e8e\u7a7a\u95f4\u5b9a\u4f4d\uff08Where\uff09\u3002\u5173\u952e\u7ec4\u4ef6\u5305\u62ec\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff1aAlignment Loss\uff08\u76d1\u7763\u878d\u5408\u9884\u6d4b\uff09\u548cPseudo-Label Loss\uff08\u60e9\u7f5a2D\u4e3b\u5bfc\u7684\u4f2a\u8f93\u51fa\uff09\u3002", "result": "\u5728ScanRefer\u548cScanQA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cW2R2\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u3002", "conclusion": "W2R2\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6291\u5236\u6377\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u7684'2D\u8bed\u4e49\u504f\u5dee'\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17035", "abs": "https://arxiv.org/abs/2510.17035", "authors": ["Syed Konain Abbas", "Sandip Purnapatra", "M. G. Sarwar Murshed", "Conor Miller-Lynch", "Lambert Igene", "Soumyabrata Dey", "Stephanie Schuckers", "Faraz Hussain"], "title": "Conditional Synthetic Live and Spoof Fingerprint Generation", "comment": null, "summary": "Large fingerprint datasets, while important for training and evaluation, are\ntime-consuming and expensive to collect and require strict privacy measures.\nResearchers are exploring the use of synthetic fingerprint data to address\nthese issues. This paper presents a novel approach for generating synthetic\nfingerprint images (both spoof and live), addressing concerns related to\nprivacy, cost, and accessibility in biometric data collection. Our approach\nutilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce\nhigh-resolution synthetic live fingerprints, conditioned on specific finger\nidentities (thumb through little finger). Additionally, we employ CycleGANs to\ntranslate these into realistic spoof fingerprints, simulating a variety of\npresentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof\nfingerprints are crucial for developing robust spoof detection systems. Through\nthese generative models, we created two synthetic datasets (DB2 and DB3), each\ncontaining 1,500 fingerprint images of all ten fingers with multiple\nimpressions per finger, and including corresponding spoofs in eight material\ntypes. The results indicate robust performance: our StyleGAN3 model achieves a\nFr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints\nachieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The\nStyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess\nfingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,\nmatching experiments confirm strong privacy preservation, with no significant\nevidence of identity leakage, confirming the strong privacy-preserving\nproperties of our synthetic datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStyleGAN\u548cCycleGAN\u7684\u5408\u6210\u6307\u7eb9\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6536\u96c6\u7684\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\uff0c\u751f\u6210\u7684\u6307\u7eb9\u6027\u80fd\u4f18\u5f02\u4e14\u9690\u79c1\u4fdd\u62a4\u826f\u597d\u3002", "motivation": "\u89e3\u51b3\u6307\u7eb9\u6570\u636e\u96c6\u6536\u96c6\u7684\u9ad8\u6210\u672c\u3001\u9690\u79c1\u4fdd\u62a4\u53ca\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u6761\u4ef6StyleGAN2-ADA\u548cStyleGAN3\u67b6\u6784\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u6d3b\u4f53\u6307\u7eb9\uff0c\u5e76\u7ed3\u5408CycleGANs\u5c06\u5176\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u5047\u6307\u7eb9\u3002", "result": "StyleGAN3\u6a21\u578b\u7684FID\u4f4e\u81f35\uff0c\u751f\u6210\u7684\u6307\u7eb9\u57280.01% FAR\u4e0bTAR\u8fbe99.47%\uff1bStyleGAN2-ADA\u6a21\u578b\u5728\u76f8\u540cFAR\u4e0bTAR\u4e3a98.67%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5408\u6210\u6307\u7eb9\u751f\u6210\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u672a\u53d1\u73b0\u663e\u8457\u7684\u8eab\u4efd\u6cc4\u9732\u8bc1\u636e\uff0c\u540c\u65f6\u751f\u6210\u7684\u6307\u7eb9\u5728\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.17039", "categories": ["cs.CV", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17039", "abs": "https://arxiv.org/abs/2510.17039", "authors": ["Mohammad R. Salmanpour", "Sonya Falahati", "Amir Hossein Pouria", "Amin Mousavi", "Somayeh Sadat Mehrnia", "Morteza Alizadeh", "Arman Gorji", "Zeinab Farsangi", "Alireza Safarian", "Mehdi Maghsudi", "Carlos Uribe", "Arman Rahmim", "Ren Yuan"], "title": "Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework", "comment": "13 pages, 2 figures, and 2 tables", "summary": "Lung cancer remains the leading cause of cancer mortality, with CT imaging\ncentral to screening, prognosis, and treatment. Manual segmentation is variable\nand time-intensive, while deep learning (DL) offers automation but faces\nbarriers to clinical adoption. Guided by the Knowledge-to-Action framework,\nthis study develops a clinician-in-the-loop DL pipeline to enhance\nreproducibility, prognostic accuracy, and clinical trust. Multi-center CT data\nfrom 999 patients across 12 public datasets were analyzed using five DL models\n(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against\nexpert contours on whole and click-point cropped images. Segmentation\nreproducibility was assessed using 497 PySERA-extracted radiomic features via\nSpearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic\nmodeling compared supervised (SL) and semi-supervised learning (SSL) across 38\ndimensionality reduction strategies and 24 classifiers. Six physicians\nqualitatively evaluated masks across seven domains, including clinical\nmeaningfulness, boundary quality, prognostic value, trust, and workflow\nintegration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),\nradiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive\naccuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed\nSL across models. Radiologists favored VNet for peritumoral representation and\nsmoother boundaries, preferring AI-generated initial masks for refinement\nrather than replacement. These results demonstrate that integrating VNet with\nSSL yields accurate, reproducible, and clinically trusted CT-based lung cancer\nprognosis, highlighting a feasible path toward physician-centered AI\ntranslation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u901a\u8fc7VNet\u548c\u534a\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u53ef\u91cd\u590d\u4e14\u4e34\u5e8a\u53ef\u4fe1\u7684\u80ba\u764cCT\u9884\u540e\u5206\u6790\uff0c\u4e3aAI\u5728\u533b\u7597\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "motivation": "\u80ba\u764c\u662f\u764c\u75c7\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0cCT\u6210\u50cf\u662f\u7b5b\u67e5\u3001\u9884\u540e\u548c\u6cbb\u7597\u7684\u6838\u5fc3\u3002\u624b\u52a8\u5206\u5272\u5b58\u5728\u53d8\u5f02\u6027\u4e14\u8017\u65f6\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u867d\u80fd\u81ea\u52a8\u5316\u4f46\u9762\u4e34\u4e34\u5e8a\u91c7\u7eb3\u7684\u969c\u788d\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684DL\u6d41\u7a0b\u63d0\u5347\u53ef\u91cd\u590d\u6027\u3001\u9884\u540e\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u591a\u4e2d\u5fc3CT\u6570\u636e\uff08999\u540d\u60a3\u8005\uff0c12\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff09\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff083D Attention U-Net\u3001ResUNet\u3001VNet\u3001ReconNet\u3001SAM-Med3D\uff09\uff0c\u5e76\u901a\u8fc7497\u4e2aPySERA\u63d0\u53d6\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u8bc4\u4f30\u5206\u5272\u53ef\u91cd\u590d\u6027\u3002\u9884\u540e\u5efa\u6a21\u6bd4\u8f83\u4e86\u76d1\u7763\u5b66\u4e60\uff08SL\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u572838\u79cd\u964d\u7ef4\u7b56\u7565\u548c24\u79cd\u5206\u7c7b\u5668\u4e2d\u7684\u8868\u73b0\u3002", "result": "VNet\u8868\u73b0\u6700\u4f73\uff08Dice = 0.83, IoU = 0.71\uff09\uff0c\u653e\u5c04\u7ec4\u5b66\u7a33\u5b9a\u6027\u6700\u9ad8\uff08\u5e73\u5747\u76f8\u5173\u6027=0.76\uff0cICC=0.65\uff09\uff0c\u5728SSL\u4e0b\u9884\u6d4b\u51c6\u786e\u6027\u6700\u9ad8\uff08\u51c6\u786e\u7387=0.88\uff0cF1=0.83\uff09\u3002SSL\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5747\u4f18\u4e8eSL\u3002\u653e\u5c04\u79d1\u533b\u751f\u66f4\u9752\u7750VNet\u7684\u7624\u5468\u8868\u73b0\u548c\u5e73\u6ed1\u8fb9\u754c\uff0c\u503e\u5411\u4e8e\u4f7f\u7528AI\u751f\u6210\u7684\u521d\u59cb\u63a9\u6a21\u8fdb\u884c\u6539\u8fdb\u800c\u975e\u66ff\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408VNet\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u51c6\u786e\u3001\u53ef\u91cd\u590d\u4e14\u4e34\u5e8a\u53ef\u4fe1\u7684\u57fa\u4e8eCT\u7684\u80ba\u764c\u9884\u540e\u5206\u6790\u6d41\u7a0b\uff0c\u4e3a\u533b\u751f\u4e3a\u4e2d\u5fc3\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.17043", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.17043", "abs": "https://arxiv.org/abs/2510.17043", "authors": ["Md Ahmed Al Muzaddid", "William J. Beksi"], "title": "Person Re-Identification via Generalized Class Prototypes", "comment": "18 pages, 11 figures, and 4 tables", "summary": "Advanced feature extraction methods have significantly contributed to\nenhancing the task of person re-identification. In addition, modifications to\nobjective functions have been developed to further improve performance.\nNonetheless, selecting better class representatives is an underexplored area of\nresearch that can also lead to advancements in re-identification performance.\nAlthough past works have experimented with using the centroid of a gallery\nimage class during training, only a few have investigated alternative\nrepresentations during the retrieval stage. In this paper, we demonstrate that\nthese prior techniques yield suboptimal results in terms of re-identification\nmetrics. To address the re-identification problem, we propose a generalized\nselection method that involves choosing representations that are not limited to\nclass centroids. Our approach strikes a balance between accuracy and mean\naverage precision, leading to improvements beyond the state of the art. For\nexample, the actual number of representations per class can be adjusted to meet\nspecific application requirements. We apply our methodology on top of multiple\nre-identification embeddings, and in all cases it substantially improves upon\ncontemporary results", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u5c40\u9650\u4e8e\u7c7b\u522b\u8d28\u5fc3\u7684\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u4e3b\u8981\u4f9d\u8d56\u7c7b\u522b\u8d28\u5fc3\u4f5c\u4e3a\u8868\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u68c0\u7d22\u9636\u6bb5\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u5c40\u9650\u4e8e\u7c7b\u522b\u8d28\u5fc3\uff0c\u800c\u662f\u7075\u6d3b\u9009\u62e9\u8868\u793a\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u9700\u6c42\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u91cd\u8bc6\u522b\u5d4c\u5165\u4e0a\u5e94\u7528\uff0c\u5747\u663e\u8457\u63d0\u5347\u4e86\u5f53\u524d\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u5c40\u9650\u4e8e\u7c7b\u522b\u8d28\u5fc3\u7684\u8868\u793a\u9009\u62e9\uff0c\u5728\u51c6\u786e\u7387\u548c\u5e73\u5747\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17045", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17045", "abs": "https://arxiv.org/abs/2510.17045", "authors": ["Deepak Sridhar", "Kartikeya Bhardwaj", "Jeya Pradha Jeyaraj", "Nuno Vasconcelos", "Ankita Nayak", "Harris Teague"], "title": "Video Reasoning without Training", "comment": null, "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.", "AI": {"tldr": "V-Reason\u901a\u8fc7\u71b5\u8c03\u8282\u4f18\u5316LMM\u63a8\u7406\u884c\u4e3a\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u63d0\u5347\u89c6\u9891\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684RL\u548c\u5197\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u63a7\u5236\u673a\u5236\u6709\u9650\u3002", "method": "\u901a\u8fc7\u71b5\u57fa\u76ee\u6807\u5728\u5c0f\u578b\u53ef\u8bad\u7ec3\u63a7\u5236\u5668\u4e0a\u4f18\u5316LMM\u7684\u503c\u7f13\u5b58\uff0c\u8c03\u6574\u6a21\u578b\u7684\u5fae\u63a2\u7d22\u548c\u5f00\u53d1\u884c\u4e3a\u3002", "result": "V-Reason\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4e0eRL\u6a21\u578b\u5dee\u8ddd\u7f29\u5c0f\u81f30.6%\uff0c\u540c\u65f6\u8f93\u51fa\u4ee4\u724c\u51cf\u5c1158.6%\u3002", "conclusion": "\u5229\u7528\u71b5\u4f5c\u4e3a\u4fe1\u53f7\u8c03\u8282\u6a21\u578b\u884c\u4e3a\uff0cV-Reason\u65b9\u6cd5\u5728\u65e0\u9700RL\u6216\u76d1\u7763\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4e0eRL\u8bad\u7ec3\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2510.17051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17051", "abs": "https://arxiv.org/abs/2510.17051", "authors": ["Masoud Khairi Atani", "Alon Harell", "Hyomin Choi", "Runyu Yang", "Fabien Racape", "Ivan V. Bajic"], "title": "How Universal Are SAM2 Features?", "comment": "This work has been accepted for publication in IEEE Picture Coding\n  Symposium (PCS) 2025", "summary": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.", "AI": {"tldr": "\u6bd4\u8f83\u901a\u7528\u4e0e\u4e13\u7528\u89c6\u89c9\u6a21\u578b\u7684\u7279\u5f81\u591a\u7528\u9014\u6027\uff0c\u53d1\u73b0\u4e13\u7528\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u9ad8\u6548\u4f46\u727a\u7272\u4e86\u8bed\u4e49\u5e7f\u5ea6\uff0c\u4e3a\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e\u3002", "motivation": "\u63a2\u8ba8\u901a\u7528\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u4e0e\u5176\u4e13\u7528\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u4f18\u5316\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u901a\u7528Hiera\u7f16\u7801\u5668\u4e0e\u4e13\u7528Segment Anything Model 2\uff08SAM2\uff09\u7684\u7279\u5f81\u591a\u7528\u9014\u6027\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9888\u90e8\u91cf\u5316\u5176\u51bb\u7ed3\u7279\u5f81\u7684\u9002\u5e94\u6027\u6210\u672c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cSAM2\u5728\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6982\u5ff5\u8f83\u8fdc\u7684\u4efb\u52a1\uff08\u5982\u59ff\u6001\u4f30\u8ba1\u548c\u56fe\u50cf\u63cf\u8ff0\uff09\u4e0a\u4e0d\u5982\u901a\u7528\u6a21\u578bHiera\uff0c\u8868\u660e\u5176\u5b58\u5728\u8bed\u4e49\u4fe1\u606f\u635f\u5931\u3002\u8de8\u9888\u90e8\u5206\u6790\u8fd8\u53d1\u73b0\uff0cSAM2\u7684\u6bcf\u4e00\u7ea7\u9002\u5e94\u90fd\u4f1a\u9020\u6210\u8868\u793a\u74f6\u9888\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u901a\u7528\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u4e0e\u4e13\u7528\u6a21\u578b\u5728\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u4e2d\u7684\u6743\u8861\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u7684\u9ad8\u6548\u7279\u5f81\u7f16\u7801\u548c\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7840\u3002"}}
{"id": "2510.17068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17068", "abs": "https://arxiv.org/abs/2510.17068", "authors": ["Zhe Luo", "Wenjing Jia", "Stuart Perry"], "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding", "comment": null, "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet", "AI": {"tldr": "ProDAT\u662f\u4e00\u79cd\u5bc6\u5ea6\u611f\u77e5\u6e10\u8fdb\u5f0f\u70b9\u4e91\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89e3\u7801\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\uff0c\u652f\u6301\u591a\u6bd4\u7279\u7387\u6e10\u8fdb\u89e3\u7801\u3002", "motivation": "\u4e09\u7ef4\u70b9\u4e91\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5927\u6570\u636e\u91cf\u548c\u5e26\u5bbd\u9650\u5236\u963b\u788d\u4e86\u9ad8\u8d28\u91cf\u670d\u52a1\u7684\u90e8\u7f72\uff0c\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u7684\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u4e0d\u652f\u6301\u6e10\u8fdb\u89e3\u7801\u3002", "method": "\u901a\u8fc7\u5bc6\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u6307\u5bfc\u4fe1\u53f7\uff0c\u81ea\u9002\u5e94\u89e3\u7801\u6f5c\u5728\u7279\u5f81\u548c\u5750\u6807\uff0c\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u652f\u6301\u591a\u6bd4\u7279\u7387\u6e10\u8fdb\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProDAT\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc728.6%\u7684BD-rate\u63d0\u5347\uff08PSNR-D2\u5728SemanticKITTI\uff09\u548c\u8d85\u8fc718.15%\uff08ShapeNet\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u7f16\u7801\u6280\u672f\u3002", "conclusion": "ProDAT\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u5ea6\u611f\u77e5\u5c3e\u4e22\u673a\u5236\uff0c\u652f\u6301\u6e10\u8fdb\u5f0f\u70b9\u4e91\u7f16\u7801\uff0c\u4e0d\u4ec5\u5728\u591a\u4e2a\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u6e10\u8fdb\u89e3\u7801\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u6548\u7387\u3002"}}
{"id": "2510.17078", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.17078", "abs": "https://arxiv.org/abs/2510.17078", "authors": ["Jad Berjawi", "Yoann Dupas", "Christophe C'erin"], "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection", "comment": "8 pages, 8 figures, accepted at ICCV 2025 MIRA Workshop", "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.", "AI": {"tldr": "FMCAF\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u9891\u57df\u8fc7\u6ee4\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u63d0\u5347RGB\u548c\u7ea2\u5916\u8f93\u5165\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u3002", "motivation": "\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u901a\u8fc7\u5229\u7528\u591a\u4f20\u611f\u5668\u6a21\u6001\u7684\u4e92\u8865\u7ebf\u7d22\uff0c\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u63d0\u9ad8\u9c81\u68d2\u6027\u3002FMCAF\u65e8\u5728\u63d0\u5347RGB\u548c\u7ea2\u5916\uff08IR\uff09\u8f93\u5165\u7684\u878d\u5408\u6548\u679c\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u591a\u6a21\u6001\u6311\u6218\u3002", "method": "FMCAF\u7ed3\u5408\u4e86\u9891\u57df\u8fc7\u6ee4\u5757\uff08Freq-Filter\uff09\u4ee5\u51cf\u5c11\u5197\u4f59\u9891\u8c31\u7279\u5f81\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u878d\u5408\u6a21\u5757\uff08MCAF\uff09\u4ee5\u589e\u5f3a\u6a21\u6001\u95f4\u7279\u5f81\u5171\u4eab\u3002", "result": "\u5728LLVIP\uff08\u4f4e\u5149\u884c\u4eba\u68c0\u6d4b\uff09\u548cVEDAI\uff08\u7a7a\u4e2d\u8f66\u8f86\u68c0\u6d4b\uff09\u4e0a\uff0cFMCAF\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\uff08\u62fc\u63a5\uff09\uff0c\u5206\u522b\u63d0\u5347+1.1%\u548c+13.9%\u7684mAP@50\u3002", "conclusion": "FMCAF\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u7684\u9884\u5904\u7406\u67b6\u6784\uff0c\u5728\u591a\u79cd\u591a\u6a21\u6001\u68c0\u6d4b\u6311\u6218\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u8c03\u4f18\uff0c\u5c55\u73b0\u51fa\u4f5c\u4e3a\u672a\u6765\u68c0\u6d4b\u6d41\u7a0b\u4e2d\u7a33\u5065\u591a\u6a21\u6001\u878d\u5408\u57fa\u7840\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17095", "abs": "https://arxiv.org/abs/2510.17095", "authors": ["Ruitong Gan", "Junran Peng", "Yang Liu", "Chuanchen Luo", "Qing Li", "Zhaoxiang Zhang"], "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation", "comment": null, "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.", "AI": {"tldr": "GSPlane\u901a\u8fc7\u5e73\u9762\u5148\u9a8c\u548c\u52a8\u6001\u9ad8\u65af\u91cd\u65b0\u5206\u7c7b\u5668\uff0c\u63d0\u5347\u4e86\u9ad8\u65af\u6e85\u5c04\u5728\u5e73\u9762\u91cd\u5efa\u4e2d\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u6e85\u5c04\uff08GS\uff09\u5728\u91cd\u5efa\u5e73\u9762\u533a\u57df\u65f6\u5e73\u6ed1\u6027\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u573a\u666f\u7f16\u8f91\u548c\u7269\u7406\u6a21\u62df\u3002", "method": "GSPlane\u5229\u7528\u73b0\u6210\u7684\u5206\u5272\u548c\u6cd5\u7ebf\u9884\u6d4b\u6a21\u578b\u63d0\u53d6\u5e73\u9762\u5148\u9a8c\uff0c\u5efa\u7acb\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u9ad8\u65af\u91cd\u65b0\u5206\u7c7b\u5668\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5229\u7528\u4f18\u5316\u7684\u5e73\u9762\u5148\u9a8c\u7ec6\u5316\u7f51\u683c\u5e03\u5c40\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSPlane\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63d0\u53d6\u7f51\u683c\u7684\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GSPlane\u901a\u8fc7\u5f15\u5165\u5e73\u9762\u5148\u9a8c\u548c\u52a8\u6001\u9ad8\u65af\u91cd\u65b0\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u573a\u666f\u4e2d\u5e73\u9762\u533a\u57df\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6e32\u67d3\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u5e73\u9762\u8868\u793a\u5728\u573a\u666f\u7f16\u8f91\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2510.17105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17105", "abs": "https://arxiv.org/abs/2510.17105", "authors": ["Xiaogang Xu", "Jian Wang", "Yunfan Lu", "Ruihang Chu", "Ruixing Wang", "Jiafei Wu", "Bei Yu", "Liang Lin"], "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement", "comment": null, "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6f5c\u5728\u7ec6\u5316\u4e0e\u52a8\u6001\u4ea4\u4e92\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u4f4e\u5149\u573a\u666f\u4e0b\u7684\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u5149\u573a\u666f\u4e0b\u7531\u4e8e\u4fe1\u606f\u4e25\u91cd\u9000\u5316\uff0c\u5f80\u5f80\u727a\u7272\u5185\u5bb9\u4fdd\u771f\u5ea6\u4ee5\u83b7\u5f97\u66f4\u9ad8\u7684\u611f\u77e5\u771f\u5b9e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6761\u4ef6\u4f18\u5316\u7b56\u7565\uff0c\u5305\u62ec\u6f5c\u5728\u7ec6\u5316\u7ba1\u9053\u548c\u52a8\u6001\u4ea4\u4e92\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u63a7\u5236\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5185\u5bb9\u4fdd\u771f\u5ea6\uff0c\u4e14\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u7279\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5f15\u5165\u6761\u4ef6\u6f5c\u5728\u5efa\u6a21\u548c\u53cc\u5411\u4ea4\u4e92\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u4f4e\u5149\u573a\u666f\u4e0b\u7684\u5185\u5bb9\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u771f\u5b9e\u6027\u548c\u7f8e\u5b66\u6548\u679c\u3002"}}
{"id": "2510.17114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17114", "abs": "https://arxiv.org/abs/2510.17114", "authors": ["Hodaka Kawachi", "Tomoya Nakamura", "Hiroaki Santo", "SaiKiran Kumar Tedla", "Trevor Dalton Canham", "Yasushi Yagi", "Michael S. Brown"], "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras", "comment": null, "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.", "AI": {"tldr": "\u5229\u7528LED\u5149\u6e90\u4f18\u5316\u5149\u8c31\u7279\u6027\uff0c\u5b9e\u73b0\u5bf9\u76f8\u673a\u4e0d\u53ef\u89c1\u4f46\u53ef\u68c0\u6d4b\u7684\u6c34\u5370\u5d4c\u5165\uff0c\u652f\u6301\u9690\u79c1\u548c\u9a8c\u8bc1\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u53ef\u89c1\u5149\u901a\u4fe1\u4e2d\u6c34\u5370\u6613\u88ab\u5bdf\u89c9\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u4f4e\u5e27\u7387\u4e0b\u7684\u6c34\u5370\u63d0\u53d6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5149\u8c31\u8c03\u5236\u800c\u975e\u5f3a\u5ea6\u8c03\u5236\uff0c\u8003\u8651\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf\u7684\u654f\u611f\u6027\u548c\u76f8\u673a\u4f20\u611f\u5668\u7684\u5149\u8c31\u54cd\u5e94\uff0c\u4f18\u5316LED\u5149\u6e90\u7684\u5149\u8c31\u7279\u6027\u3002", "result": "\u572810\u79d2\u89c6\u9891\u4e2d\u5d4c\u5165128\u4f4d\u6c34\u5370\uff0c\u4fe1\u606f\u4f20\u8f93\u901f\u7387\u9002\u4e2d\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316LED\u5149\u6e90\u7684\u5149\u8c31\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6d88\u8d39\u8005\u76f8\u673a\u4e0d\u53ef\u89c1\u4f46\u9ad8\u6548\u68c0\u6d4b\u7684\u6c34\u5370\u5d4c\u5165\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u3002"}}
{"id": "2510.17131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17131", "abs": "https://arxiv.org/abs/2510.17131", "authors": ["Xin Gao", "Jiyao Liu", "Guanghao Li", "Yueming Lyu", "Jianxiong Gao", "Weichen Yu", "Ningsheng Xu", "Liang Wang", "Caifeng Shan", "Ziwei Liu", "Chenyang Si"], "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection", "comment": "28 pages, 16 figures, conference", "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.", "AI": {"tldr": "GOOD\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u6b21\u5f15\u5bfc\u8bbe\u8ba1\uff0c\u63d0\u5347OOD\u6837\u672c\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\uff0c\u663e\u8457\u589e\u5f3aOOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u6761\u4ef6\u5d4c\u5165\u7684\u6270\u52a8\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86OOD\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "GOOD\u6846\u67b6\u91c7\u7528\u56fe\u50cf\u7ea7\u548c\u7279\u5f81\u7ea7\u53cc\u5c42\u6b21\u5f15\u5bfc\uff1a\uff081\uff09\u56fe\u50cf\u7ea7\u5f15\u5bfc\u57fa\u4e8e\u68af\u5ea6\u964d\u4f4e\u8f93\u5165\u53ef\u80fd\u6027\uff1b\uff082\uff09\u7279\u5f81\u7ea7\u5f15\u5bfc\u57fa\u4e8ek-NN\u8ddd\u79bb\u4fc3\u8fdb\u7279\u5f81\u7a00\u758f\u533a\u57df\u91c7\u6837\u3002", "result": "GOOD\u751f\u6210\u7684\u6837\u672c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GOOD\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u6b21\u5f15\u5bfc\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u6837\u672c\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\uff0c\u8fdb\u800c\u589e\u5f3a\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.17137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17137", "abs": "https://arxiv.org/abs/2510.17137", "authors": ["WenBo Xu", "Liu Liu", "Li Zhang", "Ran Zhang", "Hao Wu", "Dan Guo", "Meng Wang"], "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation", "comment": null, "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.", "AI": {"tldr": "KineDiff3D\u662f\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u5b66\u611f\u77e5\u6269\u6563\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u89c6\u56fe\u8f93\u5165\u91cd\u5efa\u94f0\u63a5\u5bf9\u8c61\u5f62\u72b6\u548c\u751f\u6210\uff0c\u901a\u8fc7KA-VAE\u7f16\u7801\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u94f0\u63a5\u5bf9\u8c61\uff08\u5982\u7b14\u8bb0\u672c\u7535\u8111\u548c\u62bd\u5c49\uff09\u56e0\u5176\u591a\u90e8\u4ef6\u51e0\u4f55\u548c\u53ef\u53d8\u5173\u8282\u914d\u7f6e\u5728\u4e0d\u540c\u72b6\u6001\u4e0b\u5f15\u5165\u7ed3\u6784\u591a\u6837\u6027\uff0c\u5bf93D\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u9996\u5148\u901a\u8fc7\u65b0\u9896\u7684Kinematic-Aware VAE\uff08KA-VAE\uff09\u5c06\u5b8c\u6574\u51e0\u4f55\uff08SDFs\uff09\u3001\u5173\u8282\u89d2\u5ea6\u548c\u90e8\u4ef6\u5206\u5272\u7f16\u7801\u5230\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002\u968f\u540e\uff0c\u4f7f\u7528\u4e24\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff1a\u4e00\u4e2a\u7528\u4e8e\u56de\u5f52\u5168\u5c40\u59ff\u6001\uff08SE(3)\uff09\u548c\u5173\u8282\u53c2\u6570\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u4ece\u90e8\u5206\u89c2\u5bdf\u751f\u6210\u8fd0\u52a8\u5b66\u611f\u77e5\u6f5c\u5728\u4ee3\u7801\u3002\u6700\u540e\uff0c\u901a\u8fc7Chamfer\u8ddd\u79bb\u6700\u5c0f\u5316\u53cc\u5411\u4f18\u5316\u91cd\u5efa\u7cbe\u5ea6\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u94f0\u63a5\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKineDiff3D\u5728\u5408\u6210\u3001\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u91cd\u5efa\u94f0\u63a5\u5bf9\u8c61\u5e76\u4f30\u8ba1\u5176\u8fd0\u52a8\u5b66\u5c5e\u6027\u3002", "conclusion": "KineDiff3D\u6846\u67b6\u5728\u5408\u6210\u3001\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u51c6\u786e\u91cd\u5efa\u94f0\u63a5\u5bf9\u8c61\u548c\u4f30\u8ba1\u5176\u8fd0\u52a8\u5b66\u5c5e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17157", "abs": "https://arxiv.org/abs/2510.17157", "authors": ["Yinghui Wang", "Xinyu Zhang", "Peng Du"], "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image", "comment": null, "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.", "AI": {"tldr": "GACO-CAD\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91CAD\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4ece2D\u56fe\u50cf\u51c6\u786e\u63a8\u65ad3D\u51e0\u4f55\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u964d\u4f4e\u5de5\u4e1a\u6982\u5ff5\u8bbe\u8ba1\u7684\u95e8\u69db\u3002", "method": "\u5f15\u5165GACO-CAD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e24\u4e2a\u9636\u6bb5\u3002\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u6df1\u5ea6\u548c\u8868\u9762\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u5bc6\u96c6\u51e0\u4f55\u5148\u9a8c\uff0c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u7ec4\u957f\u5ea6\u5956\u52b1\u4ee5\u4fc3\u8fdb\u751f\u6210\u66f4\u7b80\u6d01\u7684\u5efa\u6a21\u5e8f\u5217\u3002", "result": "GACO-CAD\u5728\u76f8\u540cMLLM\u9aa8\u5e72\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u6709\u6548\u6027\u3001\u51e0\u4f55\u51c6\u786e\u6027\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GACO-CAD\u5728DeepCAD\u548cFusion360\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u4ee3\u7801\u6709\u6548\u6027\u3001\u51e0\u4f55\u51c6\u786e\u6027\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.17169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17169", "abs": "https://arxiv.org/abs/2510.17169", "authors": ["Roland Croft", "Brian Du", "Darcy Joseph", "Sharath Kumar"], "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition", "comment": "Accepted for publication in DICTA 2025", "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u9884\u5904\u7406\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9884\u5904\u7406\u9009\u62e9\u663e\u8457\u5f71\u54cd\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u3002", "motivation": "\u4eba\u8138\u9884\u5904\u7406\u662f\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u9884\u5904\u7406\u5bf9\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387\u7684\u5f71\u554a\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u51e0\u79cd\u73b0\u6210\u7684\u6700\u5148\u8fdb\u5bf9\u6297\u653b\u51fb\u5728\u4eba\u8138\u8bc6\u522b\u4e2d\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u7684\u4e0d\u540c\u9884\u5904\u7406\u6280\u672f\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u8f93\u5165\u53d8\u6362\u7684\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u7684\u9009\u62e9\u53ef\u4f7f\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u9ad8\u8fbe78%\uff0c\u800c\u4e0b\u91c7\u6837\u4e2d\u7684\u63d2\u503c\u65b9\u6cd5\u9009\u62e9\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6240\u7814\u7a76\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u9ad8\u8fbe27%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u5728\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8003\u8651\u9884\u5904\u7406\u4ee5\u6539\u5584\u9762\u90e8\u5bf9\u6297\u6837\u672c\u7684\u5bf9\u6297\u6cdb\u5316\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.17171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17171", "abs": "https://arxiv.org/abs/2510.17171", "authors": ["Feihong Yan", "Peiru Wang", "Yao Zhu", "Kaiyu Pang", "Qingyan Wei", "Huiqi Li", "Linfeng Zhang"], "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling", "comment": "12 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.", "AI": {"tldr": "GtR\u548cFTS\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u91c7\u6837\u548c\u9891\u7387\u52a0\u6743\u9009\u62e9\uff0c\u663e\u8457\u52a0\u901fMAR\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3MAR\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u65f6\u56e0\u7a7a\u95f4\u76f8\u5173\u89c6\u89c9\u4ee4\u724c\u5efa\u6a21\u590d\u6742\u800c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86GtR\uff08Generation then Reconstruction\uff09\u548cFTS\uff08Frequency-Weighted Token Selection\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5206\u522b\u901a\u8fc7\u5206\u5c42\u91c7\u6837\u548c\u9891\u7387\u52a0\u6743\u9009\u62e9\u6765\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u7c7b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e863.72\u500d\u52a0\u901f\uff0c\u4e14\u751f\u6210\u8d28\u91cf\uff08\u5982FID\u548cIS\uff09\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "GtR\u548cFTS\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86MAR\u6a21\u578b\u7684\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.17179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17179", "abs": "https://arxiv.org/abs/2510.17179", "authors": ["Yingzi Han", "Jiakai He", "Chuanlong Xie", "Jianping Li"], "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring", "comment": null, "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e8622\u79cdOoD\u68c0\u6d4b\u65b9\u6cd5\u5728\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0cViM\u5728Far-OoD\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u7684\u5206\u5e03\u504f\u79fb\uff08OoD\uff09\u9762\u4e34\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u6574\u5408\u6700\u65b0\u8ba1\u7b97\u673a\u89c6\u89c9\u8fdb\u5c55\u5e76\u5efa\u7acb\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u57fa\u4e8eDYB-PlanktonNet\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u6a21\u62df\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u573a\u666f\u7684OoD\u57fa\u51c6\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e8622\u79cdOoD\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "ViM\u65b9\u6cd5\u5728\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5728Far-OoD\u573a\u666f\u4e2d\u5173\u952e\u6307\u6807\u6709\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u9886\u57df\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u3001\u7cfb\u7edf\u6027\u7684OoD\u68c0\u6d4b\u65b9\u6cd5\u8bc4\u4f30\u4e0e\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002ViM\u65b9\u6cd5\u5728Far-OoD\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002"}}
{"id": "2510.17181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17181", "abs": "https://arxiv.org/abs/2510.17181", "authors": ["Haonan He", "Yufeng Zheng", "Jie Song"], "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video", "comment": "ICCV 2025", "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u5934\u90e8\u865a\u62df\u5f62\u8c61\u548c\u624b-\u8138\u4e92\u52a8\u53d8\u5f62\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u987a\u5e8f\u635f\u5931\u3001PCA\u57fa\u7840\u548c\u63a5\u89e6\u635f\u5931\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4ec5\u5173\u6ce8\u9762\u90e8\u533a\u57df\uff0c\u5ffd\u7565\u4e86\u624b-\u8138\u4e92\u52a8\u4f20\u8fbe\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5982\u601d\u8003\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u987a\u5e8f\u635f\u5931\u4e0e\u63a5\u89e6\u6b63\u5219\u5316\u8fdb\u884c\u59ff\u6001\u8ddf\u8e2a\uff0c\u5b66\u4e60\u624b\u5f15\u8d77\u7684\u9762\u90e8\u53d8\u5f62\u7684PCA\u57fa\u7840\uff0c\u5e76\u5f15\u5165\u63a5\u89e6\u635f\u5931\u4ee5\u51cf\u5c11\u4e92\u7a7f\u4f2a\u5f71\u3002", "result": "\u5728RGB(D)\u89c6\u9891\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9762\u90e8\u5916\u89c2\u548c\u53d8\u5f62\u51e0\u4f55\u4e0a\u4f18\u4e8e\u73b0\u6709\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u8054\u5408\u5b66\u4e60\u8be6\u7ec6\u7684\u5934\u90e8\u865a\u62df\u5f62\u8c61\u548c\u624b-\u8138\u4e92\u52a8\u5f15\u8d77\u7684\u975e\u521a\u6027\u53d8\u5f62\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5916\u89c2\u548c\u53d8\u5f62\u51e0\u4f55\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.17188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17188", "abs": "https://arxiv.org/abs/2510.17188", "authors": ["Vaibhav Rathore", "Divyam Gupta", "Biplab Banerjee"], "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery", "comment": "Accpeted at NeurIPS (2025) Main Conference", "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.", "AI": {"tldr": "HIDISC\u662f\u4e00\u4e2a\u53cc\u66f2\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7GPT\u5f15\u5bfc\u7684\u6269\u6563\u548cTangent CutMix\u5b9e\u73b0\u57df\u548c\u7c7b\u522b\u7ea7\u6cdb\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u65f6\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u6765\u81ea\u540c\u4e00\u57df\uff0c\u9650\u5236\u4e86\u5728\u6d89\u53ca\u5206\u5e03\u53d8\u5316\u7684\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002DG-GCD\u8981\u6c42\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u4e0d\u8bbf\u95ee\u76ee\u6807\u57df\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u5230\u5305\u542b\u65b0\u7c7b\u522b\u7684\u672a\u89c1\u57df\uff0c\u73b0\u6709\u65b9\u6cd5DG2CD-Net\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u3002", "method": "HIDISC\u91c7\u7528\u53cc\u66f2\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7GPT\u5f15\u5bfc\u7684\u6269\u6563\u589e\u5f3a\u6e90\u57df\uff0c\u5f15\u5165Tangent CutMix\u8fdb\u884c\u66f2\u7387\u611f\u77e5\u63d2\u503c\uff0c\u5e76\u7ed3\u5408\u7edf\u4e00\u7684\u635f\u5931\u51fd\u6570\uff08\u5305\u62ec\u60e9\u7f5aBusemann\u5bf9\u9f50\u3001\u6df7\u5408\u53cc\u66f2\u5bf9\u6bd4\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u79bb\u7fa4\u6392\u65a5\uff09\u6765\u4fc3\u8fdb\u7d27\u51d1\u3001\u8bed\u4e49\u7ed3\u6784\u5316\u7684\u5d4c\u5165\u3002", "result": "HIDISC\u5728PACS\u3001Office-Home\u548cDomainNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HIDISC\u6846\u67b6\u5728PACS\u3001Office-Home\u548cDomainNet\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\uff08DG\uff09-GCD\u57fa\u7ebf\u3002"}}
{"id": "2510.17197", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17197", "abs": "https://arxiv.org/abs/2510.17197", "authors": ["Pu Zhang", "Yuwei Li", "Xingyuan Xian", "Guoming Tang"], "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.", "AI": {"tldr": "\u63d0\u51fa\u63d0\u793a\u611f\u77e5\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u5e73\u8861\u4efb\u52a1\u76f8\u5173\u6027\u4e0e\u591a\u6837\u6027\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u4e14\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u8f93\u5165\u89c4\u6a21\u7684\u589e\u52a0\u5bfc\u81f4\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\u548c\u63a8\u7406\u6210\u672c\u6fc0\u589e\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5ffd\u89c6\u6587\u672c\u63d0\u793a\u7684\u5f15\u5bfc\uff0c\u65e0\u6cd5\u6709\u6548\u4f18\u5148\u8003\u8651\u4efb\u52a1\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u9996\u5148\u9009\u62e9\u4e00\u7ec4\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6838\u5fc3\u89c6\u89c9\u4ee4\u724c\uff0c\u7136\u540e\u8865\u5145\u591a\u6837\u6027\u4ee4\u724c\u4ee5\u4fdd\u7559\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u526a\u679d\u9ad8\u8fbe90%\u4ee4\u724c\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u63d0\u793a\u611f\u77e5\u89c6\u89d2\uff0c\u5728\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u4e2d\u5e73\u8861\u4efb\u52a1\u76f8\u5173\u6027\u548c\u4fe1\u606f\u591a\u6837\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17198", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17198", "abs": "https://arxiv.org/abs/2510.17198", "authors": ["M Saifuzzaman Rafat", "Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Jungpil Shin"], "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh", "comment": "Submitted to the International Conference on Data and Applied\n  Analytics (IDAA 2025). 15 pages, 5 figures, 4 tables", "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528SAM\u6a21\u578b\u548c\u9996\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u7cbe\u5ea6\u7684\u6cb3\u5cb8\u4fb5\u8680\u76d1\u6d4b\u5de5\u5177\uff0c\u4e3a\u5b5f\u52a0\u62c9\u56fd\u7684\u707e\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7684\u5927\u6cb3\u6d41\u65e2\u662f\u5546\u4e1a\u548c\u751f\u8ba1\u7684\u52a8\u8109\uff0c\u4e5f\u662f\u65e0\u60c5\u7684\u7834\u574f\u8005\u3002\u6bcf\u5e74\uff0c\u5b83\u4eec\u541e\u566c\u6574\u4e2a\u6751\u5e84\u548c\u5927\u7247\u519c\u7530\uff0c\u5bfc\u81f4\u6570\u5343\u5bb6\u5ead\u6d41\u79bb\u5931\u6240\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8ddf\u8e2a\u8fd9\u4e00\u7f13\u6162\u7684\u707e\u96be\u3002", "method": "\u7814\u7a76\u9996\u5148\u4f7f\u7528\u7b80\u5355\u7684\u989c\u8272\u901a\u9053\u5206\u6790\u8fdb\u884c\u7c97\u7565\u7684\u571f\u5730\u548c\u6c34\u57df\u5206\u5272\uff0c\u7136\u540e\u5fae\u8c03Segment Anything Model\uff08SAM\uff09\u7684\u63a9\u7801\u89e3\u7801\u5668\uff0c\u4ee5\u8bc6\u522b\u6cb3\u5cb8\u4fb5\u8680\u7684\u7ec6\u5fae\u7279\u5f81\u3002", "result": "\u5f00\u53d1\u51fa\u7684\u6a21\u578b\u5728\u6cb3\u5cb8\u4fb5\u8680\u8bc6\u522b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747IoU\u4e3a86.30%\uff0cDice\u5f97\u5206\u4e3a92.60%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6210\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u4e13\u95e8\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u548c\u9996\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e3a\u5b5f\u52a0\u62c9\u56fd\u6cb3\u6d41\u4fb5\u8680\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u5de5\u5177\uff0c\u5e2e\u52a9\u653f\u7b56\u5236\u5b9a\u8005\u548c\u707e\u5bb3\u7ba1\u7406\u673a\u6784\u66f4\u597d\u5730\u9884\u6d4b\u548c\u4fdd\u62a4\u8106\u5f31\u793e\u533a\u3002"}}
{"id": "2510.17199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17199", "abs": "https://arxiv.org/abs/2510.17199", "authors": ["Nirai Hayakawa", "Kazumasa Shimari", "Kazuma Yamasaki", "Hirotatsu Hoshikawa", "Rikuto Tsuchida", "Kenichi Matsumoto"], "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis", "comment": "Accepted to IEEE 2025 Conference on Games", "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790VALORANT\u6bd4\u8d5b\u5f55\u50cf\u7684\u5c0f\u5730\u56fe\u4fe1\u606f\u63d0\u53d6\u6218\u672f\u7279\u5f81\uff0c\u6784\u5efa\u7684\u9884\u6d4b\u6a21\u578b\u5728\u56de\u5408\u4e2d\u671f\u9636\u6bb5\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe81%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u6bd4\u8d5b\u65e5\u5fd7\u6570\u636e\u548c\u7edf\u8ba1\u4fe1\u606f\uff0c\u800cVALORANT\u4f5c\u4e3a\u9700\u8981\u590d\u6742\u7b56\u7565\u7684FPS\u6e38\u620f\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u6218\u672f\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u89c6\u9891\u8bc6\u522b\u6a21\u578bTimeSformer\uff0c\u901a\u8fc7\u5206\u6790\u5c0f\u5730\u56fe\u4fe1\u606f\u63d0\u53d6\u6218\u672f\u7279\u5f81\uff08\u5982\u89d2\u8272\u4f4d\u7f6e\u4fe1\u606f\u548c\u5176\u4ed6\u6e38\u620f\u5185\u4e8b\u4ef6\uff09\uff0c\u6784\u5efa\u56de\u5408\u7ed3\u679c\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u589e\u5f3a\u6218\u672f\u4e8b\u4ef6\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u7ea6\u4e3a81%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5c0f\u5730\u56fe\u4fe1\u606f\u7684\u6a21\u578b\u3002", "conclusion": "\u5229\u7528\u6bd4\u8d5b\u5f55\u50cf\u4e2d\u7684\u6218\u672f\u7279\u5f81\u5bf9VALORANT\u56de\u5408\u7ed3\u679c\u9884\u6d4b\u975e\u5e38\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u56de\u5408\u4e2d\u671f\u9636\u6bb5\u3002"}}
{"id": "2510.17200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17200", "abs": "https://arxiv.org/abs/2510.17200", "authors": ["Bingrong Liu", "Jun Shi", "Yushan Zheng"], "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification", "comment": null, "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.", "AI": {"tldr": "EndoCIL\u662f\u4e00\u4e2a\u4e13\u4e3a\u5185\u7aa5\u955c\u56fe\u50cf\u8bca\u65ad\u8bbe\u8ba1\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u3001\u7c7b\u5e73\u8861\u548c\u68af\u5ea6\u6821\u51c6\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u5728\u73b0\u5b9e\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u91cd\u653e\u7684CIL\u65b9\u6cd5\u7531\u4e8e\u4e25\u91cd\u7684\u57df\u5dee\u5f02\u548c\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "EndoCIL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u91cd\u653e\uff08MDBR\uff09\u3001\u5148\u9a8c\u6b63\u5219\u5316\u7c7b\u5e73\u8861\u635f\u5931\uff08PRCBL\uff09\u548c\u5168\u8fde\u63a5\u68af\u5ea6\u6821\u51c6\uff08CFG\uff09\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEndoCIL\u5728\u4e0d\u540c\u7f13\u51b2\u533a\u5927\u5c0f\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u901a\u5e38\u4f18\u4e8e\u6700\u5148\u8fdb\u7684CIL\u65b9\u6cd5\u3002", "conclusion": "EndoCIL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408MDBR\u3001PRCBL\u548cCFG\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u6709\u6548\u5e73\u8861\u4e86\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u5728\u7ec8\u8eab\u5185\u7aa5\u955c\u8bca\u65ad\u4e2d\u5c55\u73b0\u51fa\u4e34\u5e8a\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2510.17201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17201", "abs": "https://arxiv.org/abs/2510.17201", "authors": ["Mika Feng", "Pierre Gallin-Martel", "Koichi Ito", "Takafumi Aoki"], "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing", "comment": "ICCV 2025 Workshop FAS", "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDINOv2\u7684\u6b3a\u9a97\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u7ec6\u5fae\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u9632\u6b62\u6076\u610f\u653b\u51fb\u8005\u5229\u7528\u7167\u7247\u7ed5\u8fc7\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff0c\u9700\u5728\u8bc6\u522b\u524d\u68c0\u6d4b\u6b3a\u9a97\u653b\u51fb\u3002", "method": "\u91c7\u7528DINOv2\u6a21\u578b\u7ed3\u5408\u5bc4\u5b58\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u7ec6\u5fae\u7279\u5f81\u3002", "result": "\u5728ICCV2025\u548cSiW\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDINOv2\u7684\u6b3a\u9a97\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u53ef\u6cdb\u5316\u7279\u5f81\u5e76\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6270\u52a8\uff0c\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.17205", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17205", "abs": "https://arxiv.org/abs/2510.17205", "authors": ["Yingqi Fan", "Anhao Zhao", "Jinlan Fu", "Junlong Tong", "Hui Su", "Yijie Pan", "Wei Zhang", "Xiaoyu Shen"], "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs", "comment": "EMNLP 2025 Main", "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u7684\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u526a\u679d\u6846\u67b6VisiPruner\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "MLLMs\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9MLLMs\u5982\u4f55\u5904\u7406\u548c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u63ed\u793a\u4e86MLLMs\u4e2d\u7684\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86VisiPruner\u6846\u67b6\u3002", "result": "VisiPruner\u5728LLaVA-v1.5 7B\u4e0a\u51cf\u5c11\u4e8699%\u7684\u89c6\u89c9\u76f8\u5173\u6ce8\u610f\u529b\u8ba1\u7b97\u548c53.9%\u7684FLOPs\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VisiPruner\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86MLLMs\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u591a\u79cdMLLMs\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17218", "abs": "https://arxiv.org/abs/2510.17218", "authors": ["Zhuo Cao", "Heming Du", "Bingqing Zhang", "Xin Yu", "Xue Li", "Sen Wang"], "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions", "comment": "Accepted to NeurIPS 2025", "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQV-M^2\u6570\u636e\u96c6\u548cFlashMMR\u6846\u67b6\uff0c\u586b\u8865\u591a\u65f6\u523b\u68c0\u7d22\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u65f6\u523b\u68c0\u7d22\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFlashMMR\u7684\u6846\u67b6\uff0c\u5305\u542b\u591a\u65f6\u523b\u540e\u9a8c\u8bc1\u6a21\u5757\uff0c\u901a\u8fc7\u7ea6\u675f\u65f6\u95f4\u8c03\u6574\u548c\u9a8c\u8bc1\u6a21\u5757\u4f18\u5316\u5019\u9009\u7247\u6bb5\u3002", "result": "\u5728QV-M^2\u6570\u636e\u96c6\u4e0a\uff0cFlashMMR\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5728G-mAP\u3001mAP@3+tgt\u548cmR@3\u4e0a\u5206\u522b\u63d0\u5347\u4e863.00%\u30012.70%\u548c2.56%\u3002", "conclusion": "\u63d0\u51fa\u7684QV-M^2\u6570\u636e\u96c6\u548cFlashMMR\u6846\u67b6\u4e3a\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\u548c\u5f3a\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u65f6\u523b\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17264", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17264", "abs": "https://arxiv.org/abs/2510.17264", "authors": ["Akihito Yoshii", "Ryosuke Sonoda", "Ramya Srinivasan"], "title": "Fair and Interpretable Deepfake Detection in Videos", "comment": "10 pages (including References)", "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.", "AI": {"tldr": "\u63d0\u51fa\u516c\u5e73\u611f\u77e5\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u5e8f\u7279\u5f81\u548c\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e14\u65e0\u6cd5\u6355\u6349\u65f6\u5e8f\u4fe1\u606f\uff0c\u5bfc\u81f4\u8de8\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u7684\u51b3\u7b56\u504f\u9887\u548c\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u7ed3\u5408\u65f6\u5e8f\u7279\u5f81\u5b66\u4e60\u548c\u4eba\u53e3\u7edf\u8ba1\u611f\u77e5\u6570\u636e\u589e\u5f3a\uff0c\u5229\u7528\u5e8f\u5217\u805a\u7c7b\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\u548c\u6982\u5ff5\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u4eba\u53e3\u7edf\u8ba1\u611f\u77e5\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5728FaceForensics++\u3001DFD\u3001Celeb-DF\u548cDFDC\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Xception\u548cResNet\u67b6\u6784\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u516c\u5e73\u611f\u77e5\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.17269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17269", "abs": "https://arxiv.org/abs/2510.17269", "authors": ["Luis Wiedmann", "Orr Zohar", "Amir Mahla", "Xiaohan Wang", "Rui Li", "Thibaud Frere", "Leandro von Werra", "Aritra Roy Gosthipaty", "Andr\u00e9s Marafioti"], "title": "FineVision: Open Data Is All You Need", "comment": null, "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.", "AI": {"tldr": "FineVision\u662f\u4e00\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u6536\u96c6\u548c\u6574\u7406\u76842400\u4e07\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u7edf\u4e00\u4e86200\u591a\u4e2a\u6765\u6e90\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0c\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u53d7\u5230\u516c\u5171\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\u548c\u6c61\u67d3\u7684\u963b\u788d\uff0cFineVision\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u3001\u4eba\u5de5\u53c2\u4e0e\u7684\u6d41\u7a0b\uff0c\u7edf\u4e00\u4e86200\u591a\u4e2a\u6765\u6e90\u7684\u6570\u636e\uff0c\u5f62\u6210185\u4e2a\u5b50\u96c6\uff0c\u5305\u62ec\u6279\u91cf\u6444\u5165\u3001\u6a21\u5f0f\u6620\u5c04\u3001\u5ba1\u6838\u6620\u5c04\u3001\u62bd\u67e5\u8f93\u51fa\u3001\u53bb\u91cd\u548c\u53bb\u6c61\u67d3\u7b49\u6b65\u9aa4\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u8bc4\u4f30\u5957\u4ef6\u4e2d\uff0c\u4f7f\u7528FineVision\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u653e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "FineVision\u7684\u53d1\u5e03\u65e8\u5728\u52a0\u901f\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u901a\u8fc7\u63d0\u4f9b\u89c4\u6a21\u5927\u3001\u6570\u636e\u536b\u751f\u826f\u597d\u4e14\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u76d1\u7763\u5e73\u8861\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2510.17274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17274", "abs": "https://arxiv.org/abs/2510.17274", "authors": ["Katie Luo", "Jingwei Ji", "Tong He", "Runsheng Xu", "Yichen Xie", "Dragomir Anguelov", "Mingxing Tan"], "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models", "comment": "In proceedings of IROS 2025", "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.", "AI": {"tldr": "PnF\u5229\u7528MLLMs\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u4e2d\u6210\u672c\u6548\u76ca\u5730\u6cdb\u5316\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u63d0\u793a\u8bcd\u4eceMLLMs\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u573a\u666f\u7406\u89e3\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u5d4c\u5165\u4ee5\u589e\u5f3a\u73b0\u6709\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728Waymo Open Motion Dataset\u548cnuScenes Dataset\u4e0a\u9a8c\u8bc1\uff0cPnF\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PnF\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17278", "categories": ["cs.CV", "68T07, 92C55", "I.4.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17278", "abs": "https://arxiv.org/abs/2510.17278", "authors": ["Mehdi Zekriyapanah Gashti", "Mostafa Mohammadpour", "Ghasem Farjamnia"], "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation", "comment": null, "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.", "AI": {"tldr": "SG-CLDFF\u6846\u67b6\u901a\u8fc7\u663e\u8457\u6027\u5f15\u5bfc\u7684\u9884\u5904\u7406\u548c\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u4e86\u767d\u7ec6\u80de\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u5206\u5272\u548c\u5206\u7c7b\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u767d\u7ec6\u80de\u5bf9\u8bca\u65ad\u548c\u76d1\u6d4b\u8bb8\u591a\u8840\u6db2\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u67d3\u8272\u53d8\u5f02\u6027\u3001\u590d\u6742\u80cc\u666f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "SG-CLDFF\u6846\u67b6\u7ed3\u5408\u4e86\u663e\u8457\u6027\u9a71\u52a8\u7684\u9884\u5904\u7406\u4e0e\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\u805a\u5408\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df7\u5408\u9aa8\u5e72\u7f51\u7edc\uff08EfficientSwin-style\uff09\u751f\u6210\u591a\u5206\u8fa8\u7387\u8868\u793a\uff0c\u5e76\u901a\u8fc7ResNeXt-CC\u542f\u53d1\u7684\u8de8\u5c42\u878d\u5408\u6a21\u5757\u878d\u5408\u6d45\u5c42\u548c\u6df1\u5c42\u4fe1\u606f\u3002\u7f51\u7edc\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\uff0c\u540c\u65f6\u8fdb\u884c\u5206\u5272\u548c\u7ec6\u80de\u7c7b\u578b\u5206\u7c7b\uff0c\u4f7f\u7528\u7c7b\u611f\u77e5\u52a0\u6743\u635f\u5931\u548c\u663e\u8457\u6027\u5bf9\u9f50\u6b63\u5219\u5316\u6765\u7f13\u89e3\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u6807\u51c6\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff08BCCD\u3001LISC\u3001ALL-IDB\uff09\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u62a5\u544a\u4e86\u5728IoU\u3001F1\u548c\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u7684\u4e00\u81f4\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u663e\u8457\u6027\u9884\u5904\u7406\u548c\u8de8\u5c42\u878d\u5408\u7684\u4e2a\u4f53\u8d21\u732e\u3002", "conclusion": "SG-CLDFF\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u767d\u7ec6\u80de\u81ea\u52a8\u5206\u6790\u3002"}}
{"id": "2510.17330", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17330", "abs": "https://arxiv.org/abs/2510.17330", "authors": ["Gyuhwan Park", "Kihyun Na", "Injung Kim"], "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration", "comment": "11 pages, 6 figures", "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.", "AI": {"tldr": "CharDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8f66\u724c\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u5f15\u5bfc\u548cCHARM\u6a21\u5757\u663e\u8457\u63d0\u5347\u6062\u590d\u548c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u8f66\u724c\u56fe\u50cf\u6062\u590d\u7684\u91cd\u8981\u6027\u4e0d\u4ec5\u5728\u4e8eLPR\u7cfb\u7edf\u7684\u9884\u5904\u7406\u9636\u6bb5\uff0c\u8fd8\u5305\u62ec\u63d0\u9ad8\u8bc1\u636e\u4ef7\u503c\u3001\u589e\u5f3a\u89c6\u89c9\u754c\u9762\u6e05\u6670\u5ea6\u4ee5\u53ca\u4fc3\u8fdb\u8f66\u724c\u56fe\u50cf\u7684\u8fdb\u4e00\u6b65\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6CharDiff\uff0c\u7ed3\u5408\u5b57\u7b26\u7ea7\u5f15\u5bfc\u548cCHARM\u6a21\u5757\uff0c\u5229\u7528\u5916\u90e8\u5206\u5272\u548cOCR\u6a21\u5757\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5b57\u7b26\u7ea7\u5148\u9a8c\u3002", "result": "CharDiff\u5728\u6062\u590d\u8d28\u91cf\u548c\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6062\u590d\u6a21\u578b\uff0c\u5728Roboflow-LP\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8628%\u7684\u76f8\u5bf9CER\u964d\u4f4e\u3002", "conclusion": "CharDiff\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u5b57\u7b26\u7ea7\u5f15\u5bfc\u6709\u6548\u589e\u5f3a\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u724c\u7167\u56fe\u50cf\u6062\u590d\u548c\u8bc6\u522b\u5728\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17287", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17287", "abs": "https://arxiv.org/abs/2510.17287", "authors": ["Amir Gharghabi", "Mahdi Hakiminezhad", "Maryam Shafaei", "Shaghayegh Gharghabi"], "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation", "comment": null, "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eYOLOv11\u7684\u81ea\u52a8\u624b\u672f\u7167\u660e\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bc6\u522b\u84dd\u8272\u6807\u8bb0\u81ea\u52a8\u8c03\u6574\u5149\u6e90\uff0c\u51cf\u5c11\u533b\u751f\u75b2\u52b3\u5e76\u63d0\u9ad8\u7167\u660e\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u7167\u660e\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\uff0c\u6613\u5bfc\u81f4\u5916\u79d1\u533b\u751f\u75b2\u52b3\u3001\u9888\u90e8\u52b3\u635f\u4ee5\u53ca\u56e0\u6f02\u79fb\u548c\u9634\u5f71\u5bfc\u81f4\u7684\u7167\u660e\u4e0d\u4e00\u81f4\u3002", "method": "\u5229\u7528YOLOv11\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u8bc6\u522b\u624b\u672f\u76ee\u6807\u4e0a\u65b9\u7684\u84dd\u8272\u6807\u8bb0\uff0c\u901a\u8fc7\u4e24\u4e2a\u5e26\u6709\u503e\u659c-\u5e73\u79fb\u652f\u67b6\u7684\u4f3a\u670d\u7535\u673a\u5c06\u9ad8\u529f\u7387LED\u5149\u6e90\u5b9a\u5411\u5230\u8bc6\u522b\u4f4d\u7f6e\u3002", "result": "YOLO\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523096.7%\u7684mAP@50\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u6a21\u62df\u624b\u672f\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8eYOLOv11\u7b97\u6cd5\u7684\u81ea\u52a8\u624b\u672f\u7167\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u51cf\u5c11\u5916\u79d1\u533b\u751f\u7684\u4f53\u529b\u8d1f\u62c5\uff0c\u63d0\u9ad8\u7167\u660e\u4e00\u81f4\u6027\uff0c\u5e76\u652f\u6301\u66f4\u597d\u7684\u624b\u672f\u7ed3\u679c\u3002"}}
{"id": "2510.17299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17299", "abs": "https://arxiv.org/abs/2510.17299", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning", "comment": "Accepted by NeurIPS 2025", "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.", "AI": {"tldr": "SSL\u8bad\u7ec3\u8fc7\u957f\u53ef\u80fd\u635f\u5bb3\u5bc6\u96c6\u4efb\u52a1\u6027\u80fd\uff08SDD\u73b0\u8c61\uff09\u3002\u8bba\u6587\u63d0\u51fa\u65e0\u76d1\u7763\u8bc4\u4f30\u5de5\u5177DSE\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u53d1\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4e2d\u957f\u65f6\u95f4\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff08\u79f0\u4e3aSDD\u73b0\u8c61\uff09\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u65e0\u76d1\u7763\u8bc4\u4f30\u5bc6\u96c6\u6027\u80fd\u7684\u6709\u6548\u624b\u6bb5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u8bbe\u8ba1\u4e86DSE\uff0c\u5305\u62ec\u7c7b\u522b\u76f8\u5173\u6027\u5ea6\u91cf\u548c\u6709\u6548\u7ef4\u5ea6\u5ea6\u91cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u5bc6\u96c6\u8868\u793a\u7684\u7ed3\u6784\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8eDSE\u7684\u6a21\u578b\u9009\u62e9\u7b56\u7565\u5e73\u5747\u63d0\u5347mIoU 3.0%\uff0cDSE\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u5bc6\u96c6\u9000\u5316\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86DSE\uff08Dense representation Structure Estimator\uff09\u4f5c\u4e3a\u65e0\u76d1\u7763\u8bc4\u4f30\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\u7684\u5de5\u5177\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u6a21\u578b\u9009\u62e9\u7b56\u7565\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u5bc6\u96c6\u9000\u5316\u73b0\u8c61\u3002"}}
{"id": "2510.17305", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17305", "abs": "https://arxiv.org/abs/2510.17305", "authors": ["ZhaoYang Han", "Qihan Lin", "Hao Liang", "Bowen Chen", "Zhou Liu", "Wentao Zhang"], "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding", "comment": "Submitted to ARR Rolling Review", "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.", "AI": {"tldr": "LongInsightBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u957f\u89c6\u9891\u591a\u6a21\u6001\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7cbe\u9009\u89c6\u9891\u548c\u8bbe\u8ba1\u590d\u6742\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5168\u6a21\u6001\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u89c6\u9891\u4e2d\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u3001\u89c6\u89d2\u3001\u52a8\u4f5c\u548c\u5176\u4ed6\u4e0a\u4e0b\u6587\u5143\u7d20\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c24\u5176\u662f\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u573a\u666f\u3002", "method": "\u901a\u8fc7\u4ece\u5f00\u6e90\u6570\u636e\u96c6FineVideo\u4e2d\u7cbe\u9009\u7ea61,000\u4e2a\u89c6\u9891\uff0c\u8bbe\u8ba1\u516d\u79cd\u6311\u6218\u6027\u4efb\u52a1\u573a\u666f\uff0c\u5e76\u5f00\u53d1\u4e09\u6b65\u534a\u81ea\u52a8\u5316\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\uff0c\u786e\u4fdd\u5408\u6210\u95ee\u9898\u548c\u7b54\u6848\u9009\u9879\u7684\u96be\u5ea6\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5168\u6a21\u6001\u6a21\u578b\u5728\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\uff08T-Loc\uff09\u548c\u957f\u8ddd\u79bb\u56e0\u679c\u63a8\u7406\uff08CE-Caus\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6269\u5c55\u5b9e\u9a8c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5904\u7406\u504f\u5dee\u3002", "conclusion": "LongInsightBench\u662f\u7b2c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5728\u6574\u5408\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u65b9\u9762\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5168\u6a21\u6001\u6a21\u578b\uff08OLMs\uff09\u5728\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u8ddd\u79bb\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2510.17482", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17482", "abs": "https://arxiv.org/abs/2510.17482", "authors": ["Chenxu Dang", "Haiyan Liu", "Guangjun Bao", "Pei An", "Xinyue Tang", "Jie Ma", "Bingchuan Sun", "Yan Wang"], "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries", "comment": "Under Review", "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.", "AI": {"tldr": "SparseWorld\u662f\u4e00\u79cd\u65b0\u578b4D\u5360\u7528\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u67e5\u8be2\u548c\u81ea\u9002\u5e94\u6a21\u5757\u63d0\u5347\u611f\u77e5\u4e0e\u9884\u6d4b\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u5360\u7528\u4e16\u754c\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u56fa\u5b9a\u5d4c\u5165\u6216\u7f51\u683c\uff0c\u9650\u5236\u4e86\u611f\u77e5\u7684\u7075\u6d3b\u6027\uff0c\u4e14\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u52a8\u6001\u8fde\u7eed\u6027\u5b58\u5728\u6f5c\u5728\u4e0d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86Range-Adaptive Perception\u6a21\u5757\u548cState-Conditioned Forecasting\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86Temporal-Aware Self-Scheduling\u8bad\u7ec3\u7b56\u7565\u3002", "result": "SparseWorld\u5728\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "SparseWorld\u901a\u8fc7\u7a00\u758f\u52a8\u6001\u67e5\u8be2\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u76844D\u5360\u7528\u4e16\u754c\u6a21\u578b\uff0c\u5728\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17318", "abs": "https://arxiv.org/abs/2510.17318", "authors": ["Sangyoon Bae", "Jiook Cha"], "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference", "comment": null, "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.", "AI": {"tldr": "CausalMamba\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7BOLD\u53bb\u5377\u79ef\u548c\u65b0\u578b\u56e0\u679c\u56fe\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86fMRI\u56e0\u679c\u63a8\u65ad\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u7684\u7f51\u7edc\u52a8\u6001\u3002", "motivation": "\u89e3\u51b3fMRI\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7684\u6839\u672c\u9650\u5236\uff1a\u4ece\u8840\u6d41\u52a8\u529b\u5b66\u626d\u66f2\u7684BOLD\u4fe1\u53f7\u63a8\u65ad\u795e\u7ecf\u56e0\u679c\u6027\u7684\u4e0d\u9002\u5b9a\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u52a8\u6001\u56e0\u679c\u5efa\u6a21DCM\uff09\u7684\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u3002", "method": "CausalMamba\u5c06\u590d\u6742\u7684\u9006\u5411\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u53ef\u5904\u7406\u7684\u9636\u6bb5\uff1aBOLD\u53bb\u5377\u79ef\u4ee5\u6062\u590d\u6f5c\u5728\u795e\u7ecf\u6d3b\u52a8\uff0c\u968f\u540e\u4f7f\u7528\u65b0\u578bConditional Mamba\u67b6\u6784\u8fdb\u884c\u56e0\u679c\u56fe\u63a8\u65ad\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\uff0cCausalMamba\u6bd4DCM\u51c6\u786e\u7387\u9ad837%\u3002\u5728\u771f\u5b9e\u4efb\u52a1fMRI\u6570\u636e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ee588%\u7684\u4fdd\u771f\u5ea6\u6062\u590d\u4e86\u5df2\u77e5\u795e\u7ecf\u901a\u8def\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u8d85\u8fc799%\u7684\u53d7\u8bd5\u8005\u4e2d\u672a\u80fd\u8bc6\u522b\u8fd9\u4e9b\u5178\u578b\u7535\u8def\u3002", "conclusion": "CausalMamba\u4e3a\u795e\u7ecf\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u56e0\u679c\u63a8\u65ad\u4e2d\u6355\u6349\u57fa\u672c\u7535\u8def\u6a21\u5f0f\u548c\u7075\u6d3b\u7684\u7f51\u7edc\u52a8\u6001\uff0c\u652f\u6301\u8ba4\u77e5\u529f\u80fd\u7814\u7a76\u3002"}}
{"id": "2510.17501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17501", "abs": "https://arxiv.org/abs/2510.17501", "authors": ["Yuanli Wu", "Long Zhang", "Yue Du", "Bin Li"], "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "comment": null, "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u89c6\u9891\u6458\u8981\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u7ea7\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u7ebf\u7d22\uff0c\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u5bf9\u4eba\u5de5\u6a21\u677f\u548c\u6570\u636e\u96c6\u7279\u5b9a\u8bc4\u5206\u5f52\u4e00\u5316\u654f\u611f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u5c06\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u8f6c\u5316\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\uff0c\u5e76\u805a\u5408\u4e3a\u7ed3\u6784\u5316\u3001\u6570\u636e\u96c6\u81ea\u9002\u5e94\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u6307\u5bfc\u53ef\u89e3\u91ca\u7684\u573a\u666f\u8bc4\u4f30\u3002", "result": "\u5728SumMe\u548cTVSum\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u8fbe\u523057.58\u548c63.05\uff0c\u8d85\u8d8a\u65e0\u76d1\u7763\u548c\u5148\u524d\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u6709\u6548\u7a33\u5b9a\u4e86\u57fa\u4e8eLLM\u7684\u8bc4\u5206\uff0c\u4e3a\u96f6\u6837\u672c\u89c6\u9891\u6458\u8981\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.17322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17322", "abs": "https://arxiv.org/abs/2510.17322", "authors": ["Wei Zhang", "Zhanhao Hu", "Xiao Li", "Xiaopei Zhu", "Xiaolin Hu"], "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World", "comment": "13 pages, 8 figures", "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5bf9\u6297\u5927\u5c3a\u5bf8\u5bf9\u6297\u8863\u7269\uff0c\u63ed\u793a\u5176\u666e\u904d\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u5bf9\u6297\u5927\u5c3a\u5bf8\u5bf9\u6297\u8865\u4e01\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u6297\u8863\u7269\u8fd9\u79cd\u66f4\u81ea\u7136\u7684\u653b\u51fb\u5f62\u5f0f\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\u5bf9\u6297\u5927\u5c3a\u5bf8\u5bf9\u6297\u8863\u7269\u7684\u6548\u679c\uff0c\u5e76\u5236\u4f5c\u4e86\u4e00\u7ec4\u80fd\u591f\u7a81\u7834\u591a\u79cd\u9632\u5fa1\u7684\u5bf9\u6297\u8863\u7269\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u6570\u5b57\u548c\u7269\u7406\u4e16\u754c\u4e2d\u5bf9\u6297\u5bf9\u6297\u8863\u7269\u65f6\u8868\u73b0\u5747\u4e0d\u4f73\uff0c\u5176\u4e2d\u4e00\u7ec4\u5bf9\u6297\u8863\u7269\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7a81\u7834\u4e86\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u73b0\u6709\u7684\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u5927\u8986\u76d6\u8303\u56f4\u7684\u5bf9\u6297\u8863\u7269\u65f6\u5b58\u5728\u666e\u904d\u8106\u5f31\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2510.17519", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17519", "abs": "https://arxiv.org/abs/2510.17519", "authors": ["Yongshun Zhang", "Zhongyi Fan", "Yonghang Zhang", "Zhangzikang Li", "Weifeng Chen", "Zhongwei Feng", "Chaoyue Wang", "Peng Hou", "Anxiang Zeng"], "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models", "comment": "Technical Report; Project Page:\n  \\href{https://github.com/Shopee-MUG/MUG-V}", "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.", "AI": {"tldr": "\u63d0\u51fa\u4f18\u5316\u56db\u5927\u652f\u67f1\u7684\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u6846\u67b6MUG-V 10B\uff0c\u6027\u80fd\u5339\u914dSOTA\u4e14\u5f00\u6e90\u5b8c\u6574\u4ee3\u7801\uff0c\u9996\u6b21\u5b9e\u73b0Megatron-Core\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u957f\u5e8f\u5217\u548c\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f18\u5316\u4e86\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u56db\u5927\u652f\u67f1\uff0c\u6db5\u76d6\u89c6\u9891\u538b\u7f29\u3001\u53c2\u6570\u7f29\u653e\u3001\u8bfe\u7a0b\u5f0f\u9884\u8bad\u7ec3\u548c\u5bf9\u9f50\u540e\u8bad\u7ec3\u7b49\u73af\u8282\u3002", "result": "MUG-V 10B\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u5339\u914d\u6700\u65b0\u89c6\u9891\u751f\u6210\u5668\uff0c\u5728\u7535\u5b50\u5546\u52a1\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u8fd1\u7ebf\u6027\u591a\u8282\u70b9\u6269\u5c55\u3002", "conclusion": "MUG-V 10B\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u9996\u6b21\u516c\u5f00\u4e86\u57fa\u4e8eMegatron-Core\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u4ee3\u7801\u548c\u63a8\u7406\u6d41\u7a0b\u3002"}}
{"id": "2510.17529", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17529", "abs": "https://arxiv.org/abs/2510.17529", "authors": ["Yovin Yahathugoda", "Davide Prezzi", "Piyalitt Ittichaiwong", "Vicky Goh", "Sebastien Ourselin", "Michela Antonelli"], "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation", "comment": null, "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.", "AI": {"tldr": "MambaX-Net \u662f\u4e00\u79cd\u65b0\u578b\u534a\u76d1\u7763 3D \u5206\u5272\u67b6\u6784\uff0c\u7528\u4e8e\u7eb5\u5411\u524d\u5217\u817a\u764c\u76d1\u6d4b\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u548c\u5f62\u72b6\u63d0\u53d6\u5668\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\u5728\u7eb5\u5411 AS \u5206\u6790\u4e2d\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u591a\u65f6\u95f4\u70b9\u548c\u4e13\u5bb6\u6807\u6ce8\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa MambaX-Net\uff0c\u4e00\u79cd\u534a\u76d1\u7763\u3001\u53cc\u626b\u63cf 3D \u5206\u5272\u67b6\u6784\uff0c\u7ed3\u5408\u4e86 Mamba \u589e\u5f3a\u7684\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u548c\u5f62\u72b6\u63d0\u53d6\u5668\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u534a\u76d1\u7763\u81ea\u8bad\u7ec3\u7b56\u7565\u3002", "result": "MambaX-Net \u5728\u7eb5\u5411 AS \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u5272\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "MambaX-Net \u5728\u7eb5\u5411\u4e3b\u52a8\u76d1\u6d4b\uff08AS\uff09\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684 U-Net \u548c Transformer \u6a21\u578b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u548c\u566a\u58f0\u6570\u636e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5353\u8d8a\u7684\u524d\u5217\u817a\u533a\u57df\u5206\u5272\u3002"}}
{"id": "2510.17332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17332", "abs": "https://arxiv.org/abs/2510.17332", "authors": ["Zhaoran Zhao", "Xinli Yue", "Jianhui Sun", "Yuhao Xie", "Tao Shao", "Liangchao Yao", "Fan Xia", "Yuetang Deng"], "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA", "comment": "Accepted to ICCV 2025 Workshop", "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.", "AI": {"tldr": "iDETEX\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u548c\u6df7\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u8be6\u7ec6\u89e3\u91ca\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u4ece\u6807\u91cf\u8d28\u91cf\u9884\u6d4b\u5411\u66f4\u53ef\u89e3\u91ca\u3001\u4e0e\u4eba\u5bf9\u9f50\u7684\u8bc4\u4f30\u8303\u5f0f\u53d1\u5c55\u7684\u65b0\u5174\u6311\u6218\u3002", "method": "\u63d0\u51faiDETEX\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u79bb\u7ebf\u589e\u5f3a\u6a21\u5757\u548c\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u8f85\u4ee5\u5728\u7ebf\u589e\u5f3a\u7b56\u7565\u4ee5\u5145\u5206\u5229\u7528\u591a\u6e90\u76d1\u7763\u3002", "result": "\u5728ViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ciDETEX\u5728\u6240\u6709\u5b50\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "iDETEX\u5728ViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728ICCV MIPI 2025\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17338", "abs": "https://arxiv.org/abs/2510.17338", "authors": ["Jiahao Huo", "Mufhumudzi Muthivhi", "Terence L. van Zyl", "Fredrik Gustafsson"], "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition", "comment": null, "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540e\u5904\u7406OSR\u65b9\u6cd5\uff0c\u901a\u8fc7NCM\u4e0esoftmax\u6982\u7387\u7684\u4e00\u81f4\u6027\u6d4b\u91cf\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5b9a\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u91ce\u751f\u52a8\u7269\u5206\u7c7b\u6a21\u578b\u5728\u5c01\u95ed\u4e16\u754c\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\uff0c\u9762\u5bf9\u672a\u77e5\u7c7b\u522b\u65f6\u4f1a\u8fc7\u5ea6\u81ea\u4fe1\u3002\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u65e8\u5728\u5206\u7c7b\u5df2\u77e5\u7c7b\u522b\u5e76\u62d2\u7edd\u672a\u77e5\u6837\u672c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8f93\u5165\u5230\u5176\u6700\u8fd1\u7c7b\u5747\u503c\uff08NCM\uff09\u8ddd\u79bb\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4e0e\u903b\u8f91\u7a7a\u95f4\u7684softmax\u6982\u7387\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8861\u91cfNCM\u4e0e\u5206\u7c7b\u5934\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u4e2d\u6392\u540d\u524d\u4e09\uff0cAUROC\u5206\u522b\u8fbe\u523093.41\uff08\u975e\u6d32\u52a8\u7269\uff09\u548c95.35\uff08\u745e\u5178\u52a8\u7269\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u7684\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u7279\u5f81\u4e0e\u9884\u6d4b\u903b\u8f91\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u7a33\u5b9a\u6027\u80fd\u3002"}}
{"id": "2510.17626", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17626", "abs": "https://arxiv.org/abs/2510.17626", "authors": ["Fr\u00e9d\u00e9ric LIN", "Biruk Abere Ambaw", "Adrian Popescu", "Hejer Ammar", "Romaric Audigier", "Herv\u00e9 Le Borgne"], "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation", "comment": "To be published in NeurIPS 2025 Track on Datasets and Benchmarks", "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.", "AI": {"tldr": "CaMiT\u662f\u4e00\u4e2a\u6355\u6349\u6c7d\u8f66\u6a21\u578b\u65f6\u95f4\u6f14\u5316\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u7b56\u7565\u548c\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u9c81\u68d2\u6027\u548c\u751f\u6210\u771f\u5b9e\u6027\u3002", "motivation": "AI\u7cfb\u7edf\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u89c6\u89c9\u73af\u5883\uff0c\u5c24\u5176\u662f\u5728\u7269\u4f53\u5916\u89c2\u968f\u65f6\u95f4\u53d8\u5316\u7684\u9886\u57df\u3002CaMiT\u6570\u636e\u96c6\u65e8\u5728\u6355\u6349\u6c7d\u8f66\u6a21\u578b\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u652f\u6301\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u8bbe\u7f6e\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u7b56\u7565\uff1a\u65f6\u95f4\u589e\u91cf\u9884\u8bad\u7ec3\u548c\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u5668\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u4e86\u5229\u7528\u65f6\u95f4\u5143\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\u3002", "result": "\u9759\u6001\u9884\u8bad\u7ec3\u5728\u9886\u57df\u5185\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u4e14\u8d44\u6e90\u9ad8\u6548\uff0c\u4f46\u5728\u8de8\u5e74\u6d4b\u8bd5\u4e2d\u51c6\u786e\u6027\u4e0b\u964d\u3002\u65f6\u95f4\u589e\u91cf\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u548c\u5206\u7c7b\u5668\u5b66\u4e60\uff09\u63d0\u9ad8\u4e86\u65f6\u95f4\u9c81\u68d2\u6027\u3002\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\u80fd\u4ea7\u751f\u66f4\u771f\u5b9e\u7684\u8f93\u51fa\u3002", "conclusion": "CaMiT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e30\u5bcc\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u7814\u7a76\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u548c\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u9002\u5e94\u6027\u3002"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "Semantic-E2VID\u901a\u8fc7\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u548c\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\uff08E2V\uff09\u4efb\u52a1\u4e2d\u56e0\u4ec5\u6355\u6349\u5f3a\u5ea6\u53d8\u5316\u800c\u5ffd\u7565\u9759\u6001\u5bf9\u8c61\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u89c6\u8bed\u4e49\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86Semantic-E2VID\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff08CFA\uff09\u6a21\u5757\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\uff08SFF\uff09\u5757\uff0c\u5229\u7528Segment Anything Model\uff08SAM\uff09\u7684\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684E2V\u76d1\u7763\u6765\u4f18\u5316\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemantic-E2VID\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5e27\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709E2V\u65b9\u6cd5\u3002", "conclusion": "Semantic-E2VID\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u65b9\u9762\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.17651", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17651", "abs": "https://arxiv.org/abs/2510.17651", "authors": ["S\u00e9bastien Thuau", "Siba Haidar", "Ayush Bajracharya", "Rachid Chelouah"], "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs", "comment": "7 pages, 1 figure, FLTA 2025", "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86VLM\u548cCNN3D\u5728\u8054\u90a6\u66b4\u529b\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0CNN3D\u66f4\u8282\u80fd\u4e14\u6027\u80fd\u7565\u4f18\uff0c\u4f46VLM\u5728\u590d\u6742\u573a\u666f\u4e2d\u66f4\u9002\u7528\u3002\u63d0\u51fa\u4e86\u6df7\u5408\u6a21\u578b\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u63a2\u7a76\u8282\u4fed\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u66b4\u529b\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u80fd\u6e90\u6548\u7387\u548c\u73af\u5883\u6307\u6807\u3002", "method": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u7b56\u7565\uff1a(i) \u96f6\u6837\u672c\u548c\u8054\u90a6\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c(ii) \u4e2a\u6027\u5316\u8bad\u7ec3\u7684\u7d27\u51d13D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN3D\uff09\u3002\u4f7f\u7528LLaVA-7B\u548c65.8M\u53c2\u6570\u7684CNN3D\u4f5c\u4e3a\u4ee3\u8868\u6848\u4f8b\uff0c\u8bc4\u4f30\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u8bbe\u7f6e\u4e0b\u7684\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u80fd\u8017\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u5747\u8d85\u8fc790%\u3002CNN3D\u5728ROC AUC\u548c\u5bf9\u6570\u635f\u5931\u4e0a\u7565\u4f18\u4e8eLoRA\u8c03\u4f18\u7684VLM\uff0c\u4e14\u80fd\u8017\u66f4\u4f4e\u3002VLM\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u4ecd\u5177\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4e00\u79cd\u6df7\u5408\u6a21\u578b\uff1a\u8f7b\u91cf\u7ea7CNN\u7528\u4e8e\u5e38\u89c4\u5206\u7c7b\uff0c\u9009\u62e9\u6027\u6fc0\u6d3bVLM\u5904\u7406\u590d\u6742\u573a\u666f\u3002\u8be5\u6846\u67b6\u4e3a\u89c6\u9891\u76d1\u63a7\u4e2d\u8d1f\u8d23\u4efb\u3001\u8d44\u6e90\u611f\u77e5\u7684AI\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2510.17364", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17364", "abs": "https://arxiv.org/abs/2510.17364", "authors": ["Vaggelis Dorovatas", "Soroush Seifi", "Gunshi Gupta", "Rahaf Aljundi"], "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs", "comment": "NeurIPS 2025", "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee4\u724c\u9009\u62e9\u548c\u9012\u5f52\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d41\u5a92\u4f53\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d41\u5a92\u4f53\u573a\u666f\u4e0b\u7684\u6311\u6218\uff0c\u5982\u5c0f\u65f6\u957f\u89c6\u9891\u7684\u5728\u7ebf\u5904\u7406\u548c\u53ca\u65f6\u54cd\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u517c\u5bb9\u6807\u51c6\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video-LLMs\uff09\uff0c\u5229\u7528\u4e09\u4e2a\u5173\u952e\u6982\u5ff5\uff1a1\uff09\u57fa\u4e8eLLM\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\uff1b2\uff09\u8fc7\u53bb\u9009\u5b9a\u4ee4\u724c\u7684\u9012\u5f52\u5904\u7406\uff1b3\uff09\u57fa\u4e8e\u6807\u9898\u7684\u8f7b\u91cf\u7ea7\u95ee\u7b54\u3002", "result": "\u80fd\u591f\u4e22\u5f03\u7ea695%\u7684\u4e0d\u91cd\u8981\u89c6\u89c9\u4ee4\u724c\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\uff0c\u5e76\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u6548\u679c\u3002"}}
{"id": "2510.17681", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17681", "abs": "https://arxiv.org/abs/2510.17681", "authors": ["Yuandong Pu", "Le Zhuo", "Songhao Han", "Jinbo Xing", "Kaiwen Zhu", "Shuo Cao", "Bin Fu", "Si Liu", "Hongsheng Li", "Yu Qiao", "Wenlong Zhang", "Xi Chen", "Yihao Liu"], "title": "PICABench: How Far Are We from Physically Realistic Image Editing?", "comment": null, "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPICABench\u8bc4\u4f30\u56fe\u50cf\u7f16\u8f91\u7684\u7269\u7406\u771f\u5b9e\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u4e0d\u8db3\uff0c\u5e76\u63a2\u7d22\u4e86\u4ece\u89c6\u9891\u5b66\u4e60\u7269\u7406\u6548\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7f16\u8f91\u6a21\u578b\u548c\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u5b8c\u6210\uff0c\u4f46\u5ffd\u89c6\u4e86\u7269\u7406\u6548\u5e94\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\u548c\u7269\u4f53\u95f4\u4ea4\u4e92\uff09\u3002\u4f5c\u8005\u65e8\u5728\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u771f\u5b9e\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86PICABench\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516b\u4e2a\u5b50\u7ef4\u5ea6\uff08\u6db5\u76d6\u5149\u5b66\u3001\u529b\u5b66\u548c\u72b6\u6001\u8f6c\u6362\uff09\u7684\u7269\u7406\u771f\u5b9e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86PICAEval\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7528VLM-as-a-judge\u7ed3\u5408\u533a\u57df\u7ea7\u4eba\u5de5\u6807\u6ce8\u548c\u95ee\u9898\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u7269\u7406\u6548\u5e94\u6784\u5efa\u4e86\u8bad\u7ec3\u6570\u636e\u96c6PICA-100K\u3002", "result": "\u8bc4\u4f30\u4e3b\u6d41\u6a21\u578b\u540e\u53d1\u73b0\uff0c\u7269\u7406\u771f\u5b9e\u6027\u4ecd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5b58\u5728\u5927\u91cf\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7269\u7406\u771f\u5b9e\u7684\u56fe\u50cf\u7f16\u8f91\u4ecd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5b58\u5728\u5927\u91cf\u63a2\u7d22\u7a7a\u95f4\u3002\u4f5c\u8005\u5e0c\u671b\u5176\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u80fd\u4e3a\u672a\u6765\u4ece\u7b80\u5355\u5185\u5bb9\u7f16\u8f91\u8fc8\u5411\u7269\u7406\u4e00\u81f4\u771f\u5b9e\u6027\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.17372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17372", "abs": "https://arxiv.org/abs/2510.17372", "authors": ["Pawe\u0142 Borsukiewicz", "Fadi Boutros", "Iyiola E. Olatunji", "Charles Beumier", "Wendk\u00fbuni C. Ouedraogo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise", "comment": null, "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.", "AI": {"tldr": "\u5408\u6210\u9762\u90e8\u6570\u636e\u5728\u8bc6\u522b\u51c6\u786e\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u4f5c\u4e3a\u9762\u90e8\u8bc6\u522b\u7814\u7a76\u7684\u4f26\u7406\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u7684\u90e8\u7f72\u5f15\u53d1\u4e86\u4f26\u7406\u56f0\u5883\uff0c\u9ad8\u7cbe\u5ea6\u9700\u8981\u5927\u91cf\u672a\u7ecf\u540c\u610f\u7684\u771f\u5b9e\u9762\u90e8\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u64a4\u56de\u548c\u6cd5\u5f8b\u98ce\u9669\u3002\u5408\u6210\u9762\u90e8\u6570\u636e\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\u7f3a\u4e4f\u5168\u9762\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\u786e\u5b9a25\u4e2a\u5408\u6210\u9762\u90e8\u8bc6\u522b\u6570\u636e\u96c6\uff082018-2025\uff09\uff0c\u7ed3\u5408\u4e25\u683c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u9690\u79c1\u4fdd\u62a4\u5408\u6210\u6570\u636e\u7684\u4e03\u4e2a\u5173\u952e\u8981\u6c42\u3002", "result": "\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6\uff08VariFace, VIGFace\uff09\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u8fbe95.67%\u548c94.91%\uff0c\u8d85\u8fc7\u771f\u5b9e\u6570\u636e\u96c6CASIA-WebFace\uff0894.70%\uff09\u3002\u5408\u6210\u6570\u636e\u5728\u4fdd\u6301\u8eab\u4efd\u53ef\u5206\u6027\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u8fc7\u751f\u6210\u53c2\u6570\u7f13\u89e3\u504f\u89c1\u7684\u524d\u6240\u672a\u6709\u7684\u63a7\u5236\u3002", "conclusion": "\u5408\u6210\u9762\u90e8\u6570\u636e\u5728\u79d1\u5b66\u4e0a\u53ef\u884c\u4e14\u4f26\u7406\u4e0a\u662f\u9762\u90e8\u8bc6\u522b\u7814\u7a76\u7684\u5fc5\u8981\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.17684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17684", "abs": "https://arxiv.org/abs/2510.17684", "authors": ["Xinwei Zhang", "Hu Chen", "Zhe Yuan", "Sukun Tian", "Peng Feng"], "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model", "comment": null, "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.", "AI": {"tldr": "IC-MoE\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u9009\u62e9\u548c\u8bed\u4e49\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u9ad8\u5c42\u7279\u5f81\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u6784\u5b8c\u6574\u6027\u88ab\u7834\u574f\u7684\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "1. \u6784\u5efa\u57fa\u7840\u4e13\u5bb6\u3001\u8bed\u4e49\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u4e13\u5bb6\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u6982\u7387\u81ea\u9002\u5e94\u6295\u7968\u7b56\u7565\u5b9e\u73b0\u4e13\u5bb6\u9009\u62e9\u548c\u878d\u5408\u30022. \u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u589e\u5f3a\u9ad8\u5c42\u7279\u5f81\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIC-MoE\u4f18\u4e8e\u5176\u4ed6SOTA\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684IC-MoE\u6a21\u578b\u6709\u6548\u8865\u5145\u4e86\u57fa\u7840\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u9ad8\u5c42\u7279\u5f81\u548c\u9884\u8bad\u7ec3\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u5272\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17373", "abs": "https://arxiv.org/abs/2510.17373", "authors": ["Yintao Zhou", "Wei Huang", "Zhengyu Li", "Jing Huang", "Meng Pang"], "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing", "comment": "3 pages, 2 figures, accepted by MIND 2025", "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u8868\u60c5\u7684\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u65b9\u6cd5\uff0c\u6574\u5408\u591a\u79cd\u8868\u60c5\u7279\u5f81\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9762\u90e8\u8868\u60c5\u7684\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5355\u4e00\u8868\u60c5\u7c7b\u578b\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u8bca\uff0c\u4e14\u5ffd\u89c6\u4e0d\u540cPD\u9636\u6bb5\u95f4\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u4e8c\u5143\u5206\u7c7b\u800c\u975ePD\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9762\u90e8\u8868\u60c5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u6574\u5408\u591a\u79cd\u9762\u90e8\u8868\u60c5\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6837\u672c\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728PD\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u4e2d\u5177\u6709\u826f\u597d\u6027\u80fd\uff0c\u4e14\u7279\u5f81\u878d\u5408\u548c\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u6709\u6548\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u4e5f\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.17685", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17685", "abs": "https://arxiv.org/abs/2510.17685", "authors": ["Min Cao", "Xinyu Zhou", "Ding Jiang", "Bo Du", "Mang Ye", "Min Zhang"], "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning", "comment": "Final version published in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI). Xplore link:\n  https://ieeexplore.ieee.org/document/11199360", "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.", "AI": {"tldr": "\u63d0\u51faBi-IRRA\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u89e3\u51b3\u591a\u8bed\u8a00\u6587\u672c\u5230\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u7684\u6a21\u6001\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u6027\u80fd\u6700\u4f18\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u4e2d\u7684\u6a21\u6001\u5f02\u8d28\u6027\u548c\u591a\u8bed\u8a00\u5e94\u7528\u9650\u5236\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u591a\u8bed\u8a00\u652f\u6301\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faBi-IRRA\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u6a21\u5757\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6a21\u5757\uff0c\u7528\u4e8e\u8de8\u8bed\u8a00\u548c\u6a21\u6001\u7684\u5bf9\u9f50\u5b66\u4e60\u3002", "result": "\u5728\u6240\u6709\u591a\u8bed\u8a00TIPR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Bi-IRRA\u6846\u67b6\u5728\u591a\u8bed\u8a00TIPR\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u5f02\u8d28\u6027\u548c\u591a\u8bed\u8a00\u6311\u6218\u3002"}}
{"id": "2510.17384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17384", "abs": "https://arxiv.org/abs/2510.17384", "authors": ["Jiajin Tang", "Zhengxuan Wei", "Ge Zheng", "Sibei Yang"], "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding", "comment": "Accepted at ICCV 2025", "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.", "AI": {"tldr": "LoopTrans\u901a\u8fc7\u95ed\u73af\u6846\u67b6\u548c\u8de8\u6a21\u6001\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u8de8\u89c6\u89d2\u77e5\u8bc6\u8f6c\u79fb\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4ea4\u4e92\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4ece\u5916\u4e2d\u5fc3\u89c6\u89d2\u5355\u5411\u8f6c\u79fb\u5230\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165LoopTrans\u95ed\u73af\u6846\u67b6\uff0c\u5305\u62ec\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u5b9a\u4f4d\u548c\u53bb\u566a\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u4ee5\u5f25\u5408\u89c6\u89d2\u5dee\u5f02\u5e76\u589e\u5f3a\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLoopTrans\u5728\u6240\u6709\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u751a\u81f3\u80fd\u5904\u7406\u5b8c\u5168\u88ab\u4eba\u4f53\u906e\u6321\u7684\u4ea4\u4e92\u533a\u57df\u3002", "conclusion": "LoopTrans\u901a\u8fc7\u95ed\u73af\u6846\u67b6\u548c\u521b\u65b0\u7684\u8de8\u6a21\u6001\u5b9a\u4f4d\u53ca\u53bb\u566a\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u89c6\u89d2\u77e5\u8bc6\u8f6c\u79fb\u7684\u6548\u679c\uff0c\u5373\u4f7f\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u4e5f\u80fd\u6709\u6548\u5904\u7406\u3002"}}
{"id": "2510.17703", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17703", "abs": "https://arxiv.org/abs/2510.17703", "authors": ["Mhd Adnan Albani", "Riad Sonbol"], "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns", "comment": "19 pages, 2 figures, 9 tables", "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u548c\u96c6\u6210\u7b56\u7565\u663e\u8457\u63d0\u5347\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u672a\u89c1\u60a3\u8005\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u5904\u7406\u672a\u89c1\u60a3\u8005\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6839\u636e\u7ed8\u56fe\u7c7b\u578b\u5206\u7c7b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5206\u5757\u7b56\u7565\uff082x2\u5206\u5757\uff09\u63d0\u53d6\u7279\u5f81\u5e76\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\uff0c\u6700\u540e\u4f7f\u7528\u96c6\u6210\u65b9\u6cd5\u5408\u5e76\u51b3\u7b56\u3002", "result": "\u5728NewHandPD\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u5df2\u77e5\u60a3\u8005\u7684\u51c6\u786e\u7387\u4e3a97.08%\uff0c\u5bf9\u672a\u89c1\u60a3\u8005\u7684\u51c6\u786e\u7387\u4e3a94.91%\uff0c\u6027\u80fd\u5dee\u8ddd\u4ec5\u4e3a2.17\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u672a\u89c1\u60a3\u8005\u6570\u636e\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.17409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17409", "abs": "https://arxiv.org/abs/2510.17409", "authors": ["Dmitrii Galimzianov", "Viacheslav Vyshegorodtsev", "Ivan Nezhivykh"], "title": "Monitoring Horses in Stalls: From Object to Event Detection", "comment": "12 pages, 4 figures, 4 tables", "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u9a6c\u53a9\u76d1\u6d4b\u7cfb\u7edf\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u8ddf\u8e2a\u9a6c\u5339\u4e0e\u4eba\uff0c\u4e3a\u5b9e\u65f6\u884c\u4e3a\u76d1\u6d4b\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u76d1\u6d4b\u9a6c\u5339\u884c\u4e3a\u5bf9\u65e9\u671f\u53d1\u73b0\u5065\u5eb7\u4e0e\u798f\u5229\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\u3002", "method": "\u5229\u7528YOLOv11\u548cBoT-SORT\u8fdb\u884c\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff0c\u901a\u8fc7\u7269\u4f53\u8f68\u8ff9\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u65ad\u4e8b\u4ef6\u72b6\u6001\u3002", "result": "\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5728\u9a6c\u76f8\u5173\u4e8b\u4ef6\u4e0a\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u68c0\u6d4b\u4eba\u5458\u65b9\u9762\u5b58\u5728\u6570\u636e\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u9a6c\u53a9\u5b9e\u65f6\u884c\u4e3a\u76d1\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u52a8\u7269\u798f\u5229\u548c\u7a33\u5b9a\u7ba1\u7406\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2510.17722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17722", "abs": "https://arxiv.org/abs/2510.17722", "authors": ["Yaning Pan", "Zekun Wang", "Qianqian Xie", "Yongqian Wen", "Yuanxing Zhang", "Guohui Zhang", "Haoxuan Hu", "Zhiyu Pan", "Yibing Huang", "Zhidong Gan", "Yonghong Lin", "An Ping", "Tianhao Peng", "Jiaheng Liu"], "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench", "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.", "AI": {"tldr": "MT-Video-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u8f6e\u95ee\u7b54\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b987\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u8f6e\u5bf9\u8bdd\u7684MT-Video-Bench\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90MLLM\u5728\u516d\u9879\u6838\u5fc3\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709MLLM\u5728\u5904\u7406\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\u3002", "conclusion": "MT-Video-Bench\u7684\u5f15\u5165\u586b\u8865\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.17422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17422", "abs": "https://arxiv.org/abs/2510.17422", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "DeepDetect: Learning All-in-One Dense Keypoints", "comment": "6 pages, 6 figures, 2 tables, 7 equations", "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).", "AI": {"tldr": "DeepDetect\u662f\u4e00\u79cd\u667a\u80fd\u3001\u4e00\u4f53\u5316\u3001\u9ad8\u5bc6\u5ea6\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u4f20\u7edf\u68c0\u6d4b\u5668\u4f18\u52bf\uff0c\u5728\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u53ca\u57fa\u4e8e\u5b66\u4e60\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u5b58\u5728\u5bf9\u5149\u5ea6\u53d8\u5316\u654f\u611f\u3001\u5173\u952e\u70b9\u5bc6\u5ea6\u548c\u91cd\u590d\u6027\u4f4e\u3001\u9002\u5e94\u6027\u6709\u9650\u3001\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u4f18\u5148\u5904\u7406\u89c6\u89c9\u91cd\u8981\u533a\u57df\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7\u878d\u54087\u79cd\u5173\u952e\u70b9\u548c2\u79cd\u8fb9\u7f18\u68c0\u6d4b\u5668\u7684\u8f93\u51fa\u521b\u5efa\u5730\u9762\u771f\u5b9e\u63a9\u7801\uff0c\u63d0\u53d6\u591a\u6837\u5316\u7684\u89c6\u89c9\u7ebf\u7d22\uff1b\u7136\u540e\uff0c\u4f7f\u7528ESPNet\u8fd9\u4e00\u8f7b\u91cf\u9ad8\u6548\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7fDeepDetect\u80fd\u591f\u8bed\u4e49\u5316\u5730\u5173\u6ce8\u56fe\u50cf\u5e76\u751f\u6210\u9ad8\u5bc6\u5ea6\u5173\u952e\u70b9\u3002", "result": "\u5728Oxford Affine Covariant Regions\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDeepDetect\u5728\u5173\u952e\u70b9\u5bc6\u5ea6\uff080.5143\uff09\u3001\u91cd\u590d\u6027\uff080.9582\uff09\u548c\u6b63\u786e\u5339\u914d\u6570\uff0859,003\uff09\u4e0a\u5747\u8fbe\u5230\u6700\u9ad8\u503c\u3002", "conclusion": "DeepDetect\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7edf\u4e00\u4e86\u4f20\u7edf\u68c0\u6d4b\u5668\u7684\u4f18\u52bf\uff0c\u5728\u5173\u952e\u70b9\u5bc6\u5ea6\u3001\u91cd\u590d\u6027\u548c\u6b63\u786e\u5339\u914d\u6570\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5728\u591a\u6837\u5316\u53ca\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u5f3a\u5927\u9002\u5e94\u6027\u3002"}}
{"id": "2510.17724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17724", "abs": "https://arxiv.org/abs/2510.17724", "authors": ["Matheus Ramos Parracho"], "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization", "comment": "Undergraduate thesis (preprint)---submitted to Escola Polit\\'ecnica,\n  Universidade Federal do Rio de Janeiro (POLI/UFRJ). The final version will\n  include official signatures and defense approval", "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u548cshell\u9884\u5904\u7406\u7684\u7b7e\u540d\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u53d1\u73b0\u539f\u59cb\u56fe\u50cf\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46shell\u9884\u5904\u7406\u6a21\u578b\u6709\u6539\u8fdb\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u79bb\u7ebf\u7b7e\u540d\u9a8c\u8bc1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u4ee5\u63d0\u9ad8\u7b7e\u540d\u4f2a\u9020\u68c0\u6d4b\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e24\u6761\u5b9e\u9a8c\u6d41\u7a0b\uff1a\u4e00\u6761\u57fa\u4e8e\u539f\u59cb\u7b7e\u540d\u56fe\u50cf\uff0c\u53e6\u4e00\u6761\u91c7\u7528\u79f0\u4e3ashell\u9884\u5904\u7406\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u7684\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cshell\u9884\u5904\u7406\u6a21\u578b\u663e\u793a\u51fa\u672a\u6765\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u672a\u660e\u786e\u786e\u5b9a\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u4f46\u53d1\u73b0\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u57fa\u4e8eshell\u9884\u5904\u7406\u7684\u6a21\u578b\u663e\u793a\u51fa\u672a\u6765\u6539\u8fdb\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17434", "abs": "https://arxiv.org/abs/2510.17434", "authors": ["Julien Zouein", "Hossein Javidnia", "Fran\u00e7ois Piti\u00e9", "Anil Kokaram"], "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching", "comment": "Accepted ICIR 2025, camera-ready version", "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.", "AI": {"tldr": "\u5229\u7528AV1\u8fd0\u52a8\u77e2\u91cf\u751f\u6210\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u4f4eCPU\u6d88\u8017\u4e0b\u5b9e\u73b0\u4e0eSIFT\u76f8\u5f53\u7684\u5339\u914d\u6548\u679c\uff0c\u9002\u7528\u4e8e\u77ed\u89c6\u9891\u573a\u666f\u3002", "motivation": "\u5728\u77ed\u89c6\u9891\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728CPU\u4f7f\u7528\u91cf\u8fdc\u4f4e\u4e8e\u987a\u5e8fSIFT\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u4ea7\u751f\u66f4\u5bc6\u96c6\u7684\u5339\u914d\u3002", "method": "\u91cd\u65b0\u5229\u7528AV1\u8fd0\u52a8\u77e2\u91cf\u751f\u6210\u5bc6\u96c6\u5b50\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u4e00\u81f4\u6027\u8fc7\u6ee4\u77ed\u8f68\u8ff9\u3002", "result": "\u5728117\u5e27\u7684\u7247\u6bb5\u4e0a\uff0cMV\u5339\u914d\u6ce8\u518c\u4e86\u6240\u6709\u56fe\u50cf\uff0c\u5e76\u91cd\u5efa\u4e860.46-0.62M\u4e2a\u70b9\uff0c\u91cd\u6295\u5f71\u8bef\u5dee\u4e3a0.51-0.53\u50cf\u7d20\uff1bBA\u65f6\u95f4\u968f\u5339\u914d\u5bc6\u5ea6\u589e\u52a0\u800c\u589e\u957f\u3002", "conclusion": "\u538b\u7f29\u57df\u5bf9\u5e94\u5173\u7cfb\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u524d\u7aef\u65b9\u6cd5\uff0c\u5177\u6709\u5728\u5b8c\u6574\u6d41\u7a0b\u4e2d\u6269\u5c55\u7684\u660e\u786e\u8def\u5f84\u3002"}}
{"id": "2510.17773", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17773", "abs": "https://arxiv.org/abs/2510.17773", "authors": ["Md. Enamul Atiq", "Shaikh Anowarul Fattah"], "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion", "comment": "15 pages, 7 Figures, 3 Tables", "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u75c5\u7076\u5206\u5272\u548c\u4e34\u5e8a\u6570\u636e\u7684\u53cc\u7f16\u7801\u5668\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ae\u80a4\u764c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u663e\u8457\u6539\u5584\u60a3\u8005\u9884\u540e\u3002\u7531\u4e8e\u9ad8\u7c7b\u5185\u53d8\u5f02\u6027\u548c\u7ec6\u5fae\u7684\u7c7b\u95f4\u5dee\u5f02\uff0c\u81ea\u52a8\u5316\u8bca\u65ad\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u8bb8\u591a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u201c\u9ed1\u76d2\u5b50\u201d\u9650\u5236\u4e86\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7f16\u7801\u5668\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5272\u75c5\u7076\u548c\u4e34\u5e8a\u5143\u6570\u636e\u8fdb\u884c\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u3002\u91c7\u7528\u5e26\u53cc\u6ce8\u610f\u95e8\uff08DAG\uff09\u548c\u7a7a\u6d1e\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08ASPP\uff09\u7684Deep-UNet\u67b6\u6784\u8fdb\u884c\u75c5\u7076\u5206\u5272\uff0c\u5206\u7c7b\u9636\u6bb5\u4f7f\u7528\u4e24\u4e2aDenseNet201\u7f16\u7801\u5668\uff08\u4e00\u4e2a\u7528\u4e8e\u539f\u59cb\u56fe\u50cf\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u5206\u5272\u75c5\u7076\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6a21\u5757\u6574\u5408\u60a3\u8005\u5143\u6570\u636e\uff08\u5e74\u9f84\u3001\u6027\u522b\u3001\u75c5\u7076\u90e8\u4f4d\uff09\u3002", "result": "\u5728HAM10000\u6570\u636e\u96c6\u53caISIC 2018\u548c2019\u6311\u6218\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u75c5\u7076\u5206\u5272\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u5e73\u5747AUC\u3002\u901a\u8fc7Grad-CAM\u751f\u6210\u7684\u70ed\u56fe\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u786e\u8ba4\u5176\u9884\u6d4b\u57fa\u4e8e\u75c5\u7076\u533a\u57df\u800c\u975e\u865a\u5047\u80cc\u666f\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u7cbe\u786e\u7684\u75c5\u7076\u5206\u5272\u3001\u4e34\u5e8a\u6570\u636e\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u76ae\u80a4\u764c\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2510.17440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17440", "abs": "https://arxiv.org/abs/2510.17440", "authors": ["Qiyuan Guan", "Xiang Chen", "Guiyue Jin", "Jiyu Jin", "Shumin Fan", "Tianyu Song", "Jinshan Pan"], "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation", "comment": "Accepted by NeurIPS 2025", "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.", "AI": {"tldr": "Proposes HQ-NightRain dataset and CST-Net for nighttime image deraining, using a learnable color space converter and implicit illumination guidance to improve rain removal in complex nighttime scenarios.", "motivation": "The motivation stems from the lack of high-quality datasets and the inherent complexities of nighttime image deraining, particularly the coupling effect between rain and illumination.", "method": "The method involves a learnable color space converter (CSC) for rain removal in the Y channel and implicit illumination guidance to enhance robustness in complex nighttime scenarios.", "result": "Extensive experiments validate the effectiveness of the CST-Net and the high harmony and realism of the HQ-NightRain dataset.", "conclusion": "The paper concludes that the proposed HQ-NightRain dataset and CST-Net method significantly improve nighttime image deraining by addressing the challenges of rain and illumination coupling, demonstrating superior performance in experiments."}}
{"id": "2510.17479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17479", "abs": "https://arxiv.org/abs/2510.17479", "authors": ["Feng Zhou", "Wenkai Guo", "Pu Cao", "Zhicheng Zhang", "Jianqin Yin"], "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS", "comment": "A preprint paper", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb3D\u9ad8\u65af\u6e85\u5c04\u521d\u59cb\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5SfM\u3001\u81ea\u521d\u59cb\u5316\u548c\u70b9\u4e91\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe3DGS\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u89c6\u56fe\uff0c\u5bfc\u81f4\u65b0\u89c6\u56fe\u6e32\u67d3\u51fa\u73b0\u6a21\u7cca\u7b49\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u8bad\u7ec3\u65f6\u7ea6\u675f\uff0c\u4f46\u521d\u59cb\u5316\u624d\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\u3002", "method": "\u8bbe\u8ba1\u4e86\u9891\u7387\u611f\u77e5SfM\u4ee5\u63d0\u5347\u4f4e\u7eb9\u7406\u8986\u76d6\uff0c3DGS\u81ea\u521d\u59cb\u5316\u901a\u8fc7\u5149\u5ea6\u76d1\u7763\u751f\u6210\u989d\u5916\u70b9\uff0c\u4ee5\u53ca\u70b9\u4e91\u6b63\u5219\u5316\u901a\u8fc7\u51e0\u4f55/\u53ef\u89c1\u6027\u5148\u9a8c\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u5747\u5300\u8986\u76d6\u3002", "result": "\u5728LLFF\u548cMip-NeRF360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u66f4\u5f3a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6539\u8fdbSfM\u521d\u59cb\u5316\u30013DGS\u81ea\u521d\u59cb\u5316\u53ca\u70b9\u4e91\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u6e32\u67d3\u6027\u80fd\u3002"}}
{"id": "2510.17484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17484", "abs": "https://arxiv.org/abs/2510.17484", "authors": ["Muhammad Umer Ramzan", "Ali Zia", "Abdelwahed Khamis", "Noman Ali", "Usman Ali", "Wei Xiang"], "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment", "comment": null, "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.", "AI": {"tldr": "AutoSOD\u901a\u8fc7\u6539\u8fdb\u4f2a\u63a9\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u4f2a\u63a9\u7801\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51faPOTNet\uff0c\u91c7\u7528\u71b5\u5f15\u5bfc\u7684\u53cc\u805a\u7c7b\u5934\uff08\u8c31\u805a\u7c7b\u548ck-means\uff09\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3MaskFormer\u98ce\u683c\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoSOD\u5728F-measure\u4e0a\u6bd4\u65e0\u76d1\u7763\u65b9\u6cd5\u63d0\u534726%\uff0c\u6bd4\u5f31\u76d1\u7763\u65b9\u6cd5\u63d0\u534736%\u3002", "conclusion": "AutoSOD\u901a\u8fc7\u7ed3\u5408POTNet\u7684\u53cc\u805a\u7c7b\u5934\u548c\u6700\u4f18\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.17566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17566", "abs": "https://arxiv.org/abs/2510.17566", "authors": ["Nachuan Ma", "Zhengfei Song", "Qiang Hu", "Xiaoyu Tang", "Chengxi Zhang", "Rui Fan", "Lihua Xie"], "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection", "comment": null, "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.", "AI": {"tldr": "WP-CrackNet\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5206\u7c7b\u5668\u3001\u91cd\u5efa\u5668\u548c\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408PAAM\u548cCECCM\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u51cf\u5c11\u5bf9\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u7684\u9ad8\u6548\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u3002", "method": "WP-CrackNet\u96c6\u6210\u4e86\u5206\u7c7b\u5668\u3001\u91cd\u5efa\u5668\u548c\u68c0\u6d4b\u5668\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\u5b9e\u73b0\u88c2\u7f1d\u68c0\u6d4b\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u8def\u5f84\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff08PAAM\uff09\u548c\u4e2d\u5fc3\u589e\u5f3aCAM\u4e00\u81f4\u6027\u6a21\u5757\uff08CECCM\uff09\u4ee5\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWP-CrackNet\u5728\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6548\u679c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "WP-CrackNet\u901a\u8fc7\u5f31\u76d1\u7763\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u88c2\u7f1d\u68c0\u6d4b\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9053\u8def\u68c0\u6d4b\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.17568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17568", "abs": "https://arxiv.org/abs/2510.17568", "authors": ["Kaichen Zhou", "Yuhan Wang", "Grace Chen", "Xinhai Chang", "Gaspard Beaudouin", "Fangneng Zhan", "Paul Pu Liang", "Mengyu Wang"], "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception", "comment": null, "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.", "AI": {"tldr": "PAGE-4D\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u63a9\u7801\u89e3\u51b3\u9759\u6001\u4e0e\u52a8\u6001\u4fe1\u606f\u51b2\u7a81\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8d85\u8d8aVGGT\uff0c\u5b9e\u73b0\u591a\u4efb\u52a13D\u91cd\u5efa\u3002", "motivation": "\u73b0\u67093D\u524d\u9988\u6a21\u578b\uff08\u5982VGGT\uff09\u5728\u9759\u6001\u573a\u666f\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u52a8\u6001\u573a\u666f\uff08\u5982\u79fb\u52a8\u4eba\u4f53\u6216\u53ef\u53d8\u5f62\u7269\u4f53\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u611f\u77e5\u805a\u5408\u5668\uff0c\u901a\u8fc7\u9884\u6d4b\u52a8\u6001\u611f\u77e5\u63a9\u7801\u5206\u79bb\u9759\u6001\u4e0e\u52a8\u6001\u4fe1\u606f\uff0c\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u51e0\u4f55\u91cd\u5efa\u3002", "result": "PAGE-4D\u5728\u52a8\u6001\u573a\u666f\u4e2d consistently \u4f18\u4e8eVGGT\uff0c\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u5355\u76ee\u548c\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u53ca\u5bc6\u96c6\u70b9\u4e91\u91cd\u5efa\u4e2d\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c\u3002", "conclusion": "PAGE-4D\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u805a\u5408\u5668\u89e3\u51b3\u4e86\u9759\u6001\u4e0e\u52a8\u6001\u4fe1\u606f\u5904\u7406\u7684\u51b2\u7a81\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8eVGGT\uff0c\u5b9e\u73b0\u4e86\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u6df1\u5ea6\u9884\u6d4b\u548c\u70b9\u4e91\u91cd\u5efa\u7684\u591a\u4efb\u52a1\u4f18\u5316\u3002"}}
{"id": "2510.17585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17585", "abs": "https://arxiv.org/abs/2510.17585", "authors": ["Chuhong Wang", "Hua Li", "Chongyi Li", "Huazhong Liu", "Xiongxin Tang", "Sam Kwong"], "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset", "comment": null, "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6c34\u4e0b\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6UCIS4K\u548cUCIS-SAM\u7f51\u7edc\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u4f18\u5316\u7279\u5f81\u5b66\u4e60\u3001\u9891\u57df\u6574\u5408\u548c\u591a\u5c3a\u5ea6\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u9762\u4e34\u989c\u8272\u5931\u771f\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9646\u5730\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6c34\u4e0b\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eSegment Anything Model\u7684UCIS-SAM\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1aCBOM\u3001FDTIM\u548cMFFAM\uff0c\u5206\u522b\u4f18\u5316\u901a\u9053\u7279\u5f81\u3001\u6574\u5408\u9891\u57df\u4fe1\u606f\u548c\u805a\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "UCIS-SAM\u5728UCIS4K\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UCIS-SAM\u7f51\u7edc\u5728UCIS4K\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.17603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17603", "abs": "https://arxiv.org/abs/2510.17603", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "comment": "NeurIPS 2025 Poster", "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.", "AI": {"tldr": "ShapeCraft\u662f\u4e00\u79cd\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7GPS\u8868\u793a\u5c06\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u4ea4\u4e92\u76843D\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u975e\u7ed3\u6784\u5316\u548c\u4f4e\u4ea4\u4e92\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\u5e38\u4ea7\u751f\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e14\u4ea4\u4e92\u6027\u5dee\uff0c\u96be\u4ee5\u6ee1\u8db3\u827a\u672f\u5de5\u4f5c\u6d41\u7a0b\u9700\u6c42\u3002ShapeCraft\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u4ea4\u4e92\u5f0f\u751f\u6210\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ShapeCraft\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u7a0b\u5e8f\u5f62\u72b6\uff08GPS\uff09\u8868\u793a\uff0c\u5c06\u590d\u6742\u81ea\u7136\u8bed\u8a00\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u5b50\u4efb\u52a1\u56fe\uff0c\u5229\u7528LLM\u4ee3\u7406\u5c42\u6b21\u5316\u89e3\u6790\u7528\u6237\u8f93\u5165\u5e76\u8fed\u4ee3\u4f18\u5316\u7a0b\u5e8f\u5efa\u6a21\u548c\u7ed8\u753b\u8fc7\u7a0b\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cShapeCraft\u5728\u751f\u6210\u51e0\u4f55\u51c6\u786e\u4e14\u8bed\u4e49\u4e30\u5bcc\u76843D\u8d44\u4ea7\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u52a8\u753b\u548c\u7528\u6237\u5b9a\u5236\u7f16\u8f91\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002", "conclusion": "ShapeCraft\u901a\u8fc7\u5176\u521b\u65b0\u7684GPS\u8868\u793a\u548c\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u51e0\u4f55\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u753b\u548c\u7528\u6237\u5b9a\u5236\u7f16\u8f91\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.17609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17609", "abs": "https://arxiv.org/abs/2510.17609", "authors": ["Siqi Chen", "Shanyue Guan"], "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation", "comment": null, "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.", "AI": {"tldr": "\u65e0\u4eba\u673a\u4e0eBIM\u7ed3\u5408\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u52a8\u53163D\u70b9\u4e91\u5206\u5272\uff0c\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4f9d\u8d56\u8017\u65f6\u4e14\u6613\u51fa\u9519\u7684\u624b\u52a8\u6807\u8bb0\u65b9\u6cd5\u5728\u5206\u52723D\u6a21\u578b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u7ed3\u5408\u65e0\u4eba\u673a\u626b\u63cf\u7684\u771f\u5b9e\u70b9\u4e91\u6570\u636e\u548cBIM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u81ea\u52a8\u5316\u5206\u5272\u3002", "result": "\u5728\u94c1\u8def\u8f68\u9053\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u548c\u5206\u5272\u4e3b\u8981\u7ec4\u4ef6\uff08\u5982\u94c1\u8f68\u548c\u6795\u6728\uff09\uff0c\u5e76\u901a\u8fc7\u8865\u5145BIM\u6570\u636e\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u53163D\u70b9\u4e91\u5206\u5272\u6846\u67b6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5e76\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\u4e0eBIM\u6280\u672f\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u6574\u5408\u3002"}}
{"id": "2510.17611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17611", "abs": "https://arxiv.org/abs/2510.17611", "authors": ["Jia Guo", "Shuai Lu", "Lei Fan", "Zelin Li", "Donglin Di", "Yang Song", "Weihang Zhang", "Wenbing Zhu", "Hong Yan", "Fang Chen", "Huiqi Li", "Hongen Liao"], "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection", "comment": "Extended version of CVPR2025", "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.", "AI": {"tldr": "Dinomaly2\u662f\u9996\u4e2a\u5168\u8c31\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u7ea6\u8bbe\u8ba1\u5728\u591a\u7c7b\u3001\u591a\u6a21\u6001\u53ca\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u7c7b\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u53ca\u9886\u57df\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u4ee5\u6ee1\u8db3\u5168\u8c31\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9700\u6c42\u3002", "method": "Dinomaly2\u901a\u8fc7\u534f\u8c03\u4e94\u4e2a\u7b80\u5355\u5143\u7d20\u5728\u6807\u51c6\u91cd\u6784\u6846\u67b6\u4e2d\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u5373\u53ef\u81ea\u7136\u6269\u5c55\u5230\u591a\u6837\u5316\u4efb\u52a1\u3002", "result": "\u572812\u4e2aUAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u7c7b\u6a21\u578b\u5728MVTec-AD\u548cVisA\u4e0a\u5206\u522b\u8fbe\u523099.9%\u548c99.3%\u7684I-AUROC\uff0c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e5f\u8d85\u8d8a\u4e4b\u524d\u5168\u6837\u672c\u6a21\u578b\u3002", "conclusion": "Dinomaly2\u901a\u8fc7\u7b80\u7ea6\u8bbe\u8ba1\u548c\u8ba1\u7b97\u6269\u5c55\u6027\uff0c\u6210\u4e3a\u5168\u8c31\u5f02\u5e38\u68c0\u6d4b\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.17644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17644", "abs": "https://arxiv.org/abs/2510.17644", "authors": ["Zexian Huang", "Mashnoon Islam", "Brian Armstrong", "Kourosh Khoshelham", "Martin Tomko"], "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives", "comment": null, "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.", "AI": {"tldr": "DINO-CV\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387DEM\u81ea\u52a8\u6620\u5c04\u5e72\u77f3\u5899\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u690d\u88ab\u906e\u6321\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5e72\u77f3\u5899\u5177\u6709\u91cd\u8981\u7684\u9057\u4ea7\u548c\u73af\u5883\u4ef7\u503c\uff0c\u4f46\u7531\u4e8e\u5176\u96be\u4ee5\u63a5\u8fd1\u548c\u624b\u52a8\u6620\u5c04\u7684\u9ad8\u6210\u672c\uff0c\u8bb8\u591a\u5899\u672a\u88ab\u8bc6\u522b\u3002\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u89c6\u89c9\u906e\u6321\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002", "method": "DINO-CV\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u81ea\u76d1\u7763\u8de8\u89c6\u56fe\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u5b83\u5b66\u4e60\u8de8\u591a\u4e2aDEM\u884d\u751f\u7269\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u8868\u793a\uff0c\u652f\u6301\u5305\u62ecResNet\u3001Wide ResNet\u548cVision Transformers\u5728\u5185\u7684\u5404\u79cd\u89c6\u89c9\u9aa8\u5e72\u3002", "result": "DINO-CV\u5728\u6d4b\u8bd5\u533a\u57df\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u8fbe\u523068.6%\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u4ec510%\u6807\u8bb0\u6570\u636e\u5fae\u8c03\u65f6\u4ecd\u4fdd\u630163.8%\u7684mIoU\u3002", "conclusion": "DINO-CV\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u9ad8\u5206\u8fa8\u7387DEM\u884d\u751f\u7269\u4e0a\u5bf9\u690d\u88ab\u8302\u5bc6\u4e14\u6ce8\u91ca\u7a00\u7f3a\u7684\u9057\u4ea7\u73af\u5883\u4e2d\u81ea\u52a8\u5e72\u77f3\u5899\u6620\u5c04\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17664", "abs": "https://arxiv.org/abs/2510.17664", "authors": ["Ling Liu", "Jun Tian", "Li Yi"], "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads", "comment": null, "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.", "AI": {"tldr": "4DSegStreamer\u901a\u8fc7\u53cc\u7ebf\u7a0b\u7cfb\u7edf\u5b9e\u73b0\u5b9e\u65f64D\u5168\u666f\u5206\u5272\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u9488\u5bf9\u9ad8\u52a8\u6001\u73af\u5883\uff08\u5982\u5bc6\u96c6\u4eba\u7fa4\u758f\u6563\u548c\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u5b9e\u65f6\u3001\u7ec6\u7c92\u5ea6\u611f\u77e5\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u7ebf\u7a0b\u7cfb\u7edf\uff1a\u9884\u6d4b\u7ebf\u7a0b\u5229\u7528\u5386\u53f2\u8fd0\u52a8\u4e0e\u51e0\u4f55\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u52a8\u6001\uff0c\u63a8\u7406\u7ebf\u7a0b\u786e\u4fdd\u5bf9\u8f93\u5165\u5e27\u7684\u53ca\u65f6\u9884\u6d4b\u3002", "result": "\u5728HOI4D\u3001SemanticKITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u52a8\u6001\u7269\u4f53\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "4DSegStreamer\u6846\u67b6\u901a\u8fc7\u53cc\u7ebf\u7a0b\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f64D\u5168\u666f\u5206\u5272\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17686", "abs": "https://arxiv.org/abs/2510.17686", "authors": ["Taichi Liu", "Zhenyu Wang", "Ruofeng Liu", "Guang Wang", "Desheng Zhang"], "title": "Towards 3D Objectness Learning in an Open World", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.", "AI": {"tldr": "OP3Det\u662f\u4e00\u79cd\u65e0\u9700\u6587\u672c\u63d0\u793a\u7684\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\uff0c\u7ed3\u54082D\u548c3D\u5148\u9a8c\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u5b9e\u73b0\u5e7f\u4e493D\u7269\u4f53\u53d1\u73b0\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u96c63D\u68c0\u6d4b\u5668\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u76f4\u63a5\u5f15\u51653D\u5f00\u653e\u8bcd\u6c47\u6a21\u578b\u5219\u9762\u4e34\u8bcd\u6c47\u6269\u5c55\u548c\u8bed\u4e49\u91cd\u53e0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faOP3Det\uff0c\u4e00\u79cd\u65e0\u9700\u624b\u5de5\u6587\u672c\u63d0\u793a\u7684\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\uff0c\u7ed3\u54082D\u8bed\u4e49\u5148\u9a8c\u548c3D\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4e13\u5bb6\u6df7\u5408\u52a8\u6001\u8def\u7531\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "OP3Det\u5728AR\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u566816.0%\uff0c\u76f8\u6bd4\u5c01\u95ed\u4e16\u754c3D\u68c0\u6d4b\u5668\u63d0\u534713.5%\u3002", "conclusion": "OP3Det\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\uff0c\u5e76\u5728\u5c01\u95ed\u4e16\u754c3D\u68c0\u6d4b\u5668\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5e7f\u4e493D\u7269\u4f53\u53d1\u73b0\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.17699", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17699", "abs": "https://arxiv.org/abs/2510.17699", "authors": ["Aleksandr Oganov", "Ilya Bykov", "Eva Neudachina", "Mishan Aliev", "Alexander Tolmachev", "Alexander Sidorov", "Aleksandr Zuev", "Andrey Okhotin", "Denis Rakitin", "Aibek Alanov"], "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver", "comment": null, "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.", "AI": {"tldr": "\u63d0\u51fa\u5e7f\u4e49\u5bf9\u6297\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u7b80\u5355\u53c2\u6570\u5316\u4e0e\u5bf9\u6297\u8bad\u7ec3\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u91c7\u6837\u6548\u7387\u4e0e\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f46\u91c7\u6837\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8bad\u7ec3\u6280\u5de7\u4e14\u672a\u660e\u786e\u5173\u6ce8\u7ec6\u8282\u4fdd\u7559\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u6c42\u89e3\u5668\uff08Generalized Solver\uff09\u4f5c\u4e3aODE\u91c7\u6837\u5668\u7684\u7b80\u5355\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u6280\u5de7\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u84b8\u998f\u635f\u5931\u4e0e\u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u5347\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "result": "\u5e7f\u4e49\u5bf9\u6297\u6c42\u89e3\u5668\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u4e0e\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u5e7f\u4e49\u5bf9\u6297\u6c42\u89e3\u5668\uff08Generalized Adversarial Solver\uff09\u5728\u76f8\u4f3c\u8d44\u6e90\u9650\u5236\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6c42\u89e3\u5668\u8bad\u7ec3\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17700", "abs": "https://arxiv.org/abs/2510.17700", "authors": ["Walter Simoncini", "Michael Dorkenwald", "Tijmen Blankevoort", "Cees G. M. Snoek", "Yuki M. Asano"], "title": "Elastic ViTs from Pretrained Models without Retraining", "comment": "Accepted at NeurIPS 2025", "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/", "AI": {"tldr": "SnapViT \u662f\u4e00\u79cd\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\u7684\u540e\u9884\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u548c\u81ea\u76d1\u7763\u8bc4\u5206\u5b9e\u73b0\u9ad8\u6548\u5f39\u6027\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4ec5\u5728\u9884\u5b9a\u4e49\u89c4\u6a21\u4e0b\u53ef\u7528\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5f39\u6027\u90e8\u7f72\u9009\u62e9\u4ee5\u9002\u5e94\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u4fe1\u606f\u4e0e\u8de8\u7f51\u7edc\u7ed3\u6784\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u8fd1\u4f3c Hessian \u975e\u5bf9\u89d2\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u81ea\u76d1\u7763\u91cd\u8981\u6027\u8bc4\u5206\u673a\u5236\u3002", "result": "\u5728 DINO\u3001SigLIPv2\u3001DeIT \u548c AugReg \u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5f39\u6027\u6a21\u578b\u4ec5\u9700\u4e0d\u5230 5 \u5206\u949f\uff08\u5355 A100 GPU\uff09\u3002", "conclusion": "SnapViT \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u9884\u8bad\u7ec3\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u5f39\u6027\u63a8\u7406\uff0c\u4e14\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17716", "abs": "https://arxiv.org/abs/2510.17716", "authors": ["Suqiang Ma", "Subhadeep Sengupta", "Yao Lee", "Beikang Gu", "Xianyan Chen", "Xianqiao Wang", "Yang Liu", "Mengjia Xu", "Galit H. Frydman", "He Li"], "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging", "comment": null, "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528YOLOv11\u548c\u591a\u901a\u9053\u8367\u5149\u5206\u6790CCC\u56fe\u50cf\uff0c\u5206\u7c7b\u548c\u8bc6\u522b\u51c6\u786e\u7387\u8d8595%\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u7ec6\u80de\u7c07\u7814\u7a76\u3002", "motivation": "\u5faa\u73af\u8840\u7ec6\u80de\u7c07\uff08CCCs\uff09\u662f\u8840\u6813\u3001\u611f\u67d3\u548c\u708e\u75c7\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u81ea\u52a8\u5206\u6790CCC\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5904\u7406\u4e0d\u89c4\u5219\u5f62\u72b6\u3001\u5f02\u8d28\u7ec6\u80de\u7c7b\u578b\u548c\u591a\u901a\u9053\u67d3\u8272\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u5206\u6790\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7\u5fae\u8c03YOLOv11\u6a21\u578b\u5c06\u56fe\u50cf\u5206\u7c7b\u4e3a\u7ec6\u80de\u7c07\u548c\u975e\u7ec6\u80de\u7c07\u7ec4\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u548cViT\uff1b\u7136\u540e\u901a\u8fc7\u53e0\u52a0\u7c07\u8f6e\u5ed3\u4e0e\u591a\u901a\u9053\u8367\u5149\u67d3\u8272\u533a\u57df\u8bc6\u522b\u7ec6\u80de\u7c7b\u578b\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u6846\u67b6\u5728\u7ec6\u80de\u7c07\u5206\u7c7b\u548c\u8868\u578b\u8bc6\u522b\u4e2d\u5747\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u6709\u6548\u514b\u670d\u7ec6\u80de\u788e\u7247\u548c\u67d3\u8272\u4f2a\u5f71\u7684\u5e72\u6270\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6846\u67b6\u6709\u6548\u5206\u6790\u4e86\u6d41\u5f0f\u7ec6\u80de\u672f\u4e2d\u7684\u5faa\u73af\u8840\u7ec6\u80de\u7c07\uff08CCCs\uff09\u56fe\u50cf\uff0c\u7ed3\u5408\u660e\u573a\u548c\u8367\u5149\u6570\u636e\uff0c\u5206\u7c7b\u548c\u8868\u578b\u8bc6\u522b\u51c6\u786e\u7387\u8d85\u8fc795%\u3002\u521d\u6b65\u6d4b\u8bd5\u4e8e\u8840\u7ec6\u80de\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u514d\u75ab\u548c\u80bf\u7624\u7ec6\u80de\u7c07\u5206\u6790\uff0c\u652f\u6301\u591a\u79cd\u75be\u75c5\u7684\u7ec6\u80de\u7814\u7a76\u3002"}}
{"id": "2510.17719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17719", "abs": "https://arxiv.org/abs/2510.17719", "authors": ["Zhiqiang Teng", "Beibei Lin", "Tingting Chen", "Zifeng Yuan", "Xuanyi Li", "Xuanyu Zhang", "Shunli Zhang"], "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions", "comment": null, "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.", "AI": {"tldr": "RaindropGS\u662f\u4e00\u4e2a\u9488\u5bf9\u96e8\u6ef4\u6761\u4ef6\u4e0b3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\u5404\u73af\u8282\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u5e76\u6307\u5bfc\u4f18\u5316\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u4f7f\u7528\u5408\u6210\u96e8\u6ef4\u56fe\u50cf\uff08\u5df2\u77e5\u76f8\u673a\u59ff\u6001\uff09\u8bc4\u4f303DGS\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u96e8\u6ef4\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u70b9\u4e91\u521d\u59cb\u5316\u7684\u5e72\u6270\uff0c\u4e14\u5408\u6210\u4e0e\u771f\u5b9e\u96e8\u6ef4\u5b58\u5728\u663e\u8457\u57df\u5dee\u8ddd\u3002", "method": "RaindropGS\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec\u6570\u636e\u51c6\u5907\u3001\u6570\u636e\u5904\u7406\u548c\u96e8\u6ef4\u611f\u77e5\u76843DGS\u8bc4\u4f30\u4e09\u90e8\u5206\uff0c\u6db5\u76d6\u96e8\u6ef4\u5e72\u6270\u7c7b\u578b\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u70b9\u4e91\u521d\u59cb\u5316\u3001\u5355\u56fe\u50cf\u53bb\u96e8\u6bd4\u8f83\u548c3D\u9ad8\u65af\u8bad\u7ec3\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u673a\u7126\u70b9\u4f4d\u7f6e\u5bf93DGS\u91cd\u5efa\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0d\u51c6\u786e\u7684\u59ff\u6001\u548c\u70b9\u4e91\u521d\u59cb\u5316\u4f1a\u4e25\u91cd\u5e72\u6270\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "RaindropGS\u4f5c\u4e3a\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u67093DGS\u65b9\u6cd5\u5728\u96e8\u6ef4\u5e72\u6270\u4e0b\u7684\u6027\u80fd\u5c40\u9650\uff0c\u5e76\u660e\u786e\u4e86\u5404\u5904\u7406\u73af\u8282\u7684\u5f71\u54cd\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u76843DGS\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.17731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17731", "abs": "https://arxiv.org/abs/2510.17731", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?", "comment": "Appeared in the ICML 2025 Workshop on Building Physically Plausible\n  World Models, July 2025, https://physical-world-modeling.github.io/", "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86DiT-based I2V\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u9ad8\u8868\u73b0\u529b\u7684I2V\u6a21\u578b\u662f\u5426\u80fd\u591f\u751f\u6210\u771f\u5b9e\u7684\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u4ece\u884c\u4eba\u8f68\u8ff9\u57fa\u51c6\u4e2d\u63d0\u53d6\u5173\u952e\u5e27\uff0c\u5e76\u5229\u7528\u5b9a\u91cf\u884c\u4eba\u52a8\u6001\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u6a21\u578b\u5728\u62e5\u6324\u516c\u5171\u573a\u666f\u4e2d\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5c55\u73b0\u51fa\u5176\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17739", "abs": "https://arxiv.org/abs/2510.17739", "authors": ["Timur Ismagilov", "Shakaiba Majeed", "Michael Milford", "Tan Viet Tuyen Nguyen", "Sarvapali D. Ramchurn", "Shoaib Ehsan"], "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition", "comment": "13 pages", "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u53c2\u8003\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u63d0\u5347\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u53c2\u8003\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e2d\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u5206\u89e3\u7684\u6295\u5f71\u6b8b\u5dee\u5339\u914d\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u63cf\u8ff0\u7b26\u65e0\u5173\u3002", "result": "\u5728\u591a\u89c6\u89d2\u6570\u636e\u4e0a\uff0cRecall@1\u63d0\u5347\u4e86\u7ea618%\uff0c\u5e76\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u4e5f\u6709\u7ea65%\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u8f7b\u91cf\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ec\u56de\u7387\u3002"}}
{"id": "2510.17777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17777", "abs": "https://arxiv.org/abs/2510.17777", "authors": ["Samir Khaki", "Junxian Guo", "Jiaming Tang", "Shang Yang", "Yukang Chen", "Konstantinos N. Plataniotis", "Yao Lu", "Song Han", "Zhijian Liu"], "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "comment": null, "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.", "AI": {"tldr": "SparseVILA\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u7a00\u758f\u6027\uff0c\u663e\u8457\u52a0\u901f\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u53d7\u5230\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u589e\u52a0\u7684\u5236\u7ea6\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u3002SparseVILA\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u7a00\u758f\u6027\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SparseVILA\u5728\u9884\u586b\u5145\u9636\u6bb5\u526a\u679d\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\uff0c\u5e76\u5728\u89e3\u7801\u9636\u6bb5\u4ec5\u68c0\u7d22\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u4ee4\u724c\uff0c\u4ece\u800c\u5728AWQ\u4f18\u5316\u7684\u63a8\u7406\u7ba1\u9053\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002", "result": "SparseVILA\u5728\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad84.0\u500d\u7684\u9884\u586b\u5145\u52a0\u901f\u30012.5\u500d\u7684\u89e3\u7801\u52a0\u901f\u548c2.6\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u5728\u6587\u6863\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "SparseVILA\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u7a00\u758f\u6027\uff0c\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u52a0\u901f\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u67b6\u6784\u65e0\u5173\u7684\u6846\u67b6\u3002"}}
{"id": "2510.17790", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17790", "abs": "https://arxiv.org/abs/2510.17790", "authors": ["Yuhao Yang", "Zhen Yang", "Zi-Yi Dou", "Anh Nguyen", "Keen You", "Omar Attia", "Andrew Szot", "Michael Feng", "Ram Ramrakhya", "Alexander Toshev", "Chao Huang", "Yinfei Yang", "Zhe Gan"], "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action", "comment": null, "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.", "AI": {"tldr": "UltraCUA\u901a\u8fc7\u6df7\u5408\u52a8\u4f5c\u673a\u5236\u6574\u5408GUI\u539f\u59cb\u64cd\u4f5c\u548c\u9ad8\u7ea7\u7a0b\u5e8f\u5316\u5de5\u5177\u8c03\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u56e0\u4f9d\u8d56\u539f\u59cb\u52a8\u4f5c\uff08\u70b9\u51fb\u3001\u952e\u5165\u3001\u6eda\u52a8\uff09\u5bfc\u81f4\u7684\u89c6\u89c9\u57fa\u7840\u4e0d\u51c6\u786e\u548c\u6267\u884c\u94fe\u8fc7\u957f\u95ee\u9898\uff0c\u4ee5\u53ca\u5176\u4e0e\u4e30\u5bcc\u7a0b\u5e8f\u5316\u63a5\u53e3\uff08\u5982API\u3001\u5de5\u5177\uff09\u7684\u9694\u79bb\u95ee\u9898\u3002", "method": "1) \u4ece\u8f6f\u4ef6\u6587\u6863\u3001\u5f00\u6e90\u4ed3\u5e93\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u6269\u5c55\u7a0b\u5e8f\u5316\u5de5\u5177\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff1b2) \u751f\u6210\u8d85\u8fc717,000\u4e2a\u53ef\u9a8c\u8bc1\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u5f15\u64ce\uff1b3) \u6536\u96c6\u5305\u542b\u4f4e\u7ea7GUI\u52a8\u4f5c\u548c\u9ad8\u7ea7\u7a0b\u5e8f\u5316\u5de5\u5177\u8c03\u7528\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6df7\u5408\u52a8\u4f5c\u8f68\u8ff9\uff1b4) \u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "UltraCUA\u6a21\u578b\u5728OSWorld\u4e0a\u76f8\u5bf9\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u534722%\uff0c\u6b65\u9aa4\u6267\u884c\u901f\u5ea6\u63d0\u534711%\uff1b\u5728WindowsAgentArena\u4e0a\u7684\u8de8\u57df\u8bc4\u4f30\u4e2d\u6210\u529f\u7387\u8fbe\u523021.7%\uff0c\u4f18\u4e8e\u57fa\u4e8eWindows\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "UltraCUA\u6a21\u578b\u901a\u8fc7\u6df7\u5408\u52a8\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u9519\u8bef\u4f20\u64ad\u5e76\u4fdd\u6301\u4e86\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2510.17800", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17800", "abs": "https://arxiv.org/abs/2510.17800", "authors": ["Jiale Cheng", "Yusen Liu", "Xinyu Zhang", "Yulin Fei", "Wenyi Hong", "Ruiliang Lyu", "Weihan Wang", "Zhe Su", "Xiaotao Gu", "Xiao Liu", "Yushi Bai", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "Glyph: Scaling Context Windows via Visual-Text Compression", "comment": null, "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.", "AI": {"tldr": "Glyph\u901a\u8fc7\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6807\u8bb0\u538b\u7f29\u548c\u901f\u5ea6\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u5728\u6587\u6863\u7406\u89e3\u3001\u4ee3\u7801\u5206\u6790\u548c\u591a\u6b65\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u6269\u5c55\u5230\u767e\u4e07\u6807\u8bb0\u7ea7\u522b\u4f1a\u5e26\u6765\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86Glyph\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u9057\u4f20\u641c\u7d22\u4f18\u5316\u89c6\u89c9\u6e32\u67d3\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGlyph\u5b9e\u73b0\u4e863-4\u500d\u7684\u6807\u8bb0\u538b\u7f29\uff0c\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\uff0cSFT\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\uff0c\u4e14\u5728\u6781\u7aef\u538b\u7f29\u4e0b\u53ef\u5904\u74061M\u6807\u8bb0\u7ea7\u6587\u672c\u4efb\u52a1\u3002", "conclusion": "Glyph\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u5b9e\u73b0\u4e863-4\u500d\u7684\u6807\u8bb0\u538b\u7f29\uff0c\u540c\u65f6\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u4e0e\u9886\u5148LLM\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u9884\u586b\u5145\u3001\u89e3\u7801\u548cSFT\u8bad\u7ec3\u7684\u901f\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6781\u7aef\u538b\u7f29\u4e0b\u5904\u7406\u767e\u4e07\u6807\u8bb0\u7ea7\u6587\u672c\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17803", "abs": "https://arxiv.org/abs/2510.17803", "authors": ["Zixin Yin", "Ling-Hao Chen", "Lionel Ni", "Xili Dai"], "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing", "comment": "SIGGRAPH Asia 2025", "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.", "AI": {"tldr": "\u63d0\u51faConsistEdit\u65b9\u6cd5\uff0c\u901a\u8fc7MM-DiT\u7684\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u591a\u8f6e\u548c\u89c6\u9891\u7f16\u8f91\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7f16\u8f91\u5f3a\u5ea6\u4e0e\u6e90\u4e00\u81f4\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5c24\u5176\u5728\u591a\u8f6e\u548c\u89c6\u9891\u7f16\u8f91\u4e2d\u89c6\u89c9\u9519\u8bef\u4f1a\u7d2f\u79ef\uff0c\u4e14\u5168\u5c40\u4e00\u81f4\u6027\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790MM-DiT\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51faConsistEdit\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u4e13\u7528\u6ce8\u610f\u529b\u63a7\u5236\u3001\u63a9\u7801\u5f15\u5bfc\u9884\u6ce8\u610f\u529b\u878d\u5408\u53ca\u5dee\u5f02\u5316\u64cd\u4f5c\u67e5\u8be2\u3001\u952e\u548c\u503c\u4ee4\u724c\u3002", "result": "ConsistEdit\u5728\u5e7f\u6cdb\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9996\u6b21\u652f\u6301\u6240\u6709\u63a8\u7406\u6b65\u9aa4\u548c\u6ce8\u610f\u529b\u5c42\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "ConsistEdit\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u591a\u8f6e\u548c\u591a\u533a\u57df\u7f16\u8f91\uff0c\u4e14\u80fd\u6e10\u8fdb\u8c03\u6574\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002"}}
