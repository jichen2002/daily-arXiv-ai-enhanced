<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 无需训练，利用文本到3D对象扩散模型生成模块化瓦片，通过多瓦片去噪合成大规模、连贯的3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于单对象生成、需要领域特定训练或缺乏对全360度可视性的支持，限制了应用范围。

Method: 将场景生成重新定义为多瓦片去噪问题，通过独立生成重叠的3D区域并通过加权平均无缝混合。

Result: 该方法支持多样化的场景布局、高效生成和灵活编辑，为通用、语言驱动的3D场景构建提供了简单而强大的基础。

Conclusion: 本文提出了一种无需训练的3D场景合成方法，通过重新利用通用的文本到3D对象扩散模型作为模块化瓦片生成器，实现了大规模、连贯场景的可扩展合成。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [2] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 结合轻量级卷积分类器和视觉语言模型，提出了一种可解释的图像真实性检测系统，适用于低分辨率图像，并在多个领域具有潜在应用。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的日益真实化对验证视觉真实性提出了挑战。

Method: 提出了一种结合轻量级卷积分类器（“Faster-Than-Lies”）和视觉语言模型（Qwen2-VL-7B）的系统，用于分类、定位和解释32x32图像中的伪影。通过自编码器重建误差图生成伪影定位热图，增强了对人类和VLM的可解释性。

Result: 该模型在扩展的CiFAKE数据集上实现了96.5%的准确率，并在8核CPU上保持175ms的推理时间，适合本地或边缘设备部署。此外，将70种视觉伪影类型分类为八个语义组，并为每种检测到的异常生成可解释的文本。

Conclusion: 该研究展示了在低分辨率图像中结合视觉和语言推理进行可解释的真实性检测的可行性，并概述了在法医学、工业检测和社交媒体审核等领域的潜在跨领域应用。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [3] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: 论文提出DisK-NeuralRTI，通过知识蒸馏降低NeuralRTI的计算成本，解决了高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PTM和HSH）在复杂反射场捕捉上存在不足，而NeuralRTI虽质量优越但计算成本高，难以在有限硬件上实现。

Method: 采用知识蒸馏技术，通过训练一个更小的网络来模仿原始大型网络的行为，从而减少计算开销。

Result: 提出的DisK-NeuralRTI方法在降低计算成本的同时，保持了与原始NeuralRTI相近的渲染质量。

Conclusion: 论文提出了一种基于知识蒸馏的新方法（DisK-NeuralRTI），有效降低了NeuralRTI的计算成本，同时保持了较高的渲染质量。

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [4] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: CountFormer通过DINOv2和位置嵌入融合实现类无关计数，接近人类结构感知能力。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过感知视觉重复和结构关系轻松计数多样对象，而现有模型在复杂形状、内部对称或重叠组件时容易出错。

Method: 基于CounTR架构，用自监督基础模型DINOv2替换视觉编码器，结合位置嵌入融合和轻量级卷积解码器生成密度图。

Result: 在FSC-147数据集上表现与当前最先进方法相当，且在结构复杂或密集场景中表现出更高的准确性。

Conclusion: 整合基础模型（如DINOv2）使计数系统能够接近人类的结构感知能力，迈向真正通用且无需示例的计数范式。

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [5] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 本研究利用深度学习和几何模型开发了一种低成本、自动化的漂浮垃圾监测系统，重点解决了数据泄漏和模型性能问题，为城市水环境监测提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 河流中漂浮的人为垃圾扩散已成为紧迫的环境问题，对生物多样性、水质及人类活动（如航行和娱乐）产生不利影响，因此需要开发有效的监测方法。

Method: 研究提出了一种利用固定原位摄像头监测漂浮垃圾的新方法框架，包括使用深度学习进行连续量化与监测，以及在复杂环境条件下评估模型的准确性和推理速度。此外，还实施了几何模型以从2D图像估计物体的实际大小。

Result: 研究发现数据集构建协议的重要性，特别是负样本的整合和时间泄漏的考虑。研究还验证了在复杂环境条件下深度学习模型的性能。

Conclusion: 本研究通过结合投影几何与回归修正，展示了度量物体估计的可行性，为开发低成本、自动化城市水环境监测系统奠定了基础。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [6] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: RareFlow是一种物理感知的超分辨率框架，通过双条件架构和多方面损失函数确保OOD条件下的鲁棒性，显著提升遥感影像的超分辨率质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决遥感影像超分辨率在分布外条件下的失败问题，即产生视觉上合理但物理上不准确的结果。

Method: RareFlow采用双条件架构，包括Gated ControlNet和文本提示，以及多方面的损失函数来确保输出物理合理性。

Result: 在盲评中，RareFlow的输出接近地面实况图像的真实度，显著优于现有基线，定量指标（如FID）也有近40%的提升。

Conclusion: RareFlow提供了一个在数据稀缺科学领域中进行高保真合成的鲁棒框架，并为在严重领域偏移下的可控生成提供了新范式。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [7] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: LHT-CLIP是一种无需训练的框架，通过优化CLIP的视觉区分能力，显著提升了语义分割性能，并在多个基准测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于CLIP模型的图像级预训练目标与密集预测所需的像素级视觉理解之间的不对齐，现有方法在语义分割中表现不佳。

Method: 提出了三种互补技术：语义空间重加权、选择性头部增强和异常令牌替换，以恢复视觉区分能力并提升分割性能。

Result: 在8个常见的语义分割基准测试中，LHT-CLIP表现优异，验证了其有效性和实用性。

Conclusion: LHT-CLIP通过系统地利用CLIP在不同层次（层、头、令牌）上的视觉区分能力，提出了一种无需训练的框架，显著提高了语义分割性能，并在多个基准测试中达到了最先进的水平。

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [8] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride是一种无需手动场景分割的管道，通过自适应帧采样和多模态窗口化技术生成连贯的场景级字幕，在指标和语义相似度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 教学视频中的场景级字幕需要结合视觉线索和时间结构，以支持程序性学习和多模态推理。若字幕未能捕捉这种结构，可能导致缺乏连贯性和质量，影响教学效果。DynaStride旨在填补这一空白。

Method: DynaStride采用自适应帧采样和多模态窗口化技术捕捉场景内的关键过渡，通过多模态思维链过程生成多个动作-对象对，并利用动态步长窗口选择算法优化和融合这些对。

Result: 与VLLaMA3和GPT-4o等基线相比，DynaStride在N-gram指标（BLEU、METEOR）和语义相似度（BERTScore、CLIPScore）上均表现更优，生成的字幕在时间连贯性和信息量上更佳。

Conclusion: DynaStride通过自适应帧采样和多模态窗口化技术，结合动态步长窗口选择算法，生成了更具时间连贯性和信息性的场景级字幕，为AI驱动的教学内容生成提供了有前景的方向。

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [9] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: TurboPortrait3D 结合图像扩散模型与图像到3D方法，提升肖像新视角合成的质量和效率，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的肖像生成方法存在视觉伪影、细节不足和身份保留不完整的问题，而图像扩散模型虽然能生成高质量图像，但计算成本高且缺乏3D基础。TurboPortrait3D旨在融合两者的优势。

Method: 该方法采用前馈式的图像到3D生成流程，首先获取初始3D表示和噪声渲染图，然后通过单步扩散模型进行多视角一致的细化。训练策略包括在大规模合成多视角数据上进行预训练，随后在高质量真实图像上微调。

Result: 实验证明，TurboPortrait3D在质量和效率上均优于当前最先进的肖像新视角合成方法。

Conclusion: TurboPortrait3D 通过结合图像扩散模型和现有的图像到3D方法，显著提升了肖像新视角合成的质量，同时保持了3D感知能力和低延迟运行。

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [10] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: PlanarGS通过平面和几何先验优化3D高斯溅射，显著提升室内场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射(3DGS)在低纹理室内场景中因光度损失导致的几何模糊和重建质量低的问题。

Method: 设计了Language-Prompted Planar Priors (LP3)流程，结合预训练的视觉-语言分割模型，并通过跨视图融合和几何先验优化区域提案。3D高斯通过平面先验和几何先验监督项进行优化。

Result: 在标准室内基准测试中，PlanarGS重建了准确且详细的3D表面，大幅优于现有方法。

Conclusion: PlanarGS通过引入平面先验和几何先验监督项，显著提升了在低纹理室内场景中的3D重建质量，优于现有技术。

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [11] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: AIRe是一种自适应训练方案，通过神经元剪枝和输入频率密集化优化INR架构，减少模型大小并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入频率选择和架构管理上依赖启发式和繁重的超参数优化，AIRe旨在通过自适应训练解决这些问题。

Method: AIRe采用神经元剪枝机制减少冗余，并通过输入频率密集化提升表示能力。剪枝阶段识别贡献较小的神经元，应用定向权重衰减转移信息后剪枝；密集化阶段在信号欠拟合的频谱区域增加输入频率。

Result: 实验表明，AIRe在图像和SDFs上减少了模型大小，同时保持甚至提升了重建质量。

Conclusion: AIRe通过自适应训练方案，包括神经元剪枝和输入频率密集化，在优化过程中改进了INR架构，实现了模型大小与重建质量之间更好的权衡。

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [12] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: Neural USD 是一种层次化场景表示框架，实现对生成图像中对象的精确编辑，避免全局变化。


<details>
  <summary>Details</summary>
Motivation: 解决可控生成模型中精确和迭代对象编辑的挑战，避免因改变条件信号导致的全局变化问题。

Method: 引入 Neural Universal Scene Descriptor (Neural USD) 框架，采用层次化表示场景和对象，并应用微调方法确保控制信号解耦。

Result: Neural USD 支持对对象外观、几何和姿态的独立控制，并展示了迭代和增量工作流程的可行性。

Conclusion: Neural USD 框架通过结构化和层次化的场景表示，实现了对生成图像中对象的精确和迭代编辑，解决了现有方法中全局变化的问题。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [13] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision 是一种新型图像防护栏，通过类人推理增强适应性和透明度，无需重新训练，在性能上显著优于 GPT-4o，并引入高质量数据集 VisionHarm。


<details>
  <summary>Details</summary>
Motivation: 传统图像防护栏模型受限于预定义类别，缺乏语义推理，难以适应新兴威胁，且需要昂贵的重新训练。

Method: 结合有效的数据收集与生成框架、遵循策略的训练流程以及定制的损失函数，并提出多样化的QA生成与训练策略以增强学习效果。

Result: SafeVision 在不同基准测试中表现出色，在 VisionHarm-T 和 VisionHarm-C 上分别比 GPT-4o 高出 8.6% 和 15.5%，且速度快 16 倍以上。

Conclusion: SafeVision 作为一种新颖的图像防护栏，通过集成类人推理，实现了对新兴威胁的动态适应和透明性，无需重新训练。

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [14] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 论文提出了一种结合链式思维推理的框架，通过两阶段训练方法提升胸部X光分析的透明度和准确性，支持临床审计和人类-AI协作。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型（VLMs）在医学图像分析中缺乏透明推理的问题，旨在提供可解释的中间步骤以支持临床审计。

Method: 结合高保真视觉编码与两阶段训练方法（包括推理风格的监督微调和基于可验证奖励的强化学习），模型能够模拟放射科医师的系统思维过程。

Result: 在分布外评估中，模型在保持多标签分类竞争力的同时提高了可解释性；专家读者研究表明，完整推理轨迹增强了信心并减少了报告时间。

Conclusion: 该论文提出的框架通过结合链式思维（CoT）推理，显著提升了胸部X光片分析的透明度和临床可审计性，同时保持了高分类准确性。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [15] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出联合预测多个傅里叶分量的方法，解决了现有逐个预测导致的性能下降和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法逐个预测傅里叶分量导致性能下降和效率低下，因此需要改进。

Method: 提出了一种联合预测多个傅里叶分量的方法，取代了传统的逐个预测方式。

Result: 新方法在质量和效率上均有提升。

Conclusion: 联合预测多个傅里叶分量可以显著提升超分辨率的效率和质量。

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [16] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo是一个长期、流式、全模态的基准测试，用于评估现实场景中的AI助手，涵盖记忆、理解和跨记忆推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常孤立评估多模态输入处理能力，缺乏现实的流式场景或长期任务支持。

Method: 引入TeleEgo，一个长期、流式、全模态的基准测试，包含14小时/参与者的多模态数据，涵盖12个子任务和3,291个人工验证的QA项目。

Result: TeleEgo定义了12个子任务和两个关键指标（实时准确性和记忆持久时间），以全面评估AI助手。

Conclusion: TeleEgo提供了一个现实且全面的评估框架，推动了实用AI助手的发展。

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [17] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: AdvBlur通过对抗性模糊图像和双损失函数，显著提升糖尿病视网膜病变分类的域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在糖尿病视网膜病变检测中因设备、人群和成像条件差异导致的分布变化而表现不佳，亟需提升域泛化能力。

Method: 提出AdvBlur方法，通过生成对抗性模糊图像并结合双损失函数框架，增强模型对分布变化的鲁棒性。

Result: AdvBlur在多个外部数据集上表现优异，优于现有域泛化方法，并通过消融实验验证了其设计有效性。

Conclusion: AdvBlur方法通过引入对抗性模糊图像和双损失函数框架，有效提升了糖尿病视网膜病变分类的域泛化能力，在多个数据集上验证了其优越性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [18] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: SEG.A挑战赛引入公开数据集，3D U-Net和模型融合表现最佳，为AVT分割设新基准。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量共享数据阻碍了AVT自动分析的发展，因此需要公开数据集和基准测试以促进技术进步。

Method: 使用3D U-Net架构的深度学习方法和模型融合策略，结合定制后处理步骤。

Result: 模型融合显著优于单一模型，性能与算法设计和训练数据特征密切相关。

Conclusion: 该研究通过SEG.A挑战赛引入了一个大型公开数据集，推动了主动脉血管树（AVT）分割领域的进展，并确立了新的性能基准。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [19] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench是首个用于系统评估火星相关任务的基准，包含20个数据集，结果显示火星特定模型可能优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 火星科学缺乏标准化基准和评估框架，限制了火星任务基础模型的发展。

Method: 引入Mars-Bench，包含20个数据集，涵盖分类、分割和目标检测任务，专注于关键地质特征，并提供标准化、即用的数据集和基线评估。

Result: 结果表明，火星特定的基础模型可能优于通用领域模型。

Conclusion: Mars-Bench旨在为火星科学中的机器学习模型开发和比较建立一个标准化基础，促进领域特定预训练的进一步探索。

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [20] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: APT利用LLM生成可读对抗提示，高效绕过T2I模型安全过滤，展现强迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的安全机制易受对抗性提示攻击，且传统红队方法效率低且易被过滤。

Method: 提出APT框架，结合交替优化-微调流程和双重规避策略，优化对抗性后缀生成并绕过过滤机制。

Result: 实验证明APT生成的人类可读对抗性提示在红队测试中表现优异，并能有效绕过商业API的安全过滤。

Conclusion: APT框架通过利用大型语言模型生成人类可读的对抗性后缀，有效提升了文本到图像模型的安全性评估能力，并展示了卓越的零样本迁移性能。

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [21] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: ResNet通过跳跃连接解决深度CNN的梯度消失问题，在CIFAR-10上表现优于传统CNN，准确率更高且训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的深度卷积神经网络（CNN）在训练极深网络时会遇到梯度消失问题，限制了网络的性能。

Method: 本文采用ResNet架构，通过跳跃连接绕过中间层，使梯度能够直接流动，从而训练更深层次的网络。在CIFAR-10数据集上实现了ResNet-18的具体实现。

Result: 在CIFAR-10数据集上，ResNet-18达到了89.9%的准确率，比类似深度的传统深度CNN（84.1%）更高，且收敛更快、训练更稳定。

Conclusion: Residual Networks (ResNet) 通过引入跳跃连接有效解决了深度卷积神经网络中的梯度消失问题，显著提升了训练深度网络的性能和稳定性。

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [22] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA提出了一种高效的稀疏调优方法，通过一阶段处理和自适应机制，显著提升性能并减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏调优方法存在两阶段处理的内存消耗大和性能限制问题，SNELLA旨在通过一阶段方法解决这些问题。

Method: SNELLA采用非线性核函数扩展低秩分解，并引入自适应双层次稀疏分配机制，以端到端方式优化权重更新。

Result: 在分类、分割和生成任务中，SNELLA表现出色，内存使用显著降低（31.1%-39.9%），并在FGVC基准测试中Top-1准确率提升1.8%。

Conclusion: SNELLA是一种高效的一阶段参数微调方法，通过选择性更新权重矩阵和自适应双层次稀疏分配机制，显著提高了性能并降低了内存使用。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [23] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: COLA是一种基于最优传输的框架，通过跨模态对齐提升CLIP模型在对抗性扰动下的鲁棒性，显著改善分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了CLIP模型中图像与文本特征之间的不对齐问题，这种不对齐在对抗性扰动下被放大，导致分类性能下降。

Method: COLA采用基于最优传输的框架，通过子空间投影和局部结构一致性恢复图像与文本特征的全局对齐。

Result: 在14个零样本分类基准测试中，COLA平均提升了6.7%的对抗性攻击下的分类准确率，尤其是在ImageNet及其变体上表现突出。

Conclusion: COLA框架通过跨模态对齐显著提升了CLIP模型在对抗性扰动下的鲁棒性，同时保持了干净样本上的高准确性。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [24] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: BOB是一种针对T2I模型的微调策略，通过条件化类别无关属性减少过拟合，提升合成数据质量，显著改善低样本细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决T2I模型在生成合成训练数据时可能导致的过拟合和多样性降低问题，尤其是在细粒度分类任务中。

Method: 提出了一种名为BOB的微调策略，通过提取类别无关属性（如场景背景和物体姿态）并在微调和生成过程中显式条件化这些属性，以减少过拟合并保持生成多样性。

Result: 在多个T2I模型、骨干网络和数据集上的实验表明，BOB方法在低样本条件下显著提升了分类性能，且在多个基准测试中优于现有方法。

Conclusion: BOB方法在低样本细粒度分类任务中表现优异，显著优于现有方法，尤其在多数据集和模型上展现了其泛化能力。

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [25] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: OmniText是一种无需训练的通用方法，通过自注意力反转和交叉注意力重分布解决文本修复中的三大限制（无法移除文本、样式不可控、字母重复），在多样化TIM任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本修复方法在文本插入和编辑方面表现优异，但存在无法移除文本、缺乏样式控制以及字母重复生成等局限性，限制了其在更广泛TIM任务中的应用。

Method: 研究了交叉和自注意力机制的两个关键属性，提出自注意力反转实现文本去除，交叉注意力重分布控制文本样式和内容；引入潜在优化框架中的新型损失函数（交叉注意力内容损失和自注意力样式损失）以实现可控修复。

Result: OmniText在多项TIM任务中表现优于其他文本修复方法，与专用方法性能相当；同时发布了OmniText-Bench基准数据集用于多样化TIM任务评估。

Conclusion: OmniText框架通过自注意力反转和交叉注意力重分布解决了文本图像操作（TIM）中的关键限制，成为首个能够执行多样化TIM任务的通用方法，并在多项任务和指标上达到最先进水平。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [26] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: 预训练视觉模型的表示可解释性与分类能力正相关，且可通过最大化可解释性微调提升分类能力。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练表示是否能同时实现高可解释性和高分类能力。

Method: 提出了固有可解释性评分（IIS），通过评估信息损失和可解释语义比例来量化表示的可解释性。

Result: 发现可解释性和分类能力正相关，且通过可解释性最大化微调可提升分类能力。

Conclusion: 预训练视觉模型的表示可解释性和分类能力之间存在正相关关系，且通过最大化可解释性微调可进一步提升分类能力。

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [27] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: UHKD通过频率域特征对齐改进异构模型知识蒸馏，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 异构模型间的语义差异限制了中间特征的利用，现有方法在异构场景下效果不佳，需改进以充分利用中间层语义信息。

Method: 提出UHKD框架，利用傅里叶变换捕捉全局特征，结合FTM和FAM模块实现特征转换与对齐，并通过联合损失函数优化训练。

Result: 在CIFAR-100和ImageNet-1K上分别取得5.59%和0.83%的性能提升。

Conclusion: UHKD框架通过频率域特征对齐和联合优化目标，有效解决了异构模型间的知识蒸馏问题，显著提升了性能。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [28] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo是一个大规模多视角RGB-D视频数据集，用于从图像中恢复狗的运动。它填补了现有数据集的不足，并提出了三阶段优化方法，为相关研究提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有狗运动数据集在多视角、真实3D数据、规模和多样性方面存在不足，DogMo旨在弥补这些局限性。

Method: 采用三阶段实例特定优化流程，通过粗对齐、密集对应监督和时间正则化逐步优化SMAL模型以适应运动序列。

Result: DogMo包含1.2k个运动序列，覆盖10种不同犬种，建立了四种运动恢复基准设置，支持单目和多视角、RGB和RGB-D输入的系统评估。

Conclusion: DogMo数据集和提出的方法为狗的运动恢复研究提供了系统化的基础，并促进了计算机视觉、计算机图形学和动物行为建模领域的交叉研究。

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [29] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: ETC框架通过趋势一致性预测和误差容忍搜索，显著加速扩散模型采样且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成质量上表现出色，但其迭代采样过程成本高昂。现有训练无关的加速方法通过复用模型输出来加速，但忽视了去噪趋势和模型特定误差容忍度，导致轨迹偏差和结果不一致。

Method: ETC框架包括两个核心部分：(1) 趋势一致性预测器，利用扩散轨迹的平滑连续性，通过历史去噪模式预测稳定的未来方向，并在多步近似中渐进分布以实现无偏差加速；(2) 模型特定误差容忍搜索机制，通过识别从语义规划到质量优化的过渡点，推导出校正阈值。

Result: 实验表明，ETC实现了2.65倍于FLUX的加速效果，且一致性退化可忽略（-0.074 SSIM分数）。

Conclusion: ETC框架通过引入趋势一致性预测器和模型特定误差容忍搜索机制，有效解决了扩散模型在加速过程中因输出复用导致的轨迹偏差和结果不一致性问题，实现了显著的加速效果且保持了生成质量。

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [30] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 提出一种无需训练的对象中心框架，结合自我精炼和显式布局，提升文本到图像模型的组合性表现。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型虽然在真实感上表现优异，但在组合性（如对象数量、属性和空间关系）方面仍存在不足，本文旨在解决这一问题。

Method: 利用大型语言模型（LLMs）从输入提示中合成显式布局，并将这些布局注入图像生成过程，通过对象中心的视觉语言模型（VLM）对多个候选图像进行重新排名，迭代选择最符合提示的结果。

Result: 框架在保持美学质量的同时，显著提升了布局的忠实度，与现有模型相比，实现了更优的场景对齐效果。

Conclusion: 该论文提出了一种无需训练的新框架，通过结合对象中心方法和自我精炼，显著提升了文本到图像模型在组合性方面的表现，实现了更强的场景对齐。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [31] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: VC4VG是一个专为文本到视频生成优化的字幕框架，通过分析字幕需求和构建新基准，显著提升了生成视频的质量。


<details>
  <summary>Details</summary>
Motivation: 目前针对T2V训练的视频字幕优化策略研究不足，高质量视频文本对对齐对生成连贯视频至关重要。

Method: 提出了VC4VG框架，包括从T2V角度分析字幕内容、分解视频重建所需要素，并设计了一套字幕优化方法。同时构建了VC4VG-Bench基准用于评估。

Result: 实验表明字幕质量提升与视频生成性能强相关，验证了方法的有效性。

Conclusion: VC4VG框架通过优化视频字幕显著提升了文本到视频生成的性能，并开源了相关工具和代码以促进后续研究。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [32] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 提出基于混合提示和空间基础的系统框架，显著提升VLM在自动驾驶任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在自动驾驶场景理解中的多任务干扰问题，提升其在感知、预测、规划等任务中的表现。

Method: 1. 混合提示路由器分类问题并分发至专家提示；2. 任务特定提示嵌入坐标系、空间推理规则、角色扮演等；3. 视觉组装模块整合多视角图像；4. 按任务配置推理参数。

Result: 在Qwen2.5-VL-72B上实现Phase-1（干净数据）70.87%和Phase-2（污染数据）72.85%的平均准确率。

Conclusion: 通过结构化提示和空间基础设计，显著提升了VLM在安全关键自动驾驶任务中的表现。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [33] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 本文提出UAP-SAM2，一种针对SAM2的跨提示通用对抗攻击方法，通过目标扫描和双语义偏差框架解决了现有攻击的性能差距问题，实验证明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索SAM2的鲁棒性，解决现有攻击方法在SAM和SAM2之间的性能差距，以及由架构差异带来的两个关键挑战。

Method: 提出UAP-SAM2，一种基于双语义偏差的跨提示通用对抗攻击方法，包括目标扫描策略和双语义偏差框架的设计。

Result: 在六个数据集上的实验表明，UAP-SAM2显著优于现有攻击方法。

Conclusion: UAP-SAM2显著优于现有的攻击方法，展示了其在对抗SAM2时的有效性。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [34] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: CLFSeg是一种结合模糊逻辑和卷积层的分割框架，显著提升了性能并确保计算效率，适用于医疗诊断场景。


<details>
  <summary>Details</summary>
Motivation: 传统的基于卷积神经网络（CNN）的模型在泛化性、鲁棒性和处理不确定性方面表现有限，影响了分割性能。为了解决这些问题，本文提出了CLFSeg框架。

Method: 本文提出了CLFSeg，一种基于编码器-解码器的框架，集成了模糊卷积（FC）模块，结合了卷积层和模糊逻辑。该模块通过识别局部和全局特征，同时最小化边界区域的不确定性、噪声和模糊性来增强分割性能。此外，还结合了二元交叉熵（BCE）和dice损失来处理类别不平衡问题。

Result: 在四个公开数据集（CVC-ColonDB、CVC-ClinicDB、EtisLaribPolypDB和ACDC）上的实验表明，CLFSeg超越了现有SOTA性能，并专注于解剖结构的相关感兴趣区域。

Conclusion: CLFSeg框架通过结合模糊逻辑和卷积层，显著提升了分割性能，同时确保了计算效率，为实际医疗诊断场景提供了潜在解决方案。

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [35] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: MC-SJD是一种无需训练、无损的并行解码框架，通过耦合方法显著加速自回归视觉生成，性能提升高达4.2倍（图像）和13.3倍（视频）。


<details>
  <summary>Details</summary>
Motivation: 自回归建模在视觉生成中因逐令牌生成的缓慢推理速度（需数千步生成单个样本）而受限，需解决此瓶颈。

Method: 提出MC-SJD，基于信息论耦合方法，最大化连续迭代中采样相同草稿令牌的概率，提升SJD的接受率。

Result: 在图像和视频生成中分别实现约4.2倍和13.3倍加速，且输出质量无损。

Conclusion: MC-SJD通过简单修改显著加速自回归生成，为实际应用提供了高效解决方案。

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [36] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: ID²Face 是一种训练中心化的人脸匿名化框架，通过结构化潜在空间学习实现身份与非身份属性的显式解耦，无需推理时优化，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖推理时干预（如负指导或基于能量的优化），可能导致分布偏移和身份与非身份属性纠缠，降低视觉保真度和数据效用。

Method: 设计了一个条件扩散模型，采用身份掩码学习方案，结合身份解耦潜在重组器和身份引导潜在协调器，通过软门控融合身份与非身份特征，并引入正交身份映射策略抑制身份泄漏。

Result: ID²Face 在匿名化任务中表现出色，显著提升了视觉质量和数据效用，同时有效抑制了身份信息。

Conclusion: ID²Face 在视觉质量、身份抑制和效用保持方面优于现有方法，通过训练中心化的匿名化框架和结构化潜在空间学习，实现了直接且可控的匿名化。

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [37] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: GenTrack是一种新型多目标跟踪方法，结合随机和确定性跟踪方式，利用PSO和社交交互提升性能，实验证明其优于现有方法，并提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 解决多目标跟踪中目标身份一致性维护、非线性动态管理、遮挡期间的ID切换和轨迹丢失等问题。

Method: 采用混合跟踪方法，结合随机和确定性方式处理目标数量未知和时变的情况，利用粒子群优化（PSO）和提出的适应度度量引导粒子，整合目标间社交交互以增强跟踪效果。

Result: 实验结果表明GenTrack在性能上优于现有跟踪器，尤其是在弱噪声检测器和遮挡情况下表现突出。

Conclusion: GenTrack在标准基准测试和实际场景中表现出色，优于现有最先进的跟踪器，并提供了源代码实现以便公平比较。

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [38] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: SCOPE通过联合建模显著性和覆盖度优化视觉令牌剪枝，提升语义完整性，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌剪枝方法仅依赖注意力分数，导致语义不完整，需联合考虑显著性和覆盖度。

Method: 提出SCOPE评分，结合显著性和覆盖度增益，迭代选择评分最高的令牌。

Result: 在多个视觉语言理解基准测试中，SCOPE方法显著优于现有方法。

Conclusion: SCOPE策略通过联合建模显著性和覆盖度，有效提升了视觉令牌剪枝的语义完整性，显著优于现有方法。

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [39] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 提出结合随机和确定性机制的多目标跟踪方法，通过粒子滤波和确定性关联确保标识一致性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决未知和时变目标数量下的非线性动态跟踪问题，确保在多目标交互和长时间遮挡下的标识一致性。

Method: 该方法联合使用随机粒子滤波和确定性关联机制，粒子滤波处理非线性和非高斯噪声，粒子群优化引导粒子朝向状态分布模式，确定性关联通过成本矩阵确保标识一致性。

Result: 实验证实该方法在预录视频和实时摄像流中均表现优异，优于现有先进跟踪器。

Conclusion: 该论文提出了一种结合随机和确定性机制的多目标视觉跟踪方法，通过粒子滤波和粒子群优化处理非线性和非高斯噪声，并利用确定性关联确保标识一致性，实验结果表明其性能优于现有先进跟踪器。

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [40] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: 该论文提出了一个基于事件的新型微眼动数据集，利用Spiking-VGG模型实现了约90%的分类准确率，为事件视觉研究设立了基准。


<details>
  <summary>Details</summary>
Motivation: 传统微眼动研究方法成本高且扩展性有限，事件传感提供了一种高速、低延迟的替代方案。

Method: 使用Blender渲染高保真眼动场景，模拟不同角位移的微眼动，并通过v2e转换为事件流，评估了多种Spiking-VGG模型。

Result: 模型平均准确率约90%，能独立于事件数量或持续时间对微眼动进行分类。

Conclusion: 该研究展示了脉冲神经网络在精细运动识别中的潜力，并为基于事件的视觉研究提供了数据集和基准。

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [41] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出LROD框架，通过协调恢复和检测网络的Lipschitz连续性，解决了传统级联框架中的功能不匹配问题，实验证明LR-YOLO显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 在恶劣条件下（如雾霾和低光），图像恢复通常作为预处理步骤用于提高检测器的图像质量，但恢复网络和检测网络之间的功能不匹配会导致不稳定性和集成效果不佳。这一问题尚未得到充分研究。

Method: 论文通过Lipschitz连续性的视角分析了恢复网络和检测网络在输入空间和参数空间中的功能差异，并提出了Lipschitz-regularized YOLO (LR-YOLO)框架，将图像恢复任务与检测器的特征学习相协调。

Result: 在雾霾和低光基准测试上的大量实验表明，LR-YOLO在检测稳定性、优化平滑性和整体准确性方面均有显著提升。

Conclusion: 论文提出了一种名为Lipschitz-regularized object detection (LROD)的框架，通过将图像恢复直接集成到检测器的特征学习中，解决了传统级联框架中恢复和检测网络功能不匹配的问题，显著提高了检测稳定性、优化平滑性和整体准确性。

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [42] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: 提出结合CrossGate和ColorShift的DeshadowMamba方法，显著提升阴影去除效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的深度模型在图像阴影去除中因固定注意力模式导致结构扭曲和颜色不一致，需改进。

Method: 提出CrossGate方向调制机制和ColorShift正则化，结合Mamba的选择性状态空间模型，优化阴影去除任务。

Result: 在公开基准测试中，DeshadowMamba表现出卓越的视觉质量和定量性能。

Conclusion: DeshadowMamba结合CrossGate和ColorShift正则化，在公开基准测试中实现了最先进的视觉质量和强大的定量性能。

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [43] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: UtilGen是一种任务效用驱动的数据增强框架，通过双层优化策略生成高效用合成数据，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要关注内在数据属性（如保真度和多样性），而忽略了任务特定需求，导致生成的合成数据在任务性能上可能不足。

Method: UtilGen引入了权重分配网络评估合成样本的任务特定效用，并通过双层优化策略（模型级和实例级优化）迭代优化数据生成过程。

Result: 在八个不同复杂度和粒度的基准数据集上，UtilGen平均准确率提升了3.87%，优于现有最佳方法，并生成更具影响力和任务相关的合成数据。

Conclusion: UtilGen提出了一种以任务效用为中心的数据增强框架，通过下游任务反馈自适应优化数据生成过程，显著提升了任务性能，验证了从视觉特征为中心到任务效用为中心的范式转变的有效性。

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [44] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: 提出一种基于图像重合成的无训练单次来源归属方法，在数据稀缺条件下优于现有技术，并引入新的合成图像归属数据集。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺条件下合成图像来源归属的挑战，尤其是少样本和零样本分类能力的需求。

Method: 通过生成描述图像的提示，利用所有候选来源重合成图像，在特征空间中找到最接近原图的重合成图像进行归属。

Result: 提出的重合成方法在少样本条件下优于现有技术，新数据集为开发少样本和零样本方法提供了有价值的基准。

Conclusion: 该方法在数据稀缺条件下表现优越，新数据集对未来的少样本和零样本方法开发具有挑战性和实用性。

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [45] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: ViPER通过自举框架提升视觉语言模型的细粒度视觉感知能力，在多个任务中表现优异，同时验证了生成与理解的互惠关系。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在细粒度视觉感知上的能力瓶颈，现有方法（如监督微调和强化微调）存在数据不足和偏重文本推理而忽视视觉感知的问题。

Method: 提出了一种两阶段任务，将视觉感知学习构建为从粗到细的渐进过程，并开发了ViPER自举框架，通过自评和自预测实现迭代进化，结合图像级和实例级重建与两阶段强化学习策略。

Result: ViPER应用于Qwen2.5-VL系列后，生成的Qwen-Viper系列在七个综合基准测试中平均提升1.7%，细粒度感知任务上最高提升6.0%，且保持了通用性。

Conclusion: ViPER框架通过自举式闭环训练，显著提升了视觉语言模型在细粒度视觉感知上的性能，同时保持了通用性，为生成与理解之间的互惠关系提供了实证支持。

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [46] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: Prompt learning is effective for few-shot remote sensing scene classification, outperforming baselines, with self-regulating constraints showing the best cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of performance constraints in remote sensing scene classification due to scarcity of labeled data and high annotation costs across diverse domains.

Method: Systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification, evaluating methods like Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints.

Result: Prompt learning consistently outperforms zero-shot CLIP with hand-crafted prompts and linear probe baselines in few-shot scenarios, with Prompting with Self-Regulating Constraints achieving the most robust cross-domain performance.

Conclusion: Prompt learning, particularly with self-regulating constraints, is a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research.

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [47] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出切换式双学生架构和损失感知策略，提升半监督医学图像分割性能，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 教师-学生框架在半监督医学图像分割中表现优异，但受限于网络间强相关性和不可靠的知识传递过程。

Method: 引入切换式双学生架构和损失感知指数移动平均策略，动态选择可靠学生并优化教师网络的伪标签质量。

Result: 在3D医学图像分割数据集上，该方法优于现有半监督方法，有效提高了有限监督下的分割准确性。

Conclusion: 本文提出的切换式双学生架构和损失感知指数移动平均策略显著提升了半监督医学图像分割的性能，优于现有方法。

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [48] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: W2-Net通过双查询机制和子类可分匹配策略，显著提升Referring Expression Counting任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标注时通常关注类代表性位置（如头部），而忽略了其他视觉区域（如腿部）的属性信息，导致模型难以精确区分子类。

Method: 提出W2-Net框架，采用双查询机制（what-to-count和where-to-see查询）和子类可分匹配策略（SSM），以解决现有方法忽略属性信息的问题。

Result: 在REC-8K数据集上，W2-Net将计数误差减少了22.5%（验证集）和18.0%（测试集），定位F1分数提高了7%和8%。

Conclusion: W2-Net通过双查询机制和子类可分匹配策略，显著提升了Referring Expression Counting任务的性能，减少了计数错误并提高了定位精度。

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [49] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mahé,Stéphanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg是一个模块化、轻量级的框架，能将研究级脑卒中分割模型转化为临床可部署工具，性能与原流程相当。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习框架（如nnU-Net）在脑卒中病变分割中表现优异，但由于沉重的依赖和单一的设计，难以在临床环境中部署。

Method: StrokeSeg采用了模块化和轻量级的设计，解耦了预处理、推理和后处理。预处理依赖于Anima工具箱并生成BIDS兼容的输出，推理使用ONNX Runtime和Float16量化，减小了约50%的模型体积。

Result: 在300名亚急性和慢性卒中受试者的测试集上，StrokeSeg的分割性能与原PyTorch流程相当（Dice差异<10^-3）。

Conclusion: StrokeSeg成功地将高性能的研究级脑卒中病变分割模型转化为便携、临床可用的工具，且保持了与原PyTorch流程相当的Dice分数差异（<10^-3）。

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [50] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: MLSN网络通过多尺度亮度感知和特征融合，解决了偏振图像融合中的对比度差异问题，并在多个数据集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 偏振图像融合在复杂光照环境下存在固有对比度差异和高质量数据集稀缺的问题，需要一种能有效整合互补信息的方法。

Method: 提出了亮度感知多尺度网络（MLSN），包括多尺度空间权重矩阵、全局-局部特征融合机制和亮度增强模块，以动态注入亮度信息并平衡全局与局部特征。

Result: MLSN在MSP、PIF和GAND数据集上表现优于现有方法，MS-SSIM和SD指标平均提高8.57%至63.53%。

Conclusion: MLSN网络通过亮度感知和多尺度特征融合，显著提升了偏振图像融合的质量，在复杂光照环境下表现出色，并通过MSP数据集填补了高质量偏振图像数据集的空白。

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [51] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergström,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: 研究探讨放射学报告在医学图像分类器训练中的应用，发现预训练和微调阶段的效果取决于任务与文本的关联性，并提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中忽视标签与文本弱关联任务的问题，探索放射学报告在训练过程中的最佳使用时机和方法。

Method: 通过系统研究放射学报告在预训练和微调阶段的应用，涵盖诊断和预后任务，以及不同训练集大小的影响。

Result: 1. 在标签与文本高度相关的任务中，预训练阶段利用报告有益；但在不相关时，显式图像-文本对齐可能有害。2. 微调阶段使用报告可显著提升性能，在某些情况下甚至比预训练方法影响更大。

Conclusion: 研究提供了关于如何利用放射学报告来训练医学图像分类器的实用建议，同时指出了当前研究的不足。

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [52] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: REFLECT模型通过无监督方法检测卒中后结构异常，使用健康数据训练效果更佳。


<details>
  <summary>Details</summary>
Motivation: 卒中后MRI不仅显示局灶性病变，还揭示继发性结构变化，如萎缩和心室扩大。这些异常作为恢复和预后的影像生物标志物越来越受重视，但监督分割方法难以捕捉。

Method: 评估REFLECT（一种基于流的生成模型）用于无监督检测卒中后患者的局灶性和非局灶性异常。使用ATLAS数据的双专家中心切片注释，通过自由响应ROC分析在对象级别评估异常图的性能。

Result: 在ATLAS测试对象上，使用IXI训练的模型在病变分割（Dice = 0.37 vs 0.27）和对非局灶性异常的敏感性（FROC = 0.62 vs 0.43）方面表现更优。

Conclusion: 使用完全健康的解剖数据进行训练可以更好地建模正常变异，从而更广泛、更可靠地检测结构异常。

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [53] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 无传感器方法通过卫星影像和机器学习高精度估算水库容量，误差低于1.5%，适用于全球水体监测。


<details>
  <summary>Details</summary>
Motivation: 由于传感器故障和维护能力有限，Qaraaoun水库的可持续管理需要一种可靠且不依赖地面测量的监测方法。

Method: 研究整合了Sentinel-2和Landsat卫星影像，提出了一种新的水域分割指数，并基于支持向量回归（SVR）训练机器学习模型，仅依赖卫星影像提取的水域面积估算水库容量。

Result: 提出的水域分割指数与地面真实数据吻合度超过95%，优化后的SVR模型误差低于水库总容量的1.5%，确定系数超过0.98。

Conclusion: 该研究提出了一种无传感器的水库容量监测方法，通过结合开源卫星影像、先进的水域分割技术和机器学习，实现了高精度的实时水库容量估算，为可持续水资源管理提供了实用且经济的解决方案。

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [54] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 该论文提出了一个专门评估语义分割中可解释AI（XAI）的系统框架，通过像素级策略和定制指标提升模型透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: 随着AI在安全关键和高风险领域的应用增加，确保模型的透明度和信任度变得尤为重要。尽管在分类任务的XAI评估方面取得了进展，但针对语义分割的评估策略仍相对不足。

Method: 该研究引入了一个专门设计用于评估语义分割中XAI的综合系统框架，采用像素级评估策略和精心设计的指标。

Result: 模拟结果表明，基于CAM的XAI方案在效率、鲁棒性和可靠性方面表现良好。

Conclusion: 该研究通过提出的系统评估框架，推动了透明、可信且可问责的语义分割模型的发展。

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [55] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: DCIC-sgp通过功能分解和深度条件调制，有效解决了LIC在低比特率下的几何变形问题，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 当前LIC方法难以建模自然图像中复杂的相关性结构，尤其是全局不变结构与局部瞬态纹理的纠缠，导致低比特率下严重的几何变形。

Method: 提出了一种基于功能分解的框架DCIC-sgp，首先生成一个自生成的先验来捕捉图像的结构主干，并以此全局调制整个压缩流程，特别是分析变换，以专注于剩余的高熵细节。

Result: 实验证明，DCIC-sgp显著减少了低比特率下的几何变形伪影，并在Kodak、CLIC和Tecnick数据集上实现了对VTM-12.1的显著BD-rate降低（14.4%、15.7%、15.1%）。

Conclusion: DCIC-sgp框架通过功能分解和深度条件调制，有效解决了传统LIC方法在低比特率下的几何变形问题，显著提升了图像压缩的率失真性能。

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [56] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 视频扩散模型（VDMs）通过时空数据预训练表现出比语言模型更高的数据效率，为视觉基础模型的发展提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 探索视频扩散模型（VDMs）是否能够通过时空数据预训练获得结构和动态的强归纳偏差，从而弥补语言模型在视觉领域的不足。

Method: 通过对比预训练的语言模型（LLM）和视频扩散模型（VDM）在轻量级适配器支持下的任务表现，评估其在ARC-AGI、ConceptARC、视觉游戏、路径规划和元胞自动机等基准上的数据效率。

Result: VDMs在数据效率上优于语言模型，展示了视频预训练在多任务适应中的潜力。

Conclusion: 视频预训练为视觉基础模型提供了有益的归纳偏差，支持其在多任务中的高效适应。

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [57] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: 该论文提出深度学习技术，用于分类三种茶叶病害并定位受损区域，对比两种模型后发现 Faster R-CNN 效果更优。


<details>
  <summary>Details</summary>
Motivation: 开发一种能分类三种茶叶病害（红锈病、茶蝽和红蜘蛛螨）并显示叶片受损区域的深度学习技术。

Method: 评估了 SSD MobileNet V2 和 Faster R-CNN ResNet50 V1 两种模型进行目标检测，并采用自定义方法通过 Mask R-CNN 计算叶片受损区域。

Result: Faster R-CNN ResNet50 V1 的 mAP 为 25%，优于 SSD MobileNet V2 的 20.9%。

Conclusion: Faster R-CNN ResNet50 V1 在检测茶叶病害方面表现优于 SSD MobileNet V2，且通过 Mask R-CNN 实现了病害区域的精确分割。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [58] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavoué*

Main category: cs.CV

TL;DR: Kineo是一种无需校准的多视角运动捕捉流程，通过优化算法显著提升精度和效率，适用于非专家和野外场景。


<details>
  <summary>Details</summary>
Motivation: 现有无需校准的运动捕捉方法计算成本高且重建精度低，限制了非专家和野外捕捉的可访问性。

Method: Kineo利用现成的2D关键点检测器，通过基于图的全局优化和置信度驱动的时空关键点采样策略，同时校准相机并重建3D关键点和密集场景点图。

Result: Kineo在EgoHumans和Human3.6M上评估显示，相机平移误差减少83-85%，角度误差减少86-92%，W-MPJPE减少83-91%。

Conclusion: Kineo是一种全自动、无需校准的标记运动捕捉流程，显著提高了重建精度和计算效率，并在实际场景中表现出色。

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [59] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: Decoupled MeanFlow 通过简单解码策略将流模型转换为流映射模型，无需架构修改，显著加速采样并保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪生成模型（如扩散和基于流的模型）需要大量去噪步骤，而流映射虽能加速采样，但通常需要架构修改，限制了与预训练模型的兼容性。

Method: 通过调节扩散变换器的最终块以条件化后续时间步，将流模型转换为流映射模型，并结合改进的训练技术。

Result: 在 ImageNet 256x256 和 512x512 上，模型在 1 步和 4 步采样时分别达到 FID 2.16/2.12 和 1.51/1.68，显著超越现有技术，并实现 100 倍以上的推理加速。

Conclusion: Decoupled MeanFlow 提供了一种简单的方法，将预训练的流模型直接转换为流映射模型，无需架构修改，显著提高了采样速度，同时在图像生成质量上超越了现有技术。

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [60] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Latent Sketchpad为MLLMs引入视觉草稿本，增强视觉思维，性能优于骨干模型，适用于多种前沿MLLMs。


<details>
  <summary>Details</summary>
Motivation: 受人类通过草图作为视觉思维工具的启发，旨在解决MLLMs在复杂视觉规划和想象力场景中的局限性。

Method: 该框架整合了Context-Aware Vision Head和预训练的Sketch Decoder，使MLLMs能够在自回归推理过程中生成视觉潜在表示，并转换为可解释的草图图像。

Result: 实验证明，Latent Sketchpad在推理性能上与骨干模型相当甚至更优，且能泛化至不同前沿MLLMs（如Gemma3和Qwen2.5-VL）。

Conclusion: Latent Sketchpad框架通过为MLLMs引入内部视觉草稿本，成功扩展了其文本推理能力至视觉思维领域，为更丰富的人机交互和广泛应用开辟了新机会。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [61] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: OSWorld-MCP 是首个全面且公平的基准测试，用于评估计算机使用代理在真实环境中的工具调用、GUI 操作和决策能力，强调了工具调用能力的重要性并设定了新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有的评估主要关注 GUI 交互技能，而忽视了工具调用能力（如通过 MCP 启用的能力），导致对集成工具调用的代理与仅评估 GUI 交互的代理之间的不公平比较。

Method: 设计了一个新颖的自动代码生成流水线来创建工具，并结合现有工具的精选集合，通过严格的手动验证生成了 158 个高质量工具。

Result: 在多模态代理上的广泛评估显示，MCP 工具通常提高了任务成功率（例如，OpenAI o3 在 15 步时从 8.3% 提高到 20.4%，Claude 4 Sonnet 在 50 步时从 40.1% 提高到 43.3%），但最强的模型的工具调用率仍然较低（仅 36.3%）。

Conclusion: OSWorld-MCP 通过明确测量 MCP 工具使用技能，加深了对多模态代理的理解，并为评估复杂工具辅助环境中的性能设定了新标准。

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [62] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: 提出一种结合物理先验和深度学习的CBCT散射伪影校正方法，实验证明其优于当前技术。


<details>
  <summary>Details</summary>
Motivation: CBCT在数据采集过程中易受散射影响，导致CT值偏差和组织对比度降低，从而影响诊断准确性。

Method: 利用高斯径向基函数（RBF）建模点散射函数，并将其嵌入Kolmogorov-Arnold Networks（KAN）层，以学习高维散射特征的非线性映射。

Result: 合成和实际扫描实验验证了该方法的有效性，其在定量指标上优于现有方法。

Conclusion: 该论文提出的基于深度学习的散射伪影校正方法，结合物理先验知识和高维非线性映射能力，有效提升了CBCT图像重建的质量和诊断准确性。

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [63] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: 提出一种双分支CNN用于面部伪造检测，结合空间和频率域特征，通过FSC损失优化性能，在DiFF基准上表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 生成AI的快速发展使得伪造面部图像高度逼真，威胁AI安全、数字媒体完整性和公众信任。迫切需要强大且通用的面部伪造检测方法以应对这一挑战。

Method: 采用双分支卷积神经网络，分别从空间域和频率域提取互补线索，并通过通道注意力模块自适应融合这些异质特征。设计了统一的FSC损失函数，结合焦点损失、监督对比损失和频率中心边际损失。

Result: 在DiFF基准测试中，模型在所有伪造类别中表现优异，超越人类平均准确率。

Conclusion: 本研究提出的双分支卷积神经网络在面部伪造检测中表现出色，显著优于人类平均水平，展示了其在保护AI生态系统免受视觉伪造攻击方面的潜力。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [64] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0数据集通过眼动追踪等技术记录了病理学家诊断WSIs的行为数据，揭示了诊断不一致性，并可能用于改进人类和AI的诊断训练。


<details>
  <summary>Details</summary>
Motivation: 病理学家在解读WSIs时的诊断准确率仅约70%，且增加第二位病理学家并未显著提高一致性，缺乏解释诊断错误和不一致的行为数据。

Method: 通过眼动追踪、鼠标交互、刺激追踪、视口导航和诊断决策数据（EMSVD）收集了19位病理学家解读397张全切片图像（WSIs）的行为数据，强调生态有效性。

Result: 记录了171,909次注视、263,320次扫视和1,867,362次鼠标交互事件，数据集可用于改进病理学家和AI系统的训练。

Conclusion: PathoGaze1.0提供了一个全面的行为数据集，用于研究病理学家在癌症诊断中的视觉搜索和决策过程，旨在填补诊断错误和不一致行为数据的空白。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [65] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: 提出GRAG方法，通过重新加权DiT模型中的token delta值，实现了对图像编辑强度的连续和细粒度控制，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Diffusion-in-Transformer模型的图像编辑方法缺乏对编辑程度的有效控制，限制了定制化结果的实现。

Method: 研究了DiT模型中的MM-Attention机制，发现Query和Key token共享一个仅依赖于层的偏置向量。基于此，提出了GRAG方法，重新加权不同token的delta值以调制模型对输入图像和编辑指令的关注。

Result: GRAG方法能以最少四行代码集成到现有编辑框架中，显著提升编辑质量，并在控制编辑程度上表现优于Classifier-Free Guidance。

Conclusion: 本文提出了一种名为Group Relative Attention Guidance（GRAG）的方法，通过在DiT模型中重新加权不同token的delta值，实现了对编辑强度的连续和细粒度控制，无需额外调参。实验证明，GRAG能显著提升编辑质量，并在控制编辑程度上比常用的Classifier-Free Guidance更平滑和精确。

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [66] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: SAGE是一种零样本视频过渡方法，通过结构引导和生成合成技术，实现高质量、语义一致的过渡效果。


<details>
  <summary>Details</summary>
Motivation: 传统视频过渡方法在处理大时间跨度或语义差异大的剪辑时效果不佳，需要一种内容感知且视觉连贯的过渡方法。

Method: 提出SAGE方法，利用线条图和运动流提供结构引导，结合生成合成技术，实现零样本视频过渡。

Result: 实验表明，SAGE在定量指标和用户研究中均优于现有方法（如FILM、TVG、DiffMorpher、VACE、GI）。

Conclusion: SAGE（Structure-Aware Generative vidEo transitions）通过结合结构引导和生成合成，无需微调即可实现平滑、语义一致的视频过渡，优于现有方法。

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [67] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: MIC-BEV是一个基于Transformer的BEV感知框架，用于基础设施多相机3D物体检测，支持异构相机配置并在极端条件下保持鲁棒性，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的基于相机的检测模型在基础设施场景中表现不佳，原因包括多视角设置、多样化相机配置、视觉输入退化及不同道路布局等挑战。

Method: MIC-BEV是一个基于Transformer的鸟瞰图（BEV）感知框架，支持可变数量的相机和异构内外参数，并提出了图增强融合模块，通过利用相机与BEV单元之间的几何关系及潜在视觉线索，将多视角图像特征整合到BEV空间。

Result: 在合成数据集M2I和真实数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测中达到了最先进的性能。

Conclusion: MIC-BEV在3D物体检测中实现了最先进的性能，并在极端天气和传感器退化等挑战性条件下表现出强鲁棒性，展示了其实际部署的潜力。

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [68] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: ViTs naturally develop object-binding through self-supervised pretraining, with over 90% accuracy in decoding 'IsSameObject'. This ability is weaker in supervised models and actively guides attention.


<details>
  <summary>Details</summary>
Motivation: To investigate whether object-binding naturally emerges in pre-trained Vision Transformers (ViTs) and its role in downstream tasks.

Method: Decoding 'IsSameObject' from patch embeddings using a similarity probe across ViT layers.

Result: Self-supervised ViTs (DINO, MAE, CLIP) reliably exhibit object-binding with over 90% accuracy, while ImageNet-supervised models show weaker performance. 'IsSameObject' is encoded in a low-dimensional subspace and guides attention.

Conclusion: ViTs naturally develop object-binding capabilities through self-supervised pretraining, challenging the notion that they lack this ability. This emergent property is not an architectural artifact but serves the pretraining objective.

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [69] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: ProMoE通过显式路由指导和路由对比损失改进视觉MoE，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉令牌的空间冗余和功能异质性阻碍了专家专业化，需要通过显式路由指导来弥补。

Method: 提出了ProMoE框架，包含两步路由器和显式路由指导，通过条件路由和原型路由实现专家专业化。

Result: 在ImageNet基准测试中，ProMoE在Rectified Flow和DDPM训练目标下均优于现有方法。

Conclusion: ProMoE框架通过显式路由指导和路由对比损失，显著提升了视觉MoE的性能，超越了现有方法。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [70] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: URSA 是一种创新的离散视频生成框架，通过优化设计和异步微调策略，显著提升了生成效率和效果，性能媲美连续方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决离散生成模型在视频生成中因误差累积和长上下文不一致而表现不佳的问题，URSA 旨在通过创新设计弥补这一差距。

Method: URSA 将视频生成任务视为离散时空标记的迭代全局优化，结合线性化度量路径和分辨率相关时间步偏移机制，显著提升了生成效率和效果。

Result: 实验表明，URSA 不仅超越了现有离散方法，还能与最先进的连续扩散方法相媲美，尤其在高效推理和多任务统一方面表现突出。

Conclusion: URSA 通过其独特的设计（线性化度量路径和分辨率相关时间步偏移机制）以及异步时间微调策略，成功缩小了离散方法与连续方法在视频生成领域的性能差距，并在多个基准测试中表现优异。

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [71] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: GVS通过并行采样和Omni Guidance技术，解决了自回归视频扩散模型的未来条件缺失问题，实现了稳定、无碰撞的相机引导视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型无法利用未来条件引导当前生成的问题，避免场景碰撞和生成崩溃。

Method: 提出了Generative View Stitching (GVS)采样算法，结合Omni Guidance技术，利用现有视频模型进行并行采样，增强时间一致性。

Result: GVS在多种预定义相机路径下实现了稳定、无碰撞且帧间一致的视频生成，支持长程一致性。

Conclusion: GVS通过并行采样和Omni Guidance技术，实现了稳定、无碰撞且帧间一致的相机引导视频生成，能够处理包括‘不可能阶梯’在内的多种预定义相机路径。

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [72] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: MSRANetV2是一种优化的CNN架构，用于结直肠癌组织分类，结合残差注意力和SE块，在公开数据集上表现优异，且具有高可解释性。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌（CRC）是全球癌症相关死亡的主要原因，传统诊断方法存在主观性、耗时且易变，深度学习算法可提升诊断精确性和效率。

Method: 提出了一种名为MSRANetV2的卷积神经网络架构，采用ResNet50V2主干，结合残差注意力机制和SE块，通过通道对齐和上采样操作融合多尺度表示。

Result: 在CRC-VAL-HE-7K和NCT-CRC-HE-100K数据集上，MSRANetV2表现出色，平均Precision、recall、F1-score、AUC和测试准确率均接近0.99，Grad-CAM可视化增强了模型的可解释性。

Conclusion: MSRANetV2被验证为一种可靠、可解释且高性能的结直肠癌组织分类架构模型。

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [73] [AI-Driven Development of a Publishing Imprint: Xynapse Traces](https://arxiv.org/abs/2510.23627)
*Fred Zimmerman*

Main category: cs.SE

TL;DR: Xynapse Traces 通过人机协作和自动化技术，显著提升了出版效率和质量，为行业带来革新。


<details>
  <summary>Details</summary>
Motivation: 探索人机协作在出版领域的应用，以缩短上市时间、降低成本并保持高质量，同时开拓传统出版无法触及的利基市场。

Method: 采用配置驱动的架构和多模型AI集成框架，包括持续创意流水线、锦标赛式评估、新颖的转录冥想实践编码设计，以及从创意到生产和分发的全面自动化。

Result: 系统实现了90%的上市时间缩短（从6-12个月降至2-4周），80%的成本削减，第一年出版了52本书，并保持99%的引用准确率和100%的验证成功率。

Conclusion: Xynapse Traces 展示了人机协作在出版领域的巨大潜力，通过技术创新实现了高效、低成本且高质量的出版流程，为未来出版业提供了新的范式。

Abstract: Xynapse Traces is an experimental publishing imprint created via a fusion of
human and algorithmic methods using a configuration-driven architecture and a
multi-model AI integration framework. The system achieved a remarkable 90%
reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),
with 80% cost reduction compared to traditional imprint development, while
publishing 52 books in its first year and maintaining exceptional quality
metrics, including 99% citation accuracy and 100% validation success after
initial corrections. Key technical innovations include a continuous ideation
pipeline with tournament-style evaluation, a novel codex design for
transcriptive meditation practice, comprehensive automation spanning from
ideation through production and distribution, and publisher personas that
define and guide the imprint's mission. The system also integrates automated
verification with human oversight, ensuring that gains in speed do not
compromise publishing standards. This effort has significant implications for
the future of book publishing, suggesting new paradigms for human-AI
collaboration that democratize access to sophisticated publishing capabilities
and make previously unviable niche markets accessible.

</details>


### [74] [VisCoder2: Building Multi-Language Visualization Coding Agents](https://arxiv.org/abs/2510.23642)
*Yuansheng Ni,Songcheng Cai,Xiangchao Chen,Jiarong Liang,Zhiheng Lyu,Jiaqi Deng,Kai Zou,Ping Nie,Fei Yuan,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 该论文通过大规模数据集、系统性基准和多语言模型，提升了可视化编码代理的性能，显著超越开源基线并接近专有模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有模型在实际工作流程中表现不佳，主要受限于语言覆盖不足、执行不可靠以及缺乏迭代校正机制。进展受限于强调单轮生成和单语言任务的狭窄数据集和基准。

Method: 引入了三个互补资源：VisCode-Multi-679K数据集、VisPlotBench基准和VisCoder2模型家族。VisCode-Multi-679K包含679K个验证可执行的可视化样本和跨12种编程语言的多轮校正对话。VisPlotBench提供可执行任务、渲染输出以及初始生成和多轮自调试协议。

Result: VisCoder2在多轮自调试后性能进一步提升，尤其在依赖符号或编译器的语言中表现优异。

Conclusion: VisCoder2模型在VisCode-Multi-679K数据集上训练后，显著超越了开源基线模型，接近GPT-4.1等专有模型的性能，并在32B规模下达到了82.4%的整体执行通过率，尤其在依赖符号或编译器的语言中表现突出。

Abstract: Large language models (LLMs) have recently enabled coding agents capable of
generating, executing, and revising visualization code. However, existing
models often fail in practical workflows due to limited language coverage,
unreliable execution, and lack of iterative correction mechanisms. Progress has
been constrained by narrow datasets and benchmarks that emphasize single-round
generation and single-language tasks. To address these challenges, we introduce
three complementary resources for advancing visualization coding agents.
VisCode-Multi-679K is a large-scale, supervised dataset containing 679K
validated and executable visualization samples with multi-turn correction
dialogues across 12 programming languages. VisPlotBench is a benchmark for
systematic evaluation, featuring executable tasks, rendered outputs, and
protocols for both initial generation and multi-round self-debug. Finally, we
present VisCoder2, a family of multi-language visualization models trained on
VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms
strong open-source baselines and approaches the performance of proprietary
models like GPT-4.1, with further gains from iterative self-debug, reaching
82.4% overall execution pass rate at the 32B scale, particularly in symbolic or
compiler-dependent languages.

</details>


### [75] [Agentsway -- Software Development Methodology for AI Agents-based Teams](https://arxiv.org/abs/2510.23664)
*Eranga Bandara,Ross Gore,Xueping Liang,Sachini Rajapakse,Isurunima Kularathne,Pramoda Karunarathna,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Amin Hass,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.SE

TL;DR: Agentsway是一种新型软件开发框架，专为AI代理协作设计，通过角色定义和微调LLMs提升开发效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发方法（如Agile、Kanban）主要为人类团队设计，在AI代理参与的环境下逐渐不适应，需要新的方法论。

Method: Agentsway通过定义规划、提示、编码、测试和微调代理的明确角色，结合微调LLMs和高级推理模型，实现了隐私保护和协作开发。

Result: Agentsway通过结构化的生命周期和协作机制，增强了领域特定推理和可解释决策，并嵌入了负责任AI原则。

Conclusion: Agentsway为AI代理作为一等协作者的生态系统提供了一种新型软件开发框架，推动了软件工程向AI原生、自我改进的方法论迈进。

Abstract: The emergence of Agentic AI is fundamentally transforming how software is
designed, developed, and maintained. Traditional software development
methodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for
human-centric teams and are increasingly inadequate in environments where
autonomous AI agents contribute to planning, coding, testing, and continuous
learning. To address this methodological gap, we present "Agentsway" a novel
software development framework designed for ecosystems where AI agents operate
as first-class collaborators. Agentsway introduces a structured lifecycle
centered on human orchestration, and privacy-preserving collaboration among
specialized AI agents. The framework defines distinct roles for planning,
prompting, coding, testing, and fine-tuning agents, each contributing to
iterative improvement and adaptive learning throughout the development process.
By integrating fine-tuned LLMs that leverage outputs and feedback from
different agents throughout the development cycle as part of a retrospective
learning process, Agentsway enhances domain-specific reasoning, and explainable
decision-making across the entire software development lifecycle. Responsible
AI principles are further embedded across the agents through the coordinated
use of multiple fine-tuned LLMs and advanced reasoning models, ensuring
balanced, transparent, and accountable decision-making. This work advances
software engineering by formalizing agent-centric collaboration, integrating
privacy-by-design principles, and defining measurable metrics for productivity
and trust. Agentsway represents a foundational step toward the next generation
of AI-native, self-improving software development methodologies. To the best of
our knowledge, this is the first research effort to introduce a dedicated
methodology explicitly designed for AI agent-based software engineering teams.

</details>


### [76] [RefleXGen:The unexamined code is not worth using](https://arxiv.org/abs/2510.23674)
*Bin Wang,Hui Li,AoFan Liu,BoTao Yang,Ao Yang,YiLu Zhong,Weixiang Huang,Yanping Zhang,Runhuai Huang,Weimin Zeng*

Main category: cs.SE

TL;DR: RefleXGen通过RAG和LLM自我反思机制提升代码安全性，实验显示在多模型上显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在代码生成中的安全性挑战，避免传统方法（如微调或开发专门的安全代码数据集）的资源密集性问题。

Method: RefleXGen通过结合检索增强生成（RAG）技术和LLMs自带的引导性自我反思机制，迭代优化代码生成过程，无需大量资源。

Result: RefleXGen显著提升了代码安全性，在GPT-3.5 Turbo、GPT-4o、CodeQwen和Gemini上分别实现了13.6%、6.7%、4.5%和5.8%的改进。

Conclusion: 提高模型自我反思的质量是增强AI生成代码安全性的有效且实用的策略。

Abstract: Security in code generation remains a pivotal challenge when applying large
language models (LLMs). This paper introduces RefleXGen, an innovative method
that significantly enhances code security by integrating Retrieval-Augmented
Generation (RAG) techniques with guided self-reflection mechanisms inherent in
LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing
specialized secure code datasets - processes that can be resource-intensive -
RefleXGen iteratively optimizes the code generation process through
self-assessment and reflection without the need for extensive resources. Within
this framework, the model continuously accumulates and refines its knowledge
base, thereby progressively improving the security of the generated code.
Experimental results demonstrate that RefleXGen substantially enhances code
security across multiple models, achieving a 13.6% improvement with GPT-3.5
Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a
5.8% improvement with Gemini. Our findings highlight that improving the quality
of model self-reflection constitutes an effective and practical strategy for
strengthening the security of AI-generated code.

</details>


### [77] [TDFlow: Agentic Workflows for Test Driven Software Engineering](https://arxiv.org/abs/2510.23761)
*Kevin Han,Siddharth Maddikayala,Tim Knappe,Om Patel,Austen Liao,Amir Barati Farimani*

Main category: cs.SE

TL;DR: TDFlow是一种测试驱动的代理工作流，通过分解任务提升LLM在软件工程中的表现，尤其在解决人类测试时接近人类水平，但生成有效测试仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 为解决人类编写的测试，设计了TDFlow这一测试驱动的代理工作流，旨在简化仓库级软件工程任务。

Method: TDFlow将软件工程程序修复分解为四个由子代理管理的组件：补丁提案、调试、补丁修订和可选测试生成。

Result: TDFlow在SWE-Bench Lite上达到88.8%通过率（绝对提升27.8%），在SWE-Bench Verified上达到94.3%通过率，仅发现7例测试作弊。

Conclusion: 现代大语言模型（LLM）在精心设计的测试驱动工作流中已能达到人类水平的测试解决能力，完全自主修复仓库的最后障碍在于生成准确的再现测试。

Abstract: We introduce TDFlow, a novel test-driven agentic workflow that frames
repository-scale software engineering as a test-resolution task, specifically
designed to solve human-written tests. Given a set of tests, TDFlow repeatedly
proposes, revises, and debugs repository-scale patches using precisely
engineered sub-agents and tightly constrained tools. The workflow decomposes
software engineering program repair into four components governed by respective
sub-agents. This simple, forced decoupling of patch proposing, debugging, patch
revision, and optional test generation (1) reduces long-context burden on any
individual sub-agent, (2) focuses each sub-agent on specific, pre-defined
sub-tasks, and (3) allows for specialized performance improvement on specific
sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on
SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and
94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within
SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which
were subsequently counted as failures. Furthermore, we show that the primary
obstacle to human-level software engineering performance lies within writing
successful reproduction tests. We envision a human-LLM interactive system
powered by TDFlow where human developers write tests solved by LLM systems.
Together, these results indicate that modern LLMs, when embedded in a narrowly
engineered, test-driven workflow, already achieve human-level test resolution
-- with the final frontier for fully autonomous repository repair being the
accurate generation of valid reproduction tests.

</details>


### [78] [Evaluating the effectiveness of LLM-based interoperability](https://arxiv.org/abs/2510.23893)
*Rodrigo Falcão,Stefan Schweitzer,Julien Siebert,Emily Calvet,Frank Elberzhager*

Main category: cs.SE

TL;DR: LLMs（如qwen2.5-coder:32b）在特定策略下能实现系统自主互操作性，但在复杂任务（如单位转换）中表现不一，需进一步跨领域评估和可靠性研究。


<details>
  <summary>Details</summary>
Motivation: 随着系统日益动态和异构，互操作性挑战加剧，开发互操作性工件需要大量时间投入。本研究旨在分析基于LLM的策略在实现系统运行时自主互操作性（无需人工干预）的有效性。

Method: 选择了13个开源LLMs，并在农业互操作性用例中整理了四个版本的数据集。每个模型与每个数据集版本进行了三次运行，采用两种不同策略（DIRECT和CODEGEN），并比较了模型的有效性和结果的一致性。

Result: qwen2.5-coder:32b在两种策略（DIRECT和CODEGEN）下表现最有效，在四个数据集版本中的三个版本中DIRECT策略平均pass@1≥0.99，CODEGEN策略平均pass@1≥0.89。在包含单位转换的第四个版本中，所有模型使用DIRECT策略均失败，而CODEGEN策略下qwen2.5-coder:32b仍以平均pass@1=0.75成功。

Conclusion: 部分大型语言模型（LLMs）能够实现系统间的自主互操作性。建议在不同领域进一步评估，并深入研究可靠性策略。

Abstract: Background: Systems of systems are becoming increasingly dynamic and
heterogeneous, and this adds pressure on the long-standing challenge of
interoperability. Besides its technical aspect, interoperability has also an
economic side, as development time efforts are required to build the
interoperability artifacts. Objectives: With the recent advances in the field
of large language models (LLMs), we aim at analyzing the effectiveness of
LLM-based strategies to make systems interoperate autonomously, at runtime,
without human intervention. Method: We selected 13 open source LLMs and curated
four versions of a dataset in the agricultural interoperability use case. We
performed three runs of each model with each version of the dataset, using two
different strategies. Then we compared the effectiveness of the models and the
consistency of their results across multiple runs. Results: qwen2.5-coder:32b
was the most effective model using both strategies DIRECT (average pass@1 >=
0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset
versions. In the fourth dataset version, which included an unit conversion, all
models using the strategy DIRECT failed, whereas using CODEGEN
qwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some
LLMs can make systems interoperate autonomously. Further evaluation in
different domains is recommended, and further research on reliability
strategies should be conducted.

</details>


### [79] [Validating Alerts in Cloud-Native Observability](https://arxiv.org/abs/2510.23970)
*Maria C. Borges,Julian Legler,Lucca Di Benedetto*

Main category: cs.SE

TL;DR: OXN工具的警报扩展让工程师在设计阶段测试和调整警报规则，避免运行时问题，解决了警报设计中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代可靠性工程中，警报设计面临平衡早期问题捕捉和减少误报的挑战，且警报代码因覆盖罕见故障而缺乏测试。

Method: 引入OXN观测实验工具的警报扩展，使工程师能够在设计阶段调整规则并验证警报的触发行为。

Result: 通过OXN的警报扩展，工程师可以在设计时优化警报规则并验证其行为，提高警报系统的可靠性。

Conclusion: 本文提出了OXN工具的警报扩展，帮助工程师在开发早期阶段测试和调整警报规则，从而避免运行时问题。

Abstract: Observability and alerting form the backbone of modern reliability
engineering. Alerts help teams catch faults early before they turn into
production outages and serve as first clues for troubleshooting. However,
designing effective alerts is challenging. They need to strike a fine balance
between catching issues early and minimizing false alarms. On top of this,
alerts often cover uncommon faults, so the code is rarely executed and
therefore rarely checked. To address these challenges, several industry
practitioners advocate for testing alerting code with the same rigor as
application code. Still, there's a lack of tools that support such systematic
design and validation of alerts.
  This paper introduces a new alerting extension for the observability
experimentation tool OXN. It lets engineers experiment with alerts early during
development. With OXN, engineers can now tune rules at design time and
routinely validate the firing behavior of their alerts, avoiding future
problems at runtime.

</details>


### [80] [Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs](https://arxiv.org/abs/2510.24019)
*Xing Xing,Wei Wang,Lipeng Ma,Weidong Yang,Junjie Zheng*

Main category: cs.SE

TL;DR: 论文提出了一种生命周期感知的代码生成框架，通过整合软件开发中间产物，显著提升了大语言模型的代码生成质量，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面取得了进展，但现有方法通常忽略软件工程的结构化实践，导致生成的代码质量有限。

Method: 提出了一个生命周期感知的框架，将软件开发的中间产物（如需求分析、状态机建模和伪代码）纳入训练和推理阶段，实现结构化推理。

Result: 实验显示，框架显著提升了代码正确性（最高75%），并在多个基准测试中表现优于现有模型（如CodeBLEU改进达34.3%）。开源模型在框架微调后甚至超越了预训练模型。

Conclusion: 该论文提出了一个生命周期感知的框架，通过整合软件工程中的中间产物（如需求分析、状态机建模和伪代码）来提升大语言模型的代码生成能力。实验证明，该方法显著提高了代码正确性，并在多个基准测试中超越了现有模型。

Abstract: Recent progress in large language models (LLMs) has advanced automatic code
generation, yet most approaches rely on direct, single-step translation from
problem descriptions to code, disregarding structured software engineering
practices. We introduce a lifecycle-aware framework that systematically
incorporates intermediate artifacts such as requirements analysis, state
machine modeling, and pseudocode into both the training and inference stages.
This design aligns code generation with standard software development phases
and enables more structured reasoning. Experiments show that lifecycle-level
fine-tuning improves code correctness by up to 75% over the same model before
fine-tuning, with performance gains compounding across intermediate stages.
Multi-step inference consistently surpasses single-step generation,
demonstrating the effectiveness of intermediate scaffolding. Notably,
open-source LLMs, once fine-tuned under our framework, match or slightly
outperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our
framework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and
22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,
respectively. Our pipeline also proves robust with up to 80\% less training
data, confirming its resilience. Ablation studies further reveal that each
intermediate artifact contributes distinctly to final code quality, with state
machine modeling yielding the most substantial impact. Our source code and
detailed experimental data are available at
https://anonymous.4open.science/r/Lifecycle-Aware-3CCB.

</details>


### [81] [Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps](https://arxiv.org/abs/2510.24142)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 研究发现生产ML系统无声失败问题，通过焦点小组实证分析实践者的可观测性行为，提出了改进工具和研究的方向。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的机器学习系统经常无声失败（即作出错误决策而非崩溃），但缺乏关于实践者实际捕获哪些信息的实证研究。

Method: 通过七个焦点小组会议，研究多个领域的ML实践者，系统记录了他们捕获的信息及其使用方式。

Result: 研究详细列出了实践者系统捕获的信息及其在验证模型、检测和诊断故障以及解释性能退化中的应用，并识别了当前实践的差距。

Conclusion: 研究指出了当前机器学习可观测性实践中的不足，并提出了工具设计和研究方向的建议。

Abstract: Production machine learning (ML) systems fail silently -- not with crashes,
but through wrong decisions. While observability is recognized as critical for
ML operations, there is a lack empirical evidence of what practitioners
actually capture. This study presents empirical results on ML observability in
practice through seven focus group sessions in several domains. We catalog the
information practitioners systematically capture across ML systems and their
environment and map how they use it to validate models, detect and diagnose
faults, and explain observed degradations. Finally, we identify gaps in current
practice and outline implications for tooling design and research to establish
ML observability practices.

</details>


### [82] [Investigating Software Aging in LLM-Generated Software Systems](https://arxiv.org/abs/2510.24188)
*César Santos,Ermeson Andrade,Roberto Natella*

Main category: cs.SE

TL;DR: LLM生成的软件在长期运行中会出现老化现象，表现为内存增长、性能下降，需进一步研究缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成软件在持续执行下的长期可靠性，特别是软件老化现象。

Method: 使用Bolt平台和Baxbench标准化提示生成四个面向服务的应用，并进行50小时的负载测试，持续监控资源使用、响应时间和吞吐量。

Result: 所有应用均表现出显著的老化现象，包括内存增长、响应时间增加和性能不稳定，统计分析证实了这些趋势，并显示不同应用类型老化严重程度存在差异。

Conclusion: 研究结果表明，自动生成的软件需要考虑老化现象，为未来研究缓解策略和长期可靠性评估奠定了基础。

Abstract: Automatically generated software, especially code produced by Large Language
Models (LLMs), is increasingly adopted to accelerate development and reduce
manual effort. However, little is known about the long-term reliability of such
systems under sustained execution. In this paper, we experimentally investigate
the phenomenon of software aging in applications generated by LLM-based tools.
Using the Bolt platform and standardized prompts from Baxbench, we generated
four service-oriented applications and subjected them to 50-hour load tests.
Resource usage, response time, and throughput were continuously monitored to
detect degradation patterns. The results reveal significant evidence of
software aging, including progressive memory growth, increased response time,
and performance instability across all applications. Statistical analyzes
confirm these trends and highlight variability in the severity of aging
according to the type of application. Our findings show the need to consider
aging in automatically generated software and provide a foundation for future
studies on mitigation strategies and long-term reliability evaluation.

</details>


### [83] [MAGNET: A Multi-Graph Attentional Network for Code Clone Detection](https://arxiv.org/abs/2510.24241)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.SE

TL;DR: MAGNET是一种多图注意力框架，整合AST、CFG和DFG表示，显著提升代码克隆检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一表示（如AST、CFG、DFG），仅捕获代码语义的部分方面；混合方法的融合策略通常是手工设计且效果不佳。

Method: MAGNET结合了残差图神经网络与节点级自注意力机制，引入门控跨注意力机制进行细粒度图间交互，并使用Set2Set池化融合多图嵌入。

Result: 在BigCloneBench和Google Code Jam数据集上，MAGNET分别达到96.5%和99.2%的F1分数。

Conclusion: MAGNET通过多图注意力框架有效整合了AST、CFG和DFG表示，实现了代码克隆检测的最先进性能，F1分数分别达到96.5%和99.2%。

Abstract: Code clone detection is a fundamental task in software engineering that
underpins refactoring, debugging, plagiarism detection, and vulnerability
analysis. Existing methods often rely on singular representations such as
abstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs
(DFGs), which capture only partial aspects of code semantics. Hybrid approaches
have emerged, but their fusion strategies are typically handcrafted and
ineffective. In this study, we propose MAGNET, a multi-graph attentional
framework that jointly leverages AST, CFG, and DFG representations to capture
syntactic and semantic features of source code. MAGNET integrates residual
graph neural networks with node-level self-attention to learn both local and
long-range dependencies, introduces a gated cross-attention mechanism for
fine-grained inter-graph interactions, and employs Set2Set pooling to fuse
multi-graph embeddings into unified program-level representations. Extensive
experiments on BigCloneBench and Google Code Jam demonstrate that MAGNET
achieves state-of-the-art performance with an overall F1 score of 96.5\% and
99.2\% on the two datasets, respectively. Ablation studies confirm the critical
contributions of multi-graph fusion and each attentional component. Our code is
available at https://github.com/ZixianReid/Multigraph_match

</details>


### [84] [Developer Productivity with GenAI](https://arxiv.org/abs/2510.24265)
*Sadia Afroz,Zixuan Feng,Katie Kimura,Bianca Trinkenreich,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 调查发现，生成式AI工具在软件开发中虽提升速度，但整体生产力改善有限，存在生产力悖论。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI工具在软件开发中的实际生产力影响，以明确其在哪些方面能有效提升生产力。

Method: 通过调查415名软件从业者，采用SPACE框架（满意度与幸福感、绩效、活动、沟通与合作、效率与流畅性）来捕捉他们对AI辅助开发生产力变化的感知。

Result: 结果按AI使用频率分类显示，整体生产力变化有限，揭示了开发者速度提升但软件质量或满意度未必提高的生产力悖论。

Conclusion: 生成式AI（GenAI）工具在软件开发中作为生产力辅助工具的采用日益增加，但实际提升生产力的具体情况尚不明确。研究表明，AI的采用对开发者生产力的不同维度影响有限，揭示了生产力悖论，即开发者速度提升但软件质量或满意度未必提高。

Abstract: Generative AI (GenAI) tools are increasingly being adopted in software
development as productivity aids. However, evidence regarding where and when
these tools actually enhance productivity is unclear. In this paper, we
investigate how GenAI adoption affects different dimensions of developer
productivity. We surveyed 415 software practitioners to capture their
perceptions of productivity changes associated with AI-assisted development
using the SPACE framework - Satisfaction and well-being, Performance, Activity,
Communication and collaboration, and Efficiency and flow. Our results,
disaggregated by frequency of AI usage, reveal limited overall productivity
change, highlighting the productivity paradox in which developers become faster
but do not necessarily create better software or feel more fulfilled.

</details>


### [85] [Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation](https://arxiv.org/abs/2510.24358)
*Lingyue Fu,Bolun Zhang,Hao Guan,Yaoming Zhu,Lin Qiu,Weiwen Liu,Xuezhi Cao,Xunliang Cai,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: PRDBench是一个新型基准，通过代理驱动管道构建，解决了现有基准的高成本和僵化问题，包含50个Python项目，实验证明其评估有效性。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理评估基准存在高标注成本和专业知识要求，以及主要依赖单元测试的僵化评估指标问题。

Method: 提出了一种基于人工监督的代理驱动基准构建管道，生成多样化和挑战性的项目级任务，并引入Agent-as-a-Judge范式进行评分。

Result: PRDBench包含50个跨20个领域的真实Python项目，每个项目都有结构化PRD需求、全面评估标准和参考实现。实验证明了其在评估代码代理和评估代理能力方面的有效性。

Conclusion: PRDBench提供了一个可扩展且稳健的框架，用于评估代码代理和评估代理的能力，解决了现有基准标注成本高和评估指标僵化的问题。

Abstract: Recent advances in code agents have enabled automated software development at
the project level, supported by large language models (LLMs) and widely adopted
tools. However, existing benchmarks for code agent evaluation face two major
limitations: high annotation cost and expertise requirements, and rigid
evaluation metrics that rely primarily on unit tests. To address these
challenges, we propose an agent-driven benchmark construction pipeline that
leverages human supervision to efficiently generate diverse and challenging
project-level tasks. Based on this approach, we introduce PRDBench, a novel
benchmark comprising 50 real-world Python projects across 20 domains, each with
structured Product Requirement Document (PRD) requirements, comprehensive
evaluation criteria, and reference implementations. PRDBench features rich data
sources, high task complexity, and flexible metrics. We further employ an
Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of
various test types beyond unit tests. Extensive experiments on PRDBench
demonstrate its effectiveness in assessing the capabilities of both code agents
and evaluation agents, providing a scalable and robust framework for annotation
and evaluation.

</details>


### [86] [LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead](https://arxiv.org/abs/2510.24367)
*Junda He,Jieke Shi,Terry Yue Zhuo,Christoph Treude,Jiamou Sun,Zhenchang Xing,Xiaoning Du,David Lo*

Main category: cs.SE

TL;DR: 本文探讨了使用LLMs自动评估软件产物的方法，分析了现有研究的不足，并提出了到2030年的发展路线图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件工程中的广泛应用产生了大量软件产物，但缺乏可扩展且可靠的评估方法，促使了LLM-as-a-Judge范式的出现。

Method: 通过文献综述分析现有研究的局限性，识别关键研究空白，并提出详细的路线图。

Result: 提出了一个详细的路线图，以推动LLM-as-a-Judge框架的研究和采用，最终提升软件产物评估的可扩展性。

Conclusion: 本文提出了一种展望，旨在引导社区推进LLM-as-a-Judge框架，目标是到2030年实现可靠、鲁棒和可扩展的自动化评估方法，以提升软件产物的评估效率。

Abstract: The rapid integration of Large Language Models (LLMs) into software
engineering (SE) has revolutionized tasks like code generation, producing a
massive volume of software artifacts. This surge has exposed a critical
bottleneck: the lack of scalable, reliable methods to evaluate these outputs.
Human evaluation is costly and time-consuming, while traditional automated
metrics like BLEU fail to capture nuanced quality aspects. In response, the
LLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.
This approach leverages the advanced reasoning of LLMs, offering a path toward
human-like nuance at automated scale. However, LLM-as-a-Judge research in SE is
still in its early stages. This forward-looking SE 2030 paper aims to steer the
community toward advancing LLM-as-a-Judge for evaluating LLM-generated software
artifacts. We provide a literature review of existing SE studies, analyze their
limitations, identify key research gaps, and outline a detailed roadmap. We
envision these frameworks as reliable, robust, and scalable human surrogates
capable of consistent, multi-faceted artifact evaluation by 2030. Our work aims
to foster research and adoption of LLM-as-a-Judge frameworks, ultimately
improving the scalability of software artifact evaluation.

</details>


### [87] [CodeWiki: Automated Repository-Level Documentation at Scale](https://arxiv.org/abs/2510.24428)
*Nguyen Hoang Anh,Minh Le-Anh,Bach Le,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: CodeWiki 是一个开源框架，通过分层分解、递归代理处理和文本/视觉合成，显著提升仓库级文档质量，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 开发者花费近58%的时间理解代码库，但由于复杂性和手动工作，维护全面文档仍然具有挑战性。现有大型语言模型（LLMs）在仓库级文档记录上表现不足。

Method: CodeWiki 采用三种创新方法：(i) 分层分解以保留架构上下文，(ii) 递归代理处理与动态委托，(iii) 结合文本和视觉工件的合成（如架构图和数据流）。

Result: CodeWiki 在专有模型和开源替代方案上分别达到68.79%和64.80%的质量得分，优于现有闭源系统。

Conclusion: CodeWiki 是第一个开源框架，能够在七种编程语言中实现全面的仓库级文档记录，其质量得分显著优于现有闭源系统。

Abstract: Developers spend nearly 58% of their time understanding codebases, yet
maintaining comprehensive documentation remains challenging due to complexity
and manual effort. While recent Large Language Models (LLMs) show promise for
function-level documentation, they fail at the repository level, where
capturing architectural patterns and cross-module interactions is essential. We
introduce CodeWiki, the first open-source framework for holistic
repository-level documentation across seven programming languages. CodeWiki
employs three innovations: (i) hierarchical decomposition that preserves
architectural context, (ii) recursive agentic processing with dynamic
delegation, and (iii) synthesis of textual and visual artifacts including
architecture diagrams and data flows. We also present CodeWikiBench, the first
repository-level documentation benchmark with multi-level rubrics and agentic
assessment. CodeWiki achieves 68.79% quality score with proprietary models and
64.80% with open-source alternatives, outperforming existing closed-source
systems and demonstrating scalable, accurate documentation for real-world
repositories.

</details>


### [88] [The Divine Software Engineering Comedy -- Inferno: The Okinawa Files](https://arxiv.org/abs/2510.24483)
*Michele Lanza*

Main category: cs.SE

TL;DR: 本文通过FUSE研讨会的讨论，以讽刺和悲观视角分析了软件工程未来的三大问题：开发者能力不足、领域知识遗忘过快和技术无序增长。


<details>
  <summary>Details</summary>
Motivation: 探讨软件工程的未来发展趋势及其潜在问题。

Method: 通过组织FUSE研讨会，收集并分析与会者的讨论和观点。

Result: 作者提炼出三个主要的噩梦方向：无知的软件开发者、快速遗忘的领域知识，以及技术爆炸式增长。

Conclusion: 作者对软件工程的未来持悲观态度，认为其发展如同慢动作的车祸，既无法避免又无法忽视。

Abstract: In June 2024 I co-organized the FUture of Software Engineering symposium in
Okinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were
general chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo
were program chairs, some members of my group, Carmen Armenti, Stefano
Campanella, Roberto Minelli, were the tables, can't have a room with only
chairs, after all. We invited a crowd of people to discuss what future software
engineering has. FUSE became a 3-day marathon on whether there is actually a
future at all for SE. This essay is a slightly dark take about what I saw at
that event, very loosely based on the discussions that took place, adding some
healthy sarcasm and cynicism, the intellectual salt and pepper I never seem to
run out of. I listened to the brilliant people who gathered to talk about where
we're headed, and distilled three nightmares headed in our direction: software
makers who don't know what they're doing, but get the job done anyway, a field
moving so fast it can't remember its own lessons, and technologies multiplying
like rabbits in Spring. So, let's start. The future, eh? The future of software
engineering looks like a car crash in slow motion: you can see it coming but
you can't look away. The thing is...

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [89] [Expander Decomposition for Non-Uniform Vertex Measures](https://arxiv.org/abs/2510.23913)
*Daniel Agassy,Dani Dorfman,Haim Kaplan*

Main category: cs.DS

TL;DR: 本文提出了一种广义扩展分解算法，能在Õ(m)时间内计算基于顶点度量μ的扩展分解，扩展了现有成果。


<details>
  <summary>Details</summary>
Motivation: 传统的扩展分解算法在处理广义扩展概念时存在局限性，本文旨在解决这一问题，为更广泛的图算法应用提供支持。

Method: 采用随机化算法，时间复杂度为Õ(m)，计算基于μ-扩展的分解。

Result: 成功实现了在Õ(m)时间内计算基于μ-扩展的分解，扩展了[ADK23]的结果。

Conclusion: 本文提出了一种广义的扩展分解算法，能够在随机化线性时间内计算基于顶点度量μ的扩展分解，为更广泛的图算法应用提供了基础。

Abstract: A $(\phi,\epsilon)$-expander-decomposition of a graph $G$ (with $n$ vertices
and $m$ edges) is a partition of $V$ into clusters $V_1,\ldots,V_k$ with
conductance $\Phi(G[V_i]) \ge \phi$, such that there are at most $\epsilon m$
inter-cluster edges. Such a decomposition plays a crucial role in many graph
algorithms. [ADK23] gave a randomized $\tilde{O}(m)$ time algorithm for
computing a $(\phi, \phi\log^2 {n})$-expander decomposition.
  In this paper we generalize this result for a broader notion of expansion.
Let $\mu \in {\mathbb{R}}_{\ge 0 }^n$ be a vertex measure. A standard
generalization of conductance of a cut $(S,\bar{S})$ is its $\mu$-expansion
$\Phi^{\mu}_G(S,\bar{S}) = |E(S,\bar{S})|/\min \mu(S)),\mu(\bar{S})\}$, where
$\mu(S) = \sum_{v\in S} \mu(v)$. We present a randomized $\tilde{O}(m)$ time
algorithm for computing a $(\phi, \phi \log^2
{n}\left(\frac{\mu(V)}{m}\right))$-expander decomposition with respect to
$\mu$-expansion.

</details>


### [90] [On Competitiveness of Dynamic Replication for Distributed Data Access](https://arxiv.org/abs/2510.24098)
*Tianyu Zuo,Xueyan Tang,Bu Sung Lee,Jianfei Cai*

Main category: cs.DS

TL;DR: 本文反驳了现有算法的竞争比声称，提出了一个竞争比为max{2, min{γ, 3}}的新在线算法，并通过理论和实证验证其性能。


<details>
  <summary>Details</summary>
Motivation: 研究分布式存储和访问中的在线成本优化问题，旨在动态创建和删除地理分布式服务器上的数据副本，以最小化总存储和网络成本。

Method: 重新审视了文献中的算法，构建反例证明其竞争比并非如声称的2。进一步证明对于一般成本优化问题，没有确定性在线算法能达到竞争比2。开发了一个新的在线算法，并进行了竞争比分析。

Result: 提出的在线算法实现了max{2, min{γ, 3}}的竞争比，并通过实例和实证评估验证了其紧密度和有效性。

Conclusion: 本文通过反例和理论证明，指出现有算法无法达到所声称的竞争比为2的性能，并提出了一个新的在线算法，其竞争比为max{2, min{γ, 3}}，其中γ是所有服务器存储成本的最大/最小比。通过实证评估验证了算法的有效性。

Abstract: This paper studies an online cost optimization problem for distributed
storage and access. The goal is to dynamically create and delete copies of data
objects over time at geo-distributed servers to serve access requests and
minimize the total storage and network cost. We revisit a recent algorithm in
the literature and show that it does not have a competitive ratio of $2$ as
claimed by constructing a counterexample. We further prove that no
deterministic online algorithm can achieve a competitive ratio bounded by $2$
for the general cost optimization problem. We develop an online algorithm and
prove that it achieves a competitive ratio of $\max\{2, \min\{\gamma, 3\}\}$,
where $\gamma$ is the max/min storage cost ratio among all servers. Examples
are given to confirm the tightness of competitive analysis. We also empirically
evaluate algorithms using real object access traces.

</details>


### [91] [Coreset for Robust Geometric Median: Eliminating Size Dependency on Outliers](https://arxiv.org/abs/2510.24621)
*Ziyi Fang,Lingxiao Huang,Runkai Yang*

Main category: cs.DS

TL;DR: 本文提出新的核心集构建方法，减少异常值影响，并在多种数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究欧几里得空间中的鲁棒几何中位数问题，特别是核心集构建，以改进现有工作中的异常值依赖问题。

Method: 采用非分量误差分析技术，构建核心集以减少异常值影响。

Result: 在$n \geq 4m$时，核心集大小为$\tilde{O}(\varepsilon^{-2} \cdot \min\{\varepsilon^{-2}, d\})$，消除了先前工作中的$O(m)$依赖。对于$d=1$的特殊情况，实现了最优核心集大小。

Conclusion: 本文提出了一种新的核心集构建方法，显著减少了异常值的影响，并在多种数据集上验证了其优越性。

Abstract: We study the robust geometric median problem in Euclidean space
$\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact
summary of a dataset $P$ of size $n$ that approximates the robust cost for all
centers $c$ within a multiplicative error $\varepsilon$. Given an outlier count
$m$, we construct a coreset of size $\tilde{O}(\varepsilon^{-2} \cdot
\min\{\varepsilon^{-2}, d\})$ when $n \geq 4m$, eliminating the $O(m)$
dependency present in prior work [Huang et al., 2022 & 2023]. For the special
case of $d = 1$, we achieve an optimal coreset size of
$\tilde{\Theta}(\varepsilon^{-1/2} + \frac{m}{n} \varepsilon^{-1})$, revealing
a clear separation from the vanilla case studied in [Huang et al., 2023;
Afshani and Chris, 2024]. Our results further extend to robust
$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence
under mild data assumptions. The key technical contribution is a novel
non-component-wise error analysis, enabling substantial reduction of outlier
influence, unlike prior methods that retain them.Empirically, our algorithms
consistently outperform existing baselines in terms of size-accuracy tradeoffs
and runtime, even when data assumptions are violated across a wide range of
datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS是一种通用游戏智能体，通过统一动作空间和大规模预训练，在多种游戏任务中表现优异，展示了通用智能体的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决API或GUI方法在跨领域大规模预训练中的限制，提升智能体在异构游戏环境中的通用性。

Method: 使用基于键盘-鼠标输入的通用动作空间，结合衰减持续损失和高效稀疏思维策略，进行大规模预训练。

Result: 在开放世界Minecraft任务中成功率是之前最佳模型的两倍，接近人类在新Web 3D游戏中的通用性，FPS基准测试中超越GPT-5等模型。

Conclusion: Game-TARS通过统一的、可扩展的动作空间结合大规模预训练，展示了构建具有广泛计算机使用能力的通用智能体的潜力。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [93] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: AI在科学问题解决中可能扩展或转移学科创造力，影响科学价值。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在科学问题解决中对学科创造力的影响，区分创造性方法与创造性产物。

Method: 通过两个数学案例研究，分析计算和AI如何扩展或转移学科创造力。

Result: 计算可以扩展学科创造力，但某些AI方法可能转移创造力，影响科学价值。

Conclusion: AI在科学问题解决中的角色可能导致学科创造力的转移，从而可能改变（甚至削弱）科学追求的价值。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [94] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: ME-POMDPs扩展了标准POMDPs以处理模型不确定性，提出了AB-POMDPs和算法来计算稳健策略。


<details>
  <summary>Details</summary>
Motivation: 解决多领域专家对问题建模不一致时，如何找到单一策略以最大化最坏情况下的奖励。

Method: 通过将任意ME-POMDP简化为仅在其转移和奖励函数或观察和奖励函数上变化的ME-POMDP，并设计精确和近似（基于点的）算法来求解AB-POMDPs的稳健策略。

Result: 成功将标准POMDP基准扩展到多环境设置，并计算出稳健策略。

Conclusion: 论文展示了ME-POMDPs可以推广到具有初始信念集的POMDPs（AB-POMDPs），并提出了精确和近似算法来计算稳健策略。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [95] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出一个基于测试时调优的Transformer框架，直接从质谱生成分子结构，显著提升未知化合物的鉴定性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖于数据库匹配或多步骤流程，难以鉴定参考数据库中未收录的化合物。

Method: 利用测试时调优技术增强预训练Transformer模型，直接从串联质谱和分子式生成分子结构。

Result: 在NPLIB1和MassSpecGym基准测试中，分别超越DiffMS方法100%和20%，测试时调优在MassSpecGym上相对传统微调有62%的性能提升。

Conclusion: 该框架通过测试时调优显著提升了预训练Transformer模型的能力，实现了直接从串联质谱和分子式进行端到端的分子结构生成，无需人工标注或中间步骤，为未知化合物的鉴定提供了更可靠的方法。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [96] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 论文研究了AI生成国际象棋谜题的创造力，专家评审结果显示AI能创造出具有美学和创意价值的谜题。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI是否能够创造出具有美学价值和创新性的输出，特别是在国际象棋谜题领域。

Method: 研究开发了一个AI系统，专门用于生成具有特定美学和创意特征的国际象棋谜题，并通过专家评审进行评估。

Result: 三位国际象棋专家对AI生成的谜题给予了高度评价，认可了其在创造力、挑战性和美学设计方面的表现。

Conclusion: 该论文通过专家评估证实了AI在生成具有美学吸引力、新颖性和反直觉解决方案的国际象棋谜题方面的创造力。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [97] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 基础模型在病理学中表现不佳，需重新设计以匹配组织形态学的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在计算病理学中的应用效果，发现其存在显著缺陷，需要改进。

Method: 通过系统性评估，揭示基础模型在病理学应用中的七个根本问题。

Result: 识别出七个导致基础模型在病理学中表现不佳的原因。

Conclusion: 当前病理学基础模型在概念上与组织形态学的本质不匹配，需要从根本上重新思考这一范式。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [98] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP是一种结合前瞻性分解、结构化父计划重注入和内存高效执行的框架，显著提升LLMs在长时程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长时程任务中因上下文漂移、目标信息丢失和重复失败循环等问题。

Method: ReCAP结合了三种机制：(i) 前瞻性分解，(ii) 结构化父计划重注入，(iii) 内存高效执行。

Result: 在多个长时程推理基准测试中，ReCAP显著提升了子目标对齐和成功率，如Robotouille任务中分别取得32%和29%的改进。

Conclusion: ReCAP框架通过结合前瞻性分解、结构化父计划重注入和内存高效执行，显著提升了LLMs在多步推理和动态重规划任务中的表现。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [99] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: LLM智能体在分散式多智能体路径规划中，通过结构化信息和提示设计，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决分散条件下多智能体在共享环境中的目标分配问题，以提升路径规划的效率和协调性。

Method: 通过系统地比较贪心启发式、最优分配和基于大语言模型（LLM）的智能体在完全可观察的网格世界环境中的表现。

Result: 基于LLM的智能体在得到精心设计的提示和相关定量信息时，能够实现接近最优的完成时间，并持续优于传统启发式方法。

Conclusion: 本研究展示了语言模型在分散式多智能体路径规划中的潜力，特别是在信息结构设计得当的情况下，能够接近最优解并超越传统启发式方法。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [100] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: IBM开发的通用智能代理CUGA在企业试点中表现优异，展示了减少开发成本和时间的能力，并提出了企业级系统的发展方向。


<details>
  <summary>Details</summary>
Motivation: 企业面临从原型到部署系统的挑战，通用智能代理在学术基准上表现优异，但在企业生产环境中的应用仍有限。

Method: 采用分层规划-执行架构，具有坚实的分析基础，并在AppWorld和WebArena上实现了最先进的性能。

Result: CUGA在企业业务流程外包的人才招聘领域试点中表现接近专业化代理的准确性，并显示出减少开发时间和成本的潜力。

Conclusion: CUGA作为通用智能代理在企业和学术环境中展现出潜力，但需进一步研究以增强其企业级系统的稳健性和成熟度。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [101] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的框架，通过设计新型奖励机制，显著提升了AI生成国际象棋谜题的创意、美感和反直觉性，成果获专家认可。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造真正具有创意、美感和反直觉的输出方面仍面临挑战，特别是在国际象棋谜题领域。本文旨在解决这些难题。

Method: 通过基准测试生成式AI架构，并引入基于国际象棋引擎搜索统计的强化学习框架，设计奖励机制以增强谜题的独特性、反直觉性、多样性和真实性。

Result: 强化学习方法将反直觉谜题的生成率从0.22%（监督学习）提升至2.5%，超过现有数据集（2.1%）和最佳Lichess训练模型（0.4%）。谜题在新颖性、多样性和美学主题上表现优异，并获专家高度评价。

Conclusion: AI生成的国际象棋谜题在创意、美感和反直觉性方面超越了传统书籍谜题，甚至接近经典作品的水平。最终成果是一本由世界知名专家认可其创意的AI生成谜题集。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [102] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 研究数字孪生在动态系统控制中的应用，比较四种模型和三种控制策略的性能，HAM和MPC表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生在动态系统建模与控制中的应用，探索物理基础、数据驱动和混合方法的整合，以及传统与AI驱动控制器的性能对比。

Method: 采用了四种预测模型（Linear、PBM、LSTM、HAM）和三种控制策略（MPC、RL、LLM），在微型温室平台上进行测试和比较。

Result: HAM在建模中表现出最佳平衡性能，LSTM精度高但资源消耗大；MPC稳健，RL适应性强，LLM控制器实现灵活交互。

Conclusion: HAM模型在建模中表现出最佳的平衡性能，而LSTM虽精度高但资源消耗大。控制策略中，MPC稳健可靠，RL适应性强，LLM控制器结合预测工具实现了灵活的人机交互。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [103] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文综述了代理型AI的安全风险，提出了威胁分类和防御策略，并讨论了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统因其自主执行任务的能力，带来了新的安全风险，需要系统性研究以应对这些独特的威胁。

Method: 通过调查和分类代理型AI特有的威胁，回顾最近的基准和评估方法，并从技术和治理角度讨论防御策略。

Result: 提出了一个代理型AI威胁的分类法，并综合了当前的研究，指出了未来的挑战。

Conclusion: 本文总结了当前关于代理型AI安全的研究，并强调了设计安全代理系统的挑战和方向。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [104] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 提出了一种基于摊销变分推断的训练算法，通过多样性追求的强化学习和稀疏奖励函数，提升了LVLMs在推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的训练算法（如SFT、PPO、GRPO）在未见过的推理任务上泛化能力不足，且依赖于有偏见的奖励模型。

Method: 通过将LVLMs中的推理重新表述为后验推断，并提出了一种基于摊销变分推断的可扩展训练算法。利用多样性追求的强化学习算法，引入了一种新颖的稀疏奖励函数，用于鼓励多样且高可能性的潜在CoT。

Result: 实证结果表明，所提出的方法在七个推理基准上提升了LVLMs的表现。

Conclusion: 该方法在七个推理基准上提升了最先进的大型视觉语言模型（LVLMs）的表现，包括有效性、泛化性和可解释性。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [105] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 本文提出两种架构模式，帮助软件代理高效处理复杂网页结构和动态集成网络服务，提升其构建世界模型的能力。


<details>
  <summary>Details</summary>
Motivation: 软件代理的自主性依赖于其从结构化数据（如网页DOM和网络服务语义描述）构建可操作的世界模型的能力。然而，原始HTML的冗长性和硬编码API集成的静态性带来了挑战。

Method: 介绍了两种互补的架构模式：DOM转导模式（用于简化复杂的网页DOM结构）和超媒体功能识别模式（用于动态解析和集成未知网络服务的语义描述）。

Result: 通过这两种模式，代理能够高效地构建和维护准确的世界模型，实现可扩展、自适应和互操作的自动化。

Conclusion: 本文提出的两种架构模式（DOM转导模式和超媒体功能识别模式）为软件代理提供了一个强大的框架，使其能够高效构建和维护准确的世界模型，实现在网络及其扩展资源上的可扩展、自适应和互操作的自动化。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [106] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 论文提出judo calculus框架，形式化因果发现的上下文依赖性，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中因果效应依赖于多种情境（如年龄、国家、剂量等），需要一种能形式化这种上下文依赖性的理论框架。

Method: 结合judo calculus与标准的基于分数、约束和梯度的因果发现方法，提出算法和实现框架。

Result: 实验证明，基于sheaf理论的分散式因果发现在计算效率和性能上有显著提升。

Conclusion: 论文提出了judo calculus的理论和实现框架，通过j-stable因果推断在sheaves的topos中形式化因果发现的上下文依赖性，实验结果表明该方法在计算效率和性能上优于经典因果发现方法。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [107] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: sign estimator通过二元分类损失改进LLM对齐，显著减少偏好失真，提升与真实偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM对齐方法对人类偏好的异质性敏感，导致人口平均效用的估计不一致。

Method: 提出了sign estimator，通过用二元分类损失替代交叉熵损失来改进聚合步骤。

Result: 在数字孪生模拟中，sign estimator将估计误差减少近35%，与真实人口偏好的不一致性从12%降至8%。

Conclusion: sign estimator方法在LLM对齐中显著减少了偏好失真，提高了与真实人口偏好的一致性，同时保持了现有对齐流程的简单性。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [108] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 研究通过条件深度学习模型结合个体SIR和空间背景，提升了预测破坏性事件后个体移动模式的能力。


<details>
  <summary>Details</summary>
Motivation: 预测个体在破坏性事件后的移动模式变化具有挑战性，原因包括缺乏衡量个体SIR的方法、传统特征（如社会人口特征）的局限性，以及个体移动模式与空间背景的复杂互动未被充分捕捉。

Method: 研究采用条件深度学习模型，结合大规模稀疏的个体级数据，分析个体移动模式与空间背景的复杂互动。

Result: 实验表明，结合个体的SIR和空间背景能增强模型预测后事件个体移动模式的能力，模型还能捕捉到具有相似事前模式但SIR不同的个体移动模式的差异变化。

Conclusion: 该研究通过将个体的社会基础设施韧性（SIR）纳入条件深度学习模型，成功捕捉了个人移动模式与局部空间背景之间的复杂关系，提升了预测后事件个体移动模式的能力。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [109] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*İbrahim Oğuz Çetinkaya,İ. Esra Büyüktahtakın,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: LLM发现的新启发式方法EDDC和MDDC在SMTT问题中表现优异，尤其在大型实例中超越传统方法，展示了人机协作在组合优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对单机总延迟（SMTT）问题，探索利用LLM发现新的启发式方法，以弥补传统规则启发式方法的不足，并在大规模实例中保持竞争力。

Method: 开发并对比了两种由LLM发现的启发式方法：EDD Challenger (EDDC) 和 MDD Challenger (MDDC)，灵感来源于经典的Earliest Due Date (EDD) 和 Modified Due Date (MDD) 规则。通过混合整数规划（MIP）的SMTT问题模型，评估了这些方法的优化间隙和求解时间。

Result: EDDC在500个作业以内优于经典EDD规则和其他广泛使用的算法；MDDC在更大更复杂的实例中持续优于传统启发式方法，并与精确方法保持竞争力。

Conclusion: 本研究展示了人类与大型语言模型（LLM）协作在NP难约束组合优化问题中能够生成可扩展且高性能的启发式方法，即使在资源有限的情况下，只要配置得当。

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [110] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast是一种模块化框架，通过解耦时间序列的季节性和趋势成分并进行独立建模，显著提升了跨域预测性能。


<details>
  <summary>Details</summary>
Motivation: 跨域时间序列预测在异构数据上泛化能力不足，现有方法难以应对域特异性趋势变化和不一致的周期性模式。

Method: OneCast框架将时间序列分解为季节性和趋势成分，分别通过轻量级投影模块和语义感知的分段标记化建模，并结合掩码离散扩散机制进行推断。

Result: 在八个领域的广泛实验中，OneCast在大多数情况下优于现有基线方法。

Conclusion: OneCast通过解耦时间序列的周期性模式和趋势成分，并采用定制化的生成路径进行建模，显著提升了跨域时间序列预测的泛化能力。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [111] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer利用LLM和ML简化日志分析，性能显著优于现有工具，适用于各类用户。


<details>
  <summary>Details</summary>
Motivation: 解决日志分析中高成本、专业人才缺乏和时间限制等挑战，同时克服LLM在上下文窗口限制和结构化文本处理上的不足。

Method: 结合大型语言模型（LLMs）和机器学习（ML）算法，采用模块化架构（包括路由器、日志识别器、日志解析器和搜索工具）。

Result: 在不同领域的日志分析任务中，性能优于现有LLM聊天机器人（如ChatGPT、ChatPDF和NotebookLM），提升幅度达39%至68%，且结果变异性显著降低（ROUGE-1 IQR减少93%）。

Conclusion: LLMLogAnalyzer通过其模块化架构显著提升了日志分析的效率和准确性，为网络安全专家和非技术人员提供了宝贵资源。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [112] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 研究比较了传统和机器学习模型在预测电动汽车跟车行为中的表现，发现随机森林回归器在所有场景中表现最优。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，理解其驾驶行为对提升交通安全和开发智能驾驶系统至关重要。研究旨在比较传统和机器学习模型在预测电动汽车跟车行为中的表现。

Method: 研究比较了传统模型（IDM、OVM、OVRV和简化CACC模型）和机器学习方法（随机森林回归器）。传统模型参数通过最小化RMSE进行校准，而机器学习模型以间距、速度和间隙类型作为输入预测加速度。

Result: 随机森林模型在所有场景中表现最优，RMSE分别为0.0046（中等间隙）、0.0016（长间隙）和0.0025（超长间隙）。传统模型中，CACC表现最佳，RMSE为2.67（长间隙）。

Conclusion: 该研究展示了机器学习模型，特别是随机森林回归器，在预测电动汽车跟车行为方面优于传统物理模型。这对于模拟电动汽车行为和分析混合自动驾驶交通动态具有重要意义。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [113] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens1是一个透明的AI助手，帮助病理学家通过自然语言提问和可视化证据更快、更自信地诊断。


<details>
  <summary>Details</summary>
Motivation: 为了让医生真正信任人工智能，需要消除其“黑盒”特性，使其推理过程透明化，就像与同事咨询一样。

Method: 开发了HistoLens1系统，能够将病理学家的自然语言问题转化为AI引擎的精确查询，并生成清晰的结构化报告和可视化证据（如热图）。

Result: HistoLens1能够智能地回答问题并提供可视化证据，同时专注于患者组织分析，避免了背景噪声的干扰。

Conclusion: HistoLens1提供了一个透明、协作的AI助手，帮助病理学家更快、更自信地进行诊断，同时保持其作为专家的主导地位。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [114] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: OpsAgent是一个自进化的多代理系统，通过免训练数据处理和多代理协作实现高效、透明的事件管理，适用于云系统。


<details>
  <summary>Details</summary>
Motivation: 由于手动事件管理在面临海量异构数据时劳动密集且易出错，现有自动化方法又难以跨系统通用、缺乏解释性且部署成本高，因此需要一种更高效的解决方案。

Method: OpsAgent采用免训练的数据处理器将异构可观测数据转化为结构化文本描述，并结合多代理协作框架实现透明且可审计的诊断推理。

Result: 在OPENRCA基准测试中，OpsAgent展现了最先进的性能，验证了其通用性、可解释性、成本效益和自我进化能力。

Conclusion: OpsAgent作为一种轻量级、自进化的多代理系统，在事件管理中表现出色，具有通用性、可解释性、成本效益和自我进化能力，适合在真实云系统中长期部署。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [115] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: 自动化生成高难度多跳问题的框架，减少人工标注，适用于训练和评估。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答数据集稀缺且多为评估设计，难以用于监督微调或强化学习，人工标注成本高且难以扩展，亟需自动化解决方案。

Method: 系统通过自然语言推理（NLI）的关系类型标注和多样性扩展生成多样化的证据簇，采用反向问题构造方法组合间接线索，并通过多模型共识过滤和结构化约束分解确保问题质量。

Result: 该框架生成了复杂且难以检索但可验证的问题，适用于训练和评估，显著降低了人工成本。

Conclusion: 本文提出了一种自动化框架，用于从半结构化知识源生成高难度的多跳问题，显著减少了人工标注的需求，同时保持了评估基准的高难度特征。

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [116] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: BLM$_1$是一个多模态空间基础模型，通过两阶段训练实现了跨空间、跨任务和跨体现的泛化，在数字和物理任务中均显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在数字-物理空间和体现间的泛化能力不足，缺乏统一的跨空间、跨体现模型。

Method: 采用两阶段训练范式：第一阶段通过精选的数字语料库将体现知识注入MLLM，同时保持语言能力；第二阶段通过意图桥接接口训练策略模块，提取MLLM的高层语义指导控制，无需微调MLLM骨干。

Result: BLM$_1$在数字任务中提升约6%，在物理任务中提升约3%，显著优于MLLMs、ELLMs、VLAs和GMLMs。

Conclusion: BLM$_1$模型在数字和物理任务中均表现出色，显著优于现有的多模态大语言模型家族，展示了其在跨空间、跨任务和跨体现泛化方面的强大能力。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [117] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: UniPlanner是首个多数据集整合的自动驾驶规划框架，通过HFTDN、GFTM和S2D三项创新实现跨数据集统一学习和高效规划。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法局限于单数据集训练，限制了规划鲁棒性。研究发现不同数据集的轨迹分布和关联具有一致性，因此提出多数据集整合的规划框架。

Method: 1. 历史-未来轨迹字典网络（HFTDN）通过历史轨迹相似性检索相关未来轨迹；2. 无梯度轨迹映射器（GFTM）学习多数据集的历史-未来关联；3. 稀疏到密集（S2D）范式在训练和推理阶段分别优化先验使用。

Result: UniPlanner实现了跨数据集统一学习，提升了轨迹规划的泛化能力和性能。

Conclusion: UniPlanner通过多数据集整合和三项协同创新，显著提升了自动驾驶规划的安全性和效率，为多数据集学习提供了首个统一框架。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [118] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: MGA是一种新型GUI代理，通过'先观察后决策'机制和动态记忆管理，解决了现有方法的依赖历史轨迹和局部探索偏差问题，实验证明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理存在两个主要问题：依赖历史轨迹导致错误传播，以及'决策优先、观察滞后'机制忽视关键界面提示。因此，需要一种新的方法来提升GUI交互的鲁棒性和泛化能力。

Method: MGA将每一步建模为一个独立、上下文丰富的环境状态，由三部分组成：当前屏幕截图、任务无关的空间信息以及动态更新的结构化记忆。

Result: 在OSworld基准测试、真实桌面应用（如Chrome、VSCode、VLC）和跨任务迁移实验中，MGA在鲁棒性、泛化性和效率方面均显著优于现有基线方法。

Conclusion: MGA通过引入'先观察后决策'的机制，解决了现有GUI代理依赖历史轨迹和局部探索偏差的问题，显著提升了鲁棒性、泛化能力和效率。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [119] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow是一个自动化管道，用于大规模MCP工具发现和训练，显著提升了LLM代理在MCP环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCP研究覆盖的服务器少，依赖昂贵的手工管理，且缺乏训练支持，阻碍了实际部署的进展。

Method: 引入了MCP-Flow，这是一个自动化的web-agent驱动的管道，用于大规模服务器发现、数据合成和模型训练。

Result: MCP-Flow从1166个服务器和11536个工具中收集并筛选数据，生成了68733个高质量指令-函数调用对和6439个轨迹，远超先前工作的规模和多样性。

Conclusion: MCP-Flow提供了一个可扩展的基础，提升了LLM代理在真实世界MCP环境中的熟练度。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [120] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出并评估了多种替代MCTS中随机策略的抽象内策略，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索（MCTS）的样本效率问题可以通过构建和使用抽象来解决，但现有方法未考虑到同一抽象节点中多个动作具有相同UCB值的情况。

Method: 通过构建和使用状态和/或动作抽象与树搜索并行，以共享同一层节点之间的信息。并提出并评估了几种替代的抽象内策略。

Result: 提出的几种抽象内策略在大多数环境和参数设置中表现优于随机策略。

Conclusion: 本文提出并评估了多种替代的抽象内策略，其中一些策略在大多数环境和参数设置中优于随机策略。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [121] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 论文提出通过LLM内部行为的相关性矩阵排名评估推理路径可信度，设计Self-Indicator方法，显著提升性能且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具备强大推理能力，但其易出错和产生幻觉的问题使得如何有效且高效地检查其输出成为关键挑战。现有方法依赖外部资源，导致计算开销高且适用性有限。

Method: 研究利用输入问题与输出推理路径之间的相关性矩阵排名作为推理正确性的指标，设计了Self-Indicator方法对候选推理路径进行重新加权。

Result: 实验表明，Self-Indicator在多种规模和家族的LLM中均表现出色，正确区分推理路径的准确率超过75%，并在三个推理基准上提升了8%以上的准确率。

Conclusion: 论文提出了一种名为Self-Indicator的简单、即插即用方法，通过LLM内部行为的相关性矩阵排名来评估推理路径的可信度，显著提升了性能且计算开销极小。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [122] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体框架，通过不同LLM驱动的智能体（ArgLLM、RbAM、RAG-ArgLLM）组合证据，提升判断预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 将判断预测任务视为声明验证的一种形式，通过多智能体的不同观点和证据来评估未来事件的合理性。

Method: 提出了一种新颖的多智能体框架，用于声明验证，包括ArgLLM、RbAM和RAG-ArgLLM三种智能体，并基于大型语言模型（LLMs）实现。

Result: 实验表明，结合多智能体证据可以提升预测准确性，尤其是在三个智能体的情况下。

Conclusion: 结合多智能体证据可以提高预测准确性，尤其是在三个智能体的情况下，同时为声明验证提供了可解释的证据组合。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [123] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文探讨gLLMs在传播研究内容分析中的应用潜力，并提出应对七大关键挑战的最佳实践指南，以提升研究质量和可及性。


<details>
  <summary>Details</summary>
Motivation: gLLMs在传播研究内容分析中展现出巨大潜力，但如何有效整合到研究方法中仍待探索。本文旨在解决这一整合过程中的关键挑战，提升研究质量和可及性。

Method: 本文通过综合新兴研究，提出了一个最佳实践指南，包括代码书开发、提示工程、模型选择、参数调整、迭代优化、模型可靠性验证和性能提升等七个关键步骤。

Result: 本文提出了一个全面的最佳实践指南，帮助研究者应对gLLM辅助内容分析中的七大挑战，确保研究质量。

Conclusion: 本文总结了大语言模型（gLLMs）在传播研究内容分析中的潜力，并提出了一个全面的最佳实践指南，以应对七大关键挑战，旨在使gLLM辅助的内容分析更易于广泛应用，并确保符合学科的质量标准。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [124] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是基于PCS原则的多智能体系统，在数据科学自动化中优于现有端到端系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的数据科学系统缺乏科学和理论指导，影响其可信度和鲁棒性。

Method: 采用基于PCS原则的多智能体系统，模块化处理数据清洗、特征工程、建模和评估，结合扰动分析、单元测试和模型验证。

Result: 在九个数据集上评估，VDSAgents表现优于AutoKaggle和DataInterpreter。

Conclusion: VDSAgents通过嵌入PCS原则在LLM驱动的数据科学自动化中展现出更高的可行性和优越性，显著优于现有的端到端系统。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [125] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: 提出Brain-like Space概念，用于统一比较AI模型与大脑的组织方式，发现模型脑相似性受多种因素影响，且与任务性能不完全一致。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型是否以类似大脑的方式组织信息，并提供一个统一的比较框架。

Method: 通过将AI模型的内在空间注意力拓扑组织映射到规范的人类功能脑网络上，构建了Brain-like Space，并分析了151个基于Transformer的模型。

Result: 发现模型在Brain-like Space中呈现连续的弧状几何结构，反映了不同程度的脑相似性，且这种相似性不仅受模态影响，还与预训练范式和位置编码方案有关。

Conclusion: Brain-like Space提供了一个统一的框架，用于跨领域定位、量化和比较智能，揭示了连接机器与大脑的深层组织原则。

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [126] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 医疗AI通常服务于平均患者，但在边缘患者中表现不佳。本文提出多智能体生态系统，通过协调智能提供个体化决策支持，旨在提升透明度和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI服务于平均患者，但在边缘患者（如罕见变异、多病共存或代表性不足人群）中表现不佳，导致公平性和信任问题。

Method: 提出了一种多智能体生态系统，用于N-of-1决策支持，智能体按器官系统、患者群体和分析模态聚类，共享模型库和证据合成工具。结果在协调层中汇总，权衡可靠性、不确定性和数据密度，最终为临床医生提供决策支持包。

Result: 验证从群体平均值转向个体可靠性，通过低密度区域的误差、小样本校准和风险-覆盖权衡来衡量。预期挑战包括计算需求、自动化偏见和监管适应性，通过缓存策略、共识检查和自适应试验框架解决。

Conclusion: 通过从单一模型转向协调智能，这种方法旨在使医疗AI与医学的第一原则一致：透明、公平且以个体为中心的护理。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [127] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Policy Cards是AI代理的机器可读约束标准，支持运行时约束遵循、自动验证和合规性，整合治理与工程实践。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在运行时遵循约束的需求，整合高级治理与工程实践，实现规模化的可问责自治。

Method: 通过定义规范层，编码允许/拒绝规则、义务、证据要求和与保障框架（如NIST AI RMF、ISO/IEC 42001和EU AI Act）的交叉映射。

Result: Policy Cards可以自动验证、版本控制，并与运行时强制执行或持续审计管道链接，为自主代理提供可验证的合规性。

Conclusion: Policy Cards为AI代理提供了一个机器可读的标准，用于表达操作、监管和伦理约束，成为部署代理的核心部分，支持分布式保障和多代理生态系统的可验证合规性。

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [128] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion框架通过分解查询和并行扩展，显著提升LLMs在Web应用中的推理效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在实时Web应用中面临的推理效率与质量难以兼顾的挑战，满足高吞吐量和低延迟的交互服务需求。

Method: Orion将单个查询推理过程分解为两个协同阶段：关键点生成（通过检索增强的少量示例提示）和内容并行扩展（基于依赖图确保逻辑一致性），并引入管道调度机制实现跨查询并行。

Result: 实验表明，Orion在生成速度上比基线快4.33倍，答案延迟降低3.42倍，推理质量提升高达18.75%。

Conclusion: Orion框架通过依赖感知的查询分解和逻辑并行内容扩展，显著提高了大规模语言模型（LLMs）在实时Web应用中的推理性能，同时满足了高效率和高质量的双重要求。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [129] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: APTBench 是一个新框架，将代理任务转化为适合基础模型的问题，填补了预训练阶段代理能力评估的空白。


<details>
  <summary>Details</summary>
Motivation: 当前预训练基准测试主要关注孤立和静态技能，未能反映模型的代理能力，而代理基准测试通常设计用于后训练模型，基础模型难以支持多轮任务执行能力。

Method: 将现实世界的代理任务和成功轨迹转化为适合基础模型的多选或文本补全问题，专注于核心代理能力（如规划和行动）。

Result: APTBench 相比现有的通用基准测试，能更有效地预测模型作为代理的下游性能，同时比后训练的端到端代理评估更轻量和经济。

Conclusion: APTBench 提供了一种轻量且高效的方法，用于在预训练阶段评估和指导 LLM 的代理潜力，填补了当前基准测试的空白。

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [130] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 通过优化替代梯度斜率和引入特权引导策略，SNN在机器人控制任务中性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在复杂控制任务中面临的两大挑战：非可微的脉冲神经元和状态动态导致的训练序列长度限制。

Method: 系统分析了替代梯度的斜率设置，提出了一种结合特权引导策略和在线环境交互的训练方法，并采用自适应斜率调度。

Result: 在RL环境中，浅斜率或调度斜率使训练和最终部署性能提升2.1倍；在无人机位置控制任务中，平均回报达到400分，显著优于现有方法。

Conclusion: 本研究通过分析替代梯度的斜率设置并提出一种新颖的训练方法，显著提升了SNN在复杂控制任务中的性能，特别是在强化学习（RL）环境中，展示了实际机器人系统中的优势。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [131] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: 论文提出了MobileRisk-Live动态沙盒和OS-Sentinel混合安全检测框架，显著提升了移动代理的安全性检测性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于VLM驱动的计算机使用代理在移动环境中的潜在安全风险（如系统破坏和隐私泄漏），当前缺乏对这些风险的全面检测方法。

Method: 提出了MobileRisk-Live动态沙盒环境和安全检测基准，并在此基础上开发了OS-Sentinel混合安全检测框架，结合形式化验证器和基于VLM的上下文判断。

Result: 实验表明，OS-Sentinel在多个指标上比现有方法提高了10%-30%。

Conclusion: OS-Sentinel框架通过结合形式化验证和基于VLM的上下文判断，显著提升了移动代理的安全性检测能力，为开发更安全可靠的自主移动代理提供了关键见解。

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [132] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 研究比较了多种大型语言模型的逻辑和抽象推理能力，发现它们与人类表现存在显著差异，尤其在演绎推理上表现较弱。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力对于人工智能的发展至关重要，因为这不仅涉及语言任务的执行，还关系到模型是否真正理解信息、进行推理并得出逻辑有效的结论。

Method: 研究通过设计八道自定义推理问题，比较了多种大型语言模型（包括GPT、Claude、DeepSeek等）的表现，并与人类在相同任务上的表现进行对比。

Result: 研究发现，大型语言模型在逻辑和抽象推理任务上的表现与人类存在显著差异，尤其是在演绎推理方面表现不佳。

Conclusion: 研究表明，大型语言模型在逻辑和抽象推理方面与人类存在显著差异，尤其是在演绎推理方面表现较弱。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [133] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: LLM框架Law in Silico模拟法律社会，宏观匹配犯罪趋势，微观揭示良好法律体系保护弱势群体。


<details>
  <summary>Details</summary>
Motivation: 现实法律实验成本高或不可行，需通过AI模拟验证和发展法律理论，支持法律管理。

Method: 引入Law in Silico框架，基于LLM的代理模拟法律场景，包括个体决策和立法、裁决、执行等制度机制。

Result: 实验显示LLM代理能宏观再现犯罪趋势，微观表明良好法律体系更保护弱势个体权益。

Conclusion: LLM-based agent frameworks can有效模拟法律社会，宏观上再现犯罪趋势，微观上揭示良好法律体系对弱势群体权益的保护作用。

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [134] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种降低标注成本的两阶段伪标注方法，结合跨任务监督和图传播，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 减少对新任务或具有挑战性任务的高质量示例收集的依赖，降低标注成本。

Method: 提出了一个两阶段流程，首先利用跨任务示例伪标注少量目标任务实例，然后通过基于图的标签传播方法将标签信息传播到其余目标示例中。

Result: 在五个任务中表现出色，同时显著降低了标注成本。

Conclusion: 该方法通过结合跨任务监督的灵活性和无需LLM的传播可扩展性，显著降低了标注成本，同时在五个任务中表现出色。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [135] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 本文提出以数据为中心的生成式AI医疗系统设计范式，通过优化医疗数据生态系统支持高质量医疗服务。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域的应用潜力巨大，但需要深入理解医疗任务及其可实现性，因此提出数据中心的范式以确保高效、高质量的医疗服务。

Method: 通过重新设计医疗数据生态系统，支持多模态数据的集成、表示和检索，并利用语义向量搜索和上下文查询等技术优化数据处理流程。

Result: 提出的医疗数据生态系统能够为生成式AI提供高质量的多模态数据，支持大规模预训练和领域微调，并通过代理层支持任务特定推理。

Conclusion: 本文提出了以数据为中心的设计范式，重新定位医疗数据生态系统作为生成式医疗系统的基础，旨在通过高效的数据处理流程支持高质量医疗服务。

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [136] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT是一个创新的多轮工具使用数据合成框架，通过三种关键技术解决现有方法的不足，并在实际评估中展现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法无法生成高质量的多轮函数调用数据，需解决目标模型训练、工具架构隔离和多轮逻辑依赖等实际问题。

Method: FunReason-MT采用环境-API图交互、高级工具查询合成和引导迭代链三种方法，以解决目标模型训练、工具架构隔离和多轮逻辑依赖等挑战。

Result: 基于FunReason-MT生成数据训练的4B模型在Berkeley Function-Calling Leaderboard (BFCLv3)上达到同类模型的最先进性能，并在BFCLv4上进一步验证了其稳健性。

Conclusion: FunReason-MT框架通过解决多轮函数调用数据的复杂性障碍，为智能体学习提供了可靠且强大的数据合成方法，并在实际评估中展现了其优越性。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [137] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: FMs are revolutionizing SSDM by integrating visual/textual data and enabling adaptive/digital twin frameworks, though challenges like sim-to-real gaps and human-robot collaboration remain.


<details>
  <summary>Details</summary>
Motivation: To explore the rapid advancements in SSDM through machine and deep learning, particularly the role of FMs in integrating visual and textual data for more effective disease management.

Method: This review screened approximately 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and discussing their role in adaptive learning (AL), reinforcement learning (RL), and digital twin frameworks.

Result: Key findings include the growing traction of FMs, the dominance of VLMs over LLMs, the nascent stage of RL and AL for smart spraying, and the potential of digital twins with RL for targeted spraying.

Conclusion: Foundation models (FMs) are transforming site-specific disease management (SSDM) through advanced integration of visual and textual data, adaptive learning, and digital twin frameworks. Addressing the sim-to-real gap and enhancing human-robot collaboration are critical for real-world deployment.

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [138] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: OrchDAG是一个模拟多轮工具交互复杂性的DAG数据生成方法，实验证明其有效性，并强调了拓扑结构和数据复杂度的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽略了多轮工具交互的复杂性，因此需要一种能够模拟和评估这种复杂性的方法。

Method: 引入了OrchDAG，一个模拟工具执行的有向无环图（DAGs）并控制复杂度的合成数据生成流程，用于模型性能的基准测试，并提出了一种基于图的奖励来增强RLVR训练。

Result: 实验表明，OrchDAG数据集提供了一个具有挑战性但可解决的基准，提出的奖励方法在与GRPO风格算法结合时有效。

Conclusion: 研究强调了在多轮工具使用中利用拓扑结构和数据复杂性的重要性，提出的图基奖励在与GRPO风格算法结合时表现出有效性。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [139] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出一种结合工具和领域知识图的框架，通过深度-稀疏集成策略提升示例工件生成效果。


<details>
  <summary>Details</summary>
Motivation: 旨在通过挖掘工具与文档间的依赖关系，提升示例工件的生成质量。

Method: 首先从工具模式构建工具知识图，同时从内部文档和SOP中提取补充知识图，然后采用深度-稀疏集成策略生成示例计划。

Result: 实验证明，该统一框架能有效建模工具交互并改进计划生成。

Conclusion: 该框架通过结合工具图和领域知识图，有效提升了工具增强的推理和规划能力。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [140] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: RoboOmni 是一种基于全模态 LLMs 的框架，通过融合听觉和视觉信号主动识别用户意图，实验证明其在机器人操作中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLMs）在机器人操作中主要依赖显式指令，而现实世界中人类很少直接发出指令。为了促进有效协作，机器人需要能够主动推断用户意图。

Method: RoboOmni 采用基于端到端全模态大型语言模型（LLMs）的 Perceiver-Thinker-Talker-Executor 框架，统一了意图识别、交互确认和行动执行，并通过时空融合听觉和视觉信号实现了稳健的意图识别。

Result: RoboOmni 在意图识别和主动协助方面表现优异，实验结果表明其在成功率和推理速度上均优于基线模型。

Conclusion: RoboOmni 在模拟和真实环境中均表现出色，超越了基于文本和自动语音识别（ASR）的基线模型，在成功率、推理速度、意图识别和主动协助方面取得了显著进展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>


### [141] [Motivating Students' Self-study with Goal Reminder and Emotional Support](https://arxiv.org/abs/2510.23860)
*Hyung Chan Cho,Go-Eum Cha,Yanfu Liu,Sooyeon Jeong*

Main category: cs.RO

TL;DR: 研究表明，社交机器人通过目标提醒和情感支持可有效提升自主学习的易用性和参与度，满意度与社交感知相关，且能预测目标达成。


<details>
  <summary>Details</summary>
Motivation: 探索社交机器人作为同伴学习伴侣在自主学习任务中的潜在影响，尤其是在提供任务导向的目标提醒和积极情感支持方面。

Method: 采用探索性的Wizard-of-Oz研究方法，对比了目标提醒和情感支持行为与仅提供物理存在的机器人（对照组）对学生感知焦点、生产力和参与度的影响。

Result: 目标提醒和情感支持条件下的参与者报告了更高的易用性，其中目标提醒条件还显示出更高的未来使用意愿。参与者对机器人的满意度与其将机器人视为社会存在的感知相关，这种感知是自主学习任务中目标达成水平的预测因素。

Conclusion: 该研究强调了社交辅助机器人在通过功能和情感参与支持自主学习方面的潜力。

Abstract: While the efficacy of social robots in supporting people in learning tasks
has been extensively investigated, their potential impact in assisting students
in self-studying contexts has not been investigated much. This study explores
how a social robot can act as a peer study companion for college students
during self-study tasks by delivering task-oriented goal reminder and positive
emotional support. We conducted an exploratory Wizard-of-Oz study to explore
how these robotic support behaviors impacted students' perceived focus,
productivity, and engagement in comparison to a robot that only provided
physical presence (control). Our study results suggest that participants in the
goal reminder and the emotional support conditions reported greater ease of
use, with the goal reminder condition additionally showing a higher willingness
to use the robot in future study sessions. Participants' satisfaction with the
robot was correlated with their perception of the robot as a social other, and
this perception was found to be a predictor for their level of goal achievement
in the self-study task. These findings highlight the potential of socially
assistive robots to support self-study through both functional and emotional
engagement.

</details>


### [142] [Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped](https://arxiv.org/abs/2510.23902)
*Jans Solano,Diego Quiroz*

Main category: cs.RO

TL;DR: 低成本轮腿四足机器人通过视觉-惯性导航系统和深度强化学习，实现自主跌倒恢复和稳健导航，降低部署门槛。


<details>
  <summary>Details</summary>
Motivation: 结合轮式的高效性和腿式的障碍物穿越能力，但现有系统依赖昂贵执行器和传感器，且缺乏跌倒恢复功能。

Method: 利用深度相机的视觉感知和深度强化学习策略，实现稳健的运动和自主跌倒恢复。

Result: 仿真实验显示在非规则地形上使用低扭矩执行器实现灵活移动，并能可靠地从外部扰动和自诱导故障中恢复。

Conclusion: 该方法降低了在预算有限的机器人平台上部署自主导航和稳健运动策略的门槛。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the obstacle
negotiation of legs, yet many state-of-the-art systems rely on costly actuators
and sensors, and fall-recovery is seldom integrated, especially for
wheeled-legged morphologies. This work presents a recovery-aware
visual-inertial navigation system on a low-cost wheeled quadruped. The proposed
system leverages vision-based perception from a depth camera and deep
reinforcement learning policies for robust locomotion and autonomous recovery
from falls across diverse terrains. Simulation experiments show agile mobility
with low-torque actuators over irregular terrain and reliably recover from
external perturbations and self-induced failures. We further show goal directed
navigation in structured indoor spaces with low-cost perception. Overall, this
approach lowers the barrier to deploying autonomous navigation and robust
locomotion policies in budget-constrained robotic platforms.

</details>


### [143] [Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments](https://arxiv.org/abs/2510.23928)
*Raman Jha,Yang Zhou,Giuseppe Loianno*

Main category: cs.RO

TL;DR: An adaptive keyframe selection method improves 3D reconstruction in dynamic scenes by combining error-based and momentum-based modules, outperforming static strategies.


<details>
  <summary>Details</summary>
Motivation: To address the data bottleneck in real-time perception by dynamically selecting the most informative frames for high-quality 3D world representations.

Method: The method integrates an error-based selection module (using photometric and SSIM errors) and a momentum-based update module to dynamically adjust keyframe selection thresholds.

Result: Experimental results show consistent improvements in reconstruction quality on Spann3r and CUT3R networks, with ablation studies confirming the effectiveness of each component.

Conclusion: The proposed adaptive keyframe selection method significantly improves 3D scene reconstruction quality in dynamic environments, outperforming traditional static strategies.

Abstract: In this paper, we propose an adaptive keyframe selection method for improved
3D scene reconstruction in dynamic environments. The proposed method integrates
two complementary modules: an error-based selection module utilizing
photometric and structural similarity (SSIM) errors, and a momentum-based
update module that dynamically adjusts keyframe selection thresholds according
to scene motion dynamics. By dynamically curating the most informative frames,
our approach addresses a key data bottleneck in real-time perception. This
allows for the creation of high-quality 3D world representations from a
compressed data stream, a critical step towards scalable robot learning and
deployment in complex, dynamic environments. Experimental results demonstrate
significant improvements over traditional static keyframe selection strategies,
such as fixed temporal intervals or uniform frame skipping. These findings
highlight a meaningful advancement toward adaptive perception systems that can
dynamically respond to complex and evolving visual scenes. We evaluate our
proposed adaptive keyframe selection module on two recent state-of-the-art 3D
reconstruction networks, Spann3r and CUT3R, and observe consistent improvements
in reconstruction quality across both frameworks. Furthermore, an extensive
ablation study confirms the effectiveness of each individual component in our
method, underlining their contribution to the overall performance gains.

</details>


### [144] [A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons](https://arxiv.org/abs/2510.23954)
*Pejman Kheradmand,Behnam Moradkhani,Raghavasimhan Sankaranarayanan,Kent K. Yamamoto,Tanner J. Zachem,Patrick J. Codd,Yash Chitalia,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出了一种基于Cosserat杆的通用模型，用于肌腱驱动同心管机器人，解决了现有设计的限制，实验验证了其高精度和适用性。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动同心管机制结合了肌腱驱动连续体机器人和同心管机器人的优势，同时解决了它们各自的限制，如自由度受限和不稳定问题。

Method: 提出了一种基于Cosserat杆的框架，用于建模n个同心管（每个管由mi肌腱驱动）的一般情况，允许每个管在弯曲时扭转和伸长，同时强制共享中心线。

Result: 通过实验验证，实现了机器人总长度<4%的尖端预测误差，并在现有机器人应用中最大尖端偏差保持在总长度的5%左右。

Conclusion: 该模型为精确的形状估计和高级肌腱驱动同心管机器人的控制提供了基础。

Abstract: Tendon-actuated concentric tube mechanisms combine the advantages of
tendon-driven continuum robots and concentric tube robots while addressing
their respective limitations. They overcome the restricted degrees of freedom
often seen in tendon-driven designs, and mitigate issues such as snapping
instability associated with concentric tube robots. However, a complete and
general mechanical model for these systems remains an open problem. In this
work, we propose a Cosserat rod-based framework for modeling the general case
of $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \{1,
\ldots, n\}$. The model allows each tube to twist and elongate while enforcing
a shared centerline for bending. We validate the proposed framework through
experiments with two-tube and three tube assemblies under various tendon
routing configurations, achieving tip prediction errors $<4\%$ of the robot's
total length. We further demonstrate the model's generality by applying it to
existing robots in the field, where maximum tip deviations remain around $5\%$
of the total length. This model provides a foundation for accurate shape
estimation and control of advanced tendon-actuated concentric tube robots.

</details>


### [145] [Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping](https://arxiv.org/abs/2510.23963)
*Hiroki Ishikawa,Kyosuke Ishibashi,Ko Yamamoto*

Main category: cs.RO

TL;DR: 软机器人手指通过自适应扭曲变形和可变刚度机制，实现对密集物体中单个物体的高效抓取，实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 为实现在密集堆放的物体中抓取单个物体，软手指需要具备平面内外的自适应扭曲变形功能。

Method: 采用可变刚度机制，通过单一驱动源实现自适应扭曲变形和抓取力控制，并结合有限元分析（FEA）优化设计参数。

Result: 实验展示了软手指在多种物体上的抓取能力，验证了设计的有效性。

Conclusion: 本文提出的软机器人手指通过自适应扭曲变形实现了对物体的包裹抓取，实验验证了其在不同物体上的抓取效果。

Abstract: This paper presents a soft robot finger capable of adaptive-twist deformation
to grasp objects by wrapping them. For a soft hand to grasp and pick-up one
object from densely contained multiple objects, a soft finger requires the
adaptive-twist deformation function in both in-plane and out-of-plane
directions. The function allows the finger to be inserted deeply into a limited
gap among objects. Once inserted, the soft finger requires appropriate control
of grasping force normal to contact surface, thereby maintaining the twisted
deformation. In this paper, we refer to this type of grasping as grasping by
wrapping. To achieve these two functions by a single actuation source, we
propose a variable stiffness mechanism that can adaptively change the stiffness
as the pressure is higher. We conduct a finite element analysis (FEA) on the
proposed mechanism and determine its design parameter based on the FEA result.
Using the developed soft finger, we report basic experimental results and
demonstrations on grasping various objects.

</details>


### [146] [A Survey on Collaborative SLAM with 3D Gaussian Splatting](https://arxiv.org/abs/2510.23988)
*Phuc Nguyen Xuan,Thanh Nguyen Canh,Huu-Hung Nguyen,Nak Young Chong,Xiem HoangVan*

Main category: cs.RO

TL;DR: 该综述全面回顾了基于3D高斯喷绘的多机器人协作SLAM技术，分析了挑战与解决方案，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯喷绘（3DGS）作为显式场景表示技术，为机器人学提供了实时高保真渲染的可能性，但在多机器人系统中应用时面临全局一致性、通信管理和异构数据融合等挑战。

Method: 通过系统性分类方法，分析了集中式和分布式架构，并探讨了多智能体一致性、通信效率、高斯表示、语义蒸馏、融合与位姿优化以及实时可扩展性等核心组件。

Result: 综述总结了关键数据集和评估指标，以量化性能，并分析了不同架构在多机器人协作SLAM中的表现。

Conclusion: 该综述指出了多机器人协作SLAM领域的开放挑战，并提出了未来研究方向，如终身映射、语义关联与映射、多模型鲁棒性以及缩小仿真与现实的差距。

Abstract: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity rendering, ideal for robotics. However,
its use in multi-robot systems introduces significant challenges in maintaining
global consistency, managing communication, and fusing data from heterogeneous
sources. We systematically categorize approaches by their architecture --
centralized, distributed -- and analyze core components like multi-agent
consistency and alignment, communication-efficient, Gaussian representation,
semantic distillation, fusion and pose optimization, and real-time scalability.
In addition, a summary of critical datasets and evaluation metrics is provided
to contextualize performance. Finally, we identify key open challenges and
chart future research directions, including lifelong mapping, semantic
association and mapping, multi-model for robustness, and bridging the Sim2Real
gap.

</details>


### [147] [VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion](https://arxiv.org/abs/2510.23997)
*Stanley Wu,Mohamad H. Danesh,Simon Li,Hanna Yurchyk,Amin Abyaneh,Anas El Houssaini,David Meger,Hsiu-Chin Lin*

Main category: cs.RO

TL;DR: VOCALoco框架通过模块化策略选择提升腿式机器人在复杂地形中的安全性和能效。


<details>
  <summary>Details</summary>
Motivation: 现有端到端深度强化学习方法在安全性和可解释性上存在局限，尤其是在新地形泛化时。

Method: VOCALoco是一个模块化技能选择框架，通过预测执行安全性和运输成本来动态选择预训练的运动策略。

Result: 在仿真和现实场景中，VOCALoco在楼梯行走任务中表现优于传统端到端DRL策略。

Conclusion: VOCALoco框架通过动态选择预训练策略，在楼梯行走任务中提升了腿式机器人的鲁棒性和安全性。

Abstract: Recent advancements in legged robot locomotion have facilitated traversal
over increasingly complex terrains. Despite this progress, many existing
approaches rely on end-to-end deep reinforcement learning (DRL), which poses
limitations in terms of safety and interpretability, especially when
generalizing to novel terrains. To overcome these challenges, we introduce
VOCALoco, a modular skill-selection framework that dynamically adapts
locomotion strategies based on perceptual input. Given a set of pre-trained
locomotion policies, VOCALoco evaluates their viability and energy-consumption
by predicting both the safety of execution and the anticipated cost of
transport over a fixed planning horizon. This joint assessment enables the
selection of policies that are both safe and energy-efficient, given the
observed local terrain. We evaluate our approach on staircase locomotion tasks,
demonstrating its performance in both simulated and real-world scenarios using
a quadrupedal robot. Empirical results show that VOCALoco achieves improved
robustness and safety during stair ascent and descent compared to a
conventional end-to-end DRL policy

</details>


### [148] [Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model](https://arxiv.org/abs/2510.24029)
*Andrew Gerstenslager,Bekarys Dukenbaev,Ali A. Minai*

Main category: cs.RO

TL;DR: A 3D BVC model with vertical angular sensitivity improves spatial localization in complex environments while maintaining 2D performance in simpler cases.


<details>
  <summary>Details</summary>
Motivation: Most computational BVC models are limited to 2D environments, leading to spatial ambiguities in horizontally symmetric environments.

Method: The proposed model incorporates vertical angular sensitivity into the BVC framework and processes LiDAR data to capture vertical contours.

Result: The 3D model matches 2D performance in minimal vertical variation environments and outperforms it in complex 3D scenarios with more distinct place fields and reduced spatial aliasing.

Conclusion: Adding a vertical dimension to BVC-based localization enhances navigation and mapping in 3D spaces while maintaining performance in simpler, 2D scenarios.

Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of
vertebrates that encode environmental boundaries at specific distances and
allocentric directions, playing a central role in forming place fields in the
hippocampus. Most computational BVC models are restricted to two-dimensional
(2D) environments, making them prone to spatial ambiguities in the presence of
horizontal symmetries in the environment. To address this limitation, we
incorporate vertical angular sensitivity into the BVC framework, thereby
enabling robust boundary detection in three dimensions, and leading to
significantly more accurate spatial localization in a biologically-inspired
robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby
disambiguating locations that would be indistinguishable under a purely 2D
representation. Experimental results show that in environments with minimal
vertical variation, the proposed 3D model matches the performance of a 2D
baseline; yet, as 3D complexity increases, it yields substantially more
distinct place fields and markedly reduces spatial aliasing. These findings
show that adding a vertical dimension to BVC-based localization can
significantly enhance navigation and mapping in real-world 3D spaces while
retaining performance parity in simpler, near-planar scenarios.

</details>


### [149] [SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration](https://arxiv.org/abs/2510.24052)
*Jongsuk Kim,Jaeyoung Lee,Gyojin Han,Dongjae Lee,Minki Jeong,Junmo Kim*

Main category: cs.RO

TL;DR: SynAD是首个利用合成数据增强真实端到端自动驾驶模型的框架，通过指定多智能体场景中的ego车辆、地图投影和新网络获取鸟瞰图特征，结合真实数据训练，显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和高质量真实驾驶数据集的进步推动了端到端自动驾驶的发展，但仅依赖真实数据限制了训练场景的多样性。合成场景生成虽被视为丰富训练数据多样性的有前景的解决方案，但其在端到端自动驾驶模型中的应用尚未充分探索，主要原因是缺乏指定的ego车辆及传感器输入。

Method: SynAD框架通过在多智能体合成场景中指定具有最全面驾驶信息的智能体作为ego车辆，将路径级场景投影到地图上，并利用新开发的Map-to-BEV网络在不依赖传感器输入的情况下获取鸟瞰图特征，最后设计了一种训练策略将这些基于地图的合成数据与真实驾驶数据有效整合。

Result: 实验结果表明，SynAD有效整合了所有组件，并显著提升了安全性性能。

Conclusion: SynAD成功地将合成场景生成与端到端自动驾驶模型结合，显著提升了安全性性能，为更全面和鲁棒的自动驾驶模型铺平了道路。

Abstract: Recent advancements in deep learning and the availability of high-quality
real-world driving datasets have propelled end-to-end autonomous driving.
Despite this progress, relying solely on real-world data limits the variety of
driving scenarios for training. Synthetic scenario generation has emerged as a
promising solution to enrich the diversity of training data; however, its
application within E2E AD models remains largely unexplored. This is primarily
due to the absence of a designated ego vehicle and the associated sensor
inputs, such as camera or LiDAR, typically provided in real-world scenarios. To
address this gap, we introduce SynAD, the first framework designed to enhance
real-world E2E AD models using synthetic data. Our method designates the agent
with the most comprehensive driving information as the ego vehicle in a
multi-agent synthetic scenario. We further project path-level scenarios onto
maps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view
features without relying on sensor inputs. Finally, we devise a training
strategy that effectively integrates these map-based synthetic data with real
driving data. Experimental results demonstrate that SynAD effectively
integrates all components and notably enhances safety performance. By bridging
synthetic scenario generation and E2E AD, SynAD paves the way for more
comprehensive and robust autonomous driving models.

</details>


### [150] [Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation](https://arxiv.org/abs/2510.24055)
*Xiucheng Zhang,Yang Jiang,Hongwei Qing,Jiashuo Bai*

Main category: cs.RO

TL;DR: 通过语言条件视觉表示和混合专家策略，解决了机器人多任务操作中的感知模糊性和任务冲突，显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 感知模糊性和任务冲突限制了通过模仿学习实现的多任务机器人操作。

Method: 提出了一个结合语言条件视觉表示（LCVR）模块和语言条件混合专家密度策略（LMoE-DP）的框架。LCVR通过语言指令将视觉特征基础化，以区分视觉相似的任务；LMoE-DP则通过稀疏专家架构专注于不同的多模态动作分布，并通过梯度调制稳定。

Result: 在真实机器人基准测试中，LCVR将Action Chunking with Transformers（ACT）和Diffusion Policy（DP）的成功率分别提高了33.75%和25%。完整框架实现了79%的平均成功率，比先进基线高出21%。

Conclusion: 结合语义基础和专家专业化，能够实现稳健、高效的多任务机器人操作。

Abstract: Perceptual ambiguity and task conflict limit multitask robotic manipulation
via imitation learning. We propose a framework combining a Language-Conditioned
Visual Representation (LCVR) module and a Language-conditioned
Mixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual
ambiguities by grounding visual features with language instructions, enabling
differentiation between visually similar tasks. To mitigate task conflict,
LMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal
action distributions, stabilized by gradient modulation. On real-robot
benchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion
Policy (DP) success rates by 33.75% and 25%, respectively. The full framework
achieves a 79% average success, outperforming the advanced baseline by 21%. Our
work shows that combining semantic grounding and expert specialization enables
robust, efficient multi-task manipulation

</details>


### [151] [Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition](https://arxiv.org/abs/2510.24067)
*Tianyi Ding,Ronghao Zheng,Senlin Zhang,Meiqin Liu*

Main category: cs.RO

TL;DR: 提出新拓扑地图和Voronoi算法，提升多机器人在复杂环境中的探索效率和平衡性。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在障碍密集非凸环境中的协同自主在线探索问题，特别是动态平衡的探索区域划分和任务分配。

Method: 采用增量更新的拓扑地图结构，结合分布式加权拓扑图Voronoi算法，实现平衡的图空间分区和任务分配。局部规划器优化探索目标的访问序列，生成安全、平滑且动态可行的运动轨迹。

Result: 综合基准测试表明，与现有方法相比，在探索效率、完整性和机器人团队的工作负载平衡方面有显著提升。

Conclusion: 本文提出了一种新颖的拓扑地图结构和分布式加权拓扑图Voronoi算法，显著提高了多机器人团队在障碍密集非凸环境中的探索效率、完整性和工作负载平衡。

Abstract: This work addresses the collaborative multi-robot autonomous online
exploration problem, particularly focusing on distributed exploration planning
for dynamically balanced exploration area partition and task allocation among a
team of mobile robots operating in obstacle-dense non-convex environments.
  We present a novel topological map structure that simultaneously
characterizes both spatial connectivity and global exploration completeness of
the environment. The topological map is updated incrementally to utilize known
spatial information for updating reachable spaces, while exploration targets
are planned in a receding horizon fashion under global coverage guidance.
  A distributed weighted topological graph Voronoi algorithm is introduced
implementing balanced graph space partitions of the fused topological maps.
Theoretical guarantees are provided for distributed consensus convergence and
equitable graph space partitions with constant bounds.
  A local planner optimizes the visitation sequence of exploration targets
within the balanced partitioned graph space to minimize travel distance, while
generating safe, smooth, and dynamically feasible motion trajectories.
  Comprehensive benchmarking against state-of-the-art methods demonstrates
significant improvements in exploration efficiency, completeness, and workload
balance across the robot team.

</details>


### [152] [Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition](https://arxiv.org/abs/2510.24069)
*Sangmin Kim,Hajun Kim,Gijeong Kim,Min-Gyu Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 提出基于相位的轨迹优化方法，利用线性微分方程和B{\'e}zier多项式确保平移动力学和摩擦锥约束，生成可靠腿式机器人运动。


<details>
  <summary>Details</summary>
Motivation: 为腿式机器人通过轨迹优化生成可靠运动，需同时计算路径和接触序列，并准确考虑动力学问题。

Method: 利用线性微分方程的叠加特性解耦各接触点的平移动力学，结合B{\'e}zier多项式的微分矩阵和凸包性质，确保平移动力学和摩擦锥约束的一致性满足。

Result: 框架成功生成动态可靠的运动，支持多种步态序列，并通过四足机器人模型验证了可行性和有效性。

Conclusion: 提出的轨迹优化框架能够为腿式机器人生成动态可靠的运动，支持多种步态序列，并通过四足机器人模型验证了动力学和运动生成的可行性。

Abstract: To generate reliable motion for legged robots through trajectory
optimization, it is crucial to simultaneously compute the robot's path and
contact sequence, as well as accurately consider the dynamics in the problem
formulation. In this paper, we present a phase-based trajectory optimization
that ensures the feasibility of translational dynamics and friction cone
constraints throughout the entire trajectory. Specifically, our approach
leverages the superposition properties of linear differential equations to
decouple the translational dynamics for each contact point, which operates
under different phase sequences. Furthermore, we utilize the differentiation
matrix of B{\'e}zier polynomials to derive an analytical relationship between
the robot's position and force, thereby ensuring the consistent satisfaction of
translational dynamics. Additionally, by exploiting the convex closure property
of B{\'e}zier polynomials, our method ensures compliance with friction cone
constraints. Using the aforementioned approach, the proposed trajectory
optimization framework can generate dynamically reliable motions with various
gait sequences for legged robots. We validate our framework using a quadruped
robot model, focusing on the feasibility of dynamics and motion generation.

</details>


### [153] [ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring](https://arxiv.org/abs/2510.24108)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Jingde Chen,Nadine Chang,Maying Shen,Jingyu Song,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: ZTRS是一种无需模仿学习的端到端自动驾驶框架，结合传感器输入和强化学习，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有框架主要依赖模仿学习，但受限于专家演示的质量和部署时的协变量偏移；强化学习虽可扩展，但通常局限于低维符号输入。ZTRS旨在结合两者的优势，直接从传感器输入进行端到端学习。

Method: ZTRS采用离线强化学习，并结合提出的Exhaustive Policy Optimization (EPO)方法，针对可枚举的动作和奖励进行优化。

Result: ZTRS在Navtest、Navhard和HUGSIM三个基准测试中表现优异，尤其在Navhard上达到最新技术水平，并在HUGSIM上优于基于模仿学习的基线方法。

Conclusion: ZTRS框架通过结合传感器输入和强化学习的优势，实现了端到端自动驾驶，且在多个基准测试中表现优异，尤其在Navhard基准上达到了最新技术水平。

Abstract: End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

</details>


### [154] [PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI](https://arxiv.org/abs/2510.24109)
*Wenbin Ding,Jun Chen,Mingjia Chen,Fei Xie,Qi Mao,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了一种基于VLM的机器人代理框架，显著提高了复杂自然语言指令任务的执行成功率，实验结果显示其比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，人类中心人工智能（HAI）对机器人智能水平提出了更高要求，尤其是在自然语言交互和复杂任务规划与执行方面。现有基于LLM的实体代理往往缺乏在线规划和执行复杂自然语言控制任务的能力。

Method: 提出了一种新型机器人代理框架，包含人机语音交互模块、视觉语言代理模块和动作执行模块。视觉语言代理模块进一步细分为基于视觉的任务规划器、自然语言指令转换器和任务表现反馈评估器。

Result: 实验证明，该代理框架在模拟和真实环境中平均任务成功率比传统LLM+CLIP方法高出28%。

Conclusion: 本文提出的基于视觉语言模型（VLM）的机器人代理框架显著提高了复杂自然语言指令任务的执行成功率，实验结果表明其在模拟和真实环境中平均任务成功率比传统LLM+CLIP方法高出28%。

Abstract: The rapid advancement of Large Language Models (LLMs) has marked a
significant breakthrough in Artificial Intelligence (AI), ushering in a new era
of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
welfare and needs, thereby placing higher demands on the intelligence level of
robots, particularly in aspects such as natural language interaction, complex
task planning, and execution. Intelligent agents powered by LLMs have opened up
new pathways for realizing HAI. However, existing LLM-based embodied agents
often lack the ability to plan and execute complex natural language control
tasks online. This paper explores the implementation of intelligent robotic
manipulating agents based on Vision-Language Models (VLMs) in the physical
world. We propose a novel embodied agent framework for robots, which comprises
a human-robot voice interaction module, a vision-language agent module and an
action execution module. The vision-language agent itself includes a
vision-based task planner, a natural language instruction converter, and a task
performance feedback evaluator. Experimental results demonstrate that our agent
achieves a 28\% higher average task success rate in both simulated and real
environments compared to approaches relying solely on LLM+CLIP, significantly
improving the execution success rate of high-level natural language instruction
tasks.

</details>


### [155] [LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation](https://arxiv.org/abs/2510.24118)
*Haotian Zhou,Xiaole Wang,He Li,Fusheng Sun,Shengyu Guo,Guolei Qi,Jianghuan Xu,Huijing Zhao*

Main category: cs.RO

TL;DR: LagMemo是一种基于语言3D高斯泼溅记忆的导航系统，支持多模态开放词汇多目标视觉导航，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态、开放词汇目标查询和多目标视觉导航的实际需求，克服传统方法在单目标、单模态和封闭集目标设置上的限制。

Method: LagMemo构建统一的3D语言记忆，通过任务目标查询记忆、预测候选目标位置，并整合局部感知验证机制动态匹配和验证目标。

Result: 实验结果表明，LagMemo在多模态开放词汇目标定位和多目标视觉导航任务中表现优异。

Conclusion: LagMemo导航系统通过其语言3D高斯泼溅记忆模块，在多模态开放词汇多目标视觉导航任务中表现出色，优于现有最先进方法。

Abstract: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo

</details>


### [156] [Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames](https://arxiv.org/abs/2510.24194)
*Ev Zisselman,Mirco Mutti,Shelly Francis-Meretzki,Elisei Shafer,Aviv Tamar*

Main category: cs.RO

TL;DR: 论文提出了一种‘盲人专家’行为克隆方法，通过隐藏任务信息使专家进行非平凡探索，实验证明其在泛化性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统行为克隆依赖专家全信息演示，泛化性受限，论文旨在探索通过限制专家信息提升泛化能力的方法。

Method: 设计‘盲人专家’实验，隐藏部分任务信息迫使专家进行探索，结合真实机器人任务和Procgen基准游戏进行验证。

Result: 实验显示盲人专家克隆在未见过任务上表现更优，理论分析支持其泛化误差与√(I/m)成正比。

Conclusion: 盲人专家行为克隆方法在泛化性和任务数量需求上均优于传统方法，理论与实践一致。

Abstract: Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home

</details>


### [157] [Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors](https://arxiv.org/abs/2510.24257)
*Ziqi Ma,Changda Tian,Yue Gao*

Main category: cs.RO

TL;DR: HMAMP通过对抗网络学习人类风格的操作技能，在锤击任务中表现优异，展示了实际应用的潜力，推动了机器人更自然互动的发展。


<details>
  <summary>Details</summary>
Motivation: 开发能够以更自然、直观方式与人类互动的机器人和自主系统，关键挑战之一是让这些系统以类似人类的方式操作物体和工具。

Method: 该方法利用对抗网络建模工具和物体操作的复杂动态以及操作任务的目标，训练一个策略生成与人类运动统计特性匹配的逼真运动轨迹。

Result: HMAMP在锤击任务中表现出色，能够学习人类风格的操作技能，并超越当前基线方法。此外，通过真实机器人手臂锤击任务展示了其在实际应用中的潜力。

Conclusion: HMAMP代表了一个重要步骤，通过学习和模仿人类的工具和物体操作方式，使机器人和自主系统能够以更自然、直观的方式与人类互动。

Abstract: In recent years, there has been growing interest in developing robots and
autonomous systems that can interact with human in a more natural and intuitive
way. One of the key challenges in achieving this goal is to enable these
systems to manipulate objects and tools in a manner that is similar to that of
humans. In this paper, we propose a novel approach for learning human-style
manipulation skills by using adversarial motion priors, which we name HMAMP.
The approach leverages adversarial networks to model the complex dynamics of
tool and object manipulation, as well as the aim of the manipulation task. The
discriminator is trained using a combination of real-world data and simulation
data executed by the agent, which is designed to train a policy that generates
realistic motion trajectories that match the statistical properties of human
motion. We evaluated HMAMP on one challenging manipulation task: hammering, and
the results indicate that HMAMP is capable of learning human-style manipulation
skills that outperform current baseline methods. Additionally, we demonstrate
that HMAMP has potential for real-world applications by performing real robot
arm hammering tasks. In general, HMAMP represents a significant step towards
developing robots and autonomous systems that can interact with humans in a
more natural and intuitive way, by learning to manipulate tools and objects in
a manner similar to how humans do.

</details>


### [158] [DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation](https://arxiv.org/abs/2510.24261)
*Jingyi Tian,Le Wang,Sanping Zhou,Sen Wang,Jiayi Li,Gang Hua*

Main category: cs.RO

TL;DR: DynaRend通过3D感知和动态信息的联合学习，提升了机器人操作策略的泛化能力和现实应用效果。


<details>
  <summary>Details</summary>
Motivation: 解决由于现实世界训练数据稀缺导致的机器人操作策略泛化能力不足的问题，现有方法未能联合学习几何、语义和动态信息。

Method: DynaRend是一个通过可微分体积渲染进行掩码重建和未来预测的表示学习框架，利用多视角RGB-D视频数据进行预训练。

Result: 在RLBench和Colosseum基准测试及现实实验中，DynaRend在策略成功率、环境扰动泛化和现实适用性方面表现优异。

Conclusion: DynaRend通过联合学习3D感知和动态信息的triplane特征，显著提升了机器人操作策略的成功率、环境扰动的泛化能力以及现实世界应用的多样性。

Abstract: Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.

</details>


### [159] [Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation](https://arxiv.org/abs/2510.24315)
*Baozhe Zhang,Xinwei Chen,Qingcheng Chen,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出CoNi-OA算法，利用单帧LiDAR数据实现无人机-无人地面车辆协同避障，无需全局状态估计或障碍物预测，高效且安全。


<details>
  <summary>Details</summary>
Motivation: CoNi-MPC框架在无人机控制中虽高效，但因缺乏环境信息而难以应对避障挑战。

Method: 提出了一种基于调制的避障算法CoNi-OA，利用单帧LiDAR数据生成调制矩阵，直接调整无人机速度以实现避障。

Result: CoNi-OA算法能在非惯性坐标系下实时生成无碰撞轨迹，计算需求低（每次迭代小于5毫秒），适用于动态和不可预测环境。

Conclusion: CoNi-OA成功解决了CoNi-MPC在无人机-无人地面车辆协同任务中缺乏环境信息的问题，通过利用单帧LiDAR数据实时生成调制矩阵，实现了高效且安全的避障。

Abstract: CoNi-MPC provides an efficient framework for UAV control in air-ground
cooperative tasks by relying exclusively on relative states, eliminating the
need for global state estimation. However, its lack of environmental
information poses significant challenges for obstacle avoidance. To address
this issue, we propose a novel obstacle avoidance algorithm, Cooperative
Non-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for
UAV-UGV cooperative scenarios without reliance on global state estimation or
obstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data
from the UAV to generate a modulation matrix, which directly adjusts the
quadrotor's velocity to achieve obstacle avoidance. This modulation-based
method enables real-time generation of collision-free trajectories within the
UGV's non-inertial frame, significantly reducing computational demands (less
than 5 ms per iteration) while maintaining safety in dynamic and unpredictable
environments. The key contributions of this work include: (1) a
modulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV
cooperation in non-inertial frames without global states; (2) rapid, real-time
trajectory generation based solely on single-frame LiDAR data, removing the
need for obstacle modeling or prediction; and (3) adaptability to both static
and dynamic environments, thus extending applicability to featureless or
unknown scenarios.

</details>


### [160] [NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation](https://arxiv.org/abs/2510.24335)
*Mingyu Jeong,Eunsung Kim,Sehun Park,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: NVSim利用改进的3D Gaussian Splatting和无网格算法，从图像序列自动构建可导航的室内模拟器，解决了传统3D扫描的成本和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法在构建大规模、可导航的室内模拟器时面临高成本和可扩展性差的问题，NVSim旨在通过仅使用普通图像序列来克服这些限制。

Method: 采用3D Gaussian Splatting技术，并引入Floor-Aware Gaussian Splatting来优化稀疏观察地板上的视觉伪影，提出了一种新型的无网格遍历性检查算法，通过直接分析渲染视图构建拓扑图。

Result: NVSim能够从真实世界数据生成有效的大规模导航图，证明了其在实际应用中的可行性。

Conclusion: NVSim框架通过创新的Floor-Aware Gaussian Splatting技术和无网格遍历性检查算法，成功解决了传统3D扫描在成本和可扩展性上的限制，能够从普通图像序列自动构建大规模、可导航的室内模拟器。

Abstract: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

</details>


### [161] [Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance](https://arxiv.org/abs/2510.24457)
*Jorge Vicente-Martinez,Edgar Ramirez-Laboreo*

Main category: cs.RO

TL;DR: 本文提出了一种基于微分平坦性的3D桥式起重机最优轨迹生成方法，通过直接纳入非线性摩擦和碰撞避免等约束，实现了激进行动中的安全操作。


<details>
  <summary>Details</summary>
Motivation: 解决3D桥式起重机在激进行动中面临的非线性摩擦和碰撞避免问题。

Method: 基于微分平坦性的最优轨迹生成方法，直接纳入复杂物理和动态约束。

Result: 比较模拟研究表明，忽略干摩擦会导致执行器饱和和碰撞。

Conclusion: 摩擦建模是实现快速且安全起重机轨迹的基本要求。

Abstract: This paper presents an optimal trajectory generation method for 3D overhead
cranes by leveraging differential flatness. This framework enables the direct
inclusion of complex physical and dynamic constraints, such as nonlinear
friction and collision avoidance for both payload and rope. Our approach allows
for aggressive movements by constraining payload swing only at the final point.
A comparative simulation study validates our approach, demonstrating that
neglecting dry friction leads to actuator saturation and collisions. The
results show that friction modeling is a fundamental requirement for fast and
safe crane trajectories.

</details>


### [162] [Supervisory Measurement-Guided Noise Covariance Estimation](https://arxiv.org/abs/2510.24508)
*Haoying Li,Yifan Peng,Junfeng Wu*

Main category: cs.RO

TL;DR: 论文提出了一种双层优化方法，用于高效估计传感器噪声协方差，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传感器噪声协方差的准确估计对可靠的状态估计至关重要，但实际中由于环境变化、前端预处理等因素，协方差难以识别。

Method: 通过将噪声协方差估计问题转化为双层优化问题，并利用贝叶斯视角下的联合似然分解，实现了计算效率与信息利用的平衡。下层使用不变扩展卡尔曼滤波和状态增广估计轨迹，上层通过梯度更新优化协方差。

Result: 在合成和真实数据集上的实验表明，该方法在效率上优于现有基线。

Conclusion: 该论文提出了一种通过双层优化框架来估计传感器噪声协方差的方法，该方法在实验数据上显示出比现有基线更高的效率。

Abstract: Reliable state estimation hinges on accurate specification of sensor noise
covariances, which weigh heterogeneous measurements. In practice, these
covariances are difficult to identify due to environmental variability,
front-end preprocessing, and other reasons. We address this by formulating
noise covariance estimation as a bilevel optimization that, from a Bayesian
perspective, factorizes the joint likelihood of so-called odometry and
supervisory measurements, thereby balancing information utilization with
computational efficiency. The factorization converts the nested Bayesian
dependency into a chain structure, enabling efficient parallel computation: at
the lower level, an invariant extended Kalman filter with state augmentation
estimates trajectories, while a derivative filter computes analytical gradients
in parallel for upper-level gradient updates. The upper level refines the
covariance to guide the lower-level estimation. Experiments on synthetic and
real-world datasets show that our method achieves higher efficiency over
existing baselines.

</details>


### [163] [Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems](https://arxiv.org/abs/2510.24515)
*Malintha Fernando,Petter Ögren,Silun Zhang*

Main category: cs.RO

TL;DR: SPCG扩展TOP以处理自利机器人在奖励稀缺环境中的竞争。ORS和FORL算法提升学习效率和泛化能力，策略接近最优解。


<details>
  <summary>Details</summary>
Motivation: 现有的TOP变种假设机器人合作追求单一目标，无法适用于自利机器人在奖励稀缺环境中的竞争场景。

Method: 提出了两种算法：ORS（获取‘序数排名’）和FORL（针对高排名对手学习最佳响应策略）。在路网和合成图上进行了实证评估。

Result: ORS通过OR条件减少状态混淆，提升大规模团队策略学习效率；FORL训练的策略在奖励分布不均时泛化能力更强。学习策略在SPCG中接近TOP最优解。

Conclusion: SPCG扩展了TOP，适用于自利机器人在奖励稀缺环境中的规划。通过ORS和FORL算法，团队规模扩大时策略学习效率更高，且在奖励分布不均时泛化能力更强。学习策略在SPCG中达到87%-95%的最优性。

Abstract: The Team Orienteering Problem (TOP) generalizes many real-world multi-robot
scheduling and routing tasks that occur in autonomous mobility, aerial
logistics, and surveillance applications. While many flavors of the TOP exist
for planning in multi-robot systems, they assume that all the robots cooperate
toward a single objective; thus, they do not extend to settings where the
robots compete in reward-scarce environments. We propose Stochastic
Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the
presence of self-interested robots operating on a graph, under energy
constraints and stochastic transitions. A theoretical study on complete and
star graphs establishes that there is a unique pure Nash equilibrium in SPCGs
that coincides with the optimal routing solution of an equivalent TOP given a
rank-based conflict resolution rule. This work proposes two algorithms: Ordinal
Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in
temporarily-formed local neighborhoods during the games' stages, and Fictitious
Ordinal Response Learning (FORL) to obtain best-response policies against one's
senior-rank opponents. Empirical evaluations conducted on road networks and
synthetic graphs under both dynamic and stationary prize distributions show
that 1) the state-aliasing induced by OR-conditioning enables learning policies
that scale more efficiently to large team sizes than those trained with the
global index, and 2) Policies trained with FORL generalize better to imbalanced
prize distributions than those with other multi-agent training methods.
Finally, the learned policies in the SPCG achieved between 87% and 95%
optimality compared to an equivalent TOP solution obtained by mixed-integer
linear programming.

</details>


### [164] [GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots](https://arxiv.org/abs/2510.24533)
*Yuan Shen,Yuze Hong,Guangyang Zeng,Tengfei Zhang,Pui Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: GeVI-SLAM是一种重力增强的立体视觉惯性SLAM系统，通过4-DOF PnP估计器和IMU协方差联合估计，显著提升了水下机器人在视觉退化和低加速度动态下的定位与建图性能。


<details>
  <summary>Details</summary>
Motivation: 水下机器人视觉惯性SLAM面临视觉退化和IMU运动激励不足的挑战，现有方法在低加速度动态下表现不稳定。

Method: 利用立体相机的直接深度估计能力，消除IMU初始化时的尺度估计需求；通过精确的重力初始化，解耦俯仰和横滚角，采用4-DOF PnP问题求解姿态跟踪；提出偏差消除的4-DOF PnP估计器，并联合估计IMU协方差以处理动态运动。

Result: 在模拟和真实数据上的实验表明，GeVI-SLAM相比现有方法具有更高的准确性和稳定性。

Conclusion: GeVI-SLAM 通过重力增强和4-DOF PnP估计器，显著提高了水下机器人视觉惯性SLAM的准确性和稳定性，在低加速度动态和视觉退化情况下表现优异。

Abstract: Accurate visual inertial simultaneous localization and mapping (VI SLAM) for
underwater robots remains a significant challenge due to frequent visual
degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system
designed to address these issues. By leveraging the stereo camera's direct
depth estimation ability, we eliminate the need to estimate scale during IMU
initialization, enabling stable operation even under low acceleration dynamics.
With precise gravity initialization, we decouple the pitch and roll from the
pose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point
(PnP) problem for pose tracking. This allows the use of a minimal 3-point
solver, which significantly reduces computational time to reject outliers
within a Random Sample Consensus framework. We further propose a
bias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the
relative pose converges to the true value as the feature number increases. To
handle dynamic motion, we refine the full 6-DOF pose while jointly estimating
the IMU covariance, enabling adaptive weighting of the gravity prior. Extensive
experiments on simulated and real-world data demonstrate that GeVI-SLAM
achieves higher accuracy and greater stability compared to state-of-the-art
methods.

</details>


### [165] [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](https://arxiv.org/abs/2510.24554)
*Vignesh Kottayam Viswanathan,Yifan Bai,Scott Fredriksson,Sumeet Satpute,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 该论文提出了一种分层框架，用于在环境不确定的情况下支持机器人检查，通过全局规划和局部重新规划相结合，实现了鲁棒的任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于已知环境模型来规划和跟踪检查路线，但模型与实际现场条件之间的差异（由自然或人类活动引起）可能导致表面形态变化或路径阻塞。为了解决这一问题，提出了这种分层框架。

Method: 框架将检查任务分为两部分：基于历史地图生成初始全局视图计划，以及根据当前场景形态进行局部视图重新规划。这种方法保留了全局覆盖目标，同时允许对局部形态变化做出反应。

Result: 通过在真实地下矿井中使用四足机器人进行部署验证，证明了该方法的有效性。

Conclusion: 该研究提出了一种分层框架，旨在在环境不确定的情况下支持机器人检查任务。该框架通过结合全局规划和局部重新规划，实现了在环境变化时的鲁棒性和任务完成能力。

Abstract: In this work, we present a hierarchical framework designed to support robotic
inspection under environment uncertainty. By leveraging a known environment
model, existing methods plan and safely track inspection routes to visit points
of interest. However, discrepancies between the model and actual site
conditions, caused by either natural or human activities, can alter the surface
morphology or introduce path obstructions. To address this challenge, the
proposed framework divides the inspection task into: (a) generating the initial
global view-plan for region of interests based on a historical map and (b)
local view replanning to adapt to the current morphology of the inspection
scene. The proposed hierarchy preserves global coverage objectives while
enabling reactive adaptation to the local surface morphology. This enables the
local autonomy to remain robust against environment uncertainty and complete
the inspection tasks. We validate the approach through deployments in
real-world subterranean mines using quadrupedal robot.

</details>


### [166] [Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots](https://arxiv.org/abs/2510.24571)
*Hongxu Zhao,Guangyang Zeng,Yunling Shao,Tengfei Zhang,Junfeng Wu*

Main category: cs.RO

TL;DR: 论文提出UIC框架，通过MAP和GP先验解决水下SLAM系统中传感器校准问题，适用于多种传感器，并通过测试验证。


<details>
  <summary>Details</summary>
Motivation: 现有DVL校准方法局限于特定传感器配置或依赖过度简化假设，且未联合估计平移外参和时间偏移，因此需要一种更通用的校准方法。

Method: 提出了统一迭代校准（UIC）框架，采用最大后验（MAP）估计和高斯过程（GP）运动先验进行高保真运动插值，交替进行高效的GP运动状态更新和基于梯度的校准变量更新。

Result: UIC框架在仿真和实际测试中表现优异，适用于IMU、相机等多种传感器，并发布了开源DVL-相机校准工具箱。

Conclusion: 该论文提出的UIC框架不仅适用于水下SLAM系统，还可广泛应用于其他多传感器校准问题，并通过仿真和实际测试验证了其有效性。

Abstract: The calibration of extrinsic parameters and clock offsets between sensors for
high-accuracy performance in underwater SLAM systems remains insufficiently
explored. Existing methods for Doppler Velocity Log (DVL) calibration are
either constrained to specific sensor configurations or rely on oversimplified
assumptions, and none jointly estimate translational extrinsics and time
offsets. We propose a Unified Iterative Calibration (UIC) framework for general
DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a
Gaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC
alternates between efficient GP-based motion state updates and gradient-based
calibration variable updates, supported by a provably statistically consistent
sequential initialization scheme. The proposed UIC can be applied to IMU,
cameras and other modalities as co-sensors. We release an open-source
DVL-camera calibration toolbox. Beyond underwater applications, several aspects
of UIC-such as the integration of GP priors for MAP-based calibration and the
design of provably reliable initialization procedures-are broadly applicable to
other multi-sensor calibration problems. Finally, simulations and real-world
tests validate our approach.

</details>


### [167] [Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning](https://arxiv.org/abs/2510.24584)
*Jørgen Anker Olsen,Lars Rønhaug Pettersen,Kostas Alexis*

Main category: cs.RO

TL;DR: 该论文提出了一种基于课程的强化学习框架，训练机器人‘Olympus’实现高精度跳跃，水平跳跃达1.25米，垂直跳跃达1.0米，并结合行走策略解锁动态运动能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人跳跃任务中奖励稀疏和动态行为探索困难的问题，同时提升跳跃性能和精度，以实现更复杂的动态运动能力。

Method: 采用基于课程的强化学习框架，通过分离垂直和水平跳跃策略，利用抛体运动定律稠密化稀疏的跳跃奖励，并采用参考状态初始化方案加速动态跳跃行为的探索，无需依赖参考轨迹。此外，还结合了行走策略以解锁更多动态运动能力。

Result: 实验验证了水平跳跃可达1.25米（厘米级精度）和垂直跳跃1.0米的性能，并展示了该方法仅需少量修改即可学习全向跳跃。

Conclusion: 该论文提出的基于课程的强化学习框架成功训练出了高精度和高性能的跳跃策略，显著提升了机器人‘Olympus’的动态运动能力，并在实验中验证了其卓越的跳跃性能，有效跨越了仿真到现实的鸿沟。

Abstract: This paper presents a curriculum-based reinforcement learning framework for
training precise and high-performance jumping policies for the robot `Olympus'.
Separate policies are developed for vertical and horizontal jumps, leveraging a
simple yet effective strategy. First, we densify the inherently sparse jumping
reward using the laws of projectile motion. Next, a reference state
initialization scheme is employed to accelerate the exploration of dynamic
jumping behaviors without reliance on reference trajectories. We also present a
walking policy that, when combined with the jumping policies, unlocks versatile
and dynamic locomotion capabilities. Comprehensive testing validates walking on
varied terrain surfaces and jumping performance that exceeds previous works,
effectively crossing the Sim2Real gap. Experimental validation demonstrates
horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to
1.0 m. Additionally, we show that with only minor modifications, the proposed
method can be used to learn omnidirectional jumping.

</details>


### [168] [GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization](https://arxiv.org/abs/2510.24623)
*Nicolai Steinke,Daniel Goehring*

Main category: cs.RO

TL;DR: GroundLoc是一种高效的LiDAR定位系统，适用于大规模户外环境，支持多种传感器，存储需求低。


<details>
  <summary>Details</summary>
Motivation: 为大规模户外环境中的移动机器人提供高效的LiDAR-only定位方案。

Method: 采用鸟瞰图（BEV）投影和R2D2或SIFT进行关键点识别与地图注册。

Result: 在SemanticKITTI和HeLiPR数据集上表现优于现有方法，平均轨迹误差低于50厘米。

Conclusion: GroundLoc在多种传感器上表现优异，支持多种传感器模型，且存储需求低。

Abstract: In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.

</details>


### [169] [Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder](https://arxiv.org/abs/2510.24671)
*Li Li,Tobias Brinkmann,Till Temmen,Markus Eisenbarth,Jakob Andert*

Main category: cs.RO

TL;DR: 提出CVAE-T模型生成多智能体交通场景，用于智能驾驶功能验证和数据增强，结果显示模型能准确重建并生成多样化场景，潜在空间分析揭示可解释的维度影响。


<details>
  <summary>Details</summary>
Motivation: 随着智能驾驶功能在量产车中的广泛应用，确保其功能和鲁棒性面临更大挑战。基于场景的虚拟测试在时间、成本效率、可重复性和边缘案例探索方面具有显著优势。

Method: 提出了一种Transformer增强的条件变分自编码器（CVAE-T）模型，用于生成多智能体交通场景，特别是在环岛等复杂环境下。

Result: 结果表明，该模型能够准确重建原始场景并生成真实且多样化的合成场景。潜在空间分析显示部分解纠缠，多个潜在维度对场景属性（如车辆进入/退出时间和速度曲线）具有明显且可解释的影响。

Conclusion: 该模型能够生成用于验证涉及多智能体交互的智能驾驶功能的场景，并能为其开发和迭代改进提供数据增强。

Abstract: With the increasing integration of intelligent driving functions into
serial-produced vehicles, ensuring their functionality and robustness poses
greater challenges. Compared to traditional road testing, scenario-based
virtual testing offers significant advantages in terms of time and cost
efficiency, reproducibility, and exploration of edge cases. We propose a
Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for
generating multi-agent traffic scenarios in roundabouts, which are
characterized by high vehicle dynamics and complex layouts, yet remain
relatively underexplored in current research. The results show that the
proposed model can accurately reconstruct original scenarios and generate
realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators
(KPIs) are employed to evaluate the interactive behavior in the generated
scenarios. Analysis of the latent space reveals partial disentanglement, with
several latent dimensions exhibiting distinct and interpretable effects on
scenario attributes such as vehicle entry timing, exit timing, and velocity
profiles. The results demonstrate the model's capability to generate scenarios
for the validation of intelligent driving functions involving multi-agent
interactions, as well as to augment data for their development and iterative
improvement.

</details>


### [170] [Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis](https://arxiv.org/abs/2510.24676)
*Jiaxuan Zhang,Yuquan Leng,Yixuan Guo,Chenglong Fu*

Main category: cs.RO

TL;DR: 研究通过惯性传感器和遗传算法优化神经网络，有效预测假肢关节角度和步态，提升障碍物导航能力。


<details>
  <summary>Details</summary>
Motivation: 解决动力型大腿假肢使用者在复杂地形或障碍物导航中的挑战。

Method: 使用惯性传感器在健康脚踝上引导跨越障碍物的动作，遗传算法计算最优神经网络结构以预测大腿和膝关节所需角度，步态进展预测算法确定假肢膝关节电机的驱动角度索引。

Result: 当添加到大腿角度数据的高斯噪声标准差小于1时，该方法能有效消除噪声干扰，在150 Hz下实现100%的步态相位估计准确度，大腿角度预测误差为8.71%，膝关节角度预测误差为6.78%。

Conclusion: 该方法能够准确预测步态进展和关节角度，为动力型大腿假肢的障碍物导航提供了重要的实用价值。

Abstract: For amputees with powered transfemoral prosthetics, navigating obstacles or
complex terrain remains challenging. This study addresses this issue by using
an inertial sensor on the sound ankle to guide obstacle-crossing movements. A
genetic algorithm computes the optimal neural network structure to predict the
required angles of the thigh and knee joints. A gait progression prediction
algorithm determines the actuation angle index for the prosthetic knee motor,
ultimately defining the necessary thigh and knee angles and gait progression.
Results show that when the standard deviation of Gaussian noise added to the
thigh angle data is less than 1, the method can effectively eliminate noise
interference, achieving 100\% accuracy in gait phase estimation under 150 Hz,
with thigh angle prediction error being 8.71\% and knee angle prediction error
being 6.78\%. These findings demonstrate the method's ability to accurately
predict gait progression and joint angles, offering significant practical value
for obstacle negotiation in powered transfemoral prosthetics.

</details>


### [171] [Fare: Failure Resilience in Learned Visual Navigation Control](https://arxiv.org/abs/2510.24680)
*Zishuo Wang,Joel Loo,David Hsu*

Main category: cs.RO

TL;DR: Fare框架通过嵌入OOD检测与识别功能，无需显式故障数据即可构建故障恢复策略，提升模仿学习在复杂环境中的导航鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在OOD场景中易出现不可预测的故障，需要能自动检测并从故障中恢复的策略。

Method: Fare框架通过嵌入OOD检测与识别功能，并结合恢复启发式方法，构建故障恢复策略。

Result: 实验表明，Fare能在两种不同策略架构中实现故障恢复，提升复杂环境中的导航鲁棒性。

Conclusion: Fare框架成功构建了具备故障恢复能力的模仿学习策略，无需显式故障数据即可实现OOD检测与识别，并在复杂环境中实现了稳健的长距离导航。

Abstract: While imitation learning (IL) enables effective visual navigation, IL
policies are prone to unpredictable failures in out-of-distribution (OOD)
scenarios. We advance the notion of failure-resilient policies, which not only
detect failures but also recover from them automatically. Failure recognition
that identifies the factors causing failure is key to informing recovery: e.g.
pinpointing image regions triggering failure detections can provide cues to
guide recovery. We present Fare, a framework to construct failure-resilient IL
policies, embedding OOD-detection and recognition in them without using
explicit failure data, and pairing them with recovery heuristics. Real-world
experiments show that Fare enables failure recovery across two different policy
architectures, enabling robust long-range navigation in complex environments.

</details>


### [172] [A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers](https://arxiv.org/abs/2510.24683)
*Caleb Escobedo,Nataliya Nechyporenko,Shreyas Kadekodi,Alessandro Roncone*

Main category: cs.RO

TL;DR: 提出一个分析物体感知控制器的框架，通过实验验证发现现有方法在运动学和稳定性上的不足，结论是该框架有助于未来避障方法的设计与比较。


<details>
  <summary>Details</summary>
Motivation: 实时控制是机器人在动态物体环境中安全操作的关键，需要改进物体感知控制器的设计。

Method: 通过三个设计考量（运动学、运动曲线和虚拟约束）分析物体感知控制器，并利用基本机器人-障碍物实验场景验证机器人行为。

Result: 分析发现物体感知控制器常缺乏运动学考量、控制点连续性和运动曲线的稳定性。

Conclusion: 该框架未来可用于设计、比较和基准测试避障方法。

Abstract: Real-time control is an essential aspect of safe robot operation in the real
world with dynamic objects. We present a framework for the analysis of
object-aware controllers, methods for altering a robot's motion to anticipate
and avoid possible collisions. This framework is focused on three design
considerations: kinematics, motion profiles, and virtual constraints.
Additionally, the analysis in this work relies on verification of robot
behaviors using fundamental robot-obstacle experimental scenarios. To showcase
the effectiveness of our method we compare three representative object-aware
controllers. The comparison uses metrics originating from the design
considerations. From the analysis, we find that the design of object-aware
controllers often lacks kinematic considerations, continuity of control points,
and stability in movement profiles. We conclude that this framework can be used
in the future to design, compare, and benchmark obstacle avoidance methods.

</details>


### [173] [Embodying Physical Computing into Soft Robots](https://arxiv.org/abs/2510.24692)
*Jun Wang,Ziyang Zhou,Ardalan Kahak,Suyi Li*

Main category: cs.RO

TL;DR: 本文提出了一种将物理计算融入软机器人的框架，讨论了三种策略及其应用，展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 软机器人技术的最后一个前沿领域是软化和集成计算机与控制器，以实现其在日常使用中的稳健性和智能性。物理计算为实现这一目标提供了新的可能性。

Method: 通过分析三种独特的物理计算策略：模拟振荡器、物理储备计算和物理算法计算，详细阐述了这些方法的工作原理。

Result: 这些嵌入式计算机使软机器人能够执行复杂行为，如协调运动避障、有效载荷重量和方向分类，以及基于逻辑规则的可编程操作。

Conclusion: 本文提出了将物理计算融入软机器人的框架，并展望了未来发展的方向，强调了物理计算在提升软机器人稳健性和智能性方面的潜力。

Abstract: Softening and onboarding computers and controllers is one of the final
frontiers in soft robotics towards their robustness and intelligence for
everyday use. In this regard, embodying soft and physical computing presents
exciting potential. Physical computing seeks to encode inputs into a mechanical
computing kernel and leverage the internal interactions among this kernel's
constituent elements to compute the output. Moreover, such input-to-output
evolution can be re-programmable. This perspective paper proposes a framework
for embodying physical computing into soft robots and discusses three unique
strategies in the literature: analog oscillators, physical reservoir computing,
and physical algorithmic computing. These embodied computers enable the soft
robot to perform complex behaviors that would otherwise require CMOS-based
electronics -- including coordinated locomotion with obstacle avoidance,
payload weight and orientation classification, and programmable operation based
on logical rules. This paper will detail the working principles of these
embodied physical computing methods, survey the current state-of-the-art, and
present a perspective for future development.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [174] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: Grace是一个卫星-地面协作系统，通过异步RAG和任务调度算法，实现低延迟高精度的LVLM遥感任务处理。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在遥感任务中潜力巨大，但受限于星上计算资源和短暂的星地接触，实际部署困难。

Method: 设计了Grace系统，包含异步卫星-地面检索增强生成（RAG）和任务调度算法，结合紧凑型LVLM在星上部署和大型LVLM在地面站的部署策略。

Result: 基于真实卫星轨道数据的实验表明，Grace相比现有方法平均延迟降低76-95%，且不损失推理精度。

Conclusion: Grace系统通过卫星-地面协作，显著降低了延迟（76-95%），同时保持了推理精度，为实时LVLM在遥感任务中的应用提供了可行方案。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [175] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 提出一种混合预编码方法，通过联合角度和相位熵优化MIMO系统性能，仿真显示和速率提升18.31%，鲁棒性提高11.47%。


<details>
  <summary>Details</summary>
Motivation: 利用混合预编码技术充分挖掘多用户大规模MIMO系统的潜力，优化数据传输并抑制旁瓣干扰。

Method: 结合数字和模拟预编码，首次定义联合角度和相位熵来测量无线信道中角度和相位变化的不确定性。

Result: 仿真结果表明，所提方法在和速率上提升了18.31%，鲁棒性提高了11.47%，优于现有先进方法。

Conclusion: 本文提出了一种新的混合预编码方法，通过联合角度和相位熵优化多用户大规模MIMO系统的数据传输，显著提升了和速率和鲁棒性。

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


### [176] [Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives](https://arxiv.org/abs/2510.24611)
*Azadeh Pourkabirian,Amir Masoud Rahmani,Kai Li,Wei Ni*

Main category: cs.NI

TL;DR: 论文提出了一种基于经济供需模型和VCG拍卖的任务卸载方法，通过游戏理论框架和激励机制，实现了市场资源平衡和延迟保证。


<details>
  <summary>Details</summary>
Motivation: 由于IoT设备处理资源有限且需要实时响应，延迟敏感的IoT应用在设备上运行具有挑战性。任务卸载可以最小化延迟，但需确保资源分配的最佳平衡。

Method: 论文将任务卸载问题建模为经济供需模型，并设计了一个基于VCG拍卖的游戏理论框架，同时开发了激励机制以促进用户和服务提供商的参与。

Result: 模拟实验表明，该方法能够最大化社会福利、确保诚实性、维持市场平衡，并为延迟敏感的IoT应用提供延迟保证。

Conclusion: 该论文提出的基于经济供需模型和VCG拍卖的任务卸载方法，能够有效平衡市场资源，最大化社会福利，并为延迟敏感的IoT应用提供延迟保证。

Abstract: Delay-sensitive Internet of Things (IoT) applications have drawn significant
attention. Running many of these applications on IoT devices is challenging due
to the limited processing resources of these devices and the need for real-time
responses. Task offloading can minimize latency by transferring computationally
intensive tasks from IoT devices to resource-rich edge servers, ensuring delay
and performance guarantees. In this paper, we develop a task-offloading
approach for delay-sensitive IoT applications in edge computing environments.
Unlike existing schemes, we model the task offloading problem as an economic
demand and supply model to achieve market balance. The proposed model avoids
under- and over-supply, ensuring the computational resources at edge servers
(supply) are allocated in a manner that best meets the processing and
computational needs of user devices (demand). Given the multi-agent nature of
task offloading involving users and service providers with different
preferences and objectives, we design a game-theoretic framework using a
Vickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions
and decision-making processes. Additionally, we develop an incentive mechanism
to encourage both parties to participate in the auction. The mechanism
maximizes user task offloading to edge servers and motivates edge servers to
share their computational resources, achieving profitability for both IoT users
and edge servers. Simulations demonstrate our method maximizes social welfare,
ensures truthfulness, maintains market balance, and provides latency guarantees
for delay-sensitive IoT applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [177] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: 论文分析了SAP云平台中虚拟机的调度和放置问题，基于大规模观测数据揭示了资源分配的低效现象，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 研究目的是分析分布式环境中的资源分配挑战，特别是在SAP云平台中虚拟机调度和放置的问题。

Method: 论文基于SAP云平台中约1,800台虚拟机和48,000个虚拟机的30天观测数据，通过可观测性工具跟踪资源使用和性能指标。

Result: 研究发现包括CPU资源争用超过40%、CPU就绪时间长达220秒、计算主机负载不均（最高达99%）、以及超过80%的虚拟机使用不到70%的资源。

Conclusion: 基于研究发现，论文提出了新颖的调度和放置算法的设计要求，并提供了优化资源分配的指导。

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [178] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: 该论文提出了一种针对多GPU优化的高性能可压缩反应流求解器，通过内存访问和负载均衡优化，在燃烧应用中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 高速化学反应流因空间和时间尺度的巨大差异及刚性化学反应的复杂性，导致现有GPU求解器在内存管理、负载均衡和化学反应局部性处理上存在瓶颈，亟需优化。

Method: 采用列优先存储优化内存访问模式，通过批量稀疏积分策略处理化学动力学的计算负载可变性，并针对自适应网格细化应用优化多GPU负载分布。

Result: 在1-96个NVIDIA H100 GPU上实现了近理想的弱扩展性，对流和化学反应的算术强度分别提升了约10倍和4倍，有效利用了GPU内存带宽和计算资源。

Conclusion: 该论文展示了一种基于AMReX框架的高性能可压缩反应流求解器，通过优化内存访问模式、计算负载均衡和多GPU负载分布，显著提升了GPU计算效率，并在氢-空气爆轰和超音速横流喷射等应用中实现了2-5倍的性能提升。

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [179] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: SPACE-CoE优化天体物理数值代码，三个代码在Leonardo系统上测试显示高效扩展至1,024个GPU。


<details>
  <summary>Details</summary>
Motivation: 为大规模模拟开发并重新设计适用于现有和下一代加速器的数值代码，以满足天体物理、宇宙学和空间等离子体领域的需求。

Method: 使用分析工具对单节点和多节点上的性能进行分析，评估gPLUTO、OpenGadget3和iPIC3D三个代码的性能。

Result: 初步测试显示，所有三个代码在1,024个GPU上可达到80%的可扩展性。

Conclusion: SPACE-CoE通过优化天体物理、宇宙学和空间等离子体数值代码，为现有和下一代加速器提供支持，初步测试显示三个旗舰代码在Leonardo系统上具有高效的可扩展性。

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [180] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: CoMPSeT 是一个开源工具，帮助研究者和教师通过动画和比较理解多党会话类型（MPST）的不同特性。


<details>
  <summary>Details</summary>
Motivation: 并发系统设计复杂，MPST 等编排语言虽能描述全局交互协议，但现有 MPST 变体各具特性且复杂，需更清晰的工具来理解和比较这些特性。

Method: 通过选择代表性的 MPST 示例，提供机制以结合不同特性，并通过动画和比较具体示例的语义来实现。

Result: 开发了 CoMPSeT 工具，能够结合不同 MPST 特性，并通过动画和比较帮助理解和教学。

Conclusion: CoMPSeT 是一个开源工具，编译为 JavaScript，可直接在浏览器中执行，为研究者和教师提供了理解和教授多党会话类型（MPST）的有力工具。

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


### [181] [ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery](https://arxiv.org/abs/2510.24452)
*Xi Cheng,Weijie Shen,Haoming Chen,Chaoyi Shen,Jean Ortega,Jiashang Liu,Steve Thomas,Honglin Zheng,Haoyun Wu,Yuxiang Li,Casey Lichtendahl,Jenny Ortiz,Gang Liu,Haiyang Qi,Omid Fatemieh,Chris Fry,Jing Jing Long*

Main category: cs.DC

TL;DR: ARIMA_PLUS结合模块化模型与云基础设施，高效解决大规模时间序列预测和异常检测问题，性能优于传统和深度学习方法，并支持高可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模时间序列自动预测/异常检测的效率与准确性挑战，并确保结果的可解释性以融入业务洞察。

Method: 采用模块化结构处理时间序列的多个组件（如节假日效应、季节性、趋势和异常），并对每个模块进行创新增强，同时建立统一框架以同时处理预测和异常检测任务。

Result: 在42个公开数据集的基准测试中，ARIMA_PLUS性能优于传统统计方法（如ETS、ARIMA）和新型神经网络模型（如DeepAR、N-BEATS）。基础设施上，通过Google Cloud的BigQuery实现高效扩展，支持每秒18000条时间序列的处理。

Conclusion: ARIMA_PLUS框架通过结合高精度、可解释的时间序列模型与可扩展的系统基础设施，成功解决了大规模时间序列预测和异常检测的两大挑战，并在公开数据集上展现了优越性能。

Abstract: Time series forecasting and anomaly detection are common tasks for
practitioners in industries such as retail, manufacturing, advertising and
energy. Two unique challenges stand out: (1) efficiently and accurately
forecasting time series or detecting anomalies in large volumes automatically;
and (2) ensuring interpretability of results to effectively incorporate
business insights. We present ARIMA_PLUS, a novel framework to overcome these
two challenges by a unique combination of (a) accurate and interpretable time
series models and (b) scalable and fully managed system infrastructure. The
model has a sequential and modular structure to handle different components of
the time series, including holiday effects, seasonality, trend, and anomalies,
which enables high interpretability of the results. Novel enhancements are made
to each module, and a unified framework is established to address both
forecasting and anomaly detection tasks simultaneously. In terms of accuracy,
its comprehensive benchmark on the 42 public datasets in the Monash forecasting
repository shows superior performance over not only well-established
statistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer
neural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms
of infrastructure, it is directly built into the query engine of BigQuery in
Google Cloud. It uses a simple SQL interface and automates tedious
technicalities such as data cleaning and model selection. It automatically
scales with managed cloud computational and storage resources, making it
possible to forecast 100 million time series using only 1.5 hours with a
throughput of more than 18000 time series per second. In terms of
interpretability, we present several case studies to demonstrate time series
insights it generates and customizability it offers.

</details>
