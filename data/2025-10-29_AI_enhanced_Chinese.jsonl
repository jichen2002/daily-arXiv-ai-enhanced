{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces \u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u548c\u81ea\u52a8\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fa\u7248\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4e3a\u884c\u4e1a\u5e26\u6765\u9769\u65b0\u3002", "motivation": "\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u5728\u51fa\u7248\u9886\u57df\u7684\u5e94\u7528\uff0c\u4ee5\u7f29\u77ed\u4e0a\u5e02\u65f6\u95f4\u3001\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\uff0c\u540c\u65f6\u5f00\u62d3\u4f20\u7edf\u51fa\u7248\u65e0\u6cd5\u89e6\u53ca\u7684\u5229\u57fa\u5e02\u573a\u3002", "method": "\u91c7\u7528\u914d\u7f6e\u9a71\u52a8\u7684\u67b6\u6784\u548c\u591a\u6a21\u578bAI\u96c6\u6210\u6846\u67b6\uff0c\u5305\u62ec\u6301\u7eed\u521b\u610f\u6d41\u6c34\u7ebf\u3001\u9526\u6807\u8d5b\u5f0f\u8bc4\u4f30\u3001\u65b0\u9896\u7684\u8f6c\u5f55\u51a5\u60f3\u5b9e\u8df5\u7f16\u7801\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u4ece\u521b\u610f\u5230\u751f\u4ea7\u548c\u5206\u53d1\u7684\u5168\u9762\u81ea\u52a8\u5316\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e8690%\u7684\u4e0a\u5e02\u65f6\u95f4\u7f29\u77ed\uff08\u4ece6-12\u4e2a\u6708\u964d\u81f32-4\u5468\uff09\uff0c80%\u7684\u6210\u672c\u524a\u51cf\uff0c\u7b2c\u4e00\u5e74\u51fa\u7248\u4e8652\u672c\u4e66\uff0c\u5e76\u4fdd\u630199%\u7684\u5f15\u7528\u51c6\u786e\u7387\u548c100%\u7684\u9a8c\u8bc1\u6210\u529f\u7387\u3002", "conclusion": "Xynapse Traces \u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u51fa\u7248\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u9ad8\u8d28\u91cf\u7684\u51fa\u7248\u6d41\u7a0b\uff0c\u4e3a\u672a\u6765\u51fa\u7248\u4e1a\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u7cfb\u7edf\u6027\u57fa\u51c6\u548c\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u5f00\u6e90\u57fa\u7ebf\u5e76\u63a5\u8fd1\u4e13\u6709\u6a21\u578b\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u3001\u6267\u884c\u4e0d\u53ef\u9760\u4ee5\u53ca\u7f3a\u4e4f\u8fed\u4ee3\u6821\u6b63\u673a\u5236\u3002\u8fdb\u5c55\u53d7\u9650\u4e8e\u5f3a\u8c03\u5355\u8f6e\u751f\u6210\u548c\u5355\u8bed\u8a00\u4efb\u52a1\u7684\u72ed\u7a84\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "method": "\u5f15\u5165\u4e86\u4e09\u4e2a\u4e92\u8865\u8d44\u6e90\uff1aVisCode-Multi-679K\u6570\u636e\u96c6\u3001VisPlotBench\u57fa\u51c6\u548cVisCoder2\u6a21\u578b\u5bb6\u65cf\u3002VisCode-Multi-679K\u5305\u542b679K\u4e2a\u9a8c\u8bc1\u53ef\u6267\u884c\u7684\u53ef\u89c6\u5316\u6837\u672c\u548c\u8de812\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u591a\u8f6e\u6821\u6b63\u5bf9\u8bdd\u3002VisPlotBench\u63d0\u4f9b\u53ef\u6267\u884c\u4efb\u52a1\u3001\u6e32\u67d3\u8f93\u51fa\u4ee5\u53ca\u521d\u59cb\u751f\u6210\u548c\u591a\u8f6e\u81ea\u8c03\u8bd5\u534f\u8bae\u3002", "result": "VisCoder2\u5728\u591a\u8f6e\u81ea\u8c03\u8bd5\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u5c24\u5176\u5728\u4f9d\u8d56\u7b26\u53f7\u6216\u7f16\u8bd1\u5668\u7684\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VisCoder2\u6a21\u578b\u5728VisCode-Multi-679K\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u63a5\u8fd1GPT-4.1\u7b49\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u572832B\u89c4\u6a21\u4e0b\u8fbe\u5230\u4e8682.4%\u7684\u6574\u4f53\u6267\u884c\u901a\u8fc7\u7387\uff0c\u5c24\u5176\u5728\u4f9d\u8d56\u7b26\u53f7\u6216\u7f16\u8bd1\u5668\u7684\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "Agentsway\u662f\u4e00\u79cd\u65b0\u578b\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u4e13\u4e3aAI\u4ee3\u7406\u534f\u4f5c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u89d2\u8272\u5b9a\u4e49\u548c\u5fae\u8c03LLMs\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\uff08\u5982Agile\u3001Kanban\uff09\u4e3b\u8981\u4e3a\u4eba\u7c7b\u56e2\u961f\u8bbe\u8ba1\uff0c\u5728AI\u4ee3\u7406\u53c2\u4e0e\u7684\u73af\u5883\u4e0b\u9010\u6e10\u4e0d\u9002\u5e94\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u8bba\u3002", "method": "Agentsway\u901a\u8fc7\u5b9a\u4e49\u89c4\u5212\u3001\u63d0\u793a\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u5fae\u8c03\u4ee3\u7406\u7684\u660e\u786e\u89d2\u8272\uff0c\u7ed3\u5408\u5fae\u8c03LLMs\u548c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u534f\u4f5c\u5f00\u53d1\u3002", "result": "Agentsway\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u751f\u547d\u5468\u671f\u548c\u534f\u4f5c\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u5e76\u5d4c\u5165\u4e86\u8d1f\u8d23\u4efbAI\u539f\u5219\u3002", "conclusion": "Agentsway\u4e3aAI\u4ee3\u7406\u4f5c\u4e3a\u4e00\u7b49\u534f\u4f5c\u8005\u7684\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u578b\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u5411AI\u539f\u751f\u3001\u81ea\u6211\u6539\u8fdb\u7684\u65b9\u6cd5\u8bba\u8fc8\u8fdb\u3002"}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen\u901a\u8fc7RAG\u548cLLM\u81ea\u6211\u53cd\u601d\u673a\u5236\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u6a21\u578b\u4e0a\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6027\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u6216\u5f00\u53d1\u4e13\u95e8\u7684\u5b89\u5168\u4ee3\u7801\u6570\u636e\u96c6\uff09\u7684\u8d44\u6e90\u5bc6\u96c6\u6027\u95ee\u9898\u3002", "method": "RefleXGen\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u548cLLMs\u81ea\u5e26\u7684\u5f15\u5bfc\u6027\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0c\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u65e0\u9700\u5927\u91cf\u8d44\u6e90\u3002", "result": "RefleXGen\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u5728GPT-3.5 Turbo\u3001GPT-4o\u3001CodeQwen\u548cGemini\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8613.6%\u30016.7%\u30014.5%\u548c5.8%\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u9ad8\u6a21\u578b\u81ea\u6211\u53cd\u601d\u7684\u8d28\u91cf\u662f\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u7684\u6709\u6548\u4e14\u5b9e\u7528\u7684\u7b56\u7565\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u65e0\u9700\u8bad\u7ec3\uff0c\u5229\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u751f\u6210\u6a21\u5757\u5316\u74e6\u7247\uff0c\u901a\u8fc7\u591a\u74e6\u7247\u53bb\u566a\u5408\u6210\u5927\u89c4\u6a21\u3001\u8fde\u8d2f\u76843D\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u5bf9\u8c61\u751f\u6210\u3001\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6216\u7f3a\u4e4f\u5bf9\u5168360\u5ea6\u53ef\u89c6\u6027\u7684\u652f\u6301\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5c06\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u74e6\u7247\u53bb\u566a\u95ee\u9898\uff0c\u901a\u8fc7\u72ec\u7acb\u751f\u6210\u91cd\u53e0\u76843D\u533a\u57df\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u65e0\u7f1d\u6df7\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u591a\u6837\u5316\u7684\u573a\u666f\u5e03\u5c40\u3001\u9ad8\u6548\u751f\u6210\u548c\u7075\u6d3b\u7f16\u8f91\uff0c\u4e3a\u901a\u7528\u3001\u8bed\u8a00\u9a71\u52a8\u76843D\u573a\u666f\u6784\u5efa\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u5f3a\u5927\u7684\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u76843D\u573a\u666f\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u901a\u7528\u7684\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6a21\u5757\u5316\u74e6\u7247\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u3001\u8fde\u8d2f\u573a\u666f\u7684\u53ef\u6269\u5c55\u5408\u6210\u3002"}}
{"id": "2510.23911", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.23911", "abs": "https://arxiv.org/abs/2510.23911", "authors": ["Arno Uhlig", "Iris Braun", "Matthias W\u00e4hlisch"], "title": "The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing", "comment": "15 pages", "summary": "Allocating resources in a distributed environment is a fundamental challenge.\nIn this paper, we analyze the scheduling and placement of virtual machines\n(VMs) in the cloud platform of SAP, the world's largest enterprise resource\nplanning software vendor. Based on data from roughly 1,800 hypervisors and\n48,000 VMs within a 30-day observation period, we highlight potential\nimprovements for workload management. The data was measured through\nobservability tooling that tracks resource usage and performance metrics across\nthe entire infrastructure. In contrast to existing datasets, ours uniquely\noffers fine-grained time-series telemetry data of fully virtualized\nenterprise-level workloads from both long-running and memory-intensive SAP\nS/4HANA and diverse, general-purpose applications. Our key findings include\nseveral suboptimal scheduling situations, such as CPU resource contention\nexceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced\ncompute hosts with a maximum CPU~utilization on intra-building block hosts of\nup to 99%, and overprovisioned CPU and memory resources resulting into over 80%\nof VMs using less than 70% of the provided resources. Bolstered by these\nfindings, we derive requirements for the design and implementation of novel\nplacement and scheduling algorithms and provide guidance to optimize resource\nallocations. We make the full dataset used in this study publicly available to\nenable data-driven evaluations of scheduling approaches for large-scale cloud\ninfrastructures in future research.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86SAP\u4e91\u5e73\u53f0\u4e2d\u865a\u62df\u673a\u7684\u8c03\u5ea6\u548c\u653e\u7f6e\u95ee\u9898\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u89c2\u6d4b\u6570\u636e\u63ed\u793a\u4e86\u8d44\u6e90\u5206\u914d\u7684\u4f4e\u6548\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5206\u6790\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u7279\u522b\u662f\u5728SAP\u4e91\u5e73\u53f0\u4e2d\u865a\u62df\u673a\u8c03\u5ea6\u548c\u653e\u7f6e\u7684\u95ee\u9898\u3002", "method": "\u8bba\u6587\u57fa\u4e8eSAP\u4e91\u5e73\u53f0\u4e2d\u7ea61,800\u53f0\u865a\u62df\u673a\u548c48,000\u4e2a\u865a\u62df\u673a\u768430\u5929\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u8ddf\u8e2a\u8d44\u6e90\u4f7f\u7528\u548c\u6027\u80fd\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5305\u62ecCPU\u8d44\u6e90\u4e89\u7528\u8d85\u8fc740%\u3001CPU\u5c31\u7eea\u65f6\u95f4\u957f\u8fbe220\u79d2\u3001\u8ba1\u7b97\u4e3b\u673a\u8d1f\u8f7d\u4e0d\u5747\uff08\u6700\u9ad8\u8fbe99%\uff09\u3001\u4ee5\u53ca\u8d85\u8fc780%\u7684\u865a\u62df\u673a\u4f7f\u7528\u4e0d\u523070%\u7684\u8d44\u6e90\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u8c03\u5ea6\u548c\u653e\u7f6e\u7b97\u6cd5\u7684\u8bbe\u8ba1\u8981\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u8d44\u6e90\u5206\u914d\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.24242", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24242", "abs": "https://arxiv.org/abs/2510.24242", "authors": ["Zihan Li", "Jiahao Yang", "Yuxin Zhang", "Zhe Chen", "Yue Gao"], "title": "Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models", "comment": "15 pages, 11 figures", "summary": "Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.", "AI": {"tldr": "Grace\u662f\u4e00\u4e2a\u536b\u661f-\u5730\u9762\u534f\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65RAG\u548c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u7cbe\u5ea6\u7684LVLM\u9065\u611f\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u661f\u4e0a\u8ba1\u7b97\u8d44\u6e90\u548c\u77ed\u6682\u7684\u661f\u5730\u63a5\u89e6\uff0c\u5b9e\u9645\u90e8\u7f72\u56f0\u96be\u3002", "method": "\u8bbe\u8ba1\u4e86Grace\u7cfb\u7edf\uff0c\u5305\u542b\u5f02\u6b65\u536b\u661f-\u5730\u9762\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7ed3\u5408\u7d27\u51d1\u578bLVLM\u5728\u661f\u4e0a\u90e8\u7f72\u548c\u5927\u578bLVLM\u5728\u5730\u9762\u7ad9\u7684\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u536b\u661f\u8f68\u9053\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGrace\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e76-95%\uff0c\u4e14\u4e0d\u635f\u5931\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "Grace\u7cfb\u7edf\u901a\u8fc7\u536b\u661f-\u5730\u9762\u534f\u4f5c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0876-95%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u7cbe\u5ea6\uff0c\u4e3a\u5b9e\u65f6LVLM\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.23913", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.23913", "abs": "https://arxiv.org/abs/2510.23913", "authors": ["Daniel Agassy", "Dani Dorfman", "Haim Kaplan"], "title": "Expander Decomposition for Non-Uniform Vertex Measures", "comment": null, "summary": "A $(\\phi,\\epsilon)$-expander-decomposition of a graph $G$ (with $n$ vertices\nand $m$ edges) is a partition of $V$ into clusters $V_1,\\ldots,V_k$ with\nconductance $\\Phi(G[V_i]) \\ge \\phi$, such that there are at most $\\epsilon m$\ninter-cluster edges. Such a decomposition plays a crucial role in many graph\nalgorithms. [ADK23] gave a randomized $\\tilde{O}(m)$ time algorithm for\ncomputing a $(\\phi, \\phi\\log^2 {n})$-expander decomposition.\n  In this paper we generalize this result for a broader notion of expansion.\nLet $\\mu \\in {\\mathbb{R}}_{\\ge 0 }^n$ be a vertex measure. A standard\ngeneralization of conductance of a cut $(S,\\bar{S})$ is its $\\mu$-expansion\n$\\Phi^{\\mu}_G(S,\\bar{S}) = |E(S,\\bar{S})|/\\min \\mu(S)),\\mu(\\bar{S})\\}$, where\n$\\mu(S) = \\sum_{v\\in S} \\mu(v)$. We present a randomized $\\tilde{O}(m)$ time\nalgorithm for computing a $(\\phi, \\phi \\log^2\n{n}\\left(\\frac{\\mu(V)}{m}\\right))$-expander decomposition with respect to\n$\\mu$-expansion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u6269\u5c55\u5206\u89e3\u7b97\u6cd5\uff0c\u80fd\u5728O\u0303(m)\u65f6\u95f4\u5185\u8ba1\u7b97\u57fa\u4e8e\u9876\u70b9\u5ea6\u91cf\u03bc\u7684\u6269\u5c55\u5206\u89e3\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6210\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u5c55\u5206\u89e3\u7b97\u6cd5\u5728\u5904\u7406\u5e7f\u4e49\u6269\u5c55\u6982\u5ff5\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u56fe\u7b97\u6cd5\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO\u0303(m)\uff0c\u8ba1\u7b97\u57fa\u4e8e\u03bc-\u6269\u5c55\u7684\u5206\u89e3\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728O\u0303(m)\u65f6\u95f4\u5185\u8ba1\u7b97\u57fa\u4e8e\u03bc-\u6269\u5c55\u7684\u5206\u89e3\uff0c\u6269\u5c55\u4e86[ADK23]\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u6269\u5c55\u5206\u89e3\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u968f\u673a\u5316\u7ebf\u6027\u65f6\u95f4\u5185\u8ba1\u7b97\u57fa\u4e8e\u9876\u70b9\u5ea6\u91cf\u03bc\u7684\u6269\u5c55\u5206\u89e3\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u56fe\u7b97\u6cd5\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u7684\u65e5\u76ca\u771f\u5b9e\u5316\u5bf9\u9a8c\u8bc1\u89c6\u89c9\u771f\u5b9e\u6027\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\uff08\u201cFaster-Than-Lies\u201d\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Qwen2-VL-7B\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca32x32\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\u3002\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u91cd\u5efa\u8bef\u5dee\u56fe\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u56fe\uff0c\u589e\u5f3a\u4e86\u5bf9\u4eba\u7c7b\u548cVLM\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6269\u5c55\u7684CiFAKE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8696.5%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u57288\u6838CPU\u4e0a\u4fdd\u6301175ms\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u9002\u5408\u672c\u5730\u6216\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u6b64\u5916\uff0c\u5c0670\u79cd\u89c6\u89c9\u4f2a\u5f71\u7c7b\u578b\u5206\u7c7b\u4e3a\u516b\u4e2a\u8bed\u4e49\u7ec4\uff0c\u5e76\u4e3a\u6bcf\u79cd\u68c0\u6d4b\u5230\u7684\u5f02\u5e38\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6587\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u63a8\u7406\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6982\u8ff0\u4e86\u5728\u6cd5\u533b\u5b66\u3001\u5de5\u4e1a\u68c0\u6d4b\u548c\u793e\u4ea4\u5a92\u4f53\u5ba1\u6838\u7b49\u9886\u57df\u7684\u6f5c\u5728\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "RoboOmni \u662f\u4e00\u79cd\u57fa\u4e8e\u5168\u6a21\u6001 LLMs \u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u4e3b\u52a8\u8bc6\u522b\u7528\u6237\u610f\u56fe\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u5f88\u5c11\u76f4\u63a5\u53d1\u51fa\u6307\u4ee4\u3002\u4e3a\u4e86\u4fc3\u8fdb\u6709\u6548\u534f\u4f5c\uff0c\u673a\u5668\u4eba\u9700\u8981\u80fd\u591f\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002", "method": "RoboOmni \u91c7\u7528\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684 Perceiver-Thinker-Talker-Executor \u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u610f\u56fe\u8bc6\u522b\u3001\u4ea4\u4e92\u786e\u8ba4\u548c\u884c\u52a8\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u65f6\u7a7a\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u610f\u56fe\u8bc6\u522b\u3002", "result": "RoboOmni \u5728\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6210\u529f\u7387\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RoboOmni \u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6587\u672c\u548c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u79cd\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u79cd\u6e38\u620f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3API\u6216GUI\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u9650\u5236\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u5f02\u6784\u6e38\u620f\u73af\u5883\u4e2d\u7684\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u952e\u76d8-\u9f20\u6807\u8f93\u5165\u7684\u901a\u7528\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ed3\u5408\u8870\u51cf\u6301\u7eed\u635f\u5931\u548c\u9ad8\u6548\u7a00\u758f\u601d\u7ef4\u7b56\u7565\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u7684\u4e24\u500d\uff0c\u63a5\u8fd1\u4eba\u7c7b\u5728\u65b0Web 3D\u6e38\u620f\u4e2d\u7684\u901a\u7528\u6027\uff0cFPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u7b49\u6a21\u578b\u3002", "conclusion": "Game-TARS\u901a\u8fc7\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u6784\u5efa\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow\u662f\u4e00\u79cd\u6d4b\u8bd5\u9a71\u52a8\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u63d0\u5347LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u89e3\u51b3\u4eba\u7c7b\u6d4b\u8bd5\u65f6\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u751f\u6210\u6709\u6548\u6d4b\u8bd5\u4ecd\u662f\u6311\u6218\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4eba\u7c7b\u7f16\u5199\u7684\u6d4b\u8bd5\uff0c\u8bbe\u8ba1\u4e86TDFlow\u8fd9\u4e00\u6d4b\u8bd5\u9a71\u52a8\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u7b80\u5316\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002", "method": "TDFlow\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u7a0b\u5e8f\u4fee\u590d\u5206\u89e3\u4e3a\u56db\u4e2a\u7531\u5b50\u4ee3\u7406\u7ba1\u7406\u7684\u7ec4\u4ef6\uff1a\u8865\u4e01\u63d0\u6848\u3001\u8c03\u8bd5\u3001\u8865\u4e01\u4fee\u8ba2\u548c\u53ef\u9009\u6d4b\u8bd5\u751f\u6210\u3002", "result": "TDFlow\u5728SWE-Bench Lite\u4e0a\u8fbe\u523088.8%\u901a\u8fc7\u7387\uff08\u7edd\u5bf9\u63d0\u534727.8%\uff09\uff0c\u5728SWE-Bench Verified\u4e0a\u8fbe\u523094.3%\u901a\u8fc7\u7387\uff0c\u4ec5\u53d1\u73b07\u4f8b\u6d4b\u8bd5\u4f5c\u5f0a\u3002", "conclusion": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u5df2\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6d4b\u8bd5\u89e3\u51b3\u80fd\u529b\uff0c\u5b8c\u5168\u81ea\u4e3b\u4fee\u590d\u4ed3\u5e93\u7684\u6700\u540e\u969c\u788d\u5728\u4e8e\u751f\u6210\u51c6\u786e\u7684\u518d\u73b0\u6d4b\u8bd5\u3002"}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDisK-NeuralRTI\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u964d\u4f4eNeuralRTI\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u89e3\u51b3\u4e86\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982PTM\u548cHSH\uff09\u5728\u590d\u6742\u53cd\u5c04\u573a\u6355\u6349\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u800cNeuralRTI\u867d\u8d28\u91cf\u4f18\u8d8a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u6709\u9650\u786c\u4ef6\u4e0a\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u66f4\u5c0f\u7684\u7f51\u7edc\u6765\u6a21\u4eff\u539f\u59cb\u5927\u578b\u7f51\u7edc\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u63d0\u51fa\u7684DisK-NeuralRTI\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u539f\u59cbNeuralRTI\u76f8\u8fd1\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u65b9\u6cd5\uff08DisK-NeuralRTI\uff09\uff0c\u6709\u6548\u964d\u4f4e\u4e86NeuralRTI\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2510.23993", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23993", "abs": "https://arxiv.org/abs/2510.23993", "authors": ["Anthony Carreon", "Jagmohan Singh", "Shivank Sharma", "Shuzhi Zhang", "Venkat Raman"], "title": "A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales", "comment": "32 pages, 12 figures", "summary": "High-speed chemically active flows present significant computational\nchallenges due to their disparate space and time scales, where stiff chemistry\noften dominates simulation time. While modern supercomputing scientific codes\nachieve exascale performance by leveraging graphics processing units (GPUs),\nexisting GPU-based compressible combustion solvers face critical limitations in\nmemory management, load balancing, and handling the highly localized nature of\nchemical reactions. To this end, we present a high-performance compressible\nreacting flow solver built on the AMReX framework and optimized for multi-GPU\nsettings. Our approach addresses three GPU performance bottlenecks: memory\naccess patterns through column-major storage optimization, computational\nworkload variability via a bulk-sparse integration strategy for chemical\nkinetics, and multi-GPU load distribution for adaptive mesh refinement\napplications. The solver adapts existing matrix-based chemical kinetics\nformulations to multigrid contexts. Using representative combustion\napplications including hydrogen-air detonations and jet in supersonic crossflow\nconfigurations, we demonstrate $2-5\\times$ performance improvements over\ninitial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA\nH100 GPUs. Roofline analysis reveals substantial improvements in arithmetic\nintensity for both convection ($\\sim 10 \\times$) and chemistry ($\\sim 4\n\\times$) routines, confirming efficient utilization of GPU memory bandwidth and\ncomputational resources.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591aGPU\u4f18\u5316\u7684\u9ad8\u6027\u80fd\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5185\u5b58\u8bbf\u95ee\u548c\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\uff0c\u5728\u71c3\u70e7\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9ad8\u901f\u5316\u5b66\u53cd\u5e94\u6d41\u56e0\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5de8\u5927\u5dee\u5f02\u53ca\u521a\u6027\u5316\u5b66\u53cd\u5e94\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u73b0\u6709GPU\u6c42\u89e3\u5668\u5728\u5185\u5b58\u7ba1\u7406\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u5316\u5b66\u53cd\u5e94\u5c40\u90e8\u6027\u5904\u7406\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5217\u4f18\u5148\u5b58\u50a8\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u901a\u8fc7\u6279\u91cf\u7a00\u758f\u79ef\u5206\u7b56\u7565\u5904\u7406\u5316\u5b66\u52a8\u529b\u5b66\u7684\u8ba1\u7b97\u8d1f\u8f7d\u53ef\u53d8\u6027\uff0c\u5e76\u9488\u5bf9\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u5e94\u7528\u4f18\u5316\u591aGPU\u8d1f\u8f7d\u5206\u5e03\u3002", "result": "\u57281-96\u4e2aNVIDIA H100 GPU\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\uff0c\u5bf9\u6d41\u548c\u5316\u5b66\u53cd\u5e94\u7684\u7b97\u672f\u5f3a\u5ea6\u5206\u522b\u63d0\u5347\u4e86\u7ea610\u500d\u548c4\u500d\uff0c\u6709\u6548\u5229\u7528\u4e86GPU\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8eAMReX\u6846\u67b6\u7684\u9ad8\u6027\u80fd\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u548c\u591aGPU\u8d1f\u8f7d\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u6c22-\u7a7a\u6c14\u7206\u8f70\u548c\u8d85\u97f3\u901f\u6a2a\u6d41\u55b7\u5c04\u7b49\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e862-5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24595", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24595", "abs": "https://arxiv.org/abs/2510.24595", "authors": ["Azadeh Pourkabirian", "Kai Li", "Photios A. Stavrou", "Wei Ni"], "title": "A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels", "comment": null, "summary": "Hybrid precoding is an indispensable technique to harness the full potential\nof a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In\nthis paper, we propose a new hybrid precoding approach that combines digital\nand analog precoding to optimize data transmission over multiple antennas. This\napproach steers signals in specific directions, leading to maximizing sum-rate\nand suppressing side-lobe interference. When dealing with complex signals,\nchanges in phase are naturally associated with changes in angle, and these\nvariations are inherently correlated. The correlation between the angle and\nphase is essential for accurately determining the channel characteristics. An\nimportant aspect of this approach is that we model the angle and phase as\ncorrelated variables following a bivariate Gaussian distribution, and for the\nfirst time, we define a joint angle and phase entropy to measure the\nuncertainty of angle and phase variations in wireless channels. This entropy is\ncrucial to adapt the proposed precoding method with variations. Simulation\nresult validate the accuracy of our analytical findings, demonstrating 18.31%\nincrease in sum-rate and an 11.47% improvement in robustness compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u4f18\u5316MIMO\u7cfb\u7edf\u6027\u80fd\uff0c\u4eff\u771f\u663e\u793a\u548c\u901f\u7387\u63d0\u534718.31%\uff0c\u9c81\u68d2\u6027\u63d0\u9ad811.47%\u3002", "motivation": "\u5229\u7528\u6df7\u5408\u9884\u7f16\u7801\u6280\u672f\u5145\u5206\u6316\u6398\u591a\u7528\u6237\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u4f18\u5316\u6570\u636e\u4f20\u8f93\u5e76\u6291\u5236\u65c1\u74e3\u5e72\u6270\u3002", "method": "\u7ed3\u5408\u6570\u5b57\u548c\u6a21\u62df\u9884\u7f16\u7801\uff0c\u9996\u6b21\u5b9a\u4e49\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6765\u6d4b\u91cf\u65e0\u7ebf\u4fe1\u9053\u4e2d\u89d2\u5ea6\u548c\u76f8\u4f4d\u53d8\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u548c\u901f\u7387\u4e0a\u63d0\u5347\u4e8618.31%\uff0c\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8611.47%\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u4f18\u5316\u591a\u7528\u6237\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u548c\u901f\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24098", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.24098", "abs": "https://arxiv.org/abs/2510.24098", "authors": ["Tianyu Zuo", "Xueyan Tang", "Bu Sung Lee", "Jianfei Cai"], "title": "On Competitiveness of Dynamic Replication for Distributed Data Access", "comment": "Extended version of a paper that will appear in ICDCN 2026 conference", "summary": "This paper studies an online cost optimization problem for distributed\nstorage and access. The goal is to dynamically create and delete copies of data\nobjects over time at geo-distributed servers to serve access requests and\nminimize the total storage and network cost. We revisit a recent algorithm in\nthe literature and show that it does not have a competitive ratio of $2$ as\nclaimed by constructing a counterexample. We further prove that no\ndeterministic online algorithm can achieve a competitive ratio bounded by $2$\nfor the general cost optimization problem. We develop an online algorithm and\nprove that it achieves a competitive ratio of $\\max\\{2, \\min\\{\\gamma, 3\\}\\}$,\nwhere $\\gamma$ is the max/min storage cost ratio among all servers. Examples\nare given to confirm the tightness of competitive analysis. We also empirically\nevaluate algorithms using real object access traces.", "AI": {"tldr": "\u672c\u6587\u53cd\u9a73\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u7ade\u4e89\u6bd4\u58f0\u79f0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ade\u4e89\u6bd4\u4e3amax{2, min{\u03b3, 3}}\u7684\u65b0\u5728\u7ebf\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u5b58\u50a8\u548c\u8bbf\u95ee\u4e2d\u7684\u5728\u7ebf\u6210\u672c\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u52a8\u6001\u521b\u5efa\u548c\u5220\u9664\u5730\u7406\u5206\u5e03\u5f0f\u670d\u52a1\u5668\u4e0a\u7684\u6570\u636e\u526f\u672c\uff0c\u4ee5\u6700\u5c0f\u5316\u603b\u5b58\u50a8\u548c\u7f51\u7edc\u6210\u672c\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u4e86\u6587\u732e\u4e2d\u7684\u7b97\u6cd5\uff0c\u6784\u5efa\u53cd\u4f8b\u8bc1\u660e\u5176\u7ade\u4e89\u6bd4\u5e76\u975e\u5982\u58f0\u79f0\u76842\u3002\u8fdb\u4e00\u6b65\u8bc1\u660e\u5bf9\u4e8e\u4e00\u822c\u6210\u672c\u4f18\u5316\u95ee\u9898\uff0c\u6ca1\u6709\u786e\u5b9a\u6027\u5728\u7ebf\u7b97\u6cd5\u80fd\u8fbe\u5230\u7ade\u4e89\u6bd42\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u7ade\u4e89\u6bd4\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684\u5728\u7ebf\u7b97\u6cd5\u5b9e\u73b0\u4e86max{2, min{\u03b3, 3}}\u7684\u7ade\u4e89\u6bd4\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u7d27\u5bc6\u5ea6\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u53cd\u4f8b\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u6307\u51fa\u73b0\u6709\u7b97\u6cd5\u65e0\u6cd5\u8fbe\u5230\u6240\u58f0\u79f0\u7684\u7ade\u4e89\u6bd4\u4e3a2\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u5176\u7ade\u4e89\u6bd4\u4e3amax{2, min{\u03b3, 3}}\uff0c\u5176\u4e2d\u03b3\u662f\u6240\u6709\u670d\u52a1\u5668\u5b58\u50a8\u6210\u672c\u7684\u6700\u5927/\u6700\u5c0f\u6bd4\u3002\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "CountFormer\u901a\u8fc7DINOv2\u548c\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u5b9e\u73b0\u7c7b\u65e0\u5173\u8ba1\u6570\uff0c\u63a5\u8fd1\u4eba\u7c7b\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u611f\u77e5\u89c6\u89c9\u91cd\u590d\u548c\u7ed3\u6784\u5173\u7cfb\u8f7b\u677e\u8ba1\u6570\u591a\u6837\u5bf9\u8c61\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u5f62\u72b6\u3001\u5185\u90e8\u5bf9\u79f0\u6216\u91cd\u53e0\u7ec4\u4ef6\u65f6\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u57fa\u4e8eCounTR\u67b6\u6784\uff0c\u7528\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\u66ff\u6362\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u548c\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u751f\u6210\u5bc6\u5ea6\u56fe\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u7ed3\u6784\u590d\u6742\u6216\u5bc6\u96c6\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6574\u5408\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv2\uff09\u4f7f\u8ba1\u6570\u7cfb\u7edf\u80fd\u591f\u63a5\u8fd1\u4eba\u7c7b\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u8fc8\u5411\u771f\u6b63\u901a\u7528\u4e14\u65e0\u9700\u793a\u4f8b\u7684\u8ba1\u6570\u8303\u5f0f\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u901a\u8fc7\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u53ef\u6709\u6548\u63d0\u5347\u81ea\u4e3b\u5b66\u4e60\u7684\u6613\u7528\u6027\u548c\u53c2\u4e0e\u5ea6\uff0c\u6ee1\u610f\u5ea6\u4e0e\u793e\u4ea4\u611f\u77e5\u76f8\u5173\uff0c\u4e14\u80fd\u9884\u6d4b\u76ee\u6807\u8fbe\u6210\u3002", "motivation": "\u63a2\u7d22\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u540c\u4f34\u5b66\u4e60\u4f34\u4fa3\u5728\u81ea\u4e3b\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u63d0\u4f9b\u4efb\u52a1\u5bfc\u5411\u7684\u76ee\u6807\u63d0\u9192\u548c\u79ef\u6781\u60c5\u611f\u652f\u6301\u65b9\u9762\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u7684Wizard-of-Oz\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u4e86\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u884c\u4e3a\u4e0e\u4ec5\u63d0\u4f9b\u7269\u7406\u5b58\u5728\u7684\u673a\u5668\u4eba\uff08\u5bf9\u7167\u7ec4\uff09\u5bf9\u5b66\u751f\u611f\u77e5\u7126\u70b9\u3001\u751f\u4ea7\u529b\u548c\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u6761\u4ef6\u4e0b\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u66f4\u9ad8\u7684\u6613\u7528\u6027\uff0c\u5176\u4e2d\u76ee\u6807\u63d0\u9192\u6761\u4ef6\u8fd8\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u672a\u6765\u4f7f\u7528\u610f\u613f\u3002\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u7684\u6ee1\u610f\u5ea6\u4e0e\u5176\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u793e\u4f1a\u5b58\u5728\u7684\u611f\u77e5\u76f8\u5173\uff0c\u8fd9\u79cd\u611f\u77e5\u662f\u81ea\u4e3b\u5b66\u4e60\u4efb\u52a1\u4e2d\u76ee\u6807\u8fbe\u6210\u6c34\u5e73\u7684\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u5728\u901a\u8fc7\u529f\u80fd\u548c\u60c5\u611f\u53c2\u4e0e\u652f\u6301\u81ea\u4e3b\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u53ef\u80fd\u6269\u5c55\u6216\u8f6c\u79fb\u5b66\u79d1\u521b\u9020\u529b\uff0c\u5f71\u54cd\u79d1\u5b66\u4ef7\u503c\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u4e0e\u521b\u9020\u6027\u4ea7\u7269\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u8ba1\u7b97\u548cAI\u5982\u4f55\u6269\u5c55\u6216\u8f6c\u79fb\u5b66\u79d1\u521b\u9020\u529b\u3002", "result": "\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u8f6c\u79fb\u521b\u9020\u529b\uff0c\u5f71\u54cd\u79d1\u5b66\u4ef7\u503c\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\u53ef\u80fd\u5bfc\u81f4\u5b66\u79d1\u521b\u9020\u529b\u7684\u8f6c\u79fb\uff0c\u4ece\u800c\u53ef\u80fd\u6539\u53d8\uff08\u751a\u81f3\u524a\u5f31\uff09\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "LLMs\uff08\u5982qwen2.5-coder:32b\uff09\u5728\u7279\u5b9a\u7b56\u7565\u4e0b\u80fd\u5b9e\u73b0\u7cfb\u7edf\u81ea\u4e3b\u4e92\u64cd\u4f5c\u6027\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5355\u4f4d\u8f6c\u6362\uff09\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u9700\u8fdb\u4e00\u6b65\u8de8\u9886\u57df\u8bc4\u4f30\u548c\u53ef\u9760\u6027\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u7cfb\u7edf\u65e5\u76ca\u52a8\u6001\u548c\u5f02\u6784\uff0c\u4e92\u64cd\u4f5c\u6027\u6311\u6218\u52a0\u5267\uff0c\u5f00\u53d1\u4e92\u64cd\u4f5c\u6027\u5de5\u4ef6\u9700\u8981\u5927\u91cf\u65f6\u95f4\u6295\u5165\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u57fa\u4e8eLLM\u7684\u7b56\u7565\u5728\u5b9e\u73b0\u7cfb\u7edf\u8fd0\u884c\u65f6\u81ea\u4e3b\u4e92\u64cd\u4f5c\u6027\uff08\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff09\u7684\u6709\u6548\u6027\u3002", "method": "\u9009\u62e9\u4e8613\u4e2a\u5f00\u6e90LLMs\uff0c\u5e76\u5728\u519c\u4e1a\u4e92\u64cd\u4f5c\u6027\u7528\u4f8b\u4e2d\u6574\u7406\u4e86\u56db\u4e2a\u7248\u672c\u7684\u6570\u636e\u96c6\u3002\u6bcf\u4e2a\u6a21\u578b\u4e0e\u6bcf\u4e2a\u6570\u636e\u96c6\u7248\u672c\u8fdb\u884c\u4e86\u4e09\u6b21\u8fd0\u884c\uff0c\u91c7\u7528\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\uff08DIRECT\u548cCODEGEN\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "result": "qwen2.5-coder:32b\u5728\u4e24\u79cd\u7b56\u7565\uff08DIRECT\u548cCODEGEN\uff09\u4e0b\u8868\u73b0\u6700\u6709\u6548\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u7248\u672c\u4e2d\u7684\u4e09\u4e2a\u7248\u672c\u4e2dDIRECT\u7b56\u7565\u5e73\u5747pass@1\u22650.99\uff0cCODEGEN\u7b56\u7565\u5e73\u5747pass@1\u22650.89\u3002\u5728\u5305\u542b\u5355\u4f4d\u8f6c\u6362\u7684\u7b2c\u56db\u4e2a\u7248\u672c\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u4f7f\u7528DIRECT\u7b56\u7565\u5747\u5931\u8d25\uff0c\u800cCODEGEN\u7b56\u7565\u4e0bqwen2.5-coder:32b\u4ecd\u4ee5\u5e73\u5747pass@1=0.75\u6210\u529f\u3002", "conclusion": "\u90e8\u5206\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u5b9e\u73b0\u7cfb\u7edf\u95f4\u7684\u81ea\u4e3b\u4e92\u64cd\u4f5c\u6027\u3002\u5efa\u8bae\u5728\u4e0d\u540c\u9886\u57df\u8fdb\u4e00\u6b65\u8bc4\u4f30\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u53ef\u9760\u6027\u7b56\u7565\u3002"}}
{"id": "2510.24175", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24175", "abs": "https://arxiv.org/abs/2510.24175", "authors": ["Nitin Shukla", "Alessandro Romeo", "Caterina Caravita", "Michael Redenti", "Radim Vavrik", "Lubomir Riha", "Andrea Mignone", "Marco Rossazza", "Stefano Truzzi", "Luca Tornatore", "Antonio Ragagnin", "Tiago Castro", "Geray S. Karademir", "Klaus Dolag", "Pranab J. Deka", "Fabio Bacchini", "Rostislav-Paul Wilhelm", "Daniele Gregori", "Elisabetta Boella"], "title": "Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System", "comment": null, "summary": "Developing and redesigning astrophysical, cosmological, and space plasma\nnumerical codes for existing and next-generation accelerators is critical for\nenabling large-scale simulations. To address these challenges, the SPACE Center\nof Excellence (SPACE-CoE) fosters collaboration between scientists, code\ndevelopers, and high-performance computing experts to optimize applications for\nthe exascale era. This paper presents our strategy and initial results on the\nLeonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3\nand iPIC3D, using profiling tools to analyze performance on single and multiple\nnodes. Preliminary tests show all three codes scale efficiently, reaching 80%\nscalability up to 1,024 GPUs.", "AI": {"tldr": "SPACE-CoE\u4f18\u5316\u5929\u4f53\u7269\u7406\u6570\u503c\u4ee3\u7801\uff0c\u4e09\u4e2a\u4ee3\u7801\u5728Leonardo\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u663e\u793a\u9ad8\u6548\u6269\u5c55\u81f31,024\u4e2aGPU\u3002", "motivation": "\u4e3a\u5927\u89c4\u6a21\u6a21\u62df\u5f00\u53d1\u5e76\u91cd\u65b0\u8bbe\u8ba1\u9002\u7528\u4e8e\u73b0\u6709\u548c\u4e0b\u4e00\u4ee3\u52a0\u901f\u5668\u7684\u6570\u503c\u4ee3\u7801\uff0c\u4ee5\u6ee1\u8db3\u5929\u4f53\u7269\u7406\u3001\u5b87\u5b99\u5b66\u548c\u7a7a\u95f4\u7b49\u79bb\u5b50\u4f53\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5206\u6790\u5de5\u5177\u5bf9\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u5206\u6790\uff0c\u8bc4\u4f30gPLUTO\u3001OpenGadget3\u548ciPIC3D\u4e09\u4e2a\u4ee3\u7801\u7684\u6027\u80fd\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u663e\u793a\uff0c\u6240\u6709\u4e09\u4e2a\u4ee3\u7801\u57281,024\u4e2aGPU\u4e0a\u53ef\u8fbe\u523080%\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SPACE-CoE\u901a\u8fc7\u4f18\u5316\u5929\u4f53\u7269\u7406\u3001\u5b87\u5b99\u5b66\u548c\u7a7a\u95f4\u7b49\u79bb\u5b50\u4f53\u6570\u503c\u4ee3\u7801\uff0c\u4e3a\u73b0\u6709\u548c\u4e0b\u4e00\u4ee3\u52a0\u901f\u5668\u63d0\u4f9b\u652f\u6301\uff0c\u521d\u6b65\u6d4b\u8bd5\u663e\u793a\u4e09\u4e2a\u65d7\u8230\u4ee3\u7801\u5728Leonardo\u7cfb\u7edf\u4e0a\u5177\u6709\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.24611", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24611", "abs": "https://arxiv.org/abs/2510.24611", "authors": ["Azadeh Pourkabirian", "Amir Masoud Rahmani", "Kai Li", "Wei Ni"], "title": "Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives", "comment": null, "summary": "Delay-sensitive Internet of Things (IoT) applications have drawn significant\nattention. Running many of these applications on IoT devices is challenging due\nto the limited processing resources of these devices and the need for real-time\nresponses. Task offloading can minimize latency by transferring computationally\nintensive tasks from IoT devices to resource-rich edge servers, ensuring delay\nand performance guarantees. In this paper, we develop a task-offloading\napproach for delay-sensitive IoT applications in edge computing environments.\nUnlike existing schemes, we model the task offloading problem as an economic\ndemand and supply model to achieve market balance. The proposed model avoids\nunder- and over-supply, ensuring the computational resources at edge servers\n(supply) are allocated in a manner that best meets the processing and\ncomputational needs of user devices (demand). Given the multi-agent nature of\ntask offloading involving users and service providers with different\npreferences and objectives, we design a game-theoretic framework using a\nVickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions\nand decision-making processes. Additionally, we develop an incentive mechanism\nto encourage both parties to participate in the auction. The mechanism\nmaximizes user task offloading to edge servers and motivates edge servers to\nshare their computational resources, achieving profitability for both IoT users\nand edge servers. Simulations demonstrate our method maximizes social welfare,\nensures truthfulness, maintains market balance, and provides latency guarantees\nfor delay-sensitive IoT applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\u548cVCG\u62cd\u5356\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e38\u620f\u7406\u8bba\u6846\u67b6\u548c\u6fc0\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5e02\u573a\u8d44\u6e90\u5e73\u8861\u548c\u5ef6\u8fdf\u4fdd\u8bc1\u3002", "motivation": "\u7531\u4e8eIoT\u8bbe\u5907\u5904\u7406\u8d44\u6e90\u6709\u9650\u4e14\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\uff0c\u5ef6\u8fdf\u654f\u611f\u7684IoT\u5e94\u7528\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u5177\u6709\u6311\u6218\u6027\u3002\u4efb\u52a1\u5378\u8f7d\u53ef\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\uff0c\u4f46\u9700\u786e\u4fdd\u8d44\u6e90\u5206\u914d\u7684\u6700\u4f73\u5e73\u8861\u3002", "method": "\u8bba\u6587\u5c06\u4efb\u52a1\u5378\u8f7d\u95ee\u9898\u5efa\u6a21\u4e3a\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eVCG\u62cd\u5356\u7684\u6e38\u620f\u7406\u8bba\u6846\u67b6\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u6fc0\u52b1\u673a\u5236\u4ee5\u4fc3\u8fdb\u7528\u6237\u548c\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u53c2\u4e0e\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3001\u786e\u4fdd\u8bda\u5b9e\u6027\u3001\u7ef4\u6301\u5e02\u573a\u5e73\u8861\uff0c\u5e76\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684IoT\u5e94\u7528\u63d0\u4f9b\u5ef6\u8fdf\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\u548cVCG\u62cd\u5356\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u5e02\u573a\u8d44\u6e90\uff0c\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\uff0c\u5e76\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684IoT\u5e94\u7528\u63d0\u4f9b\u5ef6\u8fdf\u4fdd\u8bc1\u3002"}}
{"id": "2510.24621", "categories": ["cs.DS", "cs.CG", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24621", "abs": "https://arxiv.org/abs/2510.24621", "authors": ["Ziyi Fang", "Lingxiao Huang", "Runkai Yang"], "title": "Coreset for Robust Geometric Median: Eliminating Size Dependency on Outliers", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "We study the robust geometric median problem in Euclidean space\n$\\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact\nsummary of a dataset $P$ of size $n$ that approximates the robust cost for all\ncenters $c$ within a multiplicative error $\\varepsilon$. Given an outlier count\n$m$, we construct a coreset of size $\\tilde{O}(\\varepsilon^{-2} \\cdot\n\\min\\{\\varepsilon^{-2}, d\\})$ when $n \\geq 4m$, eliminating the $O(m)$\ndependency present in prior work [Huang et al., 2022 & 2023]. For the special\ncase of $d = 1$, we achieve an optimal coreset size of\n$\\tilde{\\Theta}(\\varepsilon^{-1/2} + \\frac{m}{n} \\varepsilon^{-1})$, revealing\na clear separation from the vanilla case studied in [Huang et al., 2023;\nAfshani and Chris, 2024]. Our results further extend to robust\n$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence\nunder mild data assumptions. The key technical contribution is a novel\nnon-component-wise error analysis, enabling substantial reduction of outlier\ninfluence, unlike prior methods that retain them.Empirically, our algorithms\nconsistently outperform existing baselines in terms of size-accuracy tradeoffs\nand runtime, even when data assumptions are violated across a wide range of\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u7684\u6838\u5fc3\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7814\u7a76\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u9c81\u68d2\u51e0\u4f55\u4e2d\u4f4d\u6570\u95ee\u9898\uff0c\u7279\u522b\u662f\u6838\u5fc3\u96c6\u6784\u5efa\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u5de5\u4f5c\u4e2d\u7684\u5f02\u5e38\u503c\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u91c7\u7528\u975e\u5206\u91cf\u8bef\u5dee\u5206\u6790\u6280\u672f\uff0c\u6784\u5efa\u6838\u5fc3\u96c6\u4ee5\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "\u5728$n \\geq 4m$\u65f6\uff0c\u6838\u5fc3\u96c6\u5927\u5c0f\u4e3a$\\tilde{O}(\\varepsilon^{-2} \\cdot \\min\\{\\varepsilon^{-2}, d\\})$\uff0c\u6d88\u9664\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684$O(m)$\u4f9d\u8d56\u3002\u5bf9\u4e8e$d=1$\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u6838\u5fc3\u96c6\u5927\u5c0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6838\u5fc3\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u51e0\u4f55\u6a21\u578b\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u7684\u6f02\u6d6e\u5783\u573e\u76d1\u6d4b\u7cfb\u7edf\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u6570\u636e\u6cc4\u6f0f\u548c\u6a21\u578b\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u57ce\u5e02\u6c34\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u7684\u4eba\u4e3a\u5783\u573e\u6269\u6563\u5df2\u6210\u4e3a\u7d27\u8feb\u7684\u73af\u5883\u95ee\u9898\uff0c\u5bf9\u751f\u7269\u591a\u6837\u6027\u3001\u6c34\u8d28\u53ca\u4eba\u7c7b\u6d3b\u52a8\uff08\u5982\u822a\u884c\u548c\u5a31\u4e50\uff09\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fa\u5b9a\u539f\u4f4d\u6444\u50cf\u5934\u76d1\u6d4b\u6f02\u6d6e\u5783\u573e\u7684\u65b0\u65b9\u6cd5\u6846\u67b6\uff0c\u5305\u62ec\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u8fde\u7eed\u91cf\u5316\u4e0e\u76d1\u6d4b\uff0c\u4ee5\u53ca\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u65bd\u4e86\u51e0\u4f55\u6a21\u578b\u4ee5\u4ece2D\u56fe\u50cf\u4f30\u8ba1\u7269\u4f53\u7684\u5b9e\u9645\u5927\u5c0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u636e\u96c6\u6784\u5efa\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u8d1f\u6837\u672c\u7684\u6574\u5408\u548c\u65f6\u95f4\u6cc4\u6f0f\u7684\u8003\u8651\u3002\u7814\u7a76\u8fd8\u9a8c\u8bc1\u4e86\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u6295\u5f71\u51e0\u4f55\u4e0e\u56de\u5f52\u4fee\u6b63\uff0c\u5c55\u793a\u4e86\u5ea6\u91cf\u7269\u4f53\u4f30\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u57ce\u5e02\u6c34\u73af\u5883\u76d1\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u4f4e\u6210\u672c\u8f6e\u817f\u56db\u8db3\u673a\u5668\u4eba\u901a\u8fc7\u89c6\u89c9-\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u548c\u7a33\u5065\u5bfc\u822a\uff0c\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\u3002", "motivation": "\u7ed3\u5408\u8f6e\u5f0f\u7684\u9ad8\u6548\u6027\u548c\u817f\u5f0f\u7684\u969c\u788d\u7269\u7a7f\u8d8a\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u6602\u8d35\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\uff0c\u4e14\u7f3a\u4e4f\u8dcc\u5012\u6062\u590d\u529f\u80fd\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u7684\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u8fd0\u52a8\u548c\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u5728\u975e\u89c4\u5219\u5730\u5f62\u4e0a\u4f7f\u7528\u4f4e\u626d\u77e9\u6267\u884c\u5668\u5b9e\u73b0\u7075\u6d3b\u79fb\u52a8\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4ece\u5916\u90e8\u6270\u52a8\u548c\u81ea\u8bf1\u5bfc\u6545\u969c\u4e2d\u6062\u590d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5728\u9884\u7b97\u6709\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u81ea\u4e3b\u5bfc\u822a\u548c\u7a33\u5065\u8fd0\u52a8\u7b56\u7565\u7684\u95e8\u69db\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "ME-POMDPs\u6269\u5c55\u4e86\u6807\u51c6POMDPs\u4ee5\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86AB-POMDPs\u548c\u7b97\u6cd5\u6765\u8ba1\u7b97\u7a33\u5065\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u591a\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u4e0d\u4e00\u81f4\u65f6\uff0c\u5982\u4f55\u627e\u5230\u5355\u4e00\u7b56\u7565\u4ee5\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5956\u52b1\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u610fME-POMDP\u7b80\u5316\u4e3a\u4ec5\u5728\u5176\u8f6c\u79fb\u548c\u5956\u52b1\u51fd\u6570\u6216\u89c2\u5bdf\u548c\u5956\u52b1\u51fd\u6570\u4e0a\u53d8\u5316\u7684ME-POMDP\uff0c\u5e76\u8bbe\u8ba1\u7cbe\u786e\u548c\u8fd1\u4f3c\uff08\u57fa\u4e8e\u70b9\u7684\uff09\u7b97\u6cd5\u6765\u6c42\u89e3AB-POMDPs\u7684\u7a33\u5065\u7b56\u7565\u3002", "result": "\u6210\u529f\u5c06\u6807\u51c6POMDP\u57fa\u51c6\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\uff0c\u5e76\u8ba1\u7b97\u51fa\u7a33\u5065\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86ME-POMDPs\u53ef\u4ee5\u63a8\u5e7f\u5230\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u7684POMDPs\uff08AB-POMDPs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u6765\u8ba1\u7b97\u7a33\u5065\u7b56\u7565\u3002"}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "OXN\u5de5\u5177\u7684\u8b66\u62a5\u6269\u5c55\u8ba9\u5de5\u7a0b\u5e08\u5728\u8bbe\u8ba1\u9636\u6bb5\u6d4b\u8bd5\u548c\u8c03\u6574\u8b66\u62a5\u89c4\u5219\uff0c\u907f\u514d\u8fd0\u884c\u65f6\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u8b66\u62a5\u8bbe\u8ba1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u53ef\u9760\u6027\u5de5\u7a0b\u4e2d\uff0c\u8b66\u62a5\u8bbe\u8ba1\u9762\u4e34\u5e73\u8861\u65e9\u671f\u95ee\u9898\u6355\u6349\u548c\u51cf\u5c11\u8bef\u62a5\u7684\u6311\u6218\uff0c\u4e14\u8b66\u62a5\u4ee3\u7801\u56e0\u8986\u76d6\u7f55\u89c1\u6545\u969c\u800c\u7f3a\u4e4f\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165OXN\u89c2\u6d4b\u5b9e\u9a8c\u5de5\u5177\u7684\u8b66\u62a5\u6269\u5c55\uff0c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u5728\u8bbe\u8ba1\u9636\u6bb5\u8c03\u6574\u89c4\u5219\u5e76\u9a8c\u8bc1\u8b66\u62a5\u7684\u89e6\u53d1\u884c\u4e3a\u3002", "result": "\u901a\u8fc7OXN\u7684\u8b66\u62a5\u6269\u5c55\uff0c\u5de5\u7a0b\u5e08\u53ef\u4ee5\u5728\u8bbe\u8ba1\u65f6\u4f18\u5316\u8b66\u62a5\u89c4\u5219\u5e76\u9a8c\u8bc1\u5176\u884c\u4e3a\uff0c\u63d0\u9ad8\u8b66\u62a5\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86OXN\u5de5\u5177\u7684\u8b66\u62a5\u6269\u5c55\uff0c\u5e2e\u52a9\u5de5\u7a0b\u5e08\u5728\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\u6d4b\u8bd5\u548c\u8c03\u6574\u8b66\u62a5\u89c4\u5219\uff0c\u4ece\u800c\u907f\u514d\u8fd0\u884c\u65f6\u95ee\u9898\u3002"}}
{"id": "2510.24205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24205", "abs": "https://arxiv.org/abs/2510.24205", "authors": ["Telmo Ribeiro", "Jos\u00e9 Proen\u00e7a", "M\u00e1rio Florido"], "title": "CoMPSeT: A Framework for Comparing Multiparty Session Types", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "Concurrent systems are often complex and difficult to design. Choreographic\nlanguages, such as Multiparty Session Types (MPST), allow the description of\nglobal protocols of interactions by capturing valid patterns of interactions\nbetween participants. Many variations of MPST exist, each one with its rather\nspecific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that\nprovides clearer insights over different features in existing MPST. We select a\nrepresentative set of MPST examples and provide mechanisms to combine different\nfeatures and to animate and compare the semantics of concrete examples. CoMPSeT\nis open-source, compiled into JavaScript, and can be directly executed from any\nbrowser, becoming useful both for researchers who want to better understand the\nlandscape of MPST and for teachers who want to explain global choreographies.", "AI": {"tldr": "CoMPSeT \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u6559\u5e08\u901a\u8fc7\u52a8\u753b\u548c\u6bd4\u8f83\u7406\u89e3\u591a\u515a\u4f1a\u8bdd\u7c7b\u578b\uff08MPST\uff09\u7684\u4e0d\u540c\u7279\u6027\u3002", "motivation": "\u5e76\u53d1\u7cfb\u7edf\u8bbe\u8ba1\u590d\u6742\uff0cMPST \u7b49\u7f16\u6392\u8bed\u8a00\u867d\u80fd\u63cf\u8ff0\u5168\u5c40\u4ea4\u4e92\u534f\u8bae\uff0c\u4f46\u73b0\u6709 MPST \u53d8\u4f53\u5404\u5177\u7279\u6027\u4e14\u590d\u6742\uff0c\u9700\u66f4\u6e05\u6670\u7684\u5de5\u5177\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u8fd9\u4e9b\u7279\u6027\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u4ee3\u8868\u6027\u7684 MPST \u793a\u4f8b\uff0c\u63d0\u4f9b\u673a\u5236\u4ee5\u7ed3\u5408\u4e0d\u540c\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u753b\u548c\u6bd4\u8f83\u5177\u4f53\u793a\u4f8b\u7684\u8bed\u4e49\u6765\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u4e86 CoMPSeT \u5de5\u5177\uff0c\u80fd\u591f\u7ed3\u5408\u4e0d\u540c MPST \u7279\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u753b\u548c\u6bd4\u8f83\u5e2e\u52a9\u7406\u89e3\u548c\u6559\u5b66\u3002", "conclusion": "CoMPSeT \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7f16\u8bd1\u4e3a JavaScript\uff0c\u53ef\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u6267\u884c\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u6559\u5e08\u63d0\u4f9b\u4e86\u7406\u89e3\u548c\u6559\u6388\u591a\u515a\u4f1a\u8bdd\u7c7b\u578b\uff08MPST\uff09\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow\u662f\u4e00\u79cd\u7269\u7406\u611f\u77e5\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6761\u4ef6\u67b6\u6784\u548c\u591a\u65b9\u9762\u635f\u5931\u51fd\u6570\u786e\u4fddOOD\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u5f71\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9065\u611f\u5f71\u50cf\u8d85\u5206\u8fa8\u7387\u5728\u5206\u5e03\u5916\u6761\u4ef6\u4e0b\u7684\u5931\u8d25\u95ee\u9898\uff0c\u5373\u4ea7\u751f\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u7269\u7406\u4e0a\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\u3002", "method": "RareFlow\u91c7\u7528\u53cc\u6761\u4ef6\u67b6\u6784\uff0c\u5305\u62ecGated ControlNet\u548c\u6587\u672c\u63d0\u793a\uff0c\u4ee5\u53ca\u591a\u65b9\u9762\u7684\u635f\u5931\u51fd\u6570\u6765\u786e\u4fdd\u8f93\u51fa\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728\u76f2\u8bc4\u4e2d\uff0cRareFlow\u7684\u8f93\u51fa\u63a5\u8fd1\u5730\u9762\u5b9e\u51b5\u56fe\u50cf\u7684\u771f\u5b9e\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9a\u91cf\u6307\u6807\uff08\u5982FID\uff09\u4e5f\u6709\u8fd140%\u7684\u63d0\u5347\u3002", "conclusion": "RareFlow\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u6570\u636e\u7a00\u7f3a\u79d1\u5b66\u9886\u57df\u4e2d\u8fdb\u884c\u9ad8\u4fdd\u771f\u5408\u6210\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u5e76\u4e3a\u5728\u4e25\u91cd\u9886\u57df\u504f\u79fb\u4e0b\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "An adaptive keyframe selection method improves 3D reconstruction in dynamic scenes by combining error-based and momentum-based modules, outperforming static strategies.", "motivation": "To address the data bottleneck in real-time perception by dynamically selecting the most informative frames for high-quality 3D world representations.", "method": "The method integrates an error-based selection module (using photometric and SSIM errors) and a momentum-based update module to dynamically adjust keyframe selection thresholds.", "result": "Experimental results show consistent improvements in reconstruction quality on Spann3r and CUT3R networks, with ablation studies confirming the effectiveness of each component.", "conclusion": "The proposed adaptive keyframe selection method significantly improves 3D scene reconstruction quality in dynamic environments, outperforming traditional static strategies."}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8c03\u4f18\u7684Transformer\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u8d28\u8c31\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u672a\u77e5\u5316\u5408\u7269\u7684\u9274\u5b9a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u96be\u4ee5\u9274\u5b9a\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u672a\u6536\u5f55\u7684\u5316\u5408\u7269\u3002", "method": "\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\u6280\u672f\u589e\u5f3a\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\u3002", "result": "\u5728NPLIB1\u548cMassSpecGym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u8d85\u8d8aDiffMS\u65b9\u6cd5100%\u548c20%\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u5728MassSpecGym\u4e0a\u76f8\u5bf9\u4f20\u7edf\u5fae\u8c03\u670962%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8c03\u4f18\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4e3a\u672a\u77e5\u5316\u5408\u7269\u7684\u9274\u5b9a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u95f4\u4ea7\u7269\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u8f6f\u4ef6\u5de5\u7a0b\u7684\u7ed3\u6784\u5316\u5b9e\u8df5\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u6846\u67b6\uff0c\u5c06\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e2d\u95f4\u4ea7\u7269\uff08\u5982\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u548c\u4f2a\u4ee3\u7801\uff09\u7eb3\u5165\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6b63\u786e\u6027\uff08\u6700\u9ad875%\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982CodeBLEU\u6539\u8fdb\u8fbe34.3%\uff09\u3002\u5f00\u6e90\u6a21\u578b\u5728\u6846\u67b6\u5fae\u8c03\u540e\u751a\u81f3\u8d85\u8d8a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4e2d\u95f4\u4ea7\u7269\uff08\u5982\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u548c\u4f2a\u4ee3\u7801\uff09\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2510.24452", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24452", "abs": "https://arxiv.org/abs/2510.24452", "authors": ["Xi Cheng", "Weijie Shen", "Haoming Chen", "Chaoyi Shen", "Jean Ortega", "Jiashang Liu", "Steve Thomas", "Honglin Zheng", "Haoyun Wu", "Yuxiang Li", "Casey Lichtendahl", "Jenny Ortiz", "Gang Liu", "Haiyang Qi", "Omid Fatemieh", "Chris Fry", "Jing Jing Long"], "title": "ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery", "comment": null, "summary": "Time series forecasting and anomaly detection are common tasks for\npractitioners in industries such as retail, manufacturing, advertising and\nenergy. Two unique challenges stand out: (1) efficiently and accurately\nforecasting time series or detecting anomalies in large volumes automatically;\nand (2) ensuring interpretability of results to effectively incorporate\nbusiness insights. We present ARIMA_PLUS, a novel framework to overcome these\ntwo challenges by a unique combination of (a) accurate and interpretable time\nseries models and (b) scalable and fully managed system infrastructure. The\nmodel has a sequential and modular structure to handle different components of\nthe time series, including holiday effects, seasonality, trend, and anomalies,\nwhich enables high interpretability of the results. Novel enhancements are made\nto each module, and a unified framework is established to address both\nforecasting and anomaly detection tasks simultaneously. In terms of accuracy,\nits comprehensive benchmark on the 42 public datasets in the Monash forecasting\nrepository shows superior performance over not only well-established\nstatistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer\nneural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms\nof infrastructure, it is directly built into the query engine of BigQuery in\nGoogle Cloud. It uses a simple SQL interface and automates tedious\ntechnicalities such as data cleaning and model selection. It automatically\nscales with managed cloud computational and storage resources, making it\npossible to forecast 100 million time series using only 1.5 hours with a\nthroughput of more than 18000 time series per second. In terms of\ninterpretability, we present several case studies to demonstrate time series\ninsights it generates and customizability it offers.", "AI": {"tldr": "ARIMA_PLUS\u7ed3\u5408\u6a21\u5757\u5316\u6a21\u578b\u4e0e\u4e91\u57fa\u7840\u8bbe\u65bd\uff0c\u9ad8\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u81ea\u52a8\u9884\u6d4b/\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u6311\u6218\uff0c\u5e76\u786e\u4fdd\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u878d\u5165\u4e1a\u52a1\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u7ed3\u6784\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u4e2a\u7ec4\u4ef6\uff08\u5982\u8282\u5047\u65e5\u6548\u5e94\u3001\u5b63\u8282\u6027\u3001\u8d8b\u52bf\u548c\u5f02\u5e38\uff09\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u6a21\u5757\u8fdb\u884c\u521b\u65b0\u589e\u5f3a\uff0c\u540c\u65f6\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u4ee5\u540c\u65f6\u5904\u7406\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u572842\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARIMA_PLUS\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982ETS\u3001ARIMA\uff09\u548c\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5982DeepAR\u3001N-BEATS\uff09\u3002\u57fa\u7840\u8bbe\u65bd\u4e0a\uff0c\u901a\u8fc7Google Cloud\u7684BigQuery\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\uff0c\u652f\u6301\u6bcf\u79d218000\u6761\u65f6\u95f4\u5e8f\u5217\u7684\u5904\u7406\u3002", "conclusion": "ARIMA_PLUS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u4e0e\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u4e24\u5927\u6311\u6218\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u6746\u7684\u901a\u7528\u6a21\u578b\uff0c\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bbe\u8ba1\u7684\u9650\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u9002\u7528\u6027\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5236\u7ed3\u5408\u4e86\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5b83\u4eec\u5404\u81ea\u7684\u9650\u5236\uff0c\u5982\u81ea\u7531\u5ea6\u53d7\u9650\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u6746\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21n\u4e2a\u540c\u5fc3\u7ba1\uff08\u6bcf\u4e2a\u7ba1\u7531mi\u808c\u8171\u9a71\u52a8\uff09\u7684\u4e00\u822c\u60c5\u51b5\uff0c\u5141\u8bb8\u6bcf\u4e2a\u7ba1\u5728\u5f2f\u66f2\u65f6\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u540c\u65f6\u5f3a\u5236\u5171\u4eab\u4e2d\u5fc3\u7ebf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u603b\u957f\u5ea6<4%\u7684\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u73b0\u6709\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u6700\u5927\u5c16\u7aef\u504f\u5dee\u4fdd\u6301\u5728\u603b\u957f\u5ea6\u76845%\u5de6\u53f3\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7cbe\u786e\u7684\u5f62\u72b6\u4f30\u8ba1\u548c\u9ad8\u7ea7\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u7684\u521b\u9020\u529b\uff0c\u4e13\u5bb6\u8bc4\u5ba1\u7ed3\u679c\u663e\u793aAI\u80fd\u521b\u9020\u51fa\u5177\u6709\u7f8e\u5b66\u548c\u521b\u610f\u4ef7\u503c\u7684\u8c1c\u9898\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u662f\u5426\u80fd\u591f\u521b\u9020\u51fa\u5177\u6709\u7f8e\u5b66\u4ef7\u503c\u548c\u521b\u65b0\u6027\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u5177\u6709\u7279\u5b9a\u7f8e\u5b66\u548c\u521b\u610f\u7279\u5f81\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u5bf9AI\u751f\u6210\u7684\u8c1c\u9898\u7ed9\u4e88\u4e86\u9ad8\u5ea6\u8bc4\u4ef7\uff0c\u8ba4\u53ef\u4e86\u5176\u5728\u521b\u9020\u529b\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86AI\u5728\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u548c\u53cd\u76f4\u89c9\u89e3\u51b3\u65b9\u6848\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u65b9\u9762\u7684\u521b\u9020\u529b\u3002"}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u4ea7ML\u7cfb\u7edf\u65e0\u58f0\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u7126\u70b9\u5c0f\u7ec4\u5b9e\u8bc1\u5206\u6790\u5b9e\u8df5\u8005\u7684\u53ef\u89c2\u6d4b\u6027\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u5de5\u5177\u548c\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7ecf\u5e38\u65e0\u58f0\u5931\u8d25\uff08\u5373\u4f5c\u51fa\u9519\u8bef\u51b3\u7b56\u800c\u975e\u5d29\u6e83\uff09\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5b9e\u8df5\u8005\u5b9e\u9645\u6355\u83b7\u54ea\u4e9b\u4fe1\u606f\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e03\u4e2a\u7126\u70b9\u5c0f\u7ec4\u4f1a\u8bae\uff0c\u7814\u7a76\u591a\u4e2a\u9886\u57df\u7684ML\u5b9e\u8df5\u8005\uff0c\u7cfb\u7edf\u8bb0\u5f55\u4e86\u4ed6\u4eec\u6355\u83b7\u7684\u4fe1\u606f\u53ca\u5176\u4f7f\u7528\u65b9\u5f0f\u3002", "result": "\u7814\u7a76\u8be6\u7ec6\u5217\u51fa\u4e86\u5b9e\u8df5\u8005\u7cfb\u7edf\u6355\u83b7\u7684\u4fe1\u606f\u53ca\u5176\u5728\u9a8c\u8bc1\u6a21\u578b\u3001\u68c0\u6d4b\u548c\u8bca\u65ad\u6545\u969c\u4ee5\u53ca\u89e3\u91ca\u6027\u80fd\u9000\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\u4e86\u5f53\u524d\u673a\u5668\u5b66\u4e60\u53ef\u89c2\u6d4b\u6027\u5b9e\u8df5\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u5de5\u5177\u8bbe\u8ba1\u548c\u7814\u7a76\u65b9\u5411\u7684\u5efa\u8bae\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316CLIP\u7684\u89c6\u89c9\u533a\u5206\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8eCLIP\u6a21\u578b\u7684\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u5bc6\u96c6\u9884\u6d4b\u6240\u9700\u7684\u50cf\u7d20\u7ea7\u89c6\u89c9\u7406\u89e3\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a\u8bed\u4e49\u7a7a\u95f4\u91cd\u52a0\u6743\u3001\u9009\u62e9\u6027\u5934\u90e8\u589e\u5f3a\u548c\u5f02\u5e38\u4ee4\u724c\u66ff\u6362\uff0c\u4ee5\u6062\u590d\u89c6\u89c9\u533a\u5206\u80fd\u529b\u5e76\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "result": "\u57288\u4e2a\u5e38\u89c1\u7684\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLHT-CLIP\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "LHT-CLIP\u901a\u8fc7\u7cfb\u7edf\u5730\u5229\u7528CLIP\u5728\u4e0d\u540c\u5c42\u6b21\uff08\u5c42\u3001\u5934\u3001\u4ee4\u724c\uff09\u4e0a\u7684\u89c6\u89c9\u533a\u5206\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u8f6f\u673a\u5668\u4eba\u624b\u6307\u901a\u8fc7\u81ea\u9002\u5e94\u626d\u66f2\u53d8\u5f62\u548c\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u5bc6\u96c6\u7269\u4f53\u4e2d\u5355\u4e2a\u7269\u4f53\u7684\u9ad8\u6548\u6293\u53d6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u5728\u5bc6\u96c6\u5806\u653e\u7684\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\uff0c\u8f6f\u624b\u6307\u9700\u8981\u5177\u5907\u5e73\u9762\u5185\u5916\u7684\u81ea\u9002\u5e94\u626d\u66f2\u53d8\u5f62\u529f\u80fd\u3002", "method": "\u91c7\u7528\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u6e90\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u66f2\u53d8\u5f62\u548c\u6293\u53d6\u529b\u63a7\u5236\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u5143\u5206\u6790\uff08FEA\uff09\u4f18\u5316\u8bbe\u8ba1\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u8f6f\u624b\u6307\u5728\u591a\u79cd\u7269\u4f53\u4e0a\u7684\u6293\u53d6\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8f6f\u673a\u5668\u4eba\u624b\u6307\u901a\u8fc7\u81ea\u9002\u5e94\u626d\u66f2\u53d8\u5f62\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u7684\u5305\u88f9\u6293\u53d6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u7269\u4f53\u4e0a\u7684\u6293\u53d6\u6548\u679c\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u5339\u914d\u7ec4\u7ec7\u5f62\u6001\u5b66\u7684\u590d\u6742\u6027\u3002", "motivation": "\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u63ed\u793a\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u5e94\u7528\u4e2d\u7684\u4e03\u4e2a\u6839\u672c\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u51fa\u4e03\u4e2a\u5bfc\u81f4\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u7684\u672c\u8d28\u4e0d\u5339\u914d\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u601d\u8003\u8fd9\u4e00\u8303\u5f0f\u3002"}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u8f6f\u4ef6\u5728\u957f\u671f\u8fd0\u884c\u4e2d\u4f1a\u51fa\u73b0\u8001\u5316\u73b0\u8c61\uff0c\u8868\u73b0\u4e3a\u5185\u5b58\u589e\u957f\u3001\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u63a2\u8ba8LLM\u751f\u6210\u8f6f\u4ef6\u5728\u6301\u7eed\u6267\u884c\u4e0b\u7684\u957f\u671f\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u8f6f\u4ef6\u8001\u5316\u73b0\u8c61\u3002", "method": "\u4f7f\u7528Bolt\u5e73\u53f0\u548cBaxbench\u6807\u51c6\u5316\u63d0\u793a\u751f\u6210\u56db\u4e2a\u9762\u5411\u670d\u52a1\u7684\u5e94\u7528\uff0c\u5e76\u8fdb\u884c50\u5c0f\u65f6\u7684\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u6301\u7eed\u76d1\u63a7\u8d44\u6e90\u4f7f\u7528\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u541e\u5410\u91cf\u3002", "result": "\u6240\u6709\u5e94\u7528\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u8001\u5316\u73b0\u8c61\uff0c\u5305\u62ec\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u7edf\u8ba1\u5206\u6790\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u8d8b\u52bf\uff0c\u5e76\u663e\u793a\u4e0d\u540c\u5e94\u7528\u7c7b\u578b\u8001\u5316\u4e25\u91cd\u7a0b\u5ea6\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u52a8\u751f\u6210\u7684\u8f6f\u4ef6\u9700\u8981\u8003\u8651\u8001\u5316\u73b0\u8c61\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u7f13\u89e3\u7b56\u7565\u548c\u957f\u671f\u53ef\u9760\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride\u662f\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u5316\u6280\u672f\u751f\u6210\u8fde\u8d2f\u7684\u573a\u666f\u7ea7\u5b57\u5e55\uff0c\u5728\u6307\u6807\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u573a\u666f\u7ea7\u5b57\u5e55\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u4ee5\u652f\u6301\u7a0b\u5e8f\u6027\u5b66\u4e60\u548c\u591a\u6a21\u6001\u63a8\u7406\u3002\u82e5\u5b57\u5e55\u672a\u80fd\u6355\u6349\u8fd9\u79cd\u7ed3\u6784\uff0c\u53ef\u80fd\u5bfc\u81f4\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u5f71\u54cd\u6559\u5b66\u6548\u679c\u3002DynaStride\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DynaStride\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u5316\u6280\u672f\u6355\u6349\u573a\u666f\u5185\u7684\u5173\u952e\u8fc7\u6e21\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u8fc7\u7a0b\u751f\u6210\u591a\u4e2a\u52a8\u4f5c-\u5bf9\u8c61\u5bf9\uff0c\u5e76\u5229\u7528\u52a8\u6001\u6b65\u957f\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u4f18\u5316\u548c\u878d\u5408\u8fd9\u4e9b\u5bf9\u3002", "result": "\u4e0eVLLaMA3\u548cGPT-4o\u7b49\u57fa\u7ebf\u76f8\u6bd4\uff0cDynaStride\u5728N-gram\u6307\u6807\uff08BLEU\u3001METEOR\uff09\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff08BERTScore\u3001CLIPScore\uff09\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\uff0c\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u91cf\u4e0a\u66f4\u4f73\u3002", "conclusion": "DynaStride\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u5316\u6280\u672f\uff0c\u7ed3\u5408\u52a8\u6001\u6b65\u957f\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\uff0c\u751f\u6210\u4e86\u66f4\u5177\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u6027\u7684\u573a\u666f\u7ea7\u5b57\u5e55\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6559\u5b66\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u56de\u987e\u4e86\u57fa\u4e8e3D\u9ad8\u65af\u55b7\u7ed8\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5cSLAM\u6280\u672f\uff0c\u5206\u6790\u4e86\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u55b7\u7ed8\uff083DGS\uff09\u4f5c\u4e3a\u663e\u5f0f\u573a\u666f\u8868\u793a\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u63d0\u4f9b\u4e86\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6e32\u67d3\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e94\u7528\u65f6\u9762\u4e34\u5168\u5c40\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u7ba1\u7406\u548c\u5f02\u6784\u6570\u636e\u878d\u5408\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u3001\u8bed\u4e49\u84b8\u998f\u3001\u878d\u5408\u4e0e\u4f4d\u59ff\u4f18\u5316\u4ee5\u53ca\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u5173\u952e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u91cf\u5316\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u67b6\u6784\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5cSLAM\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u6307\u51fa\u4e86\u591a\u673a\u5668\u4eba\u534f\u4f5cSLAM\u9886\u57df\u7684\u5f00\u653e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u7ec8\u8eab\u6620\u5c04\u3001\u8bed\u4e49\u5173\u8054\u4e0e\u6620\u5c04\u3001\u591a\u6a21\u578b\u9c81\u68d2\u6027\u4ee5\u53ca\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u79cd\u7ed3\u5408\u524d\u77bb\u6027\u5206\u89e3\u3001\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u56e0\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u91cd\u590d\u5931\u8d25\u5faa\u73af\u7b49\u95ee\u9898\u3002", "method": "ReCAP\u7ed3\u5408\u4e86\u4e09\u79cd\u673a\u5236\uff1a(i) \u524d\u77bb\u6027\u5206\u89e3\uff0c(ii) \u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\uff0c(iii) \u5185\u5b58\u9ad8\u6548\u6267\u884c\u3002", "result": "\u5728\u591a\u4e2a\u957f\u65f6\u7a0b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReCAP\u663e\u8457\u63d0\u5347\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5982Robotouille\u4efb\u52a1\u4e2d\u5206\u522b\u53d6\u5f9732%\u548c29%\u7684\u6539\u8fdb\u3002", "conclusion": "ReCAP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u524d\u77bb\u6027\u5206\u89e3\u3001\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "MAGNET\u662f\u4e00\u79cd\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u6574\u5408AST\u3001CFG\u548cDFG\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u8868\u793a\uff08\u5982AST\u3001CFG\u3001DFG\uff09\uff0c\u4ec5\u6355\u83b7\u4ee3\u7801\u8bed\u4e49\u7684\u90e8\u5206\u65b9\u9762\uff1b\u6df7\u5408\u65b9\u6cd5\u7684\u878d\u5408\u7b56\u7565\u901a\u5e38\u662f\u624b\u5de5\u8bbe\u8ba1\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "MAGNET\u7ed3\u5408\u4e86\u6b8b\u5dee\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u8282\u70b9\u7ea7\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f15\u5165\u95e8\u63a7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7ec6\u7c92\u5ea6\u56fe\u95f4\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528Set2Set\u6c60\u5316\u878d\u5408\u591a\u56fe\u5d4c\u5165\u3002", "result": "\u5728BigCloneBench\u548cGoogle Code Jam\u6570\u636e\u96c6\u4e0a\uff0cMAGNET\u5206\u522b\u8fbe\u523096.5%\u548c99.2%\u7684F1\u5206\u6570\u3002", "conclusion": "MAGNET\u901a\u8fc7\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86AST\u3001CFG\u548cDFG\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0cF1\u5206\u6570\u5206\u522b\u8fbe\u523096.5%\u548c99.2%\u3002"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D \u7ed3\u5408\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e0e\u56fe\u50cf\u52303D\u65b9\u6cd5\uff0c\u63d0\u5347\u8096\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u8096\u50cf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\u3001\u7ec6\u8282\u4e0d\u8db3\u548c\u8eab\u4efd\u4fdd\u7559\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u800c\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f3D\u57fa\u7840\u3002TurboPortrait3D\u65e8\u5728\u878d\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u524d\u9988\u5f0f\u7684\u56fe\u50cf\u52303D\u751f\u6210\u6d41\u7a0b\uff0c\u9996\u5148\u83b7\u53d6\u521d\u59cb3D\u8868\u793a\u548c\u566a\u58f0\u6e32\u67d3\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u5355\u6b65\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u7ec6\u5316\u3002\u8bad\u7ec3\u7b56\u7565\u5305\u62ec\u5728\u5927\u89c4\u6a21\u5408\u6210\u591a\u89c6\u89d2\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u968f\u540e\u5728\u9ad8\u8d28\u91cf\u771f\u5b9e\u56fe\u50cf\u4e0a\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTurboPortrait3D\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8096\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u3002", "conclusion": "TurboPortrait3D \u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u73b0\u6709\u7684\u56fe\u50cf\u52303D\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8096\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e863D\u611f\u77e5\u80fd\u529b\u548c\u4f4e\u5ef6\u8fdf\u8fd0\u884c\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u9009\u62e9\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5b89\u5168\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u65b0\u5730\u5f62\u6cdb\u5316\u65f6\u3002", "method": "VOCALoco\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6267\u884c\u5b89\u5168\u6027\u548c\u8fd0\u8f93\u6210\u672c\u6765\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\uff0cVOCALoco\u5728\u697c\u68af\u884c\u8d70\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7aef\u5230\u7aefDRL\u7b56\u7565\u3002", "conclusion": "VOCALoco\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u697c\u68af\u884c\u8d70\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "LLM\u667a\u80fd\u4f53\u5728\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4fe1\u606f\u548c\u63d0\u793a\u8bbe\u8ba1\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5206\u6563\u6761\u4ef6\u4e0b\u591a\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u534f\u8c03\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5730\u6bd4\u8f83\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u5f97\u5230\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u7ed3\u6784\u8bbe\u8ba1\u5f97\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u63a5\u8fd1\u6700\u4f18\u89e3\u5e76\u8d85\u8d8a\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "\u8c03\u67e5\u53d1\u73b0\uff0c\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u867d\u63d0\u5347\u901f\u5ea6\uff0c\u4f46\u6574\u4f53\u751f\u4ea7\u529b\u6539\u5584\u6709\u9650\uff0c\u5b58\u5728\u751f\u4ea7\u529b\u6096\u8bba\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u751f\u4ea7\u529b\u5f71\u54cd\uff0c\u4ee5\u660e\u786e\u5176\u5728\u54ea\u4e9b\u65b9\u9762\u80fd\u6709\u6548\u63d0\u5347\u751f\u4ea7\u529b\u3002", "method": "\u901a\u8fc7\u8c03\u67e5415\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\uff0c\u91c7\u7528SPACE\u6846\u67b6\uff08\u6ee1\u610f\u5ea6\u4e0e\u5e78\u798f\u611f\u3001\u7ee9\u6548\u3001\u6d3b\u52a8\u3001\u6c9f\u901a\u4e0e\u5408\u4f5c\u3001\u6548\u7387\u4e0e\u6d41\u7545\u6027\uff09\u6765\u6355\u6349\u4ed6\u4eec\u5bf9AI\u8f85\u52a9\u5f00\u53d1\u751f\u4ea7\u529b\u53d8\u5316\u7684\u611f\u77e5\u3002", "result": "\u7ed3\u679c\u6309AI\u4f7f\u7528\u9891\u7387\u5206\u7c7b\u663e\u793a\uff0c\u6574\u4f53\u751f\u4ea7\u529b\u53d8\u5316\u6709\u9650\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u901f\u5ea6\u63d0\u5347\u4f46\u8f6f\u4ef6\u8d28\u91cf\u6216\u6ee1\u610f\u5ea6\u672a\u5fc5\u63d0\u9ad8\u7684\u751f\u4ea7\u529b\u6096\u8bba\u3002", "conclusion": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f5c\u4e3a\u751f\u4ea7\u529b\u8f85\u52a9\u5de5\u5177\u7684\u91c7\u7528\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u5b9e\u9645\u63d0\u5347\u751f\u4ea7\u529b\u7684\u5177\u4f53\u60c5\u51b5\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u8868\u660e\uff0cAI\u7684\u91c7\u7528\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u4e0d\u540c\u7ef4\u5ea6\u5f71\u54cd\u6709\u9650\uff0c\u63ed\u793a\u4e86\u751f\u4ea7\u529b\u6096\u8bba\uff0c\u5373\u5f00\u53d1\u8005\u901f\u5ea6\u63d0\u5347\u4f46\u8f6f\u4ef6\u8d28\u91cf\u6216\u6ee1\u610f\u5ea6\u672a\u5fc5\u63d0\u9ad8\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS\u901a\u8fc7\u5e73\u9762\u548c\u51e0\u4f55\u5148\u9a8c\u4f18\u53163D\u9ad8\u65af\u6e85\u5c04\uff0c\u663e\u8457\u63d0\u5347\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04(3DGS)\u5728\u4f4e\u7eb9\u7406\u5ba4\u5185\u573a\u666f\u4e2d\u56e0\u5149\u5ea6\u635f\u5931\u5bfc\u81f4\u7684\u51e0\u4f55\u6a21\u7cca\u548c\u91cd\u5efa\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86Language-Prompted Planar Priors (LP3)\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u878d\u5408\u548c\u51e0\u4f55\u5148\u9a8c\u4f18\u5316\u533a\u57df\u63d0\u6848\u30023D\u9ad8\u65af\u901a\u8fc7\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\u9879\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlanarGS\u91cd\u5efa\u4e86\u51c6\u786e\u4e14\u8be6\u7ec6\u76843D\u8868\u9762\uff0c\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PlanarGS\u901a\u8fc7\u5f15\u5165\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\u9879\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4f4e\u7eb9\u7406\u5ba4\u5185\u573a\u666f\u4e2d\u76843D\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "A 3D BVC model with vertical angular sensitivity improves spatial localization in complex environments while maintaining 2D performance in simpler cases.", "motivation": "Most computational BVC models are limited to 2D environments, leading to spatial ambiguities in horizontally symmetric environments.", "method": "The proposed model incorporates vertical angular sensitivity into the BVC framework and processes LiDAR data to capture vertical contours.", "result": "The 3D model matches 2D performance in minimal vertical variation environments and outperforms it in complex 3D scenarios with more distinct place fields and reduced spatial aliasing.", "conclusion": "Adding a vertical dimension to BVC-based localization enhances navigation and mapping in 3D spaces while maintaining performance in simpler, 2D scenarios."}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM\u5f00\u53d1\u7684\u901a\u7528\u667a\u80fd\u4ee3\u7406CUGA\u5728\u4f01\u4e1a\u8bd5\u70b9\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u51cf\u5c11\u5f00\u53d1\u6210\u672c\u548c\u65f6\u95f4\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4f01\u4e1a\u7ea7\u7cfb\u7edf\u7684\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34\u4ece\u539f\u578b\u5230\u90e8\u7f72\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u901a\u7528\u667a\u80fd\u4ee3\u7406\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f01\u4e1a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5177\u6709\u575a\u5b9e\u7684\u5206\u6790\u57fa\u7840\uff0c\u5e76\u5728AppWorld\u548cWebArena\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "result": "CUGA\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u7684\u4eba\u624d\u62db\u8058\u9886\u57df\u8bd5\u70b9\u4e2d\u8868\u73b0\u63a5\u8fd1\u4e13\u4e1a\u5316\u4ee3\u7406\u7684\u51c6\u786e\u6027\uff0c\u5e76\u663e\u793a\u51fa\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u7684\u6f5c\u529b\u3002", "conclusion": "CUGA\u4f5c\u4e3a\u901a\u7528\u667a\u80fd\u4ee3\u7406\u5728\u4f01\u4e1a\u548c\u5b66\u672f\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u589e\u5f3a\u5176\u4f01\u4e1a\u7ea7\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u6210\u719f\u5ea6\u3002"}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "PRDBench\u662f\u4e00\u4e2a\u65b0\u578b\u57fa\u51c6\uff0c\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7ba1\u9053\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u9ad8\u6210\u672c\u548c\u50f5\u5316\u95ee\u9898\uff0c\u5305\u542b50\u4e2aPython\u9879\u76ee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8bc4\u4f30\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u4e13\u4e1a\u77e5\u8bc6\u8981\u6c42\uff0c\u4ee5\u53ca\u4e3b\u8981\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u7684\u50f5\u5316\u8bc4\u4f30\u6307\u6807\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u76d1\u7763\u7684\u4ee3\u7406\u9a71\u52a8\u57fa\u51c6\u6784\u5efa\u7ba1\u9053\uff0c\u751f\u6210\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u7684\u9879\u76ee\u7ea7\u4efb\u52a1\uff0c\u5e76\u5f15\u5165Agent-as-a-Judge\u8303\u5f0f\u8fdb\u884c\u8bc4\u5206\u3002", "result": "PRDBench\u5305\u542b50\u4e2a\u8de820\u4e2a\u9886\u57df\u7684\u771f\u5b9ePython\u9879\u76ee\uff0c\u6bcf\u4e2a\u9879\u76ee\u90fd\u6709\u7ed3\u6784\u5316PRD\u9700\u6c42\u3001\u5168\u9762\u8bc4\u4f30\u6807\u51c6\u548c\u53c2\u8003\u5b9e\u73b0\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "PRDBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u6307\u6807\u50f5\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u526a\u679d\u548c\u8f93\u5165\u9891\u7387\u5bc6\u96c6\u5316\u4f18\u5316INR\u67b6\u6784\uff0c\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8f93\u5165\u9891\u7387\u9009\u62e9\u548c\u67b6\u6784\u7ba1\u7406\u4e0a\u4f9d\u8d56\u542f\u53d1\u5f0f\u548c\u7e41\u91cd\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff0cAIRe\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u8bad\u7ec3\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AIRe\u91c7\u7528\u795e\u7ecf\u5143\u526a\u679d\u673a\u5236\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u8f93\u5165\u9891\u7387\u5bc6\u96c6\u5316\u63d0\u5347\u8868\u793a\u80fd\u529b\u3002\u526a\u679d\u9636\u6bb5\u8bc6\u522b\u8d21\u732e\u8f83\u5c0f\u7684\u795e\u7ecf\u5143\uff0c\u5e94\u7528\u5b9a\u5411\u6743\u91cd\u8870\u51cf\u8f6c\u79fb\u4fe1\u606f\u540e\u526a\u679d\uff1b\u5bc6\u96c6\u5316\u9636\u6bb5\u5728\u4fe1\u53f7\u6b20\u62df\u5408\u7684\u9891\u8c31\u533a\u57df\u589e\u52a0\u8f93\u5165\u9891\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIRe\u5728\u56fe\u50cf\u548cSDFs\u4e0a\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "AIRe\u901a\u8fc7\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6848\uff0c\u5305\u62ec\u795e\u7ecf\u5143\u526a\u679d\u548c\u8f93\u5165\u9891\u7387\u5bc6\u96c6\u5316\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6539\u8fdb\u4e86INR\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u5927\u5c0f\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u66f4\u597d\u7684\u6743\u8861\u3002"}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "SynAD\u662f\u9996\u4e2a\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u771f\u5b9e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u5b9a\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684ego\u8f66\u8f86\u3001\u5730\u56fe\u6295\u5f71\u548c\u65b0\u7f51\u7edc\u83b7\u53d6\u9e1f\u77b0\u56fe\u7279\u5f81\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u548c\u9ad8\u8d28\u91cf\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u53d1\u5c55\uff0c\u4f46\u4ec5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u9650\u5236\u4e86\u8bad\u7ec3\u573a\u666f\u7684\u591a\u6837\u6027\u3002\u5408\u6210\u573a\u666f\u751f\u6210\u867d\u88ab\u89c6\u4e3a\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6307\u5b9a\u7684ego\u8f66\u8f86\u53ca\u4f20\u611f\u5668\u8f93\u5165\u3002", "method": "SynAD\u6846\u67b6\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u5177\u6709\u6700\u5168\u9762\u9a7e\u9a76\u4fe1\u606f\u7684\u667a\u80fd\u4f53\u4f5c\u4e3aego\u8f66\u8f86\uff0c\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff0c\u5e76\u5229\u7528\u65b0\u5f00\u53d1\u7684Map-to-BEV\u7f51\u7edc\u5728\u4e0d\u4f9d\u8d56\u4f20\u611f\u5668\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u83b7\u53d6\u9e1f\u77b0\u56fe\u7279\u5f81\uff0c\u6700\u540e\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\u5c06\u8fd9\u4e9b\u57fa\u4e8e\u5730\u56fe\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u6709\u6548\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSynAD\u6709\u6548\u6574\u5408\u4e86\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u6027\u80fd\u3002", "conclusion": "SynAD\u6210\u529f\u5730\u5c06\u5408\u6210\u573a\u666f\u751f\u6210\u4e0e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u6027\u80fd\uff0c\u4e3a\u66f4\u5168\u9762\u548c\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u578b\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u7684\u521b\u610f\u3001\u7f8e\u611f\u548c\u53cd\u76f4\u89c9\u6027\uff0c\u6210\u679c\u83b7\u4e13\u5bb6\u8ba4\u53ef\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u521b\u9020\u771f\u6b63\u5177\u6709\u521b\u610f\u3001\u7f8e\u611f\u548c\u53cd\u76f4\u89c9\u7684\u8f93\u51fa\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u96be\u9898\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u751f\u6210\u5f0fAI\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u4ee5\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u7684\u751f\u6210\u7387\u4ece0.22%\uff08\u76d1\u7763\u5b66\u4e60\uff09\u63d0\u5347\u81f32.5%\uff0c\u8d85\u8fc7\u73b0\u6709\u6570\u636e\u96c6\uff082.1%\uff09\u548c\u6700\u4f73Lichess\u8bad\u7ec3\u6a21\u578b\uff080.4%\uff09\u3002\u8c1c\u9898\u5728\u65b0\u9896\u6027\u3001\u591a\u6837\u6027\u548c\u7f8e\u5b66\u4e3b\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u83b7\u4e13\u5bb6\u9ad8\u5ea6\u8bc4\u4ef7\u3002", "conclusion": "AI\u751f\u6210\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u5728\u521b\u610f\u3001\u7f8e\u611f\u548c\u53cd\u76f4\u89c9\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u4f20\u7edf\u4e66\u7c4d\u8c1c\u9898\uff0c\u751a\u81f3\u63a5\u8fd1\u7ecf\u5178\u4f5c\u54c1\u7684\u6c34\u5e73\u3002\u6700\u7ec8\u6210\u679c\u662f\u4e00\u672c\u7531\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u8ba4\u53ef\u5176\u521b\u610f\u7684AI\u751f\u6210\u8c1c\u9898\u96c6\u3002"}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528LLMs\u81ea\u52a8\u8bc4\u4f30\u8f6f\u4ef6\u4ea7\u7269\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u52302030\u5e74\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4ea7\u751f\u4e86\u5927\u91cf\u8f6f\u4ef6\u4ea7\u7269\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4fc3\u4f7f\u4e86LLM-as-a-Judge\u8303\u5f0f\u7684\u51fa\u73b0\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5206\u6790\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u8be6\u7ec6\u7684\u8def\u7ebf\u56fe\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8be6\u7ec6\u7684\u8def\u7ebf\u56fe\uff0c\u4ee5\u63a8\u52a8LLM-as-a-Judge\u6846\u67b6\u7684\u7814\u7a76\u548c\u91c7\u7528\uff0c\u6700\u7ec8\u63d0\u5347\u8f6f\u4ef6\u4ea7\u7269\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c55\u671b\uff0c\u65e8\u5728\u5f15\u5bfc\u793e\u533a\u63a8\u8fdbLLM-as-a-Judge\u6846\u67b6\uff0c\u76ee\u6807\u662f\u52302030\u5e74\u5b9e\u73b0\u53ef\u9760\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u8f6f\u4ef6\u4ea7\u7269\u7684\u8bc4\u4f30\u6548\u7387\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "Neural USD \u662f\u4e00\u79cd\u5c42\u6b21\u5316\u573a\u666f\u8868\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u7cbe\u786e\u7f16\u8f91\uff0c\u907f\u514d\u5168\u5c40\u53d8\u5316\u3002", "motivation": "\u89e3\u51b3\u53ef\u63a7\u751f\u6210\u6a21\u578b\u4e2d\u7cbe\u786e\u548c\u8fed\u4ee3\u5bf9\u8c61\u7f16\u8f91\u7684\u6311\u6218\uff0c\u907f\u514d\u56e0\u6539\u53d8\u6761\u4ef6\u4fe1\u53f7\u5bfc\u81f4\u7684\u5168\u5c40\u53d8\u5316\u95ee\u9898\u3002", "method": "\u5f15\u5165 Neural Universal Scene Descriptor (Neural USD) \u6846\u67b6\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u8868\u793a\u573a\u666f\u548c\u5bf9\u8c61\uff0c\u5e76\u5e94\u7528\u5fae\u8c03\u65b9\u6cd5\u786e\u4fdd\u63a7\u5236\u4fe1\u53f7\u89e3\u8026\u3002", "result": "Neural USD \u652f\u6301\u5bf9\u5bf9\u8c61\u5916\u89c2\u3001\u51e0\u4f55\u548c\u59ff\u6001\u7684\u72ec\u7acb\u63a7\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u8fed\u4ee3\u548c\u589e\u91cf\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Neural USD \u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u5c42\u6b21\u5316\u7684\u573a\u666f\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u7cbe\u786e\u548c\u8fed\u4ee3\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5168\u5c40\u53d8\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u901a\u8fc7\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a\u548c\u6df7\u5408\u4e13\u5bb6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u591a\u4efb\u52a1\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u6a21\u7cca\u6027\u548c\u4efb\u52a1\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u611f\u77e5\u6a21\u7cca\u6027\u548c\u4efb\u52a1\u51b2\u7a81\u9650\u5236\u4e86\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u7684\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a\uff08LCVR\uff09\u6a21\u5757\u548c\u8bed\u8a00\u6761\u4ef6\u6df7\u5408\u4e13\u5bb6\u5bc6\u5ea6\u7b56\u7565\uff08LMoE-DP\uff09\u7684\u6846\u67b6\u3002LCVR\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u5c06\u89c6\u89c9\u7279\u5f81\u57fa\u7840\u5316\uff0c\u4ee5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u7684\u4efb\u52a1\uff1bLMoE-DP\u5219\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8c03\u5236\u7a33\u5b9a\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLCVR\u5c06Action Chunking with Transformers\uff08ACT\uff09\u548cDiffusion Policy\uff08DP\uff09\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e8633.75%\u548c25%\u3002\u5b8c\u6574\u6846\u67b6\u5b9e\u73b0\u4e8679%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u5148\u8fdb\u57fa\u7ebf\u9ad8\u51fa21%\u3002", "conclusion": "\u7ed3\u5408\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u3001\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u56db\u79cd\u6a21\u578b\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\u7684\u6027\u80fd\uff0cHAM\u548cMPC\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e0e\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22\u7269\u7406\u57fa\u7840\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\u7684\u6574\u5408\uff0c\u4ee5\u53ca\u4f20\u7edf\u4e0eAI\u9a71\u52a8\u63a7\u5236\u5668\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "method": "\u91c7\u7528\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08Linear\u3001PBM\u3001LSTM\u3001HAM\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\uff09\uff0c\u5728\u5fae\u578b\u6e29\u5ba4\u5e73\u53f0\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u548c\u6bd4\u8f83\u3002", "result": "HAM\u5728\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u5e73\u8861\u6027\u80fd\uff0cLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u5b9e\u73b0\u7075\u6d3b\u4ea4\u4e92\u3002", "conclusion": "HAM\u6a21\u578b\u5728\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u7684\u5e73\u8861\u6027\u80fd\uff0c\u800cLSTM\u867d\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\u3002\u63a7\u5236\u7b56\u7565\u4e2d\uff0cMPC\u7a33\u5065\u53ef\u9760\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u7ed3\u5408\u9884\u6d4b\u5de5\u5177\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002"}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki \u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u548c\u6587\u672c/\u89c6\u89c9\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4ed3\u5e93\u7ea7\u6587\u6863\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u5f00\u53d1\u8005\u82b1\u8d39\u8fd158%\u7684\u65f6\u95f4\u7406\u89e3\u4ee3\u7801\u5e93\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u6027\u548c\u624b\u52a8\u5de5\u4f5c\uff0c\u7ef4\u62a4\u5168\u9762\u6587\u6863\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ed3\u5e93\u7ea7\u6587\u6863\u8bb0\u5f55\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "CodeWiki \u91c7\u7528\u4e09\u79cd\u521b\u65b0\u65b9\u6cd5\uff1a(i) \u5206\u5c42\u5206\u89e3\u4ee5\u4fdd\u7559\u67b6\u6784\u4e0a\u4e0b\u6587\uff0c(ii) \u9012\u5f52\u4ee3\u7406\u5904\u7406\u4e0e\u52a8\u6001\u59d4\u6258\uff0c(iii) \u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u5de5\u4ef6\u7684\u5408\u6210\uff08\u5982\u67b6\u6784\u56fe\u548c\u6570\u636e\u6d41\uff09\u3002", "result": "CodeWiki \u5728\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u4e0a\u5206\u522b\u8fbe\u523068.79%\u548c64.80%\u7684\u8d28\u91cf\u5f97\u5206\uff0c\u4f18\u4e8e\u73b0\u6709\u95ed\u6e90\u7cfb\u7edf\u3002", "conclusion": "CodeWiki \u662f\u7b2c\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b9e\u73b0\u5168\u9762\u7684\u4ed3\u5e93\u7ea7\u6587\u6863\u8bb0\u5f55\uff0c\u5176\u8d28\u91cf\u5f97\u5206\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u95ed\u6e90\u7cfb\u7edf\u3002"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "SafeVision \u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u9632\u62a4\u680f\uff0c\u901a\u8fc7\u7c7b\u4eba\u63a8\u7406\u589e\u5f3a\u9002\u5e94\u6027\u548c\u900f\u660e\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e GPT-4o\uff0c\u5e76\u5f15\u5165\u9ad8\u8d28\u91cf\u6570\u636e\u96c6 VisionHarm\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u9632\u62a4\u680f\u6a21\u578b\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u5174\u5a01\u80c1\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u4e0e\u751f\u6210\u6846\u67b6\u3001\u9075\u5faa\u7b56\u7565\u7684\u8bad\u7ec3\u6d41\u7a0b\u4ee5\u53ca\u5b9a\u5236\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u591a\u6837\u5316\u7684QA\u751f\u6210\u4e0e\u8bad\u7ec3\u7b56\u7565\u4ee5\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "result": "SafeVision \u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728 VisionHarm-T \u548c VisionHarm-C \u4e0a\u5206\u522b\u6bd4 GPT-4o \u9ad8\u51fa 8.6% \u548c 15.5%\uff0c\u4e14\u901f\u5ea6\u5feb 16 \u500d\u4ee5\u4e0a\u3002", "conclusion": "SafeVision \u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u9632\u62a4\u680f\uff0c\u901a\u8fc7\u96c6\u6210\u7c7b\u4eba\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65b0\u5174\u5a01\u80c1\u7684\u52a8\u6001\u9002\u5e94\u548c\u900f\u660e\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u62d3\u6251\u5730\u56fe\u548cVoronoi\u7b97\u6cd5\uff0c\u63d0\u5347\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u5e73\u8861\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u969c\u788d\u5bc6\u96c6\u975e\u51f8\u73af\u5883\u4e2d\u7684\u534f\u540c\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u662f\u52a8\u6001\u5e73\u8861\u7684\u63a2\u7d22\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\u3002", "method": "\u91c7\u7528\u589e\u91cf\u66f4\u65b0\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5e73\u8861\u7684\u56fe\u7a7a\u95f4\u5206\u533a\u548c\u4efb\u52a1\u5206\u914d\u3002\u5c40\u90e8\u89c4\u5212\u5668\u4f18\u5316\u63a2\u7d22\u76ee\u6807\u7684\u8bbf\u95ee\u5e8f\u5217\uff0c\u751f\u6210\u5b89\u5168\u3001\u5e73\u6ed1\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u673a\u5668\u4eba\u56e2\u961f\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\u548c\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u969c\u788d\u5bc6\u96c6\u975e\u51f8\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4ee3\u7406\u578bAI\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4ee3\u7406\u578bAI\u7cfb\u7edf\u56e0\u5176\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u72ec\u7279\u7684\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u548c\u5206\u7c7b\u4ee3\u7406\u578bAI\u7279\u6709\u7684\u5a01\u80c1\uff0c\u56de\u987e\u6700\u8fd1\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u89d2\u5ea6\u8ba8\u8bba\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee3\u7406\u578bAI\u5a01\u80c1\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u7efc\u5408\u4e86\u5f53\u524d\u7684\u7814\u7a76\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u5173\u4e8e\u4ee3\u7406\u578bAI\u5b89\u5168\u7684\u7814\u7a76\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u5b89\u5168\u4ee3\u7406\u7cfb\u7edf\u7684\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7FUSE\u7814\u8ba8\u4f1a\u7684\u8ba8\u8bba\uff0c\u4ee5\u8bbd\u523a\u548c\u60b2\u89c2\u89c6\u89d2\u5206\u6790\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7684\u4e09\u5927\u95ee\u9898\uff1a\u5f00\u53d1\u8005\u80fd\u529b\u4e0d\u8db3\u3001\u9886\u57df\u77e5\u8bc6\u9057\u5fd8\u8fc7\u5feb\u548c\u6280\u672f\u65e0\u5e8f\u589e\u957f\u3002", "motivation": "\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u53d1\u5c55\u8d8b\u52bf\u53ca\u5176\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7FUSE\u7814\u8ba8\u4f1a\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e0e\u4f1a\u8005\u7684\u8ba8\u8bba\u548c\u89c2\u70b9\u3002", "result": "\u4f5c\u8005\u63d0\u70bc\u51fa\u4e09\u4e2a\u4e3b\u8981\u7684\u5669\u68a6\u65b9\u5411\uff1a\u65e0\u77e5\u7684\u8f6f\u4ef6\u5f00\u53d1\u8005\u3001\u5feb\u901f\u9057\u5fd8\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u4ee5\u53ca\u6280\u672f\u7206\u70b8\u5f0f\u589e\u957f\u3002", "conclusion": "\u4f5c\u8005\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u6301\u60b2\u89c2\u6001\u5ea6\uff0c\u8ba4\u4e3a\u5176\u53d1\u5c55\u5982\u540c\u6162\u52a8\u4f5c\u7684\u8f66\u7978\uff0c\u65e2\u65e0\u6cd5\u907f\u514d\u53c8\u65e0\u6cd5\u5ffd\u89c6\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u80f8\u90e8X\u5149\u5206\u6790\u7684\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u652f\u6301\u4e34\u5e8a\u5ba1\u8ba1\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7f3a\u4e4f\u900f\u660e\u63a8\u7406\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u6b65\u9aa4\u4ee5\u652f\u6301\u4e34\u5e8a\u5ba1\u8ba1\u3002", "method": "\u7ed3\u5408\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff08\u5305\u62ec\u63a8\u7406\u98ce\u683c\u7684\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u6a21\u578b\u80fd\u591f\u6a21\u62df\u653e\u5c04\u79d1\u533b\u5e08\u7684\u7cfb\u7edf\u601d\u7ef4\u8fc7\u7a0b\u3002", "result": "\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\uff0c\u6a21\u578b\u5728\u4fdd\u6301\u591a\u6807\u7b7e\u5206\u7c7b\u7ade\u4e89\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff1b\u4e13\u5bb6\u8bfb\u8005\u7814\u7a76\u8868\u660e\uff0c\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u589e\u5f3a\u4e86\u4fe1\u5fc3\u5e76\u51cf\u5c11\u4e86\u62a5\u544a\u65f6\u95f4\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80f8\u90e8X\u5149\u7247\u5206\u6790\u7684\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u53ef\u5ba1\u8ba1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u4f4d\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u548cB{\\'e}zier\u591a\u9879\u5f0f\u786e\u4fdd\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\uff0c\u751f\u6210\u53ef\u9760\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "motivation": "\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u751f\u6210\u53ef\u9760\u8fd0\u52a8\uff0c\u9700\u540c\u65f6\u8ba1\u7b97\u8def\u5f84\u548c\u63a5\u89e6\u5e8f\u5217\uff0c\u5e76\u51c6\u786e\u8003\u8651\u52a8\u529b\u5b66\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u7279\u6027\u89e3\u8026\u5404\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\uff0c\u7ed3\u5408B{\\'e}zier\u591a\u9879\u5f0f\u7684\u5fae\u5206\u77e9\u9635\u548c\u51f8\u5305\u6027\u8d28\uff0c\u786e\u4fdd\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u4e00\u81f4\u6027\u6ee1\u8db3\u3002", "result": "\u6846\u67b6\u6210\u529f\u751f\u6210\u52a8\u6001\u53ef\u9760\u7684\u8fd0\u52a8\uff0c\u652f\u6301\u591a\u79cd\u6b65\u6001\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u751f\u6210\u52a8\u6001\u53ef\u9760\u7684\u8fd0\u52a8\uff0c\u652f\u6301\u591a\u79cd\u6b65\u6001\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u9a8c\u8bc1\u4e86\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u751f\u6210\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u65ad\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u6027\u8ffd\u6c42\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u5347\u4e86LVLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u8bad\u7ec3\u7b97\u6cd5\uff08\u5982SFT\u3001PPO\u3001GRPO\uff09\u5728\u672a\u89c1\u8fc7\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4f9d\u8d56\u4e8e\u6709\u504f\u89c1\u7684\u5956\u52b1\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5c06LVLMs\u4e2d\u7684\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u65ad\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\u3002\u5229\u7528\u591a\u6837\u6027\u8ffd\u6c42\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u7528\u4e8e\u9f13\u52b1\u591a\u6837\u4e14\u9ad8\u53ef\u80fd\u6027\u7684\u6f5c\u5728CoT\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u5347\u4e86LVLMs\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u8868\u73b0\uff0c\u5305\u62ec\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff0c\u5e2e\u52a9\u8f6f\u4ef6\u4ee3\u7406\u9ad8\u6548\u5904\u7406\u590d\u6742\u7f51\u9875\u7ed3\u6784\u548c\u52a8\u6001\u96c6\u6210\u7f51\u7edc\u670d\u52a1\uff0c\u63d0\u5347\u5176\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u8f6f\u4ef6\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u4f9d\u8d56\u4e8e\u5176\u4ece\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u7f51\u9875DOM\u548c\u7f51\u7edc\u670d\u52a1\u8bed\u4e49\u63cf\u8ff0\uff09\u6784\u5efa\u53ef\u64cd\u4f5c\u7684\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u539f\u59cbHTML\u7684\u5197\u957f\u6027\u548c\u786c\u7f16\u7801API\u96c6\u6210\u7684\u9759\u6001\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u5bfc\u6a21\u5f0f\uff08\u7528\u4e8e\u7b80\u5316\u590d\u6742\u7684\u7f51\u9875DOM\u7ed3\u6784\uff09\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\uff08\u7528\u4e8e\u52a8\u6001\u89e3\u6790\u548c\u96c6\u6210\u672a\u77e5\u7f51\u7edc\u670d\u52a1\u7684\u8bed\u4e49\u63cf\u8ff0\uff09\u3002", "result": "\u901a\u8fc7\u8fd9\u4e24\u79cd\u6a21\u5f0f\uff0c\u4ee3\u7406\u80fd\u591f\u9ad8\u6548\u5730\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff08DOM\u8f6c\u5bfc\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\uff09\u4e3a\u8f6f\u4ef6\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u73b0\u5728\u7f51\u7edc\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u4e0a\u7684\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9010\u4e2a\u9884\u6d4b\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9010\u4e2a\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u9010\u4e2a\u9884\u6d4b\u65b9\u5f0f\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS\u662f\u4e00\u79cd\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u8f93\u5165\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u53d7\u9650\u4e8e\u4e13\u5bb6\u6f14\u793a\u7684\u8d28\u91cf\u548c\u90e8\u7f72\u65f6\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff1b\u5f3a\u5316\u5b66\u4e60\u867d\u53ef\u6269\u5c55\uff0c\u4f46\u901a\u5e38\u5c40\u9650\u4e8e\u4f4e\u7ef4\u7b26\u53f7\u8f93\u5165\u3002ZTRS\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u8f93\u5165\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "ZTRS\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u63d0\u51fa\u7684Exhaustive Policy Optimization (EPO)\u65b9\u6cd5\uff0c\u9488\u5bf9\u53ef\u679a\u4e3e\u7684\u52a8\u4f5c\u548c\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "ZTRS\u5728Navtest\u3001Navhard\u548cHUGSIM\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Navhard\u4e0a\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u5728HUGSIM\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ZTRS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4f20\u611f\u5668\u8f93\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Navhard\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fajudo calculus\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u56e0\u679c\u53d1\u73b0\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u56e0\u679c\u6548\u5e94\u4f9d\u8d56\u4e8e\u591a\u79cd\u60c5\u5883\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u7b49\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5f62\u5f0f\u5316\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u7ed3\u5408judo calculus\u4e0e\u6807\u51c6\u7684\u57fa\u4e8e\u5206\u6570\u3001\u7ea6\u675f\u548c\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u7b97\u6cd5\u548c\u5b9e\u73b0\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8esheaf\u7406\u8bba\u7684\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86judo calculus\u7684\u7406\u8bba\u548c\u5b9e\u73b0\u6846\u67b6\uff0c\u901a\u8fc7j-stable\u56e0\u679c\u63a8\u65ad\u5728sheaves\u7684topos\u4e2d\u5f62\u5f0f\u5316\u56e0\u679c\u53d1\u73b0\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ecf\u5178\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo\u662f\u4e00\u4e2a\u957f\u671f\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u5b9e\u573a\u666f\u4e2d\u7684AI\u52a9\u624b\uff0c\u6db5\u76d6\u8bb0\u5fc6\u3001\u7406\u89e3\u548c\u8de8\u8bb0\u5fc6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u7684\u6d41\u5f0f\u573a\u666f\u6216\u957f\u671f\u4efb\u52a1\u652f\u6301\u3002", "method": "\u5f15\u5165TeleEgo\uff0c\u4e00\u4e2a\u957f\u671f\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b14\u5c0f\u65f6/\u53c2\u4e0e\u8005\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u6db5\u76d612\u4e2a\u5b50\u4efb\u52a1\u548c3,291\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684QA\u9879\u76ee\u3002", "result": "TeleEgo\u5b9a\u4e49\u4e8612\u4e2a\u5b50\u4efb\u52a1\u548c\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff08\u5b9e\u65f6\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\uff09\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30AI\u52a9\u624b\u3002", "conclusion": "TeleEgo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u73b0\u5b9e\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u5b9e\u7528AI\u52a9\u624b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVLM\u7684\u673a\u5668\u4eba\u4ee3\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u7c7b\u4e2d\u5fc3\u4eba\u5de5\u667a\u80fd\uff08HAI\uff09\u5bf9\u673a\u5668\u4eba\u667a\u80fd\u6c34\u5e73\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u65b9\u9762\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5b9e\u4f53\u4ee3\u7406\u5f80\u5f80\u7f3a\u4e4f\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u6a21\u5757\u3001\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u6a21\u5757\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\u3002\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u6a21\u5757\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u548c\u4efb\u52a1\u8868\u73b0\u53cd\u9988\u8bc4\u4f30\u5668\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u4ee3\u7406\u6846\u67b6\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u4f20\u7edfLLM+CLIP\u65b9\u6cd5\u9ad8\u51fa28%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u673a\u5668\u4eba\u4ee3\u7406\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u4f20\u7edfLLM+CLIP\u65b9\u6cd5\u9ad8\u51fa28%\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "sign estimator\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u6539\u8fdbLLM\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u504f\u597d\u5931\u771f\uff0c\u63d0\u5347\u4e0e\u771f\u5b9e\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u7684LLM\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u654f\u611f\uff0c\u5bfc\u81f4\u4eba\u53e3\u5e73\u5747\u6548\u7528\u7684\u4f30\u8ba1\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86sign estimator\uff0c\u901a\u8fc7\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u4ee3\u4ea4\u53c9\u71b5\u635f\u5931\u6765\u6539\u8fdb\u805a\u5408\u6b65\u9aa4\u3002", "result": "\u5728\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u4e2d\uff0csign estimator\u5c06\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c11\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u4eba\u53e3\u504f\u597d\u7684\u4e0d\u4e00\u81f4\u6027\u4ece12%\u964d\u81f38%\u3002", "conclusion": "sign estimator\u65b9\u6cd5\u5728LLM\u5bf9\u9f50\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u504f\u597d\u5931\u771f\uff0c\u63d0\u9ad8\u4e86\u4e0e\u771f\u5b9e\u4eba\u53e3\u504f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u73b0\u6709\u5bf9\u9f50\u6d41\u7a0b\u7684\u7b80\u5355\u6027\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "AdvBlur\u901a\u8fc7\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u7684\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u4e2d\u56e0\u8bbe\u5907\u3001\u4eba\u7fa4\u548c\u6210\u50cf\u6761\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u5206\u5e03\u53d8\u5316\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u63d0\u5347\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faAdvBlur\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u5e76\u7ed3\u5408\u53cc\u635f\u5931\u51fd\u6570\u6846\u67b6\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "AdvBlur\u5728\u591a\u4e2a\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\u6709\u6548\u6027\u3002", "conclusion": "AdvBlur\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u635f\u5931\u51fd\u6570\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u7684\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u67e5\u8be2\u548c\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u5355\u76ee\u6807\u3001\u5355\u6a21\u6001\u548c\u5c01\u95ed\u96c6\u76ee\u6807\u8bbe\u7f6e\u4e0a\u7684\u9650\u5236\u3002", "method": "LagMemo\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\uff0c\u901a\u8fc7\u4efb\u52a1\u76ee\u6807\u67e5\u8be2\u8bb0\u5fc6\u3001\u9884\u6d4b\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\uff0c\u5e76\u6574\u5408\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u52a8\u6001\u5339\u914d\u548c\u9a8c\u8bc1\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLagMemo\u5728\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\u548c\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LagMemo\u5bfc\u822a\u7cfb\u7edf\u901a\u8fc7\u5176\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u6a21\u5757\uff0c\u5728\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u4e2a\u4f53SIR\u548c\u7a7a\u95f4\u80cc\u666f\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "motivation": "\u9884\u6d4b\u4e2a\u4f53\u5728\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u7684\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u539f\u56e0\u5305\u62ec\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53SIR\u7684\u65b9\u6cd5\u3001\u4f20\u7edf\u7279\u5f81\uff08\u5982\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\uff09\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u590d\u6742\u4e92\u52a8\u672a\u88ab\u5145\u5206\u6355\u6349\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u7a00\u758f\u7684\u4e2a\u4f53\u7ea7\u6570\u636e\uff0c\u5206\u6790\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u590d\u6742\u4e92\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e2a\u4f53\u7684SIR\u548c\u7a7a\u95f4\u80cc\u666f\u80fd\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u540e\u4e8b\u4ef6\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u6a21\u578b\u8fd8\u80fd\u6355\u6349\u5230\u5177\u6709\u76f8\u4f3c\u4e8b\u524d\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u5dee\u5f02\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\uff08SIR\uff09\u7eb3\u5165\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6210\u529f\u6355\u6349\u4e86\u4e2a\u4eba\u79fb\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u80cc\u666f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u540e\u4e8b\u4ef6\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "SEG.A\u6311\u6218\u8d5b\u5f15\u5165\u516c\u5f00\u6570\u636e\u96c6\uff0c3D U-Net\u548c\u6a21\u578b\u878d\u5408\u8868\u73b0\u6700\u4f73\uff0c\u4e3aAVT\u5206\u5272\u8bbe\u65b0\u57fa\u51c6\u3002", "motivation": "\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5171\u4eab\u6570\u636e\u963b\u788d\u4e86AVT\u81ea\u52a8\u5206\u6790\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u516c\u5f00\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u4fc3\u8fdb\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u4f7f\u75283D U-Net\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u5b9a\u5236\u540e\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u6a21\u578b\u878d\u5408\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7SEG.A\u6311\u6218\u8d5b\u5f15\u5165\u4e86\u4e00\u4e2a\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\uff08AVT\uff09\u5206\u5272\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5e76\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u2018\u76f2\u4eba\u4e13\u5bb6\u2019\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u85cf\u4efb\u52a1\u4fe1\u606f\u4f7f\u4e13\u5bb6\u8fdb\u884c\u975e\u5e73\u51e1\u63a2\u7d22\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u4f9d\u8d56\u4e13\u5bb6\u5168\u4fe1\u606f\u6f14\u793a\uff0c\u6cdb\u5316\u6027\u53d7\u9650\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u9650\u5236\u4e13\u5bb6\u4fe1\u606f\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u2018\u76f2\u4eba\u4e13\u5bb6\u2019\u5b9e\u9a8c\uff0c\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\u8feb\u4f7f\u4e13\u5bb6\u8fdb\u884c\u63a2\u7d22\uff0c\u7ed3\u5408\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u548cProcgen\u57fa\u51c6\u6e38\u620f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u76f2\u4eba\u4e13\u5bb6\u514b\u9686\u5728\u672a\u89c1\u8fc7\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u7406\u8bba\u5206\u6790\u652f\u6301\u5176\u6cdb\u5316\u8bef\u5dee\u4e0e\u221a(I/m)\u6210\u6b63\u6bd4\u3002", "conclusion": "\u76f2\u4eba\u4e13\u5bb6\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u4efb\u52a1\u6570\u91cf\u9700\u6c42\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e00\u81f4\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "LLM\u53d1\u73b0\u7684\u65b0\u542f\u53d1\u5f0f\u65b9\u6cd5EDDC\u548cMDDC\u5728SMTT\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5927\u578b\u5b9e\u4f8b\u4e2d\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u9488\u5bf9\u5355\u673a\u603b\u5ef6\u8fdf\uff08SMTT\uff09\u95ee\u9898\uff0c\u63a2\u7d22\u5229\u7528LLM\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ee5\u5f25\u8865\u4f20\u7edf\u89c4\u5219\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "method": "\u5f00\u53d1\u5e76\u5bf9\u6bd4\u4e86\u4e24\u79cd\u7531LLM\u53d1\u73b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff1aEDD Challenger (EDDC) \u548c MDD Challenger (MDDC)\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u7ecf\u5178\u7684Earliest Due Date (EDD) \u548c Modified Due Date (MDD) \u89c4\u5219\u3002\u901a\u8fc7\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u7684SMTT\u95ee\u9898\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u5316\u95f4\u9699\u548c\u6c42\u89e3\u65f6\u95f4\u3002", "result": "EDDC\u5728500\u4e2a\u4f5c\u4e1a\u4ee5\u5185\u4f18\u4e8e\u7ecf\u5178EDD\u89c4\u5219\u548c\u5176\u4ed6\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b97\u6cd5\uff1bMDDC\u5728\u66f4\u5927\u66f4\u590d\u6742\u7684\u5b9e\u4f8b\u4e2d\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e0e\u7cbe\u786e\u65b9\u6cd5\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u534f\u4f5c\u5728NP\u96be\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u80fd\u591f\u751f\u6210\u53ef\u6269\u5c55\u4e14\u9ad8\u6027\u80fd\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u8981\u914d\u7f6e\u5f97\u5f53\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "Mars-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u706b\u661f\u76f8\u5173\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u5305\u542b20\u4e2a\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u706b\u661f\u7279\u5b9a\u6a21\u578b\u53ef\u80fd\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u706b\u661f\u79d1\u5b66\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u9650\u5236\u4e86\u706b\u661f\u4efb\u52a1\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165Mars-Bench\uff0c\u5305\u542b20\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u5173\u952e\u5730\u8d28\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u5373\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u706b\u661f\u7279\u5b9a\u7684\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4f18\u4e8e\u901a\u7528\u9886\u57df\u6a21\u578b\u3002", "conclusion": "Mars-Bench\u65e8\u5728\u4e3a\u706b\u661f\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u548c\u6bd4\u8f83\u5efa\u7acb\u4e00\u4e2a\u6807\u51c6\u5316\u57fa\u7840\uff0c\u4fc3\u8fdb\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "HMAMP\u901a\u8fc7\u5bf9\u6297\u7f51\u7edc\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u4f5c\u6280\u80fd\uff0c\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u66f4\u81ea\u7136\u4e92\u52a8\u7684\u53d1\u5c55\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u3001\u76f4\u89c2\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5173\u952e\u6311\u6218\u4e4b\u4e00\u662f\u8ba9\u8fd9\u4e9b\u7cfb\u7edf\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u64cd\u4f5c\u7269\u4f53\u548c\u5de5\u5177\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6297\u7f51\u7edc\u5efa\u6a21\u5de5\u5177\u548c\u7269\u4f53\u64cd\u4f5c\u7684\u590d\u6742\u52a8\u6001\u4ee5\u53ca\u64cd\u4f5c\u4efb\u52a1\u7684\u76ee\u6807\uff0c\u8bad\u7ec3\u4e00\u4e2a\u7b56\u7565\u751f\u6210\u4e0e\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u7279\u6027\u5339\u914d\u7684\u903c\u771f\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "HMAMP\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u4f5c\u6280\u80fd\uff0c\u5e76\u8d85\u8d8a\u5f53\u524d\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u624b\u81c2\u9524\u51fb\u4efb\u52a1\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "HMAMP\u4ee3\u8868\u4e86\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u6a21\u4eff\u4eba\u7c7b\u7684\u5de5\u5177\u548c\u7269\u4f53\u64cd\u4f5c\u65b9\u5f0f\uff0c\u4f7f\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u3001\u76f4\u89c2\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u95f4\u5e8f\u5217\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6210\u5206\u5e76\u8fdb\u884c\u72ec\u7acb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u5f02\u6784\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u57df\u7279\u5f02\u6027\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u7684\u5468\u671f\u6027\u6a21\u5f0f\u3002", "method": "OneCast\u6846\u67b6\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6210\u5206\uff0c\u5206\u522b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u548c\u8bed\u4e49\u611f\u77e5\u7684\u5206\u6bb5\u6807\u8bb0\u5316\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u8fdb\u884c\u63a8\u65ad\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cOneCast\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OneCast\u901a\u8fc7\u89e3\u8026\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u6a21\u5f0f\u548c\u8d8b\u52bf\u6210\u5206\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u5316\u7684\u751f\u6210\u8def\u5f84\u8fdb\u884c\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "APT\u5229\u7528LLM\u751f\u6210\u53ef\u8bfb\u5bf9\u6297\u63d0\u793a\uff0c\u9ad8\u6548\u7ed5\u8fc7T2I\u6a21\u578b\u5b89\u5168\u8fc7\u6ee4\uff0c\u5c55\u73b0\u5f3a\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u6613\u53d7\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff0c\u4e14\u4f20\u7edf\u7ea2\u961f\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6613\u88ab\u8fc7\u6ee4\u3002", "method": "\u63d0\u51faAPT\u6846\u67b6\uff0c\u7ed3\u5408\u4ea4\u66ff\u4f18\u5316-\u5fae\u8c03\u6d41\u7a0b\u548c\u53cc\u91cd\u89c4\u907f\u7b56\u7565\uff0c\u4f18\u5316\u5bf9\u6297\u6027\u540e\u7f00\u751f\u6210\u5e76\u7ed5\u8fc7\u8fc7\u6ee4\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAPT\u751f\u6210\u7684\u4eba\u7c7b\u53ef\u8bfb\u5bf9\u6297\u6027\u63d0\u793a\u5728\u7ea2\u961f\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6709\u6548\u7ed5\u8fc7\u5546\u4e1aAPI\u7684\u5b89\u5168\u8fc7\u6ee4\u3002", "conclusion": "APT\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend\u901a\u8fc73D\u611f\u77e5\u548c\u52a8\u6001\u4fe1\u606f\u7684\u8054\u5408\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u73b0\u5b9e\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u8054\u5408\u5b66\u4e60\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u52a8\u6001\u4fe1\u606f\u3002", "method": "DynaRend\u662f\u4e00\u4e2a\u901a\u8fc7\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u8fdb\u884c\u63a9\u7801\u91cd\u5efa\u548c\u672a\u6765\u9884\u6d4b\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728RLBench\u548cColosseum\u57fa\u51c6\u6d4b\u8bd5\u53ca\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\uff0cDynaRend\u5728\u7b56\u7565\u6210\u529f\u7387\u3001\u73af\u5883\u6270\u52a8\u6cdb\u5316\u548c\u73b0\u5b9e\u9002\u7528\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DynaRend\u901a\u8fc7\u8054\u5408\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u4fe1\u606f\u7684triplane\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6210\u529f\u7387\u3001\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u5229\u7528LLM\u548cML\u7b80\u5316\u65e5\u5fd7\u5206\u6790\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5404\u7c7b\u7528\u6237\u3002", "motivation": "\u89e3\u51b3\u65e5\u5fd7\u5206\u6790\u4e2d\u9ad8\u6210\u672c\u3001\u4e13\u4e1a\u4eba\u624d\u7f3a\u4e4f\u548c\u65f6\u95f4\u9650\u5236\u7b49\u6311\u6218\uff0c\u540c\u65f6\u514b\u670dLLM\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u7ed3\u6784\u5316\u6587\u672c\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7b97\u6cd5\uff0c\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff08\u5305\u62ec\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\uff09\u3002", "result": "\u5728\u4e0d\u540c\u9886\u57df\u7684\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709LLM\u804a\u5929\u673a\u5668\u4eba\uff08\u5982ChatGPT\u3001ChatPDF\u548cNotebookLM\uff09\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe39%\u81f368%\uff0c\u4e14\u7ed3\u679c\u53d8\u5f02\u6027\u663e\u8457\u964d\u4f4e\uff08ROUGE-1 IQR\u51cf\u5c1193%\uff09\u3002", "conclusion": "LLMLogAnalyzer\u901a\u8fc7\u5176\u6a21\u5757\u5316\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u65e5\u5fd7\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "ResNet\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u89e3\u51b3\u6df1\u5ea6CNN\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5728CIFAR-10\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfCNN\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u8bad\u7ec3\u6781\u6df1\u7f51\u7edc\u65f6\u4f1a\u9047\u5230\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7f51\u7edc\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u91c7\u7528ResNet\u67b6\u6784\uff0c\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u7ed5\u8fc7\u4e2d\u95f4\u5c42\uff0c\u4f7f\u68af\u5ea6\u80fd\u591f\u76f4\u63a5\u6d41\u52a8\uff0c\u4ece\u800c\u8bad\u7ec3\u66f4\u6df1\u5c42\u6b21\u7684\u7f51\u7edc\u3002\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86ResNet-18\u7684\u5177\u4f53\u5b9e\u73b0\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cResNet-18\u8fbe\u5230\u4e8689.9%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u7c7b\u4f3c\u6df1\u5ea6\u7684\u4f20\u7edf\u6df1\u5ea6CNN\uff0884.1%\uff09\u66f4\u9ad8\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3001\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3002", "conclusion": "Residual Networks (ResNet) \u901a\u8fc7\u5f15\u5165\u8df3\u8dc3\u8fde\u63a5\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6df1\u5ea6\u7f51\u7edc\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "\u63d0\u51faCoNi-OA\u7b97\u6cd5\uff0c\u5229\u7528\u5355\u5e27LiDAR\u6570\u636e\u5b9e\u73b0\u65e0\u4eba\u673a-\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u534f\u540c\u907f\u969c\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u9ad8\u6548\u4e14\u5b89\u5168\u3002", "motivation": "CoNi-MPC\u6846\u67b6\u5728\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\u867d\u9ad8\u6548\uff0c\u4f46\u56e0\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\u800c\u96be\u4ee5\u5e94\u5bf9\u907f\u969c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c03\u5236\u7684\u907f\u969c\u7b97\u6cd5CoNi-OA\uff0c\u5229\u7528\u5355\u5e27LiDAR\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u65e0\u4eba\u673a\u901f\u5ea6\u4ee5\u5b9e\u73b0\u907f\u969c\u3002", "result": "CoNi-OA\u7b97\u6cd5\u80fd\u5728\u975e\u60ef\u6027\u5750\u6807\u7cfb\u4e0b\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u8ba1\u7b97\u9700\u6c42\u4f4e\uff08\u6bcf\u6b21\u8fed\u4ee3\u5c0f\u4e8e5\u6beb\u79d2\uff09\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u3002", "conclusion": "CoNi-OA\u6210\u529f\u89e3\u51b3\u4e86CoNi-MPC\u5728\u65e0\u4eba\u673a-\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u534f\u540c\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5355\u5e27LiDAR\u6570\u636e\u5b9e\u65f6\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u907f\u969c\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u5728\u6240\u6709\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\uff0c\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u5bf9\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\uff08IDM\u3001OVM\u3001OVRV\u548c\u7b80\u5316CACC\u6a21\u578b\uff09\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff09\u3002\u4f20\u7edf\u6a21\u578b\u53c2\u6570\u901a\u8fc7\u6700\u5c0f\u5316RMSE\u8fdb\u884c\u6821\u51c6\uff0c\u800c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ee5\u95f4\u8ddd\u3001\u901f\u5ea6\u548c\u95f4\u9699\u7c7b\u578b\u4f5c\u4e3a\u8f93\u5165\u9884\u6d4b\u52a0\u901f\u5ea6\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f18\uff0cRMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u9699\uff09\u30010.0016\uff08\u957f\u95f4\u9699\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u9699\uff09\u3002\u4f20\u7edf\u6a21\u578b\u4e2d\uff0cCACC\u8868\u73b0\u6700\u4f73\uff0cRMSE\u4e3a2.67\uff08\u957f\u95f4\u9699\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u5728\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7269\u7406\u6a21\u578b\u3002\u8fd9\u5bf9\u4e8e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u52a8\u6001\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u9636\u6bb5\u5904\u7406\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\u5b58\u5728\u4e24\u9636\u6bb5\u5904\u7406\u7684\u5185\u5b58\u6d88\u8017\u5927\u548c\u6027\u80fd\u9650\u5236\u95ee\u9898\uff0cSNELLA\u65e8\u5728\u901a\u8fc7\u4e00\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SNELLA\u91c7\u7528\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u53cc\u5c42\u6b21\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u4f18\u5316\u6743\u91cd\u66f4\u65b0\u3002", "result": "\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSNELLA\u8868\u73b0\u51fa\u8272\uff0c\u5185\u5b58\u4f7f\u7528\u663e\u8457\u964d\u4f4e\uff0831.1%-39.9%\uff09\uff0c\u5e76\u5728FGVC\u57fa\u51c6\u6d4b\u8bd5\u4e2dTop-1\u51c6\u786e\u7387\u63d0\u53471.8%\u3002", "conclusion": "SNELLA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e00\u9636\u6bb5\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u6743\u91cd\u77e9\u9635\u548c\u81ea\u9002\u5e94\u53cc\u5c42\u6b21\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim\u5229\u7528\u6539\u8fdb\u76843D Gaussian Splatting\u548c\u65e0\u7f51\u683c\u7b97\u6cd5\uff0c\u4ece\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u7684\u6210\u672c\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u5728\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\u65f6\u9762\u4e34\u9ad8\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0cNVSim\u65e8\u5728\u901a\u8fc7\u4ec5\u4f7f\u7528\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u75283D Gaussian Splatting\u6280\u672f\uff0c\u5e76\u5f15\u5165Floor-Aware Gaussian Splatting\u6765\u4f18\u5316\u7a00\u758f\u89c2\u5bdf\u5730\u677f\u4e0a\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u7f51\u683c\u904d\u5386\u6027\u68c0\u67e5\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5206\u6790\u6e32\u67d3\u89c6\u56fe\u6784\u5efa\u62d3\u6251\u56fe\u3002", "result": "NVSim\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u751f\u6210\u6709\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u56fe\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "NVSim\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684Floor-Aware Gaussian Splatting\u6280\u672f\u548c\u65e0\u7f51\u683c\u904d\u5386\u6027\u68c0\u67e5\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u5728\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\uff0c\u80fd\u591f\u4ece\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens1\u662f\u4e00\u4e2a\u900f\u660e\u7684AI\u52a9\u624b\uff0c\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u548c\u53ef\u89c6\u5316\u8bc1\u636e\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efb\u4eba\u5de5\u667a\u80fd\uff0c\u9700\u8981\u6d88\u9664\u5176\u201c\u9ed1\u76d2\u201d\u7279\u6027\uff0c\u4f7f\u5176\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u5316\uff0c\u5c31\u50cf\u4e0e\u540c\u4e8b\u54a8\u8be2\u4e00\u6837\u3002", "method": "\u5f00\u53d1\u4e86HistoLens1\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u75c5\u7406\u5b66\u5bb6\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u5316\u4e3aAI\u5f15\u64ce\u7684\u7cbe\u786e\u67e5\u8be2\uff0c\u5e76\u751f\u6210\u6e05\u6670\u7684\u7ed3\u6784\u5316\u62a5\u544a\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff08\u5982\u70ed\u56fe\uff09\u3002", "result": "HistoLens1\u80fd\u591f\u667a\u80fd\u5730\u56de\u7b54\u95ee\u9898\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u540c\u65f6\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u5206\u6790\uff0c\u907f\u514d\u4e86\u80cc\u666f\u566a\u58f0\u7684\u5e72\u6270\u3002", "conclusion": "HistoLens1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u534f\u4f5c\u7684AI\u52a9\u624b\uff0c\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u8fdb\u884c\u8bca\u65ad\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4f5c\u4e3a\u4e13\u5bb6\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "COLA\u662f\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347CLIP\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u6539\u5584\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86CLIP\u6a21\u578b\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u9f50\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u88ab\u653e\u5927\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\u3002", "method": "COLA\u91c7\u7528\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u548c\u5c40\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\u6062\u590d\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\u7684\u5168\u5c40\u5bf9\u9f50\u3002", "result": "\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOLA\u5e73\u5747\u63d0\u5347\u4e866.7%\u7684\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "COLA\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e72\u51c0\u6837\u672c\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u76843D\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u7eb3\u5165\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7b49\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u6fc0\u8fdb\u884c\u52a8\u4e2d\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b33D\u6865\u5f0f\u8d77\u91cd\u673a\u5728\u6fc0\u8fdb\u884c\u52a8\u4e2d\u9762\u4e34\u7684\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u7684\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u76f4\u63a5\u7eb3\u5165\u590d\u6742\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\u3002", "result": "\u6bd4\u8f83\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\u3002", "conclusion": "\u6469\u64e6\u5efa\u6a21\u662f\u5b9e\u73b0\u5feb\u901f\u4e14\u5b89\u5168\u8d77\u91cd\u673a\u8f68\u8ff9\u7684\u57fa\u672c\u8981\u6c42\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u5b9e\u73b0\u9ad8\u6548\u3001\u900f\u660e\u7684\u4e8b\u4ef6\u7ba1\u7406\uff0c\u9002\u7528\u4e8e\u4e91\u7cfb\u7edf\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u5728\u9762\u4e34\u6d77\u91cf\u5f02\u6784\u6570\u636e\u65f6\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u53c8\u96be\u4ee5\u8de8\u7cfb\u7edf\u901a\u7528\u3001\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "OpsAgent\u91c7\u7528\u514d\u8bad\u7ec3\u7684\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u7ed3\u5408\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u5ba1\u8ba1\u7684\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpsAgent\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u3002", "conclusion": "OpsAgent\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u8fdb\u5316\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u4e8b\u4ef6\u7ba1\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u9002\u5408\u5728\u771f\u5b9e\u4e91\u7cfb\u7edf\u4e2d\u957f\u671f\u90e8\u7f72\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "BOB\u662f\u4e00\u79cd\u9488\u5bf9T2I\u6a21\u578b\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u7c7b\u522b\u65e0\u5173\u5c5e\u6027\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u6539\u5584\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3T2I\u6a21\u578b\u5728\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65f6\u53ef\u80fd\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u964d\u4f4e\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOB\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u63d0\u53d6\u7c7b\u522b\u65e0\u5173\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u548c\u7269\u4f53\u59ff\u6001\uff09\u5e76\u5728\u5fae\u8c03\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6761\u4ef6\u5316\u8fd9\u4e9b\u5c5e\u6027\uff0c\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2aT2I\u6a21\u578b\u3001\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBOB\u65b9\u6cd5\u5728\u4f4e\u6837\u672c\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BOB\u65b9\u6cd5\u5728\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u591a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5c55\u73b0\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u7684\u51c6\u786e\u4f30\u8ba1\u5bf9\u53ef\u9760\u7684\u72b6\u6001\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e2d\u7531\u4e8e\u73af\u5883\u53d8\u5316\u3001\u524d\u7aef\u9884\u5904\u7406\u7b49\u56e0\u7d20\uff0c\u534f\u65b9\u5dee\u96be\u4ee5\u8bc6\u522b\u3002", "method": "\u901a\u8fc7\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\u7684\u8054\u5408\u4f3c\u7136\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u4fe1\u606f\u5229\u7528\u7684\u5e73\u8861\u3002\u4e0b\u5c42\u4f7f\u7528\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u72b6\u6001\u589e\u5e7f\u4f30\u8ba1\u8f68\u8ff9\uff0c\u4e0a\u5c42\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u4f18\u5316\u534f\u65b9\u5dee\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u6765\u4f30\u8ba1\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u6570\u636e\u4e0a\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u9ad8\u7684\u6548\u7387\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u96be\u5ea6\u591a\u8df3\u95ee\u9898\u7684\u6846\u67b6\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u591a\u4e3a\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u96be\u4ee5\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u7684\u5173\u7cfb\u7c7b\u578b\u6807\u6ce8\u548c\u591a\u6837\u6027\u6269\u5c55\u751f\u6210\u591a\u6837\u5316\u7684\u8bc1\u636e\u7c07\uff0c\u91c7\u7528\u53cd\u5411\u95ee\u9898\u6784\u9020\u65b9\u6cd5\u7ec4\u5408\u95f4\u63a5\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u548c\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\u3002", "result": "\u8be5\u6846\u67b6\u751f\u6210\u4e86\u590d\u6742\u4e14\u96be\u4ee5\u68c0\u7d22\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6210\u672c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u7684\u591a\u8df3\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bc4\u4f30\u57fa\u51c6\u7684\u9ad8\u96be\u5ea6\u7279\u5f81\u3002"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "OmniText\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u91cd\u5206\u5e03\u89e3\u51b3\u6587\u672c\u4fee\u590d\u4e2d\u7684\u4e09\u5927\u9650\u5236\uff08\u65e0\u6cd5\u79fb\u9664\u6587\u672c\u3001\u6837\u5f0f\u4e0d\u53ef\u63a7\u3001\u5b57\u6bcd\u91cd\u590d\uff09\uff0c\u5728\u591a\u6837\u5316TIM\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u4fee\u590d\u65b9\u6cd5\u5728\u6587\u672c\u63d2\u5165\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u65e0\u6cd5\u79fb\u9664\u6587\u672c\u3001\u7f3a\u4e4f\u6837\u5f0f\u63a7\u5236\u4ee5\u53ca\u5b57\u6bcd\u91cd\u590d\u751f\u6210\u7b49\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdbTIM\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u4e86\u4ea4\u53c9\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u63d0\u51fa\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u5b9e\u73b0\u6587\u672c\u53bb\u9664\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u91cd\u5206\u5e03\u63a7\u5236\u6587\u672c\u6837\u5f0f\u548c\u5185\u5bb9\uff1b\u5f15\u5165\u6f5c\u5728\u4f18\u5316\u6846\u67b6\u4e2d\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff08\u4ea4\u53c9\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\u548c\u81ea\u6ce8\u610f\u529b\u6837\u5f0f\u635f\u5931\uff09\u4ee5\u5b9e\u73b0\u53ef\u63a7\u4fee\u590d\u3002", "result": "OmniText\u5728\u591a\u9879TIM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6587\u672c\u4fee\u590d\u65b9\u6cd5\uff0c\u4e0e\u4e13\u7528\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff1b\u540c\u65f6\u53d1\u5e03\u4e86OmniText-Bench\u57fa\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u591a\u6837\u5316TIM\u4efb\u52a1\u8bc4\u4f30\u3002", "conclusion": "OmniText\u6846\u67b6\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u91cd\u5206\u5e03\u89e3\u51b3\u4e86\u6587\u672c\u56fe\u50cf\u64cd\u4f5c\uff08TIM\uff09\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u6210\u4e3a\u9996\u4e2a\u80fd\u591f\u6267\u884c\u591a\u6837\u5316TIM\u4efb\u52a1\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "SPCG\u6269\u5c55TOP\u4ee5\u5904\u7406\u81ea\u5229\u673a\u5668\u4eba\u5728\u5956\u52b1\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u7ade\u4e89\u3002ORS\u548cFORL\u7b97\u6cd5\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7b56\u7565\u63a5\u8fd1\u6700\u4f18\u89e3\u3002", "motivation": "\u73b0\u6709\u7684TOP\u53d8\u79cd\u5047\u8bbe\u673a\u5668\u4eba\u5408\u4f5c\u8ffd\u6c42\u5355\u4e00\u76ee\u6807\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u81ea\u5229\u673a\u5668\u4eba\u5728\u5956\u52b1\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u7ade\u4e89\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aORS\uff08\u83b7\u53d6\u2018\u5e8f\u6570\u6392\u540d\u2019\uff09\u548cFORL\uff08\u9488\u5bf9\u9ad8\u6392\u540d\u5bf9\u624b\u5b66\u4e60\u6700\u4f73\u54cd\u5e94\u7b56\u7565\uff09\u3002\u5728\u8def\u7f51\u548c\u5408\u6210\u56fe\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "ORS\u901a\u8fc7OR\u6761\u4ef6\u51cf\u5c11\u72b6\u6001\u6df7\u6dc6\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u56e2\u961f\u7b56\u7565\u5b66\u4e60\u6548\u7387\uff1bFORL\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u5956\u52b1\u5206\u5e03\u4e0d\u5747\u65f6\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u5b66\u4e60\u7b56\u7565\u5728SPCG\u4e2d\u63a5\u8fd1TOP\u6700\u4f18\u89e3\u3002", "conclusion": "SPCG\u6269\u5c55\u4e86TOP\uff0c\u9002\u7528\u4e8e\u81ea\u5229\u673a\u5668\u4eba\u5728\u5956\u52b1\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u89c4\u5212\u3002\u901a\u8fc7ORS\u548cFORL\u7b97\u6cd5\uff0c\u56e2\u961f\u89c4\u6a21\u6269\u5927\u65f6\u7b56\u7565\u5b66\u4e60\u6548\u7387\u66f4\u9ad8\uff0c\u4e14\u5728\u5956\u52b1\u5206\u5e03\u4e0d\u5747\u65f6\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u5b66\u4e60\u7b56\u7565\u5728SPCG\u4e2d\u8fbe\u523087%-95%\u7684\u6700\u4f18\u6027\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM$_1$\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u4f53\u73b0\u7684\u6cdb\u5316\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u4f53\u73b0\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8de8\u7a7a\u95f4\u3001\u8de8\u4f53\u73b0\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7cbe\u9009\u7684\u6570\u5b57\u8bed\u6599\u5e93\u5c06\u4f53\u73b0\u77e5\u8bc6\u6ce8\u5165MLLM\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u63d0\u53d6MLLM\u7684\u9ad8\u5c42\u8bed\u4e49\u6307\u5bfc\u63a7\u5236\uff0c\u65e0\u9700\u5fae\u8c03MLLM\u9aa8\u5e72\u3002", "result": "BLM$_1$\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea66%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea63%\uff0c\u663e\u8457\u4f18\u4e8eMLLMs\u3001ELLMs\u3001VLAs\u548cGMLMs\u3002", "conclusion": "BLM$_1$\u6a21\u578b\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u4f53\u73b0\u6cdb\u5316\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8868\u793a\u53ef\u89e3\u91ca\u6027\u4e0e\u5206\u7c7b\u80fd\u529b\u6b63\u76f8\u5173\uff0c\u4e14\u53ef\u901a\u8fc7\u6700\u5927\u5316\u53ef\u89e3\u91ca\u6027\u5fae\u8c03\u63d0\u5347\u5206\u7c7b\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u8868\u793a\u662f\u5426\u80fd\u540c\u65f6\u5b9e\u73b0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u5206\u7c7b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u56fa\u6709\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\uff08IIS\uff09\uff0c\u901a\u8fc7\u8bc4\u4f30\u4fe1\u606f\u635f\u5931\u548c\u53ef\u89e3\u91ca\u8bed\u4e49\u6bd4\u4f8b\u6765\u91cf\u5316\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u53d1\u73b0\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u80fd\u529b\u6b63\u76f8\u5173\uff0c\u4e14\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6700\u5927\u5316\u5fae\u8c03\u53ef\u63d0\u5347\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8868\u793a\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u4e14\u901a\u8fc7\u6700\u5927\u5316\u53ef\u89e3\u91ca\u6027\u5fae\u8c03\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u7c7b\u80fd\u529b\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM\u662f\u4e00\u79cd\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\uff0c\u901a\u8fc74-DOF PnP\u4f30\u8ba1\u5668\u548cIMU\u534f\u65b9\u5dee\u8054\u5408\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u89c6\u89c9\u9000\u5316\u548c\u4f4e\u52a0\u901f\u5ea6\u52a8\u6001\u4e0b\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u89c6\u89c9\u60ef\u6027SLAM\u9762\u4e34\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u52a0\u901f\u5ea6\u52a8\u6001\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "method": "\u5229\u7528\u7acb\u4f53\u76f8\u673a\u7684\u76f4\u63a5\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u6d88\u9664IMU\u521d\u59cb\u5316\u65f6\u7684\u5c3a\u5ea6\u4f30\u8ba1\u9700\u6c42\uff1b\u901a\u8fc7\u7cbe\u786e\u7684\u91cd\u529b\u521d\u59cb\u5316\uff0c\u89e3\u8026\u4fef\u4ef0\u548c\u6a2a\u6eda\u89d2\uff0c\u91c7\u75284-DOF PnP\u95ee\u9898\u6c42\u89e3\u59ff\u6001\u8ddf\u8e2a\uff1b\u63d0\u51fa\u504f\u5dee\u6d88\u9664\u76844-DOF PnP\u4f30\u8ba1\u5668\uff0c\u5e76\u8054\u5408\u4f30\u8ba1IMU\u534f\u65b9\u5dee\u4ee5\u5904\u7406\u52a8\u6001\u8fd0\u52a8\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGeVI-SLAM\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "GeVI-SLAM \u901a\u8fc7\u91cd\u529b\u589e\u5f3a\u548c4-DOF PnP\u4f30\u8ba1\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u89c6\u89c9\u60ef\u6027SLAM\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u4f4e\u52a0\u901f\u5ea6\u52a8\u6001\u548c\u89c6\u89c9\u9000\u5316\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner\u662f\u9996\u4e2a\u591a\u6570\u636e\u96c6\u6574\u5408\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7HFTDN\u3001GFTM\u548cS2D\u4e09\u9879\u521b\u65b0\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\u548c\u9ad8\u6548\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u89c4\u5212\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u7684\u8f68\u8ff9\u5206\u5e03\u548c\u5173\u8054\u5177\u6709\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u591a\u6570\u636e\u96c6\u6574\u5408\u7684\u89c4\u5212\u6846\u67b6\u3002", "method": "1. \u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc\uff08HFTDN\uff09\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\uff1b2. \u65e0\u68af\u5ea6\u8f68\u8ff9\u6620\u5c04\u5668\uff08GFTM\uff09\u5b66\u4e60\u591a\u6570\u636e\u96c6\u7684\u5386\u53f2-\u672a\u6765\u5173\u8054\uff1b3. \u7a00\u758f\u5230\u5bc6\u96c6\uff08S2D\uff09\u8303\u5f0f\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5206\u522b\u4f18\u5316\u5148\u9a8c\u4f7f\u7528\u3002", "result": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u8f68\u8ff9\u89c4\u5212\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "UniPlanner\u901a\u8fc7\u591a\u6570\u636e\u96c6\u6574\u5408\u548c\u4e09\u9879\u534f\u540c\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4e3a\u591a\u6570\u636e\u96c6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "UHKD\u901a\u8fc7\u9891\u7387\u57df\u7279\u5f81\u5bf9\u9f50\u6539\u8fdb\u5f02\u6784\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f02\u6784\u6a21\u578b\u95f4\u7684\u8bed\u4e49\u5dee\u5f02\u9650\u5236\u4e86\u4e2d\u95f4\u7279\u5f81\u7684\u5229\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5f02\u6784\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u4ee5\u5145\u5206\u5229\u7528\u4e2d\u95f4\u5c42\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51faUHKD\u6846\u67b6\uff0c\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u6355\u6349\u5168\u5c40\u7279\u5f81\uff0c\u7ed3\u5408FTM\u548cFAM\u6a21\u5757\u5b9e\u73b0\u7279\u5f81\u8f6c\u6362\u4e0e\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728CIFAR-100\u548cImageNet-1K\u4e0a\u5206\u522b\u53d6\u5f975.59%\u548c0.83%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "UHKD\u6846\u67b6\u901a\u8fc7\u9891\u7387\u57df\u7279\u5f81\u5bf9\u9f50\u548c\u8054\u5408\u4f18\u5316\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u84b8\u998f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u673a\u5668\u4eba\u68c0\u67e5\uff0c\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u91cd\u65b0\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5df2\u77e5\u73af\u5883\u6a21\u578b\u6765\u89c4\u5212\u548c\u8ddf\u8e2a\u68c0\u67e5\u8def\u7ebf\uff0c\u4f46\u6a21\u578b\u4e0e\u5b9e\u9645\u73b0\u573a\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u5f02\uff08\u7531\u81ea\u7136\u6216\u4eba\u7c7b\u6d3b\u52a8\u5f15\u8d77\uff09\u53ef\u80fd\u5bfc\u81f4\u8868\u9762\u5f62\u6001\u53d8\u5316\u6216\u8def\u5f84\u963b\u585e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8fd9\u79cd\u5206\u5c42\u6846\u67b6\u3002", "method": "\u6846\u67b6\u5c06\u68c0\u67e5\u4efb\u52a1\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u751f\u6210\u521d\u59cb\u5168\u5c40\u89c6\u56fe\u8ba1\u5212\uff0c\u4ee5\u53ca\u6839\u636e\u5f53\u524d\u573a\u666f\u5f62\u6001\u8fdb\u884c\u5c40\u90e8\u89c6\u56fe\u91cd\u65b0\u89c4\u5212\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fdd\u7559\u4e86\u5168\u5c40\u8986\u76d6\u76ee\u6807\uff0c\u540c\u65f6\u5141\u8bb8\u5bf9\u5c40\u90e8\u5f62\u6001\u53d8\u5316\u505a\u51fa\u53cd\u5e94\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u5730\u4e0b\u77ff\u4e95\u4e2d\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u90e8\u7f72\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u65e8\u5728\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u673a\u5668\u4eba\u68c0\u67e5\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u91cd\u65b0\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u5728\u73af\u5883\u53d8\u5316\u65f6\u7684\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "MGA\u662f\u4e00\u79cd\u65b0\u578bGUI\u4ee3\u7406\uff0c\u901a\u8fc7'\u5148\u89c2\u5bdf\u540e\u51b3\u7b56'\u673a\u5236\u548c\u52a8\u6001\u8bb0\u5fc6\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\uff0c\u4ee5\u53ca'\u51b3\u7b56\u4f18\u5148\u3001\u89c2\u5bdf\u6ede\u540e'\u673a\u5236\u5ffd\u89c6\u5173\u952e\u754c\u9762\u63d0\u793a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347GUI\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "MGA\u5c06\u6bcf\u4e00\u6b65\u5efa\u6a21\u4e3a\u4e00\u4e2a\u72ec\u7acb\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u73af\u5883\u72b6\u6001\uff0c\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff1a\u5f53\u524d\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u4ee5\u53ca\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\uff08\u5982Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MGA\u901a\u8fc7\u5f15\u5165'\u5148\u89c2\u5bdf\u540e\u51b3\u7b56'\u7684\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GUI\u4ee3\u7406\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u6062\u590d\u72d7\u7684\u8fd0\u52a8\u3002\u5b83\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u72d7\u8fd0\u52a8\u6570\u636e\u96c6\u5728\u591a\u89c6\u89d2\u3001\u771f\u5b9e3D\u6570\u636e\u3001\u89c4\u6a21\u548c\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cDogMo\u65e8\u5728\u5f25\u8865\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5b9e\u4f8b\u7279\u5b9a\u4f18\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u7c97\u5bf9\u9f50\u3001\u5bc6\u96c6\u5bf9\u5e94\u76d1\u7763\u548c\u65f6\u95f4\u6b63\u5219\u5316\u9010\u6b65\u4f18\u5316SMAL\u6a21\u578b\u4ee5\u9002\u5e94\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "DogMo\u5305\u542b1.2k\u4e2a\u8fd0\u52a8\u5e8f\u5217\uff0c\u8986\u76d610\u79cd\u4e0d\u540c\u72ac\u79cd\uff0c\u5efa\u7acb\u4e86\u56db\u79cd\u8fd0\u52a8\u6062\u590d\u57fa\u51c6\u8bbe\u7f6e\uff0c\u652f\u6301\u5355\u76ee\u548c\u591a\u89c6\u89d2\u3001RGB\u548cRGB-D\u8f93\u5165\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "DogMo\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u72d7\u7684\u8fd0\u52a8\u6062\u590d\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u52a8\u7269\u884c\u4e3a\u5efa\u6a21\u9886\u57df\u7684\u4ea4\u53c9\u7814\u7a76\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUIC\u6846\u67b6\uff0c\u901a\u8fc7MAP\u548cGP\u5148\u9a8c\u89e3\u51b3\u6c34\u4e0bSLAM\u7cfb\u7edf\u4e2d\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f20\u611f\u5668\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709DVL\u6821\u51c6\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u4f20\u611f\u5668\u914d\u7f6e\u6216\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u5047\u8bbe\uff0c\u4e14\u672a\u8054\u5408\u4f30\u8ba1\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u8fed\u4ee3\u6821\u51c6\uff08UIC\uff09\u6846\u67b6\uff0c\u91c7\u7528\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u4f30\u8ba1\u548c\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u9ad8\u4fdd\u771f\u8fd0\u52a8\u63d2\u503c\uff0c\u4ea4\u66ff\u8fdb\u884c\u9ad8\u6548\u7684GP\u8fd0\u52a8\u72b6\u6001\u66f4\u65b0\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u51c6\u53d8\u91cf\u66f4\u65b0\u3002", "result": "UIC\u6846\u67b6\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8eIMU\u3001\u76f8\u673a\u7b49\u591a\u79cd\u4f20\u611f\u5668\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90DVL-\u76f8\u673a\u6821\u51c6\u5de5\u5177\u7bb1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684UIC\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6c34\u4e0bSLAM\u7cfb\u7edf\uff0c\u8fd8\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MCP\u5de5\u5177\u53d1\u73b0\u548c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728MCP\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u7684\u670d\u52a1\u5668\u5c11\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u5de5\u7ba1\u7406\uff0c\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165\u4e86MCP-Flow\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684web-agent\u9a71\u52a8\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "MCP-Flow\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u5e76\u7b5b\u9009\u6570\u636e\uff0c\u751f\u6210\u4e8668733\u4e2a\u9ad8\u8d28\u91cf\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u4e2a\u8f68\u8ff9\uff0c\u8fdc\u8d85\u5148\u524d\u5de5\u4f5c\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "conclusion": "MCP-Flow\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u3002"}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "ETC\u6846\u67b6\u901a\u8fc7\u8d8b\u52bf\u4e00\u81f4\u6027\u9884\u6d4b\u548c\u8bef\u5dee\u5bb9\u5fcd\u641c\u7d22\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u91c7\u6837\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u8bad\u7ec3\u65e0\u5173\u7684\u52a0\u901f\u65b9\u6cd5\u901a\u8fc7\u590d\u7528\u6a21\u578b\u8f93\u51fa\u6765\u52a0\u901f\uff0c\u4f46\u5ffd\u89c6\u4e86\u53bb\u566a\u8d8b\u52bf\u548c\u6a21\u578b\u7279\u5b9a\u8bef\u5dee\u5bb9\u5fcd\u5ea6\uff0c\u5bfc\u81f4\u8f68\u8ff9\u504f\u5dee\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "ETC\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u6838\u5fc3\u90e8\u5206\uff1a(1) \u8d8b\u52bf\u4e00\u81f4\u6027\u9884\u6d4b\u5668\uff0c\u5229\u7528\u6269\u6563\u8f68\u8ff9\u7684\u5e73\u6ed1\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u5386\u53f2\u53bb\u566a\u6a21\u5f0f\u9884\u6d4b\u7a33\u5b9a\u7684\u672a\u6765\u65b9\u5411\uff0c\u5e76\u5728\u591a\u6b65\u8fd1\u4f3c\u4e2d\u6e10\u8fdb\u5206\u5e03\u4ee5\u5b9e\u73b0\u65e0\u504f\u5dee\u52a0\u901f\uff1b(2) \u6a21\u578b\u7279\u5b9a\u8bef\u5dee\u5bb9\u5fcd\u641c\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u8bc6\u522b\u4ece\u8bed\u4e49\u89c4\u5212\u5230\u8d28\u91cf\u4f18\u5316\u7684\u8fc7\u6e21\u70b9\uff0c\u63a8\u5bfc\u51fa\u6821\u6b63\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cETC\u5b9e\u73b0\u4e862.65\u500d\u4e8eFLUX\u7684\u52a0\u901f\u6548\u679c\uff0c\u4e14\u4e00\u81f4\u6027\u9000\u5316\u53ef\u5ffd\u7565\uff08-0.074 SSIM\u5206\u6570\uff09\u3002", "conclusion": "ETC\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8d8b\u52bf\u4e00\u81f4\u6027\u9884\u6d4b\u5668\u548c\u6a21\u578b\u7279\u5b9a\u8bef\u5dee\u5bb9\u5fcd\u641c\u7d22\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u52a0\u901f\u8fc7\u7a0b\u4e2d\u56e0\u8f93\u51fa\u590d\u7528\u5bfc\u81f4\u7684\u8f68\u8ff9\u504f\u5dee\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u4e14\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u673a\u5668\u4eba\u2018Olympus\u2019\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8df3\u8dc3\uff0c\u6c34\u5e73\u8df3\u8dc3\u8fbe1.25\u7c73\uff0c\u5782\u76f4\u8df3\u8dc3\u8fbe1.0\u7c73\uff0c\u5e76\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u89e3\u9501\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u8df3\u8dc3\u4efb\u52a1\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u52a8\u6001\u884c\u4e3a\u63a2\u7d22\u56f0\u96be\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u8df3\u8dc3\u6027\u80fd\u548c\u7cbe\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff0c\u5229\u7528\u629b\u4f53\u8fd0\u52a8\u5b9a\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u7684\u8df3\u8dc3\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u7684\u63a2\u7d22\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u8f68\u8ff9\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u884c\u8d70\u7b56\u7565\u4ee5\u89e3\u9501\u66f4\u591a\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6c34\u5e73\u8df3\u8dc3\u53ef\u8fbe1.25\u7c73\uff08\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff09\u548c\u5782\u76f4\u8df3\u8dc31.0\u7c73\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u5373\u53ef\u5b66\u4e60\u5168\u5411\u8df3\u8dc3\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u8bad\u7ec3\u51fa\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6027\u80fd\u7684\u8df3\u8dc3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u2018Olympus\u2019\u7684\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5353\u8d8a\u7684\u8df3\u8dc3\u6027\u80fd\uff0c\u6709\u6548\u8de8\u8d8a\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u66ff\u4ee3MCTS\u4e2d\u968f\u673a\u7b56\u7565\u7684\u62bd\u8c61\u5185\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u548c\u4f7f\u7528\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u5230\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u4e2d\u591a\u4e2a\u52a8\u4f5c\u5177\u6709\u76f8\u540cUCB\u503c\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u548c\u4f7f\u7528\u72b6\u6001\u548c/\u6216\u52a8\u4f5c\u62bd\u8c61\u4e0e\u6811\u641c\u7d22\u5e76\u884c\uff0c\u4ee5\u5171\u4eab\u540c\u4e00\u5c42\u8282\u70b9\u4e4b\u95f4\u7684\u4fe1\u606f\u3002\u5e76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u51e0\u79cd\u66ff\u4ee3\u7684\u62bd\u8c61\u5185\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u51e0\u79cd\u62bd\u8c61\u5185\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u66ff\u4ee3\u7684\u62bd\u8c61\u5185\u7b56\u7565\uff0c\u5176\u4e2d\u4e00\u4e9b\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u8c61\u4e2d\u5fc3\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u6211\u7cbe\u70bc\u548c\u663e\u5f0f\u5e03\u5c40\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7ec4\u5408\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u867d\u7136\u5728\u771f\u5b9e\u611f\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec4\u5408\u6027\uff08\u5982\u5bf9\u8c61\u6570\u91cf\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u8f93\u5165\u63d0\u793a\u4e2d\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5e03\u5c40\u6ce8\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5bf9\u591a\u4e2a\u5019\u9009\u56fe\u50cf\u8fdb\u884c\u91cd\u65b0\u6392\u540d\uff0c\u8fed\u4ee3\u9009\u62e9\u6700\u7b26\u5408\u63d0\u793a\u7684\u7ed3\u679c\u3002", "result": "\u6846\u67b6\u5728\u4fdd\u6301\u7f8e\u5b66\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u573a\u666f\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u548c\u81ea\u6211\u7cbe\u70bc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ec4\u5408\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u573a\u666f\u5bf9\u9f50\u3002"}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "GroundLoc\u662f\u4e00\u79cd\u9ad8\u6548\u7684LiDAR\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\uff0c\u5b58\u50a8\u9700\u6c42\u4f4e\u3002", "motivation": "\u4e3a\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u7684LiDAR-only\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6295\u5f71\u548cR2D2\u6216SIFT\u8fdb\u884c\u5173\u952e\u70b9\u8bc6\u522b\u4e0e\u5730\u56fe\u6ce8\u518c\u3002", "result": "\u5728SemanticKITTI\u548cHeLiPR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u8f68\u8ff9\u8bef\u5dee\u4f4e\u4e8e50\u5398\u7c73\u3002", "conclusion": "GroundLoc\u5728\u591a\u79cd\u4f20\u611f\u5668\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u6a21\u578b\uff0c\u4e14\u5b58\u50a8\u9700\u6c42\u4f4e\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7LLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u6027\u77e9\u9635\u6392\u540d\u8bc4\u4f30\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\uff0c\u8bbe\u8ba1Self-Indicator\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6613\u51fa\u9519\u548c\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\u4f7f\u5f97\u5982\u4f55\u6709\u6548\u4e14\u9ad8\u6548\u5730\u68c0\u67e5\u5176\u8f93\u51fa\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u9ad8\u4e14\u9002\u7528\u6027\u6709\u9650\u3002", "method": "\u7814\u7a76\u5229\u7528\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u77e9\u9635\u6392\u540d\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u7684\u6307\u6807\uff0c\u8bbe\u8ba1\u4e86Self-Indicator\u65b9\u6cd5\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u91cd\u65b0\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSelf-Indicator\u5728\u591a\u79cd\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6b63\u786e\u533a\u5206\u63a8\u7406\u8def\u5f84\u7684\u51c6\u786e\u7387\u8d85\u8fc775%\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u5347\u4e868%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf-Indicator\u7684\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u6027\u77e9\u9635\u6392\u540d\u6765\u8bc4\u4f30\u63a8\u7406\u8def\u5f84\u7684\u53ef\u4fe1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "VC4VG\u662f\u4e00\u4e2a\u4e13\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4f18\u5316\u7684\u5b57\u5e55\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5b57\u5e55\u9700\u6c42\u548c\u6784\u5efa\u65b0\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9T2V\u8bad\u7ec3\u7684\u89c6\u9891\u5b57\u5e55\u4f18\u5316\u7b56\u7565\u7814\u7a76\u4e0d\u8db3\uff0c\u9ad8\u8d28\u91cf\u89c6\u9891\u6587\u672c\u5bf9\u5bf9\u9f50\u5bf9\u751f\u6210\u8fde\u8d2f\u89c6\u9891\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86VC4VG\u6846\u67b6\uff0c\u5305\u62ec\u4eceT2V\u89d2\u5ea6\u5206\u6790\u5b57\u5e55\u5185\u5bb9\u3001\u5206\u89e3\u89c6\u9891\u91cd\u5efa\u6240\u9700\u8981\u7d20\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u5957\u5b57\u5e55\u4f18\u5316\u65b9\u6cd5\u3002\u540c\u65f6\u6784\u5efa\u4e86VC4VG-Bench\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5b57\u5e55\u8d28\u91cf\u63d0\u5347\u4e0e\u89c6\u9891\u751f\u6210\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "VC4VG\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u89c6\u9891\u5b57\u5e55\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u5de5\u5177\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "\u63d0\u51faCVAE-T\u6a21\u578b\u751f\u6210\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\uff0c\u7528\u4e8e\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u9a8c\u8bc1\u548c\u6570\u636e\u589e\u5f3a\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u80fd\u51c6\u786e\u91cd\u5efa\u5e76\u751f\u6210\u591a\u6837\u5316\u573a\u666f\uff0c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u53ef\u89e3\u91ca\u7684\u7ef4\u5ea6\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u5728\u91cf\u4ea7\u8f66\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u529f\u80fd\u548c\u9c81\u68d2\u6027\u9762\u4e34\u66f4\u5927\u6311\u6218\u3002\u57fa\u4e8e\u573a\u666f\u7684\u865a\u62df\u6d4b\u8bd5\u5728\u65f6\u95f4\u3001\u6210\u672c\u6548\u7387\u3001\u53ef\u91cd\u590d\u6027\u548c\u8fb9\u7f18\u6848\u4f8b\u63a2\u7d22\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdTransformer\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE-T\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u73af\u5c9b\u7b49\u590d\u6742\u73af\u5883\u4e0b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u539f\u59cb\u573a\u666f\u5e76\u751f\u6210\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u5408\u6210\u573a\u666f\u3002\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u663e\u793a\u90e8\u5206\u89e3\u7ea0\u7f20\uff0c\u591a\u4e2a\u6f5c\u5728\u7ef4\u5ea6\u5bf9\u573a\u666f\u5c5e\u6027\uff08\u5982\u8f66\u8f86\u8fdb\u5165/\u9000\u51fa\u65f6\u95f4\u548c\u901f\u5ea6\u66f2\u7ebf\uff09\u5177\u6709\u660e\u663e\u4e14\u53ef\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u7528\u4e8e\u9a8c\u8bc1\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u7684\u573a\u666f\uff0c\u5e76\u80fd\u4e3a\u5176\u5f00\u53d1\u548c\u8fed\u4ee3\u6539\u8fdb\u63d0\u4f9b\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u540cLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff08ArgLLM\u3001RbAM\u3001RAG-ArgLLM\uff09\u7ec4\u5408\u8bc1\u636e\uff0c\u63d0\u5347\u5224\u65ad\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c06\u5224\u65ad\u9884\u6d4b\u4efb\u52a1\u89c6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7684\u4e0d\u540c\u89c2\u70b9\u548c\u8bc1\u636e\u6765\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u58f0\u660e\u9a8c\u8bc1\uff0c\u5305\u62ecArgLLM\u3001RbAM\u548cRAG-ArgLLM\u4e09\u79cd\u667a\u80fd\u4f53\uff0c\u5e76\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u53ef\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002"}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "MSRANetV2\u662f\u4e00\u79cd\u4f18\u5316\u7684CNN\u67b6\u6784\uff0c\u7528\u4e8e\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u5206\u7c7b\uff0c\u7ed3\u5408\u6b8b\u5dee\u6ce8\u610f\u529b\u548cSE\u5757\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u662f\u5168\u7403\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u8017\u65f6\u4e14\u6613\u53d8\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u53ef\u63d0\u5347\u8bca\u65ad\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSRANetV2\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u91c7\u7528ResNet50V2\u4e3b\u5e72\uff0c\u7ed3\u5408\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u548cSE\u5757\uff0c\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u548c\u4e0a\u91c7\u6837\u64cd\u4f5c\u878d\u5408\u591a\u5c3a\u5ea6\u8868\u793a\u3002", "result": "\u5728CRC-VAL-HE-7K\u548cNCT-CRC-HE-100K\u6570\u636e\u96c6\u4e0a\uff0cMSRANetV2\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747Precision\u3001recall\u3001F1-score\u3001AUC\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u5747\u63a5\u8fd10.99\uff0cGrad-CAM\u53ef\u89c6\u5316\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MSRANetV2\u88ab\u9a8c\u8bc1\u4e3a\u4e00\u79cd\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6027\u80fd\u7684\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u5206\u7c7b\u67b6\u6784\u6a21\u578b\u3002"}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u60ef\u6027\u4f20\u611f\u5668\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u6709\u6548\u9884\u6d4b\u5047\u80a2\u5173\u8282\u89d2\u5ea6\u548c\u6b65\u6001\uff0c\u63d0\u5347\u969c\u788d\u7269\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u52a8\u529b\u578b\u5927\u817f\u5047\u80a2\u4f7f\u7528\u8005\u5728\u590d\u6742\u5730\u5f62\u6216\u969c\u788d\u7269\u5bfc\u822a\u4e2d\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u60ef\u6027\u4f20\u611f\u5668\u5728\u5065\u5eb7\u811a\u8e1d\u4e0a\u5f15\u5bfc\u8de8\u8d8a\u969c\u788d\u7269\u7684\u52a8\u4f5c\uff0c\u9057\u4f20\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4ee5\u9884\u6d4b\u5927\u817f\u548c\u819d\u5173\u8282\u6240\u9700\u89d2\u5ea6\uff0c\u6b65\u6001\u8fdb\u5c55\u9884\u6d4b\u7b97\u6cd5\u786e\u5b9a\u5047\u80a2\u819d\u5173\u8282\u7535\u673a\u7684\u9a71\u52a8\u89d2\u5ea6\u7d22\u5f15\u3002", "result": "\u5f53\u6dfb\u52a0\u5230\u5927\u817f\u89d2\u5ea6\u6570\u636e\u7684\u9ad8\u65af\u566a\u58f0\u6807\u51c6\u5dee\u5c0f\u4e8e1\u65f6\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6d88\u9664\u566a\u58f0\u5e72\u6270\uff0c\u5728150 Hz\u4e0b\u5b9e\u73b0100%\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u51c6\u786e\u5ea6\uff0c\u5927\u817f\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e3a8.71%\uff0c\u819d\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e3a6.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6b65\u6001\u8fdb\u5c55\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u4e3a\u52a8\u529b\u578b\u5927\u817f\u5047\u80a2\u7684\u969c\u788d\u7269\u5bfc\u822a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8gLLMs\u5728\u4f20\u64ad\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u5e94\u5bf9\u4e03\u5927\u5173\u952e\u6311\u6218\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u548c\u53ef\u53ca\u6027\u3002", "motivation": "gLLMs\u5728\u4f20\u64ad\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u6709\u6548\u6574\u5408\u5230\u7814\u7a76\u65b9\u6cd5\u4e2d\u4ecd\u5f85\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6574\u5408\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u548c\u53ef\u53ca\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u5408\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u5305\u62ec\u4ee3\u7801\u4e66\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u6574\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u6a21\u578b\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u63d0\u5347\u7b49\u4e03\u4e2a\u5173\u952e\u6b65\u9aa4\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5e94\u5bf9gLLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u4e03\u5927\u6311\u6218\uff0c\u786e\u4fdd\u7814\u7a76\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08gLLMs\uff09\u5728\u4f20\u64ad\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e94\u5bf9\u4e03\u5927\u5173\u952e\u6311\u6218\uff0c\u65e8\u5728\u4f7fgLLM\u8f85\u52a9\u7684\u5185\u5bb9\u5206\u6790\u66f4\u6613\u4e8e\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e76\u786e\u4fdd\u7b26\u5408\u5b66\u79d1\u7684\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u63d0\u793a\u548c\u7a7a\u95f4\u57fa\u7840\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684\u591a\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u7b49\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "1. \u6df7\u5408\u63d0\u793a\u8def\u7531\u5668\u5206\u7c7b\u95ee\u9898\u5e76\u5206\u53d1\u81f3\u4e13\u5bb6\u63d0\u793a\uff1b2. \u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5d4c\u5165\u5750\u6807\u7cfb\u3001\u7a7a\u95f4\u63a8\u7406\u89c4\u5219\u3001\u89d2\u8272\u626e\u6f14\u7b49\uff1b3. \u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\u6574\u5408\u591a\u89c6\u89d2\u56fe\u50cf\uff1b4. \u6309\u4efb\u52a1\u914d\u7f6e\u63a8\u7406\u53c2\u6570\u3002", "result": "\u5728Qwen2.5-VL-72B\u4e0a\u5b9e\u73b0Phase-1\uff08\u5e72\u51c0\u6570\u636e\uff0970.87%\u548cPhase-2\uff08\u6c61\u67d3\u6570\u636e\uff0972.85%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u57fa\u7840\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u5728\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare\u6846\u67b6\u901a\u8fc7\u5d4c\u5165OOD\u68c0\u6d4b\u4e0e\u8bc6\u522b\u529f\u80fd\uff0c\u65e0\u9700\u663e\u5f0f\u6545\u969c\u6570\u636e\u5373\u53ef\u6784\u5efa\u6545\u969c\u6062\u590d\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9c81\u68d2\u6027\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728OOD\u573a\u666f\u4e2d\u6613\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u7684\u6545\u969c\uff0c\u9700\u8981\u80fd\u81ea\u52a8\u68c0\u6d4b\u5e76\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u7b56\u7565\u3002", "method": "Fare\u6846\u67b6\u901a\u8fc7\u5d4c\u5165OOD\u68c0\u6d4b\u4e0e\u8bc6\u522b\u529f\u80fd\uff0c\u5e76\u7ed3\u5408\u6062\u590d\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6784\u5efa\u6545\u969c\u6062\u590d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFare\u80fd\u5728\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u67b6\u6784\u4e2d\u5b9e\u73b0\u6545\u969c\u6062\u590d\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9c81\u68d2\u6027\u3002", "conclusion": "Fare\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u5177\u5907\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u6545\u969c\u6570\u636e\u5373\u53ef\u5b9e\u73b0OOD\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u57fa\u4e8ePCS\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\u4f18\u4e8e\u73b0\u6709\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u6307\u5bfc\uff0c\u5f71\u54cd\u5176\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8ePCS\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6a21\u5757\u5316\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cVDSAgents\u8868\u73b0\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u3002", "conclusion": "VDSAgents\u901a\u8fc7\u5d4c\u5165PCS\u539f\u5219\u5728LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUAP-SAM2\uff0c\u4e00\u79cd\u9488\u5bf9SAM2\u7684\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u626b\u63cf\u548c\u53cc\u8bed\u4e49\u504f\u5dee\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22SAM2\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728SAM\u548cSAM2\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4ee5\u53ca\u7531\u67b6\u6784\u5dee\u5f02\u5e26\u6765\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faUAP-SAM2\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u8bed\u4e49\u504f\u5dee\u7684\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u62ec\u76ee\u6807\u626b\u63cf\u7b56\u7565\u548c\u53cc\u8bed\u4e49\u504f\u5dee\u6846\u67b6\u7684\u8bbe\u8ba1\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUAP-SAM2\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "UAP-SAM2\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5bf9\u6297SAM2\u65f6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u8fd0\u52a8\u5b66\u548c\u7a33\u5b9a\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u672a\u6765\u907f\u969c\u65b9\u6cd5\u7684\u8bbe\u8ba1\u4e0e\u6bd4\u8f83\u3002", "motivation": "\u5b9e\u65f6\u63a7\u5236\u662f\u673a\u5668\u4eba\u5728\u52a8\u6001\u7269\u4f53\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u5173\u952e\uff0c\u9700\u8981\u6539\u8fdb\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u8bbe\u8ba1\u8003\u91cf\uff08\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\uff09\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\uff0c\u5e76\u5229\u7528\u57fa\u672c\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u9a8c\u8bc1\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u5e38\u7f3a\u4e4f\u8fd0\u52a8\u5b66\u8003\u91cf\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u548c\u8fd0\u52a8\u66f2\u7ebf\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u672a\u6765\u53ef\u7528\u4e8e\u8bbe\u8ba1\u3001\u6bd4\u8f83\u548c\u57fa\u51c6\u6d4b\u8bd5\u907f\u969c\u65b9\u6cd5\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u63d0\u51faBrain-like Space\u6982\u5ff5\uff0c\u7528\u4e8e\u7edf\u4e00\u6bd4\u8f83AI\u6a21\u578b\u4e0e\u5927\u8111\u7684\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u53d1\u73b0\u6a21\u578b\u8111\u76f8\u4f3c\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u4e14\u4e0e\u4efb\u52a1\u6027\u80fd\u4e0d\u5b8c\u5168\u4e00\u81f4\u3002", "motivation": "\u63a2\u8ba8AI\u6a21\u578b\u662f\u5426\u4ee5\u7c7b\u4f3c\u5927\u8111\u7684\u65b9\u5f0f\u7ec4\u7ec7\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u89c4\u8303\u7684\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\u4e0a\uff0c\u6784\u5efa\u4e86Brain-like Space\uff0c\u5e76\u5206\u6790\u4e86151\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728Brain-like Space\u4e2d\u5448\u73b0\u8fde\u7eed\u7684\u5f27\u72b6\u51e0\u4f55\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u8111\u76f8\u4f3c\u6027\uff0c\u4e14\u8fd9\u79cd\u76f8\u4f3c\u6027\u4e0d\u4ec5\u53d7\u6a21\u6001\u5f71\u54cd\uff0c\u8fd8\u4e0e\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u6709\u5173\u3002", "conclusion": "Brain-like Space\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u9886\u57df\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u667a\u80fd\uff0c\u63ed\u793a\u4e86\u8fde\u63a5\u673a\u5668\u4e0e\u5927\u8111\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u5219\u3002"}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "CLFSeg\u662f\u4e00\u79cd\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u5377\u79ef\u5c42\u7684\u5206\u5272\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u533b\u7597\u8bca\u65ad\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u6a21\u578b\u5728\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86CLFSeg\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CLFSeg\uff0c\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u6a21\u7cca\u5377\u79ef\uff08FC\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u4e86\u5377\u79ef\u5c42\u548c\u6a21\u7cca\u903b\u8f91\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u8bc6\u522b\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8fb9\u754c\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u566a\u58f0\u548c\u6a21\u7cca\u6027\u6765\u589e\u5f3a\u5206\u5272\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u548cdice\u635f\u5931\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08CVC-ColonDB\u3001CVC-ClinicDB\u3001EtisLaribPolypDB\u548cACDC\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLFSeg\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u6027\u80fd\uff0c\u5e76\u4e13\u6ce8\u4e8e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u5173\u611f\u5174\u8da3\u533a\u57df\u3002", "conclusion": "CLFSeg\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u5377\u79ef\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u533b\u7597\u8bca\u65ad\u573a\u666f\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7269\u7406\u8ba1\u7b97\u878d\u5165\u8f6f\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u4e09\u79cd\u7b56\u7565\u53ca\u5176\u5e94\u7528\uff0c\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u6280\u672f\u7684\u6700\u540e\u4e00\u4e2a\u524d\u6cbf\u9886\u57df\u662f\u8f6f\u5316\u548c\u96c6\u6210\u8ba1\u7b97\u673a\u4e0e\u63a7\u5236\u5668\uff0c\u4ee5\u5b9e\u73b0\u5176\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u7a33\u5065\u6027\u548c\u667a\u80fd\u6027\u3002\u7269\u7406\u8ba1\u7b97\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u72ec\u7279\u7684\u7269\u7406\u8ba1\u7b97\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\u3002", "result": "\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u4f7f\u8f6f\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\uff0c\u5982\u534f\u8c03\u8fd0\u52a8\u907f\u969c\u3001\u6709\u6548\u8f7d\u8377\u91cd\u91cf\u548c\u65b9\u5411\u5206\u7c7b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u53ef\u7f16\u7a0b\u64cd\u4f5c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u5c06\u7269\u7406\u8ba1\u7b97\u878d\u5165\u8f6f\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u7269\u7406\u8ba1\u7b97\u5728\u63d0\u5347\u8f6f\u673a\u5668\u4eba\u7a33\u5065\u6027\u548c\u667a\u80fd\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u533b\u7597AI\u901a\u5e38\u670d\u52a1\u4e8e\u5e73\u5747\u60a3\u8005\uff0c\u4f46\u5728\u8fb9\u7f18\u60a3\u8005\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u63d0\u4f9b\u4e2a\u4f53\u5316\u51b3\u7b56\u652f\u6301\uff0c\u65e8\u5728\u63d0\u5347\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u670d\u52a1\u4e8e\u5e73\u5747\u60a3\u8005\uff0c\u4f46\u5728\u8fb9\u7f18\u60a3\u8005\uff08\u5982\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u667a\u80fd\u4f53\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u6001\u805a\u7c7b\uff0c\u5171\u4eab\u6a21\u578b\u5e93\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\u3002\u7ed3\u679c\u5728\u534f\u8c03\u5c42\u4e2d\u6c47\u603b\uff0c\u6743\u8861\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u6700\u7ec8\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5305\u3002", "result": "\u9a8c\u8bc1\u4ece\u7fa4\u4f53\u5e73\u5747\u503c\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u8bef\u5dee\u3001\u5c0f\u6837\u672c\u6821\u51c6\u548c\u98ce\u9669-\u8986\u76d6\u6743\u8861\u6765\u8861\u91cf\u3002\u9884\u671f\u6311\u6218\u5305\u62ec\u8ba1\u7b97\u9700\u6c42\u3001\u81ea\u52a8\u5316\u504f\u89c1\u548c\u76d1\u7ba1\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u7f13\u5b58\u7b56\u7565\u3001\u5171\u8bc6\u68c0\u67e5\u548c\u81ea\u9002\u5e94\u8bd5\u9a8c\u6846\u67b6\u89e3\u51b3\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597AI\u4e0e\u533b\u5b66\u7684\u7b2c\u4e00\u539f\u5219\u4e00\u81f4\uff1a\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002"}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "MC-SJD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u635f\u7684\u5e76\u884c\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4.2\u500d\uff08\u56fe\u50cf\uff09\u548c13.3\u500d\uff08\u89c6\u9891\uff09\u3002", "motivation": "\u81ea\u56de\u5f52\u5efa\u6a21\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u56e0\u9010\u4ee4\u724c\u751f\u6210\u7684\u7f13\u6162\u63a8\u7406\u901f\u5ea6\uff08\u9700\u6570\u5343\u6b65\u751f\u6210\u5355\u4e2a\u6837\u672c\uff09\u800c\u53d7\u9650\uff0c\u9700\u89e3\u51b3\u6b64\u74f6\u9888\u3002", "method": "\u63d0\u51faMC-SJD\uff0c\u57fa\u4e8e\u4fe1\u606f\u8bba\u8026\u5408\u65b9\u6cd5\uff0c\u6700\u5927\u5316\u8fde\u7eed\u8fed\u4ee3\u4e2d\u91c7\u6837\u76f8\u540c\u8349\u7a3f\u4ee4\u724c\u7684\u6982\u7387\uff0c\u63d0\u5347SJD\u7684\u63a5\u53d7\u7387\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u5206\u522b\u5b9e\u73b0\u7ea64.2\u500d\u548c13.3\u500d\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\u3002", "conclusion": "MC-SJD\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u751f\u6210\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards\u662fAI\u4ee3\u7406\u7684\u673a\u5668\u53ef\u8bfb\u7ea6\u675f\u6807\u51c6\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u7ea6\u675f\u9075\u5faa\u3001\u81ea\u52a8\u9a8c\u8bc1\u548c\u5408\u89c4\u6027\uff0c\u6574\u5408\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u5728\u8fd0\u884c\u65f6\u9075\u5faa\u7ea6\u675f\u7684\u9700\u6c42\uff0c\u6574\u5408\u9ad8\u7ea7\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u7684\u53ef\u95ee\u8d23\u81ea\u6cbb\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u89c4\u8303\u5c42\uff0c\u7f16\u7801\u5141\u8bb8/\u62d2\u7edd\u89c4\u5219\u3001\u4e49\u52a1\u3001\u8bc1\u636e\u8981\u6c42\u548c\u4e0e\u4fdd\u969c\u6846\u67b6\uff08\u5982NIST AI RMF\u3001ISO/IEC 42001\u548cEU AI Act\uff09\u7684\u4ea4\u53c9\u6620\u5c04\u3002", "result": "Policy Cards\u53ef\u4ee5\u81ea\u52a8\u9a8c\u8bc1\u3001\u7248\u672c\u63a7\u5236\uff0c\u5e76\u4e0e\u8fd0\u884c\u65f6\u5f3a\u5236\u6267\u884c\u6216\u6301\u7eed\u5ba1\u8ba1\u7ba1\u9053\u94fe\u63a5\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u3002", "conclusion": "Policy Cards\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u673a\u5668\u53ef\u8bfb\u7684\u6807\u51c6\uff0c\u7528\u4e8e\u8868\u8fbe\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u6210\u4e3a\u90e8\u7f72\u4ee3\u7406\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u4fdd\u969c\u548c\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u9a8c\u8bc1\u5408\u89c4\u6027\u3002"}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "ID\u00b2Face \u662f\u4e00\u79cd\u8bad\u7ec3\u4e2d\u5fc3\u5316\u7684\u4eba\u8138\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u5b66\u4e60\u5b9e\u73b0\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u5c5e\u6027\u7684\u663e\u5f0f\u89e3\u8026\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u4f18\u5316\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u63a8\u7406\u65f6\u5e72\u9884\uff08\u5982\u8d1f\u6307\u5bfc\u6216\u57fa\u4e8e\u80fd\u91cf\u7684\u4f18\u5316\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\u548c\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u5c5e\u6027\u7ea0\u7f20\uff0c\u964d\u4f4e\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6570\u636e\u6548\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u8eab\u4efd\u63a9\u7801\u5b66\u4e60\u65b9\u6848\uff0c\u7ed3\u5408\u8eab\u4efd\u89e3\u8026\u6f5c\u5728\u91cd\u7ec4\u5668\u548c\u8eab\u4efd\u5f15\u5bfc\u6f5c\u5728\u534f\u8c03\u5668\uff0c\u901a\u8fc7\u8f6f\u95e8\u63a7\u878d\u5408\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6b63\u4ea4\u8eab\u4efd\u6620\u5c04\u7b56\u7565\u6291\u5236\u8eab\u4efd\u6cc4\u6f0f\u3002", "result": "ID\u00b2Face \u5728\u533f\u540d\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u6570\u636e\u6548\u7528\uff0c\u540c\u65f6\u6709\u6548\u6291\u5236\u4e86\u8eab\u4efd\u4fe1\u606f\u3002", "conclusion": "ID\u00b2Face \u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8eab\u4efd\u6291\u5236\u548c\u6548\u7528\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e2d\u5fc3\u5316\u7684\u533f\u540d\u5316\u6846\u67b6\u548c\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4e14\u53ef\u63a7\u7684\u533f\u540d\u5316\u3002"}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "GenTrack\u662f\u4e00\u79cd\u65b0\u578b\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u8ddf\u8e2a\u65b9\u5f0f\uff0c\u5229\u7528PSO\u548c\u793e\u4ea4\u4ea4\u4e92\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u76ee\u6807\u8eab\u4efd\u4e00\u81f4\u6027\u7ef4\u62a4\u3001\u975e\u7ebf\u6027\u52a8\u6001\u7ba1\u7406\u3001\u906e\u6321\u671f\u95f4\u7684ID\u5207\u6362\u548c\u8f68\u8ff9\u4e22\u5931\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u65b9\u5f0f\u5904\u7406\u76ee\u6807\u6570\u91cf\u672a\u77e5\u548c\u65f6\u53d8\u7684\u60c5\u51b5\uff0c\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u548c\u63d0\u51fa\u7684\u9002\u5e94\u5ea6\u5ea6\u91cf\u5f15\u5bfc\u7c92\u5b50\uff0c\u6574\u5408\u76ee\u6807\u95f4\u793e\u4ea4\u4ea4\u4e92\u4ee5\u589e\u5f3a\u8ddf\u8e2a\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eGenTrack\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u8ddf\u8e2a\u5668\uff0c\u5c24\u5176\u662f\u5728\u5f31\u566a\u58f0\u68c0\u6d4b\u5668\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "GenTrack\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e90\u4ee3\u7801\u5b9e\u73b0\u4ee5\u4fbf\u516c\u5e73\u6bd4\u8f83\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u548c\u5e76\u884c\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728Web\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u63a8\u7406\u6548\u7387\u4e0e\u8d28\u91cf\u96be\u4ee5\u517c\u987e\u7684\u6311\u6218\uff0c\u6ee1\u8db3\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u7684\u4ea4\u4e92\u670d\u52a1\u9700\u6c42\u3002", "method": "Orion\u5c06\u5355\u4e2a\u67e5\u8be2\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u5173\u952e\u70b9\u751f\u6210\uff08\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\uff09\u548c\u5185\u5bb9\u5e76\u884c\u6269\u5c55\uff08\u57fa\u4e8e\u4f9d\u8d56\u56fe\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\uff09\uff0c\u5e76\u5f15\u5165\u7ba1\u9053\u8c03\u5ea6\u673a\u5236\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOrion\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u6bd4\u57fa\u7ebf\u5feb4.33\u500d\uff0c\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e3.42\u500d\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe18.75%\u3002", "conclusion": "Orion\u6846\u67b6\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u9ad8\u6548\u7387\u548c\u9ad8\u8d28\u91cf\u7684\u53cc\u91cd\u8981\u6c42\u3002"}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "SCOPE\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\u4f18\u5316\u89c6\u89c9\u4ee4\u724c\u526a\u679d\uff0c\u63d0\u5347\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u5b8c\u6574\uff0c\u9700\u8054\u5408\u8003\u8651\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\u3002", "method": "\u63d0\u51faSCOPE\u8bc4\u5206\uff0c\u7ed3\u5408\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\u589e\u76ca\uff0c\u8fed\u4ee3\u9009\u62e9\u8bc4\u5206\u6700\u9ad8\u7684\u4ee4\u724c\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCOPE\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SCOPE\u7b56\u7565\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7684\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u673a\u5236\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c92\u5b50\u6ee4\u6ce2\u548c\u786e\u5b9a\u6027\u5173\u8054\u786e\u4fdd\u6807\u8bc6\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u548c\u65f6\u53d8\u76ee\u6807\u6570\u91cf\u4e0b\u7684\u975e\u7ebf\u6027\u52a8\u6001\u8ddf\u8e2a\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u591a\u76ee\u6807\u4ea4\u4e92\u548c\u957f\u65f6\u95f4\u906e\u6321\u4e0b\u7684\u6807\u8bc6\u4e00\u81f4\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u8054\u5408\u4f7f\u7528\u968f\u673a\u7c92\u5b50\u6ee4\u6ce2\u548c\u786e\u5b9a\u6027\u5173\u8054\u673a\u5236\uff0c\u7c92\u5b50\u6ee4\u6ce2\u5904\u7406\u975e\u7ebf\u6027\u548c\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u7c92\u5b50\u7fa4\u4f18\u5316\u5f15\u5bfc\u7c92\u5b50\u671d\u5411\u72b6\u6001\u5206\u5e03\u6a21\u5f0f\uff0c\u786e\u5b9a\u6027\u5173\u8054\u901a\u8fc7\u6210\u672c\u77e9\u9635\u786e\u4fdd\u6807\u8bc6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u9884\u5f55\u89c6\u9891\u548c\u5b9e\u65f6\u6444\u50cf\u6d41\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u8ddf\u8e2a\u5668\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u673a\u5236\u7684\u591a\u76ee\u6807\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c92\u5b50\u6ee4\u6ce2\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u5904\u7406\u975e\u7ebf\u6027\u548c\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u5e76\u5229\u7528\u786e\u5b9a\u6027\u5173\u8054\u786e\u4fdd\u6807\u8bc6\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u8ddf\u8e2a\u5668\u3002"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "APTBench \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u4efb\u52a1\u8f6c\u5316\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u95ee\u9898\uff0c\u586b\u8865\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u4ee3\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u548c\u9759\u6001\u6280\u80fd\uff0c\u672a\u80fd\u53cd\u6620\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u800c\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u8bbe\u8ba1\u7528\u4e8e\u540e\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "method": "\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u4ee3\u7406\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u5316\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9009\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\uff08\u5982\u89c4\u5212\u548c\u884c\u52a8\uff09\u3002", "result": "APTBench \u76f8\u6bd4\u73b0\u6709\u7684\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u66f4\u6709\u6548\u5730\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u540e\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u8bc4\u4f30\u66f4\u8f7b\u91cf\u548c\u7ecf\u6d4e\u3002", "conclusion": "APTBench \u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u548c\u6307\u5bfc LLM \u7684\u4ee3\u7406\u6f5c\u529b\uff0c\u586b\u8865\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u65b0\u578b\u5fae\u773c\u52a8\u6570\u636e\u96c6\uff0c\u5229\u7528Spiking-VGG\u6a21\u578b\u5b9e\u73b0\u4e86\u7ea690%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u7814\u7a76\u8bbe\u7acb\u4e86\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edf\u5fae\u773c\u52a8\u7814\u7a76\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u6709\u9650\uff0c\u4e8b\u4ef6\u4f20\u611f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Blender\u6e32\u67d3\u9ad8\u4fdd\u771f\u773c\u52a8\u573a\u666f\uff0c\u6a21\u62df\u4e0d\u540c\u89d2\u4f4d\u79fb\u7684\u5fae\u773c\u52a8\uff0c\u5e76\u901a\u8fc7v2e\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u6d41\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdSpiking-VGG\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u7ea690%\uff0c\u80fd\u72ec\u7acb\u4e8e\u4e8b\u4ef6\u6570\u91cf\u6216\u6301\u7eed\u65f6\u95f4\u5bf9\u5fae\u773c\u52a8\u8fdb\u884c\u5206\u7c7b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u548c\u5f15\u5165\u7279\u6743\u5f15\u5bfc\u7b56\u7565\uff0cSNN\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3SNN\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e24\u5927\u6311\u6218\uff1a\u975e\u53ef\u5fae\u7684\u8109\u51b2\u795e\u7ecf\u5143\u548c\u72b6\u6001\u52a8\u6001\u5bfc\u81f4\u7684\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u66ff\u4ee3\u68af\u5ea6\u7684\u659c\u7387\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u548c\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u3002", "result": "\u5728RL\u73af\u5883\u4e2d\uff0c\u6d45\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u53472.1\u500d\uff1b\u5728\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u7684\u659c\u7387\u8bbe\u7f6e\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u73af\u5883\u4e2d\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u548cOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u4ee3\u7406\u7684\u5b89\u5168\u6027\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u9274\u4e8eVLM\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u79fb\u52a8\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\uff08\u5982\u7cfb\u7edf\u7834\u574f\u548c\u9690\u79c1\u6cc4\u6f0f\uff09\uff0c\u5f53\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u98ce\u9669\u7684\u5168\u9762\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86OS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8610%-30%\u3002", "conclusion": "OS-Sentinel\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u4ee3\u7406\u7684\u5b89\u5168\u6027\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLROD\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u6062\u590d\u548c\u68c0\u6d4b\u7f51\u7edc\u7684Lipschitz\u8fde\u7eed\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ea7\u8054\u6846\u67b6\u4e2d\u7684\u529f\u80fd\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660eLR-YOLO\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\uff08\u5982\u96fe\u973e\u548c\u4f4e\u5149\uff09\uff0c\u56fe\u50cf\u6062\u590d\u901a\u5e38\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u7528\u4e8e\u63d0\u9ad8\u68c0\u6d4b\u5668\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u4f46\u6062\u590d\u7f51\u7edc\u548c\u68c0\u6d4b\u7f51\u7edc\u4e4b\u95f4\u7684\u529f\u80fd\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u548c\u96c6\u6210\u6548\u679c\u4e0d\u4f73\u3002\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u8bba\u6587\u901a\u8fc7Lipschitz\u8fde\u7eed\u6027\u7684\u89c6\u89d2\u5206\u6790\u4e86\u6062\u590d\u7f51\u7edc\u548c\u68c0\u6d4b\u7f51\u7edc\u5728\u8f93\u5165\u7a7a\u95f4\u548c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u529f\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86Lipschitz-regularized YOLO (LR-YOLO)\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e0e\u68c0\u6d4b\u5668\u7684\u7279\u5f81\u5b66\u4e60\u76f8\u534f\u8c03\u3002", "result": "\u5728\u96fe\u973e\u548c\u4f4e\u5149\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLR-YOLO\u5728\u68c0\u6d4b\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u5e73\u6ed1\u6027\u548c\u6574\u4f53\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLipschitz-regularized object detection (LROD)\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6062\u590d\u76f4\u63a5\u96c6\u6210\u5230\u68c0\u6d4b\u5668\u7684\u7279\u5f81\u5b66\u4e60\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ea7\u8054\u6846\u67b6\u4e2d\u6062\u590d\u548c\u68c0\u6d4b\u7f51\u7edc\u529f\u80fd\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u5e73\u6ed1\u6027\u548c\u6574\u4f53\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u4eec\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u6f14\u7ece\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5f31\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u4e0d\u4ec5\u6d89\u53ca\u8bed\u8a00\u4efb\u52a1\u7684\u6267\u884c\uff0c\u8fd8\u5173\u7cfb\u5230\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u5f97\u51fa\u903b\u8f91\u6709\u6548\u7684\u7ed3\u8bba\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u516b\u9053\u81ea\u5b9a\u4e49\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGPT\u3001Claude\u3001DeepSeek\u7b49\uff09\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u3002"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408CrossGate\u548cColorShift\u7684DeshadowMamba\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9634\u5f71\u53bb\u9664\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u6a21\u578b\u5728\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4e2d\u56e0\u56fa\u5b9a\u6ce8\u610f\u529b\u6a21\u5f0f\u5bfc\u81f4\u7ed3\u6784\u626d\u66f2\u548c\u989c\u8272\u4e0d\u4e00\u81f4\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faCrossGate\u65b9\u5411\u8c03\u5236\u673a\u5236\u548cColorShift\u6b63\u5219\u5316\uff0c\u7ed3\u5408Mamba\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4f18\u5316\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeshadowMamba\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6027\u80fd\u3002", "conclusion": "DeshadowMamba\u7ed3\u5408CrossGate\u548cColorShift\u6b63\u5219\u5316\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5f3a\u5927\u7684\u5b9a\u91cf\u6027\u80fd\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "LLM\u6846\u67b6Law in Silico\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\uff0c\u5b8f\u89c2\u5339\u914d\u72af\u7f6a\u8d8b\u52bf\uff0c\u5fae\u89c2\u63ed\u793a\u826f\u597d\u6cd5\u5f8b\u4f53\u7cfb\u4fdd\u62a4\u5f31\u52bf\u7fa4\u4f53\u3002", "motivation": "\u73b0\u5b9e\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6216\u4e0d\u53ef\u884c\uff0c\u9700\u901a\u8fc7AI\u6a21\u62df\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\uff0c\u652f\u6301\u6cd5\u5f8b\u7ba1\u7406\u3002", "method": "\u5f15\u5165Law in Silico\u6846\u67b6\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\uff0c\u5305\u62ec\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u884c\u7b49\u5236\u5ea6\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u4ee3\u7406\u80fd\u5b8f\u89c2\u518d\u73b0\u72af\u7f6a\u8d8b\u52bf\uff0c\u5fae\u89c2\u8868\u660e\u826f\u597d\u6cd5\u5f8b\u4f53\u7cfb\u66f4\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u6743\u76ca\u3002", "conclusion": "LLM-based agent frameworks can\u6709\u6548\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\uff0c\u5b8f\u89c2\u4e0a\u518d\u73b0\u72af\u7f6a\u8d8b\u52bf\uff0c\u5fae\u89c2\u4e0a\u63ed\u793a\u826f\u597d\u6cd5\u5f8b\u4f53\u7cfb\u5bf9\u5f31\u52bf\u7fa4\u4f53\u6743\u76ca\u7684\u4fdd\u62a4\u4f5c\u7528\u3002"}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "UtilGen\u662f\u4e00\u79cd\u4efb\u52a1\u6548\u7528\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7b56\u7565\u751f\u6210\u9ad8\u6548\u7528\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5185\u5728\u6570\u636e\u5c5e\u6027\uff08\u5982\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u53ef\u80fd\u4e0d\u8db3\u3002", "method": "UtilGen\u5f15\u5165\u4e86\u6743\u91cd\u5206\u914d\u7f51\u7edc\u8bc4\u4f30\u5408\u6210\u6837\u672c\u7684\u4efb\u52a1\u7279\u5b9a\u6548\u7528\uff0c\u5e76\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff08\u6a21\u578b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u4f18\u5316\uff09\u8fed\u4ee3\u4f18\u5316\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u516b\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u548c\u7c92\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cUtilGen\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e863.87%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u66f4\u5177\u5f71\u54cd\u529b\u548c\u4efb\u52a1\u76f8\u5173\u7684\u5408\u6210\u6570\u636e\u3002", "conclusion": "UtilGen\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4efb\u52a1\u6548\u7528\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u53cd\u9988\u81ea\u9002\u5e94\u4f18\u5316\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4ece\u89c6\u89c9\u7279\u5f81\u4e3a\u4e2d\u5fc3\u5230\u4efb\u52a1\u6548\u7528\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\u8f6c\u53d8\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u91cd\u5408\u6210\u7684\u65e0\u8bad\u7ec3\u5355\u6b21\u6765\u6e90\u5f52\u5c5e\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u5408\u6210\u56fe\u50cf\u5f52\u5c5e\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5408\u6210\u56fe\u50cf\u6765\u6e90\u5f52\u5c5e\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u56fe\u50cf\u7684\u63d0\u793a\uff0c\u5229\u7528\u6240\u6709\u5019\u9009\u6765\u6e90\u91cd\u5408\u6210\u56fe\u50cf\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u627e\u5230\u6700\u63a5\u8fd1\u539f\u56fe\u7684\u91cd\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u5f52\u5c5e\u3002", "result": "\u63d0\u51fa\u7684\u91cd\u5408\u6210\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u65b0\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u65b0\u6570\u636e\u96c6\u5bf9\u672a\u6765\u7684\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u5f00\u53d1\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "ViPER\u901a\u8fc7\u81ea\u4e3e\u6846\u67b6\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u751f\u6210\u4e0e\u7406\u89e3\u7684\u4e92\u60e0\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0a\u7684\u80fd\u529b\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\uff09\u5b58\u5728\u6570\u636e\u4e0d\u8db3\u548c\u504f\u91cd\u6587\u672c\u63a8\u7406\u800c\u5ffd\u89c6\u89c6\u89c9\u611f\u77e5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4efb\u52a1\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\u6784\u5efa\u4e3a\u4ece\u7c97\u5230\u7ec6\u7684\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86ViPER\u81ea\u4e3e\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u8bc4\u548c\u81ea\u9884\u6d4b\u5b9e\u73b0\u8fed\u4ee3\u8fdb\u5316\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u91cd\u5efa\u4e0e\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "ViPER\u5e94\u7528\u4e8eQwen2.5-VL\u7cfb\u5217\u540e\uff0c\u751f\u6210\u7684Qwen-Viper\u7cfb\u5217\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53471.7%\uff0c\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u4e0a\u6700\u9ad8\u63d0\u53476.0%\uff0c\u4e14\u4fdd\u6301\u4e86\u901a\u7528\u6027\u3002", "conclusion": "ViPER\u6846\u67b6\u901a\u8fc7\u81ea\u4e3e\u5f0f\u95ed\u73af\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u6027\uff0c\u4e3a\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u7684\u4e92\u60e0\u5173\u7cfb\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u4e24\u9636\u6bb5\u4f2a\u6807\u6ce8\u65b9\u6cd5\uff0c\u7ed3\u5408\u8de8\u4efb\u52a1\u76d1\u7763\u548c\u56fe\u4f20\u64ad\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u51cf\u5c11\u5bf9\u65b0\u4efb\u52a1\u6216\u5177\u6709\u6311\u6218\u6027\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u793a\u4f8b\u6536\u96c6\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u9996\u5148\u5229\u7528\u8de8\u4efb\u52a1\u793a\u4f8b\u4f2a\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5176\u4f59\u76ee\u6807\u793a\u4f8b\u4e2d\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0\u9700LLM\u7684\u4f20\u64ad\u53ef\u6269\u5c55\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u5728\u4e94\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "Prompt learning is effective for few-shot remote sensing scene classification, outperforming baselines, with self-regulating constraints showing the best cross-domain performance.", "motivation": "Address the critical challenge of performance constraints in remote sensing scene classification due to scarcity of labeled data and high annotation costs across diverse domains.", "method": "Systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification, evaluating methods like Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints.", "result": "Prompt learning consistently outperforms zero-shot CLIP with hand-crafted prompts and linear probe baselines in few-shot scenarios, with Prompting with Self-Regulating Constraints achieving the most robust cross-domain performance.", "conclusion": "Prompt learning, particularly with self-regulating constraints, is a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research."}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0fAI\u533b\u7597\u7cfb\u7edf\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u4f18\u5316\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u652f\u6301\u9ad8\u8d28\u91cf\u533b\u7597\u670d\u52a1\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u53ca\u5176\u53ef\u5b9e\u73b0\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u7684\u8303\u5f0f\u4ee5\u786e\u4fdd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u533b\u7597\u670d\u52a1\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\uff0c\u5e76\u5229\u7528\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u6280\u672f\u4f18\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u63d0\u51fa\u7684\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u80fd\u591f\u4e3a\u751f\u6210\u5f0fAI\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9886\u57df\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u5c42\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u91cd\u65b0\u5b9a\u4f4d\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u652f\u6301\u9ad8\u8d28\u91cf\u533b\u7597\u670d\u52a1\u3002"}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "\u63d0\u51fa\u5207\u6362\u5f0f\u53cc\u5b66\u751f\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u7b56\u7565\uff0c\u63d0\u5347\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u9650\u4e8e\u7f51\u7edc\u95f4\u5f3a\u76f8\u5173\u6027\u548c\u4e0d\u53ef\u9760\u7684\u77e5\u8bc6\u4f20\u9012\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u5207\u6362\u5f0f\u53cc\u5b66\u751f\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\uff0c\u52a8\u6001\u9009\u62e9\u53ef\u9760\u5b66\u751f\u5e76\u4f18\u5316\u6559\u5e08\u7f51\u7edc\u7684\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6709\u9650\u76d1\u7763\u4e0b\u7684\u5206\u5272\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5207\u6362\u5f0f\u53cc\u5b66\u751f\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u5173\u952e\u6280\u672f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9645\u8bc4\u4f30\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\uff0c\u9700\u89e3\u51b3\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "method": "FunReason-MT\u91c7\u7528\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u4e09\u79cd\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u7b49\u6311\u6218\u3002", "result": "\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u8bad\u7ec3\u76844B\u6a21\u578b\u5728Berkeley Function-Calling Leaderboard (BFCLv3)\u4e0a\u8fbe\u5230\u540c\u7c7b\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728BFCLv4\u4e0a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u7a33\u5065\u6027\u3002", "conclusion": "FunReason-MT\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u7684\u590d\u6742\u6027\u969c\u788d\uff0c\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u8bc4\u4f30\u4e2d\u5c55\u73b0\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "W2-Net\u901a\u8fc7\u53cc\u67e5\u8be2\u673a\u5236\u548c\u5b50\u7c7b\u53ef\u5206\u5339\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347Referring Expression Counting\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6807\u6ce8\u65f6\u901a\u5e38\u5173\u6ce8\u7c7b\u4ee3\u8868\u6027\u4f4d\u7f6e\uff08\u5982\u5934\u90e8\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u89c6\u89c9\u533a\u57df\uff08\u5982\u817f\u90e8\uff09\u7684\u5c5e\u6027\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u7cbe\u786e\u533a\u5206\u5b50\u7c7b\u3002", "method": "\u63d0\u51faW2-Net\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u67e5\u8be2\u673a\u5236\uff08what-to-count\u548cwhere-to-see\u67e5\u8be2\uff09\u548c\u5b50\u7c7b\u53ef\u5206\u5339\u914d\u7b56\u7565\uff08SSM\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5c5e\u6027\u4fe1\u606f\u7684\u95ee\u9898\u3002", "result": "\u5728REC-8K\u6570\u636e\u96c6\u4e0a\uff0cW2-Net\u5c06\u8ba1\u6570\u8bef\u5dee\u51cf\u5c11\u4e8622.5%\uff08\u9a8c\u8bc1\u96c6\uff09\u548c18.0%\uff08\u6d4b\u8bd5\u96c6\uff09\uff0c\u5b9a\u4f4dF1\u5206\u6570\u63d0\u9ad8\u4e867%\u548c8%\u3002", "conclusion": "W2-Net\u901a\u8fc7\u53cc\u67e5\u8be2\u673a\u5236\u548c\u5b50\u7c7b\u53ef\u5206\u5339\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86Referring Expression Counting\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u8ba1\u6570\u9519\u8bef\u5e76\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "FMs are revolutionizing SSDM by integrating visual/textual data and enabling adaptive/digital twin frameworks, though challenges like sim-to-real gaps and human-robot collaboration remain.", "motivation": "To explore the rapid advancements in SSDM through machine and deep learning, particularly the role of FMs in integrating visual and textual data for more effective disease management.", "method": "This review screened approximately 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and discussing their role in adaptive learning (AL), reinforcement learning (RL), and digital twin frameworks.", "result": "Key findings include the growing traction of FMs, the dominance of VLMs over LLMs, the nascent stage of RL and AL for smart spraying, and the potential of digital twins with RL for targeted spraying.", "conclusion": "Foundation models (FMs) are transforming site-specific disease management (SSDM) through advanced integration of visual and textual data, adaptive learning, and digital twin frameworks. Addressing the sim-to-real gap and enhancing human-robot collaboration are critical for real-world deployment."}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "StrokeSeg\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u8f7b\u91cf\u7ea7\u7684\u6846\u67b6\uff0c\u80fd\u5c06\u7814\u7a76\u7ea7\u8111\u5352\u4e2d\u5206\u5272\u6a21\u578b\u8f6c\u5316\u4e3a\u4e34\u5e8a\u53ef\u90e8\u7f72\u5de5\u5177\uff0c\u6027\u80fd\u4e0e\u539f\u6d41\u7a0b\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08\u5982nnU-Net\uff09\u5728\u8111\u5352\u4e2d\u75c5\u53d8\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u6c89\u91cd\u7684\u4f9d\u8d56\u548c\u5355\u4e00\u7684\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "StrokeSeg\u91c7\u7528\u4e86\u6a21\u5757\u5316\u548c\u8f7b\u91cf\u7ea7\u7684\u8bbe\u8ba1\uff0c\u89e3\u8026\u4e86\u9884\u5904\u7406\u3001\u63a8\u7406\u548c\u540e\u5904\u7406\u3002\u9884\u5904\u7406\u4f9d\u8d56\u4e8eAnima\u5de5\u5177\u7bb1\u5e76\u751f\u6210BIDS\u517c\u5bb9\u7684\u8f93\u51fa\uff0c\u63a8\u7406\u4f7f\u7528ONNX Runtime\u548cFloat16\u91cf\u5316\uff0c\u51cf\u5c0f\u4e86\u7ea650%\u7684\u6a21\u578b\u4f53\u79ef\u3002", "result": "\u5728300\u540d\u4e9a\u6025\u6027\u548c\u6162\u6027\u5352\u4e2d\u53d7\u8bd5\u8005\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0cStrokeSeg\u7684\u5206\u5272\u6027\u80fd\u4e0e\u539fPyTorch\u6d41\u7a0b\u76f8\u5f53\uff08Dice\u5dee\u5f02<10^-3\uff09\u3002", "conclusion": "StrokeSeg\u6210\u529f\u5730\u5c06\u9ad8\u6027\u80fd\u7684\u7814\u7a76\u7ea7\u8111\u5352\u4e2d\u75c5\u53d8\u5206\u5272\u6a21\u578b\u8f6c\u5316\u4e3a\u4fbf\u643a\u3001\u4e34\u5e8a\u53ef\u7528\u7684\u5de5\u5177\uff0c\u4e14\u4fdd\u6301\u4e86\u4e0e\u539fPyTorch\u6d41\u7a0b\u76f8\u5f53\u7684Dice\u5206\u6570\u5dee\u5f02\uff08<10^-3\uff09\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u6a21\u62df\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u590d\u6742\u6027\u7684DAG\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5ffd\u7565\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u548c\u8bc4\u4f30\u8fd9\u79cd\u590d\u6742\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86OrchDAG\uff0c\u4e00\u4e2a\u6a21\u62df\u5de5\u5177\u6267\u884c\u7684\u6709\u5411\u65e0\u73af\u56fe\uff08DAGs\uff09\u5e76\u63a7\u5236\u590d\u6742\u5ea6\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u7528\u4e8e\u6a21\u578b\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOrchDAG\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u5956\u52b1\u65b9\u6cd5\u5728\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u6027\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u56fe\u57fa\u5956\u52b1\u5728\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "MLSN\u7f51\u7edc\u901a\u8fc7\u591a\u5c3a\u5ea6\u4eae\u5ea6\u611f\u77e5\u548c\u7279\u5f81\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u504f\u632f\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u5bf9\u6bd4\u5ea6\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u504f\u632f\u56fe\u50cf\u878d\u5408\u5728\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u5b58\u5728\u56fa\u6709\u5bf9\u6bd4\u5ea6\u5dee\u5f02\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u6574\u5408\u4e92\u8865\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4eae\u5ea6\u611f\u77e5\u591a\u5c3a\u5ea6\u7f51\u7edc\uff08MLSN\uff09\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\u3001\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u673a\u5236\u548c\u4eae\u5ea6\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u52a8\u6001\u6ce8\u5165\u4eae\u5ea6\u4fe1\u606f\u5e76\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u3002", "result": "MLSN\u5728MSP\u3001PIF\u548cGAND\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cMS-SSIM\u548cSD\u6307\u6807\u5e73\u5747\u63d0\u9ad88.57%\u81f363.53%\u3002", "conclusion": "MLSN\u7f51\u7edc\u901a\u8fc7\u4eae\u5ea6\u611f\u77e5\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u504f\u632f\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\uff0c\u5728\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7MSP\u6570\u636e\u96c6\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u504f\u632f\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5de5\u5177\u548c\u9886\u57df\u77e5\u8bc6\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u63d0\u5347\u793a\u4f8b\u5de5\u4ef6\u751f\u6210\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6316\u6398\u5de5\u5177\u4e0e\u6587\u6863\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u793a\u4f8b\u5de5\u4ef6\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u9996\u5148\u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\uff0c\u540c\u65f6\u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u4e2d\u63d0\u53d6\u8865\u5145\u77e5\u8bc6\u56fe\uff0c\u7136\u540e\u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u751f\u6210\u793a\u4f8b\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u6539\u8fdb\u8ba1\u5212\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5de5\u5177\u56fe\u548c\u9886\u57df\u77e5\u8bc6\u56fe\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u653e\u5c04\u5b66\u62a5\u544a\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u4efb\u52a1\u4e0e\u6587\u672c\u7684\u5173\u8054\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u5ffd\u89c6\u6807\u7b7e\u4e0e\u6587\u672c\u5f31\u5173\u8054\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u653e\u5c04\u5b66\u62a5\u544a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6700\u4f73\u4f7f\u7528\u65f6\u673a\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u653e\u5c04\u5b66\u62a5\u544a\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u8bca\u65ad\u548c\u9884\u540e\u4efb\u52a1\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bad\u7ec3\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "result": "1. \u5728\u6807\u7b7e\u4e0e\u6587\u672c\u9ad8\u5ea6\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\uff0c\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u62a5\u544a\u6709\u76ca\uff1b\u4f46\u5728\u4e0d\u76f8\u5173\u65f6\uff0c\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u53ef\u80fd\u6709\u5bb3\u30022. \u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u62a5\u544a\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u6765\u8bad\u7ec3\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "REFLECT\u6a21\u578b\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u68c0\u6d4b\u5352\u4e2d\u540e\u7ed3\u6784\u5f02\u5e38\uff0c\u4f7f\u7528\u5065\u5eb7\u6570\u636e\u8bad\u7ec3\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u5352\u4e2d\u540eMRI\u4e0d\u4ec5\u663e\u793a\u5c40\u7076\u6027\u75c5\u53d8\uff0c\u8fd8\u63ed\u793a\u7ee7\u53d1\u6027\u7ed3\u6784\u53d8\u5316\uff0c\u5982\u840e\u7f29\u548c\u5fc3\u5ba4\u6269\u5927\u3002\u8fd9\u4e9b\u5f02\u5e38\u4f5c\u4e3a\u6062\u590d\u548c\u9884\u540e\u7684\u5f71\u50cf\u751f\u7269\u6807\u5fd7\u7269\u8d8a\u6765\u8d8a\u53d7\u91cd\u89c6\uff0c\u4f46\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u3002", "method": "\u8bc4\u4f30REFLECT\uff08\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff09\u7528\u4e8e\u65e0\u76d1\u7763\u68c0\u6d4b\u5352\u4e2d\u540e\u60a3\u8005\u7684\u5c40\u7076\u6027\u548c\u975e\u5c40\u7076\u6027\u5f02\u5e38\u3002\u4f7f\u7528ATLAS\u6570\u636e\u7684\u53cc\u4e13\u5bb6\u4e2d\u5fc3\u5207\u7247\u6ce8\u91ca\uff0c\u901a\u8fc7\u81ea\u7531\u54cd\u5e94ROC\u5206\u6790\u5728\u5bf9\u8c61\u7ea7\u522b\u8bc4\u4f30\u5f02\u5e38\u56fe\u7684\u6027\u80fd\u3002", "result": "\u5728ATLAS\u6d4b\u8bd5\u5bf9\u8c61\u4e0a\uff0c\u4f7f\u7528IXI\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u75c5\u53d8\u5206\u5272\uff08Dice = 0.37 vs 0.27\uff09\u548c\u5bf9\u975e\u5c40\u7076\u6027\u5f02\u5e38\u7684\u654f\u611f\u6027\uff08FROC = 0.62 vs 0.43\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u4f7f\u7528\u5b8c\u5168\u5065\u5eb7\u7684\u89e3\u5256\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u66f4\u597d\u5730\u5efa\u6a21\u6b63\u5e38\u53d8\u5f02\uff0c\u4ece\u800c\u66f4\u5e7f\u6cdb\u3001\u66f4\u53ef\u9760\u5730\u68c0\u6d4b\u7ed3\u6784\u5f02\u5e38\u3002"}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "\u65e0\u4f20\u611f\u5668\u65b9\u6cd5\u901a\u8fc7\u536b\u661f\u5f71\u50cf\u548c\u673a\u5668\u5b66\u4e60\u9ad8\u7cbe\u5ea6\u4f30\u7b97\u6c34\u5e93\u5bb9\u91cf\uff0c\u8bef\u5dee\u4f4e\u4e8e1.5%\uff0c\u9002\u7528\u4e8e\u5168\u7403\u6c34\u4f53\u76d1\u6d4b\u3002", "motivation": "\u7531\u4e8e\u4f20\u611f\u5668\u6545\u969c\u548c\u7ef4\u62a4\u80fd\u529b\u6709\u9650\uff0cQaraaoun\u6c34\u5e93\u7684\u53ef\u6301\u7eed\u7ba1\u7406\u9700\u8981\u4e00\u79cd\u53ef\u9760\u4e14\u4e0d\u4f9d\u8d56\u5730\u9762\u6d4b\u91cf\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e86Sentinel-2\u548cLandsat\u536b\u661f\u5f71\u50cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u57df\u5206\u5272\u6307\u6570\uff0c\u5e76\u57fa\u4e8e\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f9d\u8d56\u536b\u661f\u5f71\u50cf\u63d0\u53d6\u7684\u6c34\u57df\u9762\u79ef\u4f30\u7b97\u6c34\u5e93\u5bb9\u91cf\u3002", "result": "\u63d0\u51fa\u7684\u6c34\u57df\u5206\u5272\u6307\u6570\u4e0e\u5730\u9762\u771f\u5b9e\u6570\u636e\u543b\u5408\u5ea6\u8d85\u8fc795%\uff0c\u4f18\u5316\u540e\u7684SVR\u6a21\u578b\u8bef\u5dee\u4f4e\u4e8e\u6c34\u5e93\u603b\u5bb9\u91cf\u76841.5%\uff0c\u786e\u5b9a\u7cfb\u6570\u8d85\u8fc70.98\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4f20\u611f\u5668\u7684\u6c34\u5e93\u5bb9\u91cf\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f00\u6e90\u536b\u661f\u5f71\u50cf\u3001\u5148\u8fdb\u7684\u6c34\u57df\u5206\u5272\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u6c34\u5e93\u5bb9\u91cf\u4f30\u7b97\uff0c\u4e3a\u53ef\u6301\u7eed\u6c34\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u8bed\u4e49\u5206\u5272\u4e2d\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u7b56\u7565\u548c\u5b9a\u5236\u6307\u6807\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5728\u5b89\u5168\u5173\u952e\u548c\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u5c3d\u7ba1\u5728\u5206\u7c7b\u4efb\u52a1\u7684XAI\u8bc4\u4f30\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u7684\u8bc4\u4f30\u7b56\u7565\u4ecd\u76f8\u5bf9\u4e0d\u8db3\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u8bed\u4e49\u5206\u5272\u4e2dXAI\u7684\u7efc\u5408\u7cfb\u7edf\u6846\u67b6\uff0c\u91c7\u7528\u50cf\u7d20\u7ea7\u8bc4\u4f30\u7b56\u7565\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u6807\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eCAM\u7684XAI\u65b9\u6848\u5728\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u7684\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u900f\u660e\u3001\u53ef\u4fe1\u4e14\u53ef\u95ee\u8d23\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "DCIC-sgp\u901a\u8fc7\u529f\u80fd\u5206\u89e3\u548c\u6df1\u5ea6\u6761\u4ef6\u8c03\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LIC\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u51e0\u4f55\u53d8\u5f62\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLIC\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u81ea\u7136\u56fe\u50cf\u4e2d\u590d\u6742\u7684\u76f8\u5173\u6027\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5168\u5c40\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5c40\u90e8\u77ac\u6001\u7eb9\u7406\u7684\u7ea0\u7f20\uff0c\u5bfc\u81f4\u4f4e\u6bd4\u7279\u7387\u4e0b\u4e25\u91cd\u7684\u51e0\u4f55\u53d8\u5f62\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u5206\u89e3\u7684\u6846\u67b6DCIC-sgp\uff0c\u9996\u5148\u751f\u6210\u4e00\u4e2a\u81ea\u751f\u6210\u7684\u5148\u9a8c\u6765\u6355\u6349\u56fe\u50cf\u7684\u7ed3\u6784\u4e3b\u5e72\uff0c\u5e76\u4ee5\u6b64\u5168\u5c40\u8c03\u5236\u6574\u4e2a\u538b\u7f29\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5206\u6790\u53d8\u6362\uff0c\u4ee5\u4e13\u6ce8\u4e8e\u5269\u4f59\u7684\u9ad8\u71b5\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDCIC-sgp\u663e\u8457\u51cf\u5c11\u4e86\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u51e0\u4f55\u53d8\u5f62\u4f2a\u5f71\uff0c\u5e76\u5728Kodak\u3001CLIC\u548cTecnick\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5bf9VTM-12.1\u7684\u663e\u8457BD-rate\u964d\u4f4e\uff0814.4%\u300115.7%\u300115.1%\uff09\u3002", "conclusion": "DCIC-sgp\u6846\u67b6\u901a\u8fc7\u529f\u80fd\u5206\u89e3\u548c\u6df1\u5ea6\u6761\u4ef6\u8c03\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLIC\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u51e0\u4f55\u53d8\u5f62\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u538b\u7f29\u7684\u7387\u5931\u771f\u6027\u80fd\u3002"}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u901a\u8fc7\u65f6\u7a7a\u6570\u636e\u9884\u8bad\u7ec3\u8868\u73b0\u51fa\u6bd4\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "motivation": "\u63a2\u7d22\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u662f\u5426\u80fd\u591f\u901a\u8fc7\u65f6\u7a7a\u6570\u636e\u9884\u8bad\u7ec3\u83b7\u5f97\u7ed3\u6784\u548c\u52a8\u6001\u7684\u5f3a\u5f52\u7eb3\u504f\u5dee\uff0c\u4ece\u800c\u5f25\u8865\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u5728\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u652f\u6301\u4e0b\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u5728ARC-AGI\u3001ConceptARC\u3001\u89c6\u89c9\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u548c\u5143\u80de\u81ea\u52a8\u673a\u7b49\u57fa\u51c6\u4e0a\u7684\u6570\u636e\u6548\u7387\u3002", "result": "VDMs\u5728\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u89c6\u9891\u9884\u8bad\u7ec3\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u89c6\u9891\u9884\u8bad\u7ec3\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u652f\u6301\u5176\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u9002\u5e94\u3002"}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u5206\u7c7b\u4e09\u79cd\u8336\u53f6\u75c5\u5bb3\u5e76\u5b9a\u4f4d\u53d7\u635f\u533a\u57df\uff0c\u5bf9\u6bd4\u4e24\u79cd\u6a21\u578b\u540e\u53d1\u73b0 Faster R-CNN \u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u5206\u7c7b\u4e09\u79cd\u8336\u53f6\u75c5\u5bb3\uff08\u7ea2\u9508\u75c5\u3001\u8336\u877d\u548c\u7ea2\u8718\u86db\u87a8\uff09\u5e76\u663e\u793a\u53f6\u7247\u53d7\u635f\u533a\u57df\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002", "method": "\u8bc4\u4f30\u4e86 SSD MobileNet V2 \u548c Faster R-CNN ResNet50 V1 \u4e24\u79cd\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u91c7\u7528\u81ea\u5b9a\u4e49\u65b9\u6cd5\u901a\u8fc7 Mask R-CNN \u8ba1\u7b97\u53f6\u7247\u53d7\u635f\u533a\u57df\u3002", "result": "Faster R-CNN ResNet50 V1 \u7684 mAP \u4e3a 25%\uff0c\u4f18\u4e8e SSD MobileNet V2 \u7684 20.9%\u3002", "conclusion": "Faster R-CNN ResNet50 V1 \u5728\u68c0\u6d4b\u8336\u53f6\u75c5\u5bb3\u65b9\u9762\u8868\u73b0\u4f18\u4e8e SSD MobileNet V2\uff0c\u4e14\u901a\u8fc7 Mask R-CNN \u5b9e\u73b0\u4e86\u75c5\u5bb3\u533a\u57df\u7684\u7cbe\u786e\u5206\u5272\u3002"}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo\u662f\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u7684\u591a\u89c6\u89d2\u8fd0\u52a8\u6355\u6349\u6d41\u7a0b\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u975e\u4e13\u5bb6\u548c\u91ce\u5916\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u6821\u51c6\u7684\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u91cd\u5efa\u7cbe\u5ea6\u4f4e\uff0c\u9650\u5236\u4e86\u975e\u4e13\u5bb6\u548c\u91ce\u5916\u6355\u6349\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "Kineo\u5229\u7528\u73b0\u6210\u76842D\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u5168\u5c40\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u65f6\u7a7a\u5173\u952e\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u540c\u65f6\u6821\u51c6\u76f8\u673a\u5e76\u91cd\u5efa3D\u5173\u952e\u70b9\u548c\u5bc6\u96c6\u573a\u666f\u70b9\u56fe\u3002", "result": "Kineo\u5728EgoHumans\u548cHuman3.6M\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u673a\u5e73\u79fb\u8bef\u5dee\u51cf\u5c1183-85%\uff0c\u89d2\u5ea6\u8bef\u5dee\u51cf\u5c1186-92%\uff0cW-MPJPE\u51cf\u5c1183-91%\u3002", "conclusion": "Kineo\u662f\u4e00\u79cd\u5168\u81ea\u52a8\u3001\u65e0\u9700\u6821\u51c6\u7684\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "Decoupled MeanFlow \u901a\u8fc7\u7b80\u5355\u89e3\u7801\u7b56\u7565\u5c06\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\uff0c\u663e\u8457\u52a0\u901f\u91c7\u6837\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u566a\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\uff09\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\uff0c\u800c\u6d41\u6620\u5c04\u867d\u80fd\u52a0\u901f\u91c7\u6837\uff0c\u4f46\u901a\u5e38\u9700\u8981\u67b6\u6784\u4fee\u6539\uff0c\u9650\u5236\u4e86\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u517c\u5bb9\u6027\u3002", "method": "\u901a\u8fc7\u8c03\u8282\u6269\u6563\u53d8\u6362\u5668\u7684\u6700\u7ec8\u5757\u4ee5\u6761\u4ef6\u5316\u540e\u7eed\u65f6\u95f4\u6b65\uff0c\u5c06\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u6539\u8fdb\u7684\u8bad\u7ec3\u6280\u672f\u3002", "result": "\u5728 ImageNet 256x256 \u548c 512x512 \u4e0a\uff0c\u6a21\u578b\u5728 1 \u6b65\u548c 4 \u6b65\u91c7\u6837\u65f6\u5206\u522b\u8fbe\u5230 FID 2.16/2.12 \u548c 1.51/1.68\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5b9e\u73b0 100 \u500d\u4ee5\u4e0a\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "Decoupled MeanFlow \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6d41\u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u901f\u5ea6\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Latent Sketchpad\u4e3aMLLMs\u5f15\u5165\u89c6\u89c9\u8349\u7a3f\u672c\uff0c\u589e\u5f3a\u89c6\u89c9\u601d\u7ef4\uff0c\u6027\u80fd\u4f18\u4e8e\u9aa8\u5e72\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u524d\u6cbfMLLMs\u3002", "motivation": "\u53d7\u4eba\u7c7b\u901a\u8fc7\u8349\u56fe\u4f5c\u4e3a\u89c6\u89c9\u601d\u7ef4\u5de5\u5177\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3MLLMs\u5728\u590d\u6742\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u529b\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86Context-Aware Vision Head\u548c\u9884\u8bad\u7ec3\u7684Sketch Decoder\uff0c\u4f7fMLLMs\u80fd\u591f\u5728\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u89c6\u89c9\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u8349\u56fe\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cLatent Sketchpad\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u4e0e\u9aa8\u5e72\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u80fd\u6cdb\u5316\u81f3\u4e0d\u540c\u524d\u6cbfMLLMs\uff08\u5982Gemma3\u548cQwen2.5-VL\uff09\u3002", "conclusion": "Latent Sketchpad\u6846\u67b6\u901a\u8fc7\u4e3aMLLMs\u5f15\u5165\u5185\u90e8\u89c6\u89c9\u8349\u7a3f\u672c\uff0c\u6210\u529f\u6269\u5c55\u4e86\u5176\u6587\u672c\u63a8\u7406\u80fd\u529b\u81f3\u89c6\u89c9\u601d\u7ef4\u9886\u57df\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u5e7f\u6cdb\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "OSWorld-MCP \u662f\u9996\u4e2a\u5168\u9762\u4e14\u516c\u5e73\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u3001GUI \u64cd\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u91cd\u8981\u6027\u5e76\u8bbe\u5b9a\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8 GUI \u4ea4\u4e92\u6280\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff08\u5982\u901a\u8fc7 MCP \u542f\u7528\u7684\u80fd\u529b\uff09\uff0c\u5bfc\u81f4\u5bf9\u96c6\u6210\u5de5\u5177\u8c03\u7528\u7684\u4ee3\u7406\u4e0e\u4ec5\u8bc4\u4f30 GUI \u4ea4\u4e92\u7684\u4ee3\u7406\u4e4b\u95f4\u7684\u4e0d\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u6d41\u6c34\u7ebf\u6765\u521b\u5efa\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u5de5\u5177\u7684\u7cbe\u9009\u96c6\u5408\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u624b\u52a8\u9a8c\u8bc1\u751f\u6210\u4e86 158 \u4e2a\u9ad8\u8d28\u91cf\u5de5\u5177\u3002", "result": "\u5728\u591a\u6a21\u6001\u4ee3\u7406\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMCP \u5de5\u5177\u901a\u5e38\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff08\u4f8b\u5982\uff0cOpenAI o3 \u5728 15 \u6b65\u65f6\u4ece 8.3% \u63d0\u9ad8\u5230 20.4%\uff0cClaude 4 Sonnet \u5728 50 \u6b65\u65f6\u4ece 40.1% \u63d0\u9ad8\u5230 43.3%\uff09\uff0c\u4f46\u6700\u5f3a\u7684\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u7387\u4ecd\u7136\u8f83\u4f4e\uff08\u4ec5 36.3%\uff09\u3002", "conclusion": "OSWorld-MCP \u901a\u8fc7\u660e\u786e\u6d4b\u91cf MCP \u5de5\u5177\u4f7f\u7528\u6280\u80fd\uff0c\u52a0\u6df1\u4e86\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8bc4\u4f30\u590d\u6742\u5de5\u5177\u8f85\u52a9\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u6df1\u5ea6\u5b66\u4e60\u7684CBCT\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u5f53\u524d\u6280\u672f\u3002", "motivation": "CBCT\u5728\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u6613\u53d7\u6563\u5c04\u5f71\u54cd\uff0c\u5bfc\u81f4CT\u503c\u504f\u5dee\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u4ece\u800c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u5efa\u6a21\u70b9\u6563\u5c04\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u5d4c\u5165Kolmogorov-Arnold Networks\uff08KAN\uff09\u5c42\uff0c\u4ee5\u5b66\u4e60\u9ad8\u7ef4\u6563\u5c04\u7279\u5f81\u7684\u975e\u7ebf\u6027\u6620\u5c04\u3002", "result": "\u5408\u6210\u548c\u5b9e\u9645\u626b\u63cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u9ad8\u7ef4\u975e\u7ebf\u6027\u6620\u5c04\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86CBCT\u56fe\u50cf\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cc\u5206\u652fCNN\u7528\u4e8e\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u7387\u57df\u7279\u5f81\uff0c\u901a\u8fc7FSC\u635f\u5931\u4f18\u5316\u6027\u80fd\uff0c\u5728DiFF\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u751f\u6210AI\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4f2a\u9020\u9762\u90e8\u56fe\u50cf\u9ad8\u5ea6\u903c\u771f\uff0c\u5a01\u80c1AI\u5b89\u5168\u3001\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u3002\u8feb\u5207\u9700\u8981\u5f3a\u5927\u4e14\u901a\u7528\u7684\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u522b\u4ece\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u63d0\u53d6\u4e92\u8865\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u8fd9\u4e9b\u5f02\u8d28\u7279\u5f81\u3002\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684FSC\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u7126\u70b9\u635f\u5931\u3001\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u548c\u9891\u7387\u4e2d\u5fc3\u8fb9\u9645\u635f\u5931\u3002", "result": "\u5728DiFF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u6240\u6709\u4f2a\u9020\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4fdd\u62a4AI\u751f\u6001\u7cfb\u7edf\u514d\u53d7\u89c6\u89c9\u4f2a\u9020\u653b\u51fb\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "PathoGaze1.0\u6570\u636e\u96c6\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u7b49\u6280\u672f\u8bb0\u5f55\u4e86\u75c5\u7406\u5b66\u5bb6\u8bca\u65adWSIs\u7684\u884c\u4e3a\u6570\u636e\uff0c\u63ed\u793a\u4e86\u8bca\u65ad\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u53ef\u80fd\u7528\u4e8e\u6539\u8fdb\u4eba\u7c7b\u548cAI\u7684\u8bca\u65ad\u8bad\u7ec3\u3002", "motivation": "\u75c5\u7406\u5b66\u5bb6\u5728\u89e3\u8bfbWSIs\u65f6\u7684\u8bca\u65ad\u51c6\u786e\u7387\u4ec5\u7ea670%\uff0c\u4e14\u589e\u52a0\u7b2c\u4e8c\u4f4d\u75c5\u7406\u5b66\u5bb6\u5e76\u672a\u663e\u8457\u63d0\u9ad8\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u89e3\u91ca\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\u6570\u636e\u3002", "method": "\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u3001\u523a\u6fc0\u8ffd\u8e2a\u3001\u89c6\u53e3\u5bfc\u822a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\u6536\u96c6\u4e8619\u4f4d\u75c5\u7406\u5b66\u5bb6\u89e3\u8bfb397\u5f20\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u7684\u884c\u4e3a\u6570\u636e\uff0c\u5f3a\u8c03\u751f\u6001\u6709\u6548\u6027\u3002", "result": "\u8bb0\u5f55\u4e86171,909\u6b21\u6ce8\u89c6\u3001263,320\u6b21\u626b\u89c6\u548c1,867,362\u6b21\u9f20\u6807\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6539\u8fdb\u75c5\u7406\u5b66\u5bb6\u548cAI\u7cfb\u7edf\u7684\u8bad\u7ec3\u3002", "conclusion": "PathoGaze1.0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u75c5\u7406\u5b66\u5bb6\u5728\u764c\u75c7\u8bca\u65ad\u4e2d\u7684\u89c6\u89c9\u641c\u7d22\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u65e8\u5728\u586b\u8865\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u884c\u4e3a\u6570\u636e\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "\u63d0\u51faGRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743DiT\u6a21\u578b\u4e2d\u7684token delta\u503c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u7f16\u8f91\u5f3a\u5ea6\u7684\u8fde\u7eed\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDiffusion-in-Transformer\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7f16\u8f91\u7a0b\u5ea6\u7684\u6709\u6548\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5b9a\u5236\u5316\u7ed3\u679c\u7684\u5b9e\u73b0\u3002", "method": "\u7814\u7a76\u4e86DiT\u6a21\u578b\u4e2d\u7684MM-Attention\u673a\u5236\uff0c\u53d1\u73b0Query\u548cKey token\u5171\u4eab\u4e00\u4e2a\u4ec5\u4f9d\u8d56\u4e8e\u5c42\u7684\u504f\u7f6e\u5411\u91cf\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86GRAG\u65b9\u6cd5\uff0c\u91cd\u65b0\u52a0\u6743\u4e0d\u540ctoken\u7684delta\u503c\u4ee5\u8c03\u5236\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u548c\u7f16\u8f91\u6307\u4ee4\u7684\u5173\u6ce8\u3002", "result": "GRAG\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c11\u56db\u884c\u4ee3\u7801\u96c6\u6210\u5230\u73b0\u6709\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\uff0c\u5e76\u5728\u63a7\u5236\u7f16\u8f91\u7a0b\u5ea6\u4e0a\u8868\u73b0\u4f18\u4e8eClassifier-Free Guidance\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGroup Relative Attention Guidance\uff08GRAG\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728DiT\u6a21\u578b\u4e2d\u91cd\u65b0\u52a0\u6743\u4e0d\u540ctoken\u7684delta\u503c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u7684\u8fde\u7eed\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u65e0\u9700\u989d\u5916\u8c03\u53c2\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cGRAG\u80fd\u663e\u8457\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\uff0c\u5e76\u5728\u63a7\u5236\u7f16\u8f91\u7a0b\u5ea6\u4e0a\u6bd4\u5e38\u7528\u7684Classifier-Free Guidance\u66f4\u5e73\u6ed1\u548c\u7cbe\u786e\u3002"}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u9891\u8fc7\u6e21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5f15\u5bfc\u548c\u751f\u6210\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u8fc7\u6e21\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u8fc7\u6e21\u65b9\u6cd5\u5728\u5904\u7406\u5927\u65f6\u95f4\u8de8\u5ea6\u6216\u8bed\u4e49\u5dee\u5f02\u5927\u7684\u526a\u8f91\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u5185\u5bb9\u611f\u77e5\u4e14\u89c6\u89c9\u8fde\u8d2f\u7684\u8fc7\u6e21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSAGE\u65b9\u6cd5\uff0c\u5229\u7528\u7ebf\u6761\u56fe\u548c\u8fd0\u52a8\u6d41\u63d0\u4f9b\u7ed3\u6784\u5f15\u5bfc\uff0c\u7ed3\u5408\u751f\u6210\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u89c6\u9891\u8fc7\u6e21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982FILM\u3001TVG\u3001DiffMorpher\u3001VACE\u3001GI\uff09\u3002", "conclusion": "SAGE\uff08Structure-Aware Generative vidEo transitions\uff09\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5e73\u6ed1\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u89c6\u9891\u8fc7\u6e21\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "MIC-BEV\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684BEV\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u591a\u76f8\u673a3D\u7269\u4f53\u68c0\u6d4b\uff0c\u652f\u6301\u5f02\u6784\u76f8\u673a\u914d\u7f6e\u5e76\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u57fa\u7840\u8bbe\u65bd\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5305\u62ec\u591a\u89c6\u89d2\u8bbe\u7f6e\u3001\u591a\u6837\u5316\u76f8\u673a\u914d\u7f6e\u3001\u89c6\u89c9\u8f93\u5165\u9000\u5316\u53ca\u4e0d\u540c\u9053\u8def\u5e03\u5c40\u7b49\u6311\u6218\u3002", "method": "MIC-BEV\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u6846\u67b6\uff0c\u652f\u6301\u53ef\u53d8\u6570\u91cf\u7684\u76f8\u673a\u548c\u5f02\u6784\u5185\u5916\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u56fe\u589e\u5f3a\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u673a\u4e0eBEV\u5355\u5143\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u53ca\u6f5c\u5728\u89c6\u89c9\u7ebf\u7d22\uff0c\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u7279\u5f81\u6574\u5408\u5230BEV\u7a7a\u95f4\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6M2I\u548c\u771f\u5b9e\u6570\u636e\u96c6RoScenes\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMIC-BEV\u57283D\u7269\u4f53\u68c0\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MIC-BEV\u57283D\u7269\u4f53\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u6781\u7aef\u5929\u6c14\u548c\u4f20\u611f\u5668\u9000\u5316\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "ViTs naturally develop object-binding through self-supervised pretraining, with over 90% accuracy in decoding 'IsSameObject'. This ability is weaker in supervised models and actively guides attention.", "motivation": "To investigate whether object-binding naturally emerges in pre-trained Vision Transformers (ViTs) and its role in downstream tasks.", "method": "Decoding 'IsSameObject' from patch embeddings using a similarity probe across ViT layers.", "result": "Self-supervised ViTs (DINO, MAE, CLIP) reliably exhibit object-binding with over 90% accuracy, while ImageNet-supervised models show weaker performance. 'IsSameObject' is encoded in a low-dimensional subspace and guides attention.", "conclusion": "ViTs naturally develop object-binding capabilities through self-supervised pretraining, challenging the notion that they lack this ability. This emergent property is not an architectural artifact but serves the pretraining objective."}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "ProMoE\u901a\u8fc7\u663e\u5f0f\u8def\u7531\u6307\u5bfc\u548c\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u6539\u8fdb\u89c6\u89c9MoE\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u4ee4\u724c\u7684\u7a7a\u95f4\u5197\u4f59\u548c\u529f\u80fd\u5f02\u8d28\u6027\u963b\u788d\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u9700\u8981\u901a\u8fc7\u663e\u5f0f\u8def\u7531\u6307\u5bfc\u6765\u5f25\u8865\u3002", "method": "\u63d0\u51fa\u4e86ProMoE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u6b65\u8def\u7531\u5668\u548c\u663e\u5f0f\u8def\u7531\u6307\u5bfc\uff0c\u901a\u8fc7\u6761\u4ef6\u8def\u7531\u548c\u539f\u578b\u8def\u7531\u5b9e\u73b0\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "result": "\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProMoE\u5728Rectified Flow\u548cDDPM\u8bad\u7ec3\u76ee\u6807\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProMoE\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8def\u7531\u6307\u5bfc\u548c\u8def\u7531\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9MoE\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "URSA \u662f\u4e00\u79cd\u521b\u65b0\u7684\u79bb\u6563\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u548c\u5f02\u6b65\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u548c\u6548\u679c\uff0c\u6027\u80fd\u5ab2\u7f8e\u8fde\u7eed\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79bb\u6563\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u56e0\u8bef\u5dee\u7d2f\u79ef\u548c\u957f\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0cURSA \u65e8\u5728\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "URSA \u5c06\u89c6\u9891\u751f\u6210\u4efb\u52a1\u89c6\u4e3a\u79bb\u6563\u65f6\u7a7a\u6807\u8bb0\u7684\u8fed\u4ee3\u5168\u5c40\u4f18\u5316\uff0c\u7ed3\u5408\u7ebf\u6027\u5316\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u76f8\u5173\u65f6\u95f4\u6b65\u504f\u79fb\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u548c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cURSA \u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\uff0c\u8fd8\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u8fde\u7eed\u6269\u6563\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u5c24\u5176\u5728\u9ad8\u6548\u63a8\u7406\u548c\u591a\u4efb\u52a1\u7edf\u4e00\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "URSA \u901a\u8fc7\u5176\u72ec\u7279\u7684\u8bbe\u8ba1\uff08\u7ebf\u6027\u5316\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u76f8\u5173\u65f6\u95f4\u6b65\u504f\u79fb\u673a\u5236\uff09\u4ee5\u53ca\u5f02\u6b65\u65f6\u95f4\u5fae\u8c03\u7b56\u7565\uff0c\u6210\u529f\u7f29\u5c0f\u4e86\u79bb\u6563\u65b9\u6cd5\u4e0e\u8fde\u7eed\u65b9\u6cd5\u5728\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "GVS\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548cOmni Guidance\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u672a\u6765\u6761\u4ef6\u7f3a\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u65e0\u78b0\u649e\u7684\u76f8\u673a\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u5229\u7528\u672a\u6765\u6761\u4ef6\u5f15\u5bfc\u5f53\u524d\u751f\u6210\u7684\u95ee\u9898\uff0c\u907f\u514d\u573a\u666f\u78b0\u649e\u548c\u751f\u6210\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86Generative View Stitching (GVS)\u91c7\u6837\u7b97\u6cd5\uff0c\u7ed3\u5408Omni Guidance\u6280\u672f\uff0c\u5229\u7528\u73b0\u6709\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u5e76\u884c\u91c7\u6837\uff0c\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "GVS\u5728\u591a\u79cd\u9884\u5b9a\u4e49\u76f8\u673a\u8def\u5f84\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u65e0\u78b0\u649e\u4e14\u5e27\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\uff0c\u652f\u6301\u957f\u7a0b\u4e00\u81f4\u6027\u3002", "conclusion": "GVS\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548cOmni Guidance\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u65e0\u78b0\u649e\u4e14\u5e27\u95f4\u4e00\u81f4\u7684\u76f8\u673a\u5f15\u5bfc\u89c6\u9891\u751f\u6210\uff0c\u80fd\u591f\u5904\u7406\u5305\u62ec\u2018\u4e0d\u53ef\u80fd\u9636\u68af\u2019\u5728\u5185\u7684\u591a\u79cd\u9884\u5b9a\u4e49\u76f8\u673a\u8def\u5f84\u3002"}}
