<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [iFlyBot-VLA Technical Report](https://arxiv.org/abs/2511.01914)
*Yuan Zhang,Chenyu Xue,Wenjie Xu,Chao Ji,Jiajia wu,Jia Pan*

Main category: cs.CV

TL;DR: iFlyBot-VLA是一种新型视觉-语言-动作模型，通过潜在动作模型和双级动作表示框架，结合混合训练策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升视觉-语言-动作模型在复杂操作任务中的表现，并增强其3D感知和推理能力。

Method: 提出了一种潜在动作模型和双级动作表示框架，结合机器人轨迹数据与通用QA和空间QA数据集进行混合训练。

Result: 在LIBERO Franka基准测试和实际任务中取得了竞争性的成功率。

Conclusion: iFlyBot-VLA通过双级动作表示框架和混合训练策略，显著提升了视觉-语言-动作模型的性能，并在LIBERO Franka基准测试和实际任务中表现优异。

Abstract: We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model
trained under a novel framework. The main contributions are listed as follows:
(1) a latent action model thoroughly trained on large-scale human and robotic
manipulation videos; (2) a dual-level action representation framework that
jointly supervises both the Vision-Language Model (VLM) and the action expert
during training; (3) a mixed training strategy that combines robot trajectory
data with general QA and spatial QA datasets, effectively enhancing the 3D
perceptual and reasoning capabilities of the VLM backbone. Specifically, the
VLM is trained to predict two complementary forms of actions: latent actions,
derived from our latent action model pretrained on cross-embodiment
manipulation data, which capture implicit high-level intentions; and structured
discrete action tokens, obtained through frequency-domain transformations of
continuous control signals, which encode explicit low-level dynamics. This dual
supervision aligns the representation spaces of language, vision, and action,
enabling the VLM to directly contribute to action generation. Experimental
results on the LIBERO Franka benchmark demonstrate the superiority of our
frame-work, while real-world evaluations further show that iFlyBot-VLA achieves
competitive success rates across diverse and challenging manipulation tasks.
Furthermore, we plan to open-source a portion of our self-constructed dataset
to support future research in the community

</details>


### [2] [OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control](https://arxiv.org/abs/2511.02483)
*Xilong Zhou,Jianchun Chen,Pramod Rao,Timo Teufel,Linjie Lyu,Tigran Minasian,Oleksandr Sotnychenko,Xiaoxiao Long,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: OLATverse是一个包含765个真实物体的大规模数据集，提供多样化照明条件和高保真外观数据，旨在推动逆向渲染和重新照明技术的发展。


<details>
  <summary>Details</summary>
Motivation: 解决现有逆向渲染、新视角合成和重新照明技术因依赖合成数据集和小规模真实数据集而受限的问题，提升技术的真实性和泛化能力。

Method: 构建了一个包含765个真实世界物体的大规模数据集，每个物体使用35个DSLR相机和331个独立控制光源进行拍摄，提供多样化的照明条件模拟。辅助资源包括校准的相机参数、精确物体掩码、光度法表面法线和漫反射反照率。

Result: OLATverse成为首个全面的真实世界物体中心基准，支持逆向渲染和法线估计，数据集和所有后处理工作流程将公开。

Conclusion: OLATverse数据集通过提供大规模真实物体覆盖和精确控制照明下的高保真外观，为逆向渲染和重新照明方法的下一代发展奠定了重要基础。

Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions. While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization. To address this gap, OLATverse
offers two key advantages over existing datasets: large-scale coverage of real
objects and high-fidelity appearance under precisely controlled illuminations.
Specifically, OLATverse contains 765 common and uncommon real-world objects,
spanning a wide range of material categories. Each object is captured using 35
DSLR cameras and 331 individually controlled light sources, enabling the
simulation of diverse illumination conditions. In addition, for each object, we
provide well-calibrated camera parameters, accurate object masks, photometric
surface normals, and diffuse albedo as auxiliary resources. We also construct
an extensive evaluation set, establishing the first comprehensive real-world
object-centric benchmark for inverse rendering and normal estimation. We
believe that OLATverse represents a pivotal step toward integrating the next
generation of inverse rendering and relighting methods with real-world data.
The full dataset, along with all post-processing workflows, will be publicly
released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.

</details>


### [3] [Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound](https://arxiv.org/abs/2511.01915)
*Edoardo Conti,Riccardo Rosati,Lorenzo Federici,Adriano Mancini,Maria Chiara Fiorentin*

Main category: cs.CV

TL;DR: 本研究评估了DINOv3在低类间变异性胎儿超声成像中的表现，发现领域特定的预训练对区分解剖学相似结构至关重要，优于通用基础模型。


<details>
  <summary>Details</summary>
Motivation: 本研究首次在低类间变异性条件下对胎儿超声成像中的基础模型进行全面评估。尽管最近的视觉基础模型如DINOv3在跨医学领域表现出显著的迁移能力，但其区分解剖学相似结构的能力尚未得到系统研究。我们通过关注胎儿脑部标准平面（TT、TV、TC）来解决这一差距，这些平面具有高度重叠的解剖特征，对可靠的生物特征评估构成关键挑战。

Method: 为确保公平和可重复的评估，所有公开可用的胎儿超声数据集被整理并聚合为一个统一的多中心基准FetalUS-188K，包含超过188,000张来自不同采集设置的注释图像。DINOv3以自监督方式预训练以学习超声感知表示。通过标准化的适应协议评估学习到的特征，包括冻结主干的线性探测和完全微调，采用两种初始化方案：(i)在FetalUS-188K上预训练和(ii)从自然图像DINOv3权重初始化。

Result: 在胎儿超声数据上预训练的模型始终优于从自然图像初始化的模型，加权F1分数提高了高达20%。领域自适应预训练使网络能够保留区分中间平面如TV的关键细微回声和结构线索。

Conclusion: 结果表明，通用基础模型在低类间变异性条件下无法泛化，而领域特定的预训练对于在胎儿脑部超声成像中实现稳健且临床可靠的表示至关重要。

Abstract: Purpose: This study provides the first comprehensive evaluation of foundation
models in fetal ultrasound (US) imaging under low inter-class variability
conditions. While recent vision foundation models such as DINOv3 have shown
remarkable transferability across medical domains, their ability to
discriminate anatomically similar structures has not been systematically
investigated. We address this gap by focusing on fetal brain standard
planes--transthalamic (TT), transventricular (TV), and transcerebellar
(TC)--which exhibit highly overlapping anatomical features and pose a critical
challenge for reliable biometric assessment.
  Methods: To ensure a fair and reproducible evaluation, all publicly available
fetal ultrasound datasets were curated and aggregated into a unified
multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated
images from heterogeneous acquisition settings. DINOv3 was pretrained in a
self-supervised manner to learn ultrasound-aware representations. The learned
features were then evaluated through standardized adaptation protocols,
including linear probing with frozen backbone and full fine-tuning, under two
initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization
from natural-image DINOv3 weights.
  Results: Models pretrained on fetal ultrasound data consistently outperformed
those initialized on natural images, with weighted F1-score improvements of up
to 20 percent. Domain-adaptive pretraining enabled the network to preserve
subtle echogenic and structural cues crucial for distinguishing intermediate
planes such as TV.
  Conclusion: Results demonstrate that generic foundation models fail to
generalize under low inter-class variability, whereas domain-specific
pretraining is essential to achieve robust and clinically reliable
representations in fetal brain ultrasound imaging.

</details>


### [4] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai,Ryugo Morita,Shunsuke Kitada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: TAUE 是一种零样本分层图像生成框架，通过噪声移植与培养技术实现多层一致输出，无需训练或额外数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像扩散模型在专业应用中无法实现分层控制的瓶颈问题。

Method: 提出了训练自由的噪声移植与培养（NTC）技术，通过提取前景和复合生成过程中的中间潜在表示，并将其移植到后续层的初始噪声中。

Result: 实验表明，TAUE 无需训练即可达到与微调方法相当的性能，同时保持高图像质量和一致性。

Conclusion: TAUE 通过训练自由的噪声移植与培养技术，实现了无需微调或辅助数据集的多层一致图像生成，为专业应用提供了可控且高质量的生成工作流。

Abstract: Despite the remarkable success of text-to-image diffusion models, their
output of a single, flattened image remains a critical bottleneck for
professional applications requiring layer-wise control. Existing solutions
either rely on fine-tuning with large, inaccessible datasets or are
training-free yet limited to generating isolated foreground elements, failing
to produce a complete and coherent scene. To address this, we introduce the
Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a
novel framework for zero-shot, layer-wise image generation. Our core technique,
Noise Transplantation and Cultivation (NTC), extracts intermediate latent
representations from both foreground and composite generation processes,
transplanting them into the initial noise for subsequent layers. This ensures
semantic and structural coherence across foreground, background, and composite
layers, enabling consistent, multi-layered outputs without requiring
fine-tuning or auxiliary datasets. Extensive experiments show that our
training-free method achieves performance comparable to fine-tuned methods,
enhancing layer-wise consistency while maintaining high image quality and
fidelity. TAUE not only eliminates costly training and dataset requirements but
also unlocks novel downstream applications, such as complex compositional
editing, paving the way for more accessible and controllable generative
workflows.

</details>


### [5] [Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users](https://arxiv.org/abs/2511.01990)
*Saurabh Kaushik,Lalit Maurya,Elizabeth Tellman,ZhiJie Zhang*

Main category: cs.CV

TL;DR: GFMs（尤其是Clay）在洪水淹没地图绘制中表现优异，计算效率高，少样本学习能力强，优于传统U-Net模型。


<details>
  <summary>Details</summary>
Motivation: 尽管GFMs在卫星图像中提取时空信息方面具有潜力，但其与传统模型的优劣尚未明确。本研究旨在通过系统比较，指导终端用户选择最佳模型。

Method: 本研究比较了三种GFMs（Prithvi 2.0、Clay V1.5、DOFA）和UViT与传统模型（TransNorm、U-Net、Attention U-Net），使用PlanetScope、Sentinel-1和Sentinel-2数据进行评估。通过跨区域交叉验证和少样本实验验证模型性能。

Result: Clay模型在PlanetScope和Sentinel-2上表现最佳（mIoU分别为0.79和0.70），Prithvi在Sentinel-1上领先（0.57）。少样本实验中，Clay仅用5张训练图像即达到0.64 mIoU，显著优于其他GFMs。GFMs在计算效率上也优于传统模型。

Conclusion: Geo-Foundational Models (GFMs) 在洪水淹没地图绘制中表现出色，尤其在计算成本和标注效率方面优于传统模型如U-Net。Clay模型在多数情况下表现最佳，尤其是在细节保留和少样本学习场景中。

Abstract: Geo-Foundational Models (GFMs) enable fast and reliable extraction of
spatiotemporal information from satellite imagery, improving flood inundation
mapping by leveraging location and time embeddings. Despite their potential, it
remains unclear whether GFMs outperform traditional models like U-Net. A
systematic comparison across sensors and data availability scenarios is still
lacking, which is an essential step to guide end-users in model selection. To
address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a
Prithvi variant), against TransNorm, U-Net, and Attention U-Net using
PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance
among all GFMs, with only 2-5% variation between the best and worst models
across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and
Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In
leave-one-region-out cross-validation across five regions, Clay shows slightly
better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07),
0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA
(0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and
Sentinel-1, respectively. Across all 19 sites, leave-one-region-out
cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual
inspection highlights Clay's superior ability to retain fine details. Few-shot
experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training
images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational
time, Clay is a better choice due to its smaller model size (26M parameters),
making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M).
Contrary to previous findings, our results suggest GFMs offer small to moderate
improvements in flood mapping accuracy at lower computational cost and labeling
effort compared to traditional U-Net.

</details>


### [6] [Locally-Supervised Global Image Restoration](https://arxiv.org/abs/2511.01998)
*Benjamin Walder,Daniel Toader,Robert Nuster,Günther Paltauf,Peter Burgholzer,Gregor Langer,Lukas Krainer,Markus Haltmeier*

Main category: cs.CV

TL;DR: 本文提出一种基于学习的图像重建方法，利用图像不变性克服固定采样模式的不足，在PAM图像上采样中仅需少量数据即可媲美全监督方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督方法需要完整真实数据，以及自监督方法依赖随机采样的问题，特别是在固定、确定性采样模式下图像重建的挑战。

Method: 利用图像分布的多种不变性，克服固定确定性采样模式的局限性，实现与全监督方法相当的图像重建性能。

Result: 在光声显微镜（PAM）的光学分辨率图像上采样任务中，该方法表现出竞争力或优越性，且所需真实数据大幅减少。

Conclusion: 本文提出的方法在光学分辨率图像上采样中表现优异，仅需少量真实数据即可达到或超越全监督方法的性能。

Abstract: We address the problem of image reconstruction from incomplete measurements,
encompassing both upsampling and inpainting, within a learning-based framework.
Conventional supervised approaches require fully sampled ground truth data,
while self-supervised methods allow incomplete ground truth but typically rely
on random sampling that, in expectation, covers the entire image. In contrast,
we consider fixed, deterministic sampling patterns with inherently incomplete
coverage, even in expectation. To overcome this limitation, we exploit multiple
invariances of the underlying image distribution, which theoretically allows us
to achieve the same reconstruction performance as fully supervised approaches.
We validate our method on optical-resolution image upsampling in photoacoustic
microscopy (PAM), demonstrating competitive or superior results while requiring
substantially less ground truth data.

</details>


### [7] [Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2511.02014)
*Tuan Truong,Guillermo Jimenez Perez,Pedro Osorio,Matthias Lenga*

Main category: cs.CV

TL;DR: LMM在OCR上优于传统模型，但PHI检测改进有限；复杂场景表现最佳，提出了LMM选择和部署策略。


<details>
  <summary>Details</summary>
Motivation: 传统PHI检测方法依赖OCR和命名实体识别，LMM的出现为文本提取和语义分析提供了新机会。

Method: 系统评估了三种LMM（GPT-4o、Gemini 2.5 Flash、Qwen 2.5 7B），采用两种管道配置（纯文本分析及OCR+语义分析）。

Result: LMM在OCR效果上显著优于传统模型（WER: 0.03-0.05, CER: 0.02-0.03），但整体PHI检测改进有限，复杂印记场景提升明显。

Conclusion: LMM在OCR性能上优于传统模型，但在整体PHI检测准确率上改进不一致。复杂印记场景下表现最佳，提出了针对操作约束的LMM选择建议和模块化部署策略。

Abstract: The detection of Protected Health Information (PHI) in medical imaging is
critical for safeguarding patient privacy and ensuring compliance with
regulatory frameworks. Traditional detection methodologies predominantly
utilize Optical Character Recognition (OCR) models in conjunction with named
entity recognition. However, recent advancements in Large Multimodal Model
(LMM) present new opportunities for enhanced text extraction and semantic
analysis. In this study, we systematically benchmark three prominent closed and
open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing
two distinct pipeline configurations: one dedicated to text analysis alone and
another integrating both OCR and semantic analysis. Our results indicate that
LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to
conventional models like EasyOCR. However, this improvement in OCR performance
does not consistently correlate with enhanced overall PHI detection accuracy.
The strongest performance gains are observed on test cases with complex imprint
patterns. In scenarios where text regions are well readable with sufficient
contrast, and strong LMMs are employed for text analysis after OCR, different
pipeline configurations yield similar results. Furthermore, we provide
empirically grounded recommendations for LMM selection tailored to specific
operational constraints and propose a deployment strategy that leverages
scalable and modular infrastructure.

</details>


### [8] [StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities](https://arxiv.org/abs/2511.02027)
*Zeyu Yang,Clayton Souza Leite,Yu Xiao*

Main category: cs.CV

TL;DR: StrengthSense是一个开放的IMU信号数据集，包含11种力量需求活动和2种非力量需求活动，旨在支持人类活动识别算法和健康监测应用的开发。


<details>
  <summary>Details</summary>
Motivation: 缺乏捕捉力量需求活动的全面数据集，需要填补这一空白以支持肌肉力量、耐力和功率的监测。

Method: 通过29名健康受试者使用10个IMU传感器收集数据，并利用视频记录进行注释。进行了IMU估计的关节角度与视频直接提取的关节角度的比较分析。

Result: 引入了StrengthSense数据集，包含11种力量需求活动和2种非力量需求活动，数据经过技术验证和比较分析，验证了传感器数据的准确性和可靠性。

Conclusion: StrengthSense数据集为研究者和开发者提供了一个全面的、开放的数据资源，可用于推进人类活动识别算法的发展，以及创建健身和健康监测应用。

Abstract: Tracking strength-demanding activities with wearable sensors like IMUs is
crucial for monitoring muscular strength, endurance, and power. However, there
is a lack of comprehensive datasets capturing these activities. To fill this
gap, we introduce \textit{StrengthSense}, an open dataset that encompasses IMU
signals capturing 11 strength-demanding activities, such as sit-to-stand,
climbing stairs, and mopping. For comparative purposes, the dataset also
includes 2 non-strength demanding activities. The dataset was collected from 29
healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was
annotated using video recordings as references. This paper provides a
comprehensive overview of the data collection, pre-processing, and technical
validation. We conducted a comparative analysis between the joint angles
estimated by IMUs and those directly extracted from video to verify the
accuracy and reliability of the sensor data. Researchers and developers can
utilize \textit{StrengthSense} to advance the development of human activity
recognition algorithms, create fitness and health monitoring applications, and
more.

</details>


### [9] [Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis](https://arxiv.org/abs/2511.02046)
*Soham Joshi,Shwet Kamal Mishra,Viswanath Gopalakrishnan*

Main category: cs.CV

TL;DR: 提出首个自动化流程，结合OCR、标题生成等技术，高效合成大规模text-VQA数据集（72K QA对/44K图像）。


<details>
  <summary>Details</summary>
Motivation: 传统的大规模text-VQA数据集构建依赖人工标注，耗时且具有挑战性。利用现有基础模型和成熟的OCR技术，需要一种自动化流程来高效生成QA对。

Method: 该方法整合了OCR检测与识别、兴趣区域（ROI）检测、标题生成和问题生成等多个模块，形成一个连贯的流程。

Result: 提出的流程成功生成了一个包含约72K QA对、基于44K图像的大规模text-VQA数据集。

Conclusion: 该论文提出了一个自动合成文本视觉问答（text-VQA）数据集的首个端到端流程，能够高效生成并验证大规模QA对。

Abstract: Creation of large-scale databases for Visual Question Answering tasks
pertaining to the text data in a scene (text-VQA) involves skilful human
annotation, which is tedious and challenging. With the advent of foundation
models that handle vision and language modalities, and with the maturity of OCR
systems, it is the need of the hour to establish an end-to-end pipeline that
can synthesize Question-Answer (QA) pairs based on scene-text from a given
image. We propose a pipeline for automated synthesis for text-VQA dataset that
can produce faithful QA pairs, and which scales up with the availability of
scene text data. Our proposed method harnesses the capabilities of multiple
models and algorithms involving OCR detection and recognition (text spotting),
region of interest (ROI) detection, caption generation, and question
generation. These components are streamlined into a cohesive pipeline to
automate the synthesis and validation of QA pairs. To the best of our
knowledge, this is the first pipeline proposed to automatically synthesize and
validate a large-scale text-VQA dataset comprising around 72K QA pairs based on
around 44K images.

</details>


### [10] [Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study](https://arxiv.org/abs/2511.02086)
*Yue Yang,Fabian Necker,Christoph Leuze,Michelle Chen,Andrey Finegersh,Jake Lee,Vasu Divi,Bruce Daniel,Brian Hargreaves,Jie Ying Wu,Fred M Baik*

Main category: cs.CV

TL;DR: 头戴式显示器上的无标记AR配准流程在手术中实现了3-4毫米的中位误差，接近临床阈值，适用于小或低曲率目标。


<details>
  <summary>Details</summary>
Motivation: 开发并临床评估一种仅深度、无标记的AR配准流程，评估其在真实手术环境中对小或低曲率解剖结构的准确性。

Method: 在HoloLens 2上，通过（i）深度偏差校正，（ii）简短的人工引导初始化，（iii）全局和局部配准，将Articulated Hand Tracking（AHAT）深度与CT衍生的皮肤网格对齐。通过AR追踪工具在腿和足模型上验证了“皮肤到骨骼”相对距离与CT真实值的表面追踪误差指标。

Result: 临床前验证显示AR追踪与CT距离高度一致（腿：中位|Δd| 0.78毫米，RMSE 0.97毫米；足：0.80毫米，1.20毫米）。临床中，每点误差中位数为3.9毫米，按解剖结构分别为足3.2毫米、耳4.3毫米、小腿5.3毫米，5毫米覆盖率分别为92-95%、84-90%、72-86%。足与小腿差异显著（中位差约1.1毫米；p < 0.001）。

Conclusion: 在头戴式显示器上开发的仅深度、无标记的增强现实（AR）配准流程在实时手术环境中实现了约3-4毫米的中位误差，接近中等风险任务的典型临床误差阈值。人工引导初始化加全局到局部配准实现了对小或低曲率目标的准确对齐，提高了无标记AR引导的临床准备度。

Abstract: Purpose: In this paper, we develop and clinically evaluate a depth-only,
markerless augmented reality (AR) registration pipeline on a head-mounted
display, and assess accuracy across small or low-curvature anatomies in
real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd
Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i)
depth-bias correction, (ii) brief human-in-the-loop initialization, (iii)
global and local registration. We validated the surface-tracing error metric by
comparing "skin-to-bone" relative distances to CT ground truth on leg and foot
models, using an AR-tracked tool. We then performed seven intraoperative target
trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap
harvest and mandibular reconstruction surgery, and collected 500+ data per
trial. Results: Preclinical validation showed tight agreement between AR-traced
and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm,
1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by
anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm
coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed
significantly (Delta median ~1.1 mm; p < 0.001). Conclusion: A depth-only,
markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear,
and lower leg in live surgical settings without fiducials, approaching typical
clinical error thresholds for moderate-risk tasks. Human-guided initialization
plus global-to-local registration enabled accurate alignment on small or
low-curvature targets, improving the clinical readiness of markerless AR
guidance.

</details>


### [11] [From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera](https://arxiv.org/abs/2511.02142)
*Huahua Lin,Xiaohao Cai,Mark Nixon,James M. Mulqueeney,Thomas H. G. Ezard*

Main category: cs.CV

TL;DR: 研究提出自动化流程，结合实例分割和腔室排序算法，高效追踪有孔虫生长轨迹，减少人工干预，为生态研究提供新工具。


<details>
  <summary>Details</summary>
Motivation: 虽然有孔虫是环境和生态的重要指标，但其腔室生长的自动追踪仍依赖耗时且主观的手动分割，因此需要更高效的自动化解決方案。

Method: 研究提出了一种端到端的流程，结合实例分割技术和专用的腔室排序算法，从高分辨率计算机断层扫描数据中自动重建三维生长轨迹。

Result: 实验结果表明，该流程显著减少了人工干预，同时保持了生物学意义的准确性，腔室排序算法在部分分割情况下仍能稳健重建发育轨迹。

Conclusion: 该研究首次提供了完全自动化和可重复的有孔虫生长分析流程，为大规模数据驱动的生态研究奠定了基础。

Abstract: Planktonic foraminifera, marine protists characterized by their intricate
chambered shells, serve as valuable indicators of past and present
environmental conditions. Understanding their chamber growth trajectory
provides crucial insights into organismal development and ecological adaptation
under changing environments. However, automated tracing of chamber growth from
imaging data remains largely unexplored, with existing approaches relying
heavily on manual segmentation of each chamber, which is time-consuming and
subjective. In this study, we propose an end-to-end pipeline that integrates
instance segmentation, a computer vision technique not extensively explored in
foraminifera, with a dedicated chamber ordering algorithm to automatically
reconstruct three-dimensional growth trajectories from high-resolution computed
tomography scans. We quantitatively and qualitatively evaluate multiple
instance segmentation methods, each optimized for distinct spatial features of
the chambers, and examine their downstream influence on growth-order
reconstruction accuracy. Experimental results on expert-annotated datasets
demonstrate that the proposed pipeline substantially reduces manual effort
while maintaining biologically meaningful accuracy. Although segmentation
models exhibit under-segmentation in smaller chambers due to reduced voxel
fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm
remains robust, achieving consistent reconstruction of developmental
trajectories even under partial segmentation. This work provides the first
fully automated and reproducible pipeline for digital foraminiferal growth
analysis, establishing a foundation for large-scale, data-driven ecological
studies.

</details>


### [12] [Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis](https://arxiv.org/abs/2511.02144)
*Zhicheng Wang,Junbiao Pang*

Main category: cs.CV

TL;DR: 提出一种结合PCA和RPCA的级联框架，显著提升路面裂缝宽度测量的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 路面裂缝宽度的精确量化对评估结构完整性和指导维护至关重要，但传统方法因裂缝边界复杂性和快速测量需求而受限。

Method: 研究采用三阶段方法：1) 使用现有检测算法进行裂缝初始分割；2) 通过PCA确定准平行裂缝的主方向轴；3) 利用RPCA提取不规则裂缝几何的主传播轴。

Result: 在三个公开数据集上的评估显示，该方法在计算效率和测量准确性上优于现有技术。

Conclusion: 该研究通过集成PCA和RPCA的级联框架，显著提升了路面裂缝宽度测量的准确性和计算效率，为路面状况评估提供了有效工具。

Abstract: Accurate quantification of pavement crack width plays a pivotal role in
assessing structural integrity and guiding maintenance interventions. However,
achieving precise crack width measurements presents significant challenges due
to: (1) the complex, non-uniform morphology of crack boundaries, which limits
the efficacy of conventional approaches, and (2) the demand for rapid
measurement capabilities from arbitrary pixel locations to facilitate
comprehensive pavement condition evaluation. To overcome these limitations,
this study introduces a cascaded framework integrating Principal Component
Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from
digital images. The proposed methodology comprises three sequential stages: (1)
initial crack segmentation using established detection algorithms to generate a
binary representation, (2) determination of the primary orientation axis for
quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation
Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations
were conducted across three publicly available datasets, demonstrating that the
proposed approach achieves superior performance in both computational
efficiency and measurement accuracy compared to existing state-of-the-art
techniques.

</details>


### [13] [Autobiasing Event Cameras for Flickering Mitigation](https://arxiv.org/abs/2511.02180)
*Mehdi Sefidgar Dilmaghani,Waseem Shariff,Cian Ryan,Joe Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: 通过CNN动态调整事件相机偏置，显著减少闪烁效应，提升人脸检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机因光线强度快速变化导致的闪烁问题，提升其在多样化环境中的表现。

Method: 利用卷积神经网络（CNN）在空间域识别闪烁，并动态调整特定偏置以减少其影响。

Result: 在良好光照和低光条件下，YOLO置信度指标和捕捉到的人脸帧数均有显著提升，平均梯度（闪烁指标）分别降低了38.2%和53.6%。

Conclusion: 该论文提出的自主偏置调整机制能有效减少事件相机中的闪烁效应，提升其在各种光照条件下的性能。

Abstract: Understanding and mitigating flicker effects caused by rapid variations in
light intensity is critical for enhancing the performance of event cameras in
diverse environments. This paper introduces an innovative autonomous mechanism
for tuning the biases of event cameras, effectively addressing flicker across a
wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on
additional hardware or software for flicker filtering, our approach leverages
the event cameras inherent bias settings. Utilizing a simple Convolutional
Neural Networks -CNNs, the system identifies instances of flicker in a spatial
space and dynamically adjusts specific biases to minimize its impact. The
efficacy of this autobiasing system was robustly tested using a face detector
framework under both well-lit and low-light conditions, as well as across
various frequencies. The results demonstrated significant improvements:
enhanced YOLO confidence metrics for face detection, and an increased
percentage of frames capturing detected faces. Moreover, the average gradient,
which serves as an indicator of flicker presence through edge detection,
decreased by 38.2 percent in well-lit conditions and by 53.6 percent in
low-light conditions. These findings underscore the potential of our approach
to significantly improve the functionality of event cameras in a range of
adverse lighting scenarios.

</details>


### [14] [Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2511.02182)
*Jinhwan Seo,Yoonki Cho,Junhyug Noh,Sung-eui Yoon*

Main category: cs.CV

TL;DR: 本文提出了一个三阶段框架解决GVQA任务，通过CORTEX提示触发关键帧，显著提升了HOTA分数。


<details>
  <summary>Details</summary>
Motivation: 解决GVQA任务需要多模态模型具备复杂视频内容推理、视觉答案定位及时间对象追踪的能力。

Method: 将GVQA任务分解为三个阶段：视频推理与问答、时空定位和跟踪，并引入CORTEX提示触发的关键帧作为定位和跟踪的锚点。

Result: 在GVQA任务中，HOTA分数达到0.4968，较去年获胜分数0.2704有显著提升。

Conclusion: 提出的三阶段流程和CORTEX提示触发的关键帧方法显著提升了GVQA任务的性能，HOTA分数从0.2704提升至0.4968。

Abstract: In this technical report, we introduce a framework to address Grounded Video
Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The
GVQA task demands robust multimodal models capable of complex reasoning over
video content, grounding the resulting answers visually, and tracking the
referenced objects temporally. To achieve this capability, our proposed
approach decomposes the GVQA task into a three-stage pipeline: (1) Video
Reasoning \& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key
contribution is the introduction of a trigger moment, derived from our proposed
CORTEX prompt, which pinpoints the single most visible frame of a target object
to serve as a robust anchor for grounding and tracking. To this end, we achieve
the HOTA score of 0.4968, which marks a significant improvement over the
previous year's winning score of 0.2704 on GVQA task.

</details>


### [15] [MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation](https://arxiv.org/abs/2511.02193)
*Jiawen Liu,Yuanbo Zeng,Jiaming Liang,Yizhen Yang,Yiheng Zhang,Enhui Cai,Xiaoqi Sheng,Hongmin Cai*

Main category: cs.CV

TL;DR: MM-UNet通过新型架构设计，在视网膜血管分割任务中显著提升了精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管的极细分支结构和全局形态变化对分割精度和鲁棒性提出挑战，需要针对性解决方案。

Method: 提出MM-UNet架构，结合Morph Mamba卷积层增强分支拓扑感知，以及反向选择性状态引导模块提升几何边界感知和解码效率。

Result: 在DRIVE和STARE数据集上，MM-UNet的F1分数分别提升了1.64%和1.25%，证明了其有效性。

Conclusion: MM-UNet是一种专为高效视网膜血管分割设计的新架构，通过Morph Mamba卷积层和反向选择性状态引导模块，显著提升了分割精度和鲁棒性。

Abstract: Accurate detection of retinal vessels plays a critical role in reflecting a
wide range of health status indicators in the clinical diagnosis of ocular
diseases. Recently, advances in deep learning have led to a surge in retinal
vessel segmentation methods, which have significantly contributed to the
quantitative analysis of vascular morphology. However, retinal vasculature
differs significantly from conventional segmentation targets in that it
consists of extremely thin and branching structures, whose global morphology
varies greatly across images. These characteristics continue to pose challenges
to segmentation precision and robustness. To address these issues, we propose
MM-UNet, a novel architecture tailored for efficient retinal vessel
segmentation. The model incorporates Morph Mamba Convolution layers, which
replace pointwise convolutions to enhance branching topological perception
through morph, state-aware feature sampling. Additionally, Reverse Selective
State Guidance modules integrate reverse guidance theory with state-space
modeling to improve geometric boundary awareness and decoding efficiency.
Extensive experiments conducted on two public retinal vessel segmentation
datasets demonstrate the superior performance of the proposed method in
segmentation accuracy. Compared to the existing approaches, MM-UNet achieves
F1-score gains of 1.64 $\%$ on DRIVE and 1.25 $\%$ on STARE, demonstrating its
effectiveness and advancement. The project code is public via
https://github.com/liujiawen-jpg/MM-UNet.

</details>


### [16] [Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers](https://arxiv.org/abs/2511.02206)
*Zhengjie Zhang,Xiaoxie Mao,Qihao Guo,Shaoting Zhang,Qi Huang,Mu Zhou,Fang Xie,Mianxin Liu*

Main category: cs.CV

TL;DR: 研究开发了一种语言增强生成模型，通过MRI和BBMs合成逼真的PET图像，优化了AD诊断流程，合成图像在结构和诊断性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 淀粉样蛋白PET（Abeta-PET）在阿尔茨海默病（AD）诊断中至关重要，但其高成本和有限的可用性限制了其广泛应用。研究旨在探索是否可以通过血液生物标志物（BBMs）和MRI扫描预测Abeta-PET的空间模式。

Method: 研究收集了566名参与者的淀粉样蛋白PET图像、T1加权MRI扫描和血液生物标志物（BBMs），开发了一种由大型语言模型（LLM）和多模态信息融合驱动的语言增强生成模型，用于合成PET图像。合成图像在全自动诊断流程中评估了图像质量、诊断一致性和临床适用性。

Result: 合成的PET图像在结构细节（SSIM = 0.920 +/- 0.003）和区域模式（Pearson's r = 0.955 +/- 0.007）上与真实PET扫描高度相似。基于合成PET的诊断结果与真实PET的诊断高度一致（准确率 = 0.80）。合成PET与BBMs结合进一步提升了诊断性能（AUC = 0.79）。

Conclusion: 语言增强生成模型成功合成了逼真的PET图像，提升了MRI和血液生物标志物（BBMs）在淀粉样蛋白空间模式评估中的应用价值，并优化了阿尔茨海默病的诊断流程。

Abstract: Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta
positron emission tomography (Abeta-PET), which is limited by high cost and
limited accessibility. This study explores whether Abeta-PET spatial patterns
can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We
collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566
participants. A language-enhanced generative model, driven by a large language
model (LLM) and multimodal information fusion, was developed to synthesize PET
images. Synthesized images were evaluated for image quality, diagnostic
consistency, and clinical applicability within a fully automated diagnostic
pipeline. Findings: The synthetic PET images closely resemble real PET scans in
both structural details (SSIM = 0.920 +/- 0.003) and regional patterns
(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show
high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic
PET, we developed a fully automatic AD diagnostic pipeline integrating PET
synthesis and classification. The synthetic PET-based model (AUC = 0.78)
outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while
combining synthetic PET and BBMs further improved performance (AUC = 0.79).
Ablation analysis supports the advantages of LLM integration and prompt
engineering. Interpretation: Our language-enhanced generative model synthesizes
realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial
pattern assessment and improving the diagnostic workflow for Alzheimer's
disease.

</details>


### [17] [Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping](https://arxiv.org/abs/2511.02207)
*Jiajia Li,Keyi Zhu,Qianwen Zhang,Dong Chen,Qi Sun,Zhaojian Li*

Main category: cs.CV

TL;DR: 论文提出了一种结合SAM-2和背景遮罩的对象中心3D重建框架，显著提升了草莓植株表型分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法耗时、费力且具破坏性，而现有3D重建技术常包含背景噪声，增加计算成本并干扰性状分析。因此，需要一种更高效、精准的对象中心重建方法。

Method: 利用神经渲染技术（如3D高斯泼溅）和多视角图像序列，结合SAM-2分割和背景遮罩，实现背景无关的草莓植株3D重建，并通过DBSCAN聚类和PCA自动估算植株性状。

Result: 实验表明，该方法在准确性和效率上优于传统流程，能够自动估算植株高度和冠层宽度等性状。

Conclusion: 该论文提出了一种基于对象中心的3D重建框架，结合预处理流程（使用SAM-2和alpha通道背景遮罩），显著提升了草莓植株重建的准确性和计算效率，为植物表型分析提供了可扩展且非破坏性的解决方案。

Abstract: Strawberries are among the most economically significant fruits in the United
States, generating over $2 billion in annual farm-gate sales and accounting for
approximately 13% of the total fruit production value. Plant phenotyping plays
a vital role in selecting superior cultivars by characterizing plant traits
such as morphology, canopy structure, and growth dynamics. However, traditional
plant phenotyping methods are time-consuming, labor-intensive, and often
destructive. Recently, neural rendering techniques, notably Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful
frameworks for high-fidelity 3D reconstruction. By capturing a sequence of
multi-view images or videos around a target plant, these methods enable
non-destructive reconstruction of complex plant architectures. Despite their
promise, most current applications of 3DGS in agricultural domains reconstruct
the entire scene, including background elements, which introduces noise,
increases computational costs, and complicates downstream trait analysis. To
address this limitation, we propose a novel object-centric 3D reconstruction
framework incorporating a preprocessing pipeline that leverages the Segment
Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean
strawberry plant reconstructions. This approach produces more accurate
geometric representations while substantially reducing computational time. With
a background-free reconstruction, our algorithm can automatically estimate
important plant traits, such as plant height and canopy width, using DBSCAN
clustering and Principal Component Analysis (PCA). Experimental results show
that our method outperforms conventional pipelines in both accuracy and
efficiency, offering a scalable and non-destructive solution for strawberry
plant phenotyping.

</details>


### [18] [Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning](https://arxiv.org/abs/2511.02210)
*Anders Austlid Taskén,Thierry Judge,Erik Andreas Rye Berg,Jinyang Yu,Bjørnar Grenne,Frank Lindseth,Svend Aakhus,Pierre-Marc Jodoin,Nicolas Duchateau,Olivier Bernard,Gabriel Kiss*

Main category: cs.CV

TL;DR: 该研究开发了首个自动化流程autoStrain，利用深度学习方法改进TEE中的SLS估计，TeeTracker表现优于TeeFlow，临床验证结果与参考一致，显著提升心脏功能评估效率。


<details>
  <summary>Details</summary>
Motivation: 当前应变估计技术需要大量手动干预和专业知识，限制了效率且资源密集，无法满足监测需求。

Method: 研究提出了首个自动化流程autoStrain，利用深度学习（DL）方法进行运动估计，比较了两种DL方法：TeeFlow（基于RAFT光流模型）和TeeTracker（基于CoTracker点轨迹模型）。

Result: TeeTracker在准确性上优于TeeFlow，合成TEE测试数据集上的运动估计平均距离误差为0.65毫米。临床验证显示，autoStrain流程的SLS估计与临床参考一致，平均差异为1.09%（-8.90%至11.09%）。

Conclusion: 该研究表明，通过将AI驱动的运动估计与TEE结合，可以显著提高临床中心脏功能评估的精确性和效率。

Abstract: Segmental longitudinal strain (SLS) of the left ventricle (LV) is an
important prognostic indicator for evaluating regional LV dysfunction, in
particular for diagnosing and managing myocardial ischemia. Current techniques
for strain estimation require significant manual intervention and expertise,
limiting their efficiency and making them too resource-intensive for monitoring
purposes. This study introduces the first automated pipeline, autoStrain, for
SLS estimation in transesophageal echocardiography (TEE) using deep learning
(DL) methods for motion estimation. We present a comparative analysis of two DL
approaches: TeeFlow, based on the RAFT optical flow model for dense
frame-to-frame predictions, and TeeTracker, based on the CoTracker point
trajectory model for sparse long-sequence predictions.
  As ground truth motion data from real echocardiographic sequences are hardly
accessible, we took advantage of a unique simulation pipeline (SIMUS) to
generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with
ground truth myocardial motion to train and evaluate both models. Our
evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a
mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.
  Clinical validation on 16 patients further demonstrated that SLS estimation
with our autoStrain pipeline aligned with clinical references, achieving a mean
difference (95\% limits of agreement) of 1.09% (-8.90% to 11.09%).
Incorporation of simulated ischemia in the synTEE data improved the accuracy of
the models in quantifying abnormal deformation. Our findings indicate that
integrating AI-driven motion estimation with TEE can significantly enhance the
precision and efficiency of cardiac function assessment in clinical settings.

</details>


### [19] [Can Foundation Models Revolutionize Mobile AR Sparse Sensing?](https://arxiv.org/abs/2511.02215)
*Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: 基础模型为移动稀疏传感系统带来显著改进，尤其在几何感知图像扭曲和3D重建方面，但也面临集成挑战。


<details>
  <summary>Details</summary>
Motivation: 解决移动稀疏传感系统在计算、功率等限制下面临的传感质量与效率之间的权衡问题。

Method: 使用真实世界的移动AR数据，评估基础模型在几何感知图像扭曲中的应用，并研究其可扩展性。

Result: 基础模型显著提升了几何感知图像扭曲的准确性，并在3D场景重建中表现出领先性能。

Conclusion: 本研究揭示了将基础模型集成到移动稀疏传感系统中的前景与挑战，展示了其在几何感知图像扭曲和3D场景重建中的领先性能。

Abstract: Mobile sensing systems have long faced a fundamental trade-off between
sensing quality and efficiency due to constraints in computation, power, and
other limitations. Sparse sensing, which aims to acquire and process only a
subset of sensor data, has been a key strategy for maintaining performance
under such constraints. However, existing sparse sensing methods often suffer
from reduced accuracy, as missing information across space and time introduces
uncertainty into many sensing systems. In this work, we investigate whether
foundation models can change the landscape of mobile sparse sensing. Using
real-world mobile AR data, our evaluations demonstrate that foundation models
offer significant improvements in geometry-aware image warping, a central
technique for enabling accurate reuse of cross-frame information. Furthermore,
our study demonstrates the scalability of foundation model-based sparse sensing
and shows its leading performance in 3D scene reconstruction. Collectively, our
study reveals critical aspects of the promises and the open challenges of
integrating foundation models into mobile sparse sensing systems.

</details>


### [20] [Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2511.02228)
*Delin Ma,Menghui Zhou,Jun Qi,Yun Yang,Po Yang*

Main category: cs.CV

TL;DR: A new fusion framework for AD diagnosis using MRI and PET improves performance by balancing shared and specific features and aligning modality distributions.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal neuroimaging fusion approaches focus too much on cross-modal complementarity and neglect the diagnostic value of modality-specific features, while distributional differences between modalities can lead to biased and noisy representations, affecting classification accuracy.

Method: The framework includes a learnable parameter representation (LPR) block to handle missing modality information, shared and modality-independent encoders to preserve both shared and specific features, and a consistency-guided mechanism to align latent distributions across modalities.

Result: Experimental results on the ADNI dataset show that the proposed method outperforms existing fusion strategies in diagnostic performance.

Conclusion: The proposed Collaborative Attention and Consistent-Guided Fusion framework successfully addresses the limitations of existing multimodal neuroimaging fusion methods by integrating both shared and modality-specific features, leading to superior diagnostic performance for Alzheimer's disease.

Abstract: Alzheimer's disease (AD) is the most prevalent form of dementia, and its
early diagnosis is essential for slowing disease progression. Recent studies on
multimodal neuroimaging fusion using MRI and PET have achieved promising
results by integrating multi-scale complementary features. However, most
existing approaches primarily emphasize cross-modal complementarity while
overlooking the diagnostic importance of modality-specific features. In
addition, the inherent distributional differences between modalities often lead
to biased and noisy representations, degrading classification performance. To
address these challenges, we propose a Collaborative Attention and
Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The
proposed model introduces a learnable parameter representation (LPR) block to
compensate for missing modality information, followed by a shared encoder and
modality-independent encoders to preserve both shared and specific
representations. Furthermore, a consistency-guided mechanism is employed to
explicitly align the latent distributions across modalities. Experimental
results on the ADNI dataset demonstrate that our method achieves superior
diagnostic performance compared with existing fusion strategies.

</details>


### [21] [Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization](https://arxiv.org/abs/2511.02329)
*Shaohan Li,Yunpeng Shi,Gilad Lerman*

Main category: cs.CV

TL;DR: Cycle-Sync是一种基于改进MPLS的相机姿态估计框架，通过循环一致性和鲁棒优化实现高精度，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有相机姿态估计方法在鲁棒性和全局一致性方面的不足，作者提出了Cycle-Sync框架，旨在通过循环一致性和鲁棒优化提升估计精度。

Method: 该论文提出了一种基于消息传递最小二乘法（MPLS）的改进方法，通过重新定义循环一致性并引入Welsch型鲁棒损失函数，优化了相机位置估计。此外，还引入了基于鲁棒子空间恢复的离群值剔除模块。

Result: 实验结果表明，Cycle-Sync在合成和真实数据集上均优于现有的先进姿态估计方法，包括带有捆绑调整的完整运动恢复结构流程。

Conclusion: Cycle-Sync框架通过强调循环一致性信息并整合鲁棒损失函数，显著提升了相机姿态估计的准确性和鲁棒性，且在实验中表现优于现有先进方法。

Abstract: We introduce Cycle-Sync, a robust and global framework for estimating camera
poses (both rotations and locations). Our core innovation is a location solver
that adapts message-passing least squares (MPLS) -- originally developed for
group synchronization -- to camera location estimation. We modify MPLS to
emphasize cycle-consistent information, redefine cycle consistencies using
estimated distances from previous iterations, and incorporate a Welsch-type
robust loss. We establish the strongest known deterministic exact-recovery
guarantee for camera location estimation, showing that cycle consistency alone
-- without access to inter-camera distances -- suffices to achieve the lowest
sample complexity currently known. To further enhance robustness, we introduce
a plug-and-play outlier rejection module inspired by robust subspace recovery,
and we fully integrate cycle consistency into MPLS for rotation
synchronization. Our global approach avoids the need for bundle adjustment.
Experiments on synthetic and real datasets show that Cycle-Sync consistently
outperforms leading pose estimators, including full structure-from-motion
pipelines with bundle adjustment.

</details>


### [22] [Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency](https://arxiv.org/abs/2511.02247)
*Hao Li,Daiwei Lu,Jesse d'Almeida,Dilara Isik,Ehsan Khodapanah Aghdam,Nick DiSanto,Ayberk Acar,Susheela Sharma,Jie Ying Wu,Robert J. Webster III,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出一种潜在特征对齐方法，通过对抗学习和特征一致性减少域差距，显著提升内窥镜视频的单目深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实内窥镜图像难以获取绝对深度信息，监督学习方法受限，现有域适应方法在图像风格转换后仍存在域差距。

Method: 通过对抗学习和方向特征一致性，深度网络学习翻译合成图像和真实内窥镜图像的潜在域不变特征。

Result: 在中央气道模型的内窥镜视频评估中，该方法在绝对和相对深度度量上均优于现有技术，且在不同骨干网络和预训练权重下表现一致。

Conclusion: 本文提出了一种潜在特征对齐方法，显著提升了单目深度估计在医学内窥镜视频中的性能，尤其在绝对深度度量上优于现有方法。

Abstract: Monocular depth estimation (MDE) is a critical task to guide autonomous
medical robots. However, obtaining absolute (metric) depth from an endoscopy
camera in surgical scenes is difficult, which limits supervised learning of
depth on real endoscopic images. Current image-level unsupervised domain
adaptation methods translate synthetic images with known depth maps into the
style of real endoscopic frames and train depth networks using these translated
images with their corresponding depth maps. However a domain gap often remains
between real and translated synthetic images. In this paper, we present a
latent feature alignment method to improve absolute depth estimation by
reducing this domain gap in the context of endoscopic videos of the central
airway. Our methods are agnostic to the image translation process and focus on
the depth estimation itself. Specifically, the depth network takes translated
synthetic and real endoscopic frames as input and learns latent
domain-invariant features via adversarial learning and directional feature
consistency. The evaluation is conducted on endoscopic videos of central airway
phantoms with manually aligned absolute depth maps. Compared to
state-of-the-art MDE methods, our approach achieves superior performance on
both absolute and relative depth metrics, and consistently improves results
across various backbones and pretrained weights. Our code is available at
https://github.com/MedICL-VU/MDE.

</details>


### [23] [Synthetic Crop-Weed Image Generation and its Impact on Model Generalization](https://arxiv.org/abs/2511.02417)
*Garen Boyadjian,Cyrille Pierre,Johann Laconte,Riccardo Bertoglio*

Main category: cs.CV

TL;DR: 本文提出了一种生成合成作物-杂草图像的流程，证明合成数据在语义分割任务中具有潜力，尤其在跨域泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 农业除草机器人需要精确的语义分割，但获取大规模标注数据集成本高昂，合成数据可减轻这一负担。

Method: 提出了一个使用Blender生成合成作物-杂草图像的程序化流程，并在合成和真实数据集上对几种最先进的分割模型进行了基准测试。

Result: 在合成图像上训练导致的模拟到真实差距为10%，超越了先前的最先进方法，且在跨域场景中表现出优于真实数据集的泛化性能。

Conclusion: 合成农业数据集具有潜力，支持混合策略以实现更高效的模型训练。

Abstract: Precise semantic segmentation of crops and weeds is necessary for
agricultural weeding robots. However, training deep learning models requires
large annotated datasets, which are costly to obtain in real fields. Synthetic
data can reduce this burden, but the gap between simulated and real images
remains a challenge. In this paper, we present a pipeline for procedural
generation of synthetic crop-weed images using Blender, producing annotated
datasets under diverse conditions of plant growth, weed density, lighting, and
camera angle. We benchmark several state-of-the-art segmentation models on
synthetic and real datasets and analyze their cross-domain generalization. Our
results show that training on synthetic images leads to a sim-to-real gap of
10%, surpassing previous state-of-the-art methods. Moreover, synthetic data
demonstrates good generalization properties, outperforming real datasets in
cross-domain scenarios. These findings highlight the potential of synthetic
agricultural datasets and support hybrid strategies for more efficient model
training.

</details>


### [24] [Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework](https://arxiv.org/abs/2511.02271)
*Yucheng Song,Yifan Ge,Junhao Li,Zhining Liao,Zhifang Liao*

Main category: cs.CV

TL;DR: HTSC-CIF框架通过分层任务分解解决医学报告生成的三大挑战，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学报告生成中领域知识理解不足、文本-视觉实体嵌入对齐差以及跨模态偏差导致的虚假相关性三大挑战。

Method: 提出了一种新颖的分层任务分解方法，包括低级的医学实体特征对齐、中级的跨模态对齐增强以及高级的跨模态因果干预模块。

Result: HTSC-CIF在实验中显著优于现有的医学报告生成方法。

Conclusion: HTSC-CIF框架通过分层任务分解方法有效解决了医学报告生成中的三大挑战，显著提升了性能，并计划公开代码以促进进一步研究。

Abstract: Medical Report Generation (MRG) is a key part of modern medical diagnostics,
as it automatically generates reports from radiological images to reduce
radiologists' burden. However, reliable MRG models for lesion description face
three main challenges: insufficient domain knowledge understanding, poor
text-visual entity embedding alignment, and spurious correlations from
cross-modal biases. Previous work only addresses single challenges, while this
paper tackles all three via a novel hierarchical task decomposition approach,
proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into
low-, mid-, and high-level tasks: 1) Low-level: align medical entity features
with spatial locations to enhance domain knowledge for visual encoders; 2)
Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling
(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a
cross-modal causal intervention module (via front-door intervention) to reduce
confounders and improve interpretability. Extensive experiments confirm
HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)
MRG methods. Code will be made public upon paper acceptance.

</details>


### [25] [From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427)
*Nicolas Schuler,Lea Dewald,Nick Baldig,Jürgen Graf*

Main category: cs.CV

TL;DR: 论文研究了小型视觉语言模型在移动机器人边缘设备上的场景解释和动作识别能力，评估了其在多样化数据集上的表现，并探讨了应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）和视觉语言模型（VLMs）在视频理解、场景解释和常识推理方面取得了显著进展，但其计算复杂性限制了在边缘设备和移动机器人中的应用，尤其是在精度与推理时间之间的权衡。

Method: 研究采用了一个多样化的数据集，包括各种现实世界的城市景观、校园和室内场景，对最先进的VLMs进行了实验评估。

Result: 实验评估讨论了小型模型在边缘设备上的潜力，重点分析了挑战、弱点、固有模型偏差以及所获信息的应用。

Conclusion: 论文探讨了小型视觉语言模型（VLMs）在边缘设备上的应用潜力，特别是在移动机器人场景理解和动作识别任务中的表现，同时指出了模型的挑战、弱点及固有偏差。

Abstract: Video Understanding, Scene Interpretation and Commonsense Reasoning are
highly challenging tasks enabling the interpretation of visual information,
allowing agents to perceive, interact with and make rational decisions in its
environment. Large Language Models (LLMs) and Visual Language Models (VLMs)
have shown remarkable advancements in these areas in recent years, enabling
domain-specific applications as well as zero-shot open vocabulary tasks,
combining multiple domains. However, the required computational complexity
poses challenges for their application on edge devices and in the context of
Mobile Robotics, especially considering the trade-off between accuracy and
inference time. In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics. The proposed pipeline is
evaluated on a diverse dataset consisting of various real-world cityscape,
on-campus and indoor scenarios. The experimental evaluation discusses the
potential of these small models on edge devices, with particular emphasis on
challenges, weaknesses, inherent model biases and the application of the gained
information. Supplementary material is provided via the following repository:
https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/

</details>


### [26] [Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?](https://arxiv.org/abs/2511.02277)
*Giorgos Sfikas,Konstantina Nikolaidou,Foteini Papadopoulou,George Retsinas,Anastasios L. Kesidis*

Main category: cs.CV

TL;DR: 论文研究了欧拉角参数化在归一化流模型中的实用性，发现其在某些情况下优于更复杂的参数化方法。


<details>
  <summary>Details</summary>
Motivation: 由于传感器和投影约束或物体固有对称性，姿态估计可能不明确，概率姿态输出在这种情况下具有优势。

Method: 利用欧拉角参数化构建归一化流模型进行姿态估计。

Result: 研究表明，欧拉角参数化在某些情况下可以构建有用的模型。

Conclusion: 论文探讨了使用欧拉角参数化作为归一化流模型基础的实用性，尽管欧拉角存在一些不足，但在某些方面相比更复杂的参数化可能更有效。

Abstract: Object pose estimation is a task that is of central importance in 3D Computer
Vision. Given a target image and a canonical pose, a single point estimate may
very often be sufficient; however, a probabilistic pose output is related to a
number of benefits when pose is not unambiguous due to sensor and projection
constraints or inherent object symmetries. With this paper, we explore the
usefulness of using the well-known Euler angles parameterisation as a basis for
a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation,
3D pose has been parameterized in a number of ways, either in or out of the
context of parameter estimation. We explore the idea that Euler angles, despite
their shortcomings, may lead to useful models in a number of aspects, compared
to a model built on a more complex parameterisation.

</details>


### [27] [Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems](https://arxiv.org/abs/2511.02507)
*Nicolas Schuler,Lea Dewald,Jürgen Graf*

Main category: cs.CV

TL;DR: 本文提出了一种基于本地模型的自动化报告生成流程，适用于多模态传感器数据，保护隐私且无需外部服务，并在多样化数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，硬件认知系统（如机器人）在动态和非结构化环境中的应用日益广泛。然而，这些系统在关键任务中的应用需要评估大量异构数据，自动化报告生成可以促进评估和接受。

Method: 我们提出了一种基于本地模型的自动化报告生成流程，利用多模态传感器数据，并在多样化的数据集上进行了评估。

Result: 我们在涵盖室内、室外和城市环境的多样化数据集上评估了实现，提供了定量和定性的评估结果。

Conclusion: 本文提出了一种利用多模态传感器生成自动报告的流程，该流程完全依赖本地模型，可在边缘计算设备上部署，从而保护所有参与者的隐私并消除对外部服务的需求。

Abstract: Recent advancements in Deep Learning enable hardware-based cognitive systems,
that is, mechatronic systems in general and robotics in particular with
integrated Artificial Intelligence, to interact with dynamic and unstructured
environments. While the results are impressive, the application of such systems
to critical tasks like autonomous driving as well as service and care robotics
necessitate the evaluation of large amount of heterogeneous data. Automated
report generation for Mobile Robotics can play a crucial role in facilitating
the evaluation and acceptance of such systems in various domains. In this
paper, we propose a pipeline for generating automated reports in natural
language utilizing various multi-modal sensors that solely relies on local
models capable of being deployed on edge computing devices, thus preserving the
privacy of all actors involved and eliminating the need for external services.
In particular, we evaluate our implementation on a diverse dataset spanning
multiple domains including indoor, outdoor and urban environments, providing
quantitative as well as qualitative evaluation results. Various generated
example reports and other supplementary materials are available via a public
repository.

</details>


### [28] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL通过双奖励系统提升多模态大语言模型的推理能力，减少幻觉，性能接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于结果监督（奖励正确答案但忽视推理过程）和统一的思考策略（简单任务过度思考、复杂任务思考不足），SAIL-RL旨在解决这些问题。

Method: 采用强化学习后训练框架，通过Thinking Reward评估推理质量（事实依据、逻辑一致性、答案一致性），Judging Reward自适应决定深度推理或直接回答的适用性。

Result: 在SAIL-VL2上实验表明，SAIL-RL在4B和8B规模上均提升了推理和多模态理解能力，性能接近GPT-4o等商业闭源模型，且大幅减少幻觉。

Conclusion: SAIL-RL通过双奖励系统（Thinking Reward和Judging Reward）提升了多模态大语言模型的推理能力，显著减少了幻觉现象，使其在可靠性和适应性上更具优势。

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [29] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 提出基于GNN的手写数学表达式识别方法，通过符号分割、关系分类和结构优化，显著提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 解决手写数学表达式（HME）识别中的符号分割、识别及空间关系建模问题，提升识别准确性和结构化表达。

Method: 通过将HME建模为图（节点表示符号，边捕获空间依赖关系），使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始图；随后采用2D-CFG解析器生成所有可能的空间关系，并通过GNN链接预测模型优化结构，最终生成符号标签图。

Result: 实验证明该方法在HME结构识别中表现良好，具有较高的准确性和鲁棒性。

Conclusion: 该论文提出了一种基于图神经网络（GNN）的方法，用于手写数学表达式（HME）识别，实验结果表明该方法在HME结构识别中表现优异。

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [30] [GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection](https://arxiv.org/abs/2511.02335)
*Kun Zou,Yongheng Xu,Jianxing Yu,Yan Pan,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出GAFD-CC方法，通过特征解耦和置信度校准实现更有效的OOD检测，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了特征与logit的内在相关性，导致OOD检测效果不佳。

Method: GAFD-CC通过分类权重引导的全局感知特征解耦和自适应融合多尺度logit置信度进行OOD检测。

Result: GAFD-CC在精炼决策边界和提升判别性能方面表现优异。

Conclusion: GAFD-CC在大型基准测试中展现出优于现有方法的性能和强大的泛化能力。

Abstract: Out-of-distribution (OOD) detection is paramount to ensuring the reliability
and robustness of learning models in real-world applications. Existing post-hoc
OOD detection methods detect OOD samples by leveraging their features and
logits information without retraining. However, they often overlook the
inherent correlation between features and logits, which is crucial for
effective OOD detection. To address this limitation, we propose Global-Aware
Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to
refine decision boundaries and increase discriminative performance. Firstly, it
performs global-aware feature decoupling guided by classification weights. This
involves aligning features with the direction of global classification weights
to decouple them. From this, GAFD-CC extracts two types of critical
information: positively correlated features that promote in-distribution
(ID)/OOD boundary refinement and negatively correlated features that suppress
false positives and tighten these boundaries. Secondly, it adaptively fuses
these decoupled features with multi-scale logit-based confidence for
comprehensive and robust OOD detection. Extensive experiments on large-scale
benchmarks demonstrate GAFD-CC's competitive performance and strong
generalization ability compared to those of state-of-the-art methods.

</details>


### [31] [M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings](https://arxiv.org/abs/2511.02349)
*Jiankai Tang,Tao Zhang,Jia Li,Yiru Zhang,Mingyu Zhang,Kegang Wang,Yuming Hao,Bolin Wang,Haiyang Li,Xingyao Wang,Yuanchun Shi,Yuntao Wang,Sichong Qian*

Main category: cs.CV

TL;DR: 研究开发了F3Mamba模型和M3PD数据集，显著提升了智能手机视频光电容积描记技术在心血管疾病监测中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前便携式生理监测方法受限于设备可及性和患者姿态要求，而基于智能手机的视频光电容积描记技术虽便捷但可靠性不足，尤其是在心血管患者中的应用。

Method: 研究提出了F3Mamba模型，通过Mamba-based时间建模融合面部和指尖双视角数据，并利用M3PD数据集（首个公开的双视角移动光电容积描记数据集）进行验证。

Result: F3Mamba模型将心率误差降低了21.9%至30.2%，显著优于现有单视角基线方法，并在真实场景中展现出更强的鲁棒性。

Conclusion: 通过引入M3PD数据集和F3Mamba模型，研究解决了移动光电容积描记技术在心血管疾病监测中的可靠性问题，显著提升了心率监测的准确性。

Abstract: Portable physiological monitoring is essential for early detection and
management of cardiovascular disease, but current methods often require
specialized equipment that limits accessibility or impose impractical postures
that patients cannot maintain. Video-based photoplethysmography on smartphones
offers a convenient noninvasive alternative, yet it still faces reliability
challenges caused by motion artifacts, lighting variations, and single-view
constraints. Few studies have demonstrated reliable application to
cardiovascular patients, and no widely used open datasets exist for
cross-device accuracy. To address these limitations, we introduce the M3PD
dataset, the first publicly available dual-view mobile photoplethysmography
dataset, comprising synchronized facial and fingertip videos captured
simultaneously via front and rear smartphone cameras from 60 participants
(including 47 cardiovascular patients). Building on this dual-view setting, we
further propose F3Mamba, which fuses the facial and fingertip views through
Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to
30.2 percent over existing single-view baselines while improving robustness in
challenging real-world scenarios. Data and code:
https://github.com/Health-HCI-Group/F3Mamba.

</details>


### [32] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa提出连续跨模态推理框架，通过LQ-Former和多任务训练提升视觉语言任务的性能，弥合离散与连续表征的差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）受限于离散语言标记的推理，无法充分表达视觉感知的丰富性。

Method: CoCoVa采用迭代推理循环，通过新颖的Latent Q-Former（LQ-Former）作为动态推理引擎，结合跨模态融合和多任务目标训练（对比学习和扩散重建）。

Result: CoCoVa在1.5B骨干模型上超越更大模型（7B-9B），并在7B规模下保持竞争力，且潜在空间学习到了可解释的结构化推理模式。

Conclusion: CoCoVa框架通过连续跨模态推理，成功弥合了离散语言处理与连续视觉理解之间的表征差距，展现了在视觉语言任务中的潜力。

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [33] [RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](https://arxiv.org/abs/2511.02384)
*Jiahe Song,Chuang Wang,Bowen Jiang,Yinfan Wang,Hao Zheng,Xingjian Wei,Chengjin Liu,Junyuan Gao,Yubin Wang,Lijun Wu,Jiang Wu,Qian Yu,Conghui He*

Main category: cs.CV

TL;DR: RxnCaption框架通过BIVP策略将化学图表解析转化为图像描述问题，利用LVLMs提升提取质量，并构建了大规模数据集RxnCaption-11k。


<details>
  <summary>Details</summary>
Motivation: 现有化学反应数据多为论文中的图像，不可机器读取，无法用于机器学习模型训练。

Method: 将传统的坐标预测驱动解析过程重新定义为图像描述问题，利用LVLMs处理，并引入BIVP策略（通过MolYOLO预绘制分子边界框和索引）。

Result: RxnCaption-VL在多个指标上达到最先进性能，并构建了比现有基准大一个数量级的RxnCaption-11k数据集。

Conclusion: 提出的RxnCaption框架及BIVP策略显著提升了化学文献中结构信息提取的质量，简化了模型设计，并推动了化学领域更广泛的AI应用。

Abstract: Large-scale chemical reaction datasets are crucial for AI research in
chemistry. However, existing chemical reaction data often exist as images
within papers, making them not machine-readable and unusable for training
machine learning models. In response to this challenge, we propose the
RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).
Our framework reformulates the traditional coordinate prediction driven parsing
process into an image captioning problem, which Large Vision-Language Models
(LVLMs) handle naturally. We introduce a strategy termed "BBox and Index as
Visual Prompt" (BIVP), which uses our state-of-the-art molecular detector,
MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the
input image. This turns the downstream parsing into a natural-language
description problem. Extensive experiments show that the BIVP strategy
significantly improves structural extraction quality while simplifying model
design. We further construct the RxnCaption-11k dataset, an order of magnitude
larger than prior real-world literature benchmarks, with a balanced test subset
across four layout archetypes. Experiments demonstrate that RxnCaption-VL
achieves state-of-the-art performance on multiple metrics. We believe our
method, dataset, and models will advance structured information extraction from
chemical literature and catalyze broader AI applications in chemistry. We will
release data, models, and code on GitHub.

</details>


### [34] [Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2511.02395)
*Leon Schwarzer,Matthias Zeller,Daniel Casado Herraez,Simon Dierl,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 自监督预训练结合有限标注数据微调，显著提升了雷达点云移动物体分割的性能。


<details>
  <summary>Details</summary>
Motivation: 雷达点云通常稀疏且噪声大，使得数据标注用于监督学习非常繁琐、耗时且成本高昂。为了解决这一问题，我们致力于自监督的稀疏和噪声雷达点云的移动物体分割。

Method: 采用两步法：对比自监督表示学习和后续的监督微调。我们提出了一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类精炼，以预训练网络生成雷达数据的运动感知表示。

Result: 我们的方法通过自监督预训练和有限的标注数据微调，显著提升了标签效率，有效提升了最先进技术的性能。

Conclusion: 通过自监督预训练和有限的标注数据微调，我们的方法显著提升了标签效率，有效提升了最先进技术的性能。

Abstract: Moving object segmentation is a crucial task for safe and reliable autonomous
mobile systems like self-driving cars, improving the reliability and robustness
of subsequent tasks like SLAM or path planning. While the segmentation of
camera or LiDAR data is widely researched and achieves great results, it often
introduces an increased latency by requiring the accumulation of temporal
sequences to gain the necessary temporal context. Radar sensors overcome this
problem with their ability to provide a direct measurement of a point's Doppler
velocity, which can be exploited for single-scan moving object segmentation.
However, radar point clouds are often sparse and noisy, making data annotation
for use in supervised learning very tedious, time-consuming, and
cost-intensive. To overcome this problem, we address the task of
self-supervised moving object segmentation of sparse and noisy radar point
clouds. We follow a two-step approach of contrastive self-supervised
representation learning with subsequent supervised fine-tuning using limited
amounts of annotated data. We propose a novel clustering-based contrastive loss
function with cluster refinement based on dynamic points removal to pretrain
the network to produce motion-aware representations of the radar data. Our
method improves label efficiency after fine-tuning, effectively boosting
state-of-the-art performance by self-supervised pretraining.

</details>


### [35] [A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds](https://arxiv.org/abs/2511.02397)
*Kuo-Liang Chung,Ting-Chung Tang*

Main category: cs.CV

TL;DR: 本文提出了一种分组混合颜色校正算法，通过自适应分组及不同校正方法（KBI、JKHE、HE）有效提升颜色点云的一致性，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 颜色点云的颜色一致性校正是3D渲染与压缩应用中的基础且重要任务，现有方法多针对彩色图像，本文旨在提出一种适用于颜色点云的分组混合校正算法。

Method: 算法通过估计源点云与目标点云的重叠率，自适应地将目标点分组为近距离组（Gcl）、中距离组（Gmod）或远距离组（Gdist），并使用K近邻双边插值（KBI）、联合KBI与直方图均衡化（JKHE）及直方图均衡化（HE）方法分别对各组进行颜色校正。

Result: 实验结果表明，该算法在1086个测试点云对上表现优异，优于现有方法。

Conclusion: 本文提出的分组混合颜色校正算法在1086个测试点云对上验证了其优越性，优于现有方法。

Abstract: Color consistency correction for color point clouds is a fundamental yet
important task in 3D rendering and compression applications. In the past, most
previous color correction methods aimed at correcting color for color images.
The purpose of this paper is to propose a grouping-based hybrid color
correction algorithm for color point clouds. Our algorithm begins by estimating
the overlapping rate between the aligned source and target point clouds, and
then adaptively partitions the target points into two groups, namely the close
proximity group Gcl and the moderate proximity group Gmod, or three groups,
namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated
overlapping rate is low or high, respectively. To correct color for target
points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method
is proposed. To correct color for target points in Gmod, a joint KBI and the
histogram equalization (JKHE) method is proposed. For target points in Gdist, a
histogram equalization (HE) method is proposed for color correction. Finally,
we discuss the grouping-effect free property and the ablation study in our
algorithm. The desired color consistency correction benefit of our algorithm
has been justified through 1086 testing color point cloud pairs against the
state-of-the-art methods. The C++ source code of our algorithm can be accessed
from the website: https://github.com/ivpml84079/Point-cloud-color-correction.

</details>


### [36] [Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs](https://arxiv.org/abs/2511.02404)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.CV

TL;DR: 研究发现，自监督ViT在猫与人类视觉表征对齐上表现最佳，揭示了模型架构对跨物种视觉计算收敛的影响。


<details>
  <summary>Details</summary>
Motivation: 探索猫与人类在视觉表征上的差异，特别是猫的垂直瞳孔如何影响下游视觉表征，以及不同模型在这些差异上的表现。

Method: 使用统一的冻结编码器基准，量化猫与人类在野外的跨物种表征对齐，涵盖了卷积网络、监督Vision Transformers、窗口Transformer和自监督ViT（DINO），并采用层级的中心核对齐（线性和RBF）和表征相似性分析。

Result: DINO ViT-B/16在所有模型中表现出最高的对齐性（平均CKA-RBF≈0.814，平均CKA-linear≈0.745，平均RSA≈0.698），峰值出现在早期块，表明自监督学习在早期阶段能桥接物种特异性统计。监督ViT在CKA上表现良好，但在几何对应上较弱。CNN表现稳定但低于普通ViT，窗口Transformer表现最差。

Conclusion: 自监督学习与ViT的归纳偏置相结合，产生的表征几何比广泛使用的CNN和窗口Transformer更接近猫和人类的视觉系统对齐，为跨物种视觉计算的收敛提供了可测试的神经科学假设。

Abstract: Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic
cats) have vertically elongated pupils linked to ambush predation; yet, how
such specializations manifest in downstream visual representations remains
incompletely understood. We present a unified, frozen-encoder benchmark that
quantifies feline-human cross-species representational alignment in the wild,
across convolutional networks, supervised Vision Transformers, windowed
transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel
Alignment (linear and RBF) and Representational Similarity Analysis, with
additional distributional and stability tests reported in the paper. Across
models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF
$\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$),
peaking at early blocks, indicating that token-level self-supervision induces
early-stage features that bridge species-specific statistics. Supervised ViTs
are competitive on CKA yet show weaker geometric correspondence than DINO
(e.g., ViT-B/16 RSA $\approx0.53$ at block8; ViT-L/16 $\approx0.47$ at
block14), revealing depth-dependent divergences between similarity and
representational geometry. CNNs remain strong baselines but below plain ViTs on
alignment, and windowed transformers underperform plain ViTs, implicating
architectural inductive biases in cross-species alignment. Results indicate
that self-supervision coupled with ViT inductive biases yields representational
geometries that more closely align feline and human visual systems than widely
used CNNs and windowed Transformers, providing testable neuroscientific
hypotheses about where and how cross-species visual computations converge. We
release our code and dataset for reference and reproducibility.

</details>


### [37] [IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition](https://arxiv.org/abs/2511.02411)
*Wenyang Wei,Yang yang,Xixi Jia,Xiangchu Feng,Weiwei Wang,Renzhen Wang*

Main category: cs.CV

TL;DR: IllumFlow结合CRF和Retinex理论，通过分解图像成分和流增强去噪，显著提升低光图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像中光照变化大和噪声复杂的问题，实现精确的光照适应和噪声去除。

Method: 模型基于Retinex理论分解图像为反射和光照成分，提出条件Rectified Flow框架建模光照变化，并通过流增强的去噪网络处理反射噪声。

Result: 在低光增强和曝光校正任务中，IllumFlow在定量和定性评估上均优于现有方法。

Conclusion: IllumFlow通过结合条件Rectified Flow和Retinex理论，在低光图像增强任务中实现了优异的性能，支持自定义亮度调整，并在实验中超越了现有方法。

Abstract: We present IllumFlow, a novel framework that synergizes conditional Rectified
Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our
model addresses low-light enhancement through separate optimization of
illumination and reflectance components, effectively handling both lighting
variations and noise. Specifically, we first decompose an input image into
reflectance and illumination components following Retinex theory. To model the
wide dynamic range of illumination variations in low-light images, we propose a
conditional rectified flow framework that represents illumination changes as a
continuous flow field. While complex noise primarily resides in the reflectance
component, we introduce a denoising network, enhanced by flow-derived data
augmentation, to remove reflectance noise and chromatic aberration while
preserving color fidelity. IllumFlow enables precise illumination adaptation
across lighting conditions while naturally supporting customizable brightness
enhancement. Extensive experiments on low-light enhancement and exposure
correction demonstrate superior quantitative and qualitative performance over
existing methods.

</details>


### [38] [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](https://arxiv.org/abs/2511.02415)
*Duo Xu,Hao Cheng,Xin Lin,Zhen Xie,Hao Wang*

Main category: cs.CV

TL;DR: 本研究提出自动化多阶段代码驱动流程生成视觉推理数据集ChartM$^3$，显著提升模型推理能力，使小模型在复杂图表理解任务中表现媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 当前研究对复杂图表场景和计算密集型推理任务的覆盖有限，无法满足实际应用需求。本研究旨在通过系统生成视觉推理数据集来解决这些限制。

Method: 本研究提出了一种自动化多阶段代码驱动的流程，结合检索增强生成（RAG）检索专业图表模板，并采用思维链（CoT）策略生成模拟真实数据分布的推理代码，从而驱动图表渲染和问题相关的统计计算。

Result: 构建了ChartM$^3$数据集，包含38K图表和142K问答对用于训练，以及2,871个高质量评估样本。模型评估表明，该流程提升了图表多样性和数据质量。

Conclusion: 通过监督微调（SFT）和强化学习（RL）实验证明，ChartM$^3$数据集显著提升了模型的推理能力和跨领域泛化性能，使较小规模的模型在复杂图表理解任务中达到与大规模模型相当的性能。

Abstract: Complex chart understanding tasks demand advanced visual recognition and
reasoning capabilities from multimodal large language models (MLLMs). However,
current research provides limited coverage of complex chart scenarios and
computation-intensive reasoning tasks prevalent in real-world applications.
This study proposes an automated multi-stage code-driven pipeline for
systematically generating visual reasoning datasets to address these
limitations. The pipeline integrates retrieval-augmented generation (RAG) to
retrieve professional chart templates and employs chain-of-thought (CoT)
strategies to generate reasoning codes that simulate real data distributions,
thereby driving chart rendering and question-related statistical computations.
Through model-based evaluation, the pipeline enhances chart diversity and data
quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and
multi-step dataset containing 38K charts and 142K Q&A pairs for training, along
with 2,871 high-quality evaluation samples for enabling practical performance
assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)
experiments demonstrate that our dataset significantly improves reasoning
capabilities and cross-domain generalization performance, enabling smaller
models to achieve performance comparable to larger-scale models in complex
chart comprehension.

</details>


### [39] [KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image](https://arxiv.org/abs/2511.02462)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: KAO是一种新型扩散模型框架，用于高效修复高分辨率卫星图像，通过优化潜在空间和引入前向后向融合技术，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像修复在遥感领域至关重要，现有方法要么需要大量重新训练，要么计算开销大，KAO旨在解决这些问题。

Method: KAO采用Latent Space Conditioning方法优化紧凑的潜在空间，并结合Explicit Propagation技术促进前向后向融合，以提高方法的稳定性和精确度。

Result: 实验结果表明，KAO在VHR卫星图像修复上设立了新的基准，提供了可扩展且高性能的解决方案。

Conclusion: KAO提出了一种新颖的基于Kernel-Adaptive Optimization的扩散模型框架，为高分辨率卫星图像修复提供了高效且准确的解决方案，平衡了预条件模型和后条件模型的优势。

Abstract: Satellite image inpainting is a crucial task in remote sensing, where
accurately restoring missing or occluded regions is essential for robust image
analysis. In this paper, we propose KAO, a novel framework that utilizes
Kernel-Adaptive Optimization within diffusion models for satellite image
inpainting. KAO is specifically designed to address the challenges posed by
very high-resolution (VHR) satellite datasets, such as DeepGlobe and the
Massachusetts Roads Dataset. Unlike existing methods that rely on
preconditioned models requiring extensive retraining or postconditioned models
with significant computational overhead, KAO introduces a Latent Space
Conditioning approach, optimizing a compact latent space to achieve efficient
and accurate inpainting. Furthermore, we incorporate Explicit Propagation into
the diffusion process, facilitating forward-backward fusion, which improves the
stability and precision of the method. Experimental results demonstrate that
KAO sets a new benchmark for VHR satellite image restoration, providing a
scalable, high-performance solution that balances the efficiency of
preconditioned models with the flexibility of postconditioned models.

</details>


### [40] [MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer](https://arxiv.org/abs/2511.02473)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shotaro Tora*

Main category: cs.CV

TL;DR: MVAFormer是一种针对STAR设置的多视角动作识别方法，通过Transformer模块有效协作视图并保留空间信息，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅适用于从整个视频中识别单个动作，无法应对STAR设置下按顺序识别人物动作的需求。

Method: 提出了一种名为MVAFormer的多视角动作识别方法，采用基于Transformer的协作模块，利用特征图保留空间信息，并将自注意力分为相同视图和不同视图两部分。

Result: 在新收集的数据集上，MVAFormer在F-measure上比基线方法提高了约4.4分。

Conclusion: MVAFormer通过引入基于Transformer的视图协作模块，有效解决了STAR设置下的多视角动作识别问题，性能显著优于现有基线方法。

Abstract: Multi-view action recognition aims to recognize human actions using multiple
camera views and deals with occlusion caused by obstacles or crowds. In this
task, cooperation among views, which generates a joint representation by
combining multiple views, is vital. Previous studies have explored promising
cooperation methods for improving performance. However, since their methods
focus only on the task setting of recognizing a single action from an entire
video, they are not applicable to the recently popular spatio-temporal action
recognition~(STAR) setting, in which each person's action is recognized
sequentially. To address this problem, this paper proposes a multi-view action
recognition method for the STAR setting, called MVAFormer. In MVAFormer, we
introduce a novel transformer-based cooperation module among views. In contrast
to previous studies, which utilize embedding vectors with lost spatial
information, our module utilizes the feature map for effective cooperation in
the STAR setting, which preserves the spatial information. Furthermore, in our
module, we divide the self-attention for the same and different views to model
the relationship between multiple views effectively. The results of experiments
using a newly collected dataset demonstrate that MVAFormer outperforms the
comparison baselines by approximately $4.4$ points on the F-measure.

</details>


### [41] [Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization](https://arxiv.org/abs/2511.02489)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: 提出基于对象检测和图形神经网络的无人机定位框架，解决GNSS受限区域的跨视角图像匹配问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在GNSS受限区域中，传统卫星定位方法失效的问题，特别是跨时间、跨视角和异质航拍图像匹配的挑战。

Method: 利用现代对象检测技术从无人机和卫星图像中准确提取显著实例，并结合图形神经网络分析图像间和图像内的节点关系，通过细粒度的基于图形的节点相似性度量来实现定位。

Result: 在公开和真实数据集上的实验表明，该方法能有效处理异质外观差异，并具有良好的泛化能力，适用于更大模态差距的场景（如红外-可见光图像匹配）。

Conclusion: 该论文提出了一种基于对象检测和图形神经网络的跨视角无人机定位框架，能够在GNSS受限区域有效解决跨时间和跨视角的异质航拍图像匹配问题，并在公开和真实数据集上表现出优异的检索和定位性能。

Abstract: With the rapid growth of the low-altitude economy, UAVs have become crucial
for measurement and tracking in patrol systems. However, in GNSS-denied areas,
satellite-based localization methods are prone to failure. This paper presents
a cross-view UAV localization framework that performs map matching via object
detection, aimed at effectively addressing cross-temporal, cross-view,
heterogeneous aerial image matching. In typical pipelines, UAV visual
localization is formulated as an image-retrieval problem: features are
extracted to build a localization map, and the pose of a query image is
estimated by matching it to a reference database with known poses. Because
publicly available UAV localization datasets are limited, many approaches
recast localization as a classification task and rely on scene labels in these
datasets to ensure accuracy. Other methods seek to reduce cross-domain
differences using polar-coordinate reprojection, perspective transformations,
or generative adversarial networks; however, they can suffer from misalignment,
content loss, and limited realism. In contrast, we leverage modern object
detection to accurately extract salient instances from UAV and satellite
images, and integrate a graph neural network to reason about inter-image and
intra-image node relationships. Using a fine-grained, graph-based
node-similarity metric, our method achieves strong retrieval and localization
performance. Extensive experiments on public and real-world datasets show that
our approach handles heterogeneous appearance differences effectively and
generalizes well, making it applicable to scenarios with larger modality gaps,
such as infrared-visible image matching. Our dataset will be publicly available
at the following URL: https://github.com/liutao23/ODGNNLoc.git.

</details>


### [42] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: DetectiumFire是一个大规模多模态火灾数据集，解决了火灾领域数据不足的问题，支持多种任务研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在火灾领域的应用受限，主要因为缺乏公开可用的高质量火灾标注数据集。

Method: 引入了DetectiumFire数据集，包含22.5k高清火灾图像和2.5k真实火灾视频，覆盖多种火灾类型、环境和风险等级，并标注了传统计算机视觉标签和详细文本提示。

Result: 数据集在目标检测、基于扩散的图像生成和视觉语言推理等任务中验证了其效用，展示了其在火灾研究中的潜力。

Conclusion: DetectiumFire数据集通过提供大规模、高质量的多模态火灾数据，显著推动了火灾相关研究，并为智能安全系统的开发提供了支持。

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [43] [Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](https://arxiv.org/abs/2511.02503)
*Robinson Umeike,Neil Getty,Yin Xiangyu,Yi Jiang*

Main category: cs.CV

TL;DR: PtychoBench compares SFT and ICL for microscopy tasks, finding task-dependent optimal strategies: visual tasks benefit from combined SFT+ICL, while textual tasks favor ICL. Context-aware prompting is key.


<details>
  <summary>Details</summary>
Motivation: To address the unclear optimal domain adaptation strategy for adapting general-purpose foundation models to specialized scientific tasks in advanced microscopy.

Method: Systematic comparison of two specialization strategies (Supervised Fine-Tuning and In-Context Learning) using PtychoBench, a multi-modal, multi-task benchmark for ptychographic analysis.

Result: For visual tasks, SFT and ICL are complementary (best Micro-F1 of 0.728), while for textual tasks, ICL on a large base model is superior (peak Micro-F1 of 0.847). Context-aware prompting is superior, and contextual interference is observed in fine-tuned models.

Conclusion: The optimal specialization pathway for AI models in scientific tasks depends on the task modality, providing a clear framework for developing more effective science-based agentic systems.

Abstract: The automation of workflows in advanced microscopy is a key goal where
foundation models like Language Models (LLMs) and Vision-Language Models (VLMs)
show great potential. However, adapting these general-purpose models for
specialized scientific tasks is critical, and the optimal domain adaptation
strategy is often unclear. To address this, we introduce PtychoBench, a new
multi-modal, multi-task benchmark for ptychographic analysis. Using this
benchmark, we systematically compare two specialization strategies: Supervised
Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies
on a visual artifact detection task with VLMs and a textual parameter
recommendation task with LLMs in a data-scarce regime. Our findings reveal that
the optimal specialization pathway is task-dependent. For the visual task, SFT
and ICL are highly complementary, with a fine-tuned model guided by
context-aware examples achieving the highest mean performance (Micro-F1 of
0.728). Conversely, for the textual task, ICL on a large base model is the
superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a
powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm
the superiority of context-aware prompting and identify a consistent contextual
interference phenomenon in fine-tuned models. These results, benchmarked
against strong baselines including GPT-4o and a DINOv3-based classifier, offer
key observations for AI in science: the optimal specialization path in our
benchmark is dependent on the task modality, offering a clear framework for
developing more effective science-based agentic systems.

</details>


### [44] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: 本文提出一种基于能量的优化方法，通过视觉语义匹配和参考视频风格学习，实现自动化且符合艺术风格的镜头组装，使无经验用户也能制作高质量视频。


<details>
  <summary>Details</summary>
Motivation: 当前智能视频编辑技术虽能处理部分自动化任务，但难以捕捉创作者在镜头组装中的独特艺术表达。本文旨在解决这一问题。

Method: 1. 使用大语言模型生成的脚本与视频库进行视觉语义匹配，获取与脚本语义一致的候选镜头子集。2. 对参考视频的镜头进行分割和标注，提取镜头大小、摄像机运动和语义等属性。3. 利用基于能量的模型学习这些属性，并根据与参考风格的匹配度对候选镜头序列评分。4. 结合多种语法规则优化镜头组装。

Result: 提出的方法能够自动化地根据特定逻辑、叙事需求或艺术风格排列和组合独立镜头，并学习参考视频的组装风格，创建连贯的视觉序列或整体视觉表达。

Conclusion: 该方法通过基于能量的优化方法，结合视觉语义匹配和参考视频风格学习，实现了自动化且符合艺术风格的镜头组装，使得无经验的用户也能制作出视觉上吸引人的视频。

Abstract: Shot assembly is a crucial step in film production and video editing,
involving the sequencing and arrangement of shots to construct a narrative,
convey information, or evoke emotions. Traditionally, this process has been
manually executed by experienced editors. While current intelligent video
editing technologies can handle some automated video editing tasks, they often
fail to capture the creator's unique artistic expression in shot assembly.To
address this challenge, we propose an energy-based optimization method for
video shot assembly. Specifically, we first perform visual-semantic matching
between the script generated by a large language model and a video library to
obtain subsets of candidate shots aligned with the script semantics. Next, we
segment and label the shots from reference videos, extracting attributes such
as shot size, camera motion, and semantics. We then employ energy-based models
to learn from these attributes, scoring candidate shot sequences based on their
alignment with reference styles. Finally, we achieve shot assembly optimization
by combining multiple syntax rules, producing videos that align with the
assembly style of the reference videos. Our method not only automates the
arrangement and combination of independent shots according to specific logic,
narrative requirements, or artistic styles but also learns the assembly style
of reference videos, creating a coherent visual sequence or holistic visual
expression. With our system, even users with no prior video editing experience
can create visually compelling videos. Project page:
https://sobeymil.github.io/esa.com

</details>


### [45] [LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization](https://arxiv.org/abs/2511.02510)
*Jee Won Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: LiteVoxel是一种自调节训练流程，通过改进低频感知和动态修剪，显著降低VRAM使用并保持性能，解决了稀疏体素光栅化的关键问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏体素光栅化在优化场景重建中虽快速且可微，但存在低频内容欠拟合、依赖脆弱修剪启发式方法及VRAM过度增长的问题。LiteVoxel旨在解决这些问题。

Method: LiteVoxel采用自调节训练流程，包括逆Sobel重新加权和中期训练的gamma-ramp提升低频感知，深度分位数修剪逻辑结合EMA-滞后保护和基于射线足迹的优先级驱动细分。

Result: 在Mip-NeRF 360和Tanks & Temples数据集上的实验表明，LiteVoxel有效减少了低频区域和边界不稳定性错误，同时保持性能指标。

Conclusion: LiteVoxel在保持PSNR/SSIM、训练时间和FPS与现有SVRaster管道相当的同时，显著降低了峰值VRAM使用（40%-60%），并保留了低频细节，实现了更可预测、内存高效的训练。

Abstract: Sparse-voxel rasterization is a fast, differentiable alternative for
optimization-based scene reconstruction, but it tends to underfit low-frequency
content, depends on brittle pruning heuristics, and can overgrow in ways that
inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
makes SV rasterization both steadier and lighter. Our loss is made
low-frequency aware via an inverse-Sobel reweighting with a mid-training
gamma-ramp, shifting gradient budget to flat regions only after geometry
stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
logic on maximum blending weight, stabilized by EMA-hysteresis guards and
refines structure through ray-footprint-based, priority-driven subdivision
under an explicit growth budget. Ablations and full-system results across
Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
of errors in low-frequency regions and boundary instability while keeping
PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
detail that prior setups miss, enabling more predictable, memory-efficient
training without sacrificing perceptual quality.

</details>


### [46] [Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data](https://arxiv.org/abs/2511.02541)
*Jessica Plassmann,Nicolas Schuler,Georg von Freymann,Michael Schuth*

Main category: cs.CV

TL;DR: 本研究通过无监督学习方法实现了剪切散斑图像的自动化异常检测，师生模型表现最佳，展示了在工业环境中应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 剪切散斑检测需要专家解释，限制了其工业应用。本研究旨在减少对标注数据和手动评估的依赖，探索无监督学习方法以实现自动化异常检测。

Method: 研究评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型。所有模型仅使用无缺陷数据进行训练。开发了一个可控数据集，包含理想和实际变形条件下的剪切散斑测量。

Result: 结果表明，师生方法在分类鲁棒性和缺陷空间定位方面表现优异，特征表示的可分离性优于自编码器模型。

Conclusion: 本研究强调了无监督深度学习在工业环境中实现可扩展、标签高效的剪切散斑检测的潜力。

Abstract: Shearography is a non-destructive testing method for detecting subsurface
defects, offering high sensitivity and full-field inspection capabilities.
However, its industrial adoption remains limited due to the need for expert
interpretation. To reduce reliance on labeled data and manual evaluation, this
study explores unsupervised learning methods for automated anomaly detection in
shearographic images. Three architectures are evaluated: a fully connected
autoencoder, a convolutional autoencoder, and a student-teacher feature
matching model. All models are trained solely on defect-free data. A controlled
dataset was developed using a custom specimen with reproducible defect
patterns, enabling systematic acquisition of shearographic measurements under
both ideal and realistic deformation conditions. Two training subsets were
defined: one containing only undistorted, defect-free samples, and one
additionally including globally deformed, yet defect-free, data. The latter
simulates practical inspection conditions by incorporating deformation-induced
fringe patterns that may obscure localized anomalies. The models are evaluated
in terms of binary classification and, for the student-teacher model, spatial
defect localization. Results show that the student-teacher approach achieves
superior classification robustness and enables precise localization. Compared
to the autoencoder-based models, it demonstrates improved separability of
feature representations, as visualized through t-SNE embeddings. Additionally,
a YOLOv8 model trained on labeled defect data serves as a reference to
benchmark localization quality. This study underscores the potential of
unsupervised deep learning for scalable, label-efficient shearographic
inspection in industrial environments.

</details>


### [47] [Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction](https://arxiv.org/abs/2511.02558)
*Ali Farki,Elaheh Moradi,Deepika Koundal,Jussi Tohka*

Main category: cs.CV

TL;DR: 本研究使用五种深度学习架构预测未来脑部MRI，展示了高保真度预测和良好的跨队列泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预测未来的脑部状态对研究神经退行性疾病（如阿尔茨海默病）至关重要。现有方法大多预测认知评分或临床结果，而本研究则探索了纵向MRI图像到图像的预测。

Method: 研究实现了五种深度学习架构（UNet、U2-Net、UNETR、Time-Embedding UNet和ODE-UNet），并在两个纵向队列（ADNI和AIBL）上进行了评估。

Result: 表现最佳的模型实现了高保真度的预测，所有模型在独立外部数据集上均表现出良好的泛化能力。

Conclusion: 深度学习模型能够可靠地在体素级别预测参与者特定的脑部MRI，为个性化预后提供了新的机会。

Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI)
is a central challenge in neuroimaging and has important implications for
studying neurodegenerative diseases such as Alzheimer's disease (AD). Most
existing approaches predict future cognitive scores or clinical outcomes, such
as conversion from mild cognitive impairment to dementia. Instead, here we
investigate longitudinal MRI image-to-image prediction that forecasts a
participant's entire brain MRI several years into the future, intrinsically
modeling complex, spatially distributed neurodegenerative patterns. We
implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR,
Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL).
Predicted follow-up MRIs are directly compared with the actual follow-up scans
using metrics that capture global similarity and local differences. The best
performing models achieve high-fidelity predictions, and all models generalize
well to an independent external dataset, demonstrating robust cross-cohort
performance. Our results indicate that deep learning can reliably predict
participant-specific brain MRI at the voxel level, offering new opportunities
for individualized prognosis.

</details>


### [48] [The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic](https://arxiv.org/abs/2511.02563)
*Akash Sharma,Chinmay Mhatre,Sankalp Gawali,Ruthvik Bokkasam,Brij Kishore,Vishwajeet Pattanaik,Tarun Rambha,Abdul R. Pinjari,Vijay Kovvali,Anirban Chakraborty,Punit Rathore,Raghu Krishnapuram,Yogesh Simmhan*

Main category: cs.CV

TL;DR: UVH-26是首个针对印度交通场景的大规模标注数据集，通过众包标注和先进算法生成共识标注，训练模型性能显著优于通用数据集。


<details>
  <summary>Details</summary>
Motivation: 解决全球基准测试中缺乏针对印度复杂交通场景的特定领域训练数据的问题。

Method: 通过众包黑客马拉松标注交通摄像头图像，使用多数投票和STAPLE算法生成共识标注，并训练多种当代检测器（如YOLO11-S/X、RT-DETR-S/X和DAMO-YOLO-T/L）。

Result: 在UVH-26上训练的模型在mAP50:95指标上比COCO数据集训练的基线模型提高了8.4-31.5%，其中RT-DETR-X表现最佳（mAP50:95为0.67）。

Conclusion: UVH-26数据集填补了全球基准测试中针对印度复杂交通条件的空白，为智能交通系统的检测、分类和部署提供了基础。

Abstract: This report describes the UVH-26 dataset, the first public release by
AIM@IISc of a large-scale dataset of annotated traffic-camera images from
India. The dataset comprises 26,646 high-resolution (1080p) images sampled from
2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently
annotated through a crowdsourced hackathon involving 565 college students from
across India. In total, 1.8 million bounding boxes were labeled across 14
vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler
(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,
Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k
consensus ground truth bounding boxes and labels were derived for distinct
objects in the 26k images using Majority Voting and STAPLE algorithms. Further,
we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,
and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,
mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in
mAP50:95 over equivalent baseline models trained on COCO dataset, with
RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40
for COCO-trained weights for common classes (Car, Bus, and Truck). This
demonstrates the benefits of domain-specific training data for Indian traffic
scenarios. The release package provides the 26k images with consensus
annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the
6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the
heterogeneity of Indian urban mobility directly from operational traffic-camera
streams, UVH-26 addresses a critical gap in existing global benchmarks, and
offers a foundation for advancing detection, classification, and deployment of
intelligent transportation systems in emerging nations with complex traffic
conditions.

</details>


### [49] [Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification](https://arxiv.org/abs/2511.02564)
*Md Rashidunnabi,Kailash A. Hambarde,Vasco Lopes,Joao C. Neves,Hugo Proenca*

Main category: cs.CV

TL;DR: MTF-CVReID通过七个高效模块提升跨视角行人再识别的鲁棒性和时间一致性，性能优越且计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决跨视角域（如空中-地面监控）中视频行人再识别（ReID）存在的极端视角变化、尺度差异和时间不一致性问题。

Method: MTF-CVReID是一个参数高效的框架，基于ViT-B/16骨干网络，引入了七个互补模块：CSFN、MRFH、IAMM、TDM、IVFA、HTPL和MVICL。

Result: 在AG-VPReID基准测试中实现了最先进的性能，并展示了在G2A-VReID和MARS数据集上的强跨数据集泛化能力，同时保持实时效率（189 FPS）。

Conclusion: MTF-CVReID框架通过精心设计的适配器模块显著提升了跨视角鲁棒性和时间一致性，同时保持了计算效率。

Abstract: Video-based person re-identification (ReID) in cross-view domains (for
example, aerial-ground surveillance) remains an open problem because of extreme
viewpoint shifts, scale disparities, and temporal inconsistencies. To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone. Specifically,
we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and
view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale
stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to
reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for
motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment
(IVFA) for perspective-invariant representation alignment; (6) Hierarchical
Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;
and (7) Multi-View Identity Consistency Learning (MVICL) that enforces
cross-view identity coherence using a contrastive learning paradigm. Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets. These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency. The source code is available at
https://github.com/MdRashidunnabi/MTF-CVReID

</details>


### [50] [A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding](https://arxiv.org/abs/2511.02565)
*Jingyu Lu,Haonan Wang,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: VCFlow是一种新型分层解码框架，通过模拟人类视觉系统结构和特征级对比学习，实现快速且高效的主题无关脑解码，适用于临床。


<details>
  <summary>Details</summary>
Motivation: 探索无需特定主题训练即可从fMRI重建连续视觉体验的主题无关脑解码，因其在临床应用中的巨大潜力而备受关注。

Method: 提出了一种新颖的分层解码框架VCFlow，明确模拟人类视觉系统的腹侧-背侧结构，以学习多维表示，并通过特征级对比学习策略增强主题不变语义表示的提取。

Result: VCFlow 在跨主题泛化和处理复杂脑信号方面表现出色，相比传统方法，显著减少了数据处理时间和计算资源需求。

Conclusion: VCFlow 提供了一种快速且临床可扩展的解决方案，能够在无需重新训练的情况下，仅牺牲7%的平均准确率，即可在10秒内生成重建视频。

Abstract: Subject-agnostic brain decoding, which aims to reconstruct continuous visual
experiences from fMRI without subject-specific training, holds great potential
for clinical applications. However, this direction remains underexplored due to
challenges in cross-subject generalization and the complex nature of brain
signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a
novel hierarchical decoding framework that explicitly models the ventral-dorsal
architecture of the human visual system to learn multi-dimensional
representations. By disentangling and leveraging features from early visual
cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary
cognitive information essential for visual reconstruction. Furthermore, we
introduce a feature-level contrastive learning strategy to enhance the
extraction of subject-invariant semantic representations, thereby enhancing
subject-agnostic applicability to previously unseen subjects. Unlike
conventional pipelines that need more than 12 hours of per-subject data and
heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates
each reconstructed video in 10 seconds without any retraining, offering a fast
and clinically scalable solution. The source code will be released upon
acceptance of the paper.

</details>


### [51] [Zero-Shot Multi-Animal Tracking in the Wild](https://arxiv.org/abs/2511.02591)
*Jan Frederik Meier,Timo Lüddecke*

Main category: cs.CV

TL;DR: 本研究提出了一种零样本多动物追踪框架，结合Grounding Dino和SAM 2模型，无需重新训练即可在新数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多动物追踪对理解动物生态和行为至关重要，但传统方法需要针对每个应用场景进行大量模型微调和启发式设计，限制了其通用性。

Method: 结合Grounding Dino对象检测器和Segment Anything Model 2（SAM 2）追踪器，并设计启发式方法，构建零样本多动物追踪框架。

Result: 在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估表明，该框架在多种物种和环境中表现强劲且一致。

Conclusion: 通过结合Grounding Dino和SAM 2模型，以及精心设计的启发式方法，本研究开发了一个无需重新训练或超参数调整即可应用于新数据集的零样本多动物追踪框架，展现了跨物种和环境的强大一致性性能。

Abstract: Multi-animal tracking is crucial for understanding animal ecology and
behavior. However, it remains a challenging task due to variations in habitat,
motion patterns, and species appearance. Traditional approaches typically
require extensive model fine-tuning and heuristic design for each application
scenario. In this work, we explore the potential of recent vision foundation
models for zero-shot multi-animal tracking. By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation. Evaluations on
ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate
strong and consistent performance across diverse species and environments. The
code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.

</details>


### [52] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: UniChange是一种基于多模态大型语言模型（MLLM）的统一变化检测模型，首次将BCD和SCD任务统一，并通过特殊标记和文本提示实现高性能。


<details>
  <summary>Details</summary>
Motivation: 当前变化检测模型因局限于单一类型标注数据而泛化能力差，无法同时利用多样化的BCD和SCD数据集。

Method: 利用MLLM的语言先验和统一能力，引入三个特殊标记（[T1]、[T2]、[CHANGE]）和文本提示，统一BCD和SCD任务。

Result: 在四个公共基准测试（WHU-CD、S2Looking、LEVIR-CD+、SECOND）上实现SOTA性能，IoU分数分别为90.41、53.04、78.87和57.62。

Conclusion: UniChange通过MLLM的统一框架，显著提升了变化检测的泛化能力和性能，为多源数据集学习提供了新思路。

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [53] [Robust Face Liveness Detection for Biometric Authentication using Single Image](https://arxiv.org/abs/2511.02645)
*Poulami Raha,Yeongnam Chae*

Main category: cs.CV

TL;DR: 论文提出一种轻量CNN框架，用于快速检测面部识别系统中的多种欺骗攻击，并展示了一个新的2D欺骗攻击数据集。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统易受呈现攻击的威胁，需要一种轻量且高效的解决方案来增强其安全性。

Method: 采用新颖的轻量级CNN框架，能够识别打印/显示、视频和伪装攻击。

Result: 提出的架构在CPU上实现了1-2秒的快速生物认证，并创建了一个包含60个受试者500多个视频的2D欺骗攻击数据集。

Conclusion: 该论文提出的轻量级CNN框架在面部识别系统中有效识别多种欺骗攻击，提高了生物认证的安全性和速度。

Abstract: Biometric technologies are widely adopted in security, legal, and financial
systems. Face recognition can authenticate a person based on the unique facial
features such as shape and texture. However, recent works have demonstrated the
vulnerability of Face Recognition Systems (FRS) towards presentation attacks.
Using spoofing (aka.,presentation attacks), a malicious actor can get
illegitimate access to secure systems. This paper proposes a novel light-weight
CNN framework to identify print/display, video and wrap attacks. The proposed
robust architecture provides seamless liveness detection ensuring faster
biometric authentication (1-2 seconds on CPU). Further, this also presents a
newly created 2D spoof attack dataset consisting of more than 500 videos
collected from 60 subjects. To validate the effectiveness of this architecture,
we provide a demonstration video depicting print/display, video and wrap attack
detection approaches. The demo can be viewed in the following link:
https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa

</details>


### [54] [Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models](https://arxiv.org/abs/2511.02650)
*Tianfan Peng,Yuntao Du,Pengzhou Ji,Shijie Dong,Kailin Jiang,Mingchuan Ma,Yijun Tian,Jinhe Bi,Qian Li,Wei Du,Feng Xiao,Lizhen Cui*

Main category: cs.CV

TL;DR: UniPruneBench是一个统一的多模态LLMs视觉令牌修剪基准，实验发现随机修剪是强基线，且修剪比例是关键因素。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）由于图像编码器引入的大量视觉令牌而存在严重的推理效率问题，现有的令牌压缩方法评估零散且不一致。

Method: 提出了UniPruneBench，一个统一且可扩展的基准，用于多模态LLMs中的视觉令牌修剪，涵盖六个能力维度和十个数据集，评估了十种代表性压缩算法和三个LMM家族。

Result: 实验揭示了几个关键发现：(1) 随机修剪是一个出人意料的强基线，(2) 没有单一方法在所有场景中始终优于其他方法，(3) 修剪敏感性在不同任务间差异显著，OCR最脆弱，(4) 修剪比例是影响性能下降的主导因素。

Conclusion: UniPruneBench提供了一个可靠的基础，支持未来高效多模态建模的研究。

Abstract: Large multimodal models (LMMs) often suffer from severe inference
inefficiency due to the large number of visual tokens introduced by image
encoders. While recent token compression methods, such as pruning and merging,
have shown promise in reducing redundancy, their evaluation remains fragmented
and inconsistent. In this work, we present UniPruneBench, a unified and
extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
provides standardized protocols across six ability dimensions and ten datasets,
covering ten representative compression algorithms and three families of LMMs
(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
system-level metrics such as runtime and prefilling latency to provide a
holistic view. Our experiments uncover several key findings: (1) random pruning
is a surprisingly strong baseline, (2) no single method consistently
outperforms others across scenarios, (3) pruning sensitivity varies
significantly across tasks, with OCR being most vulnerable, and (4) pruning
ratio is the dominant factor governing performance degradation. We believe
UniPruneBench will serve as a reliable foundation for future research on
efficient multimodal modeling.

</details>


### [55] [Differentiable Hierarchical Visual Tokenization](https://arxiv.org/abs/2511.02652)
*Marius Aasan,Martine Hjelkrem-Tan,Nico Catalano,Changkyu Choi,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出一种自适应图像内容的可微分分词器，兼容现有模型，性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer使用固定patch token，忽略了图像的空间和语义结构。

Method: 采用分层模型选择和信息准则，开发了一种端到端的可微分分词器。

Result: 在图像级分类和密集预测任务中表现优异，并支持开箱即用的栅格到矢量转换。

Conclusion: 该方法通过引入可微分分词器，实现了对图像内容的像素级自适应，同时保持与现有架构的兼容性，支持图像分类和密集预测任务。

Abstract: Vision Transformers rely on fixed patch tokens that ignore the spatial and
semantic structure of images. In this work, we introduce an end-to-end
differentiable tokenizer that adapts to image content with pixel-level
granularity while remaining backward-compatible with existing architectures for
retrofitting pretrained models. Our method uses hierarchical model selection
with information criteria to provide competitive performance in both
image-level classification and dense-prediction tasks, and even supports
out-of-the-box raster-to-vector conversion.

</details>


### [56] [LLEXICORP: End-user Explainability of Convolutional Neural Networks](https://arxiv.org/abs/2511.02720)
*Vojtěch Kůr,Adam Bajger,Adam Kukučka,Marek Hradil,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LLEXICORP通过结合CRP与大型语言模型，自动生成自然语言解释，降低了解释深度神经网络的难度。


<details>
  <summary>Details</summary>
Motivation: 现有CRP流程多为手动，需专家检查激活图像命名概念并合成冗长解释，限制了解释的可访问性和可扩展性。

Method: 引入了LLEXICORP，一个模块化流程，将CRP与多模态大型语言模型结合，自动为概念原型分配描述性名称并生成自然语言解释。

Result: 在ImageNet的VGG16模型上定性评估表明，该方法能有效生成针对不同受众的定制化解释。

Conclusion: 整合基于概念的归因方法与大型语言模型可以显著降低解释深度神经网络的障碍，为更透明的AI系统铺平道路。

Abstract: Convolutional neural networks (CNNs) underpin many modern computer vision
systems. With applications ranging from common to critical areas, a need to
explain and understand the model and its decisions (XAI) emerged. Prior works
suggest that in the top layers of CNNs, the individual channels can be
attributed to classifying human-understandable concepts. Concept relevance
propagation (CRP) methods can backtrack predictions to these channels and find
images that most activate these channels. However, current CRP workflows are
largely manual: experts must inspect activation images to name the discovered
concepts and must synthesize verbose explanations from relevance maps, limiting
the accessibility of the explanations and their scalability.
  To address these issues, we introduce Large Language model EXplaIns COncept
Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a
multimodal large language model. Our approach automatically assigns descriptive
names to concept prototypes and generates natural-language explanations that
translate quantitative relevance distributions into intuitive narratives. To
ensure faithfulness, we craft prompts that teach the language model the
semantics of CRP through examples and enforce a separation between naming and
explanation tasks. The resulting text can be tailored to different audiences,
offering low-level technical descriptions for experts and high-level summaries
for non-technical stakeholders.
  We qualitatively evaluate our method on various images from ImageNet on a
VGG16 model. Our findings suggest that integrating concept-based attribution
methods with large language models can significantly lower the barrier to
interpreting deep neural networks, paving the way for more transparent AI
systems.

</details>


### [57] [Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.02685)
*Chao Yuan,Zanwu Liu,Guiwei Zhang,Haoxuan Xu,Yujian Zhao,Guanglin Niu,Bo Li*

Main category: cs.CV

TL;DR: 提出MTRL框架，通过模态转换表示学习提升VI-ReID性能，无额外参数，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖中间表示对齐跨模态特征，但存在参数多、解释性差、未充分利用中间特征的问题。

Method: 使用中间生成图像作为可见到红外模态的转换器，结合模态转换对比损失和模态查询正则化损失进行训练。

Result: 在三个典型VI-ReID数据集上表现显著优于现有方法，且推理速度与骨干网络相同。

Conclusion: 提出的MTRL框架通过模态转换表示学习，无需额外参数即可显著提升VI-ReID任务的性能，且在三个典型数据集上表现优于现有方法。

Abstract: Visible-infrared person re-identification (VI-ReID) technique could associate
the pedestrian images across visible and infrared modalities in the practical
scenarios of background illumination changes. However, a substantial gap
inherently exists between these two modalities. Besides, existing methods
primarily rely on intermediate representations to align cross-modal features of
the same person. The intermediate feature representations are usually create by
generating intermediate images (kind of data enhancement), or fusing
intermediate features (more parameters, lack of interpretability), and they do
not make good use of the intermediate features. Thus, we propose a novel
VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a
middle generated image as a transmitter from visible to infrared modals, which
are fully aligned with the original visible images and similar to the infrared
modality. After that, using a modality-transition contrastive loss and a
modality-query regularization loss for training, which could align the
cross-modal features more effectively. Notably, our proposed framework does not
need any additional parameters, which achieves the same inference speed to the
backbone while improving its performance on VI-ReID task. Extensive
experimental results illustrate that our model significantly and consistently
outperforms existing SOTAs on three typical VI-ReID datasets.

</details>


### [58] [VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models](https://arxiv.org/abs/2511.02712)
*Zhicheng Zhang,Weicheng Wang,Yongjie Zhu,Wenyu Qin,Pengfei Wan,Di Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种情感线索引导的推理框架和VidEmo模型，通过两阶段调优和情感树强化学习，显著提升了视频情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 情感具有动态性和线索依赖性，这使得理解和预测复杂且不断变化的情感状态具有挑战性。

Method: 采用两阶段调优过程：课程情感学习注入情感知识，随后通过情感树强化学习进行情感推理。

Result: 实验结果表明，该方法在15项面部感知任务中取得了具有竞争力的性能。

Conclusion: 本文提出的情感线索引导推理框架和VidEmo模型在视频情感分析任务中表现出色，为15项面部感知任务设立了新的里程碑。

Abstract: Understanding and predicting emotion from videos has gathered significant
attention in recent studies, driven by advancements in video large language
models (VideoLLMs). While advanced methods have made progress in video emotion
analysis, the intrinsic nature of emotions poses significant challenges.
Emotions are characterized by dynamic and cues-dependent properties, making it
difficult to understand complex and evolving emotional states with reasonable
rationale. To tackle these challenges, we propose a novel affective cues-guided
reasoning framework that unifies fundamental attribute perception, expression
analysis, and high-level emotional understanding in a stage-wise manner. At the
core of our approach is a family of video emotion foundation models (VidEmo),
specifically designed for emotion reasoning and instruction-following. These
models undergo a two-stage tuning process: first, curriculum emotion learning
for injecting emotion knowledge, followed by affective-tree reinforcement
learning for emotion reasoning. Moreover, we establish a foundational data
infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)
consisting of 2.1M diverse instruction-based samples. Emo-CFG includes
explainable emotional question-answering, fine-grained captions, and associated
rationales, providing essential resources for advancing emotion understanding
tasks. Experimental results demonstrate that our approach achieves competitive
performance, setting a new milestone across 15 face perception tasks.

</details>


### [59] [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)
*Tyler Zhu,Tengda Han,Leonidas Guibas,Viorica Pătrăucean,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 首次全面研究视频-文本表征对齐，发现其对编码器表征能力的零样本探测价值，并提出测试时间缩放定律和语义对齐与下游任务的相关性。


<details>
  <summary>Details</summary>
Motivation: 探索视频数据在跨模态对齐中的潜力，尤其是现代视频和语言编码器的能力。

Method: 提出了参数化的测试时间缩放定律，并研究了语义对齐与下游任务性能之间的相关性。

Result: 发现跨模态对齐高度依赖测试时提供的视觉和文本数据的丰富性，并展示了语义对齐与通用视频表征理解之间的潜在联系。

Conclusion: 视频与文本表征对齐为零样本探测不同编码器对时空数据的表征能力提供了信息丰富的方式。

Abstract: The alignment of representations from different modalities has recently been
shown to provide insights on the structural similarities and downstream
capabilities of different encoders across diverse data types. While significant
progress has been made in aligning images with text, the temporal nature of
video data remains largely unexplored in this context. In this work, we conduct
the first comprehensive study of video-text representation alignment, probing
the capabilities of modern video and language encoders. Our findings reveal
several key insights. First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs. multi-frame videos)
and text (single caption vs. a collection) data provided at test time,
especially when using state-of-the-art video encoders. We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations. Secondly, we investigate the
correlation between semantic alignment and performance on both semantic and
non-semantic downstream tasks, providing initial evidence that strong alignment
against text encoders may be linked to general-purpose video representation and
understanding. Finally, we correlate temporal reasoning with cross-modal
alignment providing a challenging test-bed for vision and language models.
Overall, our work introduces video-text alignment as an informative zero-shot
way to probe the representation power of different encoders for spatio-temporal
data. Project page can be found at https://video-prh.github.io/

</details>


### [60] [PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing](https://arxiv.org/abs/2511.02777)
*Antonio Oroz,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: PercHead 是一种用于单图像3D头部重建和语义编辑的统一模型，通过双分支编码器和ViT解码器实现，支持直观的交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 解决单图像3D头部重建和语义3D编辑的挑战，包括视角遮挡、弱感知监督和3D空间编辑的模糊性。

Method: 模型采用双分支编码器和基于ViT的解码器，通过迭代交叉注意力将2D特征提升至3D空间，并使用高斯泼溅进行渲染。感知监督策略基于DINOv2和SAM2.1。

Result: 模型在新视角合成中达到最先进性能，并在极端视角下表现出卓越的鲁棒性。基础模型可通过编码器交换和网络微调无缝扩展为语义3D编辑。

Conclusion: PercHead 提出了一种统一的基础模型，用于从单张图像重建3D头部，并支持语义3D编辑，展示了在新视角合成和极端视角下的鲁棒性。

Abstract: We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE

</details>


### [61] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: VCode 是一个将多模态理解任务重新定义为 SVG 代码生成的基准测试，VCoder 框架通过迭代修订和视觉工具提升了视觉为中心的编码任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在语言为中心的编码任务（如程序合成和调试），而忽视了视觉为中心的编码。受人类通过草图推理的启发，研究者倡导使用 SVG 代码作为一种紧凑、可解释且可执行的视觉表示方法。

Method: 研究提出了 VCode 基准测试，将多模态理解任务重新定义为代码生成任务，并引入了 CodeVQA 协议来评估 SVG 代码的符号保真度。VCoder 框架通过“Thinking with Revision”和“Acting with Visual Tools”两种策略增强 VLMs 的能力。

Result: 前沿的 VLMs 在生成准确的 SVG 代码方面表现不佳，但 VCoder 框架显著提升了性能，总体得分比表现最佳的 Claude-4-Opus 高出 12.3 分。人类研究显示，人类和 VLMs 在渲染的 SVG 上表现一致，表明符号视觉表示的潜力。

Conclusion: VCode 和 VCoder 框架展示了符号视觉表示的潜力，尤其是在视觉为中心的编码任务中。尽管前沿的视觉语言模型（VLMs）在生成准确的 SVG 代码方面存在挑战，但 VCoder 通过迭代修订和视觉工具的使用显著提升了性能。

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>


### [62] [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://arxiv.org/abs/2511.02779)
*Yiyang Zhou,Haoqin Tu,Zijun Wang,Zeyu Wang,Niklas Muennighoff,Fan Nie,Yejin Choi,James Zou,Chaorui Deng,Shen Yan,Haoqi Fan,Cihang Xie,Huaxiu Yao,Qinghao Ye*

Main category: cs.CV

TL;DR: MIRA是一个新基准，要求模型生成中间视觉图像以辅助推理，实验显示视觉线索显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法仅依赖文本，而MIRA旨在评估模型在需要生成和利用中间视觉图像（如草图、结构图）以指导推理过程的场景中的表现。

Method: 提出MIRA基准测试，包括546个多模态问题，并设计了统一的评估协议，涵盖三种输入级别（直接输入、文本CoT输入、Visual-CoT输入）。

Result: 现有多模态大语言模型在仅依赖文本提示时表现不佳，但提供中间视觉线索后性能平均提升33.7%。

Conclusion: 实验结果强调了在MIRA基准测试中，想象的视觉信息对于成功推理的关键作用。

Abstract: We propose MIRA, a new benchmark designed to evaluate models in scenarios
where generating intermediate visual images is essential for successful
reasoning. Unlike traditional CoT methods that rely solely on text, tasks in
MIRA require models to generate and utilize intermediate images - such as
sketches, structural diagrams, or path drawings - to guide their reasoning
process. This setup closely mirrors how humans solve complex problems through
"drawing to think". To solve this, MIRA focuses on tasks that are intrinsically
challenging and involve complex structures, spatial relationships, or reasoning
steps that are difficult to express through language alone. To ensure that our
evaluation data is of high-quality, we include 546 multimodal problems,
annotated with intermediate visual images and final answers. We also propose a
unified evaluation protocol for MIRA that spans three levels of evaluation
input: direct input with image and question only, text-only CoT input with
image and thinking prompts, and Visual-CoT input with both annotated image
clues and textual thinking prompts. To probe the upper bound of model capacity
on our benchmark, we also report pass@k and majority voting accuracies under
different k settings. Experimental results show that existing multimodal large
language models, including strongest private models as well as strong
open-weight models, perform poorly when relying solely on textual prompts.
However, when intermediate visual cues are provided, model performance improves
consistently, yielding an average relative gain of 33.7% across all models and
tasks. We also probe the upper bound by expanding the search space and
designing textual prompts aligned with Visual-CoT, but both yield only limited
improvements compared to our Visual-CoT setting. These results underscore the
critical role of imagined visual information in enabling successful reasoning
on MIRA.

</details>


### [63] [AI-Generated Image Detection: An Empirical Study and Future Research Directions](https://arxiv.org/abs/2511.02791)
*Nusrat Tasnim,Kutub Uddin,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 本文提出了一个统一的基准测试框架，系统评估了十种法医方法，揭示了其在泛化性和可解释性上的不足，为开发更鲁棒的解决方案提供了指导。


<details>
  <summary>Details</summary>
Motivation: AI生成的媒体（尤其是深度伪造）对多媒体取证、错误信息检测和生物识别系统构成重大威胁，导致公众对法律体系的信任度下降、欺诈行为增加和社会工程攻击频发。现有法医方法存在基准不统一、训练协议不一致和评估指标有限等问题，亟需系统性评估框架。

Method: 引入了一个统一的基准测试框架，对十种最先进的法医方法（包括从头训练、冻结和微调）和七个公开数据集（GAN和扩散生成）进行了广泛而系统的评估，使用了多种指标（如准确率、平均精度、ROC-AUC等）和解释性分析工具（如置信曲线和Grad-CAM热力图）。

Result: 评估结果显示，不同方法在泛化性上存在显著差异，某些方法在分布内表现良好但跨模型迁移能力较差。

Conclusion: 本研究提出了一个统一的基准测试框架，旨在系统评估法医方法在受控和可重复条件下的表现，揭示了当前方法在泛化性和可解释性方面的局限性，并呼吁开发更鲁棒、可泛化和可解释的解决方案。

Abstract: The threats posed by AI-generated media, particularly deepfakes, are now
raising significant challenges for multimedia forensics, misinformation
detection, and biometric system resulting in erosion of public trust in the
legal system, significant increase in frauds, and social engineering attacks.
Although several forensic methods have been proposed, they suffer from three
critical gaps: (i) use of non-standardized benchmarks with GAN- or
diffusion-generated images, (ii) inconsistent training protocols (e.g.,
scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail
to capture generalization and explainability. These limitations hinder fair
comparison, obscure true robustness, and restrict deployment in
security-critical applications. This paper introduces a unified benchmarking
framework for systematic evaluation of forensic methods under controlled and
reproducible conditions. We benchmark ten SoTA forensic methods (scratch,
frozen, and fine-tuned) and seven publicly available datasets (GAN and
diffusion) to perform extensive and systematic evaluations. We evaluate
performance using multiple metrics, including accuracy, average precision,
ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model
interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations
demonstrate substantial variability in generalization, with certain methods
exhibiting strong in-distribution performance but degraded cross-model
transferability. This study aims to guide the research community toward a
deeper understanding of the strengths and limitations of current forensic
approaches, and to inspire the development of more robust, generalizable, and
explainable solutions.

</details>


### [64] [PLUTO-4: Frontier Pathology Foundation Models](https://arxiv.org/abs/2511.02826)
*Harshith Padigela,Shima Nofallah,Atchuth Naveen Chilaparasetti,Ryun Han,Andrew Walker,Judy Shen,Chintan Shah,Blake Martin,Aashish Sood,Elliot Miller,Ben Glass,Andy Beck,Harsha Pokkalla,Syed Ashar Javed*

Main category: cs.CV

TL;DR: PLUTO-4是新一代病理学基础模型，包含高效紧凑的PLUTO-4S和前沿规模的PLUTO-4G，通过自监督训练在大规模病理图像库上，显著提升了多种病理学任务的性能，尤其在皮肤病理学诊断中提升11%。


<details>
  <summary>Details</summary>
Motivation: 基于大尺度病理图像库训练的基础模型在多种组织病理学任务中展现出强大的迁移能力，推动了PLUTO-4的开发，旨在进一步扩展病理学通用模型的规模和能力。

Method: PLUTO-4家族包含两种互补的Vision Transformer架构：高效紧凑的PLUTO-4S和前沿规模的PLUTO-4G。PLUTO-4S采用FlexiViT设置和2D-RoPE嵌入优化多尺度部署，PLUTO-4G则通过单一补丁大小训练以最大化表示能力和稳定性。两者均基于DINOv2的自监督目标在大规模多机构病理图像库上进行预训练。

Result: PLUTO-4在公开和内部基准测试中均表现出色，在需要不同空间和生物学上下文的任务（如补丁级分类、分割和幻灯片级诊断）中达到最先进水平。PLUTO-4S提供高吞吐量和稳健性能，而PLUTO-4G在多个病理学基准测试中创造了新的性能标杆。

Conclusion: PLUTO-4展现了作为转化研究和诊断用例骨干的潜力，尤其是在皮肤病理学诊断中提升了11%的性能。

Abstract: Foundation models trained on large-scale pathology image corpora have
demonstrated strong transfer capabilities across diverse histopathology tasks.
Building on this progress, we introduce PLUTO-4, our next generation of
pathology foundation models that extend the Pathology-Universal Transformer
(PLUTO) to frontier scale. We share two complementary Vision Transformer
architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model
optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE
embeddings, and a frontier-scale PLUTO-4G model trained with a single patch
size to maximize representation capacity and stability. Both models are
pretrained using a self-supervised objective derived from DINOv2 on a large
multi-institutional corpus containing 551,164 WSIs from 137,144 patients across
over 50 institutions, spanning over 60 disease types and over 100 stains.
Comprehensive evaluation across public and internal benchmarks demonstrates
that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying
spatial and biological context, including patch-level classification,
segmentation, and slide-level diagnosis. The compact PLUTO-4S provides
high-throughput and robust performance for practical deployment, while PLUTO-4G
establishes new performance frontiers across multiple pathology benchmarks,
including an 11% improvement in dermatopathology diagnosis. These diverse
improvements underscore PLUTO-4's potential to transform real-world
applications as a backbone for translational research and diagnostic use cases.

</details>


### [65] [Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks](https://arxiv.org/abs/2511.02830)
*Dmitrii Pozdeev,Alexey Artemov,Ananta R. Bhattarai,Artem Sevastopolsky*

Main category: cs.CV

TL;DR: DenseMarks 是一种基于 Vision Transformer 的头部图像密集对应表示方法，通过对比损失和多任务学习实现高鲁棒性和广泛覆盖，适用于语义匹配和跟踪任务。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种高质量的人类头部密集对应表示，解决现有方法在姿态变化和头部覆盖范围（如头发）上的局限性。

Method: 使用 Vision Transformer 网络预测每个像素的 3D 嵌入（对应规范单位立方体中的位置），并通过对比损失、多任务学习（面部标志和分割约束）以及潜在立方体特征的空间连续性来训练网络。

Result: DenseMarks 在几何感知点匹配和单目头部跟踪任务中表现出色，且能覆盖整个头部（包括头发），对姿态变化具有鲁棒性。

Conclusion: DenseMarks 提供了一种可解释且可查询的规范空间表示，适用于语义部分匹配、面部/头部跟踪和立体重建。该方法对姿态变化具有鲁棒性，并覆盖整个头部（包括头发），且在几何感知点匹配和单目头部跟踪方面达到了最先进的性能。

Abstract: We propose DenseMarks - a new learned representation for human heads,
enabling high-quality dense correspondences of human head images. For a 2D
image of a human head, a Vision Transformer network predicts a 3D embedding for
each pixel, which corresponds to a location in a 3D canonical unit cube. In
order to train our network, we collect a dataset of pairwise point matches,
estimated by a state-of-the-art point tracker over a collection of diverse
in-the-wild talking heads videos, and guide the mapping via a contrastive loss,
encouraging matched points to have close embeddings. We further employ
multi-task learning with face landmarks and segmentation constraints, as well
as imposing spatial continuity of embeddings through latent cube features,
which results in an interpretable and queryable canonical space. The
representation can be used for finding common semantic parts, face/head
tracking, and stereo reconstruction. Due to the strong supervision, our method
is robust to pose variations and covers the entire head, including hair.
Additionally, the canonical space bottleneck makes sure the obtained
representations are consistent across diverse poses and individuals. We
demonstrate state-of-the-art results in geometry-aware point matching and
monocular head tracking with 3D Morphable Models. The code and the model
checkpoint will be made available to the public.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Mirror-Neuron Patterns in AI Alignment](https://arxiv.org/abs/2511.01885)
*Robyn Wyrick*

Main category: cs.AI

TL;DR: 研究探讨了人工神经网络能否发展出类似生物镜像神经元的模式，并评估这些模式如何促进AI系统的伦理和合作决策。通过实验发现，适当调整模型可以促进共享神经表征，支持合作行为，为AI对齐提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）向超人类能力迈进，将这些系统与人类价值观对齐变得越来越关键。当前的对齐策略主要依赖于外部指定的约束，这些约束可能不足以应对未来能够规避自上而下控制的超级智能AI。

Method: 研究使用了一种新颖的Frog and Toad游戏框架来促进合作行为，识别了镜像神经元模式出现的条件，评估了其对动作电路的影响，引入了Checkpoint Mirror Neuron Index（CMNI）来量化激活强度和一致性，并提出了进一步研究的理论框架。

Result: 研究发现，人工神经网络（ANNs）可以发展出类似生物镜像神经元的模式，这些模式在共情、模仿和社会认知中起关键作用。这些共情类电路支持合作行为，为AI系统的伦理和合作决策提供了新的视角。

Conclusion: 研究发现，适当调整模型容量和自我/他人耦合可以促进人工神经网络中类似生物镜像神经元的共享神经表征。这些类似共情的电路支持合作行为，表明通过镜像神经元动态建模的内在动机可以补充现有的对齐技术，直接将类似共情的机制嵌入AI架构中。

Abstract: As artificial intelligence (AI) advances toward superhuman capabilities,
aligning these systems with human values becomes increasingly critical. Current
alignment strategies rely largely on externally specified constraints that may
prove insufficient against future super-intelligent AI capable of circumventing
top-down controls.
  This research investigates whether artificial neural networks (ANNs) can
develop patterns analogous to biological mirror neurons cells that activate
both when performing and observing actions, and how such patterns might
contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in
empathy, imitation, and social cognition in humans. The study therefore asks:
(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these
patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative
behaviors, we identify conditions under which mirror-neuron patterns emerge,
evaluate their influence on action circuits, introduce the Checkpoint Mirror
Neuron Index (CMNI) to quantify activation strength and consistency, and
propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and
self/other coupling foster shared neural representations in ANNs similar to
biological mirror neurons. These empathy-like circuits support cooperative
behavior and suggest that intrinsic motivations modeled through mirror-neuron
dynamics could complement existing alignment techniques by embedding
empathy-like mechanisms directly within AI architectures.

</details>


### [67] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 论文提出人机共融智能系统APEX，结合AI推理与人类执行，提升实验和制造的自主性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习与自动化模型局限于虚拟领域，而现实世界的实验与制造仍需人类监督，导致可重复性、可扩展性和可访问性受限。

Method: 通过混合现实技术结合智能代理推理与物理执行，开发了APEX系统，实现实时观察、3D视觉引导和逐步分析。

Result: APEX系统在洁净室中实现了超过通用多模态大语言模型的上下文感知推理能力，实时纠错，并将专业知识传递给新手。

Conclusion: 该论文提出了人机共融智能的新范式，将人类、智能代理和可穿戴硬件整合为一个系统，显著提升了科学实验和制造的自主性、可追溯性、可解释性和可扩展性。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [68] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 利用基础模型（LLM和VLM）和人类反馈自动化设计强化学习奖励函数，生成能与GT Sophy竞争的赛车代理，并展示新颖行为。


<details>
  <summary>Details</summary>
Motivation: 设计强化学习代理时，将期望行为映射到奖励函数是一个复杂过程，尤其在复杂环境（如自动驾驶赛车）中更具挑战性。

Method: 通过结合基于LLM的奖励生成、基于VLM偏好的评估和人类反馈，系统在Gran Turismo 7赛车游戏中搜索奖励函数空间。

Result: 系统生成的赛车代理能够与冠军级RL赛车代理GT Sophy竞争，并能产生新颖行为。

Conclusion: 本文展示了如何利用当前的基础模型（如LLM和VLM）结合人类反馈，自动化设计强化学习代理的奖励函数，为实际应用中的自动化奖励设计铺平了道路。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [69] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: DVB框架测试LLMs是否学习人类深层价值观，发现模型普遍更依赖表面特征，泛化深层价值观的能力低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 区分LLMs是否真正学习到人类深层价值观对AI对齐至关重要，仅捕捉表面偏好可能导致行为失准。

Method: 通过训练阶段人为关联深层价值观（如道德原则）与表面特征（如语言风格），测试阶段打破这种关联，设计实验测量模型的深层价值观泛化率（DVGR）。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型的深层价值观泛化能力均低于随机水平，且模型规模越大DVGR略低。

Conclusion: DVB框架为衡量大型语言模型（LLMs）是否学习到人类深层价值观提供了可解释的指标，结果显示当前模型的DVGR普遍低于随机水平，表明其更倾向于捕捉表面特征而非深层价值观。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [70] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 研究提出 InsurAgent，一个结合 LLM 和 RAG 的代理，用于准确模拟洪水保险决策行为，弥补了 LLMs 在定量概率估计上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管洪水保险是减轻灾害损失的有效策略，但美国高风险人群的参与率仍然很低，这凸显了理解和模拟保险决策行为机制的必要性。

Method: 研究构建了一个基准数据集来评估 LLM 的能力，并提出了 InsurAgent，一个由五个模块（感知、检索、推理、行动和记忆）组成的 LLM 赋能代理。检索模块利用 RAG 技术基于实证调查数据做出决策，推理模块利用 LLM 常识推断调查数据之外的信息，记忆模块支持模拟时间决策演变。

Result: LLMs 在定性理解因素方面表现良好，但在定量概率估计方面存在不足。InsurAgent 通过 RAG 和 LLM 常识的结合，能够准确估计边际和双变量概率，并捕捉传统模型难以处理的上下文信息。

Conclusion: InsurAgent 为行为建模和政策分析提供了一个有价值的工具，通过结合 LLM 的常识和检索增强生成技术，能够准确估计概率并捕捉传统模型难以处理的上下文信息。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [71] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC 是一种自适应奖励预测方法，通过动态控制推理长度和计算成本，显著提升推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 提高推理模型的效率和准确性，同时减少不必要的计算开销。

Method: Re-FORC 在推理模型上训练轻量级适配器，根据未来思考令牌数量预测预期奖励。

Result: Re-FORC 实现了：1) 减少26%计算量同时保持准确性；2) 在相同计算量下提高4%准确性，或在相同准确性下减少55%计算量；3) 自适应测试时扩展，在高计算和低计算场景下分别提高11%和7%准确性。

Conclusion: Re-FORC 是一种自适应奖励预测方法，通过动态控制推理长度和计算成本，显著提高了推理模型的效率和准确性。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [72] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: ATHENA框架通过两阶段方法（符号效用发现和语义适应）提升个体决策模型的准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测之间存在差距，主要源于个体决策过程的独特性（如成本和语言影响）。ATHENA旨在解决最优信息整合问题。

Method: ATHENA采用两阶段方法：首先通过LLM增强的符号发现找到群体级别的符号效用函数；其次进行个体级别的语义适应，创建个性化的语义模板以建模个性化选择。

Result: 在真实世界的旅行方式和疫苗选择任务中，ATHENA持续优于基于效用、机器学习和其他基于LLM的模型，F1分数至少提升6.5%。消融研究证实两阶段缺一不可。

Conclusion: ATHENA通过有机整合符号化效用建模和语义适应，为人类中心决策建模提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [73] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: STRMAC是一种状态感知路由框架，通过自适应智能体选择和自我进化数据生成，显著提升多智能体系统的协作效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统在复杂任务解决中存在调度僵化和协调效率低下的问题，限制了其潜力发挥。

Method: 提出STRMAC框架，分别编码交互历史和智能体知识以驱动路由器，自适应选择最合适的智能体，并引入自我进化数据生成方法加速高质量执行路径的收集。

Result: 在协作推理基准测试中，STRMAC比基线方法性能提升23.8%，数据收集开销减少90.1%。

Conclusion: STRMAC框架通过状态感知路由和自我进化数据生成方法，显著提升了多智能体系统的协作效率，实现了在复杂任务中的最先进性能。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [74] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: PPP方法通过优化生产力、主动性和个性化，显著提升AI代理在交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注任务成功，但实际应用中，AI代理需优化生产力、主动性和个性化三个维度以提升用户体验。

Method: 引入UserVille交互环境和PPP多目标强化学习方法，联合优化生产力、主动性和个性化三个维度。

Result: 实验表明，经PPP训练的代理在软件工程和深度研究任务中表现优于GPT-5等基线模型（平均提升21.6分），能提出战略性澄清问题、适应未知用户偏好并通过更好交互提高任务成功率。

Conclusion: 本研究证明，明确优化以用户为中心的交互对于构建实用且有效的AI代理至关重要。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [75] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出 \method 框架，通过分解查询、清理表格和生成可执行代码，显著提升 LLM 在复杂表格推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂查询、噪声数据和有限数值能力下的表现不佳问题。

Method: 框架包含查询分解器、表格清理器和基于程序思维的推理器。

Result: 在多个数据集上取得显著准确率提升（TAT-QA 8.79%，TableBench 6.08%，\method 19.87%）。

Conclusion: \method 框架显著提升了大型语言模型在复杂表格数值推理中的性能，实现了最先进的准确率。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [76] [Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network](https://arxiv.org/abs/2511.02238)
*Keyu Zhao,Weiquan Lin,Qirui Zheng,Fengli Xu,Yong Li*

Main category: cs.AI

TL;DR: 提出Deep Ideation框架，通过科学网络增强LLM生成的研究创意，质量提升10.67%，实用性强。


<details>
  <summary>Details</summary>
Motivation: 现有研究创意生成方法过于依赖简单统计关联或LLM内部知识，忽视科学概念间的复杂上下文关系，限制了创意的质量和实用性。

Method: 提出Deep Ideation框架，结合科学概念网络（关键词共现和上下文关系）与LLM驱动的创意生成，采用探索-扩展-演进的迭代流程，并引入基于真实审稿反馈训练的批评引擎。

Result: 实验表明，该方法生成的研究创意质量提升10.67%，超越顶级会议接受水平，人类评估和消融研究验证了其有效性和组件必要性。

Conclusion: Deep Ideation框架通过整合科学概念网络和LLM驱动的创意生成，显著提高了研究创意的质量，实验结果显示其优于现有方法，并具有实际科研价值。

Abstract: Novel research ideas play a critical role in advancing scientific inquiries.
Recent advancements in Large Language Models (LLMs) have demonstrated their
potential to generate novel research ideas by leveraging large-scale scientific
literature. However, previous work in research ideation has primarily relied on
simplistic methods, such as keyword co-occurrence or semantic similarity. These
approaches focus on identifying statistical associations in the literature but
overlook the complex, contextual relationships between scientific concepts,
which are essential to effectively leverage knowledge embedded in human
literature. For instance, papers that simultaneously mention "keyword A" and
"keyword B" often present research ideas that integrate both concepts.
Additionally, some LLM-driven methods propose and refine research ideas using
the model's internal knowledge, but they fail to effectively utilize the
scientific concept network, limiting the grounding of ideas in established
research. To address these challenges, we propose the Deep Ideation framework
to address these challenges, integrating a scientific network that captures
keyword co-occurrence and contextual relationships, enriching LLM-driven
ideation. The framework introduces an explore-expand-evolve workflow to
iteratively refine research ideas, using an Idea Stack to track progress. A
critic engine, trained on real-world reviewer feedback, guides the process by
providing continuous feedback on the novelty and feasibility of ideas. Our
experiments show that our approach improves the quality of generated ideas by
10.67% compared to other methods, with ideas surpassing top conference
acceptance levels. Human evaluation highlights their practical value in
scientific research, and ablation studies confirm the effectiveness of each
component in the workflow. Code repo is available at
https://github.com/kyZhao-1/Deep-Ideation.

</details>


### [77] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本研究提出了一个框架，将多模态大语言模型的模态跟随行为分解为相对不确定性和内在偏好，揭示了其决策机制，并提供了量化分析工具。


<details>
  <summary>Details</summary>
Motivation: 先前的研究仅通过粗粒度的数据集级统计来测量模态跟随行为，忽略了模型在单模态推理中的置信度影响。本研究旨在更深入地理解MLLMs在模态冲突时的决策机制。

Method: 通过构建可控数据集，系统性地调整视觉和文本输入的推理难度，并使用熵作为细粒度的不确定性度量，分析了模态跟随行为的两个核心因素：相对推理不确定性和内在模态偏好。

Result: 研究发现，模态跟随的概率随着其相对不确定性的增加而单调递减；在平衡点附近，模型会在不同层之间摇摆于模态之间，解释了外部观察到的犹豫现象。

Conclusion: 本研究通过引入相对不确定性分析和内在模态偏好的框架，揭示了多模态大语言模型（MLLMs）在解决模态冲突时的内在机制，为理解MLLMs如何处理矛盾信息提供了定量框架和机理洞察。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [78] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 论文分析了多智能体推理中的懒惰行为问题，提出了一种可验证奖励机制，实验证明该方法有效提升了协作效果。


<details>
  <summary>Details</summary>
Motivation: 研究发现多智能体设置中存在懒惰行为问题，即一个智能体主导而另一个贡献很少，这削弱了协作并导致设置退化为无效的单智能体。

Method: 论文首先进行了理论分析，解释了多智能体推理中懒惰行为的自然产生原因，然后引入了一种稳定且高效的因果影响测量方法。最后，提出了一种可验证的奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验证明，该框架有效缓解了懒惰智能体行为，提升了多智能体在复杂推理任务中的表现。

Conclusion: 该论文提出了一种可验证的奖励机制，有效缓解了多智能体推理中的懒惰行为，并释放了多智能体框架在复杂推理任务中的潜力。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [79] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: ProQ-BERT是一种基于transformer的框架，用于预测CKD进展，整合多模态EHR数据，表现优异，为个性化治疗提供新思路。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病（CKD）影响全球近10%人口，准确预测其进展对及时干预和资源优化至关重要。

Method: 提出了一种名为ProQ-BERT的框架，整合了多模态电子健康记录（EHR），采用量化标记化和注意力机制提高可解释性，并通过掩码语言建模预训练和二元分类微调进行模型优化。

Result: 在91,816名患者的队列中，ProQ-BERT在短期预测中的ROC-AUC达到0.995，PR-AUC达到0.989，表现优于CEHR-BERT。

Conclusion: 该研究展示了基于transformer的框架在慢性肾病（CKD）预后预测中的有效性，为个性化治疗提供了新方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [80] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 研究开发了一种基于模糊软集理论的非侵入性乳腺癌风险评估系统，利用常规血液分析参数，帮助早期识别高风险患者。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因之一，早期诊断对有效治疗和提高生存率至关重要，但由于疾病的复杂性和患者风险因素的多样性，及时检测仍面临挑战。

Method: 研究采用模糊推理规则和软集计算，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄等临床和生理参数来估计乳腺癌风险。

Result: 该系统基于UCI机器学习仓库的数据集进行开发和验证，旨在帮助医疗专业人员识别高风险患者并决定是否需要进行进一步的诊断程序。

Conclusion: 该研究提出了一种基于模糊软集理论的专家系统，用于评估患者的乳腺癌风险，为医疗专业人员提供了一种非侵入性且易于获取的初步评估工具。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [81] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出基于二分类的新框架估计生成模型的完整PR曲线，统计分析并提出估计风险上界，实验验证不同设置下曲线行为。


<details>
  <summary>Details</summary>
Motivation: 生成模型在图像和文本领域的成功引发了对其评估方法的关注，现有方法多依赖标量指标，而PR曲线的引入为研究开辟了新途径。

Method: 基于二分类视角的新框架，进行彻底的统计分析，并提出PR估计风险的极小极大上界。

Result: 提出的框架能够估计完整PR曲线，扩展了文献中仅关注曲线极端值的里程碑PR指标，并通过实验验证了不同设置下曲线的行为差异。

Conclusion: 本文提出了一个基于二分类视角的新框架，用于估计生成模型的完整PR曲线，并通过实验研究了不同设置下曲线的行为。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [82] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: ReAcTree通过分层代理树和双重记忆系统，显著提升LLM在复杂任务中的规划能力，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂长时程任务时表现不佳，主要因其依赖单一轨迹且无法有效分解任务。

Method: 提出了ReAcTree方法，通过将复杂目标分解为可管理的子目标，并在动态构建的代理树中由LLM代理节点处理，同时结合控制流节点和双重记忆系统（情景记忆和工作记忆）优化执行策略。

Result: 在WAH-NL和ALFRED数据集上，ReAcTree表现优于基线方法（如ReAct），尤其在WAH-NL上使用Qwen 2.5 72B时，成功率从31%提升至61%。

Conclusion: ReAcTree通过分层任务规划和动态构建代理树的方法，显著提升了复杂长时程任务的执行成功率，尤其在WAH-NL数据集上表现突出。

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [83] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 论文提出VMR方法，将开放式任务转化为可验证的多选形式，增强推理能力，在8个基准测试中平均提升5.99分。


<details>
  <summary>Details</summary>
Motivation: 开放式任务（如创意写作）缺乏标准答案，现有研究忽视其推理潜力，RLVR依赖标准答案无法直接应用。

Method: 引入VMR策略，将开放式数据重构为可验证的多选格式，解决无明确答案时的训练问题。

Result: 在多个基准测试中验证有效，平均提升5.99分。

Conclusion: VMR方法成功将RLVR范式扩展到开放式任务，显著提升性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [84] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero,Luis A. Hernández Gómez,Luis Mendo Tomás,Zoraida Frias Barroso*

Main category: cs.AI

TL;DR: 该论文探讨了Agentic AI在5G/6G网络中的应用，提出核心概念和设计模式，并通过RAN优化案例展示了其实际潜力。


<details>
  <summary>Details</summary>
Motivation: 5G和即将到来的6G网络的复杂性使得手动优化变得无效，因此需要Agentic AI作为动态RAN环境中自动化决策的方法。然而，目前缺乏一个明确的框架来定义Agentic AI系统的基本组件和操作原则。

Method: 论文首先介绍了Agentic AI的演变历程，讨论了从经典代理到Agentic AI的进展。随后描述了核心设计模式（如反思、规划、工具使用和多代理协作），并将这些理论概念应用于移动网络环境，特别是RAN管理和优化。最后通过一个5G RAN案例研究展示了LAM驱动的代理如何与时间序列分析协作实现基于KPI的自主决策。

Result: 论文提出了Agentic AI的核心概念和设计模式，并通过实际用例验证了其在RAN优化中的可行性。

Conclusion: 该论文提出了Agentic AI在5G和6G网络中的核心概念，并通过一个实际的RAN优化用例展示了其应用潜力，为未来的研究奠定了基础。

Abstract: Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [85] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: KLPEG框架通过知识图谱和LLMs，提升游戏更新测试的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代游戏快速迭代更新对测试效率和特异性提出挑战，现有基于LLMs的自动化测试方法缺乏知识积累机制。

Method: 提出KLPEG框架，结合知识图谱（KG）建模游戏元素、任务依赖和因果关系，并利用LLMs解析更新日志，通过多跳推理生成定制化测试用例。

Result: 在Overcooked和Minecraft两个游戏环境中，KLPEG能更准确定位受更新影响的功能并以更少步骤完成测试。

Conclusion: KLPEG框架通过构建知识图谱和利用LLMs，显著提高了游戏迭代更新测试的准确性和效率。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [86] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA Benchmark测试显示顶尖大语言模型在定量推理任务中准确率仅45-63%，错误主要为舍入和计算错误，模型在不同领域表现互补。


<details>
  <summary>Details</summary>
Motivation: 开发ORCA Benchmark的目的是评估大型语言模型在真实多领域定量推理任务中的表现，特别是在逐步推理、数值精度和领域泛化方面的能力。

Method: ORCA Benchmark通过500个跨领域自然语言任务（如金融、物理、健康、统计）评估大型语言模型的定量推理能力，使用Omni计算引擎的验证输出作为标准。

Result: 五大先进系统（ChatGPT-5、Gemini~2.5~Flash、Claude~Sonnet~4.5、Grok~4和DeepSeek~V3.2）的准确率仅为45-63%，错误主要集中在舍入（35%）和计算错误（33%）。模型在数学和工程领域表现较强，但在物理和自然科学中较弱，且模型间存在部分互补性（r≈0.40-0.65）。

Conclusion: ORCA Benchmark揭示了当前顶尖大语言模型在多领域定量推理任务中的局限性，尤其在精度和计算错误方面存在明显不足，同时也展示了模型在不同领域表现的部分互补性。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [87] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了一种基于GR(1)的自适应屏蔽框架，通过运行时修复规范，解决了静态屏蔽无法适应环境变化的问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 静态屏蔽方法在环境假设被违反时无法适应，限制了其在实际应用中的灵活性。

Method: 基于GR(1)规范，结合运行时环境假设违规检测和归纳逻辑编程(ILP)在线修复规范。

Result: 自适应屏蔽框架在Minepump和Atari Seaquest案例中，相比静态屏蔽，保持了近乎最优的奖励和完美的逻辑合规性。

Conclusion: 本文提出了首个基于GR(1)规范的自适应屏蔽框架，能够在线修复规范，确保屏蔽的优雅演进，并在两个案例研究中验证了其优越性。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [88] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu,Jiarui Tong,Sheng Xu*

Main category: cs.AI

TL;DR: 提出基于心理学理论的多智能体系统，模拟人类行为以增强教育和培训的真实性。


<details>
  <summary>Details</summary>
Motivation: 解决现有模拟人类行为的局限性，提供更真实且理论透明的训练工具。

Method: 基于多智能体架构，结合心理学理论（如自我效能、思维模式、社会建构主义）模拟‘内部议会’的决策过程。

Result: 开发了一个可应用于教师培训和研究的多智能体心理模拟系统，支持社会学习、认知学徒制等原则。

Conclusion: 该系统通过模拟人类心理过程，为教育和培训提供了高度真实的行为模拟，同时保持了理论透明度和心理学一致性。

Abstract: Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [89] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析LLMs组合空间推理能力的大型基准数据集和生成框架，可独立变化组合性的多个方面，验证了LLMs在生产力和系统性泛化方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 分析组合空间推理能力，独立变化组合性的多个方面（如生产力、替代性、过度泛化和系统性）。

Method: DecompSR通过程序化构建，确保其构造的正确性，并利用符号求解器独立验证数据集的正确性。

Result: LLMs在空间推理任务中表现出对生产力和系统性泛化的困难，但对语言变异更具鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严格的基准测试数据集，能够独立变化组合性的多个关键方面，从而实现对LLMs组合推理能力的稳健和细粒度探测。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [90] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 论文提出协作迷宫解决基准，发现模型单独表现良好但在协作时表现下降，提出'接力推理'方法改善协作效果。


<details>
  <summary>Details</summary>
Motivation: AI发展轨迹表明我们将越来越依赖由独立开发的代理人组成的系统，这些系统成功的关键在于异构代理人之间的有效协作，即使在部分可观测性下。

Method: 提出了一个协作迷宫解决基准，评估了32种领先的开源和闭源模型在单独、同质和异质配对中的表现。

Result: 研究结果揭示了'协作差距'：单独表现良好的模型在需要协作时往往表现大幅下降。'接力推理'方法可以缩小这一差距。

Conclusion: 论文呼吁（1）协作意识评估，（2）开发增强协作能力的训练策略，（3）可靠激发代理人潜在技能的交互设计，这些指导适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [91] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: CostBench是一个新型基准测试，用于评估LLM代理在成本最优规划和实时适应能力上的表现，发现当前代理在这些方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLM）代理的评估主要关注任务完成度，忽视了资源效率和适应性。为了填补这一空白，作者设计了CostBench来评估代理在成本最优规划和实时适应能力上的表现。

Method: 提出了CostBench，一个可扩展的、以成本为中心的基准测试，用于评估代理的经济推理和重新规划能力。该基准测试包含可通过多种原子和复合工具序列解决的任务，并支持四种动态阻塞事件以模拟现实世界的不确定性。

Result: 评估显示，即使是GPT-5在静态环境下也很难识别成本最优解（准确率低于75%），在动态条件下性能进一步下降约40%。

Conclusion: CostBench通过诊断当前LLM代理在成本优化规划和实时适应能力上的不足，为未来开发更具经济合理性和鲁棒性的代理奠定了基础。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [92] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出span queries以泛化推理服务器接口，支持多种非聊天用例，显著提升KV缓存和注意力局部性，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有推理服务器主要优化聊天补全任务，无法有效支持新兴的非聊天用例（如推理时间缩放和深度推理技术），导致KV缓存命中率低下。

Method: 提出span queries作为推理调用的表达式树，并通过添加可交换性约束来链接这些调用。描述了span query的语法和语义，并展示了如何自动优化以提高KV缓存局部性。对vLLM进行了少量修改以支持高性能的span query执行。

Result: span queries在两种不同的非聊天用例中实现了10-20倍的TTFT降低，并且在注意力优化后，2b参数模型的性能优于未优化的8b模型。

Conclusion: 本文通过引入span queries，成功泛化了推理服务器的接口，支持多种非聊天用例，显著提升了KV缓存利用率和注意力局部性，实现了性能的大幅提升。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [93] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的半自动化方法，用于生成结合人类可读性与机器可解释性的形式化知识表示，并展示了其在控制工程领域的应用。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域研究输出的快速增长需要新的方法来结构化和形式化领域知识。

Method: 基于PyIRK框架，利用语言模型将自然语言描述和数学定义（LaTeX源代码）转化为形式化知识图谱。

Result: 作为首次应用，提出了生成'交互式语义层'以增强源文档，促进知识传递。

Conclusion: 这为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [94] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 该论文提出一种轻量级框架，通过将模态视为代理并聚合其输出来诊断多模态模型中的模态破坏现象，揭示了模型融合动态并为干预提供依据。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型发展迅速，但其推理过程仍不透明。论文旨在揭示模态间的动态关系，特别是高置信度单模态错误如何覆盖其他证据并误导融合结果。

Method: 论文引入了模态破坏的诊断失败模式，并提出一个评估层，将每个模态视为代理，生成候选标签和简短的自我评估用于审计。通过简单的融合机制聚合这些输出，识别贡献者和破坏者。

Result: 在情感识别基准测试中，应用该诊断层揭示了系统性的可靠性特征，帮助区分失败源于数据集伪影还是模型限制。

Conclusion: 该论文提出了一个轻量级、模型无关的评估层，用于分析多模态大语言模型中的模态破坏现象，为多模态推理提供诊断框架，支持对融合动态的原则性审计并指导可能的干预措施。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [95] [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.AI

TL;DR: Orion-MSP 通过多尺度处理和块稀疏注意力等创新，提升了表格上下文学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格上下文学习架构存在单尺度特征处理、密集注意力导致的二次缩放问题以及严格顺序组件处理等局限性，影响了性能和扩展性。

Method: Orion-MSP 引入了多尺度处理、块稀疏注意力和感知器风格的内存，以解决现有架构的局限性。

Result: Orion-MSP 在多样化的基准测试中匹配或超越了现有最佳性能，并能有效扩展到高维表格。

Conclusion: Orion-MSP 通过多尺度处理、块稀疏注意力和感知器风格的内存，显著提升了表格数据的上下文学习性能，并在高维表格上展现了良好的扩展性。

Abstract: Tabular data remain the predominant format for real-world applications. Yet,
developing effective neural models for tabular data remains challenging due to
heterogeneous feature types and complex interactions occurring at multiple
scales. Recent advances in tabular in-context learning (ICL), such as TabPFN
and TabICL, have achieved state-of-the-art performance comparable to
gradient-boosted trees (GBTs) without task-specific fine-tuning. However,
current architectures exhibit key limitations: (1) single-scale feature
processing that overlooks hierarchical dependencies, (2) dense attention with
quadratic scaling in table width, and (3) strictly sequential component
processing that prevents iterative representation refinement and
cross-component communication. To address these challenges, we introduce
Orion-MSP, a tabular ICL architecture featuring three key innovations: (1)
multi-scale processing to capture hierarchical feature interactions; (2)
block-sparse attention combining windowed, global, and random patterns for
scalable efficiency and long-range connectivity; and (3) a Perceiver-style
memory enabling safe bidirectional information flow across components. Across
diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance
while scaling effectively to high-dimensional tables, establishing a new
standard for efficient tabular in-context learning. The model is publicly
available at https://github.com/Lexsi-Labs/Orion-MSP .

</details>


### [96] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 研究通过分解攻击技能和概率模型优化，在数据有限环境下显著提升AI攻击策略效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署的复杂性和风险增加，亟需有效评估其风险。现有AI控制框架需强攻击策略，但在复杂代理环境中因数据匮乏而困难。

Method: 在SHADE-Arena数据集中分解攻击能力为五个技能（怀疑建模、攻击选择、计划合成、执行和隐蔽性），并分别优化。通过概率模型模拟攻击动态优化超参数，并将结果迁移至实际环境。

Result: 优化后的攻击策略显著提升攻击强度，安全评分从基线0.87降至0.41。

Conclusion: 通过分解攻击能力为五个核心技能并分别优化，结合概率模型克服数据限制，该方法显著提升了攻击策略的效果，将安全评分从基线0.87降至0.41。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [97] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener,Angela Yiu,Benjamin Chang,Mathieu Bourdenx,Tyler Nadolski,Arvis Sulovari,Eric C. Landsness,Daniel L. Barabasi,Siddharth Narayanan,Nicky Evans,Shriya Reddy,Martha Foiani,Aizad Kamal,Leah P. Shriver,Fang Cao,Asmamaw T. Wassie,Jon M. Laurent,Edwin Melville-Green,Mayk Caldas,Albert Bou,Kaleigh F. Roberts,Sladjana Zagorac,Timothy C. Orr,Miranda E. Orr,Kevin J. Zwezdaryk,Ali E. Ghareeb,Laurie McCoy,Bruna Gomes,Euan A. Ashley,Karen E. Duff,Tonio Buonassisi,Tom Rainforth,Randall J. Bateman,Michael Skarlinski,Samuel G. Rodriques,Michaela M. Hinks,Andrew D. White*

Main category: cs.AI

TL;DR: Kosmos is an AI scientist that automates data-driven discovery, using a structured world model to maintain coherence over long research cycles. It produces accurate, traceable reports and generates both reproducible and novel scientific findings.


<details>
  <summary>Details</summary>
Motivation: Current AI agents for scientific research are limited in coherence and action depth, hindering their ability to perform deep, meaningful discoveries. Kosmos aims to overcome these limitations by integrating a structured world model for sustained, traceable research automation.

Method: Kosmos utilizes a structured world model to coordinate a data analysis agent and a literature search agent, enabling parallel cycles of data analysis, literature search, and hypothesis generation over extended periods (up to 12 hours). It cites all statements with code or primary literature for traceability.

Result: Kosmos achieved 79.4% accuracy in its reports, equivalent to 6 months of human research time per 20-cycle run. It generated both reproducible and novel discoveries across metabolomics, materials science, neuroscience, and statistical genetics.

Conclusion: Kosmos demonstrates significant potential in automating scientific research, achieving results comparable to human researchers while maintaining traceability and accuracy. Its structured world model enables coherent, long-term pursuit of scientific objectives, leading to both reproducible and novel discoveries across various fields.

Abstract: Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [98] [Neurosymbolic Deep Learning Semantics](https://arxiv.org/abs/2511.02825)
*Artur d'Avila Garcez,Simon Odense*

Main category: cs.AI

TL;DR: 本文主张逻辑为AI科学提供了语义框架，通过神经符号方法将深度学习与逻辑联系起来，解决了现有方法缺乏通用条件的问题。


<details>
  <summary>Details</summary>
Motivation: AI缺乏语义，导致其科学发现难以令人满意。需要通过一个框架将AI的洞察转化为可理解的科学知识。

Method: 使用神经符号框架中的逻辑，提出语义编码框架，明确神经网络与逻辑之间的映射，并综述了现有的神经编码和知识提取方法。

Result: 提出了一个语义编码框架，将逻辑语义与神经网络联系起来，并形式化定义了该框架。

Conclusion: 逻辑提供了一个合适的框架，为深度学习和神经符号AI提供了急需的语义，并通过语义编码框架明确了神经网络与逻辑之间的映射关系。

Abstract: Artificial Intelligence (AI) is a powerful new language of science as
evidenced by recent Nobel Prizes in chemistry and physics that recognized
contributions to AI applied to those areas. Yet, this new language lacks
semantics, which makes AI's scientific discoveries unsatisfactory at best. With
the purpose of uncovering new facts but also improving our understanding of the
world, AI-based science requires formalization through a framework capable of
translating insight into comprehensible scientific knowledge. In this paper, we
argue that logic offers an adequate framework. In particular, we use logic in a
neurosymbolic framework to offer a much needed semantics for deep learning, the
neural network-based technology of current AI. Deep learning and neurosymbolic
AI lack a general set of conditions to ensure that desirable properties are
satisfied. Instead, there is a plethora of encoding and knowledge extraction
approaches designed for particular cases. To rectify this, we introduced a
framework for semantic encoding, making explicit the mapping between neural
networks and logic, and characterizing the common ingredients of the various
existing approaches. In this paper, we describe succinctly and exemplify how
logical semantics and neural networks are linked through this framework, we
review some of the most prominent approaches and techniques developed for
neural encoding and knowledge extraction, provide a formal definition of our
framework, and discuss some of the difficulties of identifying a semantic
encoding in practice in light of analogous problems in the philosophy of mind.

</details>


### [99] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: Agent-Omni框架通过主代理协调多模态基础模型，无需重新训练即可实现高效跨模态推理，性能领先且易于扩展。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLMs局限于固定模态对、需大量对齐数据微调的问题，构建支持文本、图像、音频和视频的全能模型。

Method: 提出Agent-Omni框架，通过主代理系统协调现有基础模型，主代理负责解释用户意图、分配子任务并整合输出。

Result: 在文本、图像、音频、视频及全能基准测试中，Agent-Omni均达到最先进性能，尤其在复杂跨模态推理任务中表现突出。

Conclusion: Agent-Omni框架通过协调现有的基础模型，实现了无需重新训练的灵活多模态推理，并在多种模态任务中展现出最先进的性能。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [100] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE通过文本推理链（CoR）提升VLM在机器人操控中的精确性和可解释性，实验证明其在Where2Place基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉思维链（CoT）方法计算成本高，难以满足机器人操控任务对精确空间推理的需求。

Method: 提出TRACE方法，结合文本推理链（CoR）进行空间推理预测，并通过自主生成的TRACE数据集对VLM进行微调。

Result: TRACE-tuned模型在Where2Place基准测试中达到48.1%准确率（相对提升9.6%），在更具挑战性的W2P(h)子集中达到55.0%。消融研究表明性能与推理数据量直接相关。

Conclusion: TRACE方法通过引入文本推理链（CoR）显著提升了视觉语言模型（VLM）在机器人操控任务中的精确性、可靠性和可解释性。实验证明，该方法在Where2Place基准测试中表现优异，且性能与推理数据量直接相关。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [101] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: SOPPI方法通过SVGD动态优化MPPI的轨迹采样，提升控制性能，实验验证了其在多种任务中的优越性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI方法依赖随机采样轨迹，可能导致样本不足或空间表示不充分，从而产生次优结果。为解决这一问题，论文探索了结合SVGD以优化样本生成的方法。

Method: 论文提出了一种名为SOPPI的新方法，通过在MPPI环境步骤之间引入SVGD更新，动态调整噪声分布，优化轨迹采样，从而在不显著增加计算负担的情况下提升性能。

Result: 实验结果表明，SOPPI在Cart-Pole和二维双足行走任务中均优于标准MPPI，且在较低粒子数下仍保持可行性，适用于多种超参数配置。

Conclusion: 该论文提出的SOPPI方法通过结合MPPI和SVGD，动态优化轨迹采样分布，显著提升了控制性能，并在不同系统和参数下均表现出优越性，展示了在更高自由度系统和可微分模拟器中的潜在应用价值。

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [102] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap 通过 GPU 和 CPU 优化，显著加速视觉 SLAM 的局部建图模块，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 解决视觉 SLAM 中局部建图过程中的性能瓶颈问题。

Method: 通过 GPU 和 CPU 的针对性优化，将地图点三角测量和融合卸载到 GPU，加速 CPU 上的冗余关键帧剔除，并集成 GPU 加速求解器以加快局部束调整。

Result: 在 EuRoC 和 TUM-VI 数据集上，局部建图模块分别实现了 1.3 倍和 1.6 倍的平均加速。

Conclusion: TurboMap 在保持原有系统精度的同时，显著提升了视觉 SLAM 系统中局部建图模块的性能。

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [103] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: TACO框架通过实时优化控制器参数和轨迹，显著提升四旋翼飞行器的轨迹跟踪性能，并实现高效实时部署。


<details>
  <summary>Details</summary>
Motivation: 传统的固定参数调整方法无法适应任务特异性需求，限制了控制器性能。TACO旨在通过在线参数优化提升轨迹跟踪效果。

Method: TACO采用学习预测模型和轻量级优化方案，实时优化控制器增益，并支持轨迹的动态调整以适应平滑约束。

Result: 实验表明，TACO在多种轨迹类型上优于静态参数调整方法，且运算速度远超黑盒优化基准，显著降低了跟踪误差。

Conclusion: TACO框架通过在线调整控制器参数和轨迹优化，显著提升了四旋翼飞行器的轨迹跟踪性能，并在实际物理平台上验证了其实时部署的可行性。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [104] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本文通过分析机器人操纵方法，提炼了世界模型的核心能力，并提出了开发通用实用世界模型的路线图。


<details>
  <summary>Details</summary>
Motivation: 自主代理需要在复杂、动态和不确定的环境中操作，这要求它们理解世界的底层机制和动态，超越纯粹的反应控制或简单复制观察到的状态。

Method: 通过回顾机器人操纵方法，分析其在感知、预测和控制中的作用，识别关键挑战与解决方案，提炼核心组件、能力和功能。

Result: 分析了表现出世界模型核心能力的方法，并提炼了真实世界模型应具备的核心组件、能力和功能。

Conclusion: 本文旨在为机器人开发通用且实用的世界模型提供路线图。

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [105] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 论文提出了一种分层多机器人自主性模型，结合集体和个体决策方法，并在实验中验证了其实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在海洋环境中展现出高效性和鲁棒性，但如何建模和设计这些系统以充分发挥协作优势是一个关键挑战。论文旨在解决这一问题。

Method: 论文引入了一个分层模型，利用普查原则（非线性意见动态模型）进行集体决策，并结合多目标行为优化（区间编程）进行个体决策。此外，还提出了一种基于梯度下降的分布式优化方法。

Result: 实验验证了模型在自适应采样、高价值单位保护和竞争性游戏等三种不同场景中的有效性。

Conclusion: 该论文提出的分层多机器人自主性模型通过结合普查原则和多目标行为优化，为多机器人系统的集体和个体决策提供了新的方法。实验验证了该模型在多种场景中的实用性。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [106] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 结合3D生成AI与VLM，通过零样本推理和用户反馈实现多组件3D物体的高效生成与机器人组装，用户偏好率高达90.6%。


<details>
  <summary>Details</summary>
Motivation: 解决3D生成AI在多组件类型物体创建中的挑战，实现从自然语言到机器人组装的多组件物体生成。

Method: 提出了一种集成3D生成AI与视觉语言模型（VLM）的流程，利用VLM进行零样本、多模态的几何与功能推理，将AI生成的网格分解为多组件3D模型。

Result: 用户评估显示，VLM生成的组件分配偏好率为90.6%，显著高于基于规则（59.4%）和随机分配（2.5%）。

Conclusion: 该系统通过结合VLM的零样本推理和用户反馈，实现了对生成AI和机器人技术的更大人机控制与协作，提升了多组件3D物体生成的实用性和用户满意度。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [107] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 本文提出了一种7-DOF手术机器人手臂设计，通过运动学优化和人体工程学设计显著提高了手术精度和效率，同时降低了外科医生的疲劳。实验结果验证了设计的有效性，为下一代手术机器人开发提供了指导。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助在微创手术中可以显著提高手术精度并减少外科医生的疲劳。

Method: 提出了一种具有7自由度（7-DOF）的机器人手臂系统，该系统在器械插入点处加入了远程运动中心（RCM），并考虑了人体工程学因素以改善外科医生的交互体验。该设计在通用机器人平台上实现，并通过一系列模拟手术任务来评估目标准确性、任务效率和外科医生的舒适度。

Result: 实验结果表明，优化后的机器人设计显著提高了目标准确性（误差减少超过50%），缩短了任务完成时间，同时大幅降低了操作者的肌肉劳损和不适感。

Conclusion: 本文的研究验证了运动学优化和以人为中心的符合人体工程学设计在提升机器人辅助手术性能中的重要性，并为下一代手术机器人的开发提供了指导。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [108] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 比较软体机器人控制中集中式与分布式MARL的性能，发现$n\le4$时集中式更优，$4<n\le12$时分布式更高效，但集中式训练更快。


<details>
  <summary>Details</summary>
Motivation: 比较集中式和分布式多智能体强化学习（MARL）架构在软体机器人控制中的性能差异，为软杆状机械臂的控制策略选择提供依据。

Method: 使用PyElastica和OpenAI Gym接口，在相同预算下训练全局PPO控制器和多智能体PPO（MAPPO），系统性地改变控制段数量$n$，评估三种场景下的性能：默认基线条件、外部干扰恢复和执行器故障适应。

Result: 当控制段数量$n\le4$时，分布式策略无显著优势；$n\le2$时集中式策略更优；$4<n\le12$时分布式策略样本效率更高，成功率、鲁棒性和收敛速度更佳，但集中式策略训练时间效率更高。

Conclusion: 本研究强调了集中式与分布式策略在软体机器人强化学习控制中的权衡，并为未来软杆状机械臂的仿真到实际转移提供了设计指导。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [109] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY是一个双向语言-动作映射框架，通过自监督学习提升机器人操作任务的成功率和解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前的语言到动作（L2A）模型缺乏上下文理解和解释能力，限制了其泛化能力。双向映射（L2A和A2L）可以形成更丰富的内部表示，并开启自监督学习的新范式。

Method: LACY结合了三个协同任务：语言到动作（L2A）、动作到语言（A2L）以及语言一致性验证（L2C），并通过主动增强策略自主生成和过滤训练数据。

Result: 在模拟和真实世界的拾取放置任务中，LACY平均提高了56.46%的任务成功率，并提供了更稳健的语言-动作基础。

Conclusion: LACY框架通过双向语言-动作映射和自我改进循环，显著提升了机器人操作任务的成功率和语言-动作基础的理解能力。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [110] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 仿头足类动物吸盘设计的智能吸盘SuckTac，集成摄像头触觉传感器，优化结构以提升感知和适应性，适用于复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有吸盘缺乏高保真感知能力，难以应对不规则物体和复杂环境，受头足类动物吸盘的启发，开发智能吸盘SuckTac。

Method: 通过多材料集成铸造技术，将摄像头和光源嵌入吸盘，优化机械设计（如轮廓、柔性唇和表面微结构），实现高密度感知。

Result: 实验证明SuckTac在机器人布料操作和软体移动机器人检查等任务中表现优异，具有广泛适用性。

Conclusion: SuckTac通过仿生设计和集成摄像头触觉传感器，显著提升了吸盘在复杂环境中的感知能力和适应性，适用于多种挑战性任务。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [111] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队通过硬件集成IMU和软件模块优化，提升了机器人姿态精度和决策效率，以适应高节奏比赛。


<details>
  <summary>Details</summary>
Motivation: 为了适应高节奏的比赛动态并提升机器人的整体性能，团队在硬件和软件两方面进行了全面优化。

Method: 在硬件上集成了IMU以提升姿态精度和角速度规划；在软件上优化了策略和CUDA模块，改进了决策效率、球追踪预测和控球预测。

Result: 成功提升了机器人的姿态精度、角速度规划能力，并在决策效率、球追踪和控球预测方面取得了显著改进。

Conclusion: ZJUNlict团队在硬件和软件方面均取得了显著进展，特别是在机器人姿态精度和决策效率上的提升，以适应高节奏比赛需求。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [112] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: 提出基于超二次曲面的飞行机械臂规划和控制系统，提升复杂环境中的轨迹安全和效率。


<details>
  <summary>Details</summary>
Motivation: 解决飞行机械臂在复杂环境中规划安全、动态可行轨迹的难题，避免传统几何抽象（如包围盒或椭球）的保守性。

Method: 使用SQ-plus-proxy表示对飞行器和障碍物进行几何精确建模，结合Voronoi图和平衡流形公式生成平滑、避障的轨迹，并设计了安全关键控制器。

Result: 仿真和实际硬件实验表明，该方法在密集环境中优于基于采样的规划器，轨迹更快、更安全、更平滑，且几何保真度优于基于椭球的基线。

Conclusion: 论文提出了一个基于超二次曲面（SQs）的全身运动规划和安全关键控制框架，通过实验验证了其在实际硬件上的可行性和鲁棒性。

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [113] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: OmniPianist通过OT自动指法、大规模RL训练和Flow Matching Transformer，实现了无需人类示范的机器人钢琴演奏，能演奏近千首乐曲。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人手人类水平的灵巧性是机器人领域的长期目标，而双手机器人钢琴演奏因其高维度、接触密集和快速精确控制的要求，成为了极具挑战性的任务。

Method: 结合了Optimal Transport（OT）自动指法策略、大规模强化学习（RL）训练以及Flow Matching Transformer的大规模模仿学习。

Result: OmniPianist能够演奏近千首乐曲，并通过大规模实验验证了其方法的有效性和可扩展性。

Conclusion: OmniPianist通过创新的方法（如OT自动指法策略、大规模RL训练和Flow Matching Transformer）成功实现了高水平的机器人钢琴演奏，展示了其有效性和可扩展性。

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [114] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 研究展示了导电非磁性物体的闭环磁操纵，特别是在太空碎片回收中的应用潜力，通过涡流产生的反向磁偶极矩实现了铝球的3D位置控制。


<details>
  <summary>Details</summary>
Motivation: 将磁操纵扩展到导电非磁性物体，为以前仅限于硬磁或软磁材料的应用开辟了新途径，特别是在利用振荡磁场回收太空碎片方面具有重要意义。

Method: 利用感应涡流产生的反向磁偶极矩，实现半浮铝球的闭环位置控制，并探讨了不同力反演方法的有效性。

Result: 实验室测试中成功实现了半浮铝球的闭环位置控制，并验证了不同力反演方法的效果。

Conclusion: 闭环位置控制方法为实现感应磁偶极子的三维自由度位置控制提供了关键的第一步，为更广泛的应用奠定了基础。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [115] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1通过统一视觉-运动编码和三阶段训练，解决了视觉-语言-动作模型的精确动作生成和跨数据源领域差距问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在从高维观察生成精确低级动作和跨异构数据源的领域差距方面存在挑战，未能充分利用大规模异构数据集中的多模态知识。

Method: XR-1采用双分支VQ-VAE学习离散潜在表示（UVMC），并通过自监督学习、大规模跨机器人数据集预训练和任务特定后训练的三阶段范式进行训练。

Result: XR-1在六种不同机器人上的120多个多样化操作任务中，通过14,000多次实验，性能优于现有基准模型，并展示了对新对象、背景变化、干扰物和光照变化的强泛化能力。

Conclusion: XR-1通过引入统一的视觉-运动编码（UVMC）和三阶段训练范式，显著提升了跨机器人、任务和环境的视觉-语言-动作模型性能，并在实验中展现了优越的泛化能力。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [116] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一种便携式、无需动作捕捉的人形机器人远程操作系统，通过VR和自定义颈部实现高效数据收集和全身控制，开源且可扩展。


<details>
  <summary>Details</summary>
Motivation: 人形机器人领域缺乏有效的数据收集框架，现有系统要么控制分离，要么依赖昂贵的动作捕捉设备。

Method: 利用PICO4U VR获取实时全身人体运动数据，结合自定义2-DoF机器人颈部实现自我中心视觉，构建了层次化的视觉运动策略框架。

Result: 系统能够在15分钟内收集100次演示，成功率接近100%，并成功展示了全身灵活操作和动态踢球任务。

Conclusion: TWIST2系统通过便携式、无需动作捕捉的框架，实现了高效的人形机器人远程操作和数据收集，推动了人形机器人技术的可扩展性。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [117] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本综述提出了一种基于架构和设计的工作负载调度器分类法，重点关注吞吐量和可扩展性，并以Google的Borg为例进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个系统化的分类法，帮助理解和比较不同工作负载调度器的架构和设计，特别是在吞吐量和可扩展性方面的表现。

Method: 通过分析现有工作负载调度器的架构和设计，创建了一个分类法，重点关注影响系统吞吐量和可扩展性的关键设计因素及其改进。

Result: 提出了一个层次化的分类法，特别强调了Google的Borg系统，并分析了影响调度器性能的关键设计因素。

Conclusion: 本综述通过分析已部署和广泛使用的工作负载调度器解决方案，提出了一个基于架构和设计的层次化分类法。特别关注了Google的Borg系统，作为此类中最先进和公开的系统之一。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [118] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: FAIR的CDR概述了2028年起的研究计算基础设施计划，旨在通过联邦化和中央协调的架构满足多样化需求。


<details>
  <summary>Details</summary>
Motivation: 为满足FAIR各研究小组的计算需求，并应对2028年‘首批科学（plus）’阶段至模块化启动版本期间的数据挑战。

Method: 报告详细介绍了计算和存储基础设施的政策、FAIR计算模型，以及开放数据、软件和服务的架构设计。

Result: 提出了一个全面且灵活的计算基础设施框架，能够支持FAIR的研究线路并适应未来需求。

Conclusion: 该CDR提出了一个联邦化且中央协调的计算基础设施计划，旨在支持FAIR多样化的研究需求，并具备应对未来数据挑战的扩展性和灵活性。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [119] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 云计算创新转向AI/ML主导，科学计算资源适应性不足，需探索未来成本模型以支持科学发现。


<details>
  <summary>Details</summary>
Motivation: 云计算创新最初为满足科学计算需求，现转向AI/ML主导，导致科学计算资源适应性不足。

Method: 通过分析云计算的历史演变和当前AI/ML主导的趋势，探讨科学计算资源的未来适应性。

Result: 指出当前云计算成本模型对科学计算的不足，并探讨可能的未来发展方向。

Conclusion: 本文探讨了云计算成本模型的过去、现状及未来，以支持科学发现的持续发展。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [120] [SPHERE: Spherical partitioning for large-scale routing optimization](https://arxiv.org/abs/2511.01863)
*Robert Fabian Lindermann,Paul-Niklas Ken Kandora,Simon Caspar Zeller,Adrian Asmund Fessler,Steffen Rebennack*

Main category: cs.DC

TL;DR: SPHERE是一种高效的最短路径搜索启发式方法，通过分解任务和递归处理，在大型网络中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 研究大型加权无向图中的最短路径路由问题，解决精确求解器因搜索范围扩大而导致的时间和内存成本增加的问题。

Method: SPHERE是一种源-目标感知启发式方法，通过识别$s$-$t$重叠区域中的顶点，将路径搜索任务分解为两个子问题，并在需要时递归处理。

Result: 在包含超过百万节点和边的大型网络上，SPHERE实现了比Louvain路由和基于METIS的流程更快的运行时间和更小的最优性差距。

Conclusion: SPHERE方法在大型网络中表现出色，不仅在运行时间上优于Dijkstra算法，而且在最优性差距上也优于基于Louvain和METIS的解决方案。

Abstract: We study shortest-path routing in large weighted, undirected graphs, where
expanding search frontiers raise time and memory costs for exact solvers. We
propose \emph{SPHERE}, a source-target-aware heuristic that identifies an
$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.
Selecting an anchor $a$ in this overlap partitions the task into two
subproblems with unchanged problem-topology, $s\to a$ and $a\to t$; if either
remains large, the procedure recurses on its induced subgraph. Because the cut
lies inside the overlap, concatenating the resulting subpaths yields a valid
$s\to t$ route without boundary repair. SPHERE is independent of the downstream
solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large
networks, it achieves faster runtimes and smaller optimality gaps than
Louvain-based routing and a METIS-based pipeline, even on graphs with more than
a million nodes and edges, while also outperforming Dijkstra in runtime.

</details>


### [121] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning 研究系统分析了边缘GPU上部署推理LLM的延迟-准确性权衡，提供了优化部署的实践指南。


<details>
  <summary>Details</summary>
Motivation: 边缘智能范式在自主系统中需求增长，但部署大型语言模型（LLM）在边缘GPU上面临延迟和计算资源限制的挑战，缺乏关于如何平衡设计因素的指导。

Method: 本研究系统量化了不同LLM架构和模型大小下的延迟-准确性权衡，评估了基于提示和模型调优的技术以减少推理令牌长度，并分析了不同并行度的测试时缩放方法。

Result: EdgeReasoning 绘制了可实现的准确性-延迟配置的帕累托边界，为推理LLM的边缘部署提供了系统化的最优配置指南。

Conclusion: EdgeReasoning 通过系统分析，提供了在边缘GPU上部署推理LLM的最佳实践指南，帮助开发者在延迟和准确性之间找到最优平衡。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [122] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 论文利用逻辑概率方法建模多核处理器的可靠性、容错性和灵活性，分析了双核和四核处理器的性能趋势。


<details>
  <summary>Details</summary>
Motivation: 研究多核处理器的结构分析和效率指标（可靠性、容错性、活性和灵活性），以提升其性能。

Method: 采用逻辑概率方法，开发了评估多核处理器可靠性、容错性、灵活性和性能的模型。

Result: 提出了双核和四核处理器的结构分析结果，并展示了多核处理器效率指标提升的趋势。

Conclusion: 论文通过结构分析和效率指标评估模型，展示了多核处理器在可靠性、容错性和灵活性等方面的性能提升趋势。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [123] [Learned Cost Model for Placement on Reconfigurable Dataflow Hardware](https://arxiv.org/abs/2511.01872)
*Etash Guha,Tianxiao Jiang,Andrew Deng,Jian Zhang,Muthu Annamalai*

Main category: cs.DC

TL;DR: 学习型方法比传统手工分析方法更准确地预测映射吞吐量，提升编译图速度5.6%。


<details>
  <summary>Details</summary>
Motivation: 传统手工分析方法依赖代理特征或直觉，存在误差，且完全测量吞吐量成本高昂。

Method: 提出一种学习型方法，通过预测映射吞吐量来优化数据流图在可重构系统中的映射。

Result: 学习型方法在多种图上预测吞吐量的准确率提高了31%-52%，且移除性能注释后准确性无下降。

Conclusion: 使用学习型方法预测映射吞吐量比传统手工分析方法更准确，且不会因移除性能注释而降低准确性，最终实现编译图速度提升5.6%。

Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is
difficult, as different mappings have different throughputs and consume
resource constraints differently. To solve this, a model to evaluate the
throughput of mappings is necessary as measuring throughput completely is
expensive. Many use a hand-designed analytical model, relying on proxy features
or intuition, introducing error. We provide a Learned Approach that predicts
throughput 31%-52% more accurately over a variety of graphs. In addition, our
approach shows no accuracy degradation after removing performance annotations.
We show that using this approach results in 5.6% faster compiled graphs.

</details>


### [124] [HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing](https://arxiv.org/abs/2511.01881)
*Zhengxin Fang,Hui Ma,Gang Chen,Rajkumar Buyya*

Main category: cs.DC

TL;DR: HGraphScale利用分层图神经网络优化微服务资源扩展，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 微服务架构在容器化云环境中的资源扩展面临依赖复杂和部署方案带来的挑战。

Method: 提出了一种名为HGraphScale的新型自动扩展方法，利用分层图神经网络建模微服务依赖和部署方案。

Result: 实验表明，HGraphScale在特定VM租赁预算下，平均响应时间最多减少80.16%。

Conclusion: HGraphScale通过分层图神经网络有效捕捉微服务依赖关系和部署方案，显著降低了响应时间，优于现有方法。

Abstract: Microservice architecture has become a dominant paradigm in application
development due to its advantages of being lightweight, flexible, and
resilient. Deploying microservice applications in the container-based cloud
enables fine-grained elastic resource allocation. Autoscaling is an effective
approach to dynamically adjust the resource provisioned to containers. However,
the intricate microservice dependencies and the deployment scheme of the
container-based cloud bring extra challenges of resource scaling. This article
proposes a novel autoscaling approach named HGraphScale. In particular,
HGraphScale captures microservice dependencies and the deployment scheme by a
newly designed hierarchical graph neural network, and makes effective scaling
actions for rapidly changing user requests workloads. Extensive experiments
based on real-world traces of user requests are conducted to evaluate the
effectiveness of HGraphScale. The experiment results show that the HGraphScale
outperforms existing state-of-the-art autoscaling approaches by reducing at
most 80.16\% of the average response time under a certain VM rental budget of
application providers.

</details>


### [125] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner是一种侧边栏填充技术，通过减少数据拷贝和避免序列化，显著提升基于WebAssembly的无服务器函数性能。


<details>
  <summary>Details</summary>
Motivation: 无服务器函数通常依赖外部远程服务存储和交换数据，而数据传输中的序列化和反序列化操作会导致上下文切换和内存分配的开销，增加延迟和资源消耗。

Method: 通过映射函数内存并沿专用虚拟数据通道移动数据，绕过序列化和反序列化的高成本过程。

Result: 实验结果表明，Roadrunner将函数间通信延迟降低了44%至89%，减少了97%的数据传输序列化开销，吞吐量提升了69倍。

Conclusion: Roadrunner显著提升了基于WebAssembly的无服务器函数间的通信性能，减少了数据移动开销和上下文切换，实现了接近本地的延迟性能。

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [126] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLR通过记忆化和变量卸载技术，显著提升了ADMM-FFT的性能和扩展性，解决了其在高精度层析成像中的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT虽在层析成像中重建精度高，但存在计算时间长和内存消耗大的问题，限制了其应用。

Method: 引入记忆化技术（mLR）替代耗时的FFT操作，并采用变量卸载技术节省CPU内存，实现跨GPU和节点的扩展。

Result: mLR在2Kx2Kx2K的最大输入问题上实现了52.8%的平均性能提升（最高65.4%），并成功扩展了ADMM-FFT的应用范围。

Conclusion: mLR成功解决了ADMM-FFT在高精度层析成像中的计算时间和内存消耗问题，通过记忆化技术和变量卸载实现了性能提升和跨节点扩展。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [127] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: GPoS通过整合地理空间多样性与权益证明，显著改善了区块链的地理空间去中心化，实验显示性能开销极小。


<details>
  <summary>Details</summary>
Motivation: 由于当前PoS区块链中，少数地理区域主导了共识投票权，导致地理空间去中心化程度不足，影响了区块链的监管弹性、鲁棒性和公平性。

Method: 通过实证分析五大主要PoS区块链（Aptos、Avalanche、Ethereum、Solana和Sui），并提出GPoS（地理空间感知的权益证明）方案，将地理空间多样性与基于权益的投票权力相结合。实验评估采用基尼系数的特征向量中心性来衡量地理空间去中心化程度。

Result: 实验结果表明，GPoS平均提升了45%的地理空间去中心化程度，且在BFT协议（如HotStuff和CometBFT）中仅引入极小的性能开销。

Conclusion: GPoS（地理空间感知的权益证明）能够显著提升区块链的地理空间去中心化程度，同时在实际实验中仅引入极小的性能开销。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [128] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 论文提出了一种新方法，通过细粒度数据流同步替代传统BSP模型，显著提升了分布式LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 传统BSP模型在分布式GPU执行中存在显著的性能低效问题，尤其是"三税"导致的开销。

Method: 利用Iris for Triton等库，设计了细粒度的编程模式，通过直接的数据流同步替代全局屏障，消除了"三税"（Bulk Synchronous、Inter-Kernel Data Locality和Kernel Launch Overhead）。

Result: 在关键内核（如All-Gather + GEMM操作和Flash Decode算法）中，相比BSP方法，端到端延迟提高了10-20%。

Conclusion: 论文提出了一种超越传统BSP模型的新范式，通过细粒度编程模式和直接的数据流同步，显著提升了分布式LLM工作负载的性能和可编程性。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [129] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 通过算子级自动扩展框架，显著提升大型生成模型的资源利用率和性能，减少GPU使用和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案采用静态资源分配或模型级别的自动扩展，无法适应动态推理流量的变化，导致性能下降或资源浪费。

Method: 提出了一种基于算子级别的自动扩展框架，通过分析算子的计算和内存占用，优化资源分配、批处理和放置策略。

Result: 在生产级负载测试中，该方法在保证SLO的前提下，减少了40%的GPU使用和35%的能耗，或在固定资源下提升了1.6倍的吞吐量并降低5%的能耗。

Conclusion: 研究表明，以算子而非整个模型作为资源分配的粒度，能更高效地服务大型生成模型，显著提升资源利用率和性能。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [130] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 两种新型调度算法优化LQCD张量收缩，提升内存效率和计算速度。


<details>
  <summary>Details</summary>
Motivation: 在LQCD模拟中，计算相关函数需要大量内存密集型张量收缩操作，如何在GPU加速器上高效调度这些操作以优化张量重用和数据流量是关键挑战。

Method: 通过重新排序张量收缩操作，利用应用特定特性（如二进制收缩和收缩树中的局部性）优化内存峰值目标。

Result: 新调度算法在Redstar软件中实现了最高2.1倍的内存峰值改善、4.2倍的驱逐减少、1.8倍的数据流量降低，以及1.9倍的计算速度提升。

Conclusion: 本文提出的两种新型调度算法显著提升了LQCD模拟中张量重用的时空局部性，减少了数据流量和计算时间。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [131] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: Split Computing reduces processing time and power consumption in 3D object detection by splitting deep neural networks, achieving significant time savings.


<details>
  <summary>Details</summary>
Motivation: Address the issues of longer processing times and increased power consumption on edge devices caused by complex state-of-the-art models in 3D object detection.

Method: Leveraged Split Computing, a distributed machine learning inference method, to split the deep neural network model after voxelization and within the network.

Result: Splitting after voxelization reduces inference time by 70.8% and edge device execution time by 90.0%. Splitting within the network reduces inference time by up to 57.1% and edge device execution time by up to 69.5%.

Conclusion: Split Computing effectively reduces inference time and edge device execution time in 3D object detection, while minimizing data breach risks.

Abstract: The field of autonomous driving technology is rapidly advancing, with deep
learning being a key component. Particularly in the field of sensing, 3D point
cloud data collected by LiDAR is utilized to run deep neural network models for
3D object detection. However, these state-of-the-art models are complex,
leading to longer processing times and increased power consumption on edge
devices. The objective of this study is to address these issues by leveraging
Split Computing, a distributed machine learning inference method. Split
Computing aims to lessen the computational burden on edge devices, thereby
reducing processing time and power consumption. Furthermore, it minimizes the
risk of data breaches by only transmitting intermediate data from the deep
neural network model. Experimental results show that splitting after
voxelization reduces the inference time by 70.8% and the edge device execution
time by 90.0%. When splitting within the network, the inference time is reduced
by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.

</details>


### [132] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: FedAttn是一种新的分布式LLM推理框架，通过集成联邦范式和自注意力机制，解决了隐私、通信和计算效率问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLMs）在边缘部署中的隐私漏洞、通信开销和计算瓶颈问题。

Method: 提出了Federated Attention (FedAttn)，将联邦范式集成到自注意力机制中，允许参与者在本地执行自注意力计算，并定期交换和聚合KV矩阵，从而在不暴露私有提示的情况下协作生成LLM响应。

Result: 理论分析揭示了本地自注意力计算和异构令牌相关性对错误传播动态的影响，以及响应质量与通信/计算效率之间的权衡。实验验证了FedAttn的优化潜力。

Conclusion: FedAttn 提供了一种新的分布式LLM推理框架，通过将联邦范式集成到自注意力机制中，实现了隐私保护、通信效率和计算效率的平衡。实验验证了理论分析，并展示了通过稀疏注意力和自适应KV聚合的优化潜力。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [133] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文评估了四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）在两类科学计算应用中的表现，发现性能差异显著，需进一步优化以提升可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算在百亿亿次时代需要更高的计算能力来解决复杂问题，因此对供应商无关、性能可移植的框架需求日益突出。本文旨在评估不同框架在科学计算应用中的性能表现，以指导未来的优化方向。

Method: 本文通过在两类代表性的科学计算应用（N体模拟和结构化网格模拟）上测试四种性能可移植性框架（Kokkos、OpenMP、RAJA和OCCA），在Polaris超级计算机的单个节点上使用四块NVIDIA A100 GPU进行了实验。

Result: 实验结果显示，不同框架在性能上存在显著差异：OCCA在小规模验证问题上执行时间更快（可能得益于JIT编译），但其默认API缺乏优化的归约算法可能限制大规模模拟的可扩展性；OpenMP在结构化网格模拟中表现不佳，可能与节点间数据同步和通信效率低有关。

Conclusion: 本文的结论是，尽管现有的性能可移植性框架如Kokkos、OpenMP、RAJA和OCCA在异构计算架构上表现出不同的性能，但仍需进一步优化以最大化各框架的潜力。未来工作将集中在优化归约算法、数据通信和内存管理，并进行可扩展性研究和全面的统计分析。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [134] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos*是Egalitarian Paxos的简化且正确版本，通过改进故障恢复算法和扩展故障阈值范围，解决了原协议的复杂性和错误问题。


<details>
  <summary>Details</summary>
Motivation: 解决Egalitarian Paxos协议的复杂性、模糊规范和严重错误问题。

Method: 提出了一种更简单的故障恢复算法，并进行了严格的正确性证明。

Result: EPaxos*在保持非零吞吐量的同时，支持更广泛的故障阈值范围（n ≥ max{2e+f-1, 2f+1}）。

Conclusion: EPaxos*是一种更简单且正确的Egalitarian Paxos变体，通过简化故障恢复算法并严格证明其正确性，同时扩展了故障阈值范围。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [135] [Disjoint Paths in Expanders in Deterministic Almost-Linear Time via Hypergraph Perfect Matching](https://arxiv.org/abs/2511.02214)
*Matija Bucić,Zhongtian He,Shang-En Huang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 提出了高效确定性算法，用于在扩展图中找到短边不相交路径，填补了非恒定电导率扩展图的算法空白。


<details>
  <summary>Details</summary>
Motivation: 解决扩展图中边不相交路径问题的确定性算法效率不足，特别是对非恒定电导率扩展图。

Method: 通过设计一种几乎线性的超图完美匹配算法，并应用Haxell（1995）的Hall型条件推广。

Result: 算法能在特定条件下，以近乎线性的时间找到长度受限的边不相交路径。

Conclusion: 本文提出了在扩展图中寻找短边不相交路径的高效确定性算法，填补了先前仅适用于恒定电导率扩展图的空白。

Abstract: We design efficient deterministic algorithms for finding short edge-disjoint
paths in expanders. Specifically, given an $n$-vertex $m$-edge expander $G$ of
conductance $\phi$ and minimum degree $\delta$, and a set of pairs
$\{(s_i,t_i)\}_i$ such that each vertex appears in at most $k$ pairs, our
algorithm deterministically computes a set of edge-disjoint paths from $s_i$ to
$t_i$, one for every $i$: (1) each of length at most $18 \log (n)/\phi$ and in
$mn^{1+o(1)}\min\{k, \phi^{-1}\}$ total time, assuming $\phi^3\delta\ge (35\log
n)^3 k$, or (2) each of length at most $n^{o(1)}/\phi$ and in total
$m^{1+o(1)}$ time, assuming $\phi^3 \delta \ge n^{o(1)} k$. Before our work,
deterministic polynomial-time algorithms were known only for expanders with
constant conductance and were significantly slower. To obtain our result, we
give an almost-linear time algorithm for \emph{hypergraph perfect matching}
under generalizations of Hall-type conditions (Haxell 1995), a powerful
framework with applications in various settings, which until now has only
admitted large polynomial-time algorithms (Annamalai 2018).

</details>


### [136] [Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint](https://arxiv.org/abs/2511.02254)
*Tan D. Tran,Canh V. Pham*

Main category: cs.DS

TL;DR: 首次提出两种低复杂度（O(n log k)）的近似算法FastDrSub和FastDrSub++，用于非单调DR子模最大化问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究非单调DR子模最大化问题，旨在设计低复杂度且高效的近似算法。

Method: 提出了两种近似算法FastDrSub和FastDrSub++，分别提供0.044和1/4-ϵ的近似比率，查询复杂度均为O(n log k)。

Result: 实验结果显示，FastDrSub++在查询复杂度和解质量上均显著优于现有方法，近似比率提升至1/4-ϵ。

Conclusion: FastDrSub和FastDrSub++是针对非单调DR子模最大化问题的首个具有恒定比率近似和低查询复杂度（O(n log k)）的算法，实验证明其在解决收益最大化问题时显著优于现有方法。

Abstract: This work studies the non-monotone DR-submodular Maximization over a ground
set of $n$ subject to a size constraint $k$. We propose two approximation
algorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub
offers an approximation ratio of $0.044$ with query complexity of $O(n
\log(k))$. The second one, FastDrSub++, improves upon it with a ratio of
$1/4-\epsilon$ within query complexity of $(n \log k)$ for an input parameter
$\epsilon >0$. Therefore, our proposed algorithms are the first constant-ratio
approximation algorithms for the problem with the low complexity of $O(n
\log(k))$.
  Additionally, both algorithms are experimentally evaluated and compared
against existing state-of-the-art methods, demonstrating their effectiveness in
solving the Revenue Maximization problem with DR-submodular objective function.
The experimental results show that our proposed algorithms significantly
outperform existing approaches in terms of both query complexity and solution
quality.

</details>


### [137] [Learning CNF formulas from uniform random solutions in the local lemma regime](https://arxiv.org/abs/2511.02487)
*Weiming Feng,Xiongxin Yang,Yixiao Yu,Yiyao Zhang*

Main category: cs.DS

TL;DR: 该论文改进了学习$k$-CNF公式的样本复杂度，并提供了新的理论和实践下界。


<details>
  <summary>Details</summary>
Motivation: 解决从独立同分布均匀随机解中学习$n$变量$k$-CNF公式的问题，等同于学习具有$k$-wise硬约束的布尔马尔可夫随机场（MRF）。

Method: 重新审视Valiant的算法，并证明其能够在特定条件下（如Lovász局部引理类型条件）从$O(\log n)$样本中精确学习$k$-CNF公式，以及从$\widetilde{O}(n^{\exp(-\sqrt{k})})$样本中学习随机$k$-CNF公式。

Result: 证明了Valiant的算法可以显著提升样本复杂度，并建立了新的信息论下界。

Conclusion: 该论文显著提升了学习$k$-CNF公式的样本复杂度，从之前的$O(n^k)$降低到更高效的水平，并在理论和实践上提供了新的下界。

Abstract: We study the problem of learning a $n$-variables $k$-CNF formula $\Phi$ from
its i.i.d. uniform random solutions, which is equivalent to learning a Boolean
Markov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's
algorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with
bounded clause intersection size under Lov\'asz local lemma type conditions,
from $O(\log n)$ samples; and (2) random $k$-CNFs near the satisfiability
threshold, from $\widetilde{O}(n^{\exp(-\sqrt{k})})$ samples. These results
significantly improve the previous $O(n^k)$ sample complexity. We further
establish new information-theoretic lower bounds on sample complexity for both
exact and approximate learning from i.i.d. uniform random solutions.

</details>


### [138] [A Simple and Fast $(3+\varepsilon)$-approximation for Constrained Correlation Clustering](https://arxiv.org/abs/2511.02705)
*Nate Veldt*

Main category: cs.DS

TL;DR: 提出了一种更快的算法，将近似因子从16降至（3+ε），同时保持算法简洁性。


<details>
  <summary>Details</summary>
Motivation: 解决Fischer等人提出的开放性问题，即在保持算法速度的同时，降低近似因子。

Method: 主要依赖于对新的覆盖线性规划的快速组合近似，并通过在辅助图上应用熟悉的Pivot算法进行舍入。

Result: 设计了时间复杂度为$	ilde{O}(n^3)$的算法，将近似因子降至（3+ε），且适用于仅含友好或敌对约束的实例。

Conclusion: 该论文通过设计一种更快的算法，显著降低了近似因子，从16降至（3+ε），同时保持了算法的简洁性。

Abstract: In Constrained Correlation Clustering, the goal is to cluster a complete
signed graph in a way that minimizes the number of negative edges inside
clusters plus the number of positive edges between clusters, while respecting
hard constraints on how to cluster certain friendly or hostile node pairs.
Fischer et al. [FKKT25a] recently developed a $\tilde{O}(n^3)$-time
16-approximation algorithm for this problem. We settle an open question posed
by these authors by designing an algorithm that is equally fast but brings the
approximation factor down to $(3+\varepsilon)$ for arbitrary constant
$\varepsilon > 0$. Although several new algorithmic steps are needed to obtain
our improved approximation, our approach maintains many advantages in terms of
simplicity. In particular, it relies mainly on rounding a (new) covering linear
program, which can be approximated quickly and combinatorially. Furthermore,
the rounding step amounts to applying the very familiar Pivot algorithm to an
auxiliary graph. Finally, we develop much simpler algorithms for instances that
involve only friendly or only hostile constraints.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [139] [Detecting Vulnerabilities from Issue Reports for Internet-of-Things](https://arxiv.org/abs/2511.01941)
*Sogol Masoumzadeh*

Main category: cs.SE

TL;DR: 研究首次探索ML和LLM在IoT漏洞检测中的应用，提出两种方法并验证其效果，为IoT漏洞报告检测奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 及时识别反映软件漏洞的报告对IoT尤为重要，因为IoT系统的分析速度较非IoT系统慢。尽管ML和LLM在非IoT系统中已用于检测漏洞指示问题，但在IoT中的应用尚未探索。

Method: 提出了两种方法：(1)结合ML、LLM和NLP技术检测21个Eclipse IoT项目的漏洞指示问题；(2)在11,000个GitHub问题上微调预训练的BERT MLM模型进行分类。

Result: 基于BERT NLP特征训练的SVM表现最佳，AUC为0.65；微调BERT模型的准确率为0.26，显示训练时暴露所有数据的重要性。

Conclusion: 该研究为准确检测IoT系统中的漏洞报告奠定了基础，为非IoT系统的类似检测提供了参考。

Abstract: Timely identification of issue reports reflecting software vulnerabilities is
crucial, particularly for Internet-of-Things (IoT) where analysis is slower
than non-IoT systems. While Machine Learning (ML) and Large Language Models
(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use
remains unexplored. We are the first to tackle this problem by proposing two
approaches: (1) combining ML and LLMs with Natural Language Processing (NLP)
techniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects
and (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000
GitHub issues for classifying \vul. Our best performance belongs to a Support
Vector Machine (SVM) trained on BERT NLP features, achieving an Area Under the
receiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT
achieves 0.26 accuracy, emphasizing the importance of exposing all data during
training. Our contributions set the stage for accurately detecting IoT
vulnerabilities from issue reports, similar to non-IoT systems.

</details>


### [140] [Metamorphic Testing of Large Language Models for Natural Language Processing](https://arxiv.org/abs/2511.02108)
*Steven Cho,Stefano Ruberto,Valerio Terragni*

Main category: cs.SE

TL;DR: 本研究通过变形测试（MT）验证了其在大型语言模型（LLMs）测试中的有效性，实验覆盖36个变形关系和56万次测试，展示了MT的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在NLP任务中表现优异，但其错误行为难以自动识别，且缺乏标记数据集。MT因其不依赖显式预言（如标记数据集）的特性，成为解决这一问题的潜在方案。

Method: 通过文献综述收集了191个NLP任务的MRs（变形关系），并从中选取36个代表性MRs，对三个流行的LLMs进行了约56万次变形测试。

Result: 实验结果表明，MT能有效暴露LLMs的错误行为，但同时也揭示了其在某些任务中的局限性。

Conclusion: MT（变形测试）为LLMs（大型语言模型）的测试提供了一种有效方法，能够在不依赖显式预言的情况下识别错误行为。研究通过大规模实验验证了MT的潜力，同时也指出了其局限性。

Abstract: Using large language models (LLMs) to perform natural language processing
(NLP) tasks has become increasingly pervasive in recent times. The versatile
nature of LLMs makes them applicable to a wide range of such tasks. While the
performance of recent LLMs is generally outstanding, several studies have shown
that they can often produce incorrect results. Automatically identifying these
faulty behaviors is extremely useful for improving the effectiveness of LLMs.
One obstacle to this is the limited availability of labeled datasets, which
necessitates an oracle to determine the correctness of LLM behaviors.
Metamorphic testing (MT) is a popular testing approach that alleviates this
oracle problem. At the core of MT are metamorphic relations (MRs), which define
relationships between the outputs of related inputs. MT can expose faulty
behaviors without the need for explicit oracles (e.g., labeled datasets). This
paper presents the most comprehensive study of MT for LLMs to date. We
conducted a literature review and collected 191 MRs for NLP tasks. We
implemented a representative subset (36 MRs) to conduct a series of experiments
with three popular LLMs, running approximately 560,000 metamorphic tests. The
results shed light on the capabilities and opportunities of MT for LLMs, as
well as its limitations.

</details>


### [141] [Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs](https://arxiv.org/abs/2511.02197)
*Shufan Wang,Xing Hu,Junkai Chen,Zhiyuan Pan,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出一个LLM置信度分析与增强框架，评估了提示策略优化和数学校准的有效性，结果显示DeepSeek-Reasoner表现最佳，混合策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在代码智能领域的广泛应用，其输出在代码推理任务中的可靠性和可控性受到越来越多的关注。置信度估计是评估这些方面的有效且便捷的方法。

Method: 本文提出了一个针对代码推理任务的LLM置信度分析与增强框架，包括提示策略优化和数学校准（如Platt Scaling）等技术的有效性评估。

Result: DeepSeek-Reasoner在各种任务中表现最佳，在ECE、Brier Score和Performance Score上分别比其他模型高出0.680、0.636和13.652。结合重新评估提示策略和Platt Scaling的混合策略在上述三个指标上比原始性能提高了0.541、0.628和15.084。

Conclusion: 该研究不仅为LLM辅助软件工程中的置信度应用提供了研究基础和技术参考，还为未来置信度机制的优化和工程部署指明了方向。

Abstract: With the widespread application of large language models (LLMs) in the field
of code intelligence, increasing attention has been paid to the reliability and
controllability of their outputs in code reasoning tasks. Confidence estimation
serves as an effective and convenient approach for evaluating these aspects.
This paper proposes a confidence analysis and enhancement framework for LLMs
tailored to code reasoning tasks. We conduct a comprehensive empirical study on
the confidence reliability of mainstream LLMs across different tasks, and
further evaluate the effectiveness of techniques such as prompt strategy
optimisation and mathematical calibration (e.g., Platt Scaling) in improving
confidence reliability. Our results show that DeepSeek-Reasoner achieves the
best performance across various tasks, outperforming other models by up to
$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance
Score, respectively. The hybrid strategy combining the reassess prompt strategy
and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$
over the original performance in the aforementioned three metrics. These
results indicate that models with reasoning capabilities demonstrate superior
confidence reliability, and that the hybrid strategy is the most effective in
enhancing the confidence reliability of various models. Meanwhile, we elucidate
the impact of different task complexities, model scales, and strategies on
confidence performance, and highlight that the confidence of current LLMs in
complex reasoning tasks still has considerable room for improvement. This study
not only provides a research foundation and technical reference for the
application of confidence in LLM-assisted software engineering, but also points
the way for future optimisation and engineering deployment of confidence
mechanisms.

</details>


### [142] [LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases](https://arxiv.org/abs/2511.02203)
*Gerhard Yu,Mithila Sivakumar,Alvine B. Belle,Soude Ghari,Song Wang,Timothy C. Lethbridge*

Main category: cs.SE

TL;DR: 利用LLM自动审核保证案例，DeepSeek-R1表现最优，但仍需人工参与。


<details>
  <summary>Details</summary>
Motivation: 传统保证案例审核繁琐且易错，需半自动化技术提升效率与准确性。

Method: 提出了基于谓词的规则来形式化审核标准，并利用LLM-as-a-judge范式自动审核。

Result: 实验表明DeepSeek-R1和GPT-4.1表现最佳，但人工复审不可或缺。

Conclusion: 尽管DeepSeek-R1和GPT-4.1在自动审核中表现优异，但仍需人工复审以确保审核质量。

Abstract: Assurance cases allow verifying the correct implementation of certain
non-functional requirements of mission-critical systems, including their
safety, security, and reliability. They can be used in the specification of
autonomous driving, avionics, air traffic control, and similar systems. They
aim to reduce risks of harm of all kinds including human mortality,
environmental damage, and financial loss. However, assurance cases often tend
to be organized as extensive documents spanning hundreds of pages, making their
creation, review, and maintenance error-prone, time-consuming, and tedious.
Therefore, there is a growing need to leverage (semi-)automated techniques,
such as those powered by generative AI and large language models (LLMs), to
enhance efficiency, consistency, and accuracy across the entire assurance-case
lifecycle. In this paper, we focus on assurance case review, a critical task
that ensures the quality of assurance cases and therefore fosters their
acceptance by regulatory authorities. We propose a novel approach that
leverages the \textit{LLM-as-a-judge} paradigm to automate the review process.
Specifically, we propose new predicate-based rules that formalize
well-established assurance case review criteria, allowing us to craft LLM
prompts tailored to the review task. Our experiments on several
state-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show
that, while most LLMs yield relatively good review capabilities, DeepSeek-R1
and GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately
outperforming GPT-4.1. However, our experimental results also suggest that
human reviewers are still needed to refine the reviews LLMs yield.

</details>


### [143] [SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks](https://arxiv.org/abs/2511.02352)
*Sanket Mhatre,Yasharth Bajpai,Sumit Gulwani,Emerson Murphy-Hill,Gustavo Soares*

Main category: cs.SE

TL;DR: 作者提出了首个针对C#的软件工程基准测试SWE-Sharp-Bench，发现AI编码代理在C#任务上的表现显著低于Python。


<details>
  <summary>Details</summary>
Motivation: C#作为主流企业语言（TIOBE指数排名第5），当前缺乏相应的软件工程基准测试，因此作者提出了SWE-Sharp-Bench。

Method: 通过评估相同的模型-代理配置在不同语言中的表现，揭示了Python和C#任务解决率的显著差距。

Result: 在SWE-Sharp-Bench中，C#任务的解决率仅为40%，而Python任务在SWE-Bench Verified中的解决率为70%。

Conclusion: 作者开源了SWE-Sharp-Bench及其整个数据整理流程，为C#语言的AI编码代理提供了可复现的基准测试。

Abstract: AI coding agents have shown great progress on Python software engineering
benchmarks like SWE-Bench, and for other languages like Java and C in
benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language
ranking #5 in the TIOBE index -- remains absent from such benchmarks. We
introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for
C\# featuring 150 instances from 17 repositories. Evaluating identical
model-agent configurations across languages reveals a significant performance
gap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of
our C\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire
curation pipeline.

</details>


### [144] [EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents](https://arxiv.org/abs/2511.02399)
*Junwei Liu,Chen Xu,Chong Wang,Tong Bai,Weitong Chen,Kaseng Wong,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: EvoDev是一种迭代式软件开发框架，通过特性映射和依赖建模提升LLM代理的开发效率，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用线性的瀑布式流程，过度简化了实际开发的迭代性质，难以应对复杂大规模项目。EvoDev旨在通过迭代式框架解决这些局限。

Method: EvoDev采用迭代式软件开发框架，将用户需求分解为一系列用户价值特性，并构建有向无环图（Feature Map）明确建模特性间的依赖关系。每个节点维护多层次信息（业务逻辑、设计、代码），并沿依赖关系传播以提供后续迭代的上下文。

Result: 在Android开发任务中，EvoDev比最佳基线Claude Code显著提升56.8%，并在不同基础LLM上使单代理性能提升16.0%-76.6%。

Conclusion: EvoDev框架通过依赖建模、上下文传播和工作流感知的代理设计，显著提高了复杂软件项目的开发效率，为迭代式LLM驱动开发框架的设计提供了实用见解，并指导未来基础LLM的训练以更好支持迭代式软件开发。

Abstract: Recent advances in large language model agents offer the promise of
automating end-to-end software development from natural language requirements.
However, existing approaches largely adopt linear, waterfall-style pipelines,
which oversimplify the iterative nature of real-world development and struggle
with complex, large-scale projects. To address these limitations, we propose
EvoDev, an iterative software development framework inspired by feature-driven
development. EvoDev decomposes user requirements into a set of user-valued
features and constructs a Feature Map, a directed acyclic graph that explicitly
models dependencies between features. Each node in the feature map maintains
multi-level information, including business logic, design, and code, which is
propagated along dependencies to provide context for subsequent development
iterations. We evaluate EvoDev on challenging Android development tasks and
show that it outperforms the best-performing baseline, Claude Code, by a
substantial margin of 56.8%, while improving single-agent performance by
16.0%-76.6% across different base LLMs, highlighting the importance of
dependency modeling, context propagation, and workflow-aware agent design for
complex software projects. Our work summarizes practical insights for designing
iterative, LLM-driven development frameworks and informs future training of
base LLMs to better support iterative software development.

</details>


### [145] [Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition](https://arxiv.org/abs/2511.02434)
*Dominik Fuchß,Haoyu Liu,Sophie Corallo,Tobias Hey,Jan Keim,Johannes von Geisau,Anne Koziolek*

Main category: cs.SE

TL;DR: LLMs enable automated SAM generation and TLR via ExArch and ArTEMiS, outperforming baselines without manual SAMs, making architecture-code traceability more practical.


<details>
  <summary>Details</summary>
Motivation: Manual creation of Software Architecture Models (SAMs) is time-consuming, and there is a need to automate the identification of architecturally relevant entities in textual artifacts for TLR.

Method: Two LLM-based approaches: ExArch extracts component names as simple SAMs from SAD and source code; ArTEMiS identifies architectural entities in documentation and matches them with SAM entities.

Result: ExArch achieves comparable results (F1: 0.86) to TransArC (F1: 0.87) without manual SAMs. ArTEMiS matches the performance of SWATTR (F1: 0.81) and can replace it when integrated with TransArC. The combination of ArTEMiS and ExArch outperforms ArDoCode.

Conclusion: LLMs can effectively identify architectural entities in textual artifacts, enabling automated Software Architecture Model (SAM) generation and Traceability Link Recovery (TLR), making architecture-code traceability more practical and accessible.

Abstract: Identifying architecturally relevant entities in textual artifacts is crucial
for Traceability Link Recovery (TLR) between Software Architecture
Documentation (SAD) and source code. While Software Architecture Models (SAMs)
can bridge the semantic gap between these artifacts, their manual creation is
time-consuming. Large Language Models (LLMs) offer new capabilities for
extracting architectural entities from SAD and source code to construct SAMs
automatically or establish direct trace links. This paper presents two
LLM-based approaches: ExArch extracts component names as simple SAMs from SAD
and source code to eliminate the need for manual SAM creation, while ArTEMiS
identifies architectural entities in documentation and matches them with
(manually or automatically generated) SAM entities. Our evaluation compares
against state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC
achieves strong performance (F1: 0.87) but requires manually created SAMs;
ExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS
is on par with the traditional heuristic-based SWATTR (F1: 0.81) and can
successfully replace it when integrated with TransArC. The combination of
ArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.
Our results demonstrate that LLMs can effectively identify architectural
entities in textual artifacts, enabling automated SAM generation and TLR,
making architecture-code traceability more practical and accessible.

</details>


### [146] [When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations](https://arxiv.org/abs/2511.02445)
*Eriks Klotins,Magnus Ahlgren,Nicolas Martin Vivaldi,Even-Andre Karlsson*

Main category: cs.SE

TL;DR: 本研究通过四个工业案例探讨了持续软件工程（CSE）采用的约束，提出了更新的准备模型，强调即使全面采用不可行，内部改进仍具价值。


<details>
  <summary>Details</summary>
Motivation: 持续软件工程（CSE）承诺在软件密集型组织中提高效率、质量和响应能力，但其全面采用常受复杂产品、遗留系统、组织惯性和法规要求的限制。

Method: 我们应用并扩展了先前提出的CSE行业准备模型，通过专家访谈和叙述综合，评估每个案例中的当前和潜在采用水平。

Result: 基于研究结果，我们提出了一个更新的准备模型，引入了额外的内部和外部反馈水平，区分了市场和组织的约束，并更好地指导实践者设定现实的CSE采用目标。

Conclusion: 研究表明，虽然全面端到端的持续软件工程（CSE）采用可能并不总是可行，但有意义的内部改进仍然是可能且有益的。本研究为组织在部分或受限的CSE转型中提供了实证指导。

Abstract: Purpose: Continuous Software Engineering (CSE) promises improved efficiency,
quality, and responsiveness in software-intensive organizations. However, fully
adopting CSE is often constrained by complex products, legacy systems,
organizational inertia, and regulatory requirements. In this paper, we examine
four industrial cases from the automation, automotive, retail, and chemical
sectors to explore how such constraints shape CSE adoption in practice.
Methods: We apply and extend a previously proposed CSE Industry Readiness Model
to assess the current and potential levels of adoption in each case. Through
expert interviews and narrative synthesis, we identify common driving forces
and adoption barriers, including organizational preparedness,
cross-organizational dependencies, and limited customer demand for continuous
delivery. Results: Based on our findings, we propose an updated readiness model
that introduces additional levels of internal and external feedback,
distinguishes market- and organization-facing constraints, and better guides
practitioners in setting realistic CSE adoption goals. Conclusions: Our results
highlight that while full end-to-end CSE adoption may not always be feasible,
meaningful internal improvements are still possible and beneficial. This study
provides empirically grounded guidance for organizations navigating partial or
constrained CSE transformations.

</details>


### [147] [Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering](https://arxiv.org/abs/2511.02475)
*Jürgen Cito,Dominik Bork*

Main category: cs.SE

TL;DR: 生成式AI通过自然语言提示快速生成软件，但也带来脆弱性问题。论文提出事后恢复模型的方法，以提升系统可持续性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI降低了软件创建的门槛，但也导致了系统脆弱性增加，缺乏鲁棒性、安全性和可维护性。

Method: 通过从AI生成的代码中事后恢复模型，以恢复理解、暴露风险并指导改进。

Result: 模型可以作为中介，帮助在AI生成的代码中恢复理解、暴露风险并指导改进。

Conclusion: 生成式AI驱动的软件工程需要重新构想软件模型，使其成为人类意图、AI生成和长期系统演进之间的中介，以支持可持续的AI驱动软件开发。

Abstract: Generative AI enables rapid ``vibe coding," where natural language prompts
yield working software systems. While this lowers barriers to software
creation, it also collapses the boundary between prototypes and engineered
software, leading to fragile systems that lack robustness, security, and
maintainability. We argue that this shift motivates a reimagining of software
models. Rather than serving only as upfront blueprints, models can be recovered
post-hoc from AI-generated code to restore comprehension, expose risks, and
guide refinement. In this role, models serve as mediators between human intent,
AI generation, and long-term system evolution, providing a path toward
sustainable AI-driven software engineering.

</details>


### [148] [ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation](https://arxiv.org/abs/2511.02713)
*Qianru Meng,Zhaochun Ren,Joost Visser*

Main category: cs.SE

TL;DR: 发布说明自动生成面临数据集和任务设计限制。ReleaseEval基准引入三种任务设置，评估显示大型语言模型在结构化信息处理上优于传统方法，但长代码差异处理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 解决频繁软件更新文档化的挑战，现有数据集和任务设计的局限性（如缺乏明确许可、依赖提交消息而忽略细粒度上下文）阻碍进展。

Method: 引入ReleaseEval基准，包含94,987条发布说明，支持三种任务设置（commit2sum、tree2sum、diff2sum），并通过自动和人工评估比较模型性能。

Result: 大型语言模型在所有任务中均优于传统基线，尤其在tree2sum任务中表现突出，但在diff2sum任务中仍有困难。

Conclusion: 大型语言模型在利用结构化信息方面表现优异，但在处理长代码差异时仍面临挑战。

Abstract: Automated release note generation addresses the challenge of documenting
frequent software updates, where manual efforts are time-consuming and prone to
human error. Although recent advances in language models further enhance this
process, progress remains hindered by dataset limitations, including the lack
of explicit licensing and limited reproducibility, and incomplete task design
that relies mainly on commit messages for summarization while overlooking
fine-grained contexts such as commit hierarchies and code changes. To fill this
gap, we introduce ReleaseEval, a reproducible and openly licensed benchmark
designed to systematically evaluate language models for automated release note
generation. ReleaseEval comprises 94,987 release notes from 3,369 repositories
across 6 programming languages, and supports three task settings with three
levels of input granularity: (1) commit2sum, which generates release notes from
commit messages; (2) tree2sum, which incorporates commit tree structures; and
(3) diff2sum, which leverages fine-grained code diffs. Both automated and human
evaluations show that large language models consistently outperform traditional
baselines across all tasks, achieving substantial gains on tree2sum, while
still struggling on diff2sum. These findings highlight LLMs' proficiency in
leveraging structured information while revealing challenges in abstracting
from long code diffs.

</details>


### [149] [Investigating the Experience of Autistic Individuals in Software Engineering](https://arxiv.org/abs/2511.02736)
*Madalena Sasportes,Grischa Liebel,Miguel Goulão*

Main category: cs.SE

TL;DR: 研究发现自闭症软件工程师在逻辑思维、细节关注和编程专注度方面有优势，偏好书面沟通和远程工作，并对AI系统互动感到舒适。


<details>
  <summary>Details</summary>
Motivation: 改善自闭症成年人的社会融入，探索其在软件工程活动中的优势。

Method: 本研究结合了社会技术扎根理论，通过半结构化访谈（16名自闭症软件工程师）和调查（49名受访者，包括5名自闭症参与者），并将新兴主题与Gama等人关于神经多样性认知功能障碍对软件工程绩效影响的理论进行比较。

Result: 自闭症软件工程师通常在逻辑思维、细节关注和编程专注度方面表现出色；他们喜欢学习新的编程语言和相关技术。确认了之前的研究，他们更倾向于书面沟通和远程工作。此外，报告显示他们对基于AI的系统互动有较高的舒适度。

Conclusion: 研究结果扩展了现有工作，进一步证明了自闭症软件工程师的优势。

Abstract: Context: Autism spectrum disorder (ASD) leads to various issues in the
everyday life of autistic individuals, often resulting in unemployment and
mental health problems. To improve the inclusion of autistic adults, existing
studies have highlighted the strengths these individuals possess in comparison
to non-autistic individuals, e.g., high attention to detail or excellent
logical reasoning skills. If fostered, these strengths could be valuable in
software engineering activities, such for identifying specific kinds of bugs in
code. However, existing work in SE has primarily studied the challenges of
autistic individuals and possible accommodations, with little attention their
strengths. Objective: Our goal is to analyse the experiences of autistic
individuals in software engineering activities, such as code reviews, with a
particular emphasis on strengths. Methods: This study combines Social-Technical
Grounded Theory through semi-structured interviews with 16 autistic software
engineers and a survey with 49 respondents, including 5 autistic participants.
We compare the emerging themes with the theory by Gama et al. on the Effect of
Neurodivergent Cognitive Dysfunctions in Software Engineering Performance.
Results: Our results suggest that autistic software engineers are often skilled
in logical thinking, attention to detail, and hyperfocus in programming; and
they enjoy learning new programming languages and programming-related
technologies. Confirming previous work, they tend to prefer written
communication and remote work. Finally, we report a high comfort level in
interacting with AI-based systems. Conclusions: Our findings extend existing
work by providing further evidence on the strengths of autistic software
engineers.

</details>


### [150] [Formalizing Regression Testing for Agile and Continuous Integration Environments](https://arxiv.org/abs/2511.02810)
*Suddhasvatta Das,Kevin Gary*

Main category: cs.SE

TL;DR: 该研究形式化了连续回归测试，将其建模为时间有序的构建链，并验证了两种敏捷回归测试算法的正确性，填补了经典理论的空白。


<details>
  <summary>Details</summary>
Motivation: 现代敏捷开发实践要求持续回归测试，而经典回归测试理论假设测试在交付或维护阶段进行一次。为了填补这一理论空白，研究旨在形式化连续回归测试现象。

Method: 研究将连续回归测试形式化为一个时间有序的构建链，每个构建包含程序、需求和测试，并定义了回归测试窗口。通过将两种先进的敏捷回归测试算法表示为构建元组操作，验证了形式化的正确性。

Result: 研究成功地将连续回归测试形式化为构建链，并验证了两种敏捷回归测试算法在模型中的正确性，证明了形式化的完备性和可靠性。

Conclusion: 该研究通过将连续回归测试形式化为一个时间有序的构建链，并定义了回归测试窗口，成功地将经典的双版本案例纳入模型，验证了其形式化的正确性。

Abstract: Software developed using modern agile practices delivers a stream of software
versions that require continuous regression testing rather than testing once
close to the delivery or maintenance phase, as assumed by classical
regression-testing theory. In this work, we formalize the phenomenon of
continuous or near-continuous regression testing using successive builds as a
time-ordered chain, where each build contains the program, requirements, and
the accompanying tests. We also formalize the regression test window between
any two builds, which captures the limited time budget available for regression
testing. As the time limit is set to infinity and the chain is closed to two
builds, the model degenerates to retest-all, thereby preserving semantics for
the classical two-version case. The formalization is validated by directly
representing two state-of-the-art agile regression testing algorithms in terms
of build-tuple operations without requiring auxiliary assumptions, followed by
proof of the soundness and completeness of our formalization.

</details>


### [151] [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](https://arxiv.org/abs/2511.02827)
*Mohamed Almukhtar,Anwar Ghammam,Marouane Kessentini,Hua Ming*

Main category: cs.SE

TL;DR: 研究填补了机器学习系统代码变更与质量关系的知识空白，开发了PyQu工具，识别出61种质量提升的代码变更，为自动化质量评估提供了基础。


<details>
  <summary>Details</summary>
Motivation: 在生成式人工智能代码和Python机器学习系统日益普及的背景下，软件质量问题日益突出。现有研究缺乏对代码变更与质量关系的深入理解，且缺乏质量评估工具。

Method: 研究团队对3,340个开源Python机器学习项目进行了大规模实证研究，涵盖超过370万次提交和2.7万亿行代码。他们开发了PyQu工具，利用低级软件指标识别质量提升的提交。

Result: 研究发现61种直接影响软件质量的代码变更，并将其分类为13个类别。其中41%的变更是现有Python变更检测工具未发现的新发现。PyQu工具的平均准确率、精确率和召回率为0.84，平均F1得分为0.85。

Conclusion: 该研究为研究者、从业者、教育者和工具开发者提供了重要基础，推动了Python机器学习软件自动化质量评估和最佳实践的探索。

Abstract: In an era shaped by Generative Artificial Intelligence for code generation
and the rising adoption of Python-based Machine Learning systems (MLS),
software quality has emerged as a major concern. As these systems grow in
complexity and importance, a key obstacle lies in understanding exactly how
specific code changes affect overall quality-a shortfall aggravated by the lack
of quality assessment tools and a clear mapping between ML systems code changes
and their quality effects. Although prior work has explored code changes in
MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of
the relationship between code changes and the MLS quality. To address this gap,
we conducted a large-scale empirical study of 3,340 open-source Python ML
projects, encompassing more than 3.7 million commits and 2.7 trillion lines of
code. We introduce PyQu, a novel tool that leverages low level software metrics
to identify quality-enhancing commits with an average accuracy, precision, and
recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic
analysis, we identified 61 code changes, each demonstrating a direct impact on
enhancing software quality, and we classified them into 13 categories based on
contextual characteristics. 41% of the changes are newly discovered by our
study and have not been identified by state-of-the-art Python changes detection
tools. Our work offers a vital foundation for researchers, practitioners,
educators, and tool developers, advancing the quest for automated quality
assessment and best practices in Python-based ML software.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [152] [LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency](https://arxiv.org/abs/2511.01894)
*Fangbing Liu,Pengfei Duan,Wen Li,Yi He*

Main category: cs.GR

TL;DR: LGCC通过LGNC和CCL优化了图像编辑的细节保留和效率，比BAGEL更快且效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流匹配的多模态大语言模型（如BAGEL）在图像编辑中存在细节退化、内容不一致和效率低下的问题。

Method: 提出了LGCC框架，包含LGNC和CCL两个关键组件，通过课程学习与预训练模型BAGEL集成。

Result: 在I2EBench上，LGCC将局部细节分数提高了1.60%，整体分数提高了0.53%，推理速度提升了3x-5x（轻量级编辑）和2x（通用编辑），推理时间仅为BAGEL或Flux的40%-50%。

Conclusion: LGCC框架通过结合局部高斯噪声耦合（LGNC）和内容一致性损失（CCL），显著提升了图像编辑的质量和效率，同时保持了细节和语义一致性。

Abstract: Recent advancements have demonstrated the great potential of flow
matching-based Multimodal Large Language Models (MLLMs) in image editing.
However, state-of-the-art works like BAGEL face limitations, including detail
degradation, content inconsistency, and inefficiency due to their reliance on
random noise initialization. To address these issues, we propose LGCC, a novel
framework with two key components: Local Gaussian Noise Coupling (LGNC) and
Content Consistency Loss (CCL). LGNC preserves spatial details by modeling
target image embeddings and their locally perturbed counterparts as coupled
pairs, while CCL ensures semantic alignment between edit instructions and image
modifications, preventing unintended content removal. By integrating LGCC with
the BAGEL pre-trained model via curriculum learning, we significantly reduce
inference steps, improving local detail scores on I2EBench by 1.60% and overall
scores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x
for universal editing, requiring only 40% -- 50% of the inference time of BAGEL
or Flux. These results demonstrate LGCC's ability to preserve detail, maintain
contextual integrity, and enhance inference speed, offering a cost-efficient
solution without compromising editing quality.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [153] [Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live](https://arxiv.org/abs/2511.02230)
*Hanchen Li,Qiuyang Mang,Runyuan He,Qizheng Zhang,Huanzhi Mao,Xiaokun Chen,Alvin Cheung,Joseph Gonzalez,Ion Stoica*

Main category: cs.OS

TL;DR: Continuum是一个优化多轮代理工作负载的系统，通过工具感知的KV缓存和程序级调度提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决代理性LLM应用中工具调用导致的KV缓存失效和多轮连续性中断问题，优化服务系统的性能和延迟。

Method: 结合工具感知的KV缓存超时和程序级调度，预测工具调用持续时间并选择性固定KV缓存。

Result: 在真实代理工作负载（SWE-Bench和BFCL）上评估显示，Continuum显著改善了平均作业完成时间。

Conclusion: Continuum系统通过工具感知的KV缓存超时和程序级调度，显著提升了多轮代理工作负载的作业完成时间，并在不同硬件设置和DRAM卸载方案中保持高性能。

Abstract: Agentic LLM applications interleave LLM generation requests with tool calls.
These tool calls break the continuity of the workflow by creating pauses
between LLM requests, bringing many challenges for the serving system,
especially under multi-turn scenarios. Each pause potentially causes KV cache
eviction and extra waiting time before entering the continuous batch for the
following LLM request. Since these pauses happen for each call, this problem
becomes increasingly severe as turn number grow for agentic programs. Previous
works either fail to incorporate information from the tool call, evicting KV
cache that leads to repetitive prefill or loading, or ignore the continuity of
a multi-turn program, creating waiting time between turns that increases
per-request latency.
  We present Continuum, a serving system to optimize job completion time for
multi-turn agent workloads by combining tool-aware KV cache timeout with
program-level scheduling. By predicting tool call durations in agentic
workflows, Continuum selectively pins the KV cache in GPU memory with a
time-to-live value based on total turn number. When combined with program-level
first-come-first-serve, Continuum prevents scheduling bubbles, preserves
multi-turn continuity, and optimizes for throughput for complex agentic
workflows. By modeling the variability of tool call and agent program
continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on
real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models
shows that Continuum significantly improves the average job completion times,
and remains performant across different hardware setups and DRAM offloading
schemes. Preview code is available at:
https://github.com/Hanchenli/vllm-continuum

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [154] [Nonlinear Instabilities in Computer Network Dynamics](https://arxiv.org/abs/2511.01886)
*Priya Ranjan*

Main category: cs.NI

TL;DR: 论文研究了TCP-RED网络和速率控制模型的动态行为，揭示了非线性现象和稳定性问题，为网络优化提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解实践中由于严重非线性、延迟和广泛变化的操作条件导致的网络动态现象，为网络协议的设计和改进提供理论支持。

Method: 研究采用了一阶非线性离散时间模型来分析TCP、UDP与RED等主动队列管理方案的交互，并利用分岔理论解释了稳定性丧失的机制；同时通过延迟微分方程稳定性结果，研究了速率控制模型的稳定性。

Result: 研究发现TCP-RED网络的动态行为与实践中观察到的参数敏感性一致，并揭示了由于吞吐量函数的非线性依赖和缓冲空间限制导致的周期性分岔现象；速率控制模型的稳定性分析则提供了延迟无关稳定性和周期性振荡的边界。

Conclusion: 该论文通过研究TCP-RED网络模型和基于Kelly的速率控制模型，揭示了网络动态行为中的非线性现象及其稳定性问题，为网络协议设计和优化提供了理论依据。

Abstract: This work studies two types of computer networking models. The primary focus
is to understand the different dynamical phenomena observed in practice due to
the presence of severe nonlinearities, delays and widely varying operating
conditions. The first models considered are of senders running TCP
(Transmission Control Protocol) and traffic passing through RED (Random Early
Detection) gateways. Building on earlier work, a first order nonlinear
discrete-time model is developed for the interaction scenario between transport
protocols like TCP and UDP (User Datagram Protocol) and Active Queuing
Management schemes like RED. It is shown that the dynamics resulting from the
interaction with TCP is consistent with various dynamical behaviors and
parameter sensitivities observed in practice. Using bifurcation-theoretic ideas
it is shown that TCP-RED type networks may lose their stability through a
period doubling bifurcation followed by border collision bifurcations. The
nonlinear dependence of the throughput function of TCP-type flows on drop
probability is found to be responsible for the period doubling bifurcation,
whereas limited buffer space and lack of sufficient damping results in border
collision bifurcations. A second class of models studied in this work deals
with optimal rate control in networks and are based on the rate-control
framework proposed by Kelly. Using the results on delay-differential equation
stability, the stability and its lack thereof is studied through an underlying
map which arises naturally in time delay systems. An invariance property of
this map is used to prove delay-independent stability and to compute bounds on
periodic oscillations.

</details>


### [155] [A Modular DTaaS Architecture for Predictive Slice Management in 6G Systems](https://arxiv.org/abs/2511.01989)
*Tuğçe Bilen,Mehmet Özdem*

Main category: cs.NI

TL;DR: 提出了基于数字孪生的服务框架（DTaaS），通过实时预测和模块化智能层提升6G网络切片的编排效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要新的编排范式以满足超低延迟、高可靠性和普适智能的严格要求，网络切片成为支持多样化服务的关键。

Method: 提出了一种新颖的DTaaS框架，嵌入每片数字孪生（SDTs）到编排循环中，利用多域遥测和深度序列模型预测流量演变和SLA风险。

Result: 评估结果表明，DTaaS显著提高了SLA合规率、减少了资源过度配置，并降低了平均SLA违规概率。

Conclusion: DTaaS框架显著提高了SLA合规率、减少了资源过度配置，并降低了平均SLA违规概率，为6G网络提供了一种可扩展且可靠的编排方法。

Abstract: The sixth generation (6G) of wireless networks will require fundamentally new
orchestration paradigms to meet stringent requirements for ultra-low latency,
high reliability, and pervasive intelligence. Network slicing emerges as a key
enabler to support diverse services with customized quality-of-service (QoS)
guarantees. However, dynamic and fine-grained slice management poses
significant challenges in terms of real-time provisioning, SLA assurance, and
cross-layer observability. In this paper, we propose a novel Digital Twin as a
Service (DTaaS) framework that embeds per-slice digital twins (SDTs) into the
orchestration loop. Each SDT maintains a synchronized, real-time representation
of its slice, leveraging multi-domain telemetry and deep sequential models to
predict traffic evolution and SLA risks. The framework introduces modular
intelligence layers, programmable interfaces, and edge-embedded decision-making
to enable proactive provisioning, adaptive scaling, and closed-loop SLA
assurance. Mathematical formulations for fidelity measurement, predictive
control, and optimization objectives are provided to ensure rigor and
transparency. Evaluation results demonstrate that DTaaS significantly improves
SLA compliance ratio, reduces resource over-provisioning, and lowers average
SLA violation probability, offering a scalable and reliable orchestration
approach for 6G networks.

</details>


### [156] [Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM](https://arxiv.org/abs/2511.02171)
*Rodrigo Nunes,André Melo,Rafael Albarello,Reinaldo Gomes,Cesar Marcondes,Lourenço Pereira Jr*

Main category: cs.NI

TL;DR: 论文比较了两种符合监管的UTM架构（InterUSS和Hyperledger Fabric），发现区块链系统需针对航空性能特别设计。


<details>
  <summary>Details</summary>
Motivation: 随着无人机（UAVs）的快速普及，航空当局提出了分布式无人机交通管理（UTM）架构。区块链技术被广泛认为有潜力满足这些需求，但由于UTM是安全关键且高度监管的领域，合规性同样重要。

Method: 本研究对两种符合当前监管框架的分布式架构进行了基准测试：Linux Foundation的InterUSS平台和基于Hyperledger Fabric的私有账本。

Result: 研究发现，基于区块链的系统需要专门针对航空性能约束设计的架构。

Conclusion: 区块链为基础的无人机交通管理系统需要针对航空性能约束特别设计架构。

Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation
authorities to propose distributed Uncrewed Traffic Management (UTM)
architectures. Several studies have advocated blockchain as a promising
technology to meet these requirements. However, since UTM is a safety-critical
and highly regulated domain, compliance with standards and regulatory
frameworks is as crucial as performance and security. This work benchmarks two
distributed architectures aligned with current regulatory frameworks: the Linux
Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger.
Our findings reveal that blockchain-based systems require architectures
specifically designed for aeronautical performance constraints.

</details>


### [157] [Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains](https://arxiv.org/abs/2511.02368)
*Rushi Moliya,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez*

Main category: cs.NI

TL;DR: 本文提出了一种基于遗传算法（GA）和粒子群优化（PSO）的分层启发式框架，用于优化多无人机（UAV）在复杂地形中的协同感知性能，显著提高了检测概率并降低了悬停能耗。


<details>
  <summary>Details</summary>
Motivation: 在复杂地形中，地形导致的视线（LoS）遮挡会降低无人机对目标的检测性能，因此需要一种高效的方法来优化无人机的部署和能量使用。

Method: 结合二进制LoS指示器和边界体积层次结构（BHV）的自适应方案，提出了一种分层启发式框架，通过GA进行全局探索，PSO进行局部优化，并使用惩罚性适应度评估确保解的可行性。

Result: 在真实地形数据的蒙特卡洛模拟中，GA+PSO框架相比纯PSO方案，检测概率提高了37.02%（2 UAVs）和36.5%（3 UAVs），平均悬停能耗降低了45.0%和48.9%。

Conclusion: 该方法在少量无人机和复杂地形条件下，显著提升了检测概率并降低了能耗，证明了其有效性。

Abstract: In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative
sensing system where UAVs are deployed to sense multiple targets in
terrain-aware line of sight (LoS) conditions in uneven terrain equipped with
directional antennas. To mitigate terrain-induced LoS blockages that degrade
detection performance, we incorporate a binary LoS indicator and propose a
bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS
evaluation. We formulate a bi-objective problem that maximizes the probability
of cooperative detection with minimal hover energy constraints governing
spatial, orientational, and safety constraints. To address the problem, which
is inherently non-convex, we propose a hierarchical heuristic framework that
combines exploration through a genetic algorithm (GA) with per-UAV refinement
via particle swarm optimization (PSO), where a penalty-based fitness evaluation
guides solutions toward feasibility, bounded within constraints. The proposed
methodology is an effective trade-off method of traversing through a complex
search space and maintaining terrain-aware LoS connectivity and energy aware
deployment. Monte Carlo simulations on real-world terrain data show that the
proposed GA+PSO framework improves detection probability by 37.02% and 36.5%
for 2 and 3 UAVs, respectively, while reducing average excess hover energy by
45.0% and 48.9% compared to the PSO-only baseline. Relative to the
non-optimized scheme, it further achieves 59.5% and 54.2% higher detection
probability with 59.8% and 65.9% lower excess hover energy, thereby showing its
effectiveness with a small number of UAVs over uneven terrain.

</details>


### [158] [Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach](https://arxiv.org/abs/2511.02501)
*Mohan Liyanage,Eldiyar Zhantileuov,Ali Kadhum Idrees,Rolf Schuster*

Main category: cs.NI

TL;DR: 提出一种轻量级延迟预测方案，通过理性建模实现高精度预测，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测端到端网络延迟对实时边缘计算应用中的可靠任务卸载至关重要。

Method: 基于理性建模的轻量级延迟预测方案，利用帧大小、到达率和链路利用率等特征，无需侵入性主动探测。

Result: 通过广泛实验和5折交叉验证，模型实现了最先进的预测精度（MAE = 0.0115，R² = 0.9847），推理时间具有竞争力。

Conclusion: 该论文提出的轻量级延迟预测方案通过理性建模实现了高精度预测，并在精度与效率之间取得了显著平衡，优于传统回归器和神经网络。

Abstract: Accurately predicting end-to-end network latency is essential for enabling
reliable task offloading in real-time edge computing applications. This paper
introduces a lightweight latency prediction scheme based on rational modelling
that uses features such as frame size, arrival rate, and link utilization,
eliminating the need for intrusive active probing. The model achieves
state-of-the-art prediction accuracy through extensive experiments and 5-fold
cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference
time, offering a substantial trade-off between precision and efficiency
compared to traditional regressors and neural networks.

</details>


### [159] [Janus: Leveraging Incremental Computation for Efficient DNS Verification](https://arxiv.org/abs/2511.02559)
*Yao Wang,Kexin Yu,Wenyun Xu,Kaiqiang Hu,Ziyi Wang,Lizhao You,Qiang Su,Dong Guo,Haizhou Du,Wanjian Feng,Qingyu Song,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: Janus是一种高效DNS验证工具，通过匹配动作表转换查询处理，支持增量验证，实验显示其性能显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有DNS配置验证工具存在效率低下和缺乏增量验证支持等问题，Janus受分布式数据平面验证技术启发，旨在解决DNS配置错误问题。

Method: Janus包括：(1)基于行为划分查询空间的高效数据结构；(2)符号执行算法确保单台名称服务器覆盖所有可能查询并保证验证准确性；(3)支持增量验证的机制以减少计算开销。

Result: 在真实数据集（超过600万条资源记录）上的实验表明，Janus实现了显著加速，峰值提升达255.7倍，LEC数量最多减少6046倍。

Conclusion: Janus作为一种DNS配置验证工具，通过将查询处理转化为匹配动作表的过程，显著提升了验证效率和增量验证支持，实验数据证实了其高效性。

Abstract: Existing DNS configuration verification tools face significant issues (e.g.,
inefficient and lacking support for incremental verification). Inspired by the
advancements in recent work of distributed data plane verification and the
resemblance be- tween the data plane and DNS configuration, we tackle the
challenge of DNS misconfiguration by introducing Janus, a DNS verification
tool. Our key insight is that the process of a nameserver handling queries can
be transformed into a matching process on a match-action table. With this
insight, Janus consists of (1) an efficient data structure for partition query
space based on the behaviors, (2) a symbolic execution algorithm that specifies
how a single nameserver can efficiently cover all possible queries and ensure
the accuracy of verification, (3) a mechanism to support incremental
verification with less computational effort. Extensive experiments on
real-world datasets (with over 6 million resource records) show that Janus
achieves significant speedups, with peak improvements of up to 255.7x and a
maximum 6046x reduction in the number of LECs.

</details>


### [160] [Decentralized AI Service Placement, Selection and Routing in Mobile Networks](https://arxiv.org/abs/2511.02638)
*Jinkun Zhang,Stefan Vlaski,Kin Leung*

Main category: cs.NI

TL;DR: 论文提出了一种去中心化框架，优化AI服务放置、选择和请求路由，显著改善了服务质量和延迟的权衡，尤其在用户移动性场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的快速发展与移动用户的广泛使用将主导未来通信网络的流量负载，而现有移动边缘计算和数据密集型网络的解决方案因对网络结构或用户移动性的限制性假设而不足。

Method: 使用流量隧道支持用户移动性而无需昂贵的AI服务迁移，并制定了非凸问题以优化服务质量与端到端延迟之间的权衡。推导节点级KKT条件并开发了一种去中心化的Frank-Wolfe算法与新型消息协议。

Result: 数值评估验证了所提方法的有效性，并显示出相较于现有方法的显著性能提升。

Conclusion: 该论文提出了一种去中心化框架，通过联合优化AI服务放置、选择和请求路由，显著提升了服务质量和端到端延迟之间的权衡效果，特别是在考虑用户移动性时表现优异。

Abstract: The rapid development and usage of large-scale AI models by mobile users will
dominate the traffic load in future communication networks. The advent of AI
technology also facilitates a decentralized AI ecosystem where small
organizations or even individuals can host AI services. In such scenarios, AI
service (models) placement, selection, and request routing decisions are
tightly coupled, posing a challenging yet fundamental trade-off between service
quality and service latency, especially when considering user mobility.
Existing solutions for related problems in mobile edge computing (MEC) and
data-intensive networks fall short due to restrictive assumptions about network
structure or user mobility. To bridge this gap, we propose a decentralized
framework that jointly optimizes AI service placement, selection, and request
routing. In the proposed framework, we use traffic tunneling to support user
mobility without costly AI service migrations. To account for nonlinear queuing
delays, we formulate a nonconvex problem to optimize the trade-off between
service quality and end-to-end latency. We derive the node-level KKT conditions
and develop a decentralized Frank--Wolfe algorithm with a novel messaging
protocol. Numerical evaluations validate the proposed approach and show
substantial performance improvements over existing methods.

</details>


### [161] [CRRM: A 5G system-level simulator](https://arxiv.org/abs/2511.02692)
*Keith Briggs,Ibrahim Nur*

Main category: cs.NI

TL;DR: CRRM是一种开源Python模拟器，通过创新的架构和智能更新机制，填补了5G及未来无线网络算法开发中的工具缺口。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究社区在5G及未来无线网络算法开发中缺乏合适的工具，需要一种快速、易用且能与现代AI框架直接集成的模拟器。

Method: CRRM采用了一种不同于传统离散事件模拟的架构，将系统建模为一组相互依赖的计算块，这些块构成有向图的节点，从而实现了智能更新机制。

Result: CRRM通过其创新架构和智能更新机制，提供了一种高效、易用的解决方案，满足了研究社区的需求。

Conclusion: CRRM作为一种开源、纯Python的模拟器，通过其创新的架构和智能更新机制，有效填补了机器学习研究社区在5G及未来无线网络算法开发中的工具缺口。

Abstract: System-level simulation is indispensable for developing and testing novel
algorithms for 5G and future wireless networks, yet a gap persists between the
needs of the machine learning re- search community and the available tooling.
To address this, we introduce the Cellular Radio Reference Model (CRRM), an
open-source, pure Python simulator we designed specifically for speed,
usability, and direct integration with modern AI frameworks. The core
scientific contribution of CRRM lies in its architecture, which departs from
traditional discrete-event simulation. We model the system as a set of
inter-dependent computational blocks which form nodes in a directed graph. This
enables a compute-on-demand mechanism we term smart update.

</details>


### [162] [On the Optimization of Model Aggregation for Federated Learning at the Network Edge](https://arxiv.org/abs/2511.02703)
*Mengyao Li,Noah Ploch,Sebastian Troia,Carlo Spatocco,Wolfgang Kellerer,Guido Maier*

Main category: cs.NI

TL;DR: 论文提出了一种基于边缘聚合器的资源管理策略，利用ILP模型和启发式算法优化路由，显著降低联邦学习的训练失败率和网络拥塞。


<details>
  <summary>Details</summary>
Motivation: 现代电信网络中连接设备的快速增加显著加剧了计算和通信需求，需要将先进的机器学习技术与新兴范式（如多接入边缘计算和软件定义广域网）结合以应对挑战。

Method: 论文介绍了在线资源管理策略，特别设计了用于联邦学习模型聚合的策略，利用边缘节点的中间聚合。提出了整数线性规划（ILP）模型和启发式算法来优化覆盖网络中的路由。

Result: 提出的解决方案显著提高了网络资源利用率的适应性，联邦学习训练回合失败率降低了15%，同时缓解了云链路拥塞。

Conclusion: 论文提出了一个新颖的聚合方法，通过部署聚合器覆盖网络，显著降低了联邦学习训练回合的失败率，并缓解了云链路拥塞。

Abstract: The rapid increase in connected devices has signifi- cantly intensified the
computational and communication demands on modern telecommunication networks.
To address these chal- lenges, integrating advanced Machine Learning (ML)
techniques like Federated Learning (FL) with emerging paradigms such as
Multi-access Edge Computing (MEC) and Software-Defined Wide Area Networks
(SD-WANs) is crucial. This paper intro- duces online resource management
strategies specifically designed for FL model aggregation, utilizing
intermediate aggregation at edge nodes. Our analysis highlights the benefits of
incorporating edge aggregators to reduce network link congestion and maximize
the potential of edge computing nodes. However, the risk of network congestion
persists. To mitigate this, we propose a novel aggregation approach that
deploys an aggregator overlay network. We present an Integer Linear Programming
(ILP) model and a heuristic algorithm to optimize the routing within this
overlay network. Our solution demonstrates improved adapt- ability to network
resource utilization, significantly reducing FL training round failure rates by
up to 15% while also alleviating cloud link congestion.

</details>


### [163] [Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](https://arxiv.org/abs/2511.02748)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: 6G智能的关键是模拟和选择能力。WM-MS3M模型通过世界建模和改进控制方法，显著提升了O-RAN预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 重新定义6G智能，强调其想象和选择的能力，而非传统的令牌预测。通过改进O-RAN的近实时控制，提出一种能够进行定量“假设”预测的世界建模方法。

Method: 采用世界建模（WM）范式，结合多尺度结构化状态空间混合物（MS3M）和随机潜在变量，构建了WM-MS3M模型。该方法利用模型预测控制（MPC）和交叉熵方法（CEM）进行短期规划，并通过数据驱动的物理资源块（PRB）边界优化确定性奖励。

Result: 在真实O-RAN数据上，WM-MS3M相比MS3M减少了1.69%的平均绝对误差（MAE），参数减少了32%，延迟相似。与注意力/混合基线相比，RMSE降低了35-80%，推理速度提高了2.3-4.1倍。

Conclusion: 6G智能的核心在于模拟未来情景、权衡利弊并采取行动的能力，而非简单的令牌预测。通过世界建模范式和改进的控制方法，WM-MS3M在O-RAN控制中显著提升了预测准确性和效率。

Abstract: We argue that sixth-generation (6G) intelligence is not fluent token
prediction but the capacity to imagine and choose -- to simulate future
scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe
open radio access network (O-RAN) near-real-time (Near-RT) control via
counterfactual dynamics and a world modeling (WM) paradigm that learns an
action-conditioned generative state space. This enables quantitative "what-if"
forecasting beyond large language models (LLMs) as the primary modeling
primitive. Actions such as physical resource blocks (PRBs) are treated as
first-class control inputs in a causal world model, and both aleatoric and
epistemic uncertainty are modeled for prediction and what-if analysis. An
agentic, model predictive control (MPC)-based cross-entropy method (CEM)
planner operates over short horizons, using prior-mean rollouts within
data-driven PRB bounds to maximize a deterministic reward. The model couples
multi-scale structured state-space mixtures (MS3M) with a compact stochastic
latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories
and predicting next-step KPIs under hypothetical PRB sequences. On realistic
O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with
32% fewer parameters and similar latency, and achieves 35-80% lower root mean
squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster
inference, enabling rare-event simulation and offline policy screening.

</details>
