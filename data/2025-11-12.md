<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 117]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.AI](#cs.AI) [Total: 69]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs](https://arxiv.org/abs/2511.07429)
*Hari Lee*

Main category: cs.CV

TL;DR: TbVAD是一种纯文本驱动的视频异常检测框架，通过语言转换和结构化知识生成可解释的异常检测结果。


<details>
  <summary>Details</summary>
Motivation: 提出一种语言驱动的弱监督视频异常检测框架，解决传统依赖显式视觉特征的模型在解释性上的不足。

Method: 1. 使用视觉语言模型将视频内容转换为细粒度字幕；2. 将字幕组织为四个语义槽（动作、对象、上下文、环境）构建结构化知识；3. 生成槽级解释，揭示哪些语义因素对异常决策贡献最大。

Result: 在UCF-Crime和XD-Violence两个公开基准上验证了TbVAD的有效性，展示了文本知识推理的可解释性和可靠性。

Conclusion: TbVAD框架通过纯文本方式实现了视频异常检测与解释，提供了一种可解释且基于知识的推理方法，适用于现实监控场景。

Abstract: We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.

</details>


### [2] [Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM](https://arxiv.org/abs/2511.07438)
*Joe Kileel,Oscar Mickelin,Amit Singer,Sheng Xu*

Main category: cs.CV

TL;DR: 提出了一种新数据融合框架MoDM，利用不同实验条件下的二阶矩数据，显著提升了分子结构重建质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决从噪声投影图像中重建三维分子结构的挑战，并探索数据集多样性对重建质量的提升。

Method: 提出了双矩方法（MoDM），通过结合均匀和非均匀方向分布下的投影图像的二阶矩，开发了一种基于凸松弛的算法。

Result: 证明了这些矩通常能唯一确定底层结构（全局旋转和反射除外），并通过实验验证了算法的准确性。

Conclusion: 利用MoDM框架，通过融合不同实验条件下的二阶矩数据，能够显著提升计算成像任务中的重建质量。

Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.

</details>


### [3] [Modulo Video Recovery via Selective Spatiotemporal Vision Transformer](https://arxiv.org/abs/2511.07479)
*Tianyu Geng,Feng Ji,Wee Peng Tay*

Main category: cs.CV

TL;DR: SSViT是一种基于Transformer的深度学习框架，专为模数视频重建设计，通过令牌选择策略提升效率，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统图像传感器动态范围有限，模数相机通过折叠入射辐照度解决此问题，但需专用展开算法。现有HDR方法不适用于模数恢复，而Transformer能捕捉全局依赖和时空关系，适合解决折叠视频帧问题。

Method: 提出了选择性时空视觉Transformer（SSViT），利用令牌选择策略提升效率并聚焦关键区域，适用于模数视频重建。

Result: SSViT能从8位折叠视频中高质量重建，并在模数视频恢复中达到最先进性能。

Conclusion: SSViT是首个用于模数视频重建的深度学习框架，通过令牌选择策略提高效率并集中处理关键区域，实验证实其能从8位折叠视频中高质量重建，并在模数视频恢复中达到最先进性能。

Abstract: Conventional image sensors have limited dynamic range, causing saturation in high-dynamic-range (HDR) scenes. Modulo cameras address this by folding incident irradiance into a bounded range, yet require specialized unwrapping algorithms to reconstruct the underlying signal. Unlike HDR recovery, which extends dynamic range from conventional sampling, modulo recovery restores actual values from folded samples. Despite being introduced over a decade ago, progress in modulo image recovery has been slow, especially in the use of modern deep learning techniques. In this work, we demonstrate that standard HDR methods are unsuitable for modulo recovery. Transformers, however, can capture global dependencies and spatial-temporal relationships crucial for resolving folded video frames. Still, adapting existing Transformer architectures for modulo recovery demands novel techniques. To this end, we present Selective Spatiotemporal Vision Transformer (SSViT), the first deep learning framework for modulo video reconstruction. SSViT employs a token selection strategy to improve efficiency and concentrate on the most critical regions. Experiments confirm that SSViT produces high-quality reconstructions from 8-bit folded videos and achieves state-of-the-art performance in modulo video recovery.

</details>


### [4] [Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models](https://arxiv.org/abs/2511.07496)
*Barath Chandran. C,Srinivas Anumasa,Dianbo Liu*

Main category: cs.CV

TL;DR: 本文提出一种后处理调整方法，利用分数函数的拉普拉斯减少扩散模型的幻觉样本生成，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然成功，但已知会产生幻觉样本，导致不连贯或不现实的生成结果。现有研究将其归因于模式插值和分数平滑现象，但缺乏在采样过程中防止其生成的方法。

Method: 提出了一种在推理过程中对分数函数进行后处理调整的方法，利用了分数函数的拉普拉斯近似，并通过有限差分Hutchinson迹估计器在更高维度上实现高效计算。

Result: 该方法在1D、2D玩具分布和高维图像数据集上显著减少了幻觉样本的生成率，并探索了拉普拉斯与分数不确定性之间的关系。

Conclusion: 本文提出了一种后处理调整方法，通过利用分数函数的拉普拉斯（或锐度）来减少无条件扩散模型中的模式插值幻觉，显著降低了幻觉样本的生成率。

Abstract: Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.

</details>


### [5] [Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance](https://arxiv.org/abs/2511.07499)
*Kwanyoung Kim*

Main category: cs.CV

TL;DR: ASAG是一种基于最优传输的对抗性注意力引导方法，通过干扰自注意力机制提升扩散模型的生成质量和可控性，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的引导方法（如CFG）缺乏理论依据，依赖手动设计的扰动函数，限制了生成质量的进一步提升。

Method: ASAG在自注意力层中注入对抗性成本，通过Sinkhorn算法有意干扰传输成本，减少查询和键之间的像素级相似性，从而削弱误导性注意力对齐。

Result: ASAG在文本到图像扩散任务中表现一致提升，并增强了IP-Adapter和ControlNet等下游应用的可控性和保真度。

Conclusion: ASAG通过最优传输理论重新解释扩散模型中的注意力机制，并提出了一种轻量级、即插即用的对抗性引导方法，显著提升了生成样本的质量和可控性，且无需重新训练模型。

Abstract: Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.

</details>


### [6] [LiveNeRF: Efficient Face Replacement Through Neural Radiance Fields Integration](https://arxiv.org/abs/2511.07552)
*Tung Vu,Hai Nguyen,Cong Tran*

Main category: cs.CV

TL;DR: LiveNeRF框架通过实时面部替换技术（33 FPS）提升了娱乐和教育应用，同时强调负责任的使用以减少潜在滥用风险。


<details>
  <summary>Details</summary>
Motivation: 面部替换技术在娱乐、教育和通信应用中有显著进展，如配音、虚拟化身和跨文化内容适配。LiveNeRF旨在解决现有方法的实时性和视觉质量问题，以支持直播、视频会议等实时应用。

Method: LiveNeRF框架

Result: LiveNeRF实现了实时性能（33 FPS）并提供了卓越的视觉质量。

Conclusion: LiveNeRF框架通过实现实时性能（33 FPS）和卓越的视觉质量，解决了现有方法的关键限制，为直播、视频会议和交互式媒体等场景提供了实用部署方案。同时，作者提倡负责任的使用，包括用户同意验证和与检测系统的集成，以确保积极的社会影响并最小化风险。

Abstract: Face replacement technology enables significant advancements in entertainment, education, and communication applications, including dubbing, virtual avatars, and cross-cultural content adaptation. Our LiveNeRF framework addresses critical limitations of existing methods by achieving real-time performance (33 FPS) with superior visual quality, enabling practical deployment in live streaming, video conferencing, and interactive media. The technology particularly benefits content creators, educators, and individuals with speech impairments through accessible avatar communication. While acknowledging potential misuse in unauthorized deepfake creation, we advocate for responsible deployment with user consent verification and integration with detection systems to ensure positive societal impact while minimizing risks.

</details>


### [7] [TrackStudio: An Integrated Toolkit for Markerless Tracking](https://arxiv.org/abs/2511.07624)
*Hristo Dimitrov,Giulia Dominijanni,Viktorija Pavalkyte,Tamar R. Makin*

Main category: cs.CV

TL;DR: TrackStudio是一个无需编程技能的无标记运动跟踪工具，结合开源工具提供自动化功能，测试显示性能稳定，适合非专家使用。


<details>
  <summary>Details</summary>
Motivation: 解决现有工具需要大量技术专业知识的问题，为非专家提供跨多样环境的无障碍、集成解决方案。

Method: 结合现有的开源工具，开发了一个模块化、基于GUI的管道，提供自动2D和3D跟踪、校准、预处理、特征提取和可视化功能。

Result: 在76名参与者中测试，平均帧间相关性超过0.98，平均三角测量误差保持较低（手跟踪<13.6mm），验证了稳定和一致的跟踪性能。

Conclusion: TrackStudio提供了一个实用且易于使用的无标记运动跟踪解决方案，适用于需要可靠性能但缺乏专业知识的用户。

Abstract: Markerless motion tracking has advanced rapidly in the past 10 years and currently offers powerful opportunities for behavioural, clinical, and biomechanical research. While several specialised toolkits provide high performance for specific tasks, using existing tools still requires substantial technical expertise. There remains a gap in accessible, integrated solutions that deliver sufficient tracking for non-experts across diverse settings.
  TrackStudio was developed to address this gap by combining established open-source tools into a single, modular, GUI-based pipeline that works out of the box. It provides automatic 2D and 3D tracking, calibration, preprocessing, feature extraction, and visualisation without requiring any programming skills. We supply a user guide with practical advice for video acquisition, synchronisation, and setup, alongside documentation of common pitfalls and how to avoid them.
  To validate the toolkit, we tested its performance across three environments using either low-cost webcams or high-resolution cameras, including challenging conditions for body position, lightning, and space and obstructions. Across 76 participants, average inter-frame correlations exceeded 0.98 and average triangulation errors remained low (<13.6mm for hand tracking), demonstrating stable and consistent tracking. We further show that the same pipeline can be extended beyond hand tracking to other body and face regions. TrackStudio provides a practical, accessible route into markerless tracking for researchers or laypeople who need reliable performance without specialist expertise.

</details>


### [8] [Predicting Coronary Artery Calcium Severity based on Non-Contrast Cardiac CT images using Deep Learning](https://arxiv.org/abs/2511.07695)
*Lachlan Nguyen,Aidan Cousins,Arcot Sowmya,Hugh Dixson,Sonit Singh*

Main category: cs.CV

TL;DR: 该研究开发了一个深度学习CNN模型，用于自动分类心脏非对比CT图像的钙化评分，结果显示高准确率（96.5%）和泛化能力，验证了其在临床实践中的可行性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球高死亡率的原因之一，冠状动脉钙化（CAC）评分是评估动脉粥样硬化性心血管疾病风险的重要工具。当前的评分方法需要放射科医生和训练有素的放射技师进行耗时的手动分析，因此需要开发自动化方法以提高效率。

Method: 研究使用68例患者的心脏非对比计算机断层扫描图像，通过深度学习CNN模型进行分类，将钙化评分分为六类临床类别。数据集分为训练、验证和测试集。

Result: 模型在六类CAC评分分类任务中表现出高性能，误分类32例，其中26例倾向于高估CAC评分。总体一致性（Cohen's kappa）为0.962，准确率为96.5%，具有高泛化能力。

Conclusion: 该研究开发的深度学习卷积神经网络（CNN）模型在六类冠状动脉钙化（CAC）评分分类任务中表现出高性能，准确率为96.5%，具有高一致性和泛化能力，证明了其在实际应用中的可行性。

Abstract: Cardiovascular disease causes high rates of mortality worldwide. Coronary artery calcium (CAC) scoring is a powerful tool to stratify the risk of atherosclerotic cardiovascular disease. Current scoring practices require time-intensive semiautomatic analysis of cardiac computed tomography by radiologists and trained radiographers. The purpose of this study is to develop a deep learning convolutional neural networks (CNN) model to classify the calcium score in cardiac, non-contrast computed tomography images into one of six clinical categories. A total of 68 patient scans were retrospectively obtained together with their respective reported semiautomatic calcium score using an ECG-gated GE Discovery 570 Cardiac SPECT/CT camera. The dataset was divided into training, validation and test sets. Using the semiautomatic CAC score as the reference label, the model demonstrated high performance on a six-class CAC scoring categorisation task. Of the scans analysed, the model misclassified 32 cases, tending towards overestimating the CAC in 26 out of 32 misclassifications. Overall, the model showed high agreement (Cohen's kappa of 0.962), an overall accuracy of 96.5% and high generalisability. The results suggest that the model outputs were accurate and consistent with current semiautomatic practice, with good generalisability to test data. The model demonstrates the viability of a CNN model to stratify the calcium score into an expanded set of six clinical categories.

</details>


### [9] [FlowFeat: Pixel-Dense Embedding of Motion Profiles](https://arxiv.org/abs/2511.07696)
*Nikita Araslanov,Anna Sonnweber,Daniel Cremers*

Main category: cs.CV

TL;DR: FlowFeat是一种高分辨率多任务特征表征，通过自监督蒸馏技术提升密集预测任务的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的网络（如transformer）生成的低分辨率特征网格在密集预测任务中表现不佳，需要一种高分辨率且多任务的特征表征来提升性能。

Method: 提出FlowFeat，一种高分辨率和多任务特征表征，其核心是一种新颖的蒸馏技术，通过利用光流网络和多样化视频数据，开发了一个有效的自监督训练框架。

Result: FlowFeat显著提升了五种最先进编码器和替代上采样策略在视频对象分割、单目深度估计和语义分割三个任务中的表征能力。

Conclusion: FlowFeat通过新颖的蒸馏技术显著提升了五种最先进编码器的表征能力，并在三个密集任务中表现出色，为可靠且通用的密集图像表征迈出了重要一步。

Abstract: Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present FlowFeat, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or motion profiles. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.

</details>


### [10] [Cross Modal Fine-grained Alignment via Granularity-aware and Region-uncertain Modeling](https://arxiv.org/abs/2511.07710)
*Jiale Liu,Haoming Zhou,Yishu Zhu,Bingzhi Chen,Yuncheng Jiang*

Main category: cs.CV

TL;DR: 本文针对细粒度图像-文本对齐中的噪声注意力机制和跨模态关系建模过于简化问题，提出了一种结合显著性感知和粒度感知建模的统一方法，提升了鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中泛化能力差，缺乏对视觉和文本标记重要性的稳健评估机制，以及对区域-单词对应关系的一对多和多对一性质的细粒度不确定性建模。

Method: 该方法利用模态特定的偏置来识别显著特征，不依赖于脆弱的跨模态注意力机制，并将区域特征表示为高斯分布的混合以捕捉细粒度不确定性。

Result: 在Flickr30K和MS-COCO数据集上的广泛实验表明，该方法在各种骨干架构上均实现了最先进的性能。

Conclusion: 本文提出了一种结合显著性感知和粒度感知建模以及区域级不确定性建模的统一方法，显著提升了细粒度图像-文本对齐的鲁棒性和可解释性，并在Flickr30K和MS-COCO数据集上实现了最先进的性能。

Abstract: Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.

</details>


### [11] [UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis](https://arxiv.org/abs/2511.07743)
*Yuezhe Yang,Wenjie Cai,Dexin Yang,Yufang Dong,Xingbo Dong,Zhe Jin*

Main category: cs.CV

TL;DR: UltraGS是一个优化超声成像的高斯喷洒框架，通过深度感知策略和专用渲染函数，显著提升视图合成质量，并实现实时性能。代码和数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 超声成像在临床诊断中至关重要，但其有限的视场限制了新视图的合成。UltraGS旨在解决这一问题，优化超声成像的高斯喷洒框架。

Method: 1. 深度感知高斯喷洒策略：每个高斯分配可学习的视场，实现精确深度预测和结构表示。2. SH-DARS渲染函数：结合低阶球谐函数和超声特定波物理（深度衰减、反射和散射），准确建模组织强度。

Result: 在三个数据集上的实验表明，UltraGS在PSNR（最高29.55）、SSIM（最高0.89）和MSE（最低0.002）上达到最优性能，实时合成速度为64.69 fps。

Conclusion: UltraGS框架通过深度感知的高斯喷洒策略和轻量级渲染函数SH-DARS，显著提升了超声成像的视图合成质量，并在PSNR、SSIM和MSE等指标上达到state-of-the-art性能，同时实现实时合成。

Abstract: Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.

</details>


### [12] [Vision Transformer Based User Equipment Positioning](https://arxiv.org/abs/2511.08549)
*Parshwa Shah,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez,Siddhartan Govindasamy*

Main category: cs.CV

TL;DR: 提出基于注意力机制的ViT架构，解决DL模型在UE定位中的注意力分配和非顺序数据处理问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在UE定位中存在两个主要问题：对输入数据的注意力分配不均，以及对非顺序数据（如瞬时CSI）的处理能力不足。

Method: 采用注意力机制的Vision Transformer架构，专注于从CSI矩阵中提取角度延迟剖面（ADP）。

Result: 在DeepMIMO和ViWi数据集上测试，室内RMSE为0.55m，室外为13.59m（DeepMIMO）和3.45m（ViWi），性能提升约38%。

Conclusion: 所提出的基于注意力机制的Vision Transformer架构在UE定位任务中表现优异，显著优于现有方法。

Abstract: Recently, Deep Learning (DL) techniques have been used for User Equipment (UE) positioning. However, the key shortcomings of such models is that: i) they weigh the same attention to the entire input; ii) they are not well suited for the non-sequential data e.g., when only instantaneous Channel State Information (CSI) is available. In this context, we propose an attention-based Vision Transformer (ViT) architecture that focuses on the Angle Delay Profile (ADP) from CSI matrix. Our approach, validated on the `DeepMIMO' and `ViWi' ray-tracing datasets, achieves an Root Mean Squared Error (RMSE) of 0.55m indoors, 13.59m outdoors in DeepMIMO, and 3.45m in ViWi's outdoor blockage scenario. The proposed scheme outperforms state-of-the-art schemes by $\sim$ 38\%. It also performs substantially better than other approaches that we have considered in terms of the distribution of error distance.

</details>


### [13] [VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics](https://arxiv.org/abs/2511.07744)
*Daniel Cher,Brian Wei,Srikumar Sastry,Nathan Jacobs*

Main category: cs.CV

TL;DR: VectorSynth是一种基于扩散的框架，通过多边形地理标注和语义属性实现像素级精确的卫星图像合成，支持交互式编辑，并在语义保真度和结构真实性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本或布局条件模型无法实现像素级精确的卫星图像合成，VectorSynth旨在通过多边形地理标注与语义属性实现这一目标。

Method: VectorSynth通过视觉语言对齐模块从多边形语义生成像素级嵌入，这些嵌入指导条件图像生成框架，确保空间范围和语义线索的一致性。

Result: VectorSynth在多样化的城市场景中表现出色，支持交互式工作流，如快速假设模拟、空间编辑和地图信息内容生成。

Conclusion: VectorSynth在语义保真度和结构真实性上显著优于现有方法，展示了细粒度的空间基础能力。代码和数据已开源。

Abstract: We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.

</details>


### [14] [Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs](https://arxiv.org/abs/2511.07748)
*Yuezhe Yang,Yiyue Guo,Wenjie Cai,Qingqing Ruan,Siying Wang,Xingbo Dong,Zhe Jin,Yong Dai*

Main category: cs.CV

TL;DR: 论文提出了Auto-US，一个结合超声视频和临床诊断文本的智能诊断系统，通过构建CUV数据集和开发CTU-Net，实现了86.73%的分类准确率，并生成临床诊断建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究在数据集多样性、诊断性能和临床适用性方面存在局限，需提升AI辅助超声视频诊断的效率和准确性。

Method: 构建了包含5类3器官的495个超声视频的CUV数据集，开发了CTU-Net分类模型，并整合大语言模型生成诊断建议。

Result: CTU-Net分类准确率达86.73%，诊断评分超3/5，临床验证有效。

Conclusion: Auto-US展示了在实际超声应用中的有效性和临床潜力。

Abstract: AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: https://github.com/Bean-Young/Auto-US.

</details>


### [15] [Class Incremental Medical Image Segmentation via Prototype-Guided Calibration and Dual-Aligned Distillation](https://arxiv.org/abs/2511.07749)
*Shengqian Zhu,Chengrong Yu,Qiang Wang,Ying Song,Guangjun Li,Jiafei Wu,Xiaogang Xu,Zhang Yi,Junjie Hu*

Main category: cs.CV

TL;DR: PGCD和DAPD通过原型校准和双对齐蒸馏，解决了CIMIS任务中旧知识保留和新类学习的挑战，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理CIMIS任务时，要么采用一刀切的策略导致旧知识保留不准确，要么仅关注局部原型与全局原型的对齐而忽略新数据中的局部表示，导致知识退化。

Method: PGCD利用原型到特征的相似性来校准不同空间区域的类特定蒸馏强度，DAPD则将旧类的局部原型与全局原型及局部原型对齐。

Result: 在两个广泛使用的多器官分割基准测试中，该方法优于现有最先进方法，展示了其鲁棒性和泛化能力。

Conclusion: 提出的PGCD和DAPD方法在CIMIS任务中显著提升了旧类知识的保留和新类学习的效果，通过原型引导的校准蒸馏和双对齐原型蒸馏，有效解决了现有方法的不足。

Abstract: Class incremental medical image segmentation (CIMIS) aims to preserve knowledge of previously learned classes while learning new ones without relying on old-class labels. However, existing methods 1) either adopt one-size-fits-all strategies that treat all spatial regions and feature channels equally, which may hinder the preservation of accurate old knowledge, 2) or focus solely on aligning local prototypes with global ones for old classes while overlooking their local representations in new data, leading to knowledge degradation. To mitigate the above issues, we propose Prototype-Guided Calibration Distillation (PGCD) and Dual-Aligned Prototype Distillation (DAPD) for CIMIS in this paper. Specifically, PGCD exploits prototype-to-feature similarity to calibrate class-specific distillation intensity in different spatial regions, effectively reinforcing reliable old knowledge and suppressing misleading information from old classes. Complementarily, DAPD aligns the local prototypes of old classes extracted from the current model with both global prototypes and local prototypes, further enhancing segmentation performance on old categories. Comprehensive evaluations on two widely used multi-organ segmentation benchmarks demonstrate that our method outperforms state-of-the-art methods, highlighting its robustness and generalization capabilities.

</details>


### [16] [Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks](https://arxiv.org/abs/2511.07755)
*Aja Khanal,Ahmed Faid,Apurva Narayan*

Main category: cs.CV

TL;DR: Filtered-ViT, a new vision transformer with SMART-VMF, addresses vulnerability to multiple adversarial patches and real-world artifacts, achieving high accuracy in synthetic and real-world medical imaging scenarios.


<details>
  <summary>Details</summary>
Motivation: Deep learning vision systems are vulnerable to adversarial patches and real-world artifacts, especially in safety-critical domains like healthcare. Existing defenses often fail under multiple localized disruptions.

Method: The paper introduces Filtered-ViT, a vision transformer architecture integrated with SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism.

Result: Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1% patches on ImageNet with LaVAN attacks, outperforming existing defenses. It also effectively mitigates natural artifacts in radiographic medical imagery.

Conclusion: Filtered-ViT is the first transformer architecture to show unified robustness against both adversarial and natural patch-like disruptions, offering a reliable solution for high-stakes vision systems.

Abstract: Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.

</details>


### [17] [Beyond Randomness: Understand the Order of the Noise in Diffusion](https://arxiv.org/abs/2511.07756)
*Song Yan,Min Li,Bi Xinliang,Jian Yang,Yusen Zhang,Guanye Xiong,Yunwei Lan,Tao Zhang,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文揭示了T2C扩散模型中初始噪声的语义潜力，提出无需训练的"语义擦除-注入"两步法，有效调制噪声以优化生成。


<details>
  <summary>Details</summary>
Motivation: 传统的T2C扩散模型中，初始噪声被视为促进内容多样性的随机元素，但本文发现噪声下隐藏着可分析的模式，蕴含着丰富的语义信息。

Method: 通过信息论基础，提出了一种简单的两步法：首先从噪声中擦除不需要的语义，然后利用扩散模型生成过程与语义注入的等价性，向清理后的噪声注入语义。

Result: 实验证明，该方法在基于DiT和UNet架构的各种T2C模型中均有效，为一致性生成提供了通用工具。

Conclusion: 本文提出的"语义擦除-注入"两步法为T2C扩散模型的初始噪声调制提供了高效、无需训练的通用解决方案，为扩散模型的生成优化提供了新视角。

Abstract: In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step "Semantic Erasure-Injection" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.

</details>


### [18] [Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval](https://arxiv.org/abs/2511.07780)
*Likang Peng,Chao Su,Wenyuan Wu,Yuan Sun,Dezhong Peng,Xi Peng,Xu Wang*

Main category: cs.CV

TL;DR: SCBCH框架通过语义一致性和自适应对比学习，解决了跨模态哈希在多标签噪声数据下的挑战，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多标签数据普遍存在标签噪声，且现有跨模态哈希方法常忽略多标签数据的部分语义重叠，导致检索性能下降。

Method: SCBCH框架包含两个模块：跨模态语义一致分类（CSCC）用于估计样本可靠性并减少噪声标签影响；双向软对比哈希（BSCH）基于多标签语义重叠动态生成软对比样本对，实现跨模态自适应对比学习。

Result: 在四个广泛使用的跨模态检索基准上，SCBCH方法在噪声多标签条件下始终优于现有最先进方法。

Conclusion: SCBCH框架通过结合跨模态语义一致分类和双向软对比哈希，有效解决了多标签噪声数据下的跨模态检索问题，显著提升了检索性能。

Abstract: Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.

</details>


### [19] [Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2511.07798)
*Runmin Cong,Anpeng Wang,Bin Wan,Cong Zhang,Xiaofei Zhou,Wei Zhang*

Main category: cs.CV

TL;DR: DCDNet通过特征解耦和动态融合，显著提升跨域小样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域小样本分割中特征纠缠导致泛化性和快速适应能力受限的问题。

Method: 提出Divide-and-Conquer Decoupled Network (DCDNet)，包含Adversarial-Contrastive Feature Decomposition (ACFD)模块和Matrix-Guided Dynamic Fusion (MGDF)模块，以及微调阶段的Cross-Adaptive Modulation (CAM)模块。

Result: 在四个挑战性数据集上表现优于现有方法，实现了跨域泛化和小样本适应的新突破。

Conclusion: DCDNet通过ACFD和MGDF模块有效解耦和整合特征，结合CAM模块在微调阶段的调制，显著提升了跨域小样本分割的性能，成为当前最佳方法。

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to tackle the dual challenge of recognizing novel classes and adapting to unseen domains with limited annotations. However, encoder features often entangle domain-relevant and category-relevant information, limiting both generalization and rapid adaptation to new domains. To address this issue, we propose a Divide-and-Conquer Decoupled Network (DCDNet). In the training stage, to tackle feature entanglement that impedes cross-domain generalization and rapid adaptation, we propose the Adversarial-Contrastive Feature Decomposition (ACFD) module. It decouples backbone features into category-relevant private and domain-relevant shared representations via contrastive learning and adversarial learning. Then, to mitigate the potential degradation caused by the disentanglement, the Matrix-Guided Dynamic Fusion (MGDF) module adaptively integrates base, shared, and private features under spatial guidance, maintaining structural coherence. In addition, in the fine-tuning stage, to enhanced model generalization, the Cross-Adaptive Modulation (CAM) module is placed before the MGDF, where shared features guide private features via modulation ensuring effective integration of domain-relevant information. Extensive experiments on four challenging datasets show that DCDNet outperforms existing CD-FSS methods, setting a new state-of-the-art for cross-domain generalization and few-shot adaptation.

</details>


### [20] [Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2](https://arxiv.org/abs/2511.08130)
*Mehmet Batuhan Duman,Alejandro Carnero,Cristian Martín,Daniel Garrido,Manuel Díaz*

Main category: cs.CV

TL;DR: 该论文提出了一种结合联邦学习和SAM2的新框架，用于废水处理厂的泡沫自动跟踪，解决了数据隐私和标记数据稀缺的问题，提高了分割性能。


<details>
  <summary>Details</summary>
Motivation: 废水处理厂的泡沫形成是一个重大挑战，会降低处理效率并增加成本。自动实时检查泡沫百分比变化对工厂有很大益处，但标准机器学习模型需要大量标记数据，且由于隐私问题，不同废水处理厂不共享数据，导致系统开发缓慢。

Method: 通过Flower框架在分布式客户端（边缘节点）上对SAM2进行微调，中央Fog服务器通过聚合模型权重而不访问私有数据来协调过程。

Result: 该框架加速了训练收敛，并利用SAM2的预训练权重初始化，即使在有限的本地数据集下也提高了分割性能。

Conclusion: 该研究提出了一种结合联邦学习和SAM2的新框架，为废水处理厂的泡沫自动跟踪提供了实用、可扩展且隐私保护的解决方案，展示了将大规模基础模型集成到联邦学习系统中解决现实工业挑战的巨大潜力。

Abstract: Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.

</details>


### [21] [SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces](https://arxiv.org/abs/2511.08271)
*Sweta Banerjee,Timo Gosch,Sara Hester,Viktoria Weiss,Thomas Conrad,Taryn A. Donovan,Nils Porsche,Jonas Ammeling,Christoph Stroblberger,Robert Klopfleisch,Christopher Kaltenecker,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: SWAN 是一种通过滑动手势快速标注病理图像的工具，试点研究显示其速度快、一致性高，且用户评价良好。


<details>
  <summary>Details</summary>
Motivation: 解决大规模病理图像数据集标注的瓶颈问题，传统文件夹标注工作流程慢、易疲劳且难以扩展。

Method: 引入 SWAN，一种开源、MIT 许可的网页应用，支持通过滑动手势进行直观的图像块分类。

Result: 在四名病理学家标注 600 个有丝分裂图像块的试点研究中，SWAN 的标注速度快且一致性高（Cohen's Kappa = 0.61-0.80），与传统方法相比表现相当。

Conclusion: SWAN 提供了一种可扩展且用户友好的替代方案，能够加速图像标注并保持标注质量。

Abstract: The annotation of large scale histopathology image datasets remains a major bottleneck in developing robust deep learning models for clinically relevant tasks, such as mitotic figure classification. Folder-based annotation workflows are usually slow, fatiguing, and difficult to scale. To address these challenges, we introduce SWipeable ANnotations (SWAN), an open-source, MIT-licensed web application that enables intuitive image patch classification using a swiping gesture. SWAN supports both desktop and mobile platforms, offers real-time metadata capture, and allows flexible mapping of swipe gestures to class labels. In a pilot study with four pathologists annotating 600 mitotic figure image patches, we compared SWAN against a traditional folder-sorting workflow. SWAN enabled rapid annotations with pairwise percent agreement ranging from 86.52% to 93.68% (Cohen's Kappa = 0.61-0.80), while for the folder-based method, the pairwise percent agreement ranged from 86.98% to 91.32% (Cohen's Kappa = 0.63-0.75) for the task of classifying atypical versus normal mitotic figures, demonstrating high consistency between annotators and comparable performance. Participants rated the tool as highly usable and appreciated the ability to annotate on mobile devices. These results suggest that SWAN can accelerate image annotation while maintaining annotation quality, offering a scalable and user-friendly alternative to conventional workflows.

</details>


### [22] [Learning Sparse Label Couplings for Multilabel Chest X-Ray Diagnosis](https://arxiv.org/abs/2511.07801)
*Utkarsh Prakash Srivastava,Kaushik Gupta,Kaushik Nath*

Main category: cs.CV

TL;DR: 本文提出了一种基于SE-ResNeXt101的多标签CXR分类流程，通过MIS、不对称损失和标签图细化模块，显著提升了分类性能，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 解决胸部X光片的多标签分类问题，特别是在极端类别不平衡和非对称错误成本的情况下，提升分类性能。

Method: 使用SE-ResNeXt101 $(32 \times 4d)$作为主干网络，通过多标签迭代分层（MIS）进行稳健的交叉验证分割，优化不对称损失（Asymmetric Loss），并采用混合精度训练（AMP）、余弦学习率衰减、梯度裁剪和权重指数移动平均（EMA）。此外，提出了一种轻量级的标签图细化模块，通过学习稀疏的可训练标签间耦合矩阵来优化logits。

Result: 在数据集上，SE-ResNeXt101基线模型达到了92.64%的宏观AUC，添加标签图细化模块后，验证宏观AUC在各折中均有所提升，且计算开销可忽略。

Conclusion: 该方法提供了一种可重复、硬件友好且无需额外标注的实用途径，显著提升了多标签CXR分类器的性能。

Abstract: We study multilabel classification of chest X-rays and present a simple, strong pipeline built on SE-ResNeXt101 $(32 \times 4d)$. The backbone is finetuned for 14 thoracic findings with a sigmoid head, trained using Multilabel Iterative Stratification (MIS) for robust cross-validation splits that preserve label co-occurrence. To address extreme class imbalance and asymmetric error costs, we optimize with Asymmetric Loss, employ mixed-precision (AMP), cosine learning-rate decay with warm-up, gradient clipping, and an exponential moving average (EMA) of weights. We propose a lightweight Label-Graph Refinement module placed after the classifier: given per-label probabilities, it learns a sparse, trainable inter-label coupling matrix that refines logits via a single message-passing step while adding only an L1-regularized parameter head. At inference, we apply horizontal flip test-time augmentation (TTA) and average predictions across MIS folds (a compact deep ensemble). Evaluation uses macro AUC averaging classwise ROC-AUC and skipping single-class labels in a fold to reflect balanced performance across conditions. On our dataset, a strong SE-ResNeXt101 baseline attains competitive macro AUC (e.g., 92.64% in our runs). Adding the Label-Graph Refinement consistently improves validation macro AUC across folds with negligible compute. The resulting method is reproducible, hardware-friendly, and requires no extra annotations, offering a practical route to stronger multilabel CXR classifiers.

</details>


### [23] [PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier](https://arxiv.org/abs/2511.07806)
*Shaomeng Wang,He Wang,Xiaolu Wei,Longquan Dai,Jinhui Tang*

Main category: cs.CV

TL;DR: PC-Diffusion 是一种轻量级偏好分类器框架，解决了 DPO 的高计算成本和参考模型依赖问题，实现了高效且稳定的偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 针对 DPO 方法在扩散模型中存在的高计算成本和参考模型质量敏感性问题，提出了一种新的框架 PC-Diffusion。

Method: PC-Diffusion 通过训练一个轻量级的偏好分类器，直接建模样本间的相对偏好，从而解耦了偏好对齐与生成模型，避免了整个模型的微调和参考模型的依赖。

Result: 实验结果表明，PC-Diffusion 在偏好一致性上与 DPO 相当，同时显著降低了训练成本，实现了高效且稳定的偏好引导生成。

Conclusion: PC-Diffusion 提出了一种轻量级的偏好分类器框架，有效解决了 DPO 方法的高计算成本和参考模型依赖问题，同时在偏好一致性上取得了与 DPO 相当的效果。

Abstract: Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.

</details>


### [24] [DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model](https://arxiv.org/abs/2511.07808)
*Zhongle Ren,Hui Ding,Kai Wang,Biao Hou,Xingyu Luo,Weibin Li,Licheng Jiao*

Main category: cs.CV

TL;DR: 提出DI3CL预训练框架，通过动态实例和轮廓一致性模块提升SAR地表覆盖分类性能，并在多个任务中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有SAR地表覆盖分类方法主要依赖监督学习，需要大量标注数据，限制了可扩展性和泛化能力，亟需一种通用基础模型来加速下游模型的开发和部署。

Method: 开发了动态实例和轮廓一致性对比学习（DI3CL）预训练框架，包含动态实例（DI）模块和轮廓一致性（CC）模块，并结合了大规模多样化的SARSense数据集。

Result: DI3CL在SAR地表覆盖制图、水体检测和道路提取等任务中均表现出优越性能。

Conclusion: 提出的DI3CL预训练框架在多种SAR地表覆盖分类任务中表现优于现有方法，展现了其作为通用基础模型的潜力。

Abstract: Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: https://github.com/SARpre-train/DI3CL.

</details>


### [25] [Revisiting MLLM Based Image Quality Assessment: Errors and Remedy](https://arxiv.org/abs/2511.07812)
*Zhenchen Tang,Songlin Yang,Bo Peng,Zichuan Wang,Jing Dong*

Main category: cs.CV

TL;DR: Q-Scorer通过轻量级回归模块和专用分数标记，解决了MLLM在IQA任务中的离散-连续不匹配问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在图像质量评估（IQA）任务中表现受限，主要由于离散标记输出与连续质量分数要求不匹配，以及语义混淆问题。

Method: 提出Q-Scorer框架，结合轻量级回归模块和IQA专用分数标记，以解决离散标记输出与连续质量分数之间的不匹配问题。

Result: Q-Scorer在多个IQA基准测试中达到最先进性能，具有良好的泛化能力，并与其他方法结合时表现更优。

Conclusion: Q-Scorer框架通过轻量级回归模块和IQA专用分数标记，显著提升了MLLM在图像质量评估任务中的性能，并在多个基准测试中实现了最先进的表现。

Abstract: The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.

</details>


### [26] [Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views](https://arxiv.org/abs/2511.07813)
*Haida Feng,Hao Wei,Zewen Xu,Haolin Wang,Chade Li,Yihong Wu*

Main category: cs.CV

TL;DR: Sparse3DPR is a training-free framework for 3D scene understanding, using LLMs and sparse RGB inputs, improving accuracy and efficiency with novel scene graph and subgraph methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of training-free approaches in accuracy and efficiency for 3D scene understanding, while maintaining flexibility and generalization.

Method: The framework leverages pre-trained LLMs and sparse-view RGB inputs, introducing a hierarchical plane-enhanced scene graph and a task-adaptive subgraph extraction method to enhance reasoning and reduce noise.

Result: Sparse3DPR achieves a 28.7% improvement in EM@1 and a 78.2% speedup compared to ConceptGraphs, with comparable performance to training-based methods on ScanQA.

Conclusion: Sparse3DPR demonstrates superior performance in open-ended 3D scene understanding, achieving significant improvements in accuracy and efficiency compared to existing methods, and shows robustness and generalization in real-world applications.

Abstract: Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.

</details>


### [27] [Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging](https://arxiv.org/abs/2511.07816)
*Jarett Dewbury,Chi-en Amy Tai,Alexander Wong*

Main category: cs.CV

TL;DR: 研究证实CDI$^s$作为DWI的增强手段，可显著提升前列腺癌病变分割性能，且无需额外扫描时间，适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在前列腺癌病变分割中表现有限，Dice分数在大型患者队列中仅为0.32或更低。

Method: 通过六种最先进的分割架构对200名患者的共注册CDI$^s$、DWI和ADC序列进行全面评估。

Result: CDI$^s$集成在94%的评估配置中可靠地增强或保持了分割性能，个别架构相对基线模态实现了高达72.5%的显著相对改进。CDI$^s$ + DWI是最安全的增强途径，在一半评估架构中实现了显著改进且无性能下降。

Conclusion: CDI$^s$作为现有DWI采集的增强手段，无需额外扫描时间或架构修改，可立即部署于临床工作流，为PCa病变分割任务提供了实用的集成途径。

Abstract: Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.

</details>


### [28] [Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy](https://arxiv.org/abs/2511.07819)
*Gong Jingyu,Tong Kunkun,Chen Zhuoran,Yuan Chuanhan,Chen Mingang,Zhang Zhizhong,Tan Xin,Xie Yuan*

Main category: cs.CV

TL;DR: SSOMotion通过场景语义占据和双向分解，提升了3D人体运动合成的语义理解与效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法过于关注场景结构而忽视语义理解，限制了人体运动合成的效果。

Method: 提出了一种基于场景语义占据（SSO）的人体运动合成框架，采用双向三平面分解和CLIP编码技术，以优化场景语义结构与计算效率。

Result: 在ShapeNet家具、PROX和Replica数据集上的实验表明，SSOMotion在杂乱场景中表现优异。

Conclusion: SSOMotion框架通过统一的场景语义占据表示和双向三平面分解，显著提升了3D场景中人体运动合成的性能与泛化能力。

Abstract: Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.

</details>


### [29] [CloudMamba: Grouped Selective State Spaces for Point Cloud Analysis](https://arxiv.org/abs/2511.07823)
*Kanglin Qu,Pan Gao,Qun Dai,Zhanzhi Ye,Rui Ye,Yuanhao Sun*

Main category: cs.CV

TL;DR: CloudMamba通过序列扩展、双向扫描及参数共享优化Mamba，提升点云分析性能并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决现有Mamba方法在点云分析中的序列化不完善、高层几何感知不足及S6过拟合问题。

Method: 提出序列扩展与合并（序列化点云并融合高阶特征）、chainedMamba（双向并行扫描捕获几何信息）、GS6（参数共享缓解S6过拟合）。

Result: 实验验证CloudMamba在多种点云任务中性能优于现有方法，且复杂度更低。

Conclusion: CloudMamba通过序列扩展与合并、chainedMamba设计及GS6模型，有效解决了点云序列化、高层几何感知不足及S6过拟合问题，在多种任务中实现SOTA效果且复杂度显著降低。

Abstract: Due to the long-range modeling ability and linear complexity property, Mamba has attracted considerable attention in point cloud analysis. Despite some interesting progress, related work still suffers from imperfect point cloud serialization, insufficient high-level geometric perception, and overfitting of the selective state space model (S6) at the core of Mamba. To this end, we resort to an SSM-based point cloud network termed CloudMamba to address the above challenges. Specifically, we propose sequence expanding and sequence merging, where the former serializes points along each axis separately and the latter serves to fuse the corresponding higher-order features causally inferred from different sequences, enabling unordered point sets to adapt more stably to the causal nature of Mamba without parameters. Meanwhile, we design chainedMamba that chains the forward and backward processes in the parallel bidirectional Mamba, capturing high-level geometric information during scanning. In addition, we propose a grouped selective state space model (GS6) via parameter sharing on S6, alleviating the overfitting problem caused by the computational mode in S6. Experiments on various point cloud tasks validate CloudMamba's ability to achieve state-of-the-art results with significantly less complexity.

</details>


### [30] [MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection](https://arxiv.org/abs/2511.07862)
*Sunghun Yang,Minhyeok Lee,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TL;DR: MonoCLUE结合局部聚类和广义场景记忆，提升了单目3D检测在遮挡和有限视野下的性能，KITTI测试表现最佳。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测虽然成本低，但存在深度不确定和视野有限的缺陷，导致几何线索不足，在遮挡或截断场景中精度下降。现有方法虽引入额外深度信息解决几何模糊，但忽视了视觉线索对稳健识别的重要性。

Method: 首先，对视觉特征进行K均值聚类以捕捉物体级外观部分（如引擎盖、车顶），改善部分可见物体的检测。其次，构建广义场景记忆，通过跨图像聚合聚类特征，提供跨场景的一致性表示。最后，将局部聚类特征和广义场景记忆整合到物体查询中，引导注意力至信息丰富区域。

Result: MonoCLUE通过统一策略提升了检测鲁棒性，在KITTI基准测试中表现最优。

Conclusion: MonoCLUE通过结合局部聚类和广义场景记忆策略，提升了单目3D检测在遮挡和有限视野下的鲁棒性，并在KITTI基准测试中达到了最先进的性能。

Abstract: Monocular 3D object detection offers a cost-effective solution for autonomous driving but suffers from ill-posed depth and limited field of view. These constraints cause a lack of geometric cues and reduced accuracy in occluded or truncated scenes. While recent approaches incorporate additional depth information to address geometric ambiguity, they overlook the visual cues crucial for robust recognition. We propose MonoCLUE, which enhances monocular 3D detection by leveraging both local clustering and generalized scene memory of visual features. First, we perform K-means clustering on visual features to capture distinct object-level appearance parts (e.g., bonnet, car roof), improving detection of partially visible objects. The clustered features are propagated across regions to capture objects with similar appearances. Second, we construct a generalized scene memory by aggregating clustered features across images, providing consistent representations that generalize across scenes. This improves object-level feature consistency, enabling stable detection across varying environments. Lastly, we integrate both local cluster features and generalized scene memory into object queries, guiding attention toward informative regions. Exploiting a unified local clustering and generalized scene memory strategy, MonoCLUE enables robust monocular 3D detection under occlusion and limited visibility, achieving state-of-the-art performance on the KITTI benchmark.

</details>


### [31] [Visual Bridge: Universal Visual Perception Representations Generating](https://arxiv.org/abs/2511.07877)
*Yilin Gao,Shuguang Dou,Junzhou Li,Zhiheng Yu,Yin Li,Dongsheng Jiang,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出的通用视觉感知框架通过流匹配和多任务嵌入机制，在多任务场景中实现了高效灵活的表示迁移，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受到大型语言模型跨领域泛化能力的启发，旨在解决扩散模型在‘单任务单模型’范式下的泛化性和可扩展性限制。

Method: 通过将多任务视觉表示生成问题转化为从图像块标记到任务特定表示的通用流匹配问题，并利用自监督基础模型作为锚点，引入多尺度循环任务嵌入机制，学习通用速度场以桥接异构任务间的差距。

Result: 在分类、检测、分割、深度估计和图像-文本检索等任务上的实验表明，该模型在零样本和微调设置下均达到竞争性性能，优于之前的通用模型和多个专用模型。

Conclusion: 本文提出了一个基于流匹配的通用视觉感知框架，在多任务场景中实现了竞争性的性能，为通用视觉建模的未来研究奠定了基础。

Abstract: Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.

</details>


### [32] [Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level](https://arxiv.org/abs/2511.07889)
*Sicong Zang,Shuhui Gao,Zhijun Fang*

Main category: cs.CV

TL;DR: 提出分层次自回归草图生成方法，允许生成过程中实时调整笔画嵌入，提升操控灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成前需一次性提供所有笔画条件，无法在生成过程中进行进一步操控，限制了灵活性。因此，需要一种更灵活的方法来实现草图绘制过程中的实时操控。

Method: 采用三阶段层次结构：1)预测笔画嵌入表示即将绘制的笔画，2)将预测的笔画锚定在画布上，3)将嵌入转换为一系列绘制动作以形成完整草图。整个过程是自回归的，考虑已生成的笔画及其位置来预测当前笔画。

Result: 提出的方法能够灵活地在生成过程中调整笔画嵌入，实现对草图绘制的实时操控，生成更符合预期的草图。

Conclusion: 该论文提出了一种分层次自回归的草图生成过程，允许在生成过程中灵活调整笔画嵌入，从而实现对草图绘制的更灵活控制。

Abstract: Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.

</details>


### [33] [Theoretical Analysis of Power-law Transformation on Images for Text Polarity Detection](https://arxiv.org/abs/2511.07916)
*Narendra Singh Yadav,Pavan Kumar Perepu*

Main category: cs.CV

TL;DR: 本文理论分析了幂律变换在文本极性检测中的作用，验证了其有效性，为图像二值化提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 图像二值化是许多计算机视觉应用的重要预处理步骤，而文本极性信息是关键。现有方法基于直觉，缺乏理论支持，本文旨在填补这一空白。

Method: 通过理论分析幂律变换后的图像直方图统计特性，探讨文本与背景两类之间的最大类间方差变化规律。

Result: 理论分析表明，幂律变换后，暗文本亮背景与亮文本暗背景的最大类间方差确实呈现递增或递减趋势，与经验结果一致。

Conclusion: 本文通过理论分析验证了基于幂律变换的直观方法在文本极性检测中的有效性，为图像二值化提供了理论基础。

Abstract: Several computer vision applications like vehicle license plate recognition, captcha recognition, printed or handwriting character recognition from images etc., text polarity detection and binarization are the important preprocessing tasks. To analyze any image, it has to be converted to a simple binary image. This binarization process requires the knowledge of polarity of text in the images. Text polarity is defined as the contrast of text with respect to background. That means, text is darker than the background (dark text on bright background) or vice-versa. The binarization process uses this polarity information to convert the original colour or gray scale image into a binary image. In the literature, there is an intuitive approach based on power-law transformation on the original images. In this approach, the authors have illustrated an interesting phenomenon from the histogram statistics of the transformed images. Considering text and background as two classes, they have observed that maximum between-class variance between two classes is increasing (decreasing) for dark (bright) text on bright (dark) background. The corresponding empirical results have been presented. In this paper, we present a theoretical analysis of the above phenomenon.

</details>


### [34] [Exploring the Underwater World Segmentation without Extra Training](https://arxiv.org/abs/2511.07923)
*Bingyu Li,Tao Huo,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 首个大规模水下开放词汇分割数据集AquaOV255和基准UOVSBench，以及无需训练的Earth2Ocean框架，显著提升水下分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型主要针对陆地场景，水下分割任务缺乏大规模细粒度数据集和开放词汇评估基准。

Method: 提出了Earth2Ocean框架，包含GMG（几何引导的视觉掩码生成器）和CSA（类别-视觉语义对齐模块），利用陆地视觉-语言模型直接迁移至水下场景。

Result: 在UOVSBench基准测试中，Earth2Ocean实现了显著的性能提升，同时保持高效推理。

Conclusion: Earth2Ocean框架通过结合几何引导的视觉掩码生成器和类别-视觉语义对齐模块，显著提升了水下开放词汇分割的性能，且无需额外训练。

Abstract: Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.

</details>


### [35] [HD$^2$-SSC: High-Dimension High-Density Semantic Scene Completion for Autonomous Driving](https://arxiv.org/abs/2511.07925)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出HD$^2$-SSC框架，通过高维语义解耦和高密度精炼模块解决现有SSC方法的维度与密度差距问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SSC方法在提升3D场景表示方面有效，但因输入输出维度差距及标注-现实密度差距导致预测效果不佳。

Method: 提出HD$^2$-SSC框架，包含高维度语义解耦模块（扩展2D图像特征并解耦遮挡）和高密度占用精炼模块（检测与精炼架构增强语义密度）。

Result: 在SemanticKITTI和SSCBench-KITTI-360数据集上的实验验证了HD$^2$-SSC框架的有效性。

Conclusion: HD$^2$-SSC框架通过高维度语义解耦和高密度占用精炼模块，有效解决了输入输出维度差距和标注-现实密度差距问题，显著提升了3D语义场景补全性能。

Abstract: Camera-based 3D semantic scene completion (SSC) plays a crucial role in autonomous driving, enabling voxelized 3D scene understanding for effective scene perception and decision-making. Existing SSC methods have shown efficacy in improving 3D scene representations, but suffer from the inherent input-output dimension gap and annotation-reality density gap, where the 2D planner view from input images with sparse annotated labels leads to inferior prediction of real-world dense occupancy with a 3D stereoscopic view. In light of this, we propose the corresponding High-Dimension High-Density Semantic Scene Completion (HD$^2$-SSC) framework with expanded pixel semantics and refined voxel occupancies. To bridge the dimension gap, a High-dimension Semantic Decoupling module is designed to expand 2D image features along a pseudo third dimension, decoupling coarse pixel semantics from occlusions, and then identify focal regions with fine semantics to enrich image features. To mitigate the density gap, a High-density Occupancy Refinement module is devised with a "detect-and-refine" architecture to leverage contextual geometric and semantic structures for enhanced semantic density with the completion of missing voxels and correction of erroneous ones. Extensive experiments and analyses on the SemanticKITTI and SSCBench-KITTI-360 datasets validate the effectiveness of our HD$^2$-SSC framework.

</details>


### [36] [An Image-Based Path Planning Algorithm Using a UAV Equipped with Stereo Vision](https://arxiv.org/abs/2511.07928)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: Novel image-based path planning using UAV-captured disparity maps and computer vision outperforms A* and PRM in safety and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Terrain depth significantly impacts path safety, but 2D images fail to distinguish craters and hills. A disparity map-based approach addresses this limitation.

Method: Utilizes computer vision techniques (edge, line, corner detection, stereo depth reconstruction) on disparity maps from UAV-captured images to define way-points. Initial and desired points are detected via ArUco marker pose estimation and circle detection.

Result: Comparative analysis in V-REP simulations and lab setups shows the algorithm's superior performance over A* and PRM.

Conclusion: The proposed image-based path planning algorithm demonstrates effectiveness through promising results in both virtual and physical environments, outperforming traditional methods like A* and PRM.

Abstract: This paper presents a novel image-based path planning algorithm that was developed using computer vision techniques, as well as its comparative analysis with well-known deterministic and probabilistic algorithms, namely A* and Probabilistic Road Map algorithm (PRM). The terrain depth has a significant impact on the calculated path safety. The craters and hills on the surface cannot be distinguished in a two-dimensional image. The proposed method uses a disparity map of the terrain that is generated by using a UAV. Several computer vision techniques, including edge, line and corner detection methods, as well as the stereo depth reconstruction technique, are applied to the captured images and the found disparity map is used to define candidate way-points of the trajectory. The initial and desired points are detected automatically using ArUco marker pose estimation and circle detection techniques. After presenting the mathematical model and vision techniques, the developed algorithm is compared with well-known algorithms on different virtual scenes created in the V-REP simulation program and a physical setup created in a laboratory environment. Results are promising and demonstrate effectiveness of the proposed algorithm.

</details>


### [37] [Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification](https://arxiv.org/abs/2511.07929)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: FedMedCLIP结合CLIP与FL，通过FAM和MLP模块优化通信与计算效率，在医学图像分类中实现高性能与低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 深度模型在医学成像中表现优异，但训练需依赖源数据，引发隐私问题。联邦学习（FL）虽提供解决方案，但数据异构性和资源成本限制了其部署，尤其是在使用视觉语言模型（VLM）时。

Method: 提出了一种基于CLIP的联邦学习方法FedMedCLIP，包括掩码特征适应模块（FAM）减少通信负载，冻结CLIP编码器降低计算开销，设计掩码多层感知机（MLP）作为本地分类器，并采用自适应KL散度蒸馏正则化方法促进FAM与MLP间的相互学习。

Result: 在四个公开医学数据集上的实验表明，FedMedCLIP性能优越（如ISIC2019上比次优基线高8%），资源成本合理（如比FedAVG快120倍）。

Conclusion: 本文提出的FedMedCLIP方法通过结合对比性语言-图像预训练（CLIP）和联邦学习（FL），在医学图像分类任务中实现了高性能和低资源消耗。实验证明，该方法在四个公开医学数据集上表现优异，资源成本合理。

Abstract: Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).

</details>


### [38] [Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers](https://arxiv.org/abs/2511.07934)
*Sida Huang,Siqi Huang,Ping Luo,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 提出Laytrol网络和LaySyn数据集，解决布局到图像生成中的视觉质量与风格一致性问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有的布局到图像生成方法常导致生成图像视觉质量低且与基础模型风格不一致，表明预训练知识丢失。

Method: 通过构建Layout Synthesis（LaySyn）数据集和设计Layout Control Network（Laytrol），其中参数继承自MM-DiT，并采用特定初始化方案和Object-level Rotary Position Embedding技术。

Result: 定性和定量实验验证了所提方法的有效性。

Conclusion: 实验证明，所提出的Layout Control Network（Laytrol）有效提高了布局到图像生成任务的视觉质量和风格一致性，同时保留了预训练模型的知识。

Abstract: With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.

</details>


### [39] [DiffRegCD: Integrated Registration and Change Detection with Diffusion Features](https://arxiv.org/abs/2511.07935)
*Seyedehnanita Madani,Rama Chellappa,Vishal M. Patel*

Main category: cs.CV

TL;DR: DiffRegCD是一个统一密集配准和变化检测的框架，利用高斯平滑分类和预训练扩散特征，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像通常存在视差、视角偏移和时间间隔长等问题，导致严重的错位，传统方法和现有联合框架在大位移情况下表现不佳。

Method: DiffRegCD是一个集成的框架，将密集配准和变化检测统一在单一模型中，通过高斯平滑分类任务重新定义对应关系估计，并利用预训练的降噪扩散模型的多尺度特征。

Result: DiffRegCD在多个数据集（LEVIR-CD、DSIFN-CD、WHU-CD、SYSU-CD、VL-CMU-CD）上超越了现有基线，并在广泛的时间和几何变化下保持可靠性。

Conclusion: DiffRegCD通过将密集配准和变化检测统一在单一模型中，利用高斯平滑分类任务实现亚像素精度和稳定训练，展示了扩散特征和基于分类的对应关系作为统一变化检测的坚实基础。

Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.

</details>


### [40] [Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?](https://arxiv.org/abs/2511.07940)
*Rui-Qing Sun,Ang Li,Zhijing Wu,Tian Lan,Qianyu Lu,Xingshan Yao,Chen Xu,Xian-Ling Mao*

Main category: cs.CV

TL;DR: ISExplore通过选择信息丰富的5秒视频片段，显著提升TFG方法效率，保持高保真输出。


<details>
  <summary>Details</summary>
Motivation: 现有TFG方法需要处理数分钟的参考视频，耗时且计算负担重，研究发现视频的信息质量比长度更重要，因此探索如何高效选择关键片段。

Method: 提出ISExplore策略，基于音频特征多样性、嘴唇运动幅度和摄像机视角数量三个关键维度，自动识别信息丰富的5秒参考视频片段。

Result: 实验表明，ISExplore将NeRF和3DGS方法的数据处理和训练速度提升5倍以上，同时保持高质量输出。

Conclusion: ISExplore策略通过自动选择信息丰富的5秒参考视频片段，显著提高了数据处理和训练速度，同时保持了高保真输出，为TFG方法的实际应用提供了可行方案。

Abstract: Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.

</details>


### [41] [Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2511.07941)
*Zhenfeng Zhuang,Fangyu Zhou,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出一种多模态原型多示例学习方法，通过双向交互和立体最优传输算法提升病理图像分类的泛化和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决病理任务中仅提供袋级标签和LLM生成的实例级描述存在偏差的问题，以及现有视觉语言多示例学习方法单向指导的局限性。

Method: 利用冻结的大型语言模型生成特定任务的病理实体描述作为文本原型，同时视觉分支学习实例级原型以减少对冗余数据的依赖，并通过立体最优传输算法实现跨模态融合。

Result: 在三种癌症数据集上的少样本分类和可解释性实验中，所提方法展示了优异的泛化能力。

Conclusion: 提出的多模态原型多示例学习方法通过双向交互和立体最优传输算法，显著提升了模型在病理图像分类任务中的泛化能力和可解释性。

Abstract: While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.

</details>


### [42] [ReIDMamba: Learning Discriminative Features with Visual State Space Model for Person Re-Identification](https://arxiv.org/abs/2511.07948)
*Hongyang Gu,Qisong Yang,Lei Pu,Siming Han,Yao Ding*

Main category: cs.CV

TL;DR: ReIDMamba是一个纯Mamba驱动的行人重识别框架，通过多粒度特征提取和排名感知正则化技术，显著提升了性能并降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer方法在行人重识别中因输入序列长度增加导致的内存和计算需求二次增长的可扩展性问题。

Method: 提出了纯Mamba驱动的ReIDMamba框架，包括基于Mamba的强基线设计和两种新技术：多粒度特征提取器（MGFE）模块和排名感知的三元组正则化（RATR）。

Result: ReIDMamba在五个行人重识别基准测试中实现了最先进的性能。

Conclusion: ReIDMamba模型在五个行人重识别基准测试中表现出色，达到了最先进的性能，且参数仅为TransReID的三分之一，GPU内存使用更低，推理吞吐量更快。

Abstract: Extracting robust discriminative features is a critical challenge in person re-identification (ReID). While Transformer-based methods have successfully addressed some limitations of convolutional neural networks (CNNs), such as their local processing nature and information loss resulting from convolution and downsampling operations, they still face the scalability issue due to the quadratic increase in memory and computational requirements with the length of the input sequence. To overcome this, we propose a pure Mamba-based person ReID framework named ReIDMamba. Specifically, we have designed a Mamba-based strong baseline that effectively leverages fine-grained, discriminative global features by introducing multiple class tokens. To further enhance robust features learning within Mamba, we have carefully designed two novel techniques. First, the multi-granularity feature extractor (MGFE) module, designed with a multi-branch architecture and class token fusion, effectively forms multi-granularity features, enhancing both discrimination ability and fine-grained coverage. Second, the ranking-aware triplet regularization (RATR) is introduced to reduce redundancy in features from multiple branches, enhancing the diversity of multi-granularity features by incorporating both intra-class and inter-class diversity constraints, thus ensuring the robustness of person features. To our knowledge, this is the pioneering work that integrates a purely Mamba-driven approach into ReID research. Our proposed ReIDMamba model boasts only one-third the parameters of TransReID, along with lower GPU memory usage and faster inference throughput. Experimental results demonstrate ReIDMamba's superior and promising performance, achieving state-of-the-art performance on five person ReID benchmarks. Code is available at https://github.com/GuHY777/ReIDMamba.

</details>


### [43] [Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks](https://arxiv.org/abs/2511.07958)
*Xiaoye Liang,Lai Jiang,Minglang Qiao,Yichen Guo,Yue Zhang,Xin Deng,Shengxi Li,Yufan Liu,Mai Xu*

Main category: cs.CV

TL;DR: 提出首个爆裂图像质量评估（BuIQA）任务及数据集，并通过任务驱动提示和异构知识蒸馏框架提升评估效果，实验证明其在下游任务中的性能优势。


<details>
  <summary>Details</summary>
Motivation: 爆裂图像技术虽然提升了视觉数据的捕获和处理能力，但冗余导致存储、传输需求增加，下游任务效率降低。因此，需要一种任务驱动的爆裂图像质量评估方法（BuIQA）来优化图像选择。

Method: 通过任务驱动的提示生成网络结合异构知识蒸馏，学习下游任务的先验知识，并引入任务感知的质量评估网络，基于任务提示评估爆裂图像质量。

Result: 在10个下游场景的实验中，该方法表现优异，超越了现有技术，并在去噪和超分辨率任务中实现了PSNR提升。

Conclusion: 提出的BuIQA框架在多种下游场景中表现出色，不仅能有效评估爆裂图像质量，还能在下游任务（如去噪和超分辨率）中带来性能提升（PSNR提高0.33 dB）。

Abstract: In recent years, the development of burst imaging technology has improved the capture and processing capabilities of visual data, enabling a wide range of applications. However, the redundancy in burst images leads to the increased storage and transmission demands, as well as reduced efficiency of downstream tasks. To address this, we propose a new task of Burst Image Quality Assessment (BuIQA), to evaluate the task-driven quality of each frame within a burst sequence, providing reasonable cues for burst image selection. Specifically, we establish the first benchmark dataset for BuIQA, consisting of $7,346$ burst sequences with $45,827$ images and $191,572$ annotated quality scores for multiple downstream scenarios. Inspired by the data analysis, a unified BuIQA framework is proposed to achieve an efficient adaption for BuIQA under diverse downstream scenarios. Specifically, a task-driven prompt generation network is developed with heterogeneous knowledge distillation, to learn the priors of the downstream task. Then, the task-aware quality assessment network is introduced to assess the burst image quality based on the task prompt. Extensive experiments across 10 downstream scenarios demonstrate the impressive BuIQA performance of the proposed approach, outperforming the state-of-the-art. Furthermore, it can achieve $0.33$ dB PSNR improvement in the downstream tasks of denoising and super-resolution, by applying our approach to select the high-quality burst frames.

</details>


### [44] [Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection](https://arxiv.org/abs/2511.07966)
*Shenao Zhao,Pengpeng Liang,Zhoufan Yang*

Main category: cs.CV

TL;DR: MMAssist通过多模态辅助提升3D UDA性能，利用图像和文本特征对齐3D特征，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管点云和图像常被同时采集，但图像数据在3D UDA中的潜力尚未被充分挖掘。

Method: 提出MMAssist方法，利用图像和文本特征作为桥梁对齐3D特征，并通过预训练的视觉主干和LVLM提取特征。训练过程中对齐3D特征与图像、文本特征，并融合加权进行最终预测。

Result: 在三个流行的3D物体检测数据集上，MMAssist相比现有方法表现出色。

Conclusion: MMAssist方法通过多模态辅助显著提升了3D UDA的性能，尤其在利用图像和文本特征作为桥梁对齐3D特征方面表现突出。

Abstract: Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.

</details>


### [45] [Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection](https://arxiv.org/abs/2511.07976)
*Seyedehanita Madani,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出了一种无需修改现有网络的模块化管道，通过扩散变形和配准提升遥感变化检测的鲁棒性，实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 解决遥感变化检测中因季节或多年间隔导致的空间错位问题，提升现有模型在真实场景中的鲁棒性。

Method: 框架整合了基于扩散的语义变形、密集配准和残差流细化，通过轻量级U-Net生成高保真扭曲以配准图像对。

Result: 在LEVIR-CD、WHU-CD和DSIFN-CD数据集上的实验显示，该方法在配准精度和下游变化检测任务中均取得显著提升。

Conclusion: 提出的模块化管道通过扩散语义变形和密集配准提高了变化检测的时空鲁棒性，无需修改现有网络，实验证明了其通用性和有效性。

Abstract: Remote sensing change detection is often challenged by spatial misalignment between bi-temporal images, especially when acquisitions are separated by long seasonal or multi-year gaps. While modern convolutional and transformer-based models perform well on aligned data, their reliance on precise co-registration limits their robustness in real-world conditions. Existing joint registration-detection frameworks typically require retraining and transfer poorly across domains. We introduce a modular pipeline that improves spatial and temporal robustness without altering existing change detection networks. The framework integrates diffusion-based semantic morphing, dense registration, and residual flow refinement. A diffusion module synthesizes intermediate morphing frames that bridge large appearance gaps, enabling RoMa to estimate stepwise correspondences between consecutive frames. The composed flow is then refined through a lightweight U-Net to produce a high-fidelity warp that co-registers the original image pair. Extensive experiments on LEVIR-CD, WHU-CD, and DSIFN-CD show consistent gains in both registration accuracy and downstream change detection across multiple backbones, demonstrating the generality and effectiveness of the proposed approach.

</details>


### [46] [DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion](https://arxiv.org/abs/2511.07978)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: DANCE是一种新型点云补全框架，通过射线采样和变换器解码器处理缺失区域，无需外部图像监督即可实现类别一致的补全，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理真实场景中可变稀疏性和有限监督时的不足，提出一种密度无关且类别感知的点云补全框架。

Method: DANCE通过基于射线的采样生成候选点，利用变换器解码器精确定位并预测不透明度分数，结合轻量级分类头实现类别一致的补全。

Result: 在PCN和MVP基准测试中，DANCE在准确性和结构一致性上优于现有方法，并对输入密度和噪声具有鲁棒性。

Conclusion: DANCE框架在点云补全任务中表现出色，尤其在处理不同输入密度和噪声水平时，展现出优于现有方法的准确性和结构一致性。

Abstract: Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.

</details>


### [47] [ChexFract: From General to Specialized - Enhancing Fracture Description Generation](https://arxiv.org/abs/2511.07983)
*Nikolay Nechaev,Evgeniia Przhezdzetskaia,Dmitry Umerenkov,Dmitry V. Dylov*

Main category: cs.CV

TL;DR: 本文提出了一种针对骨折病理检测和描述的专用视觉语言模型，显著提升了罕见病理报告的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前通用视觉语言模型在描述罕见但临床重要的病理（如骨折）方面表现不足，急需改进。

Method: 利用MAIRA-2和CheXagent的编码器训练骨折专用视觉语言模型。

Result: 专用模型在生成准确骨折描述方面显著优于通用模型，并分析了不同骨折类型、位置和年龄的表现差异。

Conclusion: 公开了性能最佳的骨折报告模型，为未来罕见病理的准确报告研究提供了支持。

Abstract: Generating accurate and clinically meaningful radiology reports from chest X-ray images remains a significant challenge in medical AI. While recent vision-language models achieve strong results in general radiology report generation, they often fail to adequately describe rare but clinically important pathologies like fractures. This work addresses this gap by developing specialized models for fracture pathology detection and description. We train fracture-specific vision-language models with encoders from MAIRA-2 and CheXagent, demonstrating significant improvements over general-purpose models in generating accurate fracture descriptions. Analysis of model outputs by fracture type, location, and age reveals distinct strengths and limitations of current vision-language model architectures. We publicly release our best-performing fracture-reporting model, facilitating future research in accurate reporting of rare pathologies.

</details>


### [48] [CSF-Net: Context-Semantic Fusion Network for Large Mask Inpainting](https://arxiv.org/abs/2511.07987)
*Chae-Yeon Heo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: CSF-Net是一种语义引导的图像修复框架，通过融合结构感知候选与上下文特征提升修复质量。


<details>
  <summary>Details</summary>
Motivation: 解决大范围掩码图像修复中因内容缺失和上下文线索有限导致的挑战。

Method: 提出了一种基于transformer的Context-Semantic Fusion Network (CSF-Net)，融合预训练Amodal Completion模型生成的结构感知候选与上下文特征，生成语义引导图像。

Result: 在Places365和COCOA数据集上的实验表明，CSF-Net有效减少了对象幻觉，提升了视觉真实性和语义对齐。

Conclusion: CSF-Net通过结合语义先验和上下文特征，显著提升了图像修复的结构准确性和语义一致性，且无需修改现有模型架构即可集成。

Abstract: In this paper, we propose a semantic-guided framework to address the challenging problem of large-mask image inpainting, where essential visual content is missing and contextual cues are limited. To compensate for the limited context, we leverage a pretrained Amodal Completion (AC) model to generate structure-aware candidates that serve as semantic priors for the missing regions. We introduce Context-Semantic Fusion Network (CSF-Net), a transformer-based fusion framework that fuses these candidates with contextual features to produce a semantic guidance image for image inpainting. This guidance improves inpainting quality by promoting structural accuracy and semantic consistency. CSF-Net can be seamlessly integrated into existing inpainting models without architectural changes and consistently enhances performance across diverse masking conditions. Extensive experiments on the Places365 and COCOA datasets demonstrate that CSF-Net effectively reduces object hallucination while enhancing visual realism and semantic alignment. The code for CSF-Net is available at https://github.com/chaeyeonheo/CSF-Net.

</details>


### [49] [Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture](https://arxiv.org/abs/2511.07990)
*Charalampos S. Kouzinopoulos,Yuri Manna*

Main category: cs.CV

TL;DR: 提出低功耗边缘AI系统，通过模型压缩技术在微控制器上实现高效实时杂草检测，适用于资源有限的农业场景。


<details>
  <summary>Details</summary>
Motivation: 杂草严重影响全球作物产量，传统除草方法依赖化学药剂，存在环境污染和抗药性风险。精准除草技术因依赖高功耗计算平台而受限。

Method: 采用了结构化剪枝、整数量化和输入图像分辨率缩放等多种压缩技术，以适配严格的硬件限制。模型在包含74种植物的CropAndWeed数据集上训练和评估。

Result: 系统实现了每推理仅51.8mJ的超低能耗，支持实时原位杂草检测，平衡了检测精度与效率。

Conclusion: 该论文展示了一种基于YOLOv8n的低功耗边缘AI系统，能在STM32U575ZI微控制器上实现实时杂草检测，为资源受限的农业环境提供了可扩展的解决方案。

Abstract: Weeds significantly reduce crop yields worldwide and pose major challenges to sustainable agriculture. Traditional weed management methods, primarily relying on chemical herbicides, risk environmental contamination and lead to the emergence of herbicide-resistant species. Precision weeding, leveraging computer vision and machine learning methods, offers a promising eco-friendly alternative but is often limited by reliance on high-power computational platforms. This work presents an optimized, low-power edge AI system for weeds detection based on the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. Several compression techniques are applied to the detection model, including structured pruning, integer quantization and input image resolution scaling in order to meet strict hardware constraints. The model is trained and evaluated on the CropAndWeed dataset with 74 plant species, achieving a balanced trade-off between detection accuracy and efficiency. Our system supports real-time, in-situ weeds detection with a minimal energy consumption of 51.8mJ per inference, enabling scalable deployment in power-constrained agricultural environments.

</details>


### [50] [Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning](https://arxiv.org/abs/2511.08003)
*Jialong Qin,Xin Zou,Di Lu,Yibo Yan,Xuming Hu*

Main category: cs.CV

TL;DR: SharpV通过动态剪枝和自校准KV缓存剪枝，高效解决了视频大型语言模型的计算和缓存问题，性能优越且兼容硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前视频大型语言模型因处理冗余视觉令牌而面临二次计算复杂度和KV缓存扩展问题，亟需高效剪枝方法。

Method: SharpV采用两阶段剪枝框架，包括基于时空信息的动态剪枝比率调整和基于视觉特征相似性的自校准KV缓存剪枝。

Result: 在多个公开基准测试中，SharpV表现优越，且是首个无需暴露注意力分数的两阶段剪枝框架，兼容硬件加速技术。

Conclusion: SharpV提出了一种高效的自适应视觉令牌和KV缓存剪枝方法，通过动态调整剪枝比率和基于信息瓶颈的自校准机制，不仅解决了计算复杂度和缓存扩展问题，还在某些情况下超越了密集模型的性能。

Abstract: Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.

</details>


### [51] [EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision](https://arxiv.org/abs/2511.08007)
*Yifei Cao,Yu Liu,Guolong Wang,Zhu Liu,Kai Wang,Xianjie Zhang,Jizhe Yu,Xun Tu*

Main category: cs.CV

TL;DR: EAGLE框架通过外观和几何感知内存机制，显著提升了自我中心视觉查询定位的准确性，并在Ego4D-VQ基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自我中心视觉查询定位在AI和VR/AR中至关重要，但由于相机运动、视角变化和外观变化等原因，仍具有挑战性。

Method: EAGLE整合了由外观感知元学习内存（AMM）引导的分割和由几何感知定位内存（GLM）驱动的跟踪，通过结构化外观和几何内存库存储高置信度检索样本，支持目标外观变化的长期和短期建模。此外，通过将VQL-2D输出与视觉几何基础的Transformer（VGGT）结合，实现了2D和3D任务的高效统一。

Result: 该方法在Ego4D-VQ基准测试中达到了最先进的性能。

Conclusion: EAGLE框架通过结合外观感知和几何感知的内存机制，在自我中心视觉查询定位中实现了显著的性能提升，并在Ego4D-VQ基准测试中达到了最先进的水平。

Abstract: Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.

</details>


### [52] [Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving](https://arxiv.org/abs/2511.08015)
*Jian Wang,Lijun He,Yixing Yong,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: AdvRoad通过生成自然的路面风格对抗海报，有效攻击视觉3D检测模型，并在实验和物理攻击中验证了其泛化能力和实际威胁。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度神经网络的视觉3D检测模型易受对抗样本攻击，而现有对抗海报的外观不自然且内容固定，易被察觉和防御。

Method: 采用两阶段方法：路面风格对抗生成和场景关联适应，以最大化攻击效果并确保海报的自然外观。

Result: 实验表明AdvRoad在不同检测器、场景和欺骗位置中泛化良好，物理攻击进一步验证了其在实际环境中的威胁。

Conclusion: AdvRoad有效生成了多样化且自然的路面风格对抗海报，能够在不同检测器、场景和欺骗位置中泛化良好，并在真实环境中展示了实际威胁。

Abstract: Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.

</details>


### [53] [High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection](https://arxiv.org/abs/2511.08018)
*Zhiyuan Chen,Yuelin Guo,Zitong Huang,Haoyu He,Renhao Lu,Weizhe Zhang*

Main category: cs.CV

TL;DR: 提出Cascade HQP-DETR，通过高质量数据生成、提案引导查询和级联去噪，解决了合成数据训练的三大问题，在PASCAL VOC 2007上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成数据集上存在提示简单、图像质量差和弱监督问题，DETR检测器因随机查询初始化导致收敛慢和过拟合合成模式，均匀去噪压力导致模型过拟合伪标签噪声。

Method: 提出了Cascade HQP-DETR方法，包括高质量数据生成流程（使用LLaMA-3、Flux和Grounding DINO生成FluxVOC和FluxCOCO数据集）、高质量提案引导的查询编码（利用SAM生成的提案和RoI池化特征初始化对象查询）以及级联去噪算法（通过逐步提高IoU阈值动态调整训练权重）。

Result: Cascade HQP-DETR在PASCAL VOC 2007上取得了61.04%的mAP@0.5，表现优于现有基线。

Conclusion: Cascade HQP-DETR在仅使用FluxVOC数据集训练12个周期后，在PASCAL VOC 2007上达到了61.04%的mAP@0.5，超越了现有基线，证明了该架构的普适性。

Abstract: Object detection models demand large-scale annotated datasets, which are costly and labor-intensive to create. This motivated Imaginary Supervised Object Detection (ISOD), where models train on synthetic images and test on real images. However, existing methods face three limitations: (1) synthetic datasets suffer from simplistic prompts, poor image quality, and weak supervision; (2) DETR-based detectors, due to their random query initialization, struggle with slow convergence and overfitting to synthetic patterns, hindering real-world generalization; (3) uniform denoising pressure promotes model overfitting to pseudo-label noise. We propose Cascade HQP-DETR to address these limitations. First, we introduce a high-quality data pipeline using LLaMA-3, Flux, and Grounding DINO to generate the FluxVOC and FluxCOCO datasets, advancing ISOD from weak to full supervision. Second, our High-Quality Proposal guided query encoding initializes object queries with image-specific priors from SAM-generated proposals and RoI-pooled features, accelerating convergence while steering the model to learn transferable features instead of overfitting to synthetic patterns. Third, our cascade denoising algorithm dynamically adjusts training weights through progressively increasing IoU thresholds across decoder layers, guiding the model to learn robust boundaries from reliable visual cues rather than overfitting to noisy labels. Trained for just 12 epochs solely on FluxVOC, Cascade HQP-DETR achieves a SOTA 61.04\% mAP@0.5 on PASCAL VOC 2007, outperforming strong baselines, with its competitive real-data performance confirming the architecture's universal applicability.

</details>


### [54] [Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031)
*Chende Zheng,Ruiqi Suo,Zhoulin Ji,Jingyi Deng,Fangbin Yi,Chenhao Lin,Chao Shen*

Main category: cs.CV

TL;DR: 该论文提出一种基于FPN-Transformer的多模态深度伪造检测与定位框架，通过跨模态特征联合分析和帧级定位，有效应对复杂环境中的深度伪造挑战。


<details>
  <summary>Details</summary>
Motivation: 生成对抗网络（GANs）和扩散模型的快速发展使得深度伪造内容高度逼真，对数字信任构成重大威胁。现有的单模态检测方法在利用跨模态关联和精确定位伪造片段方面存在局限，因此需要开发更有效的多模态检测方法。

Method: 使用预训练的自监督模型（WavLM用于音频，CLIP用于视频）提取层次化时间特征，通过R-TLM块构建多尺度特征金字塔，结合局部注意力机制实现跨上下文时间依赖的联合分析。双分支预测头同时预测伪造概率并细化操纵片段的时间偏移，实现帧级定位精度。

Result: 在IJCAI'25 DDL-AV基准测试集上，该方法在跨模态深度伪造检测与定位任务中表现出色，最终得分为0.7535。

Conclusion: 该论文提出的基于特征金字塔-Transformer的多模态深度伪造检测与定位框架，在跨模态泛化和时间边界回归方面填补了关键空白，实验结果表明其有效性，并为广义深度伪造检测提供了新思路。

Abstract: The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL

</details>


### [55] [Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric](https://arxiv.org/abs/2511.08032)
*Zhaolin Wan,Yining Diao,Jingqi Xu,Hao Wang,Zhiyang Li,Xiaopeng Fan,Wangmeng Zuo,Debin Zhao*

Main category: cs.CV

TL;DR: The paper introduces 3DGS-QA, a dataset for assessing 3DGS perceptual quality, and a no-reference quality prediction model that outperforms existing methods, addressing a gap in current research.


<details>
  <summary>Details</summary>
Motivation: Prior research on 3DGS has focused on algorithmic performance and visual fidelity, but the perceptual quality under varying reconstruction conditions remains unexplored. This gap motivates the creation of a dedicated dataset and quality assessment model.

Method: The research involves creating the 3DGS-QA dataset with 225 degraded reconstructions across 15 object types and developing a no-reference quality prediction model that extracts spatial and photometric cues from Gaussian primitives.

Result: The proposed model consistently achieves superior performance in quality assessment, demonstrating robustness and effectiveness for 3DGS content evaluation.

Conclusion: The study successfully introduces 3DGS-QA, a novel dataset for assessing the perceptual quality of 3D Gaussian Splatting (3DGS) rendered content, and proposes a no-reference quality prediction model that outperforms existing methods.

Abstract: With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.

</details>


### [56] [WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation](https://arxiv.org/abs/2511.08036)
*Gongshu Wang,Zhirui Wang,Kan Yang*

Main category: cs.CV

TL;DR: WEDepth利用预训练视觉基础模型（VFM）的先验知识，在不修改其结构和权重的情况下，实现了单目深度估计（MDE）的最新性能，并展示了零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计由于从单张2D图像重建3D场景的固有不适定性而极具挑战性。预训练的VFM在大型多样化数据集上表现出卓越的世界理解能力，为改进MDE提供了潜力。

Method: WEDepth将VFM作为多级特征增强器，系统地在不同表示级别注入先验知识。

Result: 在NYU-Depth v2和KITTI数据集上的实验表明，WEDepth达到了新的SOTA性能，与扩散式方法和相对深度预训练方法相比具有竞争力。

Conclusion: WEDepth通过有效利用预训练视觉基础模型（VFM）的先验知识，在不修改其结构和权重的情况下，实现了单目深度估计（MDE）的最新性能，并展示了强大的零样本迁移能力。

Abstract: Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.

</details>


### [57] [ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation](https://arxiv.org/abs/2511.08046)
*Aya Elgebaly,Nikolaos Delopoulos,Juliane Hörner-Rieber,Carolin Rippke,Sebastian Klüter,Luca Boldrini,Lorenzo Placidi,Riccardo Dal Bello,Nicolaus Andratschke,Michael Baumgartl,Claus Belka,Christopher Kurz,Guillaume Landry,Shadi Albarqouni*

Main category: cs.CV

TL;DR: ProSona通过自然语言提示学习注释风格的连续潜在空间，实现个性化医学图像分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中高观察者间变异性问题，特别是专家意见不一致的任务（如肺结节描绘）。

Method: ProSona采用两阶段框架：概率U-Net捕捉多样化的专家假设，提示引导的投影机制在潜在空间中导航以生成个性化分割，多级对比目标对齐文本和视觉表示。

Result: 在LIDC-IDRI肺结节和多机构前列腺MRI数据集中，ProSona将广义能量距离降低17%，平均Dice分数提高超过1个百分点。

Conclusion: ProSona通过自然语言提示提供灵活、准确且可解释的个性化医学图像分割控制，显著优于现有方法。

Abstract: Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .

</details>


### [58] [Generalized-Scale Object Counting with Gradual Query Aggregation](https://arxiv.org/abs/2511.08048)
*Jer Pelhan,Alan Lukezic,Matej Kristan*

Main category: cs.CV

TL;DR: GECO2 是一种新型少样本计数和检测方法，通过跨尺度特征聚合解决物体尺寸多样性问题，性能显著提升且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有的计数器因临时解决方案（如上采样和分块处理）难以处理多样尺寸物体和密集小物体区域的问题。

Method: 提出了一种新的密集查询表示方法，逐步跨尺度聚合特定于样本的特征信息，生成高分辨率密集查询，从而实现对大小物体的检测。

Result: GECO2 在计数和检测准确率上超越了现有最优方法 10%，同时运行速度快 3 倍且 GPU 内存占用更小。

Conclusion: GECO2 是一种端到端的少样本计数和检测方法，通过显式解决物体尺度问题，显著提升了在多样尺寸物体和密集小物体区域的性能。

Abstract: Few-shot detection-based counters estimate the number of instances in the image specified only by a few test-time exemplars. A common approach to localize objects across multiple sizes is to merge backbone features of different resolutions. Furthermore, to enable small object detection in densely populated regions, the input image is commonly upsampled and tiling is applied to cope with the increased computational and memory requirements. Because of these ad-hoc solutions, existing counters struggle with images containing diverse-sized objects and densely populated regions of small objects. We propose GECO2, an end-to-end few-shot counting and detection method that explicitly addresses the object scale issues. A new dense query representation gradually aggregates exemplar-specific feature information across scales that leads to high-resolution dense queries that enable detection of large as well as small objects. GECO2 surpasses state-of-the-art few-shot counters in counting as well as detection accuracy by 10% while running 3x times faster at smaller GPU memory footprint.

</details>


### [59] [Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching](https://arxiv.org/abs/2511.08061)
*Aditi Singhania,Arushi Jain,Krutik Malani,Riddhi Dhawan,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 结合LoRA微调扩散模型和潜在连接策略，通过两阶段数据整理和CHARIS评估，实现了高一致性与多样性的图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决在多样上下文中生成特定主题图像时，身份一致性与提示多样性之间的基本权衡问题。

Method: 采用LoRA微调的扩散模型结合潜在连接策略，以及两阶段蒸馏数据整理框架（数据恢复和VLM过滤的第一阶段，参数高效微调的第二阶段）。

Result: 提出的方法在身份一致性、提示适应性、区域颜色保真度、视觉质量和变换多样性五个关键轴上实现了精细评估，展示了强大的生成能力。

Conclusion: 本文提出了一种结合LoRA微调扩散模型和潜在连接策略的方法，通过两阶段蒸馏数据整理框架和CHARIS评估框架，实现了在多样上下文中生成具有强身份一致性的图像。

Abstract: Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.

</details>


### [60] [I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks](https://arxiv.org/abs/2511.08065)
*Ruichen Ma,Liwei Meng,Guanchao Qiao,Ning Ning,Yang Liu,Shaogang Hu*

Main category: cs.CV

TL;DR: I2E框架将静态图像快速转换为事件流，解决了SNN训练数据稀缺问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: SNN的高能效计算潜力受限于事件流数据的稀缺，I2E旨在解决这一瓶颈。

Method: 通过高度并行化的卷积模拟微眼跳运动，将静态图像转换为高保真事件流，转换速度比现有方法快300倍以上。

Result: I2E在ImageNet数据集上训练的SNN达到60.50%的准确率；在CIFAR10-DVS数据集上预训练和微调后，准确率达92.5%。

Conclusion: I2E提供了一个可扩展的解决方案，解决了SNN训练中事件流数据稀缺的问题，为高性能神经形态系统的开发奠定了基础。

Abstract: Spiking neural networks (SNNs) promise highly energy-efficient computing, but their adoption is hindered by a critical scarcity of event-stream data. This work introduces I2E, an algorithmic framework that resolves this bottleneck by converting static images into high-fidelity event streams. By simulating microsaccadic eye movements with a highly parallelized convolution, I2E achieves a conversion speed over 300x faster than prior methods, uniquely enabling on-the-fly data augmentation for SNN training. The framework's effectiveness is demonstrated on large-scale benchmarks. An SNN trained on the generated I2E-ImageNet dataset achieves a state-of-the-art accuracy of 60.50%. Critically, this work establishes a powerful sim-to-real paradigm where pre-training on synthetic I2E data and fine-tuning on the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of 92.5%. This result validates that synthetic event data can serve as a high-fidelity proxy for real sensor data, bridging a long-standing gap in neuromorphic engineering. By providing a scalable solution to the data problem, I2E offers a foundational toolkit for developing high-performance neuromorphic systems. The open-source algorithm and all generated datasets are provided to accelerate research in the field.

</details>


### [61] [Radar-APLANC: Unsupervised Radar-based Heartbeat Sensing via Augmented Pseudo-Label and Noise Contrast](https://arxiv.org/abs/2511.08071)
*Ying Wang,Zhaodong Sun,Xu Cheng,Zuxian He,Xiaobai Li*

Main category: cs.CV

TL;DR: 提出首个无监督雷达心跳检测框架（Radar-APLANC），通过噪声对比和伪标签增强，无需真实标记即可达到监督方法性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统雷达心跳检测方法因噪声导致性能下降，以及基于学习的方法需要昂贵标记信号的问题。

Method: 使用雷达范围矩阵中的心跳范围和噪声范围构建正负样本，提出噪声对比三重损失（NCT）和自适应噪声感知标签选择的伪标签增强方法。

Result: 在 Equipleth 数据集和自收集雷达数据集上的实验表明，该方法性能与最先进的监督方法相当。

Conclusion: Radar-APLANC 是一种无监督框架，通过增强伪标签和噪声对比，实现了与传统监督方法相当的心跳检测性能，且无需依赖昂贵的真实生理信号。

Abstract: Frequency Modulated Continuous Wave (FMCW) radars can measure subtle chest wall oscillations to enable non-contact heartbeat sensing. However, traditional radar-based heartbeat sensing methods face performance degradation due to noise. Learning-based radar methods achieve better noise robustness but require costly labeled signals for supervised training. To overcome these limitations, we propose the first unsupervised framework for radar-based heartbeat sensing via Augmented Pseudo-Label and Noise Contrast (Radar-APLANC). We propose to use both the heartbeat range and noise range within the radar range matrix to construct the positive and negative samples, respectively, for improved noise robustness. Our Noise-Contrastive Triplet (NCT) loss only utilizes positive samples, negative samples, and pseudo-label signals generated by the traditional radar method, thereby avoiding dependence on expensive ground-truth physiological signals. We further design a pseudo-label augmentation approach featuring adaptive noise-aware label selection to improve pseudo-label signal quality. Extensive experiments on the Equipleth dataset and our collected radar dataset demonstrate that our unsupervised method achieves performance comparable to state-of-the-art supervised methods. Our code, dataset, and supplementary materials can be accessed from https://github.com/RadarHRSensing/Radar-APLANC.

</details>


### [62] [CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion](https://arxiv.org/abs/2511.08075)
*Cameron Braunstein,Mariya Toneva,Eddy Ilg*

Main category: cs.CV

TL;DR: 研究发现Stable Diffusion的语义理解能力主要来自CLIP文本编码，而非扩散过程；扩散过程更多作为视觉解码器。


<details>
  <summary>Details</summary>
Motivation: 探究潜在扩散模型（如Stable Diffusion）在文本到图像生成过程中是否包含对人类有意义的语义信息。

Method: 通过简单的回归层对Stable Diffusion进行探测，预测对象的语义属性，并与人类注释进行比较。

Result: 发现语义理解的成功主要归因于CLIP中的文本编码而非反向扩散过程；特定语义属性的解码准确率差异显著；在反向扩散过程中，属性间的区分难度增加。

Conclusion: 研究表明，CLIP视觉语言模型而非扩散过程决定了人类般的语义表征，扩散过程主要充当视觉解码器的作用。

Abstract: Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.

</details>


### [63] [Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis](https://arxiv.org/abs/2511.08087)
*Aditi Singhania,Krutik Malani,Riddhi Dhawan,Arushi Jain,Garv Tandon,Nippun Sharma,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 提出层次化评估框架Beyond the Pixels，通过特征级转换和结构化推理提升生成模型身份一致性评估，与人类判断一致并推出新基准。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖全局嵌入或粗略的VLM提示，无法捕捉细粒度身份变化，且诊断能力有限。

Method: 引入层次化评估框架，将身份评估分解为特征级转换，并通过结构化推理引导VLMs进行具体转换分析。

Result: 在四种先进生成模型上验证了框架的有效性，并推出了包含1,078个图像-提示对的新基准测试。

Conclusion: Beyond the Pixels框架通过层次化分解和结构化推理，显著提升了生成模型中身份一致性的评估效果，并与人类判断高度一致。

Abstract: Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.

</details>


### [64] [StableMorph: High-Quality Face Morph Generation with Stable Diffusion](https://arxiv.org/abs/2511.08090)
*Wassim Kabbani,Kiran Raja,Raghavendra Ramachandra,Christoph Busch*

Main category: cs.CV

TL;DR: StableMorph 是一种新型的基于扩散的图像合成方法，能生成高质量、无伪影的变形人脸图像，有效欺骗识别系统，提升生物特征安全评估的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有变形生成方法产生的图像质量差，易被检测，无法反映真实世界中的高风险攻击，因此需要更高质量的变形图像来开发和评估MAD系统。

Method: 采用基于扩散的图像合成方法，生成全头部图像，避免常见视觉缺陷，并提供对视觉属性的高度控制。

Result: StableMorph 生成的图像不仅质量高且能有效欺骗人脸识别系统，为MAD解决方案带来更大挑战，并设定了变形质量的新标准。

Conclusion: StableMorph 通过生成高质量、无伪影的变形人脸图像，为生物特征安全评估提供了更真实的攻击样本，并支持开发更强大的检测系统。

Abstract: Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.

</details>


### [65] [Introducing Nylon Face Mask Attacks: A Dataset for Evaluating Generalised Face Presentation Attack Detection](https://arxiv.org/abs/2511.08114)
*Manasa,Sushrut Patwardhan,Narayan Vetrekar,Pavan Kumar,R. S. Gad,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 论文提出了一种针对尼龙面罩（NFMs）的新型数据集，评估了五种PAD方法的性能，结果显示了在面对这种高级欺骗手段时的技术局限性，并呼吁开发更具泛化能力的PAD技术。


<details>
  <summary>Details</summary>
Motivation: 随着面部识别系统在智能手机认证、访问控制和边境安全等领域的广泛应用，这些系统对呈现攻击（PAs）的脆弱性日益凸显，尤其是尼龙面罩（NFMs）这种具有弹性结构和逼真外观的攻击工具，能够紧密模拟受害者的面部几何形状。

Method: 研究引入了一个新的数据集，专注于尼龙面罩（NFMs）这一新颖且现实的呈现攻击工具，并收集了来自100名受试者的3,760个真实样本和51,281个NFM攻击样本，涵盖了四种不同的呈现场景。使用五种最先进的PAD方法对数据集进行了基准测试。

Result: 基准测试结果显示，不同方法在面对未见过的攻击条件时表现差异显著，凸显了尼龙面罩带来的挑战。

Conclusion: 该论文强调了开发能够有效应对新型欺骗威胁的面部呈现攻击检测（PAD）技术的重要性，尤其是在面对如尼龙面罩（NFMs）这类高级3D欺骗场景时。

Abstract: Face recognition systems are increasingly deployed across a wide range of applications, including smartphone authentication, access control, and border security. However, these systems remain vulnerable to presentation attacks (PAs), which can significantly compromise their reliability. In this work, we introduce a new dataset focused on a novel and realistic presentation attack instrument called Nylon Face Masks (NFMs), designed to simulate advanced 3D spoofing scenarios. NFMs are particularly concerning due to their elastic structure and photorealistic appearance, which enable them to closely mimic the victim's facial geometry when worn by an attacker. To reflect real-world smartphone-based usage conditions, we collected the dataset using an iPhone 11 Pro, capturing 3,760 bona fide samples from 100 subjects and 51,281 NFM attack samples across four distinct presentation scenarios involving both humans and mannequins. We benchmark the dataset using five state-of-the-art PAD methods to evaluate their robustness under unseen attack conditions. The results demonstrate significant performance variability across methods, highlighting the challenges posed by NFMs and underscoring the importance of developing PAD techniques that generalise effectively to emerging spoofing threats.

</details>


### [66] [LatentPrintFormer: A Hybrid CNN-Transformer with Spatial Attention for Latent Fingerprint identification](https://arxiv.org/abs/2511.08119)
*Arnab Maity,Manasa,Pavan Kumar C,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: LatentPrintFormer结合CNN与Transformer，通过空间注意力提升潜在指纹识别率，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 潜在指纹识别因图像质量低、背景噪声和部分印记而具有挑战性，需新的方法提升识别效果。

Method: 模型结合了CNN（EfficientNet-B0）和Transformer（Swin Tiny）骨干网络，提取局部与全局特征，并利用空间注意力模块突出高质量区域。特征融合后投影至512维嵌入空间，使用余弦相似度进行匹配。

Result: 在两种公开数据集上，LatentPrintFormer在Rank-10识别率上优于三种现有技术。

Conclusion: LatentPrintFormer在公开数据集上表现优于现有技术，显著提高了潜在指纹的识别率。

Abstract: Latent fingerprint identification remains a challenging task due to low image quality, background noise, and partial impressions. In this work, we propose a novel identification approach called LatentPrintFormer. The proposed model integrates a CNN backbone (EfficientNet-B0) and a Transformer backbone (Swin Tiny) to extract both local and global features from latent fingerprints. A spatial attention module is employed to emphasize high-quality ridge regions while suppressing background noise. The extracted features are fused and projected into a unified 512-dimensional embedding, and matching is performed using cosine similarity in a closed-set identification setting. Extensive experiments on two publicly available datasets demonstrate that LatentPrintFormer consistently outperforms three state-of-the-art latent fingerprint recognition techniques, achieving higher identification rates across Rank-10.

</details>


### [67] [OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition](https://arxiv.org/abs/2511.08133)
*Lixu Sun,Nurmemet Yolwas,Wushour Silamu*

Main category: cs.CV

TL;DR: OTSNet通过三阶段网络（DAME、PAM/SQ、MMCV）解决STR中的跨模态错位问题，在多个数据集上刷新了性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有的STR框架在视觉-语言解耦优化中存在跨模态错位问题，导致错误传播加剧，尤其是在处理不规则文本时表现不佳。

Method: OTSNet采用了一个受人类视觉感知启发的Observation-Thinking-Spelling流程，包括Dual Attention Macaron Encoder（DAME）、Position-Aware Module（PAM）和Semantic Quantizer（SQ）以及Multi-Modal Collaborative Verifier（MMCV）三个核心组件。

Result: 在Union14M-L基准测试中达到83.5%的平均准确率，在OST数据集上达到79.1%的准确率，在14个评估场景中的9个创造了新记录。

Conclusion: OTSNet通过其创新的三阶段网络架构，在Scene Text Recognition（STR）领域实现了最先进的性能，特别是在处理不规则文本图案时表现优异。

Abstract: Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.

</details>


### [68] [PEOD: A Pixel-Aligned Event-RGB Benchmark for Object Detection under Challenging Conditions](https://arxiv.org/abs/2511.08140)
*Luoping Cui,Hanqing Liu,Mingjie Liu,Endian Lin,Donghong Jiang,Yuhao Wang,Chuang Zhu*

Main category: cs.CV

TL;DR: PEOD是首个大规模、像素对齐、高分辨率（1280 x 720）的Event-RGB数据集，用于挑战条件下的物体检测，揭示了现有融合方法在极端光照下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有Event-RGB数据集在极端条件覆盖和空间分辨率（<= 640 x 480）上受限，无法全面评估挑战场景下的检测器性能。

Method: 提出了PEOD数据集，包含130+时空对齐序列和340k手动标注框，57%数据在低光、过曝和高速运动条件下采集。对14种方法在三种输入配置（事件、RGB、事件-RGB融合）上进行基准测试。

Result: 在全测试集和正常子集上，融合模型表现优异；在光照挑战子集上，顶级事件模型优于所有融合模型，但融合模型仍优于RGB模型，表明现有融合方法在帧模态严重退化时的局限性。

Conclusion: PEOD提供了一个高质量、多模态感知的基准数据集，促进了未来在极端条件下的物体检测研究。

Abstract: Robust object detection for challenging scenarios increasingly relies on event cameras, yet existing Event-RGB datasets remain constrained by sparse coverage of extreme conditions and low spatial resolution (<= 640 x 480), which prevents comprehensive evaluation of detectors under challenging scenarios. To address these limitations, we propose PEOD, the first large-scale, pixel-aligned and high-resolution (1280 x 720) Event-RGB dataset for object detection under challenge conditions. PEOD contains 130+ spatiotemporal-aligned sequences and 340k manual bounding boxes, with 57% of data captured under low-light, overexposure, and high-speed motion. Furthermore, we benchmark 14 methods across three input configurations (Event-based, RGB-based, and Event-RGB fusion) on PEOD. On the full test set and normal subset, fusion-based models achieve the excellent performance. However, in illumination challenge subset, the top event-based model outperforms all fusion models, while fusion models still outperform their RGB-based counterparts, indicating limits of existing fusion methods when the frame modality is severely degraded. PEOD establishes a realistic, high-quality benchmark for multimodal perception and facilitates future research.

</details>


### [69] [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](https://arxiv.org/abs/2511.08152)
*Jun Sun,Xinxin Zhang,Simin Hong,Jian Zhu,Xiang Gao*

Main category: cs.CV

TL;DR: 提出Boomda方法，通过平衡多目标优化实现多模态领域自适应，有效解决多模态标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中手动标注成本高昂的问题，特别是在多模态设置下领域自适应研究较少的情况下。

Method: 首先引入信息瓶颈方法独立学习每个模态的表示，然后通过相关对齐在表示空间中匹配源域和目标域。将问题建模为多目标任务，寻求帕累托最优解，并简化为二次规划问题，最终得到闭式解。

Result: Boomda在多模态领域自适应任务中表现优于现有方案。

Conclusion: Boomda方法通过平衡多模态的领域对齐，提出了一种高效的模态平衡多模态领域自适应算法，并在广泛的实证结果中展示了其优越性。

Abstract: Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.

</details>


### [70] [Non-Aligned Reference Image Quality Assessment for Novel View Synthesis](https://arxiv.org/abs/2511.08155)
*Abhijay Ghildyal,Rajesh Sureddi,Nabajeet Barman,Saman Zadtootaghaj,Alan Bovik*

Main category: cs.CV

TL;DR: 提出NAR-IQA框架，通过对比学习和合成数据训练，有效评估非对齐NVS图像质量，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决新视角合成（NVS）图像质量评估中因像素不对齐导致的FR-IQA失效和NR-IQA泛化性差的问题。

Method: 基于对比学习框架，结合LoRA增强的DINOv2嵌入，并利用现有IQA方法进行监督训练。

Result: 模型在合成失真数据上训练，避免过拟合，表现优于现有方法，且用户研究表明其预测与主观评分强相关。

Conclusion: 提出的NAR-IQA框架在非对齐参考视图情况下优于现有FR-IQA、NR-IQA和NAR-IQA方法，且与主观评分高度相关。

Abstract: Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/

</details>


### [71] [LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping](https://arxiv.org/abs/2511.08156)
*Chenying Liu,Wei Huang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: LandSegmenter通过多模态弱标签数据集和自适应架构，解决了LULC基础模型的通用性与标注成本问题，零样本性能显著。


<details>
  <summary>Details</summary>
Motivation: 当前LULC模型受限于单一模态和固定分类体系，基础模型（FMs）在通用性和数据需求方面存在挑战。

Method: 提出了LandSegmenter框架，包括：（1）输入端：构建LAS数据集，利用弱标签降低标注成本；（2）模型架构：集成RS特定适配器和文本编码器；（3）输出端：引入类别置信度引导的融合策略。

Result: 在六个精确标注的LULC数据集上，LandSegmenter在迁移学习和零样本设置中表现优异，尤其在零样本场景下优于基线模型。

Conclusion: LandSegmenter框架通过输入、模型和输出三阶段的创新，有效解决了LULC领域基础模型面临的通用性和数据标注成本问题，展示了弱监督在构建任务特定基础模型中的实用性。

Abstract: Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.

</details>


### [72] [Multi-Granularity Mutual Refinement Network for Zero-Shot Learning](https://arxiv.org/abs/2511.08163)
*Ning Wang,Long Yu,Cong Hua,Guangming Zhu,Lin Mei,Syed Afaq Ali Shah,Mohammed Bennamoun,Liang Zhang*

Main category: cs.CV

TL;DR: Mg-MRN通过多粒度特征提取和跨粒度特征交互，提升了零样本学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了局部区域特征之间的内在交互，这可以进一步提升可迁移和显式视觉特征的获取。

Method: 设计了一个多粒度特征提取模块和一个跨粒度特征融合模块，用于增强局部区域特征的交互和表示能力。

Result: 在三个流行的ZSL基准数据集上的广泛实验证明了Mg-MRN方法的优越性和竞争力。

Conclusion: 提出的Mg-MRN方法通过解耦多粒度特征学习和跨粒度特征交互，显著提升了零样本学习的性能。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes with zero samples by transferring semantic knowledge from seen classes. Current approaches typically correlate global visual features with semantic information (i.e., attributes) or align local visual region features with corresponding attributes to enhance visual-semantic interactions. Although effective, these methods often overlook the intrinsic interactions between local region features, which can further improve the acquisition of transferable and explicit visual features. In this paper, we propose a network named Multi-Granularity Mutual Refinement Network (Mg-MRN), which refine discriminative and transferable visual features by learning decoupled multi-granularity features and cross-granularity feature interactions. Specifically, we design a multi-granularity feature extraction module to learn region-level discriminative features through decoupled region feature mining. Then, a cross-granularity feature fusion module strengthens the inherent interactions between region features of varying granularities. This module enhances the discriminability of representations at each granularity level by integrating region representations from adjacent hierarchies, further improving ZSL recognition performance. Extensive experiments on three popular ZSL benchmark datasets demonstrate the superiority and competitiveness of our proposed Mg-MRN method. Our code is available at https://github.com/NingWang2049/Mg-MRN.

</details>


### [73] [KPLM-STA: Physically-Accurate Shadow Synthesis for Human Relighting via Keypoint-Based Light Modeling](https://arxiv.org/abs/2511.08169)
*Xinhui Yin,Qifei Li,Yilin Guo,Hongxia Xie,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 提出KPLM和STA框架，解决合成图像中阴影真实感与几何精度不足的问题，实验验证其在复杂姿态下的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基方法（如IC-Light）在合成图像的阴影生成中仍难以兼顾高真实感和几何精度，尤其在复杂姿态下表现不足。

Method: 结合关键点线性模型（KPLM）和阴影三角算法（STA），KPLM通过九个关键点和一个边界块建模人体，实现物理合理的阴影投射和动态着色；STA通过显式几何公式计算阴影角度、长度和空间位置，提升几何精度。

Result: 实验表明，该方法在阴影真实感基准测试中达到最先进性能，尤其在复杂人体姿态下表现突出，并能泛化到多方向重光照场景。

Conclusion: 本文提出的基于关键点线性模型（KPLM）和阴影三角算法（STA）的阴影生成框架，显著提升了合成图像中阴影的真实感和几何精度，尤其在复杂人体姿态下表现优异，并能有效扩展到多方向重光照场景。

Abstract: Image composition aims to seamlessly integrate a foreground object into a background, where generating realistic and geometrically accurate shadows remains a persistent challenge. While recent diffusion-based methods have outperformed GAN-based approaches, existing techniques, such as the diffusion-based relighting framework IC-Light, still fall short in producing shadows with both high appearance realism and geometric precision, especially in composite images. To address these limitations, we propose a novel shadow generation framework based on a Keypoints Linear Model (KPLM) and a Shadow Triangle Algorithm (STA). KPLM models articulated human bodies using nine keypoints and one bounding block, enabling physically plausible shadow projection and dynamic shading across joints, thereby enhancing visual realism. STA further improves geometric accuracy by computing shadow angles, lengths, and spatial positions through explicit geometric formulations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on shadow realism benchmarks, particularly under complex human poses, and generalizes effectively to multi-directional relighting scenarios such as those supported by IC-Light.

</details>


### [74] [Distributed Zero-Shot Learning for Visual Recognition](https://arxiv.org/abs/2511.08170)
*Zhi Chen,Yadan Luo,Zi Huang,Jingjing Li,Sen Wang,Xin Yu*

Main category: cs.CV

TL;DR: DistZSL框架通过跨节点正则化和全局共识解决数据异构性，提升零样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式数据中的异构性问题，提升零样本学习在未见类别上的模型效果。

Method: 提出了跨节点属性正则化和全局属性-视觉共识两个关键组件。跨节点属性正则化确保不同节点间属性特征距离相似，全局属性-视觉共识则通过双边映射一致性减少个体节点的偏见。

Result: 实验表明，DistZSL在分布式数据学习上优于现有最先进方法。

Conclusion: DistZSL框架通过跨节点属性正则化和全局属性-视觉共识，有效解决了分布式数据中的异构性问题，显著提升了零样本学习的性能。

Abstract: In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.

</details>


### [75] [VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion](https://arxiv.org/abs/2511.08173)
*Samet Hicsonmez,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: \ours框架结合LDM和VLM，无需人工标注即实现多类别视觉异常检测，在多个数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多类别真实世界图像中的视觉异常检测存在挑战，当前基于扩散模型的方法依赖合成噪声生成且需逐类训练，限制了其泛化能力和可扩展性。

Method: \ours框架利用预训练的VLM提取详细的图像描述作为LDM训练的额外条件，从而学习到鲁棒的正常图像特征表示，用于多类别异常检测。

Result: 在Real-IAD数据集上，像素级Per-Region-Overlap (PRO)指标提升了25点；在COCO-AD数据集上提升了8点，性能优于现有最佳方法。

Conclusion: 本文提出的\ours框架通过结合Latent Diffusion Model (LDM)和Vision-Language Model (VLM)，在无需人工标注或额外训练的情况下，实现了高效的多类别视觉异常检测。其在Real-IAD和COCO-AD数据集上显著提升了性能，超越了现有基于扩散模型的方法。

Abstract: Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

</details>


### [76] [WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting](https://arxiv.org/abs/2511.08178)
*Kaitao Huang,Yan Yan,Jing-Hao Xue,Hanzi Wang*

Main category: cs.CV

TL;DR: WarpGAN通过变形修复策略改进3D GAN反演，提升遮挡区域的生成质量和多视图一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡区域生成上依赖3D GAN的生成先验，导致信息丢失和低质量结果，需改进以提升真实性和多视图一致性。

Method: 首先使用3D GAN反演编码器将单视图图像投影为潜在代码，然后利用3D GAN生成的深度图进行视图变形，最后通过SVINet结合对称性和多视图对应性修复遮挡区域。

Result: WarpGAN在遮挡区域的生成质量上表现优于现有方法。

Conclusion: WarpGAN 通过引入变形和修复策略，显著提升了遮挡区域的生成质量，定量和定性实验均证明其优于现有方法。

Abstract: 3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.

</details>


### [77] [Pixel-level Quality Assessment for Oriented Object Detection](https://arxiv.org/abs/2511.08186)
*Yunhui Zhu,Buliao Huang*

Main category: cs.CV

TL;DR: PQA框架通过像素级一致性评估替代传统IoU预测，解决了结构耦合问题，显著提升了定向目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有框级IoU预测因结构耦合问题可能导致对定位不良的框过度估计，因此需要一种更准确的定位质量评估方法。

Method: 提出了一种像素级质量评估（PQA）框架，通过测量像素级空间一致性来替代传统的框级IoU预测，并引入新的集成度量方法。

Result: 在HRSC2016和DOTA数据集上的实验表明，PQA显著提升了性能（如Rotated RetinaNet上AP$_{50:95}$提升5.96%，STD提升2.32%）。

Conclusion: PQA框架通过像素级空间一致性评估，有效解决了传统框级IoU预测的结构耦合问题，显著提升了定向目标检测器的性能。

Abstract: Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).

</details>


### [78] [UI2Code$^\text{N}$: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation](https://arxiv.org/abs/2511.08195)
*Zhen Yang,Wenyi Hong,Mingde Xu,Xinyue Fan,Weihan Wang,Jiele Cheng,Xiaotao Gu,Jie Tang*

Main category: cs.CV

TL;DR: UI2Code$^\text{N}$是一种交互式UI到代码的视觉语言模型，通过多阶段训练提升多模态编码能力，在开放源代码模型中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前UI编程自动化的两大关键限制：多模态编码能力不足和单轮范式未能充分利用迭代视觉反馈。

Method: 提出UI2Code$^\text{N}$模型，采用分阶段预训练、微调和强化学习，统一了UI到代码生成、UI编辑和UI抛光三大能力，并探索了交互式生成中的测试时扩展。

Result: 在UI到代码和UI抛光的基准测试中，UI2Code$^\text{N}$在开放源代码模型中建立了新的最佳性能，性能接近领先的闭源模型。

Conclusion: UI2Code$^\text{N}$通过交互式UI到代码的范式，结合多阶段训练和强化学习，显著提升了多模态编码能力，并在开放源代码模型中达到了新的最佳性能，与领先的闭源模型如Claude-4-Sonnet和GPT-5相媲美。

Abstract: User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.

</details>


### [79] [UCDSC: Open Set UnCertainty aware Deep Simplex Classifier for Medical Image Datasets](https://arxiv.org/abs/2511.08196)
*Arnav Aditya,Nitin Kumar,Saurabh Shigwan*

Main category: cs.CV

TL;DR: 论文提出一种通过辅助数据集惩罚开放空间区域的损失函数，有效提升医学开放集识别性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域由于数据有限和高标注成本，算法在开放集识别中面临挑战，需要有效识别未知类样本。

Method: 引入一种损失函数，利用辅助数据集惩罚开放空间区域，以拒绝未知类样本。

Result: 在四个MedMNIST数据集和一个公开皮肤数据集上，该方法显著优于现有技术。

Conclusion: 论文提出了一种通过辅助数据集惩罚开放空间区域的损失函数，有效拒绝未知类样本，在多个医学数据集上显著优于现有技术。

Abstract: Driven by advancements in deep learning, computer-aided diagnoses have made remarkable progress. However, outside controlled laboratory settings, algorithms may encounter several challenges. In the medical domain, these difficulties often stem from limited data availability due to ethical and legal restrictions, as well as the high cost and time required for expert annotations-especially in the face of emerging or rare diseases. In this context, open-set recognition plays a vital role by identifying whether a sample belongs to one of the known classes seen during training or should be rejected as an unknown. Recent studies have shown that features learned in the later stages of deep neural networks are observed to cluster around their class means, which themselves are arranged as individual vertices of a regular simplex [32]. The proposed method introduces a loss function designed to reject samples of unknown classes effectively by penalizing open space regions using auxiliary datasets. This approach achieves significant performance gain across four MedMNIST datasets-BloodMNIST, OCTMNIST, DermaMNIST, TissueMNIST and a publicly available skin dataset [29] outperforming state-of-the-art techniques.

</details>


### [80] [Twist and Compute: The Cost of Pose in 3D Generative Diffusion](https://arxiv.org/abs/2511.08203)
*Kyle Fogarty,Jack Foster,Boqiao Zhang,Jing Yang,Cengiz Öztireli*

Main category: cs.CV

TL;DR: 研究发现图像到3D生成模型存在规范视角偏见，轻量级CNN可校正输入方向以提升性能，引发对模型设计方向的思考。


<details>
  <summary>Details</summary>
Motivation: 探究大规模图像到3D生成模型的归纳偏见，特别是规范视角偏见，以理解其在不同视角下的泛化能力。

Method: 通过控制实验使用简单的2D旋转，分析了Hunyuan3D 2.0模型在旋转输入下的表现。使用轻量级CNN检测和校正输入方向以缓解性能下降。

Result: 实验显示，Hunyuan3D 2.0模型在旋转输入下性能下降，但通过轻量级CNN校正方向可恢复性能。

Conclusion: 研究表明，尽管大规模图像到3D生成模型表现优异，但其存在强烈的规范视角偏见。通过轻量级CNN校正输入方向可以在不修改生成主干的情况下恢复模型性能，这引发了对模型设计是否应追求模块化和对称感知的思考。

Abstract: Despite their impressive results, large-scale image-to-3D generative models remain opaque in their inductive biases. We identify a significant limitation in image-conditioned 3D generative models: a strong canonical view bias. Through controlled experiments using simple 2D rotations, we show that the state-of-the-art Hunyuan3D 2.0 model can struggle to generalize across viewpoints, with performance degrading under rotated inputs. We show that this failure can be mitigated by a lightweight CNN that detects and corrects input orientation, restoring model performance without modifying the generative backbone. Our findings raise an important open question: Is scale enough, or should we pursue modular, symmetry-aware designs?

</details>


### [81] [Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone](https://arxiv.org/abs/2511.08215)
*Rizal Khoirul Anam*

Main category: cs.CV

TL;DR: 研究评估了结合EfficientNet-B4与Gemini LLM的多模态食品识别系统，发现视觉准确性是性能瓶颈，并提出了语义错误传播分析。


<details>
  <summary>Details</summary>
Motivation: 解决数字食品应用中自动营养分析与烹饪指导的需求，并针对公共数据集中的文化偏见问题开发了定制中文食品数据集（CCFD）。

Method: 提出了一种多模态解耦管道，结合专用视觉骨干（EfficientNet-B4）与生成式大语言模型（Gemini LLM），并引入'语义错误传播'（SEP）分析分类误差对生成输出的影响。

Result: EfficientNet-B4（89.0% Top-1准确率）在准确性与效率间表现最佳，Gemini（9.2/10事实准确性）生成质量优越，但系统整体性能受视觉前端准确性限制。

Conclusion: 系统的整体实用性受限于视觉前端的感知准确性，高效视觉模型（EfficientNet-B4）与强大生成模型（Gemini LLM）的结合在准确性与效率间取得了最佳平衡。

Abstract: The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.

</details>


### [82] [2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time](https://arxiv.org/abs/2511.08224)
*Ignasi Mas,Ivan Huerta,Ramon Morros,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 2Dto3D-SR框架通过2D表示实现单视角3D超分辨率，无需RGB引导，提供高精度（Swin Transformer）和高效（Vision Mamba）两种实现，适用于实际场景。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统3D超分辨率方法对高分辨率RGB引导的依赖，以及3D点云或RGB引导方法的复杂性，提供一种更简单、更实用的解决方案。

Method: 该方法使用投影归一化坐标编码（PNCC）将3D几何表示为规则图像，从而可以直接利用现有的2D图像超分辨率架构。框架提供了两种实现：基于Swin Transformers的高精度模型和基于Vision Mamba的高效率模型。

Result: 实验表明，Swin Transformer模型在标准基准测试中达到了最先进的精度，而Vision Mamba模型在实时速度下提供了具有竞争力的结果。

Conclusion: 2Dto3D-SR 框架通过结构化2D表示实现了无需高分辨率RGB引导的单视角3D超分辨率，提供了一种轻量且快速的解决方案，适用于实际场景，特别是在无法获取高分辨率RGB数据的情况下。

Abstract: We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.

</details>


### [83] [Accurate and Efficient Surface Reconstruction from Point Clouds via Geometry-Aware Local Adaptation](https://arxiv.org/abs/2511.08233)
*Eito Ogawa,Taiga Hayami,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 提出了一种基于曲率自适应调整局部区域的方法，提高了点云表面重建的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常均匀放置局部区域并保持其大小固定，限制了其对几何复杂度变化的适应性。

Method: 提出了一种方法，通过根据输入点云的曲率自适应调节局部区域的间距和大小。

Result: 该方法提高了重建的准确性和效率。

Conclusion: 本研究通过基于输入点云曲率自适应调整局部区域的间距和大小，提高了表面重建的准确性和效率。

Abstract: Point cloud surface reconstruction has improved in accuracy with advances in deep learning, enabling applications such as infrastructure inspection. Recent approaches that reconstruct from small local regions rather than entire point clouds have attracted attention for their strong generalization capability. However, prior work typically places local regions uniformly and keeps their size fixed, limiting adaptability to variations in geometric complexity. In this study, we propose a method that improves reconstruction accuracy and efficiency by adaptively modulating the spacing and size of local regions based on the curvature of the input point cloud.

</details>


### [84] [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](https://arxiv.org/abs/2511.08238)
*Xiangyang Wu,Liu Liu,Baosheng Yu,Jiayan Qiu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 该论文提出一种结合语义和关系信息的多模态对齐方法，显著提升视觉-语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言微调方法在对齐视觉和语言时通常忽略了文本上下文中强调的语义关系信息，导致性能不佳。

Method: 首先从不同视觉编码器中提取多级语义特征以捕捉更多视觉关系线索，然后学习将视觉特征投影到相关语义组中，最后通过可继承的交叉注意力融合视觉与文本特征，并去除低相关度的视觉-语言特征对。

Result: 在八个基础模型和两个下游任务（视觉问答和图像描述）上的评估表明，该方法优于所有现有方法。

Conclusion: 该论文提出的方法通过结合语义和关系信息，显著提升了多模态对齐和融合的效果，在视觉问答和图像描述任务中表现优于现有方法。

Abstract: Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.

</details>


### [85] [Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning](https://arxiv.org/abs/2511.08240)
*Chenyu Hu,Xiaotong Li,Hao Zhu,Biao Hou*

Main category: cs.CV

TL;DR: DiPVNet通过局部和全局方向感知操作，解决了点云旋转扰动问题，提升了分类和分割性能。


<details>
  <summary>Details</summary>
Motivation: 点云处理中旋转扰动破坏了方向特性，现有方法未能充分利用多尺度方向信息提升特征表示。

Method: 提出了方向感知向量网络（DiPVNet），包括局部可学习点积操作（L2DP）和全局方向感知球面傅里叶变换（DASFT），分别捕捉非均匀局部结构和整体方向结构。

Result: 在噪声和大角度旋转的挑战性场景中，DiPVNet在点云分类和分割任务上表现优异。

Conclusion: DiPVNet通过创新的方向感知向量网络，结合局部和全局方向感知操作，有效解决了点云处理中的旋转扰动问题，在分类和分割任务上达到了最先进的性能。

Abstract: Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at https://github.com/wxszreal0/DiPVNet.

</details>


### [86] [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](https://arxiv.org/abs/2511.08248)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: NERVE是一种无需训练的开放词汇语义分割方法，通过结合全局与局部信息和随机游走策略，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由的开放词汇语义分割方法存在计算成本高、注意力图融合效果不佳以及依赖固定大小高斯核等问题，NERVE旨在解决这些限制。

Method: NERVE整合了稳定扩散模型自注意力层的邻域结构，采用熵基不确定性选择最相关的注意力图，并引入随机游走优化亲和力。

Result: 在7个语义分割基准上实现了最先进的零样本分割性能。

Conclusion: NERVE方法在不需要传统后处理技术的情况下，通过结合全局和局部信息以及随机游走策略，实现了在7个流行语义分割基准上的最先进零样本分割性能。

Abstract: Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.

</details>


### [87] [LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning](https://arxiv.org/abs/2511.08251)
*Fengyi Fu,Mengqi Huang,Lei Zhang,Zhendong Mao*

Main category: cs.CV

TL;DR: LayerEdit是一种无需训练的多层解耦编辑框架，通过精确的对象分层分解和融合，实现无冲突的多对象编辑，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要遵循定位-编辑范式，忽视了对象间的关键交互，导致编辑泄漏或约束。本文指出注意力纠缠在多对象冲突区域中阻碍了解耦编辑。

Method: LayerEdit提出了一种新颖的“分解-编辑-融合”框架，包括冲突感知的层分解模块、对象分层编辑模块和透明度引导的层融合模块。

Result: 大量实验验证了LayerEdit在复杂多对象场景中的优越性，展示了对象内可控性和对象间一致性的 unprecedented 表现。

Conclusion: LayerEdit通过多层解耦编辑框架，首次实现了无冲突的对象分层编辑，展示了在复杂多对象场景中前所未有的对象内可控性和对象间一致性。

Abstract: Text-driven multi-object image editing which aims to precisely modify multiple objects within an image based on text descriptions, has recently attracted considerable interest. Existing works primarily follow the localize-editing paradigm, focusing on independent object localization and editing while neglecting critical inter-object interactions. However, this work points out that the neglected attention entanglements in inter-object conflict regions, inherently hinder disentangled multi-object editing, leading to either inter-object editing leakage or intra-object editing constraints. We thereby propose a novel multi-layer disentangled editing framework LayerEdit, a training-free method which, for the first time, through precise object-layered decomposition and coherent fusion, enables conflict-free object-layered editing. Specifically, LayerEdit introduces a novel "decompose-editingfusion" framework, consisting of: (1) Conflict-aware Layer Decomposition module, which utilizes an attention-aware IoU scheme and time-dependent region removing, to enhance conflict awareness and suppression for layer decomposition. (2) Object-layered Editing module, to establish coordinated intra-layer text guidance and cross-layer geometric mapping, achieving disentangled semantic and structural modifications. (3) Transparency-guided Layer Fusion module, to facilitate structure-coherent inter-object layer fusion through precise transparency guidance learning. Extensive experiments verify the superiority of LayerEdit over existing methods, showing unprecedented intra-object controllability and inter-object coherence in complex multi-object scenarios. Codes are available at: https://github.com/fufy1024/LayerEdit.

</details>


### [88] [Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation](https://arxiv.org/abs/2511.08258)
*Jae Joong Lee,Bedrich Benes*

Main category: cs.CV

TL;DR: Top2Ground是一种扩散模型方法，直接从航空图像生成地面视角图像，结合几何约束和语义一致性，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决从航空视角生成地面视角图像的挑战，如极端视角差异、遮挡和有限视野。

Method: 基于扩散模型的方法，直接利用航空图像输入，结合VAE编码的空间特征和CLIP语义嵌入进行去噪过程，无需依赖深度图或3D体素等中间表示。

Result: 在CVUSA、CVACT和Auto Arborist三个数据集上，SSIM平均提高了7.3%，表明方法能稳健处理宽窄视野。

Conclusion: Top2Ground通过结合VAE编码的空间特征和CLIP语义嵌入，直接生成逼真的地面视角图像，展示了强大的泛化能力。

Abstract: Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.

</details>


### [89] [ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation](https://arxiv.org/abs/2511.08263)
*Yue Min,Shaobo Wang,Jiaze Li,Tianle Niu,Junxin Fan,Yongliang Miao,Lijin Yang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ImageBindDC是多模态数据压缩框架，通过特征函数损失和三层分布对齐，显著提升压缩效果和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据压缩技术在多模态场景中因无法保留复杂模态间依赖关系而失败的问题。

Method: 提出了ImageBindDC框架，使用特征函数(CF)损失在傅里叶域进行统计对齐，并通过三个层次的分布一致性（单模态对齐、跨模态对齐和联合模态对齐）来保留模态间依赖关系。

Result: 在NYU-v2数据集上，仅用每类5个压缩数据点训练的模型即可达到与完整数据集训练相当的损失less性能，绝对性能提升8.2%，且压缩时间减少4倍以上。

Conclusion: ImageBindDC通过在ImageBind的统一特征空间中操作，采用特征函数(CF)损失在傅里叶域实现更精确的统计对齐，显著提升了多模态数据压缩的效果。

Abstract: Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\% absolute improvement over the previous best method and more than 4$\times$ less condensation time.

</details>


### [90] [Re-coding for Uncertainties: Edge-awareness Semantic Concordance for Resilient Event-RGB Segmentation](https://arxiv.org/abs/2511.08269)
*Nan Bao,Yifan Zhao,Lin Zhu,Jia Li*

Main category: cs.CV

TL;DR: ESC框架通过边缘感知和潜在重编码技术，解决了极端条件下事件-RGB异构特征融合问题，显著提升了语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端条件下（如光线不足、剧烈相机运动）因RGB信息丢失导致分割性能下降，而事件模态与RGB的异构性导致特征不匹配和优化困难。

Method: 提出了一种边缘感知语义一致性框架（ESC），包括边缘感知潜在重编码（ELR）和重编码巩固与不确定性优化（RCUO），通过预建立的边缘字典和重编码分布指导特征对齐。

Result: 在提出的DERS-XS数据集上，ESC框架比现有最优方法提升了2.55%的mIoU，并在空间遮挡下表现出优异的鲁棒性。

Conclusion: ESC框架通过边缘感知的潜在重编码和不确定性优化，成功解决了极端条件下事件-RGB异构特征融合的问题，显著提升了语义分割的性能。

Abstract: Semantic segmentation has achieved great success in ideal conditions. However, when facing extreme conditions (e.g., insufficient light, fierce camera motion), most existing methods suffer from significant information loss of RGB, severely damaging segmentation results. Several researches exploit the high-speed and high-dynamic event modality as a complement, but event and RGB are naturally heterogeneous, which leads to feature-level mismatch and inferior optimization of existing multi-modality methods. Different from these researches, we delve into the edge secret of both modalities for resilient fusion and propose a novel Edge-awareness Semantic Concordance framework to unify the multi-modality heterogeneous features with latent edge cues. In this framework, we first propose Edge-awareness Latent Re-coding, which obtains uncertainty indicators while realigning event-RGB features into unified semantic space guided by re-coded distribution, and transfers event-RGB distributions into re-coded features by utilizing a pre-established edge dictionary as clues. We then propose Re-coded Consolidation and Uncertainty Optimization, which utilize re-coded edge features and uncertainty indicators to solve the heterogeneous event-RGB fusion issues under extreme conditions. We establish two synthetic and one real-world event-RGB semantic segmentation datasets for extreme scenario comparisons. Experimental results show that our method outperforms the state-of-the-art by a 2.55% mIoU on our proposed DERS-XS, and possesses superior resilience under spatial occlusion. Our code and datasets are publicly available at https://github.com/iCVTEAM/ESC.

</details>


### [91] [MAUGIF: Mechanism-Aware Unsupervised General Image Fusion via Dual Cross-Image Autoencoders](https://arxiv.org/abs/2511.08272)
*Kunjing Yang,Zhiwei Wang,Minru Bai*

Main category: cs.CV

TL;DR: 提出机制感知的无监督通用图像融合方法MAUGIF，通过双自动编码器分类融合机制，提升性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法多为任务专用或通用框架但忽视不同任务的融合机制差异，需解决这一问题。

Method: 基于双交叉图像自动编码器，通过分类加法和乘法融合机制，双编码器将源图像映射到共享潜在空间，双解码器作为特征注入器选择性重建。

Result: 在多种融合任务上验证了方法的有效性和泛化能力。

Conclusion: MAUGIF方法通过双交叉图像自动编码器实现了机制感知的无监督通用图像融合，显著提升了性能和可解释性。

Abstract: Image fusion aims to integrate structural and complementary information from multi-source images. However, existing fusion methods are often either highly task-specific, or general frameworks that apply uniform strategies across diverse tasks, ignoring their distinct fusion mechanisms. To address this issue, we propose a mechanism-aware unsupervised general image fusion (MAUGIF) method based on dual cross-image autoencoders. Initially, we introduce a classification of additive and multiplicative fusion according to the inherent mechanisms of different fusion tasks. Then, dual encoders map source images into a shared latent space, capturing common content while isolating modality-specific details. During the decoding phase, dual decoders act as feature injectors, selectively reintegrating the unique characteristics of each modality into the shared content for reconstruction. The modality-specific features are injected into the source image in the fusion process, generating the fused image that integrates information from both modalities. The architecture of decoders varies according to their fusion mechanisms, enhancing both performance and interpretability. Extensive experiments are conducted on diverse fusion tasks to validate the effectiveness and generalization ability of our method. The code is available at https://anonymous.4open.science/r/MAUGIF.

</details>


### [92] [SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer](https://arxiv.org/abs/2511.08291)
*Kaiyi Xu,Junchao Gong,Zhiwang Zhou,Zhangrui Li,Yuandong Pu,Yihao Liu,Ben Fei,Fenghua Ling,Wenlong Zhang,Lei Bei*

Main category: cs.CV

TL;DR: SynWeather数据集和SynWeatherDiff模型解决了天气数据合成中的多区域和多变量统一问题，效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前方法局限于单变量、单区域任务，缺乏跨变量互补性，导致结果过度平滑。

Method: 基于Diffusion Transformer框架构建的SynWeatherDiff模型。

Result: 实验证明SynWeatherDiff在SynWeather数据集上优于特定任务和通用模型。

Conclusion: SynWeather和SynWeatherDiff在统一多区域和多变量天气观测数据合成方面表现出色，解决了现有方法的局限性。

Abstract: With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.

</details>


### [93] [SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering](https://arxiv.org/abs/2511.08294)
*Laura Bragagnolo,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: SkelSplat 通过可微分高斯渲染实现多视角 3D 人体姿态估计，无需 3D 真值监督，表现优于现有方法并显著降低跨数据集误差。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角方法依赖于大型标注数据集，导致在测试场景不同时泛化能力差。SkelSplat 旨在克服这一限制。

Method: 提出了一种基于可微分高斯渲染的框架，将人体姿态建模为 3D 高斯骨架，并为每个关节分配一个高斯分布，通过可微分渲染优化来实现多视角的无缝融合。

Result: 在 Human3.6M 和 CMU 数据集上表现优异，跨数据集误差降低 47.8%，且在遮挡场景下表现稳健。

Conclusion: SkelSplat 是一种基于可微分高斯渲染的多视角 3D 人体姿态估计新框架，其在 Human3.6M 和 CMU 数据集上表现优于不依赖 3D 真值的方法，并在跨数据集误差上降低了 47.8%。

Abstract: Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.

</details>


### [94] [NeuSpring: Neural Spring Fields for Reconstruction and Simulation of Deformable Objects from Videos](https://arxiv.org/abs/2511.08310)
*Qingshan Xu,Jiao Liu,Shangshu Yu,Yuxuan Wang,Yuan Zhou,Junbao Zhou,Jiequan Cui,Yew-Soon Ong,Hanwang Zhang*

Main category: cs.CV

TL;DR: NeuSpring 通过神经弹簧场和分段拓扑解决方案，显著提升了可变形物体的重建和模拟性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了可变形物体的内在物理属性，导致当前状态建模的物理学习受限，且对未来预测的泛化能力较差。

Method: 1) 采用分段拓扑解决方案，通过零阶优化高效建模多区域弹簧连接拓扑；2) 使用基于坐标的神经网络表示弹簧物理属性。

Result: 在真实数据集上，NeuSpring 的 Chamfer 距离分别提高了 20% 和 25%。

Conclusion: NeuSpring 在可变形物体的重建和模拟方面表现出色，显著提升了当前状态建模和未来预测的性能。

Abstract: In this paper, we aim to create physical digital twins of deformable objects under interaction. Existing methods focus more on the physical learning of current state modeling, but generalize worse to future prediction. This is because existing methods ignore the intrinsic physical properties of deformable objects, resulting in the limited physical learning in the current state modeling. To address this, we present NeuSpring, a neural spring field for the reconstruction and simulation of deformable objects from videos. Built upon spring-mass models for realistic physical simulation, our method consists of two major innovations: 1) a piecewise topology solution that efficiently models multi-region spring connection topologies using zero-order optimization, which considers the material heterogeneity of real-world objects. 2) a neural spring field that represents spring physical properties across different frames using a canonical coordinate-based neural network, which effectively leverages the spatial associativity of springs for physical learning. Experiments on real-world datasets demonstrate that our NeuSping achieves superior reconstruction and simulation performance for current state modeling and future prediction, with Chamfer distance improved by 20% and 25%, respectively.

</details>


### [95] [Mitigating Negative Flips via Margin Preserving Training](https://arxiv.org/abs/2511.08322)
*Simone Ricci,Niccolò Biondi,Federico Pernici,Alberto Del Bimbo*

Main category: cs.CV

TL;DR: 该论文提出了一种结合边际校准和双源焦点蒸馏损失的方法，以减少模型更新时的负翻转现象，同时保持高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在图像分类中，随着训练类别的增加，模型的更新可能导致负翻转现象，即新模型错误分类之前正确分类的样本。为减少这种现象，需在保持原有类别边际的同时学习改进模型。

Method: 提出了一种新颖的方法，通过引入显式的边际校准项和双源焦点蒸馏损失，平衡新旧类别间的决策边界。

Result: 在图像分类基准测试中，该方法显著降低了负翻转率，且整体分类准确率较高。

Conclusion: 本文提出的方法通过结合边际校准和双源焦点蒸馏损失，有效减少了负翻转率，同时保持了较高的整体分类准确率。

Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.

</details>


### [96] [The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment](https://arxiv.org/abs/2511.08328)
*Solveig Thrun,Stine Hansen,Zijun Sun,Nele Blum,Suaiba A. Salahuddin,Xin Wang,Kristoffer Wickstrøm,Elisabeth Wetzer,Robert Jenssen,Maik Stille,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 图像基础的变形场在乳腺X光纵向风险建模中效果最佳，显著提升预测性能，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查的个性化间隔对高风险人群至关重要，但现有方法在纵向图像的空间对齐上存在挑战，影响模型性能。

Method: 本研究评估了多种对齐策略，包括图像基础的注册、特征级对齐（带/不带正则化）和隐式对齐方法，使用两个大规模乳腺X光数据集进行测试。

Result: 图像基础的注册在所有指标（预测准确性、精确度、召回率和变形场质量）上均优于特征级和隐式方法，且特征空间中应用图像基础的变形场表现最佳。

Conclusion: 图像基础的变形场在纵向风险建模中的空间对齐方面表现最佳，显著提升了预测准确性和鲁棒性，有望优化个性化筛查和高风险个体的早期干预。

Abstract: Regular mammography screening is crucial for early breast cancer detection. By leveraging deep learning-based risk models, screening intervals can be personalized, especially for high-risk individuals. While recent methods increasingly incorporate longitudinal information from prior mammograms, accurate spatial alignment across time points remains a key challenge. Misalignment can obscure meaningful tissue changes and degrade model performance. In this study, we provide insights into various alignment strategies, image-based registration, feature-level (representation space) alignment with and without regularization, and implicit alignment methods, for their effectiveness in longitudinal deep learning-based risk modeling. Using two large-scale mammography datasets, we assess each method across key metrics, including predictive accuracy, precision, recall, and deformation field quality.
  Our results show that image-based registration consistently outperforms the more recently favored feature-based and implicit approaches across all metrics, enabling more accurate, temporally consistent predictions and generating smooth, anatomically plausible deformation fields. Although regularizing the deformation field improves deformation quality, it reduces the risk prediction performance of feature-level alignment. Applying image-based deformation fields within the feature space yields the best risk prediction performance.
  These findings underscore the importance of image-based deformation fields for spatial alignment in longitudinal risk modeling, offering improved prediction accuracy and robustness. This approach has strong potential to enhance personalized screening and enable earlier interventions for high-risk individuals. The code is available at https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git, allowing full reproducibility of the results.

</details>


### [97] [Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter](https://arxiv.org/abs/2511.08334)
*Zhiyang Chen,Chen Zhang,Hao Fang,Runmin Cong*

Main category: cs.CV

TL;DR: DiveSeg利用DINO预训练模型，结合水下风格适配和对象先验提示，实现了水下实例分割的最优性能。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割（UIS）在海洋资源探索和生态保护中至关重要，但现有方法在适应水下环境方面存在不足。

Method: 提出DiveSeg框架，包含AquaStyle Aligner（嵌入水下颜色风格特征）和ObjectPrior Prompter（提供基于二值分割的提示）。

Result: 在UIIS和USIS10K数据集上的实验表明，DiveSeg实现了最先进的性能。

Conclusion: DiveSeg框架通过结合AquaStyle Aligner和ObjectPrior Prompter，展示了在UIS任务中的卓越性能，达到了当前最佳水平。

Abstract: Underwater instance segmentation (UIS), integrating pixel-level understanding and instance-level discrimination, is a pivotal technology in marine resource exploration and ecological protection. In recent years, large-scale pretrained visual foundation models, exemplified by DINO, have advanced rapidly and demonstrated remarkable performance on complex downstream tasks. In this paper, we demonstrate that DINO can serve as an effective feature learner for UIS, and we introduce DiveSeg, a novel framework built upon two insightful components: (1) The AquaStyle Aligner, designed to embed underwater color style features into the DINO fine-tuning process, facilitating better adaptation to the underwater domain. (2) The ObjectPrior Prompter, which incorporates binary segmentation-based prompts to deliver object-level priors, provides essential guidance for instance segmentation task that requires both object- and instance-level reasoning. We conduct thorough experiments on the popular UIIS and USIS10K datasets, and the results show that DiveSeg achieves the state-of-the-art performance. Code: https://github.com/ettof/Diveseg.

</details>


### [98] [Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning](https://arxiv.org/abs/2511.08344)
*Chen Liu,Can Han,Weishi Xu,Yaqi Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的数据增强方法SASG-DA，通过语义引导和稀疏感知采样生成高质量sEMG数据，显著提升了手势识别的泛化性能。


<details>
  <summary>Details</summary>
Motivation: sEMG手势识别在康复和假肢控制中至关重要，但训练数据稀缺导致深度学习模型过拟合和泛化能力差。现有数据增强方法在多样性和真实性上存在不足。

Method: 结合语义表示指导(SRG)和高斯建模语义采样(GMSS)，提出稀疏感知语义采样策略，生成既真实又多样的样本。

Result: 在Ninapro DB2、DB4和DB7数据集上，SASG-DA显著优于现有方法，有效缓解过拟合并提升识别性能。

Conclusion: SASG-DA通过增强数据多样性和真实性，显著改善了sEMG手势识别的泛化能力，为HMI应用提供了可靠解决方案。

Abstract: Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Modeling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.

</details>


### [99] [VideoChain: A Transformer-Based Framework for Multi-hop Video Question Generation](https://arxiv.org/abs/2511.08348)
*Arpan Phukan,Anupam Pandey,Deepjyoti Bodo,Asif Ekbal*

Main category: cs.CV

TL;DR: VideoChain是一个多跳视频问题生成框架，通过模块化架构和视频嵌入技术，成功生成需要跨多段视频推理的问题，并在多项指标上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问题生成（QG）局限于文本，而视频问题生成（VideoQG）仅限于单段视频的零跳问题，因此需要一种能够跨多段视频推理的问题生成框架。

Method: VideoChain采用模块化架构，基于改进的BART骨干网络，结合视频嵌入以捕捉文本和视觉依赖关系。

Result: 在TVQA+数据集上构建的MVQ-60数据集上，VideoChain在ROUGE-L（0.6454）、ROUGE-1（0.6854）、BLEU-1（0.6711）、BERTScore-F1（0.7967）和语义相似度（0.8110）等指标上表现优异。

Conclusion: VideoChain在生成需要跨多段视频推理的问题方面表现出色，验证了其在多跳视频问题生成（MVQG）任务中的有效性。

Abstract: Multi-hop Question Generation (QG) effectively evaluates reasoning but remains confined to text; Video Question Generation (VideoQG) is limited to zero-hop questions over single segments. To address this, we introduce VideoChain, a novel Multi-hop Video Question Generation (MVQG) framework designed to generate questions that require reasoning across multiple, temporally separated video segments. VideoChain features a modular architecture built on a modified BART backbone enhanced with video embeddings, capturing textual and visual dependencies. Using the TVQA+ dataset, we automatically construct the large-scale MVQ-60 dataset by merging zero-hop QA pairs, ensuring scalability and diversity. Evaluations show VideoChain's strong performance across standard generation metrics: ROUGE-L (0.6454), ROUGE-1 (0.6854), BLEU-1 (0.6711), BERTScore-F1 (0.7967), and semantic similarity (0.8110). These results highlight the model's ability to generate coherent, contextually grounded, and reasoning-intensive questions.

</details>


### [100] [Extreme Model Compression with Structured Sparsity at Low Precision](https://arxiv.org/abs/2511.08360)
*Dan Liu,Nikita Dvornik,Xue Liu*

Main category: cs.CV

TL;DR: SLOPE框架结合结构化稀疏和低比特量化，通过角度对齐策略减少模型大小并保持精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限设备上运行困难，现有稀疏化和量化方法单独使用有效，但结合时会对模型精度产生负面影响。

Method: 提出了训练时的正则化策略，通过促进角度对齐而非直接匹配，最小化全精度权重与其稀疏、量化版本之间的差异。

Result: 在ResNet-18上实现了约20倍的模型大小缩减，同时保持了约99%的原始精度，并在多个任务和模型上优于现有方法。

Conclusion: SLOPE框架通过结构化稀疏和低比特量化的结合，显著减少了模型大小，同时保持了高精度，优于现有方法。

Abstract: Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.

</details>


### [101] [Retrospective motion correction in MRI using disentangled embeddings](https://arxiv.org/abs/2511.08365)
*Qi Wang,Veronika Ecker,Marcel Früh,Sergios Gatidis,Thomas Küstner*

Main category: cs.CV

TL;DR: 提出一种分层VQ变分自编码器，通过解耦运动特征实现MRI运动校正的泛化，无需特定伪影训练，适用于多种运动类型和身体区域。


<details>
  <summary>Details</summary>
Motivation: 生理运动会影响磁共振成像（MRI）的诊断质量。现有回顾性运动校正方法难以泛化到不同运动类型和身体区域，尤其是基于机器学习（ML）的校正通常针对特定应用和数据集。假设尽管运动伪影多样，但存在可解耦和利用的潜在模式。

Method: 提出了一种分层矢量量化（VQ）变分自编码器，学习运动到干净图像特征的解耦嵌入。部署代码本以捕获多分辨率下的有限运动模式集合，实现从粗到细的校正。训练自回归模型学习无运动图像的先验分布，并在推理时引导校正过程。

Result: 在模拟全身运动伪影上验证了方法，观察到对不同运动强度的鲁棒校正。

Conclusion: 该模型有效解耦了模拟运动扫描中的物理运动，提升了基于机器学习的MRI运动校正的泛化能力。解耦运动特征的工作为跨解剖区域和运动类型的潜在应用提供了启示。

Abstract: Physiological motion can affect the diagnostic quality of magnetic resonance imaging (MRI). While various retrospective motion correction methods exist, many struggle to generalize across different motion types and body regions. In particular, machine learning (ML)-based corrections are often tailored to specific applications and datasets. We hypothesize that motion artifacts, though diverse, share underlying patterns that can be disentangled and exploited. To address this, we propose a hierarchical vector-quantized (VQ) variational auto-encoder that learns a disentangled embedding of motion-to-clean image features. A codebook is deployed to capture finite collection of motion patterns at multiple resolutions, enabling coarse-to-fine correction. An auto-regressive model is trained to learn the prior distribution of motion-free images and is used at inference to guide the correction process. Unlike conventional approaches, our method does not require artifact-specific training and can generalize to unseen motion patterns. We demonstrate the approach on simulated whole-body motion artifacts and observe robust correction across varying motion severity. Our results suggest that the model effectively disentangled physical motion of the simulated motion-effective scans, therefore, improving the generalizability of the ML-based MRI motion correction. Our work of disentangling the motion features shed a light on its potential application across anatomical regions and motion types.

</details>


### [102] [A Circular Argument : Does RoPE need to be Equivariant for Vision?](https://arxiv.org/abs/2511.08368)
*Chase van de Geijn,Timo Lüddecke,Polina Turishcheva,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 研究发现RoPE的严格等变性可能被高估，Spherical RoPE（非等变版本）表现优异，暗示相对位置嵌入在计算机视觉中并非必需。


<details>
  <summary>Details</summary>
Motivation: Rotary Positional Encodings (RoPE)在自然语言处理中表现出色，但其成功是否严格依赖于位置等变性（即作为相对位置编码的特性）尚不明确。本文旨在探讨这一问题，并探索在更高维数据（如图像和视频）中RoPE的泛化能力。

Method: 本文通过数学证明了RoPE是一维数据中等变位置嵌入的最通用解之一，并提出了Mixed RoPE作为M维数据的类似通用解（在需要可交换生成元的情况下）。进一步提出了Spherical RoPE，一种假设非可交换生成元的方法。

Result: 实验发现，Spherical RoPE（非等变版本）的学习行为优于或等同于其等变类似物（如Mixed RoPE），挑战了相对位置嵌入在RoPE性能中起关键作用的普遍观点。

Conclusion: 研究表明，严格的位置等变性在RoPE性能中的作用可能被高估，特别是在计算机视觉领域。Spherical RoPE的表现优于或等同于其等变类似物，暗示相对位置嵌入的重要性不如普遍认为的那样大。这一发现有望推动未来在视觉位置编码方面的研究，通过摆脱必须使用相对编码的预设，实现更快和更好的泛化。

Abstract: Rotary Positional Encodings (RoPE) have emerged as a highly effective technique for one-dimensional sequences in Natural Language Processing spurring recent progress towards generalizing RoPE to higher-dimensional data such as images and videos. The success of RoPE has been thought to be due to its positional equivariance, i.e. its status as a relative positional encoding. In this paper, we mathematically show RoPE to be one of the most general solutions for equivariant positional embedding in one-dimensional data. Moreover, we show Mixed RoPE to be the analogously general solution for M-dimensional data, if we require commutative generators -- a property necessary for RoPE's equivariance. However, we question whether strict equivariance plays a large role in RoPE's performance. We propose Spherical RoPE, a method analogous to Mixed RoPE, but assumes non-commutative generators. Empirically, we find Spherical RoPE to have the equivalent or better learning behavior compared to its equivariant analogues. This suggests that relative positional embeddings are not as important as is commonly believed, at least within computer vision. We expect this discovery to facilitate future work in positional encodings for vision that can be faster and generalize better by removing the preconception that they must be relative.

</details>


### [103] [Text-based Aerial-Ground Person Retrieval](https://arxiv.org/abs/2511.08369)
*Xinyu Zhou,Yu Wu,Jiayao Ma,Wenhao Wang,Min Cao,Mang Ye*

Main category: cs.CV

TL;DR: 提出了TAG-PR任务及TAG-CLIP框架，解决了跨视角检索问题，并发布了TAG-PEDES数据集。


<details>
  <summary>Details</summary>
Motivation: 传统的基于文本的人员检索（T-PR）仅关注地面视角图像，而TAG-PR引入了跨视角检索的实际意义和挑战。

Method: 提出了TAG-CLIP框架，包含分层路由的专家混合模块和视角解耦策略，以学习视角特定和视角无关的特征。

Result: TAG-CLIP在TAG-PEDES数据集和现有T-PR基准测试中验证了其有效性。

Conclusion: TAG-CLIP框架在TAG-PEDES数据集和现有T-PR基准测试中表现出色，成功解决了跨视角检索的挑战。

Abstract: This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at https://github.com/Flame-Chasers/TAG-PR.

</details>


### [104] [RAPTR: Radar-based 3D Pose Estimation using Transformer](https://arxiv.org/abs/2511.08387)
*Sorachi Kato,Ryoma Yataka,Pu Perry Wang,Pedro Miraldo,Takuya Fujihashi,Petros Boufounos*

Main category: cs.CV

TL;DR: RAPTR是一种基于弱监督（仅需3D BBox和2D关键点标签）的雷达3D人体姿态估计方法，通过两阶段解码器架构显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的雷达室内3D人体姿态估计依赖昂贵的3D关键点标注，在复杂室内环境中难以获取。RAPTR旨在通过更易获取的弱监督标签（3D BBox和2D关键点）解决这一问题。

Method: RAPTR采用两阶段姿态解码器架构，结合伪3D可变形注意力机制，利用3D模板损失和3D重力损失优化初始姿态。

Result: 在两个室内雷达数据集上，RAPTR将关节位置误差分别降低了34.3%（HIBER）和76.9%（MMVR）。

Conclusion: RAPTR在仅使用3D边界框和2D关键点标签的弱监督条件下，显著提高了室内雷达3D人体姿态估计的准确率，在两个数据集上的表现优于现有方法。

Abstract: Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

</details>


### [105] [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](https://arxiv.org/abs/2511.08402)
*Difei Gu,Yunhe Gao,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: Anatomy-VLM 是一种细粒度视觉语言模型，通过定位解剖特征和知识增强，实现了临床级别的疾病诊断和零样本解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）将图像视为整体，忽略了对疾病诊断至关重要的细粒度图像细节，而临床医生则通过医学知识和解剖结构分析图像。

Method: 设计了一个模型编码器来定位医学图像中的关键解剖特征，并通过结构化知识增强这些区域，实现多尺度医学信息的对齐。

Result: Anatomy-VLM 在分布内外数据集上表现优异，并在下游图像分割任务中验证了其细粒度对齐能力。

Conclusion: Anatomy-VLM 通过多尺度信息整合和细粒度视觉语言对齐，展示了在疾病诊断和图像分割任务中的卓越性能，并具备零样本解剖学解释能力。

Abstract: Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.

</details>


### [106] [OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2511.08423)
*Yuncheng Guo,Junyan Ye,Chenjue Zhang,Hengrui Kang,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: OmniAID通过解耦混合专家架构和新数据集Mirage，显著提升AIGI检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在单一纠缠伪造表示的问题，且受限于过时基准，需提升跨生成模型和语义内容的泛化能力。

Method: 提出基于解耦混合专家（MoE）架构的OmniAID框架，包含路由专用语义专家和固定通用伪影专家，采用两阶段训练策略。

Result: 实验证明OmniAID超越现有单一检测器，在传统基准和Mirage数据集上均表现优异。

Conclusion: OmniAID通过解耦内容特定瑕疵与通用伪影，并引入新的Mirage数据集，在AIGI检测领域建立了新的鲁棒标准。

Abstract: A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.

</details>


### [107] [Cross-pyramid consistency regularization for semi-supervised medical image segmentation](https://arxiv.org/abs/2511.08435)
*Matus Bojko,Maros Kollar,Marek Jakab,Wanda Benesova*

Main category: cs.CV

TL;DR: 提出DBPNet和CPCR方法，通过双解码器金字塔网络和跨金字塔一致性正则化，有效利用未标注数据提升半监督医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 利用有限的标注数据和大量未标注数据进行半监督学习（SSL），以提升医学图像分割的性能。

Method: 设计了一个混合双分支金字塔网络（DBPNet），包含一个编码器和两个略有差异的解码器，每个解码器生成多分辨率尺度的扰动辅助预测。提出了一种名为CPCR的学习策略，结合现有的一致性学习和不确定性最小化方法，并引入新的正则化项。

Result: 实验结果表明，DBPNet与CPCR在公共基准数据集上优于五种自监督学习方法，并与最新方法性能相当。

Conclusion: DBPNet结合CPCR方法在半监督医学图像分割任务中表现优异，优于五种当前最优的自监督学习方法，并与最新方法性能相当。

Abstract: Semi-supervised learning (SSL) enables training of powerful models with the assumption of limited, carefully labelled data and a large amount of unlabeled data to support the learning. In this paper, we propose a hybrid consistency learning approach to effectively exploit unlabeled data for semi-supervised medical image segmentation by leveraging Cross-Pyramid Consistency Regularization (CPCR) between two decoders. First, we design a hybrid Dual Branch Pyramid Network (DBPNet), consisting of an encoder and two decoders that differ slightly, each producing a pyramid of perturbed auxiliary predictions across multiple resolution scales. Second, we present a learning strategy for this network named CPCR that combines existing consistency learning and uncertainty minimization approaches on the main output predictions of decoders with our novel regularization term. More specifically, in this term, we extend the soft-labeling setting to pyramid predictions across decoders to support knowledge distillation in deep hierarchical features. Experimental results show that DBPNet with CPCR outperforms five state-of-the-art self-supervised learning methods and has comparable performance with recent ones on a public benchmark dataset.

</details>


### [108] [Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification](https://arxiv.org/abs/2511.08464)
*Anh Mai Vu,Tuan L. Vo,Ngoc Lam Quang Bui,Nam Nguyen Le Binh,Akash Awasthi,Huy Quoc Vo,Thanh-Huy Nguyen,Zhu Han,Chandra Mohan,Hien Van Nguyen*

Main category: cs.CV

TL;DR: CIG是一种新的WSI归因方法，通过对比梯度增强解释性，实验验证其在多种癌症数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在WSI分析中，解释性对于建立AI辅助诊断的信任至关重要。现有的归因方法（如积分梯度IG）虽然能捕捉模型决策模式，但可能忽略对区分肿瘤亚型至关重要的类区分性信号。因此，需要一种新的归因方法，既能突出类区分性区域，又能保证理论的严谨性。

Method: 本文提出了CIG（对比性积分梯度）方法，通过计算对数空间中的对比梯度来增强WSI的解释性。CIG通过比较特征相对于参考类的重要性，突出类区分性区域，从而更清晰地区分肿瘤与非肿瘤区域。此外，CIG满足积分归因的公理，保证了理论的严谨性。文中还提出了两种归因质量指标MIL-AIC和MIL-SIC，用于衡量在弱监督下，模型对显著区域的预测信息和置信度的演变。

Result: 实验在CAMELYON16（乳腺癌淋巴结转移）、TCGA-RCC（肾细胞癌）和TCGA-Lung（肺癌）三个数据集上验证了CIG的有效性。定量（通过MIL-AIC和MIL-SIC）和定性（通过可视化）结果均表明，CIG生成的归因信息与真实肿瘤区域高度一致，显著优于现有方法。

Conclusion: CIG（对比性积分梯度）方法通过计算对数空间中的对比梯度，显著提升了WSI分析的解释性。该方法不仅在理论上满足积分归因的公理，还通过MIL-AIC和MIL-SIC等量化指标，验证了其在区分肿瘤亚型中的有效性。实验结果表明，CIG在多种癌症数据集上均能生成与真实肿瘤区域高度一致的解释性结果，证明了其在可解释和可信赖的WSI诊断中的潜力。

Abstract: Interpretability is essential in Whole Slide Image (WSI) analysis for computational pathology, where understanding model predictions helps build trust in AI-assisted diagnostics. While Integrated Gradients (IG) and related attribution methods have shown promise, applying them directly to WSIs introduces challenges due to their high-resolution nature. These methods capture model decision patterns but may overlook class-discriminative signals that are crucial for distinguishing between tumor subtypes. In this work, we introduce Contrastive Integrated Gradients (CIG), a novel attribution method that enhances interpretability by computing contrastive gradients in logit space. First, CIG highlights class-discriminative regions by comparing feature importance relative to a reference class, offering sharper differentiation between tumor and non-tumor areas. Second, CIG satisfies the axioms of integrated attribution, ensuring consistency and theoretical soundness. Third, we propose two attribution quality metrics, MIL-AIC and MIL-SIC, which measure how predictive information and model confidence evolve with access to salient regions, particularly under weak supervision. We validate CIG across three datasets spanning distinct cancer types: CAMELYON16 (breast cancer metastasis in lymph nodes), TCGA-RCC (renal cell carcinoma), and TCGA-Lung (lung cancer). Experimental results demonstrate that CIG yields more informative attributions both quantitatively, using MIL-AIC and MIL-SIC, and qualitatively, through visualizations that align closely with ground truth tumor regions, underscoring its potential for interpretable and trustworthy WSI-based diagnostics

</details>


### [109] [Generalizable Blood Cell Detection via Unified Dataset and Faster R-CNN](https://arxiv.org/abs/2511.08465)
*Siddharth Sahay*

Main category: cs.CV

TL;DR: 本文提出了一种自动分类和检测外周血细胞的方法，并通过比较性能分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和异质性的关键挑战，建立统一的数据集。

Method: 开发了数据管道标准化并合并四个公共数据集，采用Faster R-CNN框架和ResNet-50-FPN骨干网络，比较随机初始化模型和迁移学习模型的性能。

Result: 迁移学习方法显著提高了收敛速度和稳定性，最终验证损失为0.08666，优于基线。

Conclusion: 验证的方法为构建高精度、可部署的自动化血液诊断系统奠定了基础。

Abstract: This paper presents a comprehensive methodology and comparative performance analysis for the automated classification and object detection of peripheral blood cells (PBCs) in microscopic images. Addressing the critical challenge of data scarcity and heterogeneity, robust data pipeline was first developed to standardize and merge four public datasets (PBC, BCCD, Chula, Sickle Cell) into a unified resource. Then employed a state-of-the-art Faster R-CNN object detection framework, leveraging a ResNet-50-FPN backbone. Comparative training rigorously evaluated a randomly initialized baseline model (Regimen 1) against a Transfer Learning Regimen (Regimen 2), initialized with weights pre-trained on the Microsoft COCO dataset. The results demonstrate that the Transfer Learning approach achieved significantly faster convergence and superior stability, culminating in a final validation loss of 0.08666, a substantial improvement over the baseline. This validated methodology establishes a robust foundation for building high-accuracy, deployable systems for automated hematological diagnosis.

</details>


### [110] [Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding](https://arxiv.org/abs/2511.08480)
*Da Li,Yuxiao Luo,Keping Bi,Jiafeng Guo,Wei Yuan,Biao Yang,Yan Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: CoMa通过压缩预训练优化VLM嵌入模型，在小数据下实现高效多模态表示学习，并在MMEB达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过解耦语义内容理解和判别性特征强调，提升视觉语言模型在多模态表示学习中的性能。

Method: 提出CoMa方法，作为对比学习的预热阶段，仅需少量预训练数据即可将VLM转化为有竞争力的嵌入模型。

Result: CoMa在MMEB基准测试中取得了同类模型中最先进的性能，实现了效率和效果的双重优化。

Conclusion: CoMa通过压缩预训练阶段显著提升了视觉语言模型在下游任务中的表现，证明了其高效性和有效性。

Abstract: Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.

</details>


### [111] [Fast Multi-Organ Fine Segmentation in CT Images with Hierarchical Sparse Sampling and Residual Transformer](https://arxiv.org/abs/2511.08509)
*Xueqi Guo,Halid Ziya Yerebakan,Yoshihisa Shinagawa,Kritika Iyer,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 提出了一种结合分层稀疏采样和残差Transformer的快速多器官分割方法，显著提升了性能并降低了计算成本，速度达2.24秒（CPU），展示了实时分割潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在3D医学图像分割中表现出色，但逐体素处理整个3D体积的时间和内存消耗巨大。现有分类器在速度和准确性之间存在权衡问题，因此需要一种更高效的解决方案。

Method: 提出了一种新颖的快速多器官分割框架，结合了分层稀疏采样和残差Transformer。分层稀疏采样策略通过多分辨率级别减少计算时间并保持层次上下文，而残差Transformer分割网络则能够高效提取和组合不同层次的信息。

Result: 在包含10,253张CT图像的内部数据集和公开数据集TotalSegmentator上，该方法在保持快速速度的同时，显著提升了定性及定量分割性能。

Conclusion: 提出的方法在内部数据集和公开数据集上都表现出色，不仅提升了分割性能，还保持了较低的计算成本，速度达到约2.24秒（CPU硬件），展示了实时精细器官分割的潜力。

Abstract: Multi-organ segmentation of 3D medical images is fundamental with meaningful applications in various clinical automation pipelines. Although deep learning has achieved superior performance, the time and memory consumption of segmenting the entire 3D volume voxel by voxel using neural networks can be huge. Classifiers have been developed as an alternative in cases with certain points of interest, but the trade-off between speed and accuracy remains an issue. Thus, we propose a novel fast multi-organ segmentation framework with the usage of hierarchical sparse sampling and a Residual Transformer. Compared with whole-volume analysis, the hierarchical sparse sampling strategy could successfully reduce computation time while preserving a meaningful hierarchical context utilizing multiple resolution levels. The architecture of the Residual Transformer segmentation network could extract and combine information from different levels of information in the sparse descriptor while maintaining a low computational cost. In an internal data set containing 10,253 CT images and the public dataset TotalSegmentator, the proposed method successfully improved qualitative and quantitative segmentation performance compared to the current fast organ classifier, with fast speed at the level of ~2.24 seconds on CPU hardware. The potential of achieving real-time fine organ segmentation is suggested.

</details>


### [112] [CleverBirds: A Multiple-Choice Benchmark for Fine-grained Human Knowledge Tracing](https://arxiv.org/abs/2511.08512)
*Leonie Bossemeyer,Samuel Heinrich,Grant Van Horn,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: CleverBirds是一个大规模鸟类识别知识追踪基准数据集，通过eBird平台收集，支持视觉专业知识发展的研究。


<details>
  <summary>Details</summary>
Motivation: 研究人类在细粒度视觉识别中的专业知识发展，并准确推断学习者的知识状态。

Method: 通过eBird公民科学平台收集大规模数据，包括40,000多名参与者的17百万多选择题回答，覆盖10,000多种鸟类。

Result: 数据集展示了追踪学习者知识的挑战性，尤其是在不同参与者子组和问题类型中，上下文信息的预测效果各异。

Conclusion: CleverBirds数据集为研究视觉专业知识的发展提供了重要资源，支持新方法的开发和评估。

Abstract: Mastering fine-grained visual recognition, essential in many expert domains, can require that specialists undergo years of dedicated training. Modeling the progression of such expertize in humans remains challenging, and accurately inferring a human learner's knowledge state is a key step toward understanding visual learning. We introduce CleverBirds, a large-scale knowledge tracing benchmark for fine-grained bird species recognition. Collected by the citizen-science platform eBird, it offers insight into how individuals acquire expertize in complex fine-grained classification. More than 40,000 participants have engaged in the quiz, answering over 17 million multiple-choice questions spanning over 10,000 bird species, with long-range learning patterns across an average of 400 questions per participant. We release this dataset to support the development and evaluation of new methods for visual knowledge tracing. We show that tracking learners' knowledge is challenging, especially across participant subgroups and question types, with different forms of contextual information offering varying degrees of predictive benefit. CleverBirds is among the largest benchmark of its kind, offering a substantially higher number of learnable concepts. With it, we hope to enable new avenues for studying the development of visual expertize over time and across individuals.

</details>


### [113] [UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist](https://arxiv.org/abs/2511.08521)
*Zhengyang Liang,Daoan Zhang,Huichi Zhou,Rui Huang,Bobo Li,Yuechen Zhang,Shengqiong Wu,Xiaohan Wang,Jiebo Luo,Lizi Liao,Hao Fei*

Main category: cs.CV

TL;DR: UniVA是一个开源的多代理框架，统一视频理解、分割、编辑和生成，支持复杂迭代工作流，并通过UniVA-Bench进行评估。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要结合视频理解、分割、编辑和生成等复杂迭代工作流，而现有专用AI模型无法满足这一需求。

Method: UniVA采用Plan-and-Act双代理架构，通过规划代理和执行代理实现高度自动化和主动的工作流程，支持模块化工具服务器和分层多级内存。

Result: UniVA实现了迭代和任意条件视频工作流，并通过UniVA-Bench基准套件对代理视频系统进行严格评估。

Conclusion: UniVA和UniVA-Bench的开源旨在推动下一代多模态AI系统中交互式、智能代理和通用视频智能的研究。

Abstract: While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\rightarrow$ multi-round editing $\rightarrow$ object segmentation $\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)

</details>


### [114] [Large Sign Language Models: Toward 3D American Sign Language Translation](https://arxiv.org/abs/2511.08535)
*Sen Zhang,Xiaoxiao He,Di Liu,Zhaoyang Xia,Mingyu Zhao,Chaowei Tan,Vivian Li,Bo Liu,Dimitris N. Metaxas,Mubbasir Kapadia*

Main category: cs.CV

TL;DR: 提出LSLM框架，利用LLMs处理3D手语数据，提升翻译准确性和灵活性，为听力障碍人群改善虚拟沟通。


<details>
  <summary>Details</summary>
Motivation: 旨在改善听力障碍人群的虚拟沟通，通过3D手语数据提供更准确和鲁棒的翻译，同时扩展LLMs对多模态语言的理解能力。

Method: 利用大型语言模型（LLMs）作为核心，直接处理3D手语数据，捕捉丰富的空间、手势和深度信息。研究探索了从3D手势特征到文本的直接翻译，以及通过外部提示调节翻译的指令引导设置。

Result: 提出的框架能够更准确地翻译3D美国手语（ASL），并展示了在指令引导下翻译的灵活性。

Conclusion: 该研究为包容性、多模态智能系统奠定了基础，能够理解多样化的语言形式。

Abstract: We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.

</details>


### [115] [3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation](https://arxiv.org/abs/2511.08536)
*Yunhong He,Zhengqing Yuan,Zhengzhong Tu,Yanfang Ye,Lichao Sun*

Main category: cs.CV

TL;DR: 3D4D是一个交互式4D可视化框架，通过WebGL和Supersplat渲染技术将静态内容转换为4D场景，支持实时交互。


<details>
  <summary>Details</summary>
Motivation: 为了支持用户驱动的复杂4D环境自适应探索，开发了这一交互式4D可视化框架。

Method: 该框架包含四个核心模块，并采用注视点渲染策略以实现高效的实时交互。

Result: 3D4D框架能够实现高效的实时多模态交互，项目页面和代码已公开。

Conclusion: 3D4D框架通过集成WebGL和Supersplat渲染技术，成功实现了静态图像和文本向连贯4D场景的转换，支持实时多模态交互。

Abstract: We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.

</details>


### [116] [RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses](https://arxiv.org/abs/2511.08545)
*Sriram Srinivasan,Gautam Ramachandra*

Main category: cs.CV

TL;DR: 提出一种鲁棒框架，直接从多视角图像重建高质量3D网格，联合优化相机姿态并学习隐式场景表示，适用于机器人应用。


<details>
  <summary>Details</summary>
Motivation: 尽管多视角图像的精确相机外参获取在现实场景中具有挑战性，但现有基于NeRF的方法严重依赖准确的外参估计，且其隐式体积表示与广泛采用的多边形网格差异较大，导致在标准3D软件中渲染和操作效率低下。

Method: 该方法联合优化相机姿态，同时学习捕获精细几何细节和逼真外观的隐式场景表示。

Result: 实验结果表明，该方法在相机姿态不确定的情况下实现了准确且鲁棒的3D重建，弥合了神经隐式表示与实际机器人应用之间的差距。

Conclusion: 本文提出了一种直接从多视角图像重建高质量、可编辑3D网格的鲁棒框架，即使在相机外参存在噪声的情况下也能实现精确的3D重建。

Abstract: Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.

</details>


### [117] [SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology](https://arxiv.org/abs/2511.08573)
*Shanaka Liyanaarachchi,Chathurya Wijethunga,Shihab Aaquil Ahamed,Akthas Absar,Ranga Rodrigo*

Main category: cs.CV

TL;DR: SENCA-st通过交叉注意力整合组织病理学与空间转录组数据，显著提升肿瘤区域检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理组织病理学与空间转录组数据时，要么过度依赖空间转录组数据导致噪声干扰，要么因普通对比学习而丢失功能信息，导致信息损失。

Method: 提出了SENCA-st（共享编码器与邻域交叉注意力）架构，通过交叉注意力强调组织病理学结构相似但空间转录组功能不同的区域。

Result: SENCA-st在检测肿瘤异质性和微环境区域方面表现优于现有方法。

Conclusion: SENCA-st架构通过交叉注意力机制有效保留了两种模态的特征，显著提升了肿瘤异质性和微环境区域的检测性能，超越了现有最先进方法。

Abstract: Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [118] [Deep Inverse Shading: Consistent Albedo and Surface Detail Recovery via Generative Refinement](https://arxiv.org/abs/2511.08079)
*Jiacheng Wu,Ruiqi Zhang,Jie Chen*

Main category: cs.GR

TL;DR: DIS是一个统一框架，通过结合生成先验和表面表示，实现了高质量、可重新光照的头像重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖体积表示或基于表面的表示，但存在训练速度慢或网格分辨率受限的问题。此外，生成先验与基于物理的人类头像建模的结合尚未充分探索。DIS旨在解决这些挑战。

Method: DIS引入了一个基于网格的模型，通过多视图2D生成表面法线预测和法线转换模块，将生成的法线输出转换为每三角形表面偏移。此外，DIS还集成了去阴影模块以恢复准确的材质属性。

Result: DIS实现了高质量的重新光照效果，提高了渲染效率，降低了内存消耗，并实现了详细的表面重建。

Conclusion: DIS框架通过联合优化几何和材质外观，实现了物理一致的高质量重建，适用于准确的重新光照。实验表明，DIS在重新光照质量、渲染效率、内存消耗和详细表面重建方面均达到先进水平。

Abstract: Reconstructing human avatars using generative priors is essential for achieving versatile and realistic avatar models. Traditional approaches often rely on volumetric representations guided by generative models, but these methods require extensive volumetric rendering queries, leading to slow training. Alternatively, surface-based representations offer faster optimization through differentiable rasterization, yet they are typically limited by vertex count, restricting mesh resolution and scalability when combined with generative priors. Moreover, integrating generative priors into physically based human avatar modeling remains largely unexplored. To address these challenges, we introduce DIS (Deep Inverse Shading), a unified framework for high-fidelity, relightable avatar reconstruction that incorporates generative priors into a coherent surface representation. DIS centers on a mesh-based model that serves as the target for optimizing both surface and material details. The framework fuses multi-view 2D generative surface normal predictions, rich in detail but often inconsistent, into the central mesh using a normal conversion module. This module converts generative normal outputs into per-triangle surface offsets via differentiable rasterization, enabling the capture of fine geometric details beyond sparse vertex limitations. Additionally, DIS integrates a de-shading module to recover accurate material properties. This module refines albedo predictions by removing baked-in shading and back-propagates reconstruction errors to optimize the geometry. Through joint optimization of geometry and material appearance, DIS achieves physically consistent, high-quality reconstructions suitable for accurate relighting. Our experiments show that DIS delivers SOTA relighting quality, enhanced rendering efficiency, lower memory consumption, and detailed surface reconstruction.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [119] [Testing noisy low-degree polynomials for sparsity](https://arxiv.org/abs/2511.07835)
*Yiqiao Bao,Anindya De,Shivam Nadimpalli,Rocco A. Servedio,Nathan White*

Main category: cs.DS

TL;DR: 论文提出了一种测试低度多项式稀疏性的方法，在特定条件下实现了恒定样本复杂度，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 扩展了Chen等人（2020）的工作，从噪声线性函数推广到一般低度多项式，解决了稀疏性测试问题。

Method: 通过定义可计算函数MSG_X,d(·)，并基于噪声评估的随机采样，设计了区分稀疏与非稀疏多项式的算法。技术方法包括对有限支持分布下傅里叶尾结果的泛化。

Result: 在T≥MSG_X,d(s)时，提出了样本复杂度与维度无关的算法；在T≤MSG_X,d(s)-1时，证明了样本复杂度必须随维度对数增长。

Conclusion: 该论文提供了低度多项式稀疏性测试的精确特征描述，并提出了在特定条件下具有恒定样本复杂度的匹配算法。同时，证明了在某些条件下，样本复杂度必须随维度对数增长。

Abstract: We consider the problem of testing whether an unknown low-degree polynomial $p$ over $\mathbb{R}^n$ is sparse versus far from sparse, given access to noisy evaluations of the polynomial $p$ at \emph{randomly chosen points}. This is a property-testing analogue of classical problems on learning sparse low-degree polynomials with noise, extending the work of Chen, De, and Servedio (2020) from noisy \emph{linear} functions to general low-degree polynomials.
  Our main result gives a \emph{precise characterization} of when sparsity testing for low-degree polynomials admits constant sample complexity independent of dimension, together with a matching constant-sample algorithm in that regime. For any mean-zero, variance-one finitely supported distribution $\boldsymbol{X}$ over the reals, degree $d$, and any sparsity parameters $s \leq T$, we define a computable function $\mathrm{MSG}_{\boldsymbol{X},d}(\cdot)$, and:
  - For $T \ge \mathrm{MSG}_{\boldsymbol{X},d}(s)$, we give an $O_{s,\boldsymbol{X},d}(1)$-sample algorithm that distinguishes whether a multilinear degree-$d$ polynomial over $\mathbb{R}^n$ is $s$-sparse versus $\varepsilon$-far from $T$-sparse, given examples $(\boldsymbol{x},\, p(\boldsymbol{x}) + \mathrm{noise})_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$. Crucially, the sample complexity is \emph{completely independent} of the ambient dimension $n$.
  - For $T \leq \mathrm{MSG}_{\boldsymbol{X},d}(s) - 1$, we show that even without noise, any algorithm given samples $(\boldsymbol{x},p(\boldsymbol{x}))_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$ must use $Ω_{\boldsymbol{X},d,s}(\log n)$ examples.
  Our techniques employ a generalization of the results of Dinur et al. (2007) on the Fourier tails of bounded functions over $\{0,1\}^n$ to a broad range of finitely supported distributions, which may be of independent interest.

</details>


### [120] [Model-agnostic super-resolution in high dimensions](https://arxiv.org/abs/2511.07846)
*Xi Chen,Anindya De,Yizhi Huang,Shivam Nadimpalli,Rocco A. Servedio,Tianqi Yang*

Main category: cs.DS

TL;DR: 本文放宽超分辨率问题的信号假设，研究了一般信号的重建。结果表明，不同重建方法对傅里叶系数的需求差异显著：Wasserstein距离需exp(d)个，而“重击者”重建仅需exp(√d)个。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率研究通常对信号施加强建模假设，如要求信号是空间分离点源的线性组合。本文旨在放宽这些限制，研究更一般的信号情况，以扩展超分辨率问题的适用范围。

Method: 本文研究了超分辨率问题的一般版本，考虑了在d维环面上的完全一般信号，不假设任何空间分离或点源的有限线性组合。通过两种重建概念（Wasserstein距离和“重击者”重建）进行分析，并提供了相应的上下界。

Result: 研究结果显示，对于d维信号，Wasserstein距离重建需要约exp(d)个傅里叶系数，而“重击者”重建仅需约exp(√d)个傅里叶系数即可实现准确重建。

Conclusion: 本文通过分析超分辨率问题的两个自然重建概念，即Wasserstein距离重建和“重击者”重建，提供了在这些条件下准确重建所需的傅里叶系数数量的上下界。结果表明，不同重建方法对傅里叶系数的需求存在显著差异。

Abstract: The problem of \emph{super-resolution}, roughly speaking, is to reconstruct an unknown signal to high accuracy, given (potentially noisy) information about its low-degree Fourier coefficients. Prior results on super-resolution have imposed strong modeling assumptions on the signal, typically requiring that it is a linear combination of spatially separated point sources.
  In this work we analyze a very general version of the super-resolution problem, by considering completely general signals over the $d$-dimensional torus $[0,1)^d$; we do not assume any spatial separation between point sources, or even that the signal is a finite linear combination of point sources. We obtain two sets of results, corresponding to two natural notions of reconstruction.
  - {\bf Reconstruction in Wasserstein distance:} We give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction in Wasserstein distance is possible. Roughly speaking, our results here show that for $d$-dimensional signals, estimates of $\approx \exp(d)$ many Fourier coefficients are necessary and sufficient for accurate reconstruction under the Wasserstein distance.
  - {\bf "Heavy hitter" reconstruction:} For nonnegative signals (equivalently, probability distributions), we introduce a new notion of "heavy hitter" reconstruction that essentially amounts to achieving high-accuracy reconstruction of all "sufficiently dense" regions of the distribution. Here too we give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction is possible. Our results show that -- in sharp contrast with Wasserstein reconstruction -- accurate estimates of only $\approx \exp(\sqrt{d})$ many Fourier coefficients are necessary and sufficient for heavy hitter reconstruction.

</details>


### [121] [Deterministic Padded Decompositions and Negative-Weight Shortest Paths](https://arxiv.org/abs/2511.07859)
*Jason Li*

Main category: cs.DS

TL;DR: 首个近线性时间确定性算法，用于整数加权图的负权重单源最短路径问题，核心是确定性填充分解。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法在负权重单源最短路径问题上的效率不足，尤其是确定性算法的缺失。

Method: 通过确定性构造有向图上的填充分解（padded decomposition）实现。

Result: 成功实现了近线性时间复杂度的确定性算法，且填充分解方法可能具有独立的应用价值。

Conclusion: 本文提出了首个在整数加权图上计算负权重单源最短路径的确定性算法，具有近线性时间复杂度。

Abstract: We obtain the first near-linear time deterministic algorithm for negative-weight single-source shortest paths on integer-weighted graphs. Our main ingredient is a deterministic construction of a padded decomposition on directed graphs, which may be of independent interest.

</details>


### [122] [Parallel Sampling via Autospeculation](https://arxiv.org/abs/2511.07869)
*Nima Anari,Carlo Baronio,CJ Chen,Alireza Haqi,Frederic Koehler,Anqi Li,Thuy-Duong Vuong*

Main category: cs.DS

TL;DR: 论文提出‘推测性拒绝采样’技术，通过并行化将采样时间从$\widetilde{O}(n)$降至$\widetilde{O}(n^{1/2})$，适用于自回归和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 为了减少标准顺序采样算法在两种模型中的高时间复杂度（$\widetilde{O}(n)$），寻求并行化加速的可能性。

Method: 引入‘推测性拒绝采样’技术，利用辅助分布$ν$近似目标分布$μ$，并通过序列级别的推测实现并行化。

Result: 在目标分布$μ$的支持有界这一温和假设下，实现了预期采样时间降至$\widetilde{O}(n^{1/2})$，较先前$\widetilde{O}(n^{2/3})$的界限有所提升。

Conclusion: 论文提出了一种名为‘推测性拒绝采样’的新技术，通过并行化加速了两种模型（任意顺序自回归模型和去噪扩散模型）的采样过程，将预期采样时间从$\widetilde{O}(n)$降低到$\widetilde{O}(n^{1/2})$。

Abstract: We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $μ$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $μ$ on $\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\widetilde{O}(n)$ time to produce a sample from $μ$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\widetilde{O}(n^{1/2})$. This improves the previous $\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $μ$ is bounded.
  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$ν$ that approximates~$μ$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $ν$ out of the same oracle that defines~$μ$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $ν$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\widetilde{O}(n^{1/2})$.

</details>


### [123] [Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation](https://arxiv.org/abs/2511.08210)
*Taisuke Izumi,Naoki Kitamura,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文提出了一种新的结构定理和算法，简化了最大基数匹配问题的解决，并改进了分布式和半流式环境中的近似算法性能。


<details>
  <summary>Details</summary>
Motivation: MV算法的正确性证明极其复杂，且其实现难度较高。作者旨在提出一种更易理解和验证的算法，并探索在分布式和流式场景中的高效近似算法。

Method: 作者引入了一种称为交替基树（ABT）的概念，并提出了新的结构定理，简化了交替路径的分析。基于此定理，设计了一个新算法，并开发了分布式和半流式环境中的近似算法框架。

Result: 提出的新算法比MV算法更易实现和验证，同时在分布式和半流式设置中提供了更高效的近似算法，显著提升了运行时间的上界。

Conclusion: 本文提出了一种新的结构定理，用于描述一般图中的最短交替路径，避免了复杂的blossom结构。基于此定理，作者设计了一个新的算法，虽然比MV算法稍慢，但更易于实现和验证。此外，还提出了在分布式和半流式设置中的新近似算法。

Abstract: Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.
  In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.
  As applications of our framework, we also present new $(1 - ε)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest.

</details>


### [124] [Fully Dynamic Set Cover: Worst-Case Recourse and Update Time](https://arxiv.org/abs/2511.08485)
*Sayan Bhattacharya,Ruoxu Cen,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 首个完全动态集合覆盖算法，在worst-case下同时优化recourse和update time，填补研究空白。


<details>
  <summary>Details</summary>
Motivation: 动态集合覆盖问题在过去十年中得到广泛研究，但现有算法要么仅实现amortized边界，要么在worst-case update time下牺牲recourse边界（达到Ω(m)）。本文旨在解决这一局限。

Method: 研究者设计了完全动态集合覆盖算法，在worst-case情况下，同时实现O(log n)的recourse和f·poly log(n)的update time，适用于两种近似方案：O(log n)和O(f)近似。

Result: 算法在worst-case情况下，同时实现了O(log n)的recourse和f·poly log(n)的update time，优于现有结果。

Conclusion: 本文提出了首个在完全动态集合覆盖问题中同时实现非平凡的worst-case边界（recourse和update time）的算法，填补了现有研究的空白。

Abstract: In (fully) dynamic set cover, the goal is to maintain an approximately optimal solution to a dynamically evolving instance of set cover, where in each step either an element is added to or removed from the instance. The two main desiderata of a dynamic set cover algorithm are to minimize at each time-step, the recourse, which is the number of sets removed from or added to the solution, and the update time to compute the updated solution. This problem has been extensively studied over the last decade leading to many results that achieve ever-improving bounds on the recourse and update time, while maintaining a solution whose cost is comparable to that of offline approximation algorithms.
  In this paper, we give the first algorithms to simultaneously achieve non-trivial worst-case bounds for recourse and update time. Specifically, we give fully-dynamic set cover algorithms that simultaneously achieve $O(\log n)$ recourse and $f\cdot \textrm{poly}\log(n)$ update time in the worst-case, for both approximation regimes: $O(\log n)$ and $O(f)$ approximation. (Here, $n, f$ respectively denote the maximum number of elements and maximum frequency of an element across all instances.) Prior to our work, all results for this problem either settled for amortized bounds on recourse and update time, or obtained $f\cdot \textrm{poly}\log(n)$ update time in the worst-case but at the cost of $Ω(m)$ worst-case recourse. (Here, $m$ denotes the number of sets. Note that any algorithm has recourse at most $m$.)

</details>


### [125] [Deterministic Negative-Weight Shortest Paths in Nearly Linear Time via Path Covers](https://arxiv.org/abs/2511.08551)
*Bernhard Haeupler,Yonggang Jiang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 提出首个确定性近线性时间算法，解决带负权边有向图的单源最短路径问题，引入路径覆盖新结构。


<details>
  <summary>Details</summary>
Motivation: 现有的近线性时间算法均依赖于随机性，特别是低直径分解，本文旨在克服这一限制，提出确定性算法。

Method: 通过引入路径覆盖这一新结构，克服了随机算法的依赖，实现了确定性近线性时间算法。

Result: 算法能够在时间O~(m)·log(nW)内计算所有从源点s出发的距离或报告负环。

Conclusion: 本文提出了一种新的确定性近线性时间算法，解决了带负权边的有向图中单源最短路径问题，并引入了路径覆盖这一新结构，为未来确定性算法设计提供了基础工具。

Abstract: We present the first deterministic nearly-linear time algorithm for single-source shortest paths with negative edge weights on directed graphs: given a directed graph $G$ with $n$ vertices, $m$ edges whose weights are integer in $\{-W,\dots,W\}$, our algorithm either computes all distances from a source $s$ or reports a negative cycle in time $\tilde{O}(m)\cdot \log(nW)$ time.
  All known near-linear time algorithms for this problem have been inherently randomized, as they crucially rely on low-diameter decompositions.
  To overcome this barrier, we introduce a new structural primitive for directed graphs called the path cover. This plays a role analogous to neighborhood covers in undirected graphs, which have long been central to derandomizing algorithms that use low-diameter decomposition in the undirected setting. We believe that path covers will serve as a fundamental tool for the design of future deterministic algorithms on directed graphs.

</details>


### [126] [Universal Connection Schedules for Reconfigurable Networking](https://arxiv.org/abs/2511.08556)
*Shaleen Baral,Robert Kleinberg,Sylvan Martin,Henry Rogers,Tegan Wilson,Ruogu Zhang*

Main category: cs.DS

TL;DR: 该论文提出了首个通用调度方案，通过循环排列和傅里叶分析实现近乎最优的跳数和延迟性能，适用于混合延迟请求。


<details>
  <summary>Details</summary>
Motivation: 现有调度方案针对特定跳数优化，无法高效处理混合延迟请求，需要一种通用调度方案以同时实现多个最优权衡点。

Method: 基于循环排列的连接调度和傅里叶分析方法，结合随机路由和确定性算法（Lovett-Meka差异最小化）。

Result: 提出的通用调度方案在吞吐量和延迟上均接近最优，且可通过确定性算法实现相同效果。

Conclusion: 该论文提出了首个用于无感知路由的通用调度方案，通过循环排列和傅里叶分析方法实现了近乎最优的跳数性能。

Abstract: Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.
  A more flexible and efficient alternative is a {\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: 本文比较了生成式和判别式模型在新冠检测中的表现，发现小型判别式模型（如Covid-Net）在准确性和碳足迹上表现最佳，而LLMs在分类任务中存在环境风险。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估AI工具（尤其是大型语言模型和小型判别式模型）在医疗诊断中的效率和环境影响，特别是在新冠检测中的应用。

Method: 将大型语言模型（LLMs）和小型判别式模型集成到Mendix应用中，用于检测胸部X光中的新冠。判别式模型还为LLMs提供知识库以提高准确性。研究了14种不同模型配置的准确性和环境影响。

Result: 研究发现，小型模型减少了碳足迹，但输出偏向阳性诊断且置信度不足。限制LLMs仅输出概率导致准确性和碳足迹表现均不佳。使用小型LLM GPT-4.1-Nano虽减少碳足迹94.2%，但仍不如判别式模型高效。Covid-Net模型碳足迹比GPT-4.5-Preview少99.9%，且准确率达95.5%，为最佳方案。

Conclusion: 本文通过比较生成式和判别式模型在新冠检测中的表现，并强调使用生成式工具进行分类任务的环境风险，丰富了相关知识。

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [128] [Agentic Educational Content Generation for African Languages on Edge Devices](https://arxiv.org/abs/2511.07437)
*Ravi Gupta,Guneet Bhatia*

Main category: cs.AI

TL;DR: 该研究提出了一种边缘设备上的自主代理框架，用于生成适应文化的教育内容，显著提升了在非洲资源受限环境中的教育可及性和质量。


<details>
  <summary>Details</summary>
Motivation: 解决撒哈拉以南非洲的教育不平等问题，通过本地化和可持续的AI驱动教育，支持联合国可持续发展目标（SDG）4、9和10。

Method: 利用四个专业代理协同工作，生成上下文适当的教育内容，并在Raspberry Pi 4B和NVIDIA Jetson Nano等边缘设备上进行实验验证。

Result: 实验显示系统性能显著，例如Jetson Nano上的TTFT为129毫秒，吞吐量为45.2令牌/秒，且在多语言质量、文化相关性和流畅性方面表现优异。

Conclusion: 该研究通过自主代理编排的框架，在边缘设备上实现去中心化、文化适应性强的教育内容生成，为资源受限环境中的教育不平等问题提供了可行解决方案。

Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.

</details>


### [129] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: 提出一种新型基于置信度的奖励模型，专门用于提升小型语言模型的STEM推理能力，通过惩罚低置信答案促进更稳健推理，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的奖励强化学习在小型模型上常导致推理链质量差或推理过程与最终答案不一致，限制了资源有限组织直接训练小型模型的能力。

Method: 通过静态评估、Best-of-N推理测试和基于PPO的强化学习训练验证模型的有效性。

Result: 新模型不仅在多样化STEM基准测试中表现优异，还惩罚了低置信度的正确答案，促进了更稳健和逻辑一致的推理。

Conclusion: 提出的基于置信度的奖励模型在提升小型语言模型的STEM推理能力方面表现出色，优于现有的开源奖励模型。

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [130] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: HTN形式的程序性知识可大幅提升LLM在代理任务中的表现，手工编码HTN效果优于LLM生成。


<details>
  <summary>Details</summary>
Motivation: LLM在执行代理任务时缺乏工具支持、提示工程或微调，而程序性知识（如HTN）可能显著提升其规划效率，但相关研究较少。

Method: 通过形式化、实现并评估一个基于分层任务网络（HTN）的代理LLM工作流程，对比手工编码和LLM生成的HTN效果。

Result: 手工编码的HTN显著提升LLM性能（20b/70b参数模型甚至超过120b基线），LLM生成的HTN也有改进但效果较弱。

Conclusion: 利用程序性知识（如HTN）可以显著提升LLM在代理任务中的表现，且专家（人类、文档或LLM）整理的程序性知识将成为优化LLM工作流的重要工具。

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


### [131] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: Orion框架通过合成轨迹生成、强化学习和推理时束搜索算法，训练紧凑模型进行迭代检索，在多项任务上超越更大规模检索器，表明检索性能可通过学习策略而非规模提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉复杂用户查询所需的迭代探索、反馈和修订动态。神经检索器缺乏推理能力，大语言模型（LLM）成本过高，查询重写或分解仅限于静态改进。

Method: Orion结合了：（1）合成轨迹生成和监督微调以鼓励模型多样化探索模式，（2）强化学习奖励有效的查询优化和回溯行为，（3）利用RL期间学到的自反能力的推理时束搜索算法。

Result: 1.2B参数的模型在SciFact上达到77.6%成功率（先前检索器为72.6%），BRIGHT上25.2%（先前22.1%），NFCorpus上63.2%（先前57.8%），并在FEVER、HotpotQA和MSMarco上保持竞争力。在六项基准测试中的五项上，其表现优于规模大200-400倍的检索器。

Conclusion: 研究发现检索性能可以通过学习到的策略而非仅仅模型规模提升，当模型被训练以搜索、反思和修订时，能够在多项基准测试中超越更大规模的检索器。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [132] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: GSW是一个神经启发的生成记忆框架，显著提升LLMs在长上下文推理中的表现，并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前的外部记忆框架无法构建基于情景的叙事表示，限制了LLMs在长上下文推理中的表现。

Method: 提出了Generative Semantic Workspace (GSW)，一个神经启发的生成记忆框架，包含Operator和Reconciler两部分，用于构建结构化的、可解释的情景表示。

Result: 在Episodic Memory Benchmark上，GSW比现有RAG基线性能提升高达20%，并减少51%的查询时间上下文令牌。

Conclusion: GSW为LLMs提供了类似人类的情景记忆能力，使其能够在长时间范围内进行推理，为更强大的智能体铺平了道路。

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [133] [AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation](https://arxiv.org/abs/2511.07667)
*Jakub Slapek,Mir Seyedebrahimi,Yang Jianhua*

Main category: cs.AI

TL;DR: 论文提出AI工具框架，通过多维度数据整合和LLM分析，解决团队贡献评估不公平问题，并讨论可行性及挑战。


<details>
  <summary>Details</summary>
Motivation: 团队合作中个体贡献评估存在不公平问题，现有工具在冲突解决和AI整合方面存在不足，需手动干预且成本高昂。

Method: 提出了一个框架，将异构数据（如提交内容、沟通记录、协调记录等）组织成三个维度（贡献、互动、角色），结合客观指标和不平等度量（基尼指数），并利用LLM进行验证和情境分析。

Result: 设计了一个AI工具框架，能够通过多维度数据分析和LLM技术生成透明、可解释的冲突解决建议。

Conclusion: 论文提出了一种新型AI增强工具框架，旨在解决团队合作中个体贡献评估的不公平问题，通过整合多维度数据和LLM技术生成透明建议，并探讨了可行性及实践挑战。

Abstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.

</details>


### [134] [Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions](https://arxiv.org/abs/2511.07669)
*Alejandro R. Jadad*

Main category: cs.AI

TL;DR: 研究提出了一种框架，通过校准和保护架构实现人类-AI认知伙伴关系，以在高风险决策中避免可预防的错误，验证了不同LLM架构的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在可验证领域表现优异，但在高风险战略决策中可靠性不足，这威胁到估值的可辩护性和投资的可持续性。

Method: 通过系统定性评估7个前沿级LLM和3个市场风险案例，采用详细提示指定决策伙伴关系，并明确避免奉承、虚构、解决方案漂移和虚无主义。

Result: 通过7阶段校准序列和5层保护架构，实现了伙伴关系的初步状态，并发现了三种关键发现：有序校准可实现伙伴关系但需维护协议、可靠性在架构漂移和上下文耗尽时下降、解体纪律可避免错误方向的追求。

Conclusion: 人类与AI团队可以通过有序校准实现认知伙伴关系，从而在高风险决策中避免可预防的遗憾，满足依赖于AI系统支持关键决策的回报期望。

Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.
  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.
  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.
  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.

</details>


### [135] [AIA Forecaster: Technical Report](https://arxiv.org/abs/2511.07678)
*Rohan Alur,Bradly C. Stadie,Daniel Kang,Ryan Chen,Matt McManus,Michael Rickert,Tyler Lee,Michael Federici,Richard Zhu,Dennis Fogerty,Hayley Williamson,Nina Lozinski,Aaron Linsky,Jasjeet S. Sekhon*

Main category: cs.AI

TL;DR: AIA Forecaster是一个LLM预测系统，结合代理搜索、监督代理和统计校准技术，表现媲美人类超级预测者，并在集成方法中提供附加信息。


<details>
  <summary>Details</summary>
Motivation: 开发一个基于大型语言模型（LLM）的系统，利用非结构化数据进行判断性预测，以挑战人类超级预测者的表现。

Method: AIA Forecaster结合了三个核心要素：对高质量新闻源的代理搜索、用于调和同一事件不同预测的监督代理，以及一组统计校准技术以应对大型语言模型中的行为偏差。

Result: 在ForecastBench基准测试中，AIA Forecaster表现与人类超级预测者相当，超越先前的LLM基线。在一个更难的预测市场基准测试中，AIA Forecaster虽不及市场共识，但与市场共识结合的集成方法优于单独共识，显示其提供附加信息。

Conclusion: 该工作确立了AI预测领域的新技术水平，并提供了实用、可转移的未来研究建议。这是首个可验证地实现大规模专家级预测的研究。

Abstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.

</details>


### [136] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 论文提出ResearchRubrics基准，用于评估DR系统的多步推理和跨文档合成能力，发现当前系统在细则遵循上表现不佳，呼吁更健壮的评估方法。


<details>
  <summary>Details</summary>
Motivation: 由于DR的响应长、多样性高且依赖动态信息源，其评估具有挑战性。需要标准化工具来衡量DR系统在事实基础、推理合理性和清晰度上的表现。

Method: 通过引入ResearchRubrics基准（包含2,500+专家编写的细则）和新的复杂性框架，结合人类与模型评估协议来分析DR系统的表现。

Result: 即使是领先的DR系统（如Gemini和OpenAI的DR）在细则遵循上的平均合规率也低于68%，主要问题在于遗漏隐含上下文和对检索信息推理不足。

Conclusion: 研究提出了ResearchRubrics作为评估深度研究(DR)能力的标准化基准，并揭示了当前先进DR系统在遵循这些标准上的不足，强调了构建健壮、可扩展评估框架的重要性。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [137] [Towards AI-Assisted Generation of Military Training Scenarios](https://arxiv.org/abs/2511.07690)
*Soham Hans,Volkan Ustun,Benjamin Nye,James Sterrett,Matthew Green*

Main category: cs.AI

TL;DR: 该论文提出了一种基于LLM的多智能体、多模态推理框架，用于自动生成军事训练中的复杂场景，通过分层子问题处理和专用智能体协同工作，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统模拟训练中复杂、适应性强的场景生成过程劳动密集且资源消耗大，而现有AI工具在LLM之前难以生成足够复杂或适应性强的场景。

Method: 通过将场景生成分解为子问题层次结构，并定义AI工具在每个子问题中的角色（生成选项供人类选择、生成候选产品供人类批准或修改、完全自动生成文本产物），采用基于LLM的专用智能体处理不同子问题。

Result: 通过概念验证，生成OPORD的机动方案和移动部分，并估计地图位置和移动，证明了框架的可行性和准确性。

Conclusion: 该框架展示了LLM驱动的多智能体系统在军事训练场景生成中的潜力，能够动态适应变化条件，推动自动化进程。

Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.

</details>


### [138] [Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources](https://arxiv.org/abs/2511.07719)
*Vít Růžička,Gonzalo Mateo-García,Itziar Irakulis-Loitxate,Juan Emmanuel Johnson,Manuel Montesino San Martín,Anna Allen,Luis Guanter,David R. Thompson*

Main category: cs.AI

TL;DR: 该论文描述了一个机器学习系统在联合国环境规划署甲烷警报和响应系统中的运行部署，用于检测甲烷排放，减少了误检测并加速了检测过程。


<details>
  <summary>Details</summary>
Motivation: 减少人为甲烷排放是减缓全球变暖最具成本效益的手段之一，但现有的卫星检测方法存在大量误检测，需要大量人工验证。

Method: 研究创建了一个全球最大且最多样化的甲烷羽流注释数据集，并定量比较了不同的深度学习模型配置。通过模型集成，减少了超过74%的误检测。

Result: 在七个月的运行部署中，该系统处理了场景并提出了羽流建议，加速了检测和分析过程，验证了1,351次甲烷泄漏并发送了479次利益相关者通知。

Conclusion: 该研究代表了一个关键的步骤，即建立一个全球AI辅助的甲烷泄漏检测系统，该系统能够处理即将到来的大量数据。

Abstract: Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. While satellite-based imaging spectrometers, such as EMIT, PRISMA, and EnMAP, can detect these point sources, current methane retrieval methods based on matched filters still produce a high number of false detections requiring laborious manual verification. This paper describes the operational deployment of a machine learning system for detecting methane emissions within the Methane Alert and Response System (MARS) of the United Nations Environment Programme's International Methane Emissions Observatory. We created the largest and most diverse global dataset of annotated methane plumes from three imaging spectrometer missions and quantitatively compared different deep learning model configurations. Focusing on the requirements for operational deployment, we extended prior evaluation methodologies from small tiled datasets to full granule evaluation. This revealed that deep learning models still produce a large number of false detections, a problem we address with model ensembling, which reduced false detections by over 74%. Deployed in the MARS pipeline, our system processes scenes and proposes plumes to analysts, accelerating the detection and analysis process. During seven months of operational deployment, it facilitated the verification of 1,351 distinct methane leaks, resulting in 479 stakeholder notifications. We further demonstrate the model's utility in verifying mitigation success through case studies in Libya, Argentina, Oman, and Azerbaijan. Our work represents a critical step towards a global AI-assisted methane leak detection system, which is required to process the dramatically higher data volumes expected from new and current imaging spectrometers.

</details>


### [139] [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842)
*Sunghyun Wee,Suyoung Kim,Hyeonjin Kim,Kyomin Hwang,Nojun Kwak*

Main category: cs.AI

TL;DR: AAQ是一种新型量化方法，通过整合APC损失，在保持模型安全对齐的同时实现高效量化，解决了传统PTQ中效率与安全的冲突。


<details>
  <summary>Details</summary>
Motivation: 传统的PTQ范式存在一个根本缺陷：仅追求低困惑度的量化可能会变成安全漏洞，模型可能表现出低困惑度但在安全政策对齐方面显著退化。

Method: 提出了Alignment-Aware Quantization（AAQ），通过将Alignment-Preserving Contrastive（APC）损失整合到PTQ流程中，显式地保持对齐。

Result: AAQ实现了稳健的安全对齐，无需依赖专门的安全校准数据集，支持4位（W4A4）量化，适用于多种模型家族（如LLaMA、Qwen、Mistral）。

Conclusion: AAQ方法解决了大型语言模型在效率和安全性之间的关键权衡，为既高效又可信赖的LLMs铺平了道路。

Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.

</details>


### [140] [GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem](https://arxiv.org/abs/2511.07850)
*Xiangling Chen,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: GAMA提出一种图感知多模态注意力模型，通过结构化状态表示和门控融合提升车辆路径问题的解决性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经邻域搜索方法在车辆路径问题中因状态表示简单和信息融合方式单一，难以捕捉丰富的结构和语义上下文。

Method: GAMA使用图神经网络编码问题实例及其动态解作为不同模态，并通过堆叠的自注意力和交叉注意力层建模其内部及跨模态交互，结合门控融合机制整合多模态表示。

Result: 在多种合成和基准实例上的实验表明，GAMA显著优于现有神经基线，消融研究证实多模态注意力机制和门控融合设计是关键。

Conclusion: GAMA通过图感知多模态注意力机制和门控融合设计，显著提升了车辆路径问题的解决性能，验证了多模态交互和结构化状态表示的重要性。

Abstract: Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problems (VRPs). However, most existing approaches rely on simplistic state representations and fuse heterogeneous information via naive concatenation, limiting their ability to capture rich structural and semantic context. To address these limitations, we propose GAMA, a neural neighborhood search method with Graph-aware Multi-modal Attention model in VRP. GAMA encodes the problem instance and its evolving solution as distinct modalities using graph neural networks, and models their intra- and inter-modal interactions through stacked self- and cross-attention layers. A gated fusion mechanism further integrates the multi-modal representations into a structured state, enabling the policy to make informed and generalizable operator selection decisions. Extensive experiments conducted across various synthetic and benchmark instances demonstrate that the proposed algorithm GAMA significantly outperforms the recent neural baselines. Further ablation studies confirm that both the multi-modal attention mechanism and the gated fusion design play a key role in achieving the observed performance gains.

</details>


### [141] [WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking](https://arxiv.org/abs/2511.07863)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: WaterMod通过模块化规则优化水印嵌入，兼顾流畅性与可追溯性，适用于多种生成任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于logit的水印方法可能排除最高概率标记、影响流畅性的问题。

Method: 使用概率感知的模块化规则，将词汇表按模型概率降序排序后按模k分区，并对选定类别施加固定偏置。

Result: WaterMod在零位和多位设置下均表现出强水印检测性能，且生成质量不受影响。

Conclusion: WaterMod通过概率感知的模块化规则，在保持生成质量的同时实现了强水印检测性能，适用于多种任务。

Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.

</details>


### [142] [Confidence-Aware Neural Decoding of Overt Speech from EEG: Toward Robust Brain-Computer Interfaces](https://arxiv.org/abs/2511.07890)
*Soowon Kim,Byung-Kwan Ko,Seo-Hyun Lee*

Main category: cs.AI

TL;DR: 本文提出了一种结合深度集成和选择性分类的信心感知解码框架，用于提高脑机接口解码口头命令的准确性和可信度，在多类数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种既准确又可信赖的非侵入式脑机接口，用于从脑电图中解码口头命令。

Method: 采用深度集成紧凑的语音导向卷积网络，结合事后校准和选择性分类，使用集成预测熵、前两边缘和互信息量化不确定性，并通过精度覆盖操作点控制决策中的放弃选项。

Result: 所提方法在多类公开语音数据集上进行了评估，结果表明其提供了更可靠的概率估计、改进的选择性性能以及平衡的每类接受率。

Conclusion: 信心感知神经解码可为现实世界的脑机接口通信系统提供稳健、面向部署的行为。

Abstract: Non-invasive brain-computer interfaces that decode spoken commands from electroencephalogram must be both accurate and trustworthy. We present a confidence-aware decoding framework that couples deep ensembles of compact, speech-oriented convolutional networks with post-hoc calibration and selective classification. Uncertainty is quantified using ensemble-based predictive entropy, top-two margin, and mutual information, and decisions are made with an abstain option governed by an accuracy-coverage operating point. The approach is evaluated on a multi-class overt speech dataset using a leakage-safe, block-stratified split that respects temporal contiguity. Compared with widely used baselines, the proposed method yields more reliable probability estimates, improved selective performance across operating points, and balanced per-class acceptance. These results suggest that confidence-aware neural decoding can provide robust, deployment-oriented behavior for real-world brain-computer interface communication systems.

</details>


### [143] [Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Aphasia](https://arxiv.org/abs/2511.07895)
*Ha-Na Jo,Jung-Sun Lee,Eunyeong Ko*

Main category: cs.AI

TL;DR: 本研究开发了一种基于EEG的软多任务学习框架，用于解码失语症患者的意图，即使在发音错误的情况下也能显著提高识别准确率，展示了EEG辅助系统的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 失语症严重限制了语言交流，尽管脑机接口技术受到广泛关注，但针对失语症患者的EEG交流支持系统研究较少。本研究旨在填补这一空白。

Method: 研究招募了一名表达性失语症患者，进行基于韩语的自动语音任务，记录EEG信号，并通过对正确和错误发音试验的频谱分析，开发了一个软多任务学习框架，最大均值差异正则化，重点优化delta特征以对齐EEG特征分布。

Result: 提出的模型在正确发音试验中达到58.6%的准确率，在错误发音试验中达到45.5%，比基线提高了45%以上，展示了在发音错误情况下的稳健意图解码能力。

Conclusion: 研究表明，基于脑电图（EEG）的辅助系统在失语症患者的实际不完美语音条件下具备可行性，能够有效解码意图，即使在发音错误的情况下也表现优异。

Abstract: Aphasia severely limits verbal communication due to impaired language production, often leading to frequent misarticulations during speech attempts. Despite growing interest in brain-computer interface technologies, relatively little attention has been paid to developing EEG-based communication support systems tailored for aphasic patients. To address this gap, we recruited a single participant with expressive aphasia and conducted an Korean-based automatic speech task. EEG signals were recorded during task performance, and each trial was labeled as either correct or incorrect depending on whether the intended word was successfully spoken. Spectral analysis revealed distinct neural activation patterns between the two trial types: misarticulated trials exhibited excessive delta power across widespread channels and increased theta-alpha activity in frontal regions. Building upon these findings, we developed a soft multitask learning framework with maximum mean discrepancy regularization that focus on delta features to jointly optimize class discrimination while aligning the EEG feature distributions of correct and misarticulated trials. The proposed model achieved 58.6 % accuracy for correct and 45.5 % for misarticulated trials-outperforming the baseline by over 45 % on the latter-demonstrating robust intention decoding even under articulation errors. These results highlight the feasibility of EEG-based assistive systems capable of supporting real-world, imperfect speech conditions in aphasia patients.

</details>


### [144] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: A psychologically inspired Scaffold Reasoning framework improves LLM code debugging by integrating reference code construction and bug analysis, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Despite advanced reasoning algorithms in LLMs, balancing complexity and computational efficiency in reasoning steps remains unsolved, especially for System 2 reasoning.

Method: The framework includes three streams: Scaffold Stream (constructing reference code), Analytic Stream (analyzing buggy code), and Integration Stream (combining results).

Result: The framework achieves an 88.91% pass rate and 5.36 seconds average inference time on DebugBench, surpassing other methods.

Conclusion: The Scaffold Reasoning framework aligns with human cognitive processes and outperforms other reasoning approaches in both accuracy and efficiency, though it has limitations across different problem difficulties and bug types.

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [145] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: SparseRM 是一种轻量级奖励模型，通过稀疏自动编码器提取偏好特征，性能优于主流方法且参数极少。


<details>
  <summary>Details</summary>
Motivation: 解决在资源有限的情况下训练可靠奖励模型的挑战，减少对大规模偏好标注和高成本微调的依赖。

Method: SparseRM 使用稀疏自动编码器（SAE）分解大型语言模型的表示，提取偏好相关特征，并通过投影计算对齐分数，最终通过简单的奖励头预测偏好分数。

Result: 在三个偏好建模任务中，SparseRM 使用不到 1% 的可训练参数，性能优于主流奖励模型，并能无缝集成到下游对齐流程中。

Conclusion: SparseRM 通过利用稀疏自动编码器提取偏好相关信息，构建了一个轻量级且可解释的奖励模型，在资源有限的情况下表现优异，并展示了在高效对齐中的潜力。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [146] [Data Descriptions from Large Language Models with Influence Estimation](https://arxiv.org/abs/2511.07897)
*Chaeri Kim,Jaeyeon Bae,Taehwan Kim*

Main category: cs.AI

TL;DR: 本文提出了一种通过大语言模型生成文本描述来解释数据的方法，结合影响力估计和CLIP评分优化描述质量，并通过跨模态迁移分类任务验证了方法的有效性，在零样本实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在许多领域取得了成功，但其行为仍被视为黑箱。现有可解释AI（XAI）方法主要关注模型的预测解释，而本文希望通过语言这一常见媒介，以人类易于理解的方式解释数据。

Method: 提出了一种新颖的流水线，通过结合外部知识库和大语言模型生成文本描述来解释数据。同时引入了影响力估计和CLIP评分来选择最具信息量的文本描述，并基于跨模态可迁移性提出了跨模态迁移分类任务。

Result: 在零样本实验中，生成的文本描述比其他基线描述更有效，并成功提升了仅基于图像训练的模型在九个图像分类数据集上的性能。这些结果进一步得到了GPT-4o的评估支持。

Conclusion: 通过提出的方法，可以更深入地理解模型决策过程的固有可解释性。

Abstract: Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.

</details>


### [147] [Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning](https://arxiv.org/abs/2511.08301)
*Valentin Tablan,Scott Taylor,Gabriel Hurtado,Kristoffer Bernhem,Anders Uhrenholt,Gabriele Farei,Karo Moilanen*

Main category: cs.AI

TL;DR: Spark是一种共享代理记忆架构，用于提升AI编码代理的代码质量，实验证明其能让小模型媲美大模型，推荐有用性极高。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发从以人为中心转向以代理为中心，传统的知识共享环境受到破坏，而AI代理缺乏有效的共享学习资源。Spark旨在填补这一空白。

Method: 论文提出了一种名为Spark的共享代理记忆架构，用于模拟人类开发者社区的集体智慧，并通过实验评估了其作为AI编码代理教练的效果。

Result: 实验显示，Spark的推荐显著提升了通用代码生成模型的代码质量，小规模模型（300亿参数）能达到与大模型相当的水平；Spark推荐的有用性在最佳两档中达到98.2%。

Conclusion: Spark架构通过共享代理记忆显著提升了AI编码代理的代码生成质量，尤其是让小规模模型也能达到与大模型相当的代码质量。

Abstract: The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.
  In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.

</details>


### [148] [DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion](https://arxiv.org/abs/2511.07901)
*Haoning Li,Qinghua Huang*

Main category: cs.AI

TL;DR: DANS-KGC提出了一种基于扩散的自适应负采样方法，通过动态调整负样本硬度，显著提升了知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有负采样策略存在假阴性漏洞、泛化能力有限和样本硬度控制不足等问题，DANS-KGC旨在解决这些局限性。

Method: DANS-KGC包含三个关键模块：难度评估模块（DAM）用于评估实体学习难度；自适应负采样模块（ANS）通过条件扩散模型生成多样硬度负样本；动态训练机制（DTM）动态调整负样本难度分布。

Result: 在六个基准数据集上的实验表明，DANS-KGC在UMLS和YAGO3-10数据集的所有三个评估指标上均达到了最先进水平。

Conclusion: DANS-KGC通过结合语义和结构特征，动态调整负样本难度，实现了知识图谱补全任务的显著性能提升，并在多个基准数据集上达到了最先进水平。

Abstract: Negative sampling (NS) strategies play a crucial role in knowledge graph representation. In order to overcome the limitations of existing negative sampling strategies, such as vulnerability to false negatives, limited generalization, and lack of control over sample hardness, we propose DANS-KGC (Diffusion-based Adaptive Negative Sampling for Knowledge Graph Completion). DANS-KGC comprises three key components: the Difficulty Assessment Module (DAM), the Adaptive Negative Sampling Module (ANS), and the Dynamic Training Mechanism (DTM). DAM evaluates the learning difficulty of entities by integrating semantic and structural features. Based on this assessment, ANS employs a conditional diffusion model with difficulty-aware noise scheduling, leveraging semantic and neighborhood information during the denoising phase to generate negative samples of diverse hardness. DTM further enhances learning by dynamically adjusting the hardness distribution of negative samples throughout training, enabling a curriculum-style progression from easy to hard examples. Extensive experiments on six benchmark datasets demonstrate the effectiveness and generalization ability of DANS-KGC, with the method achieving state-of-the-art results on all three evaluation metrics for the UMLS and YAGO3-10 datasets.

</details>


### [149] [Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy](https://arxiv.org/abs/2511.07912)
*Jun-Young Kim,Young-Seok Kweon,Gi-Hwan Shin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 人类适应性推理涉及delta-theta-alpha神经振荡协调，而AI缺乏此机制。


<details>
  <summary>Details</summary>
Motivation: 适应性推理使人类能够灵活调整推理策略以适应环境规则或上下文的变化，但其神经动态机制尚不明确。

Method: 结合卡片分类范式与脑电图技术，比较人类与多模态大型语言模型的性能。

Result: 刺激和反馈锁定分析揭示了协调的delta-theta-alpha动态：早期delta-theta活动反映探索性监测和规则推断，而枕叶alpha参与表明成功规则识别后的注意力稳定确认。多模态大型语言模型仅表现出短期反馈驱动的调整，缺乏层次规则抽象或真正的适应性推理。

Conclusion: 本研究揭示了人类适应性推理的神经特征，并强调了将振荡反馈协调纳入脑启发人工智能以实现真正的上下文敏感适应的必要性。

Abstract: Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.

</details>


### [150] [Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia](https://arxiv.org/abs/2511.07920)
*Eunyeong Ko,Soowon Kim,Ha-Na Jo*

Main category: cs.AI

TL;DR: 该论文提出了一种基于扩散的实时想象语音分类框架，针对失语症患者，利用EEG数据实现高效解码，并在实际临床条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种优化的实时想象语音分类系统，以帮助患有失语症的个体。

Method: 系统集成了一个轻量级条件扩散编码器和卷积分类器，使用从韩语范式中获取的受试者特定EEG数据进行训练。采用双标准早期停止策略在有限校准数据下实现快速收敛，同时通过dropout正则化和分组时间卷积确保稳定的泛化能力。

Result: 在二十次实时试验中，该框架实现了65%的top-1和70%的top-2准确率，优于离线评估（50% top-1）。

Conclusion: 该论文提出的基于扩散的神经解码框架在实际临床条件下可行，能够为严重表达性语言障碍患者提供临床沟通支持。

Abstract: A diffusion-based neural decoding framework optimized for real-time imagined speech classification in individuals with aphasia. The system integrates a lightweight conditional diffusion encoder and convolutional classifier trained using subject-specific EEG data acquired from a Korean-language paradigm. A dual-criterion early stopping strategy enabled rapid convergence under limited calibration data, while dropout regularization and grouped temporal convolutions ensured stable generalization. During online operation, continuous EEG streams were processed in two-second sliding windows to generate class probabilities that dynamically modulated visual and auditory feedback according to decoding confidence. Across twenty real-time trials, the framework achieved 65% top-1 and 70% top-2 accuracy, outperforming offline evaluation (50% top-1). These results demonstrate the feasibility of deploying diffusion-based EEG decoding under practical clinical constraints, maintaining reliable performance despite environmental variability and minimal preprocessing. The proposed framework advances the translation of imagined speech brain-computer interfaces toward clinical communication support for individuals with severe expressive language impairment.

</details>


### [151] [Computational Blueprints: Generating Isomorphic Mathematics Problems with Large Language Models](https://arxiv.org/abs/2511.07932)
*Jeong-Hoon Kim,Jinwoo Nam,Geunsik Jo*

Main category: cs.AI

TL;DR: CBIT框架通过元级生成和模板变体，高效生成结构一致的数学问题，错误率低于专家编写问题，已成功应用于教育平台。


<details>
  <summary>Details</summary>
Motivation: 为了填补数学问题生成直接教育应用的空白，定义了新任务IMPG，旨在生成结构一致的问题变体。

Method: 通过元级生成和基于模板的选择性变体，建立了计算蓝图（CBIT），实现了高数学正确性和结构一致性。

Result: CBIT在生成准确性和成本效益上表现优异，生成问题错误率比专家编写问题低17.8%，并在商业平台上成功部署。

Conclusion: CBIT框架在生成准确性和成本效益上表现优异，且其生成的问题错误率比专家编写的问题低17.8%，已在商业教育平台上成功部署。

Abstract: Personalized mathematics education is growing rapidly, creating a strong demand for large sets of similar practice problems. Yet existing studies on mathematics problem generation have focused on data augmentation for training neural language models rather than on direct educational deployment. To bridge this gap, we define a new task, Isomorphic Math Problem Generation (IMPG), designed to produce structurally consistent variants of source problems. Subsequently, we explored LLM-based frameworks for automatic IMPG through successive refinements, and established Computational Blueprints for Isomorphic Twins (CBIT). With meta-level generation and template-based selective variation, CBIT achieves high mathematical correctness and structural consistency while reducing the cost of generation. Empirical results across refinements demonstrate that CBIT is superior on generation accuracy and cost-effectiveness at scale. Most importantly, CBIT-generated problems exhibited an error rate 17.8% lower than expert-authored items, with deployment to 6,732 learners on a commercial education platform yielding 186,870 interactions.

</details>


### [152] [Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System](https://arxiv.org/abs/2511.07936)
*Ji-Ha Park,Heon-Gyu Kwak,Gi-Hwan Shin,Yoo-In Jeon,Sun-Min Park,Ji-Yeon Hwang,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 本文提出了一种实时无线想象语音EEG解码系统，旨在推动脑机接口的实用化，实现了在有线和无线设备上的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口研究多局限于静态和固定环境，限制了其实际应用。本文旨在推动脑机接口技术向实用化发展。

Method: 采用实时无线想象语音脑电图（EEG）解码系统，利用实验室流层（lab streaming layer）管理实时EEG信号流，并通过用户识别模块提供个性化服务。

Result: 该系统在有线设备上实现了62.00%的4类分类准确率，在便携无线头戴设备上达到46.67%。

Conclusion: 本文展示了向真正实用和可访问的脑机接口技术迈出的重要一步，为未来在稳健、实用和个性化神经接口方面的研究指明了方向。

Abstract: Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.

</details>


### [153] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: Thinker通过分层思考和逻辑函数增强LLMs的推理监督，少量训练即达基线水平，全量训练表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于强化学习的LLMs在利用外部检索器时缺乏对推理过程的监督，导致逻辑一致性和严谨性难以保证的问题。

Method: 提出Thinker模型，通过分层思考将复杂问题分解为可独立解决的子问题，每个子问题以自然语言和逻辑函数双重表示，支持知识库和网页搜索，并通过逻辑函数传递子问题间的依赖关系以增强逻辑一致性。

Result: 实验表明，Thinker在少量训练样本下性能与基线相当，全量训练时显著优于多种数据集和模型规模的现有方法。

Conclusion: Thinker模型通过多轮交互的深度搜索，显著提升了LLMs在外部知识检索和网页搜索中的推理能力，且在少量训练样本下即能达到与现有基线竞争的性能，全量训练时表现更优。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [154] [TimeFlow: Towards Stochastic-Aware and Efficient Time Series Generation via Flow Matching Modeling](https://arxiv.org/abs/2511.07968)
*He Panjing,Cheng Mingyue,Li Li,Zhang XiaoHan*

Main category: cs.AI

TL;DR: TimeFlow是一种基于SDE的高效时间序列生成框架，通过改进流匹配方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算效率低，而传统流匹配方法无法有效捕捉时间序列的随机性，因此需要一种更高效的随机性建模方法。

Method: 提出TimeFlow框架，结合SDE和流匹配方法，设计组件式分解的速度场，并增强优化目标中的随机项。

Result: 实验表明，TimeFlow在生成质量、多样性和效率上均优于基线模型。

Conclusion: TimeFlow是一种基于SDE的流匹配框架，通过组件式分解的速度场和增强的随机项，显著提升了时间序列生成的质量、多样性和效率。

Abstract: Generating high-quality time series data has emerged as a critical research topic due to its broad utility in supporting downstream time series mining tasks. A major challenge lies in modeling the intrinsic stochasticity of temporal dynamics, as real-world sequences often exhibit random fluctuations and localized variations. While diffusion models have achieved remarkable success, their generation process is computationally inefficient, often requiring hundreds to thousands of expensive function evaluations per sample. Flow matching has emerged as a more efficient paradigm, yet its conventional ordinary differential equation (ODE)-based formulation fails to explicitly capture stochasticity, thereby limiting the fidelity of generated sequences. By contrast, stochastic differential equation (SDE) are naturally suited for modeling randomness and uncertainty. Motivated by these insights, we propose TimeFlow, a novel SDE-based flow matching framework that integrates a encoder-only architecture. Specifically, we design a component-wise decomposed velocity field to capture the multi-faceted structure of time series and augment the vanilla flow-matching optimization with an additional stochastic term to enhance representational expressiveness. TimeFlow is flexible and general, supporting both unconditional and conditional generation tasks within a unified framework. Extensive experiments across diverse datasets demonstrate that our model consistently outperforms strong baselines in generation quality, diversity, and efficiency.

</details>


### [155] [Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation](https://arxiv.org/abs/2511.07973)
*Yue Wang,Yuyang Xu,Renjun Hu,Fanqi Shen,Hanyun Jiang,Jun Wang,Jintai Chen,Danny Z. Chen,Jian Wu,Haochao Ying*

Main category: cs.AI

TL;DR: VARS通过图结构统一建模异构ECG信号，结合去噪和对比学习，显著提升诊断敏感性和风险信号识别能力。


<details>
  <summary>Details</summary>
Motivation: 解决ECG信号诊断中因多样性配置和样本不平衡导致的缺乏通用性和风险信号检测不足的问题。

Method: 采用基于图的表示方法，将ECG信号转换为通用的图结构，结合去噪重建和对比学习以保留原始信息并突出病征模式。

Result: VARS在三个不同的ECG数据集上表现优于现有最先进模型，且在风险信号识别上有显著改进。

Conclusion: VARS是一种创新的心脏诊断方法，通过图结构统一建模异构ECG信号，显著提升了诊断敏感性和风险信号识别能力，有望成为全面心脏健康评估的宝贵工具。

Abstract: Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.

</details>


### [156] [Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition](https://arxiv.org/abs/2511.07974)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 提出了一种细粒度的反事实解释框架，通过量化相似性和权重成分贡献生成可解释的反事实，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于归因的解释技术在细粒度任务中缺乏足够的细节，特别是在模型错误分类的情况下，解释可能不够详细。为了解决这一限制，提出了一种细粒度的反事实解释框架。

Method: 通过量化正确分类和错误分类样本之间感兴趣区域的相似性和权重成分贡献，以非生成方式生成可解释的反事实。此外，引入基于Shapley值贡献的显著性分区模块，隔离具有区域特定相关性的特征。

Result: 广泛的实验表明，该方法在捕捉更精细、直观上有意义的区域方面优于现有的细粒度方法。

Conclusion: 该论文提出的细粒度反事实解释框架在捕捉更精细、直观上有意义的区域方面表现出色，超越了现有的细粒度方法。

Abstract: Attribution-based explanation techniques capture key patterns to enhance visual interpretability; however, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods.

</details>


### [157] [Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://arxiv.org/abs/2511.07979)
*Wenhan Yu,Xinbo Lin,Lanxin Ni,Jinhua Cheng,Lei Sha*

Main category: cs.AI

TL;DR: MSLR是首个基于IRAC框架的中文多步法律推理数据集，通过Human-LLM协作标注提升推理质量，实验显示自主生成的Chain-of-Thought提示优于人工设计。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准常将事实回忆与推理混淆，且忽略推理质量，因此需要更真实、结构化的法律推理数据集。

Method: 采用IRAC框架（Issue, Rule, Application, Conclusion）和Human-LLM协作标注流程，设计多步法律推理数据集MSLR。

Result: 评估显示LLMs在MSLR上表现中等，但自主生成的Self-Initiated Chain-of-Thought提示能显著提升推理质量和一致性。

Conclusion: MSLR数据集通过IRAC框架和Human-LLM协作标注方法，显著提升了法律推理任务的复杂性和真实性，为LLM在法律领域的应用提供了新的基准和资源。

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at https://github.com/yuwenhan07/MSLR-Bench and https://law.sjtu.edu.cn/flszyjzx/index.html.

</details>


### [158] [Capturing Complex Spatial-Temporal Dependencies in Traffic Forecasting: A Self-Attention Approach](https://arxiv.org/abs/2511.07980)
*Zheng Chenghong,Zongyin Deng,Liu Cheng,Xiong Simin,Di Deshi,Li Guanyao*

Main category: cs.AI

TL;DR: ST-SAM通过自注意力机制联合学习时空依赖，显著提升了交通流量预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效联合捕捉交通流量预测中的时空依赖性，导致预测效果受限。

Method: 提出ST-SAM模型，包括区域嵌入层和基于自注意力机制的时空依赖学习模块，联合学习时空依赖。

Result: 在两个真实数据集上的实验显示，ST-SAM在RMSE、MAPE和训练时间上分别平均提升15%、17%和32倍。

Conclusion: ST-SAM模型通过自注意力机制有效捕捉了交通流量的时空依赖性，显著提升了预测准确性和效率。

Abstract: We study the problem of traffic forecasting, aiming to predict the inflow and outflow of a region in the subsequent time slot. The problem is complex due to the intricate spatial and temporal interdependence among regions. Prior works study the spatial and temporal dependency in a decouple manner, failing to capture their joint effect. In this work, we propose ST-SAM, a novel and efficient Spatial-Temporal Self-Attention Model for traffic forecasting. ST-SAM uses a region embedding layer to learn time-specific embedding from traffic data for regions. Then, it employs a spatial-temporal dependency learning module based on self-attention mechanism to capture the joint spatial-temporal dependency for both nearby and faraway regions. ST-SAM entirely relies on self-attention to capture both local and global spatial-temporal correlations, which make it effective and efficient. Extensive experiments on two real world datasets show that ST-SAM is substantially more accurate and efficient than the state-of-the-art approaches (with an average improvement of up to 15% on RMSE, 17% on MAPE, and 32 times on training time in our experiments).

</details>


### [159] [The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends](https://arxiv.org/abs/2511.07988)
*Nico Policzer,Cameron Braunstein,Mariya Toneva*

Main category: cs.AI

TL;DR: 研究将脑调谐扩展至多模态领域，通过调整模型以更好地预测STS活动，提升了社交认知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索多模态脑调谐是否能够提升社交认知能力，特别是针对STS这一社交处理关键区域。

Method: 通过多模态音频-视频模型对STS进行脑调谐，以增强社交认知。

Result: 研究发现STS及相邻ROI的脑对齐显著增强，同时在训练数据相关的社交认知任务（情景剧中的讽刺检测）上表现提升。

Conclusion: 该研究将脑调谐方法扩展到多模态领域，展示了在调整到相关功能区域后对下游任务的改进。

Abstract: Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.

</details>


### [160] [VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation](https://arxiv.org/abs/2511.07991)
*Hyojun Choi,Seokju Hwang,Kyong-Ho Lee*

Main category: cs.AI

TL;DR: 研究提出利用LLMs自动生成验证本体语义陷阱的CQ，新方法在精度和召回率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CQ生成方法难以可靠检测语义陷阱，如'Misusing allValuesFrom'，因此需要一种新方法来验证这些语义差异。

Method: 通过LLMs生成类与属性的自然语言定义，并引入缺失或误用的公理，随后微调LLaMA-3.1-8B-Instruct以生成验证这些语义差异的CQ。

Result: 微调后的模型在生成用于陷阱验证的CQ时，精度比GPT-4.1高26%，召回率高28.2%。

Conclusion: 本研究提出了一种自动生成用于验证语义陷阱的CQ的新方法，显著减少了人工工作量，同时提高了本体与专家知识之间的语义对齐。

Abstract: Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as "Misusing allValuesFrom". Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.

</details>


### [161] [Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation](https://arxiv.org/abs/2511.07994)
*Han Yu,Xiaojuan Zhao,Aiping Li,Kai Chen,Ziniu Liu,Zhichao Peng*

Main category: cs.AI

TL;DR: PN-GNN通过路径-邻居嵌入增强GNN的逻辑表达能力，理论和实验验证其优于现有方法且不影响泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注简单单关系图的GNN表达能力，对GNN在KG中表达逻辑规则的能力讨论不足，如何增强GNN的逻辑表达能力是关键问题。

Method: 提出了Path-Neighbor enhanced GNN (PN-GNN)，通过聚合推理路径上的节点-邻居嵌入来增强GNN的逻辑表达能力。首先分析了现有GNN方法的逻辑表达能力不足，然后理论证明了PN-GNN的表达能力严格优于C-GNN，且其$(k+1)$跳的逻辑表达能力严格优于$k$跳。

Result: 在六个合成数据集和两个真实数据集上评估了PN-GNN的逻辑表达能力，理论和实验均证明其在不影响泛化的情况下增强了逻辑规则表达能力。

Conclusion: PN-GNN通过聚合推理路径上的节点-邻居嵌入，增强了GNN的逻辑表达能力，且在KG推理任务中表现出色，不影响泛化能力。

Abstract: Graph neural networks (GNNs) can effectively model structural information of graphs, making them widely used in knowledge graph (KG) reasoning. However, existing studies on the expressive power of GNNs mainly focuses on simple single-relation graphs, and there is still insufficient discussion on the power of GNN to express logical rules in KGs. How to enhance the logical expressive power of GNNs is still a key issue. Motivated by this, we propose Path-Neighbor enhanced GNN (PN-GNN), a method to enhance the logical expressive power of GNN by aggregating node-neighbor embeddings on the reasoning path. First, we analyze the logical expressive power of existing GNN-based methods and point out the shortcomings of the expressive power of these methods. Then, we theoretically investigate the logical expressive power of PN-GNN, showing that it not only has strictly stronger expressive power than C-GNN but also that its $(k+1)$-hop logical expressiveness is strictly superior to that of $k$-hop. Finally, we evaluate the logical expressive power of PN-GNN on six synthetic datasets and two real-world datasets. Both theoretical analysis and extensive experiments confirm that PN-GNN enhances the expressive power of logical rules without compromising generalization, as evidenced by its competitive performance in KG reasoning tasks.

</details>


### [162] [Multivariate Time series Anomaly Detection:A Framework of Hidden Markov Models](https://arxiv.org/abs/2511.07995)
*Jinbo Li,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: 本研究提出了一种通过转换技术和HMM进行多元时间序列异常检测的方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对多元时间序列异常检测的复杂性，研究旨在通过转换技术和统计方法提高检测效率和准确性。

Method: 采用Fuzzy C-Means聚类和模糊积分技术将多元时间序列转换为单变量时间序列，随后利用隐马尔可夫模型（HMM）进行异常检测。

Result: 实验研究表明，所提出的方法在多元时间序列异常检测中表现良好，并通过比较分析验证了不同转换方法的优劣。

Conclusion: 研究通过比较多种转换方法，构建了基于HMM的多元时间序列异常检测器，并通过实验验证了其有效性。

Abstract: In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.

</details>


### [163] [Combining LLM Semantic Reasoning with GNN Structural Modeling for Multi-view Multi-Label Feature Selection](https://arxiv.org/abs/2511.08008)
*Zhiqi Chen,Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.AI

TL;DR: 本文提出了一种结合LLM和GNN的多视图多标签特征选择方法，通过语义和统计信息联合建模，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MVMLFS方法主要关注数据统计信息，忽视语义信息，而本文旨在联合利用这两种信息提升性能。

Method: 提出了一种结合LLM语义评估、语义感知异构图设计和轻量级GAT的三步方法。

Result: 实验结果表明，该方法在多个基准数据集上优于现有基线，且在小规模数据集上仍有效。

Conclusion: 该方法通过结合LLM语义推理和GNN结构建模，在多视图多标签特征选择任务中展现出优越性，具有鲁棒性、灵活性和泛化能力。

Abstract: Multi-view multi-label feature selection aims to identify informative features from heterogeneous views, where each sample is associated with multiple interdependent labels. This problem is particularly important in machine learning involving high-dimensional, multimodal data such as social media, bioinformatics or recommendation systems. Existing Multi-View Multi-Label Feature Selection (MVMLFS) methods mainly focus on analyzing statistical information of data, but seldom consider semantic information. In this paper, we aim to use these two types of information jointly and propose a method that combines Large Language Models (LLMs) semantic reasoning with Graph Neural Networks (GNNs) structural modeling for MVMLFS. Specifically, the method consists of three main components. (1) LLM is first used as an evaluation agent to assess the latent semantic relevance among feature, view, and label descriptions. (2) A semantic-aware heterogeneous graph with two levels is designed to represent relations among features, views and labels: one is a semantic graph representing semantic relations, and the other is a statistical graph. (3) A lightweight Graph Attention Network (GAT) is applied to learn node embedding in the heterogeneous graph as feature saliency scores for ranking and selection. Experimental results on multiple benchmark datasets demonstrate the superiority of our method over state-of-the-art baselines, and it is still effective when applied to small-scale datasets, showcasing its robustness, flexibility, and generalization ability.

</details>


### [164] [Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2511.08022)
*Zhishen Sun,Guang Dai,Ivor Tsang,Haishan Ye*

Main category: cs.AI

TL;DR: 通过扰动框架评估LLMs的数学推理能力，发现其对数值信息扰动敏感，性能显著下降，且可能依赖记忆模板而非逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正具备数学理解能力，这一问题仍存在争议。

Method: 提出了一个新的扰动框架，通过注入语义无关的扰动句子并逐步增加扰动强度来评估LLMs的推理能力，同时采用核心提问指令缺失的方法进一步分析LLMs的问题解决机制。

Result: 实验结果显示，LLMs在面对无数字的扰动句子时表现稳定，但存在鲁棒性边界；面对含数字的扰动句子时性能下降更显著，部分模型性能下降超过50%。核心提问指令缺失时，模型仍能保持20%-40%的准确率。

Conclusion: 当前大型语言模型在数学推理能力上存在明显缺陷和局限性，尤其在面对数值信息扰动时表现更为敏感和易错。这些发现对LLMs的进一步发展具有重要意义。

Abstract: LLMs have made significant progress in the field of mathematical reasoning, but whether they have true the mathematical understanding ability is still controversial. To explore this issue, we propose a new perturbation framework to evaluate LLMs' reasoning ability in complex environments by injecting additional semantically irrelevant perturbation sentences and gradually increasing the perturbation intensity. At the same time, we use an additional perturbation method: core questioning instruction missing, to further analyze the LLMs' problem-solving mechanism. The experimental results show that LLMs perform stably when facing perturbation sentences without numbers, but there is also a robustness boundary. As the perturbation intensity increases, the performance exhibits varying degrees of decline; when facing perturbation sentences with numbers, the performance decreases more significantly, most open source models with smaller parameters decrease by nearly or even more than 10%, and further increasing with the enhancement of perturbation intensity, with the maximum decrease reaching 51.55%. Even the most advanced commercial LLMs have seen a 3%-10% performance drop. By analyzing the reasoning process of LLMs in detail, We find that models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information. The higher the perturbation intensity, the more obvious these defects are. At the same time, in the absence of core questioning instruction, models can still maintain an accuracy of 20%-40%, indicating that LLMs may rely on memory templates or pattern matching to complete the task, rather than logical reasoning. In general, our work reveals the shortcomings and limitations of current LLMs in their reasoning capabilities, which is of great significance for the further development of LLMs.

</details>


### [165] [Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning](https://arxiv.org/abs/2511.08024)
*Tianwen Lyu,Xiang Zhuang,Keyan Ding,Xinzhe Cao,Lei Liang,Wei Zhao,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一个知识增强的长链推理框架，整合大型语言模型与知识图，通过监督微调和强化学习提升生物分子推理的可靠性和一致性，并在新基准PrimeKGQA上验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生物分子问题应用中的逻辑不一致性和领域知识缺乏问题，以及现有方法在捕获长机制依赖性和偏离生物事实方面的不足。

Method: 提出了一个知识增强的长链推理框架，整合了大型语言模型与基于知识图的多跳推理链，通过引导多跳遍历和剪枝构建机制链，并融入监督微调和强化学习以提升事实基础和推理一致性。

Result: 在PrimeKGQA和现有数据集上的实验结果表明，随着推理深度的增加，该方法在多跳任务上表现出明显优势，实现了最先进的性能。

Conclusion: 结合结构化知识与先进推理策略可有效提升生物分子推理的可靠性和可解释性。

Abstract: Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models(LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph-based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.

</details>


### [166] [Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations](https://arxiv.org/abs/2511.08042)
*JV Roig*

Main category: cs.AI

TL;DR: KAMI v0.1是一种企业级基准测试，解决传统LLM基准测试的数据污染和代理能力评估问题，发现新一代模型在企业任务中未必表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统LLM基准测试存在训练数据污染问题，且无法评估代理能力（如多步骤工具使用和不确定性下的决策）。

Method: 通过处理170,000个LLM测试项，覆盖35种模型配置和55亿个令牌，开发了KAMI v0.1基准测试。

Result: 研究发现，传统基准测试排名无法准确预测实际代理性能，新一代模型（如Llama 4或Qwen 3）在企业相关任务中未必优于旧版本。

Conclusion: 企业采用代理式AI系统需要可靠的评估方法，以反映实际部署场景。KAMI v0.1作为一种企业级基准测试，解决了传统基准测试的数据污染问题，并有效评估代理能力。

Abstract: Enterprise adoption of agentic AI systems requires reliable evaluation methods that reflect real-world deployment scenarios. Traditional LLM benchmarks suffer from training data contamination and fail to measure agentic capabilities such as multi-step tool use and decision-making under uncertainty. We present the Kamiwaza Agentic Merit Index (KAMI) v0.1, an enterprise-focused benchmark that addresses both contamination resistance and agentic evaluation. Through 170,000 LLM test items processing over 5.5 billion tokens across 35 model configurations, we demonstrate that traditional benchmark rankings poorly predict practical agentic performance. Notably, newer generation models like Llama 4 or Qwen 3 do not always outperform their older generation variants on enterprise-relevant tasks, contradicting traditional benchmark trends. We also present insights on cost-performance tradeoffs, model-specific behavioral patterns, and the impact of reasoning capabilities on token efficiency -- findings critical for enterprises making deployment decisions.

</details>


### [167] [MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement](https://arxiv.org/abs/2511.08055)
*Zhishen Sun,Guang Dai,Haishan Ye*

Main category: cs.AI

TL;DR: MSCR方法通过多源候选替换攻击揭示LLMs在数学推理中的鲁棒性缺陷和效率瓶颈，轻微扰动即可显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务如数学推理中表现接近人类，但其在微小输入扰动下的鲁棒性缺乏系统研究，现有方法存在可扩展性有限、语义保留弱和成本高等问题。

Method: 提出MSCR方法，基于多源候选替换（结合余弦相似度、WordNet词典和掩码语言模型的上下文预测）生成语义相似的候选词进行替换攻击。

Result: 实验显示，即使仅替换一个词的轻微扰动也能显著降低模型准确率（GSM8K最高下降49.89%，MATH500最高下降35.40%），同时保持扰动问题的高语义一致性。扰动还导致响应长度增加，产生冗余推理路径和更高计算资源消耗。

Conclusion: 当前大型语言模型在数学推理任务中存在鲁棒性不足和效率瓶颈的问题。

Abstract: LLMs demonstrate performance comparable to human abilities in complex tasks such as mathematical reasoning, but their robustness in mathematical reasoning under minor input perturbations still lacks systematic investigation. Existing methods generally suffer from limited scalability, weak semantic preservation, and high costs. Therefore, we propose MSCR, an automated adversarial attack method based on multi-source candidate replacement. By combining three information sources including cosine similarity in the embedding space of LLMs, the WordNet dictionary, and contextual predictions from a masked language model, we generate for each word in the input question a set of semantically similar candidates, which are then filtered and substituted one by one to carry out the attack. We conduct large-scale experiments on LLMs using the GSM8K and MATH500 benchmarks. The results show that even a slight perturbation involving only a single word can significantly reduce the accuracy of all models, with the maximum drop reaching 49.89% on GSM8K and 35.40% on MATH500, while preserving the high semantic consistency of the perturbed questions. Further analysis reveals that perturbations not only lead to incorrect outputs but also substantially increase the average response length, which results in more redundant reasoning paths and higher computational resource consumption. These findings highlight the robustness deficiencies and efficiency bottlenecks of current LLMs in mathematical reasoning tasks.

</details>


### [168] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出信息容量作为大语言模型效率的统一度量标准，基于文本压缩性能与计算复杂度的关系，能够公平比较不同模型系列的效率，并准确预测性能，同时考虑了分词器效率的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展和广泛应用导致计算资源需求激增，而测试时扩展的普遍采用进一步加剧了模型能力与资源消耗之间的矛盾，凸显了推理效率的重要性。然而，目前缺乏一个统一的度量标准来准确反映不同模型规模和架构下LLM的效率。

Method: 基于文本压缩性能与计算复杂度的关系，引入信息容量作为模型效率的度量标准，并在主流开源模型上进行实证评估。

Result: 实证评估表明，同一系列中不同大小的模型展现出一致的信息容量。信息容量能够公平比较不同模型系列的效率，并准确预测同一模型系列内的性能表现。此外，信息容量还考虑了分词器效率的影响，这在LLM评估中常被忽略。

Conclusion: 信息容量作为一种统一的度量标准，能够公平比较不同模型系列的效率，并准确预测同一模型系列内的性能表现。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [169] [Clustering-based Anomaly Detection in Multivariate Time Series Data](https://arxiv.org/abs/2511.08072)
*Jinbo Li,Hesam Izakian,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: A clustering-based method using fuzzy clustering and Particle Swarm Optimization effectively detects anomalies in multivariate time series across various domains.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of anomaly detection in multivariate time series by leveraging temporal and variable relationships, crucial for applications in science and engineering.

Method: Uses sliding window to generate multivariate subsequences, applies extended fuzzy clustering for structure revelation, and employs a reconstruction criterion with Particle Swarm Optimization for anomaly detection.

Result: Experimental results on synthetic and real-world datasets confirm the method's ability to detect anomalies in multivariate time series, including amplitude and shape patterns.

Conclusion: The proposed clustering-based approach effectively detects anomalies in multivariate time series by considering both amplitude and shape patterns, demonstrating applicability across diverse domains like healthcare, finance, and weather analysis.

Abstract: Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix. We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering, the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.

</details>


### [170] [Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency](https://arxiv.org/abs/2511.08082)
*Stella C. Dong*

Main category: cs.AI

TL;DR: 本文提出了一个评估LLM在再保险中可靠性的框架，通过五支柱架构和RAIRAB基准，显著提高了模型的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在再保险中的可靠性，以降低信息摩擦并提高风险转移和资本分配的效率。

Method: 提出一个五支柱架构（治理、数据谱系、保证、韧性和监管一致性），并实施再保险AI可靠性和保证基准（RAIRAB）来评估LLM是否符合审慎标准。

Result: 在六个任务家族中，检索基础配置实现了更高的基础准确性（0.90），减少了约40%的幻觉和解释漂移，透明度几乎翻倍。

Conclusion: 现有的审慎原则已能适应可靠的人工智能应用，前提是治理明确、数据可追溯且保证可验证。

Abstract: This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accountability. Across six task families, retrieval-grounded configurations achieved higher grounding accuracy (0.90), reduced hallucination and interpretive drift by roughly 40%, and nearly doubled transparency. These mechanisms lower informational frictions in risk transfer and capital allocation, showing that existing prudential doctrines already accommodate reliable AI when governance is explicit, data are traceable, and assurance is verifiable.

</details>


### [171] [Gateways to Tractability for Satisfiability in Pearl's Causal Hierarchy](https://arxiv.org/abs/2511.08091)
*Robert Ganian,Marlene Gründel,Simon Wietheger*

Main category: cs.AI

TL;DR: Identifies tractable algorithms for Pearl's Causal Hierarchy satisfiability using structural parameters, with matching hardness results.


<details>
  <summary>Details</summary>
Motivation: To address the computational intractability of the satisfiability problem for Pearl's Causal Hierarchy formulas in classical settings.

Method: The approach involves structural characterizations of well-formed causal models, diverging from traditional dynamic programming for treewidth-based algorithms.

Result: The research provides the first tractability results for key probabilistic and counterfactual fragments within Pearl's Causal Hierarchy.

Conclusion: The study identifies fixed-parameter and XP-algorithms for the satisfiability problem in Pearl's Causal Hierarchy, using parameters like primal treewidth and variable count, and establishes matching hardness results to define tractability limits.

Abstract: Pearl's Causal Hierarchy (PCH) is a central framework for reasoning about probabilistic, interventional, and counterfactual statements, yet the satisfiability problem for PCH formulas is computationally intractable in almost all classical settings. We revisit this challenge through the lens of parameterized complexity and identify the first gateways to tractability. Our results include fixed-parameter and XP-algorithms for satisfiability in key probabilistic and counterfactual fragments, using parameters such as primal treewidth and the number of variables, together with matching hardness results that map the limits of tractability. Technically, we depart from the dynamic programming paradigm typically employed for treewidth-based algorithms and instead exploit structural characterizations of well-formed causal models, providing a new algorithmic toolkit for causal reasoning.

</details>


### [172] [Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification](https://arxiv.org/abs/2511.08108)
*Georg Rottenwalter,Marcel Tilly,Victor Owolabi*

Main category: cs.AI

TL;DR: 研究使用XAI技术减少特征对注塑件质量分类的影响，结果表明特征减少能提升泛化能力和推理速度，同时保持高分类性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在工业质量控制中的复杂性限制了其实用性，且许多工业机器缺乏全面的传感器技术，导致数据获取不完整和具有挑战性。可解释人工智能（XAI）通过提供模型决策的洞察和识别分类中最相关的特征，为解决这些问题提供了方案。

Method: 应用SHAP、Grad-CAM和LIME分析长短期记忆模型中的特征重要性，并将原始19个输入特征减少到9个和6个，评估模型准确性、推理速度和可解释性之间的权衡。

Result: 减少特征可以改善泛化能力，同时保持高分类性能，推理速度略有提升。

Conclusion: 该研究表明，通过XAI技术减少特征可以在保持高分类性能的同时提高模型的泛化能力，并略微提升推理速度。这种方法增强了AI驱动质量控制的可行性，特别是在传感器能力有限的工业环境中，为制造业中更高效和可解释的机器学习应用铺平了道路。

Abstract: Machine learning is an essential tool for optimizing industrial quality control processes. However, the complexity of machine learning models often limits their practical applicability due to a lack of interpretability. Additionally, many industrial machines lack comprehensive sensor technology, making data acquisition incomplete and challenging. Explainable Artificial Intelligence offers a solution by providing insights into model decision-making and identifying the most relevant features for classification. In this paper, we investigate the impact of feature reduction using XAI techniques on the quality classification of injection-molded parts. We apply SHAP, Grad-CAM, and LIME to analyze feature importance in a Long Short-Term Memory model trained on real production data. By reducing the original 19 input features to 9 and 6, we evaluate the trade-off between model accuracy, inference speed, and interpretability. Our results show that reducing features can improve generalization while maintaining high classification performance, with an small increase in inference speed. This approach enhances the feasibility of AI-driven quality control, particularly for industrial settings with limited sensor capabilities, and paves the way for more efficient and interpretable machine learning applications in manufacturing.

</details>


### [173] [Advancements in synthetic data extraction for industrial injection molding](https://arxiv.org/abs/2511.08117)
*Georg Rottenwalter,Marcel Tilly,Christian Bielenberg,Katharina Obermeier*

Main category: cs.AI

TL;DR: 研究探讨了在注塑成型过程中利用合成数据增强LSTM模型训练的有效性，结果显示该方法能优化工业流程并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决数据获取耗时且成本高的问题，同时提升机器学习模型在不同场景下的鲁棒性。

Method: 通过模拟生产周期生成合成数据，并将其与真实数据以不同比例结合，利用现有的LSTM架构进行训练。

Result: 实验结果表明，合成数据的加入提升了模型处理多样化场景的能力。

Conclusion: 本文提出了一种通过合成数据增强机器学习模型的方法，特别是在注塑成型过程中，展示了其在减少人工、机器使用和材料浪费方面的潜在工业应用价值。

Abstract: Machine learning has significant potential for optimizing various industrial processes. However, data acquisition remains a major challenge as it is both time-consuming and costly. Synthetic data offers a promising solution to augment insufficient data sets and improve the robustness of machine learning models. In this paper, we investigate the feasibility of incorporating synthetic data into the training process of the injection molding process using an existing Long Short-Term Memory architecture. Our approach is to generate synthetic data by simulating production cycles and incorporating them into the training data set. Through iterative experimentation with different proportions of synthetic data, we attempt to find an optimal balance that maximizes the benefits of synthetic data while preserving the authenticity and relevance of real data. Our results suggest that the inclusion of synthetic data improves the model's ability to handle different scenarios, with potential practical industrial applications to reduce manual labor, machine use, and material waste. This approach provides a valuable alternative for situations where extensive data collection and maintenance has been impractical or costly and thus could contribute to more efficient manufacturing processes in the future.

</details>


### [174] [National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech - The SpeechCARE Solution](https://arxiv.org/abs/2511.08132)
*Maryam Zolnoori,Hossein Azadmaleki,Yasaman Haghbin,Ali Zolnour,Mohammad Javad Momeni Nezhad,Sina Rashidi,Mehdi Naserian,Elyas Esmaeili,Sepehr Karimi Arpanahi*

Main category: cs.AI

TL;DR: SpeechCARE是一种多模态语音处理流程，结合Transformer模型和动态融合架构，显著提升认知障碍检测性能，未来将部署至实际医疗场景。


<details>
  <summary>Details</summary>
Motivation: 现有语音处理流程在认知障碍早期检测中性能有限且泛化能力不足，SpeechCARE旨在通过多模态方法提升检测准确性和鲁棒性。

Method: SpeechCARE采用多模态语音处理流程，结合预训练的多语言声学和语言Transformer模型，通过动态融合架构整合声学、语言和人口统计输入，并引入SHAP可解释性模块和LLM推理。

Result: SpeechCARE在分类认知健康、MCI和AD个体时AUC达0.88，F1达0.72；MCI检测中AUC达0.90，F1达0.62，偏差分析显示除80岁以上群体外差异较小。

Conclusion: SpeechCARE展示了在多模态语音处理中应用预训练模型和动态融合架构的潜力，尤其在认知障碍早期检测方面表现优异，未来将在实际医疗场景中进一步验证。

Abstract: Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.

</details>


### [175] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent是一个统一的多代理系统，通过分层协作实现跨学科的科学推理，在多个科学竞赛中表现优于人类专家。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在特定科学任务上实现了专家级性能，但这些系统仍然是狭窄且手工制作的。SciAgent旨在实现跨学科和难度级别的通用科学推理能力。

Method: SciAgent采用分层过程组织问题解决：一个协调者代理解释问题的领域和复杂性，动态编排专门的工人系统，每个系统由交互推理子代理组成，用于符号演绎、概念建模、数值计算和验证。这些代理协作组装并优化针对每个任务的推理管道。

Result: 在数学和物理奥林匹克竞赛（IMO、IMC、IPhO、CPhO）中，SciAgent始终达到或超过人类金牌得主的表现。此外，该系统在国际化学奥林匹克竞赛（IChO）和Humanity's Last Exam（HLE）基准测试的部分问题上也表现出跨学科泛化能力。

Conclusion: SciAgent代表了一种迈向通用科学智能的具体步骤，能够在专家水平上进行跨学科的连贯推理。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [176] [oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention](https://arxiv.org/abs/2511.08168)
*Ryusuke Mizutani,Kazuaki Matano,Tsugumi Kadowaki,Haruki Tenya,Layris,nuigurumi,Koki Hashimoto,Yu Tanaka*

Main category: cs.AI

TL;DR: 该项目开发了开源图像生成模型'oboro:'，使用版权已清除数据训练，旨在解决日本动漫产业劳动力问题，并促进国内AI生态发展。


<details>
  <summary>Details</summary>
Motivation: 解决日本动漫产业劳动力短缺等挑战，开发一个完全在日本国内开发的图像生成模型。

Method: 从零开始开发了一个新的图像生成模型'oboro:'，仅使用已清除版权的图像进行训练，其架构设计能够在有限数据集下生成高质量图像。

Result: 成功开发并公开发布了'oboro:'模型，这是首个在日本完全开发的开源、商业导向图像生成AI。

Conclusion: 该项目成功开发了开源、商业导向的图像生成AI模型'oboro:'，并公开了模型权重和推理代码，旨在促进日本AI研究社区和国内AI开发生态系统的发展。

Abstract: This project was conducted as a 2nd-term adopted project of the "Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC)," a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan's anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, "oboro:." We have developed "oboro:," a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan's AI researcher and engineer community and promote the domestic AI development ecosystem.

</details>


### [177] [An Efficient Training Pipeline for Reasoning Graphical User Interface Agents](https://arxiv.org/abs/2511.08172)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.AI

TL;DR: 通过高效数据筛选和轻量级训练策略，紧凑模型在多模态基准测试中表现优异，证明了质量胜于数量的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模、嘈杂的合成数据集，需改进以提升任务效率和效果。

Method: 结合基于模型的数据筛选与参数高效微调的高效训练流程，包括监督微调、思维链增强微调和通过Group Relative Policy Optimization的强化学习。

Result: 在ScreenSpot、Multimodal-Mind2Web和AndroidControl等基准测试中，模型表现与更大基线相当或更优。

Conclusion: 通过原则性数据筛选和稳健的适应策略，紧凑的多模态推理代理可以达到甚至超越大规模训练的效果。

Abstract: Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic datasets.This work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought- augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.

</details>


### [178] [Towards Provably Unlearnable Examples via Bayes Error Optimisation](https://arxiv.org/abs/2511.08191)
*Ruihan Zhang,Jun Sun,Ee-Peng Lim,Peixin Zhang*

Main category: cs.AI

TL;DR: 提出一种通过最大化贝叶斯误差构造不可学习样本的新方法，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 针对现有不可学习样本方法缺乏理论保证且在混合干净数据时失效的问题。

Method: 采用基于优化的方法，通过投影梯度上升提供高效解决方案。

Result: 在多数据集和模型架构上的实验结果验证了方法的有效性。

Conclusion: 本研究提出了一种基于最大化贝叶斯误差的系统性方法，成功构造了即使在混合干净数据时仍保持有效性的不可学习样本。

Abstract: The recent success of machine learning models, especially large-scale classifiers and language models, relies heavily on training with massive data. These data are often collected from online sources. This raises serious concerns about the protection of user data, as individuals may not have given consent for their data to be used in training. To address this concern, recent studies introduce the concept of unlearnable examples, i.e., data instances that appear natural but are intentionally altered to prevent models from effectively learning from them. While existing methods demonstrate empirical effectiveness, they typically rely on heuristic trials and lack formal guarantees. Besides, when unlearnable examples are mixed with clean data, as is often the case in practice, their unlearnability disappears. In this work, we propose a novel approach to constructing unlearnable examples by systematically maximising the Bayes error, a measurement of irreducible classification error. We develop an optimisation-based approach and provide an efficient solution using projected gradient ascent. Our method provably increases the Bayes error and remains effective when the unlearning examples are mixed with clean samples. Experimental results across multiple datasets and model architectures are consistent with our theoretical analysis and show that our approach can restrict data learnability, effectively in practice.

</details>


### [179] [EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks](https://arxiv.org/abs/2511.08206)
*Xiao Yang,Xuejiao Zhao,Zhiqi Shen*

Main category: cs.AI

TL;DR: 提出EHRStruct基准和EHRMaster方法，系统评估LLM在结构化EHR任务中的性能，并实现最优表现。


<details>
  <summary>Details</summary>
Motivation: 结构化EHR数据在临床决策中至关重要，但缺乏标准化的评估框架和明确任务定义，难以系统评估和比较LLM性能。

Method: 引入EHRStruct基准，定义11项代表性任务，并从两个广泛使用的EHR数据集中提取2,200个评估样本。评估了20种先进LLM，并分析输入格式、少样本泛化和微调策略等关键因素。提出EHRMaster，一种代码增强方法。

Result: EHRStruct基准成功评估了20种LLM，发现结构化EHR任务对LLM的理解和推理能力要求较高。EHRMaster方法在多项任务中达到最优性能。

Conclusion: 针对结构化电子健康记录（EHR）数据的评估挑战，EHRStruct基准和EHRMaster方法的提出为系统评估和比较LLM性能提供了标准化框架，并在多项任务中展现出卓越性能。

Abstract: Structured Electronic Health Record (EHR) data stores patient information in relational tables and plays a central role in clinical decision-making. Recent advances have explored the use of large language models (LLMs) to process such data, showing promise across various clinical tasks.However, the absence of standardized evaluation frameworks and clearly defined tasks makes it difficult to systematically assess and compare LLM performance on structured EHR data.To address these evaluation challenges, we introduce EHRStruct, a benchmark specifically designed to evaluate LLMs on structured EHR tasks.EHRStruct defines 11 representative tasks spanning diverse clinical needs and includes 2,200 task-specific evaluation samples derived from two widely used EHR datasets.We use EHRStruct to evaluate 20 advanced and representative LLMs, covering both general and medical models.We further analyze key factors influencing model performance, including input formats, few-shot generalisation, and finetuning strategies, and compare results with 11 state-of-the-art LLM-based enhancement methods for structured data reasoning. Our results indicate that many structured EHR tasks place high demands on the understanding and reasoning capabilities of LLMs.In response, we propose EHRMaster, a code-augmented method that achieves state-of-the-art performance and offers practical

</details>


### [180] [MADD: Multi-Agent Drug Discovery Orchestra](https://arxiv.org/abs/2511.08217)
*Gleb V. Solovev,Alina B. Zhidkovskaya,Anastasia Orlova,Nina Gubina,Anastasia Vepreva,Rodion Golovinskii,Ilya Tonkii,Ivan Dubrovsky,Ivan Gurev,Dmitry Gilemkhanov,Denis Chistiakov,Timur A. Aliev,Ivan Poddiakov,Galina Zubkova,Ekaterina V. Skorb,Vladimir Vinogradov,Alexander Boukhanovsky,Nikolay Nikitin,Andrei Dmitrenko,Anna Kalyuzhnaya,Andrey Savchenko*

Main category: cs.AI

TL;DR: MADD是一个多智能体系统，通过自然语言查询定制药物发现流程，显著提升了效率和可访问性。


<details>
  <summary>Details</summary>
Motivation: 解决传统药物发现方法的高成本和低效率问题，同时提高AI工具的易用性。

Method: MADD采用四个协调的智能体处理新化合物生成和筛选中的关键子任务。

Result: MADD在七个药物发现案例中表现出色，并成功应用于五个生物靶标。

Conclusion: MADD展示了多智能体系统在药物发现中的潜力，特别是在提高效率和可访问性方面。

Abstract: Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.

</details>


### [181] [Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning](https://arxiv.org/abs/2511.08234)
*Zhihao Lin*

Main category: cs.AI

TL;DR: GAC是一种新型动作生成方法，简化了球形分布的计算，在连续控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高斯策略在连续控制中存在根本性不匹配问题，其无界支持需要临时压缩函数，这会扭曲有界动作空间的几何结构。vMF分布在球面上提供了理论替代方案，但其依赖贝塞尔函数和拒绝采样阻碍了实际应用。

Method: 提出了几何动作控制(GAC)，将动作生成分解为方向向量和可学习的集中参数，实现了确定性动作和均匀球形噪声之间的高效插值。

Result: GAC在六项MuJoCo基准测试中一致匹配或超越最先进方法，在Ant-v4上比SAC提高了37.6%，并在6项任务中的4项中取得最佳结果。

Conclusion: 研究表明，稳健且高效的连续控制不需要复杂的分布，而是需要对动作空间几何的尊重。GAC通过简化的计算和几何优势，在连续控制任务中表现出色。

Abstract: Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.

</details>


### [182] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 研究提出了一套新的AI代理性能评估框架，通过实验证明混合代理在多数指标中表现最优。


<details>
  <summary>Details</summary>
Motivation: 当前基于基础设施的度量标准无法全面评估AI代理的决策质量、自主性或商业价值，需要更全面的评估框架。

Method: 研究提出了11个与任务无关的性能指标，通过大规模模拟实验，比较了四种不同代理架构在五个领域的表现。

Result: 混合代理在大多数指标中表现最佳，平均目标完成率为88.8%，投资回报率最高。

Conclusion: 该研究提出了一套全面的、基于结果的AI代理性能评估框架，并通过实验验证了其有效性。混合代理在多数指标中表现最优，为AI代理的开发、部署和治理提供了标准化方法。

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [183] [Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning](https://arxiv.org/abs/2511.08246)
*Ziyu Ma,Chenhui Gou,Yiming Hu,Yong Wang,Xiangxiang Chu,Bohan Zhuang,Jianfei Cai*

Main category: cs.AI

TL;DR: STV框架通过敏感性分析优化任务向量插入位置和值，提升多模态模型的少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态模型在少样本学习中的上下文长度限制和高推理成本问题，改进现有任务向量方法的插入位置和值选择不足。

Method: 提出敏感性感知任务向量插入框架（STV），通过分析激活差异的结构模式确定插入位置，构建预聚类激活库，并应用强化学习选择最佳插入值。

Result: 在多种多模态模型（如Qwen-VL、Idefics-2）和任务（如VizWiz、OK-VQA）上验证了STV的有效性，性能优于现有方法。

Conclusion: STV框架通过敏感性感知的任务向量插入策略，在多模态模型和任务中展现出优于现有方法的性能和泛化能力。

Abstract: Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.

</details>


### [184] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出了 Multi-Agent GraphRAG，利用 Cypher 和 LPG 数据库作为 GraphRAG 推理引擎，填补研究空白并支持工业自动化。


<details>
  <summary>Details</summary>
Motivation: 现有 GraphRAG 方法主要依赖 RDF 知识图谱，而 Cypher 和 LPG 数据库在 GraphRAG 中的潜力尚未充分探索。

Method: 提出了一个模块化的 LLM 代理系统，用于文本到 Cypher 查询的生成和执行，结合了内容感知的迭代修正和反馈循环。

Result: 在 CypherBench 图和 IFC 数据衍生的属性图上验证了系统的性能，展示了其在工业数字自动化中的实际应用潜力。

Conclusion: Multi-Agent GraphRAG 展示了如何利用 Cypher 和 LPG 数据库作为 GraphRAG 管道的推理引擎，填补了现有研究的空白，并为工业数字化自动化提供了可扩展的解决方案。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


### [185] [DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation](https://arxiv.org/abs/2511.08283)
*Vishal Kumar,Shubhra Mishra,Rebecca Hao,Rizwaan Malik,David Broman,Dorottya Demszky*

Main category: cs.AI

TL;DR: DiagramIR是一种基于LaTeX TikZ中间表示的自动评估流程，用于高效评估LLM生成的几何图形，表现优于基线且成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育领域的应用多为纯文本，无法有效支持需可视化（如数学）的领域。现有方法虽能生成可编译为教育图形的代码，但缺乏可扩展的图形评估方法。

Method: DiagramIR是一种基于LaTeX TikZ代码中间表示（IRs）的自动评估流程。

Result: DiagramIR在评估几何图形时表现优于LLM-as-a-Judge基线，并与人类评分者更一致。此外，它使较小模型能以低成本达到与大模型相当的评估性能。

Conclusion: DiagramIR提供了一种自动且可扩展的几何图形评估方法，其表现优于LLM-as-a-Judge基线，并与人类评分者具有更高的一致性。这种方法还使得较小模型（如GPT-4.1-Mini）能以较低成本实现与大模型（如GPT-5）相当的性能，对教育技术的可访问性和可扩展性部署至关重要。

Abstract: Large Language Models (LLMs) are increasingly being adopted as tools for learning; however, most tools remain text-only, limiting their usefulness for domains where visualizations are essential, such as mathematics. Recent work shows that LLMs are capable of generating code that compiles to educational figures, but a major bottleneck remains: scalable evaluation of these diagrams. We address this by proposing DiagramIR: an automatic and scalable evaluation pipeline for geometric figures. Our method relies on intermediate representations (IRs) of LaTeX TikZ code. We compare our pipeline to other evaluation baselines such as LLM-as-a-Judge, showing that our approach has higher agreement with human raters. This evaluation approach also enables smaller models like GPT-4.1-Mini to perform comparably to larger models such as GPT-5 at a 10x lower inference cost, which is important for deploying accessible and scalable education technologies.

</details>


### [186] [JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms](https://arxiv.org/abs/2511.08343)
*Srihari R,Adarsha B,Mohammed Usman Hussain,Shweta Singh*

Main category: cs.AI

TL;DR: JobSphere是一款AI驱动的职业助手，通过RAG架构和多语言支持，显著提升了旁遮普农村地区用户的就业平台体验，成本降低89%。


<details>
  <summary>Details</summary>
Motivation: 解决政府就业网站的导航复杂性、语言选项不足和缺乏个性化支持等问题，提升用户体验。

Method: 采用检索增强生成（RAG）架构，支持多语言（英语、印地语和旁遮普语），并应用4-bit量化技术，使平台能在消费级GPU（如NVIDIA RTX 3050 4GB）上部署。

Result: 实现94%的事实准确性、1.8秒的中位响应时间，系统可用性评分78.5/100，比基线PGRKAM平台提升了50%。

Conclusion: JobSphere有效填补了旁遮普/印地语使用者在农村地区的可访问性差距，同时确保了用户对政府机构提供的可信就业内容的访问。

Abstract: Users of government employment websites commonly face engagement and accessibility challenges linked to navigational complexity, a dearth of language options, and a lack of personalized support. This paper introduces JobSphere, an AI-powered career assistant that is redefining the employment platform in Punjab called PGRKAM. JobSphere employs Retrieval-Augmented Generation (RAG) architecture, and it is multilingual, available in English, Hindi and Punjabi. JobSphere technique uses 4-bit quantization, allowing the platform to deploy on consumer-grade GPUs (i.e., NVIDIA RTX 3050 4GB), making the implementation 89% cheaper than that of cloud-based systems. Key innovations include voice-enabled interaction with the assistant, automated mock tests, resume parsing with skills recognition, and embed-based job recommendation that achieves a precision@10 score of 68%. An evaluation of JobSphere's implementation reveals 94% factual accuracy, a median response time of 1.8 seconds, and a System Usability Scale score of 78.5/100, a 50% improvement compared to the baseline PGRKAM platform context. In conclusion, JobSphere effectively fills significant accessibility gaps for Punjab/Hindi-speaking users in rural locations, while also affirming the users access to trusted job content provided by government agencies.

</details>


### [187] [AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis](https://arxiv.org/abs/2511.08363)
*Srihari R,Pallavi M,Tejaswini S,Vaishnavi R C*

Main category: cs.AI

TL;DR: 该研究开发了一个AI驱动的数据可视化平台，自动化从数据清洗到可视化生成的整个流程，显著减少人工干预并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动环境中耗时的手动数据分析挑战，自动化AI驱动的分析和可视化过程。

Method: 采用Python Flask后端与React前端结合，利用Firebase云存储进行数据处理和分析，包括自动数据清理、缺失值填充、异常值检测，以及通过四种不同算法智能选择特征和生成可视化。

Result: 平台在包含多达100,000行的数据集上实时执行初始分析，并能扩展以满足多用户同时请求。

Conclusion: 基于云的数据可视化应用显著减少了数据分析过程中的人工输入，同时保持了高质量、有影响力的视觉输出和用户体验。

Abstract: An AI-powered data visualization platform that automates the entire data analysis process, from uploading a dataset to generating an interactive visualization. Advanced machine learning algorithms are employed to clean and preprocess the data, analyse its features, and automatically select appropriate visualizations. The system establishes the process of automating AI-based analysis and visualization from the context of data-driven environments, and eliminates the challenge of time-consuming manual data analysis. The combination of a Python Flask backend to access the dataset, paired with a React frontend, provides a robust platform that automatically interacts with Firebase Cloud Storage for numerous data processing and data analysis solutions and real-time sources. Key contributions include automatic and intelligent data cleaning, with imputation for missing values, and detection of outliers, via analysis of the data set. AI solutions to intelligently select features, using four different algorithms, and intelligent title generation and visualization are determined by the attributes of the dataset. These contributions were evaluated using two separate datasets to assess the platform's performance. In the process evaluation, the initial analysis was performed in real-time on datasets as large as 100000 rows, while the cloud-based demand platform scales to meet requests from multiple users and processes them simultaneously. In conclusion, the cloud-based data visualization application allowed for a significant reduction of manual inputs to the data analysis process while maintaining a high quality, impactful visual outputs, and user experiences

</details>


### [188] [SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models](https://arxiv.org/abs/2511.08379)
*Giorgio Piras,Raffaele Mura,Fabio Brau,Luca Oneto,Fabio Roli,Battista Biggio*

Main category: cs.AI

TL;DR: 该论文提出了一种利用自组织映射（SOMs）提取多个拒绝方向的新方法，实验证明该方法能有效提升模型对有害提示的拒绝能力，并分析了其机理意义。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型中概念通常被编码为嵌入高维潜在空间的低维流形的新证据，研究者们提出了提取多个拒绝方向的方法，以提升模型对有害或非伦理提示的拒绝能力。

Method: 提出了一种利用自组织映射（SOMs）从有害提示表示中提取多个拒绝方向的新方法。该方法首先证明SOMs可以泛化先前工作的均值差技术，然后通过从每个神经元中减去无害表示的质心，得到一组表达拒绝概念的多个方向。

Result: 实验验证表明，从模型内部消融多个方向不仅优于单方向基线，还能有效抑制拒绝行为，超越了专门的越狱算法。

Conclusion: 该论文通过分析提出的方法在模型内部机制中的影响，探讨了多方向拒绝概念的机理意义。

Abstract: Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.

</details>


### [189] [FaithAct: Faithfulness Planning and Acting in MLLMs](https://arxiv.org/abs/2511.08409)
*Junxian Li,Xinyue Xu,Sai Ma,Sichao Li*

Main category: cs.AI

TL;DR: 通过FaithEval和FaithAct框架，本研究提升了大语言模型在多模态推理中的忠实性，减少幻觉并稳定推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在推理过程中产生的不可靠性（如与感知证据或最终结论不符的推理链）问题。

Method: 提出了FaithEval用于量化忠实性，并开发了FaithAct框架，通过在每一步推理中强制证据基础来提升忠实性。

Result: 实验表明，FaithAct在多基准测试中将感知忠实性提升了26%，且未影响任务准确性。

Conclusion: 本研究通过FaithEval和FaithAct框架，为多模态推理中的忠实性评估与执行建立了统一框架，显著提升了感知忠实性且不降低任务准确性。

Abstract: Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.

</details>


### [190] [Dataset Safety in Autonomous Driving: Requirements, Risks, and Assurance](https://arxiv.org/abs/2511.08439)
*Alireza Abbaspour,Tejaskumar Balgonda Patil,B Ravi Kiran,Russel Mohr,Senthil Yogamani*

Main category: cs.AI

TL;DR: 本文提出了一个结构化框架，用于开发符合安全标准的自动驾驶AI系统数据集，涵盖数据生命周期和安全分析，旨在提升系统安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 数据集完整性对AI系统的安全性和可靠性至关重要，尤其是在自动驾驶领域，因此需要开发符合ISO/PAS 8800指南的安全数据集。

Method: 本文介绍了一个结合AI数据飞轮和数据集生命周期的结构化框架，涵盖数据收集、标注、整理和维护，并融入了严格的安全分析。

Result: 提出了一个框架，包括安全分析、数据集安全要求的制定过程，以及验证和验证策略，以确保符合安全标准。

Conclusion: 本文旨在通过提出的结构化框架，推进自动驾驶应用中稳健且安全保证的AI系统发展。

Abstract: Dataset integrity is fundamental to the safety and reliability of AI systems, especially in autonomous driving. This paper presents a structured framework for developing safe datasets aligned with ISO/PAS 8800 guidelines. Using AI-based perception systems as the primary use case, it introduces the AI Data Flywheel and the dataset lifecycle, covering data collection, annotation, curation, and maintenance. The framework incorporates rigorous safety analyses to identify hazards and mitigate risks caused by dataset insufficiencies. It also defines processes for establishing dataset safety requirements and proposes verification and validation strategies to ensure compliance with safety standards. In addition to outlining best practices, the paper reviews recent research and emerging trends in dataset safety and autonomous vehicle development, providing insights into current challenges and future directions. By integrating these perspectives, the paper aims to advance robust, safety-assured AI systems for autonomous driving applications.

</details>


### [191] [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](https://arxiv.org/abs/2511.08484)
*Huzaifa Arif,Keerthiram Murugesan,Ching-Yun Ko,Pin-Yu Chen,Payel Das,Alex Gittens*

Main category: cs.AI

TL;DR: A lightweight method patches LLMs with minimal parameter addition, achieving significant safety improvements without full-model updates.


<details>
  <summary>Details</summary>
Motivation: Major LLM releases are costly and infrequent, leaving safety gaps in existing models. This method addresses the need for rapid, tailored safety updates.

Method: The approach involves prepending a compact, learnable prefix to an existing model, adding only 0.003% additional parameters.

Result: Policy patches achieve safety improvements comparable to next-generation models in toxicity mitigation, bias reduction, and harmfulness refusal, while preserving fluency.

Conclusion: LLMs can be effectively patched like software, providing a scalable and efficient method for safety updates between major releases.

Abstract: We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This "patch" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be "patched" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.

</details>


### [192] [A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models](https://arxiv.org/abs/2511.08548)
*Shubhra Mishra,Yuka Machino,Gabriel Poesia,Albert Jiang,Joy Hsu,Adrian Weller,Challenger Mishra,David Broman,Joshua B. Tenenbaum,Mateja Jamnik,Cedegao E. Zhang,Katherine M. Collins*

Main category: cs.AI

TL;DR: 研究发现LLMs在数学问题趣味性判断上与人类有部分一致，但在解释人类趣味性理由和分布捕捉上表现不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统（如LLMs）在数学研究和教育中与人类的互动时，其趣味性判断与人类是否一致的重要性。

Method: 通过两项实证研究，比较了人类和LLMs对数学问题趣味性和难度的评估，研究对象包括众包平台参与者和国际数学奥林匹克竞赛选手。

Result: LLMs在趣味性判断上大体与人类一致，但未能完全捕捉人类判断的分布，且与人类选择的有趣理由相关性较弱。

Conclusion: 当前的大型语言模型（LLMs）在捕捉人类对数学问题趣味性判断方面既有潜力也有局限，尤其在解释人类为何认为某些数学问题有趣时表现较弱。

Abstract: The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.

</details>


### [193] [Hyperdimensional Decoding of Spiking Neural Networks](https://arxiv.org/abs/2511.08558)
*Cedrick Kinavuidi,Luca Peres,Oliver Rhodes*

Main category: cs.AI

TL;DR: 提出了一种结合SNN和HDC的新型解码方法，显著提升了精度、噪声鲁棒性、延迟和能耗表现，成为现有解码方法的有力替代。


<details>
  <summary>Details</summary>
Motivation: 目标是创建一种具有高精度、高噪声鲁棒性、低延迟和低能耗的解码方法。

Method: 结合脉冲神经网络（SNN）和超维计算（HDC），开发了一种新型解码方法。

Result: 在多个测试案例中，SNN-HDC模型的分类精度更高、延迟更低、能耗更少。例如，DvsGesture数据集的能耗降低了1.24倍至3.67倍，SL-Animals-DVS数据集的能耗降低了1.38倍至2.27倍。此外，该方法能有效识别未经训练的未知类别，如在DvsGesture数据集中对未训练类别的样本识别率达到100%。

Conclusion: 该解码方法结合了SNN和HDC，展现了高精度、高噪声鲁棒性、低延迟和低能耗的优势，成为速率和延迟解码的有力替代方案。

Abstract: This work presents a novel spiking neural network (SNN) decoding method, combining SNNs with Hyperdimensional computing (HDC). The goal is to create a decoding method with high accuracy, high noise robustness, low latency and low energy usage. Compared to analogous architectures decoded with existing approaches, the presented SNN-HDC model attains generally better classification accuracy, lower classification latency and lower estimated energy consumption on multiple test cases from literature. The SNN-HDC achieved estimated energy consumption reductions ranging from 1.24x to 3.67x on the DvsGesture dataset and from 1.38x to 2.27x on the SL-Animals-DVS dataset. The presented decoding method can also efficiently identify unknown classes it has not been trained on. In the DvsGesture dataset the SNN-HDC model can identify 100% of samples from an unseen/untrained class. Given the numerous benefits shown and discussed in this paper, this decoding method represents a very compelling alternative to both rate and latency decoding.

</details>


### [194] [DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs](https://arxiv.org/abs/2511.08581)
*Ying Jiao,Rodrigo Castellano Ontiveros,Luc De Raedt,Marco Gori,Francesco Giannini,Michelangelo Diligenti,Giuseppe Marra*

Main category: cs.AI

TL;DR: DPrL是一种新型NeSy系统，通过神经网络参数化和随机逻辑程序提升可扩展性，在复杂任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Neurosymbolic (NeSy) AI旨在结合神经架构和符号推理的优势，但现有方法在可扩展性上存在局限，限制了其实际应用。DPrL旨在解决这一问题。

Method: DPrL采用随机逻辑程序，并通过神经网络参数化所有推导步骤，同时建立了随机逻辑程序解析过程与马尔可夫决策过程的正式映射，从而应用动态规划和强化学习技术进行高效推理和学习。

Result: 实验表明，DPrL在标准NeSy基准测试和知识图谱推理任务中优于现有最先进的NeSy系统，显著提升了可扩展性。

Conclusion: DeepProofLog (DPrL)通过结合神经网络的参数化和随机逻辑程序，显著提升了Neurosymbolic (NeSy) AI的可扩展性，使其在复杂和大规模知识库中表现优于现有方法。

Abstract: Neurosymbolic (NeSy) AI aims to combine the strengths of neural architectures and symbolic reasoning to improve the accuracy, interpretability, and generalization capability of AI models. While logic inference on top of subsymbolic modules has been shown to effectively guarantee these properties, this often comes at the cost of reduced scalability, which can severely limit the usability of NeSy models. This paper introduces DeepProofLog (DPrL), a novel NeSy system based on stochastic logic programs, which addresses the scalability limitations of previous methods. DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. Additionally, we establish a formal mapping between the resolution process of our deep stochastic logic programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning techniques for efficient inference and learning. This theoretical connection improves scalability for complex proof spaces and large knowledge bases. Our experiments on standard NeSy benchmarks and knowledge graph reasoning tasks demonstrate that DPrL outperforms existing state-of-the-art NeSy systems, advancing scalability to larger and more complex settings than previously possible.

</details>


### [195] [Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585)
*Jingtong Yue,Ziqi Huang,Zhaoxi Chen,Xintao Wang,Pengfei Wan,Ziwei Liu*

Main category: cs.AI

TL;DR: 视频生成技术正从视觉生成转向构建交互式、物理合理的虚拟环境，视频基础模型结合隐式世界模型和视频渲染器，支持多任务规划。本文系统梳理了四代技术演进，并展望了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨视频生成技术如何从视觉生成演变为具备物理合理性和交互能力的虚拟环境，推动视频基础模型的发展，并作为隐式世界模型支持多任务规划。

Method: 调查了视频生成技术的四代演进，定义了每代的核心特性、代表作品及其应用领域，并分析了隐式世界模型和视频渲染器的作用。

Result: 提出视频基础模型由隐式世界模型和视频渲染器组成，展示了其在多时空尺度规划、物理合理性和实时交互方面的潜力。

Conclusion: 本文总结了视频生成技术的演进，从单纯的视觉生成发展到构建支持交互和物理合理性的虚拟环境，提出了视频基础模型的核心组件：隐式世界模型和视频渲染器，并展望了下一代世界模型的挑战和设计原则。

Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [196] [SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control](https://arxiv.org/abs/2511.07820)
*Zhengyi Luo,Ye Yuan,Tingwu Wang,Chenran Li,Sirui Chen,Fernando Castañeda,Zi-Ang Cao,Jiefeng Li,David Minor,Qingwei Ben,Xingye Da,Runyu Ding,Cyrus Hogg,Lina Song,Edy Lim,Eugene Jeong,Tairan He,Haoru Xue,Wenli Xiao,Zi Wang,Simon Yuen,Jan Kautz,Yan Chang,Umar Iqbal,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 通过扩展模型规模、数据量和计算资源，开发了一种通用的仿人控制器，利用大规模运动追踪数据实现自然动作生成，并展示了其在下游任务中的实用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模基础模型在多领域取得了显著进展，但仿人控制领域尚未实现类似的扩展效益。当前的仿人控制器规模有限，行为目标单一且训练资源不足。

Method: 构建了一个基于运动追踪的基础模型，通过三个维度的扩展：网络规模（从120万到4200万参数）、数据集规模（超过1亿帧，700小时高质量动作数据）和计算资源（9000 GPU小时）。

Result: 展示了扩展的效益，包括性能随计算和数据多样性提升而稳步改善，以及学习到的表征对未见动作的泛化能力。通过实时通用运动规划器和统一标记空间，展示了模型的实用价值。

Conclusion: 通过扩大模型容量、数据和计算资源，成功开发了一种通用的仿人控制器，能够生成自然且稳健的全身动作。运动追踪作为仿人控制的任务具有天然的可扩展性，且通过大规模数据监督实现了无需人工奖励工程的人类动作先验学习。

Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.

</details>


### [197] [CAVER: Curious Audiovisual Exploring Robot](https://arxiv.org/abs/2511.07619)
*Luca Macesanu,Boueny Folefack,Samik Singh,Ruchira Ray,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: CAVER是一个新型机器人，通过3D打印末端执行器、视听表征和好奇心驱动算法，高效学习物体外观与声音的关联，提升材料分类和模仿音频演示的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解锁机器人在视听感知方面的多模态潜力，需要学习物体视觉外观与互动声音之间的关联，这需要新的交互能力、表征和探索方法。

Method: 1) 设计了可附加在平行夹爪上的3D打印末端执行器，用于激发物体的音频响应；2) 结合局部和全局外观信息与声音特征的视听表征；3) 基于好奇心的探索算法，优先与高不确定性物体互动以高效覆盖令人惊讶的音频。

Result: CAVER在不同场景下比多个探索基线更高效地构建了丰富的表征，学习到的视听表征显著提升了材料分类和模仿音频演示的能力。

Conclusion: CAVER通过其新颖的末端执行器、视听表征和探索算法，显著提升了机器人在材料分类和模仿音频演示方面的表现。

Abstract: Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. https://caver-bot.github.io/

</details>


### [198] [Time-Aware Policy Learning for Adaptive and Punctual Robot Control](https://arxiv.org/abs/2511.07654)
*Yinsen Jia,Boyuan Chen*

Main category: cs.RO

TL;DR: 提出时间感知策略学习框架，使机器人能显式感知时间并优化行为，在多个任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人学习算法大多缺乏时间感知能力，无法适应变化的目标和环境。

Method: 该框架通过引入剩余时间和时间比率两个互补的时间信号，增强传统的强化学习策略，使机器人能够连续调节行为。

Result: 在多个操作领域中，时间感知策略在效率上比标准强化学习基线高出48%，在模拟到现实的迁移中鲁棒性提高8倍，同时保持接近完美的成功率。

Conclusion: 时间感知策略学习为机器人自主性提供了一个统一的基础，使其能够在效率、鲁棒性、弹性和与人类意图对齐方面实现优化。

Abstract: Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.

</details>


### [199] [Testing and Evaluation of Underwater Vehicle Using Hardware-In-The-Loop Simulation with HoloOcean](https://arxiv.org/abs/2511.07687)
*Braden Meyers,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: HoloOcean 2.0模拟器改进了动力学和ROS 2接口，成功用于CougUV鱼雷AUV的HIL/SIL测试，模拟与实际结果可比。


<details>
  <summary>Details</summary>
Motivation: 解决水下机器人系统在受控环境中测试的挑战，尤其是声学传感器和控制面仅在水下有效的问题。

Method: 利用HoloOcean 2.0模拟器进行硬件在环（HIL）和软件在环（SIL）测试，通过ROS 2桥接模拟传感器数据和控制命令。

Result: 成功展示了CougUV鱼雷AUV的HIL和SIL测试设置，模拟结果与实际试验结果进行了比较。

Conclusion: HoloOcean 2.0模拟器通过改进的动力学和新的ROS 2接口，成功支持了CougUV鱼雷AUV的HIL和SIL测试，模拟结果与实际野外试验结果具有可比性。

Abstract: Testing marine robotics systems in controlled environments before field tests is challenging, especially when acoustic-based sensors and control surfaces only function properly underwater. Deploying robots in indoor tanks and pools often faces space constraints that complicate testing of control, navigation, and perception algorithms at scale. Recent developments of high-fidelity underwater simulation tools have the potential to address these problems. We demonstrate the utility of the recently released HoloOcean 2.0 simulator with improved dynamics for torpedo AUV vehicles and a new ROS 2 interface. We have successfully demonstrated a Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) setup for testing and evaluating a CougUV torpedo autonomous underwater vehicle (AUV) that was built and developed in our lab. With this HIL and SIL setup, simulations are run in HoloOcean using a ROS 2 bridge such that simulated sensor data is sent to the CougUV (mimicking sensor drivers) and control surface commands are sent back to the simulation, where vehicle dynamics and sensor data are calculated. We compare our simulated results to real-world field trial results.

</details>


### [200] [RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph](https://arxiv.org/abs/2511.07717)
*Yifan Liu,Fangneng Zhan,Wanhua Li,Haowen Sun,Katerina Fragkiadaki,Hanspeter Pfister*

Main category: cs.RO

TL;DR: RoboTAG是一种结合3D与2D分支的方法，通过图结构减少对标注数据的依赖，实验证明其在多种机器人类型上有效。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常在2D视觉骨干网络上构建，严重依赖标注数据进行训练，这在现实场景中往往稀缺，导致模拟到现实的差距。此外，这些方法将3D问题简化为2D领域，忽略了3D先验。

Method: RoboTAG包含一个3D分支和一个2D分支，其中节点表示相机和机器人系统的状态，边捕捉这些变量之间的依赖关系或对齐关系。图中定义了闭环，可以在这些闭环上应用分支间的一致性监督。

Result: 实验结果表明，RoboTAG在多种机器人类型上有效，突显了其在机器人领域缓解数据瓶颈的潜力。

Conclusion: RoboTAG通过结合3D分支和2D分支，有效缓解了对标注数据的依赖，并在多种机器人类型上表现出色，展示了其在机器人领域缓解数据瓶颈的潜力。

Abstract: Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.

</details>


### [201] [A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation](https://arxiv.org/abs/2511.07720)
*Yuxuan Zhao,Yuanchen Tang,Jindi Zhang,Hongyu Yu*

Main category: cs.RO

TL;DR: 本研究开发了一种兼容多种设备的遥操作流水线，提出新型最优控制器以提升遥操作数据质量，实验验证了其在跟踪误差、奇异性避免和轨迹平滑性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了实现类似于大型语言模型（LLMs）在具身智能中的能力，数据质量在训练具有多样化机器人技能的基础模型中起着关键作用。本研究旨在通过遥操作设备收集操纵任务的数据，探究不同设备与控制策略组合对数据质量的影响。

Method: 研究构建了一个兼容不同遥操作设备和机械臂控制器的遥操作流水线，并开发了基于动态零空间和阻抗跟踪的新型最优控制器。该控制器根据机器人关节可操作性自适应调整权重分配，实现了阻抗控制形式的位姿跟踪和零空间跟踪的奇异性避免。

Result: 定量实验结果表明，所提出的遥操作流水线在不同遥操作接口和运动控制器组合下，显著改善了遥操作轨迹数据的质量，包括跟踪误差、奇异性发生率和关节轨迹的平滑性。

Conclusion: 本研究通过开发兼容不同遥操作设备和机械臂控制器的遥操作流水线，提出了一种新型最优控制器，实现了柔顺的位姿跟踪和奇异性避免，显著提高了遥操作轨迹数据的质量。

Abstract: Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.

</details>


### [202] [LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models](https://arxiv.org/abs/2511.07727)
*Xiaohan Zhang,Yan Ding,Yohei Hayamizu,Zainab Altaweel,Yifeng Zhu,Yuke Zhu,Peter Stone,Chris Paxton,Shiqi Zhang*

Main category: cs.RO

TL;DR: 文章提出了一种结合LLMs常识和计算机视觉的TAMP框架，用于移动操作任务，实验显示机器人成功率为84.4%，但性能仍不及人类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决移动操作（MoMa）中多物体放置的位置和方法问题，特别是在目标不明确的情况下（如“用叉子、刀子和盘子布置餐桌”）。通过结合任务规划和运动规划（TAMP），确保目标实现和运动可行性。

Method: 文章结合了大型语言模型（LLMs）的常识知识和计算机视觉方法，以促进任务级和运动级规划。具体包括利用LLMs的表具组织知识，以及通过计算机视觉学习选择基位置的策略。

Result: 在真实世界和模拟环境中的定量实验表明，机器人在完成长时域物体重排任务时的成功率和效率。真实世界中的成功率为84.4%，但性能仍低于人类服务员。

Conclusion: 文章提供了一个原则性的TAMP框架，用于移动操作任务，考虑了物体重排的常识，并能适应需要移动多个物体的新情境。尽管机器人在真实世界中的物体重排成功率为84.4%，但主观人类评估显示其性能仍低于有经验的人类服务员。

Abstract: Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.
  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.

</details>


### [203] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: ViPRA通过视频预测和潜在动作学习，实现无标注视频到机器人连续控制的转换，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 大多数视频缺乏标注动作，限制了其在机器人学习中的应用。ViPRA旨在利用这些无标注视频学习连续机器人控制。

Method: 采用视频-语言模型预测未来视觉观察和运动中心潜在动作，利用感知损失和光流一致性训练潜在动作，并通过分块流匹配解码器将潜在动作映射到机器人特定连续动作序列。

Result: 在SIMPLER基准上提升16%，在真实世界操作任务中提升13%，支持高达22Hz的高频连续控制。

Conclusion: ViPRA框架通过视频预测和潜在动作学习，实现了从无标注视频到机器人连续控制的转换，显著提升了性能并支持跨实体泛化。

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [204] [Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space](https://arxiv.org/abs/2511.07750)
*Durgakant Pushp,Weizhe Chen,Zheng Chen,Chaomin Luo,Jason M. Gregory,Lantao Liu*

Main category: cs.RO

TL;DR: Pareto-Optimal Visual Navigation框架结合语义、决策和视觉伺服，解决了传统导航方法在复杂环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统反应式方法在杂乱环境中表现不佳，基于地图的方法需要大量测绘工作，而基于学习的方法依赖大数据且泛化能力有限。

Method: 该框架整合了数据驱动的语义分析、Pareto最优决策和视觉伺服技术，实现了无需大量地图或数据集的实时导航。

Result: 提出了Pareto-Optimal Visual Navigation框架，能够在复杂环境中实现轻量级实时导航。

Conclusion: Pareto-Optimal Visual Navigation 框架通过结合数据驱动的语义、Pareto最优决策和视觉伺服，提供了一种轻量级的实时导航解决方案，有效解决了传统方法在复杂环境中的局限性。

Abstract: Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.

</details>


### [205] [High-Altitude Balloon Station-Keeping with First Order Model Predictive Control](https://arxiv.org/abs/2511.07761)
*Myles Pasetsky,Jiawei Lin,Bradley Guo,Sarah Dean*

Main category: cs.RO

TL;DR: FOMPC是一种基于梯度优化的在线规划方法，在HAB站保持任务中优于RL策略，但计算成本较高。


<details>
  <summary>Details</summary>
Motivation: 重新评估模型控制在站保持任务中的有效性，挑战现有RL方法的优势假设。

Method: 通过将风和气球动力学实现为JAX中的可微分函数，实现了基于梯度的轨迹优化在线规划。

Result: FOMPC在多种配置下均表现良好，包括在简化的风和动力学模型下。

Conclusion: FOMPC在站保持任务中表现优于现有RL策略，无需离线训练即可实现24%的TWR提升，但需更高的在线计算成本。

Abstract: High-altitude balloons (HABs) are common in scientific research due to their wide range of applications and low cost. Because of their nonlinear, underactuated dynamics and the partial observability of wind fields, prior work has largely relied on model-free reinforcement learning (RL) methods to design near-optimal control schemes for station-keeping. These methods often compare only against hand-crafted heuristics, dismissing model-based approaches as impractical given the system complexity and uncertain wind forecasts. We revisit this assumption about the efficacy of model-based control for station-keeping by developing First-Order Model Predictive Control (FOMPC). By implementing the wind and balloon dynamics as differentiable functions in JAX, we enable gradient-based trajectory optimization for online planning. FOMPC outperforms a state-of-the-art RL policy, achieving a 24% improvement in time-within-radius (TWR) without requiring offline training, though at the cost of greater online computation per control step. Through systematic ablations of modeling assumptions and control factors, we show that online planning is effective across many configurations, including under simplified wind and dynamics models.

</details>


### [206] [Benchmarking Resilience and Sensitivity of Polyurethane-Based Vision-Based Tactile Sensors](https://arxiv.org/abs/2511.07797)
*Benjamin Davis,Hannah Stuart*

Main category: cs.RO

TL;DR: 研究发现聚氨酯橡胶在VBTS中比硅胶更耐用，适合高负荷应用，但灵敏度可能较低。


<details>
  <summary>Details</summary>
Motivation: 现有VBTS使用的硅胶凝胶虽灵敏度高但易因加载和表面磨损而退化，聚氨酯橡胶因其在高负荷应用中的表现可能提供更好的物理耐久性。

Method: 提出了一系列标准评估基准协议，包括耐久性测试（正常加载、剪切加载和磨损）和灵敏度测试（无模型评估力和空间灵敏度）。

Result: 聚氨酯凝胶在耐久性测试中表现优于硅胶，适用于如瓶盖松紧等实际应用场景。

Conclusion: 聚氨酯橡胶在视觉触觉传感器（VBTS）中显示出比硅胶更好的物理耐久性，尽管可能在灵敏度上有所牺牲。

Abstract: Vision-based tactile sensors (VBTSs) are a promising technology for robots, providing them with dense signals that can be translated into an understanding of normal and shear load, contact region, texture classification, and more. However, existing VBTS tactile surfaces make use of silicone gels, which provide high sensitivity but easily deteriorate from loading and surface wear. We propose that polyurethane rubber, used for high-load applications like shoe soles, rubber wheels, and industrial gaskets, may provide improved physical gel resilience, potentially at the cost of sensitivity. To compare the resilience and sensitivity of silicone and polyurethane VBTS gels, we propose a series of standard evaluation benchmarking protocols. Our resilience tests assess sensor durability across normal loading, shear loading, and abrasion. For sensitivity, we introduce model-free assessments of force and spatial sensitivity to directly measure the physical capabilities of each gel without effects introduced from data and model quality. Finally, we include a bottle cap loosening and tightening demonstration as an example where polyurethane gels provide an advantage over their silicone counterparts.

</details>


### [207] [A CODECO Case Study and Initial Validation for Edge Orchestration of Autonomous Mobile Robots](https://arxiv.org/abs/2511.08354)
*H. Zhu,T. Samizadeh,R. C. Sofia*

Main category: cs.RO

TL;DR: CODECO在AMRs中比Kubernetes更节省CPU且通信更稳定，但内存开销略高且Pod启动稍慢。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为容器化微服务的默认编排器，其假设的稳定网络、同质资源和充足计算能力在移动、资源受限的机器人环境中并不完全适用，因此需要探索更适合的解决方案。

Method: 通过在受控的KinD环境中对CODECO编排和标准Kubernetes进行初步比较，评估了Pod部署和删除时间、CPU和内存使用情况以及Pod间数据传输速率。

Result: 观察结果表明，CODECO在CPU消耗和通信稳定性方面表现更优，但内存使用略高且Pod生命周期延迟稍有增加。

Conclusion: CODECO在智能制造的自主移动机器人（AMRs）中表现出更低的CPU消耗和更稳定的通信模式，尽管存在轻微的内存开销（10-15%）和由于安全覆盖层初始化导致的Pod生命周期延迟略增。

Abstract: Autonomous Mobile Robots (AMRs) increasingly adopt containerized micro-services across the Edge-Cloud continuum. While Kubernetes is the de-facto orchestrator for such systems, its assumptions of stable networks, homogeneous resources, and ample compute capacity do not fully hold in mobile, resource-constrained robotic environments.
  This paper describes a case study on smart-manufacturing AMRs and performs an initial comparison between CODECO orchestration and standard Kubernetes using a controlled KinD environment. Metrics include pod deployment and deletion times, CPU and memory usage, and inter-pod data rates. The observed results indicate that CODECO offers reduced CPU consumption and more stable communication patterns, at the cost of modest memory overhead (10-15%) and slightly increased pod lifecycle latency due to secure overlay initialization.

</details>


### [208] [Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution](https://arxiv.org/abs/2511.07811)
*Sagar Gupta,Thanh Vinh Nguyen,Thieu Long Phan,Vidul Attri,Archit Gupta,Niroshinie Fernando,Kevin Lee,Seng W. Loke,Ronny Kutadinata,Benjamin Champion,Akansel Cosgun*

Main category: cs.RO

TL;DR: 混合多机器人协调框架：分散规划+集中冲突解决，仿真和真实实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统集中式规划方法限制了机器人自主性，而完全分散式方法可能导致死锁，因此需要一种折中方案。

Method: 框架中每个机器人自主规划路径并与集中节点共享信息，集中系统检测潜在冲突并通过类似虚拟交通灯的方式解决冲突。

Result: 仿真实验显示该方法提高了机器人到达目标的成功率并减少了死锁，真实实验在四足机器人和轮式Duckiebots中验证了系统可行性。

Conclusion: 该论文提出了一种混合多机器人协调框架，结合了分散式路径规划和集中式冲突解决，成功在仿真和真实实验中验证了其有效性。

Abstract: We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.

</details>


### [209] [Occlusion-Aware Ground Target Search by a UAV in an Urban Environment](https://arxiv.org/abs/2511.07822)
*Collin Hague,Artur Wolek*

Main category: cs.RO

TL;DR: 论文提出了一种利用概率可视体积（VV）和迭代加深A*算法的无人机路径规划方法，用于在遮挡环境下搜索移动兴趣点（POI），并在高误报概率的传感器环境下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂的城市环境中，无人机搜索移动兴趣点（POI）时面临传感器遮挡和误报概率高的挑战，需要一种有效的路径规划策略。

Method: 采用概率可视体积（VV）结合迭代加深A*算法，通过时间变化的三维表示传感约束，并利用启发式方法优化POI的观测概率。

Result: 在蒙特卡洛模拟中，该方法在高误报概率的杂乱环境中优于基线方法。

Conclusion: 提出的路径规划方法通过概率VV和可变时间步长规划，有效平衡了长期和短期规划，提升了搜索效率。

Abstract: This paper considers the problem of searching for a point of interest (POI) moving along an urban road network with an uncrewed aerial vehicle (UAV). The UAV is modeled as a variable-speed Dubins vehicle with a line-of-sight sensor in an urban environment that may occlude the sensor's view of the POI. A search strategy is proposed that exploits a probabilistic visibility volume (VV) to plan its future motion with iterative deepening $A^\ast$. The probabilistic VV is a time-varying three-dimensional representation of the sensing constraints for a particular distribution of the POI's state. To find the path most likely to view the POI, the planner uses a heuristic to optimistically estimate the probability of viewing the POI over a time horizon. The probabilistic VV is max-pooled to create a variable-timestep planner that reduces the search space and balances long-term and short-term planning. The proposed path planning method is compared to prior work with a Monte-Carlo simulation and is shown to outperform the baseline methods in cluttered environments when the UAV's sensor has a higher false alarm probability.

</details>


### [210] [A Comprehensive Experimental Characterization of Mechanical Layer Jamming Systems](https://arxiv.org/abs/2511.07882)
*Jessica Gumowski,Krishna Manaswi Digumarti,David Howard*

Main category: cs.RO

TL;DR: 研究通过机械层堵塞结构实现软体机器人的刚度调节，测试了齿几何形状对性能的影响，展示了显著的刚度变化能力。


<details>
  <summary>Details</summary>
Motivation: 自然界中的生物（如头足类和厚皮动物）通过刚度调节实现对其附肢的惊人灵巧控制，软体机器人需要类似的刚度调节机制。

Method: 通过双层层压多材料结构（带有齿状突起）实现机械层堵塞，并对关键设计参数（主要是齿几何形状）进行了弯曲和扭转负载下的全面测试。

Result: 这些结构在弯曲和扭转下分别实现了5倍和3.2倍的刚度峰值变化，并测量了分离两层堵塞结构所需的力。

Conclusion: 本研究为机械层堵塞系统的设计提供了原则性指导，帮助研究者根据具体应用选择合适的设计。

Abstract: Organisms in nature, such as Cephalopods and Pachyderms, exploit stiffness modulation to achieve amazing dexterity in the control of their appendages. In this paper, we explore the phenomenon of layer jamming, which is a popular stiffness modulation mechanism that provides an equivalent capability for soft robots. More specifically, we focus on mechanical layer jamming, which we realise through two-layer multi material structure with tooth-like protrusions. We identify key design parameters for mechanical layer jamming systems, including the ability to modulate stiffness, and perform a variety of comprehensive tests placing the specimens under bending and torsional loads to understand the influence of our selected design parameters (mainly tooth geometry) on the performance of the jammed structures. We note the ability of these structures to produce a peak change in stiffness of 5 times in bending and 3.2 times in torsion. We also measure the force required to separate the two jammed layers, an often ignored parameter in the study of jamming-induced stiffness change. This study aims to shed light on the principled design of mechanical layer jammed systems and guide researchers in the selection of appropriate designs for their specific application domains.

</details>


### [211] [EquiMus: Energy-Equivalent Dynamic Modeling and Simulation of Musculoskeletal Robots Driven by Linear Elastic Actuators](https://arxiv.org/abs/2511.07887)
*Yinglei Zhu,Xuguang Dong,Qiyao Wang,Qi Shao,Fugui Xie,Xinjun Liu,Huichan Zhao*

Main category: cs.RO

TL;DR: 提出EquiMus框架，解决大型混合刚性-软体机器人建模挑战，通过仿真和实验验证其有效性，并展示在下游任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于软体机器人复杂的本构行为和实际工作条件，动态建模和控制仍是挑战，尤其是对于具有连续分布质量、运动学环和多样运动模式的大型混合刚性-软体机器人。

Method: 提出EquiMus，一种能量等效的动态建模框架和基于MuJoCo的仿真方法，适用于具有线性弹性执行器的肌肉骨骼刚性-软体混合机器人。

Result: 通过仿真和仿生机器人腿的实际实验验证了EquiMus方法的等效性和有效性，并展示了其在控制器设计和基于学习的控制策略等下游任务中的实用性。

Conclusion: EquiMus框架通过能量等效的动态建模和基于MuJoCo的仿真，成功解决了大型混合刚性-软体机器人的建模和仿真挑战，并在仿生机器人腿上验证了其有效性和实用性。

Abstract: Dynamic modeling and control are critical for unleashing soft robots' potential, yet remain challenging due to their complex constitutive behaviors and real-world operating conditions. Bio-inspired musculoskeletal robots, which integrate rigid skeletons with soft actuators, combine high load-bearing capacity with inherent flexibility. Although actuation dynamics have been studied through experimental methods and surrogate models, accurate and effective modeling and simulation remain a significant challenge, especially for large-scale hybrid rigid--soft robots with continuously distributed mass, kinematic loops, and diverse motion modes. To address these challenges, we propose EquiMus, an energy-equivalent dynamic modeling framework and MuJoCo-based simulation for musculoskeletal rigid--soft hybrid robots with linear elastic actuators. The equivalence and effectiveness of the proposed approach are validated and examined through both simulations and real-world experiments on a bionic robotic leg. EquiMus further demonstrates its utility for downstream tasks, including controller design and learning-based control strategies.

</details>


### [212] [Dual-MPC Footstep Planning for Robust Quadruped Locomotion](https://arxiv.org/abs/2511.07921)
*Byeong-Il Ham,Hyun-Bin Kim,Jeonguk Kang,Keun Ha Choi,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 提出基于MPC的步态规划策略，通过协调脚步和地面反作用力优化角动量控制，在四足机器人上实现更稳定的步态。


<details>
  <summary>Details</summary>
Motivation: 传统步态规划方法通常忽略角速度，仅通过地面反作用力处理角动量，导致性能受限。本文旨在通过协调脚步放置和地面反作用力，改善角动量控制。

Method: 采用模型预测控制（MPC）方法，结合线性倒立摆模型，优化脚步放置以控制角动量。通过互反馈循环耦合步态规划器和地面反作用力MPC，迭代更新脚步和地面反作用力。

Result: 在四足机器人上验证了方法的有效性，展示了在各种地形上鲁棒的步态控制，减少了振荡并延长了站立和摆动阶段。

Conclusion: 所提出的基于MPC的步态规划策略通过协调地面反作用力和脚步放置，显著提高了身体姿态的鲁棒性控制，减少了振荡，并在多种地形上实现了更长的站立和摆动阶段。

Abstract: In this paper, we propose a footstep planning strategy based on model predictive control (MPC) that enables robust regulation of body orientation against undesired body rotations by optimizing footstep placement. Model-based locomotion approaches typically adopt heuristic methods or planning based on the linear inverted pendulum model. These methods account for linear velocity in footstep planning, while excluding angular velocity, which leads to angular momentum being handled exclusively via ground reaction force (GRF). Footstep planning based on MPC that takes angular velocity into account recasts the angular momentum control problem as a dual-input approach that coordinates GRFs and footstep placement, instead of optimizing GRFs alone, thereby improving tracking performance. A mutual-feedback loop couples the footstep planner and the GRF MPC, with each using the other's solution to iteratively update footsteps and GRFs. The use of optimal solutions reduces body oscillation and enables extended stance and swing phases. The method is validated on a quadruped robot, demonstrating robust locomotion with reduced oscillations, longer stance and swing phases across various terrains.

</details>


### [213] [Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments](https://arxiv.org/abs/2511.07927)
*Okan Arif Guvenkaya,Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 论文提出一种结合切线和外推方法的局部路径规划算法，有效指导UGV在动态障碍物环境中安全导航。


<details>
  <summary>Details</summary>
Motivation: 解决无人地面车辆（UGV）在动态障碍物密集环境中的避障和路径规划问题。

Method: 结合基于切线的路径规划和外推方法，开发新的决策算法。

Result: 仿真结果表明，算法能逐步生成无碰撞路径，确保机器人安全导航。

Conclusion: 论文提出的基于切线和外推方法的局部路径规划算法有效，能够在动态障碍物环境中安全导航。

Abstract: Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.

</details>


### [214] [USV Obstacles Detection and Tracking in Marine Environments](https://arxiv.org/abs/2511.07950)
*Yara AlaaEldin,Enrico Simetti,Francesca Odone*

Main category: cs.RO

TL;DR: The paper enhances USV obstacle detection by evaluating and integrating sensor fusion and LiDAR methods, proposing a hybrid approach for better performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of robust obstacle detection and tracking in marine environments for USVs drives the research, building on previous work by the GRAAL lab.

Method: The study evaluates existing systems on marine datasets, integrates them into ROS for real-time testing, and compares sensor fusion with LiDAR-only approaches.

Result: Experimental analysis shows the effectiveness of sensor fusion and LiDAR-only methods, leading to the development of a hybrid approach for a comprehensive obstacle map.

Conclusion: The paper proposes a hybrid approach combining sensor fusion and LiDAR-only methods to enhance obstacle detection and tracking for USVs in marine environments.

Abstract: Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.

</details>


### [215] [Effective Game-Theoretic Motion Planning via Nested Search](https://arxiv.org/abs/2511.08001)
*Avishav Engle,Andrey Zhitnikov,Oren Salzman,Omer Ben-Porat,Kiril Solovey*

Main category: cs.RO

TL;DR: GTNS是一种新型游戏理论嵌套搜索方法，用于高效计算纳什均衡，解决了现有方法的扩展性和局部最小值问题，适用于自动驾驶和赛车场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法在释放游戏理论推理的全部潜力方面存在不足，优化方法需要简化的机器人动态且易陷入局部最小值，而基于收益矩阵的方法由于显式枚举所有可能轨迹而扩展性差。

Method: 引入了游戏理论嵌套搜索（GTNS），该方法通过内部搜索在低维空间中丢弃违反纳什均衡约束的轨迹，从而高效地搜索所有涉及代理的动作空间。

Result: GTNS在多种自动驾驶和赛车场景中实现了解决方案，仅需几秒钟即可在普通硬件上完成计算。

Conclusion: GTNS是一种新颖、可扩展且可证明正确的方法，用于在一般动态系统中计算纳什均衡，解决了现有方法的局限性。

Abstract: To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.

</details>


### [216] [A Two-Layer Electrostatic Film Actuator with High Actuation Stress and Integrated Brake](https://arxiv.org/abs/2511.08005)
*Huacen Wang,Hongqiang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种双层静电薄膜执行器，优化电极分布并集成制动机制，显著提升执行应力和负载保持能力，适用于轻量化机器人。


<details>
  <summary>Details</summary>
Motivation: 传统电机驱动的机器人系统存在质量大、控制算法复杂和需要额外制动机制等问题，而现有的静电薄膜执行器在空气中的执行应力仍有提升空间。

Method: 设计了一种双层静电薄膜执行器，通过在顶层和底层交替分布电极，减小有效电极间距，并集成了静电吸附制动机制。

Result: 新型执行器的执行应力达到约241 N/m²，比之前的三相电极设计提高了90.5%，并通过多种实验验证了其在执行和制动模式下的优越性能。

Conclusion: 本文提出的双层静电薄膜执行器通过优化电极分布和集成制动机制，显著提升了执行应力和负载保持能力，适用于轻量化和紧凑型机器人平台。

Abstract: Robotic systems driven by conventional motors often suffer from challenges such as large mass, complex control algorithms, and the need for additional braking mechanisms, which limit their applications in lightweight and compact robotic platforms. Electrostatic film actuators offer several advantages, including thinness, flexibility, lightweight construction, and high open-loop positioning accuracy. However, the actuation stress exhibited by conventional actuators in air still needs improvement, particularly for the widely used three-phase electrode design. To enhance the output performance of actuators, this paper presents a two-layer electrostatic film actuator with an integrated brake. By alternately distributing electrodes on both the top and bottom layers, a smaller effective electrode pitch is achieved under the same fabrication constraints, resulting in an actuation stress of approximately 241~N/m$^2$, representing a 90.5\% improvement over previous three-phase actuators operating in air. Furthermore, its integrated electrostatic adhesion mechanism enables load retention under braking mode. Several demonstrations, including a tug-of-war between a conventional single-layer actuator and the proposed two-layer actuator, a payload operation, a one-degree-of-freedom robotic arm, and a dual-mode gripper, were conducted to validate the actuator's advantageous capabilities in both actuation and braking modes.

</details>


### [217] [AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles](https://arxiv.org/abs/2511.08016)
*Adrian Schönnagel,Michael Dubé,Christoph Steup,Felix Keppler,Sanaz Mostaghim*

Main category: cs.RO

TL;DR: 提出分散式群体智能策略，有效解决重型铰接车辆的折叠和碰撞问题，实验验证高效。


<details>
  <summary>Details</summary>
Motivation: 针对长形和复杂运动学的机器人群体问题，填补了物流自动化、远程采矿、机场行李运输和农业操作等实际应用中的研究空白。

Method: 采用纯粹反应式的分散群体智能策略，专门设计用于自动化长形铰接车辆，优先避免折叠并建立相互碰撞避免的基础。

Result: 单HAV实验中，99.8%成功避免折叠，86.7%和83.4%分别达到第一和第二目标；双HAV交互中，相应比例为98.9%、79.4%和65.1%，且99.7%未发生相互碰撞。

Conclusion: 该论文提出了一种基于分散式群体智能的新方法，有效避免了重型铰接车辆（HAVs）的折叠和相互碰撞，并通过模拟实验验证了其高效性。

Abstract: This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.
  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.

</details>


### [218] [Model Predictive Control via Probabilistic Inference: A Tutorial](https://arxiv.org/abs/2511.08019)
*Kohei Honda*

Main category: cs.RO

TL;DR: 论文提供了一个关于基于概率推断的模型预测控制的教程，重点介绍了MPPI等方法，旨在解决传统优化方法在非线性系统中的局限性。


<details>
  <summary>Details</summary>
Motivation: 针对非线性或不可微分系统在机器人领域中常见的问题，传统数值优化方法难以处理，因此需要探索更高效的方法，如基于概率推断的MPC。

Method: 通过重新将最优控制问题解释为概率推断问题，论文介绍了基于采样的技术来估计最优控制分布，而不依赖基于梯度的数值优化。详细推导了标准最优控制问题中的最优控制分布，并以MPPI算法为例进行了实践说明。

Result: 论文系统地介绍了基于概率推断的MPC方法，包括理论推导、算法实现（如MPPI）、先验和变分分布设计、调参原则及理论分析。

Conclusion: 该论文旨在为研究人员和实践者提供系统指南，帮助他们理解、实现和扩展基于概率推断的模型预测控制方法，如MPPI，适用于机器人及其他领域。

Abstract: Model Predictive Control (MPC) is a fundamental framework for optimizing robot behavior over a finite future horizon. While conventional numerical optimization methods can efficiently handle simple dynamics and cost structures, they often become intractable for the nonlinear or non-differentiable systems commonly encountered in robotics. This article provides a tutorial on probabilistic inference-based MPC, presenting a unified theoretical foundation and a comprehensive overview of representative methods. Probabilistic inference-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have gained significant attention by reinterpreting optimal control as a problem of probabilistic inference. Rather than relying on gradient-based numerical optimization, these methods estimate optimal control distributions through sampling-based techniques, accommodating arbitrary cost functions and dynamics. We first derive the optimal control distribution from the standard optimal control problem, elucidating its probabilistic interpretation and key characteristics. The widely used MPPI algorithm is then derived as a practical example, followed by discussions on prior and variational distribution design, tuning principles, and theoretical aspects. This article aims to serve as a systematic guide for researchers and practitioners seeking to understand, implement, and extend these methods in robotics and beyond.

</details>


### [219] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 研究通过ReAct框架结合主动视觉探索，提升了LLMs在多智能体系统中的视角理解和协作能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多智能体交互中缺乏有效的视角理解能力，导致在复杂环境中协作时表现不佳。研究旨在探索如何通过整合视角机制提升模型性能。

Method: 研究采用ReAct框架，扩展了经典的Director任务，设计了七个逐步增加视角复杂度的场景，测试模型在不同状态表示和提示策略下的表现。

Result: 结果表明，明确的视角提示和主动探索策略显著提高了模型的解释准确性和协作效果。

Conclusion: 本研究通过结合明确视角提示和主动探索策略，显著提升了大型语言模型（LLMs）在理解与协作任务中的表现，为未来自适应和情境感知AI系统的研究奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


### [220] [Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.08214)
*Yi Huang,Zhan Qu,Lihui Jiang,Bingbing Liu,Hongbo Zhang*

Main category: cs.RO

TL;DR: PGS框架利用感知输出作为监督信号，解决了模仿学习中的因果混淆问题，显著提升了自动驾驶闭环性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习依赖于专家轨迹，但这些轨迹包含不可归因的噪声，导致因果混淆，影响在闭环场景中的性能。

Method: 通过感知输出作为主要监督信号，引入正负自监督机制，对齐决策模块的输入输出与感知结果（如车道中心线和周围智能体的预测运动）。

Result: 在Bench2Drive基准测试中，驾驶得分为78.08，平均成功率为48.64%，显著优于现有方法。

Conclusion: 感知引导自监督（PGS）框架在自动驾驶中有效解决了因果混淆问题，显著提升了闭环场景下的性能表现。

Abstract: End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.
  To address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.
  Equipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.

</details>


### [221] [Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems](https://arxiv.org/abs/2511.08231)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: 提出MFR-PINP方法，通过多保真度学习和SC框架解决实时状态估计中的模型不匹配和不确定性问题，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动模型在实时非线性状态估计中的模型不匹配问题，并确保预测的可靠性。

Method: 采用多保真度残差物理信息神经过程（MFR-PINP）方法，结合分裂共形（SC）预测框架处理模型不确定性。

Result: 实验结果表明，MFR-PINP在估计性能上优于卡尔曼滤波器的先进变体（如无迹卡尔曼滤波器和深度卡尔曼滤波器）。

Conclusion: MFR-PINP模型在实时状态估计任务中表现优异，成为一种可行的选择。

Abstract: Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.

</details>


### [222] [X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention](https://arxiv.org/abs/2511.08277)
*Dehan Shen,Changhao Chen*

Main category: cs.RO

TL;DR: X-IONet is a cross-platform inertial odometry framework that improves navigation accuracy for both pedestrians and quadruped robots via expert networks and EKF fusion.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based inertial odometry methods perform poorly on quadruped robots due to their dynamic motion patterns, necessitating a cross-platform solution.

Method: X-IONet uses a rule-based expert selection module for motion classification, a dual-stage attention architecture for displacement prediction, and an EKF for state estimation.

Result: X-IONet reduces ATE by 14.3% and RTE by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data.

Conclusion: X-IONet demonstrates superior performance in both pedestrian and quadruped robot navigation, significantly reducing trajectory errors and advancing cross-platform inertial odometry.

Abstract: Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.

</details>


### [223] [Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot](https://arxiv.org/abs/2511.08299)
*Zhiang Liu,Yang Liu,Yongchun Fang,Xian Guo*

Main category: cs.RO

TL;DR: 通过相位变量和数据增强，学习框架使四足机器人无需参考运动即可习得多样化全向步态。


<details>
  <summary>Details</summary>
Motivation: 现有控制器无法充分利用仿生形态特征，依赖预设步态或关节轨迹，限制了运动的多样性和灵活性。

Method: 采用相位变量控制每个身体部位，结合相位覆盖奖励促进腿部相位空间探索，并通过数据增强融入形态对称性。

Result: 机器人成功习得22种全向步态，展示了动态和对称运动。

Conclusion: 提出的学习框架有效帮助四足机器人获得多样化、全向步态，证明了其在现实场景中的适用性。

Abstract: Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.

</details>


### [224] [Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm](https://arxiv.org/abs/2511.08377)
*Michael Bowman,Xiaoli Zhang*

Main category: cs.RO

TL;DR: Psychic框架通过SDE和KM系数检测操作者意图突变，结合SINDy模型推断运动行为，在非结构化场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前意图推断方法常忽略操作者轨迹中的细微运动，而这些运动可能是意图突变的强指标。研究旨在解决如何检测和利用这些跳跃运动来推断操作者目标状态。

Method: Psychic框架采用跳跃漂移扩散随机微分方程（SDE）建模操作者的运动，结合Kramers-Moyal（KM）系数和统计异常检测算法检测跳跃，并使用SINDy模型推断操作者运动行为。

Result: Psychic通过600个操作者轨迹的回顾性研究验证了其有效性，能够生成概率可达集并在离线与在线学习中表现优异。

Conclusion: Psychic框架通过结合跳跃漂移扩散随机微分方程和统计异常检测算法，能够有效检测和推断操作者意图的突变，并在非结构化场景中实现早期目标检测和未定义目标的发现。

Abstract: Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.

</details>


### [225] [Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface](https://arxiv.org/abs/2511.08454)
*Tianyu Jia,Xingchen Yang,Ciaran McGeady,Yifeng Li,Jinzhi Lin,Kit San Ho,Feiyu Pan,Linhong Ji,Chong Li,Dario Farina*

Main category: cs.RO

TL;DR: 新型触觉编码脑机接口通过P300范式实现自然运动与超自由度运动的无缝整合，实验验证其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决在保持自然运动的同时，如何整合多自由度的增强命令这一关键挑战。

Method: 采用触觉诱发的P300范式，进行了多日实验，包括单一运动识别任务和双重任务范式。

Result: 脑机接口实现了对四个超自由度运动的实时可靠解码，训练后性能显著提升，且不影响自然运动。

Conclusion: 研究提出了一种新型触觉编码脑机接口，通过刺激感觉传入神经实现运动增强，在不影响自然运动的情况下扩展了运动自由度。

Abstract: Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.

</details>


### [226] [A Supervised Autonomous Resection and Retraction Framework for Transurethral Enucleation of the Prostatic Median Lobe](https://arxiv.org/abs/2511.08490)
*Mariana Smith,Tanner Watts,Susheela Sharma Stern,Brendan Burkhart,Hao Li,Alejandro O. Chara,Nithesh Kumar,James Ferguson,Ayberk Acar,Jesse F. d'Almeida,Lauren Branscombe,Lauren Shepard,Ahmed Ghazi,Ipek Oguz,Jie Ying Wu,Robert J. Webster,Axel Krieger,Alan Kuntz*

Main category: cs.RO

TL;DR: 研究结合模型规划与学习网络实现半自主前列腺切除，切除率达97.1%，为自动化微创手术奠定基础。


<details>
  <summary>Details</summary>
Motivation: 同心管机器人（CTRs）在毫米尺度上提供了灵活的运动能力，可通过自然孔道进行微创手术。本研究旨在通过半自主系统实现经尿道前列腺切除。

Method: 研究提出了一个协调的基于模型的切除规划器和基于学习的回缩网络（PushCVAE），共同实现半自主组织切除。切除规划器直接在分割的CT体积上操作，生成三阶段中叶切除工作流程的器械轨迹。回缩网络根据手术阶段生成回缩动作。

Result: 在由水凝胶材料制成的前列腺模型上，结合自主系统实现了目标中叶体积的97.1%切除。

Conclusion: 该研究为经尿道机器人手术中的图像引导自主性奠定了基础，并朝着完全自动化的微创前列腺摘除迈出了第一步。

Abstract: Concentric tube robots (CTRs) offer dexterous motion at millimeter scales, enabling minimally invasive procedures through natural orifices. This work presents a coordinated model-based resection planner and learning-based retraction network that work together to enable semi-autonomous tissue resection using a dual-arm transurethral concentric tube robot (the Virtuoso). The resection planner operates directly on segmented CT volumes of prostate phantoms, automatically generating tool trajectories for a three-phase median lobe resection workflow: left/median trough resection, right/median trough resection, and median blunt dissection. The retraction network, PushCVAE, trained on surgeon demonstrations, generates retractions according to the procedural phase. The procedure is executed under Level-3 (supervised) autonomy on a prostate phantom composed of hydrogel materials that replicate the mechanical and cutting properties of tissue. As a feasibility study, we demonstrate that our combined autonomous system achieves a 97.1% resection of the targeted volume of the median lobe. Our study establishes a foundation for image-guided autonomy in transurethral robotic surgery and represents a first step toward fully automated minimally-invasive prostate enucleation.

</details>


### [227] [Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1](https://arxiv.org/abs/2511.08502)
*Ruya Karagulle,Cristian-Ioan Vasile,Necmiye Ozay*

Main category: cs.RO

TL;DR: 提出一种基于WSTL的安全保证方法，通过结构剪枝和对数变换转化为MILP问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在安全关键领域无法保证安全性，因此需要一种既能适应行为又能确保安全的方法。

Method: 提出了一种基于WSTL的方法，通过结构剪枝和对数变换将问题转化为混合整数线性规划（MILP），同时保证安全性。

Result: 在机器人导航和真实世界F1数据上的实验表明，该方法能有效建模复杂任务目标和捕捉细微偏好。

Conclusion: 该方法通过Weighted Signal Temporal Logic (WSTL)结合结构剪枝和对数变换，不仅保证了安全性，还能有效捕捉复杂任务中的细微偏好。

Abstract: Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.

</details>


### [228] [SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment](https://arxiv.org/abs/2511.08583)
*Rong Xue,Jiageng Mao,Mingtong Zhang,Yue Wang*

Main category: cs.RO

TL;DR: SeFA通过选择性流对齐策略，解决了视觉运动策略学习中动作与观察不一致的问题，显著提升了策略的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有修正流方法在迭代蒸馏后生成动作与当前视觉观察对应的真实动作偏离的问题，避免误差累积和执行不稳定。

Method: 采用选择性流对齐策略，利用专家示范选择性校正生成的动作，保持与观察的一致性，同时保留多模态性。

Result: 在模拟和真实世界的操作任务中，SeFA策略超越了基于扩散和流的最先进策略，准确性和鲁棒性更优，推理延迟降低了98%以上。

Conclusion: SeFA框架通过选择性流对齐策略，成功解决了现有方法中动作生成与视觉观察不一致的问题，实现了高效且准确的视觉运动策略学习。

Abstract: Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [229] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,José Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: 提出DT-Create套件，基于智能技术和自适应性，支持预测性维护中的数字孪生规范与决策制定。


<details>
  <summary>Details</summary>
Motivation: 解决预测性维护中基于数据做出敏捷和准确决策的挑战，利用数字孪生（DTs）处理信息并支持决策。

Method: 采用设计科学研究（DSR）方法，通过两个开发周期开发DT-Create套件，并通过案例研究进行评估。

Result: DT-Create套件在数据收集、存储和智能处理、信息增强、模型选择及决策支持方面表现出色。

Conclusion: DT-Create套件通过智能技术、语义数据处理和自适应性成功支持了预测性维护中的决策制定，展示了在数字孪生规范中的可行性。

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [230] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: SemanticForge通过四种创新算法解决LLMs代码生成的系统性错误，提升语义感知能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成中存在逻辑和架构上的系统性错误，限制了实际应用。

Method: 1. 自动调和算法统一静态和动态知识图；2. 神经方法学习从自然语言生成结构化图查询；3. 结合SMT求解的束搜索算法；4. 增量维护算法保持语义等价性。

Result: SemanticForge在结构化图查询生成上达到73%的精确度，显著优于传统方法的51%。

Conclusion: SemanticForge通过四种核心算法创新显著提升了语义感知代码生成的准确性和效率，解决了LLMs在代码生成中的系统性问题。

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [231] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: 研究发现函数式编程范式在代码理解和调试中带来更多挑战，开发者对其自信心较低，但调试能力不受主要范式影响。


<details>
  <summary>Details</summary>
Motivation: 探讨多范式编程语言中特定范式特征如何影响代码理解和调试能力。

Method: 通过眼动追踪技术，对29名开发者（主要是学生）进行四项分类和四项调试任务的数据收集与分析。

Result: 开发者在分类函数式和过程式代码时存在混淆，函数式代码完成时间最长；调试能力不受主要范式变化影响，但开发者对函数式代码的自信心较低；调试过程中阅读模式存在显著差异。

Conclusion: 研究表明，开发者在分类和调试代码时对不同编程范式的语言特征有不同的理解和反应，尤其是函数式编程范式带来更多挑战。

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [232] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: 论文提出了一种自适应的AI安全框架SISF，通过动态反馈循环实时调整安全策略，显著降低了攻击成功率，同时保持零误报率。


<details>
  <summary>Details</summary>
Motivation: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.

Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.

Result: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.

Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [233] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: 提出了BRACE框架，通过CIRC和OTER两种方法评估代码语言模型的能源效率与功能正确性，发现模型大小不影响评级，参数效率是关键。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI技术在软件开发中的快速应用，需要系统评估其环境影响与功能正确性之间的权衡，现有方法缺乏对代码语言模型的准确性-能源权衡的系统框架。

Method: 提出了BRACE框架，包含两种评级方法：CIRC（确定性欧几里得排名）和OTER（趋势感知评估），用于在统一的能源效率和功能正确性尺度上对22个最先进的代码语言模型进行基准测试。

Result: 分析显示，模型在代码摘要任务中表现更好，因为不需要生成基于语法和句法正确的输出；模型大小对评级影响不大，表明参数利用效率高的模型排名更高。

Conclusion: BRACE框架为从业者提供了基于证据的模型选择方法，平衡了可持续性与任务需求，指导根据部署优先级选择CIRC进行确定性比较或OTER进行趋势感知评估。

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [234] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: 本文介绍了一个用于Thermal Desktop的MATLAB/C++ GUI工具，通过OpenTD API和CSR文件高效提取热流数据，提升了分析效率，并提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够快速提取和可视化Thermal Desktop中热流相关指标的软件，阻碍了热工程师的高效分析。

Method: 开发了一个基于MATLAB和C++的GUI，利用OpenTD API和自定义解析器，通过CSR文件的副作用高效加载温度、传导率和子模型指标。

Result: 该方法能显著减少模型节点和导体与子模型ID关联的运行时间，效率提升达数量级。

Conclusion: 本文总结了通过GUI和OpenTD API改进Thermal Desktop数据提取和可视化的方法，并指出了当前方法的局限性，同时为未来OpenTD版本提供了改进建议。

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [235] [Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams](https://arxiv.org/abs/2511.07742)
*Luan Lazzari,Kleinner Farias*

Main category: cs.SE

TL;DR: Harmony Validator是一款Papyrus插件，用于实时检测UML模型中的不一致性，提升建模教育的理解和学习效果。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的建模活动需要高技能，如抽象、一致性维护和精确沟通，这些技能难以掌握和教授。教育者和学生在管理建模过程中的不一致性方面常遇到困难。

Method: 该工具采用事件驱动架构，持续监控建模动作，实时通知用户出现的不一致性，包括类图和序列图。

Result: 案例研究表明，Harmony Validator在软件工程课程中提升了学生对UML建模的感知有用性和学习效果。

Conclusion: Harmony Validator工具通过实时检测和报告UML模型中的不一致性，提升了软件建模教育的模型一致性理解和反思性学习实践。

Abstract: Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.

</details>


### [236] [Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics](https://arxiv.org/abs/2511.07851)
*Sharif Ahmed,Addi Malviya Thakur,Gregory R. Watson,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文研究了GitHub上科学开源软件的可持续性，通过多模态分析和可视化技术，揭示了不同项目的可持续性差异及社区反馈的重要性。


<details>
  <summary>Details</summary>
Motivation: 科学开源软件（Sci-OSS）项目对推动研究至关重要，但长期维持这些项目仍是一个主要挑战。

Method: 采用多模态分析方法，结合文献中的存储库指标和从十个突出的Sci-OSS项目中挖掘的数据，开发了一种新颖的可视化技术。

Result: 统计分析显示，即使是相似领域的项目，其可持续性方式也各不相同。自然语言分析支持了文献中的观点，强调了项目特定反馈在维护软件质量中的关键作用。

Conclusion: 本文通过分析和可视化方法，为研究人员、资助者和开发者提供了关于科学开源软件长期可持续性的关键见解。

Abstract: Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.

</details>


### [237] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: ChaosEater是一个自动化混沌工程（CE）周期的系统，利用LLMs降低构建弹性系统的成本，效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统的混沌工程工具在执行实验和基于结果改进系统方面仍依赖人工，成本高且需多领域专业知识。ChaosEater旨在降低这些成本，使任何人都能低成本构建弹性系统。

Method: ChaosEater利用大型语言模型（LLMs）自动化CE周期，包括需求定义、代码生成、测试和调试等软件工程任务。

Result: ChaosEater在小型和大型Kubernetes系统上通过案例研究验证，能够以极低的时间和金钱成本完成合理的CE周期。

Conclusion: ChaosEater通过自动化整个混沌工程（CE）周期，显著降低了构建弹性系统的时间和成本，其效果得到了人类工程师和LLMs的验证。

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


### [238] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: CQ^2A是一种上下文驱动的问题生成方法，通过提取实体关系并利用LLM生成自然且多样的问题，显著提升了问答系统的测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法生成的提问不自然且依赖现有数据集，忽略了上下文，限制了问题的多样性和相关性。

Method: CQ^2A从上下文中提取实体和关系形成真实答案，并利用大型语言模型基于这些答案和上下文生成问题，同时通过一致性验证和约束检查提高输出可靠性。

Result: 在三个数据集上的实验表明，CQ^2A在错误检测能力、问题自然度和上下文覆盖率上优于现有方法，且生成的测试用例在微调中降低了错误率。

Conclusion: CQ^2A通过上下文驱动的问题生成方法显著提升了问答系统的测试效果，包括错误检测能力、问题自然度及上下文覆盖率，并通过微调进一步降低了错误率。

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [239] ["I need to learn better searching tactics for privacy policy laws.'' Investigating Software Developers' Behavior When Using Sources on Privacy Issues](https://arxiv.org/abs/2511.08059)
*Stefan Albert Horstmann,Sandy Hong,Maziar Niazian,Cristiana Santos,Alena Naiakshina*

Main category: cs.SE

TL;DR: 研究揭示了开发者在进行隐私合规开发时面临的挑战，现有信息资源（个人知识、在线内容和AI助手）均未能有效支持，需改进资源设计。


<details>
  <summary>Details</summary>
Motivation: 探讨GDPR和CCPA实施后，开发者如何在缺乏法律专业知识的情况下进行隐私合规开发，评估现有信息资源的有效性。

Method: 通过定性研究，包括30名开发者的思考发声会议和后续访谈，分析他们在隐私敏感场景中的决策过程。

Result: 开发者在使用个人知识、在线资源和AI助手时均遇到困难，现有支持资源在可访问性、可理解性和可操作性方面存在不足。

Conclusion: 研究发现现有隐私相关开发支持存在重大缺陷，强调需要为开发者提供更易获取、理解和操作的隐私资源。

Abstract: Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.

</details>


### [240] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 该论文研究了传统SCM和LLM4Code的漏洞可转移性，提出了一种无需下游分类器的对抗样本生成方法HABITAT，实验显示其对抗样本对LLM4Code的成功率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究既未提供有效生成对抗样本的实用方法（如需要访问SCM的下游分类器），也未关注现代软件开发平台和云端集成开发环境中广泛使用的LLM4Code。

Method: 设计了HABITAT，包括定制的扰动插入机制和分层强化学习框架，无需访问SCM的下游分类器即可自适应选择最优扰动。

Result: 系统地研究了传统SCM和LLM4Code的内在漏洞可转移性，提出了一种受害者无关的对抗样本生成方法。

Conclusion: 该研究揭示了传统SCM和LLM4Code之间的潜在漏洞关联，为未来开发鲁棒防御提供了关键焦点。实验证明，基于传统SCM构建的对抗样本对LLM4Code的成功率高达64%，超越现有技术15%以上。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [241] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: OWLAPY是一个Python框架，用于简化OWL本体工程，支持多种推理器和转换功能，并允许利用LLMs生成本体。已在GitHub和PyPI发布，下载量超过50,000次。


<details>
  <summary>Details</summary>
Motivation: 为了简化OWL 2本体的创建、修改和序列化过程，并提供更灵活的工具支持高级本体工程，特别是为从Java环境过渡到Python环境的用户。

Method: OWLAPY整合了原生Python推理器和外部Java推理器，支持多种核心本体组件的实现，并提供强大的转换能力，如OWL类表达式与描述逻辑、曼彻斯特语法和SPARQL之间的转换。此外，它还允许用户定义自定义工作流，利用大型语言模型（LLMs）从自然语言文本生成本体。

Result: OWLAPY已在GitHub和PyPI上公开发布，下载量超过50,000次，被证明是一个经过良好测试的软件框架。

Conclusion: OWLAPY是一个全面的Python框架，专为OWL本体工程而设计，提供灵活的工具和功能，支持用户从Java环境过渡到Python环境。

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [242] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 研究发现LLM-based MAS在SE任务中最常用于代码生成，设计师最关注功能性适用性，常用基于角色的合作模式，设计理由多为提高生成代码质量。


<details>
  <summary>Details</summary>
Motivation: 缺乏对LLM-based MAS在SE任务中设计的系统性研究，包括关注的QAs、设计模式及设计理由。

Method: 收集了94篇关于LLM-based MAS应用于SE任务的论文作为数据源，分析了其关注的QAs、设计模式及设计理由。

Result: (1) 代码生成是最常见的SE任务；(2) 功能性适用性是设计师最关注的QA；(3) 基于角色的合作是最常用的设计模式；(4) 提高生成代码质量是最常见的设计理由。

Conclusion: 基于研究结果，提出了设计LLM-based MAS以支持SE任务的启示。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


### [243] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 研究尝试用GPT-4o模拟KLEE的符号执行输出，结果显示准确率约20%，初步探索了LLM在此领域的潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: KLEE等符号执行工具在检测程序时资源消耗大且速度慢，尤其是在处理复杂代码时。本研究探索大型语言模型（如GPT-4o）是否能模拟KLEE的输出，以节省时间和资源。

Method: 使用GPT-4o模拟KLEE的输出，并尝试识别程序中最受约束的路径（具有最多符号条件的路径）。实验基于100个C程序的数据集。

Result: GPT-4o生成KLEE类似输出的准确率约为20%，且在识别最受约束路径方面表现有限。

Conclusion: 虽然GPT-4o在模拟符号执行输出和识别最受约束路径方面准确率较低（约20%），但这初步展示了当前大型语言模型在模拟符号执行方面的潜力与局限性。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [244] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: A3GNN是一个在异构CPU-GPU平台上实现高效、低成本GNN训练的框架，通过智能调度和优化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: GNN训练通常依赖昂贵的高性能计算平台，限制了其普及性。研究表明，通过充分利用资源，可在资源受限设备上实现显著效率提升。

Method: A3GNN采用局部感知采样和细粒度并行调度优化资源利用，并利用强化学习探索设计空间以实现吞吐量、内存占用和准确性的帕累托最优权衡。

Result: 实验证明，A3GNN能在7个Nvidia 2080Ti GPU上实现比2个A100 GPU高1.8倍的吞吐量，且准确率损失极小。

Conclusion: A3GNN框架成功地在资源受限的异构CPU-GPU平台上实现了高效、自适应的GNN训练，通过智能资源调度和优化，显著提升了性能并降低了成本。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [245] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 论文探讨了解耦推理架构，通过分离前填充和解码阶段，优化了大型语言模型的实时推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从Transformer架构发展到具有万亿参数的模型，实时推理已成为主要瓶颈，部署这些大规模模型面临内存带宽、计算吞吐量和延迟要求的复杂分布式系统挑战。

Method: 采用分布式系统原则，如服务解耦、资源解耦和工作负载分区，以克服传统单体GPU集群的限制。

Result: 解耦推理架构通过资源解耦和工作负载分区，显著降低了延迟、提高了吞吐量并减少了成本。

Conclusion: 论文提出了一种解耦推理架构，通过将计算密集型的前填充阶段与内存密集型的解码阶段分离为独立可扩展的组件，有效缓解了资源争用问题，并实现了对关键指标（如首令牌时间和令牌间延迟）的独立优化。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [246] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: Synera是一种设备-云协同的LLM服务系统，通过优化卸载决策、并行推理和批处理，提升了生成质量并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM部署中的性能挑战，如生成质量下降和延迟增加，同时克服云卸载的通信瓶颈和本地SLM的资源限制。

Method: 提出了一种设备-云协同的LLM服务系统Synera，采用了高效的SLM-LLM协同机制，包括通信高效的选择性卸载、无停顿并行推理和可扩展的云批处理设计。

Result: Synera在真实测试平台上实现了1.20-5.47倍的生成质量提升，并降低了8.2-16.5%的云服务成本。

Conclusion: Synera通过设备-云协同机制，显著提升了LLM的生成质量并降低了云服务成本，同时保持了较低的延迟。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [247] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 本文通过系统化分析LLM推理事件，提出了一种分类和方法论，识别了主要故障模式并提出了缓解措施，展示了如何提升大规模LLM服务的可靠性和成本效率。


<details>
  <summary>Details</summary>
Motivation: 超大规模LLM推理对云系统提出了极高要求，即使是短暂的故障也可能对用户和业务产生重大影响。为了更好地理解和减轻这些风险。

Method: 开发了一种基于一年运维经验的分类和方法论，并在156起高严重性事件上进行了验证，同时对2025年4月至6月的数据进行了定量研究。

Result: 实现了高标注一致性（Cohen's K ~0.89），识别了主要故障模式（数据集中约60%为推理引擎故障，其中约40%为超时），并提出了缓解措施（约74%自动检测；约28%需要热修复）。

Conclusion: 本研究展示了如何通过系统化、基于经验的分析来推动更可靠且成本高效的大规模LLM服务。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [248] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 研究首次广泛评估了在单板计算机上运行量化开源LLM的性能，发现Llamafile在吞吐量和功耗上显著优于Ollama，为边缘计算提供了实用部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着对设备端大型语言模型（LLM）推理需求的增长，研究旨在探索在边缘硬件上部署轻量级、低成本AI解决方案的可行性，尤其是在单板计算机（SBCs）上。

Method: 研究在三种SBCs（Raspberry Pi 4、Raspberry Pi 5和Orange Pi 5 Pro）上，使用两种推理运行时（Ollama和Llamafile），对25种量化开源LLM进行性能基准测试，评估生成吞吐量、内存使用和功耗。

Result: 结果显示，SBCs能支持高达1.5B参数的模型，Llamafile的吞吐量比Ollama高4倍，功耗低30-40%。研究还识别了架构特定瓶颈和运行时权衡。

Conclusion: 研究表明，单板计算机（SBCs）能够可靠支持参数高达1.5B的模型，Llamafile在吞吐量和功耗上优于Ollama。研究填补了高性能语言模型与低成本边缘计算之间的空白。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [249] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: MCP增强LLM能力但增加token使用，研究分析性能与成本权衡并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然增强了LLM能力，但上下文信息导致token使用量激增，增加了成本和计算负载。

Method: 通过测量分析MCP与LLM的交互，研究不同模型和配置对性能指标的影响。

Result: 揭示了能力、性能和成本之间的权衡，并提出了优化方向。

Conclusion: 研究提出了优化MCP配置的建议，如启用并行工具调用和任务中止机制，以开发更高效、稳健且经济的工作流程。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [250] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: DynaKV是一种自适应KVCache管理方法，通过动态调整和高效设计，显著提升智能手机上长序列解码的性能。


<details>
  <summary>Details</summary>
Motivation: 随着对长序列解码需求的增长，智能手机因DRAM容量有限，现有静态或局部更新的KVCache管理方法无法适应分布变化，导致效率低下。

Method: DynaKV结合了三种关键技术：（1）免迁移的集群自适应；（2）连续性为中心的闪存管理；（3）内存高效的缓存设计。

Result: DynaKV相比现有方案，平均提升1.38倍准确性和1.47倍速度。

Conclusion: DynaKV在智能手机上实现了高效的长序列解码，不仅提高了检索准确性，还降低了延迟，同时其设计理念可扩展至其他长上下文工作负载和多层内存层次。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [251] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: HyProv是一个混合溯源管理系统，结合集中式和联邦式方法，支持实时、可扩展的工作流溯源查询，适用于复杂分布式环境。


<details>
  <summary>Details</summary>
Motivation: 现有溯源系统在可扩展性、实时处理、在线溯源分析及跨组件和计算资源整合方面存在不足，且缺乏对工作流的专门优化。HyProv旨在解决这些问题，特别是在复杂分布式集群环境中。

Method: HyProv采用集中式组件管理稳定且小规模的工作流规范特定溯源数据，同时通过联邦查询技术整合不同可扩展的监控和溯源数据库中的大规模执行日志。

Result: 实验表明，HyProv能够扩展到大型工作流，以亚秒级延迟回答溯源查询，并为集群增加适度的CPU和内存开销。

Conclusion: HyProv通过结合集中式和联邦式范式，提供了一个可扩展、实时且工作流感知的溯源管理系统，能够高效处理大规模工作流执行日志，同时支持低延迟查询和复杂溯源分析。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [252] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本地推理通过小型LM和加速器可有效分散云负载，IPW是关键指标，实证显示其高准确性和效率潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）查询需求的快速增长，集中式云基础设施面临扩展挑战，小型LM和本地加速器的进步为重新思考这一范式提供了可能。

Method: 通过大规模实证研究，覆盖20多种最先进的本地语言模型、8种加速器及100万条真实世界单轮聊天和推理查询，测量准确性、能耗、延迟和功率。

Result: 本地LM能准确回答88.7%的单轮聊天和推理查询；2023至2025年IPW提升5.3倍，查询覆盖率从23.2%增至71.3%；本地加速器IPW至少比云加速器低1.4倍。

Conclusion: 研究表明，本地推理可以有效分散集中式基础设施的负载，其中智能每瓦（IPW）是追踪这一转变的关键指标。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [253] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: 本文提出了一种支持节点与任意数量对等体通信的通用TDM算法，克服了原有框架的限制，并在卫星间链路中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 原Python联邦学习测试床中的TDM通信算法仅支持节点间两两通信，限制了实际应用场景。本文旨在解决这一问题，提出支持节点与任意数量对等体通信的通用算法。

Method: 论文涵盖了算法的理论基础、系统设计及系统验证三部分。通过理论分析、系统实现和实验验证相结合的方法，展示了新算法的可行性和优势。

Result: 新算法成功实现了节点与任意数量对等体的通信，并在卫星间链路通信中验证了其有效性，展示了其在实际应用中的潜力。

Conclusion: 本文提出了一种新的通用TDM通信算法，克服了原有Python联邦学习测试床中仅支持节点间两两通信的限制，支持节点与任意数量对等体通信，并验证了其在实际卫星间链路通信中的有效性。

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [254] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: UniFormer是一种高效Transformer架构，适用于通用和定制计算平台，解决了跨平台优化问题，并在GPU和FPGA上表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用和定制计算平台之间根本不同的并行计算范式通常导致模型迁移和部署的妥协，而跨平台优化原则在现有研究中仍未充分探索。

Method: 通过提高并行性和计算-存储融合，UniFormer在通用和定制计算平台上实现了高效的Transformer架构。

Result: UniFormer在GPU上实现了最先进的准确性和延迟，并在FPGA上展现出强大的适应性。

Conclusion: UniFormer是一种统一的、高效的Transformer架构，适用于通用和定制计算平台，在GPU上实现了最先进的准确性和延迟，并在FPGA上展现出强大的适应性。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [255] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: ProbSelect利用分析建模和概率预测，无需历史数据或持续监控，显著提升GPU加速设备上的客户端选择效率，提高SLO合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于持续监控和历史数据收集，在动态环境中（如卫星和移动设备频繁变化操作条件）不切实际，且现有解决方案主要考虑CPU计算，无法适应GPU加速训练的复杂特性。

Method: 采用分析建模和概率预测进行客户端选择，无需历史数据或持续监控，并在用户定义的SLO内建模。

Result: 在不同GPU架构和工作负载的广泛评估中，ProbSelect平均提高了13.77%的SLO合规性，并实现了72.5%的计算浪费减少。

Conclusion: ProbSelect 是一种新颖的客户端选择方法，通过分析建模和概率预测，显著提升了在GPU加速设备上的SLO合规性，并大幅减少了计算浪费。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [256] [\uline{LO}w-c\uline{O}st yet High-\uline{P}erformant \uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)
*Kelun Lei,Hailong Yang,Kaige Zhang,Kejie Ma,Yiqing Wang,Xin You,Yufan Xu,Enrique S. Quintana-Orti,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: LOOPS是一个混合执行框架，通过结合CSR和BCSR布局及自适应并行化，显著提升SpMM性能，优于CPU和GPU基线，尤其在能效上表现突出。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-稠密矩阵乘法（SpMM）在科学计算和图学习中至关重要，但现有方法难以有效利用Armv9架构的SME扩展和传统SIMD资源。

Method: 提出了LOOPS框架，结合行式CSR部分和向量式BCSR部分布局，利用NEON向量指令和SME扩展资源，通过轻量级性能模型指导的自适应两级并行化方案支持多精度SpMM。

Result: 在M4Pro CPU上，LOOPS相比TACO和Armadillo分别实现9.93×/14.4×（FP32/FP64）和71.3×/54.8×（FP32/FP64）的加速；与A100 GPU上的cuSPARSE和Magicube相比，平均加速达19.8×-33.5×，且能效显著更优。

Conclusion: LOOPS框架通过结合CSR和BCSR布局以及自适应并行化方案，显著提升了稀疏矩阵-稠密矩阵乘法的性能，并在不同精度下均优于现有CPU和GPU方法，尤其在能效方面表现突出。

Abstract: Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.

</details>


### [257] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 在OBLOT模型下，研究了受限于图边的机器人聚集问题，设计了针对无限网格和超立方体的时间最优算法，推测可能不存在通用算法。


<details>
  <summary>Details</summary>
Motivation: 研究在OBLOT模型下，受限于图边的机器人聚集问题，特别考虑了初始配置可能包含多重性、机器人无法检测多重性以及机器人在顶点和边传递图上移动的情况。

Method: 设计了两种不同的算法，分别针对无限网格和超立方体这两种边和顶点传递图的具体拓扑结构。这两种算法都是时间最优的，并充分利用了底层拓扑的特性。

Result: 提供了一些基本的不可能性结果，并设计了两种针对特定拓扑结构的算法，这些算法在时间上是最优的。

Conclusion: 论文推测，由于算法高度依赖于特定拓扑结构，可能不存在适用于所有可解情况的通用算法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [258] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 约束编程优化Kubernetes调度器Pod分配，显著提升高优先级Pod部署成功率。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用轻量级启发式方法可能导致次优分配和资源碎片化，阻碍可部署Pod的分配。

Method: 提出并实现了一个基于OR-Tools约束求解器的插件，作为默认调度器的后备机制。

Result: 在1秒调度窗口下，44%的场景中优化调度器比默认调度器分配更多高优先级Pod；10秒窗口下提升至73%。

Conclusion: 使用约束编程优化Kubernetes调度器在无法分配Pod时作为后备机制，显著提高了高优先级Pod的分配成功率。

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [259] [Work-in-Progress: Function-as-Subtask API Replacing Publish/Subscribe for OS-Native DAG Scheduling](https://arxiv.org/abs/2511.08297)
*Takahiro Ishikawa-Aso,Atsushi Yano,Yutaro Kobayashi,Takumi Jin,Yuuki Takano,Shinpei Kato*

Main category: cs.OS

TL;DR: FasS API通过函数化子任务，确保DAG语义在API层面而非程序员规范下得到保障，实验验证有效并提出了在Linux中的应用指南。


<details>
  <summary>Details</summary>
Motivation: ROS 2的发布/订阅API无法强制执行DAG优先约束，导致DAG语义依赖惯例，一旦违反，模型即崩溃。

Method: 提出Function-as-Subtask (FasS) API，将每个子任务表达为函数，其参数和返回值分别代表子任务的输入和输出边，从而最小化描述自由度。在基于Rust的实验内核上实现了DAG原生调度器，并评估其语义保真度。

Result: FasS API成功在API层面保障DAG语义，实验验证了其有效性。

Conclusion: FasS API通过将子任务表达为函数，确保了DAG语义在API层面的保障，而非依赖程序员规范。实验验证了其在语义保真度上的有效性，并提出了在Linux sched_ext中应用FasS的设计指南。

Abstract: The Directed Acyclic Graph (DAG) task model for real-time scheduling finds its primary practical target in Robot Operating System 2 (ROS 2). However, ROS 2's publish/subscribe API leaves DAG precedence constraints unenforced: a callback may publish mid-execution, and multi-input callbacks let developers choose topic-matching policies. Thus preserving DAG semantics relies on conventions; once violated, the model collapses. We propose the Function-as-Subtask (FasS) API, which expresses each subtask as a function whose arguments/return values are the subtask's incoming/outgoing edges. By minimizing description freedom, DAG semantics is guaranteed at the API rather than by programmer discipline. We implement a DAG-native scheduler using FasS on a Rust-based experimental kernel and evaluate its semantic fidelity, and we outline design guidelines for applying FasS to Linux Linux sched_ext.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [260] [Resource Allocation in Hybrid Radio-Optical IoT Networks using GNN with Multi-task Learning](https://arxiv.org/abs/2511.07428)
*Aymen Hamrouni,Sofie Pollin,Hazem Sallouha*

Main category: cs.NI

TL;DR: DGET framework combines GNNs and Transformer for efficient scheduling in hybrid IoT networks, reducing AoI by 20% with 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the intractability of solving NP-hard scheduling problems in hybrid IoT networks and the impracticality of full channel observability.

Method: The paper introduces the DGET framework, a supervised multi-task learning architecture combining two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder for joint throughput maximization and delay minimization.

Result: Simulation results show DGET achieves over 90% classification accuracy, reduces AoI by up to 20%, and maintains comparable energy consumption while outperforming standalone RF systems.

Conclusion: The proposed DGET framework achieves near-optimal scheduling with high accuracy, reduces computational complexity, and demonstrates robustness under partial channel observability, outperforming traditional methods.

Abstract: This paper addresses the problem of dual-technology scheduling in hybrid Internet of Things (IoT) networks that integrate Optical Wireless Communication (OWC) alongside Radio Frequency (RF). We begin by formulating a Mixed-Integer Nonlinear Programming (MINLP) model that jointly considers throughput maximization and delay minimization between access points and IoT nodes under energy and link availability constraints. However, given the intractability of solving such NP-hard problems at scale and the impractical assumption of full channel observability, we propose the Dual-Graph Embedding with Transformer (DGET) framework, a supervised multi-task learning architecture combining a two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder. The first stage employs a transductive GNN that encodes the known graph topology and initial node and link states. The second stage introduces an inductive GNN for temporal refinement, which learns to generalize these embeddings to the evolved states of the same network, capturing changes in energy and queue dynamics over time, by aligning them with ground-truth scheduling decisions through a consistency loss. These enriched embeddings are then processed by a classifier for the communication links with a Transformer encoder that captures cross-link dependencies through multi-head self-attention via classification loss. Simulation results show that hybrid RF-OWC networks outperform standalone RF systems by handling higher traffic loads more efficiently and reducing the Age of Information (AoI) by up to 20%, all while maintaining comparable energy consumption. The proposed DGET framework, compared to traditional optimization-based methods, achieves near-optimal scheduling with over 90% classification accuracy, reduces computational complexity, and demonstrates higher robustness under partial channel observability.

</details>


### [261] [Pinching Antennas Meet AI in Next-Generation Wireless Networks](https://arxiv.org/abs/2511.07442)
*Fang Fang,Zhiguo Ding,Victor C. M. Leung,Lajos Hanzo*

Main category: cs.NI

TL;DR: AI与PA技术的协同优化了下一代无线网络的性能和适应性，支持新兴应用并推动研究新方向。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需在超可靠和低延迟要求下支持新兴应用，如扩展现实和自主系统。PA技术提供了一种灵活的低成本解决方案，而AI则能管理复杂的PA激活位置和资源分配。

Method: 探讨AI如何优化PA激活位置，以及PA如何支持边缘AI任务。

Result: AI与PA的协同可以实现自适应优化，支持边缘AI任务，并推动语义通信和集成感知通信的发展。

Conclusion: AI与PA技术的协同作用为下一代无线网络的自适应、弹性和自优化提供了新途径。

Abstract: Next-generation (NG) wireless networks must embrace innate intelligence in support of demanding emerging applications, such as extended reality and autonomous systems, under ultra-reliable and low-latency requirements. Pinching antennas (PAs), a new flexible low-cost technology, can create line-of-sight links by dynamically activating small dielectric pinches along a waveguide on demand. As a compelling complement, artificial intelligence (AI) offers the intelligence needed to manage the complex control of PA activation positions and resource allocation in these dynamic environments. This article explores the "win-win" cooperation between AI and PAs: AI facilitates the adaptive optimization of PA activation positions along the waveguide, while PAs support edge AI tasks such as federated learning and over-the-air aggregation. We also discuss promising research directions including large language model-driven PA control frameworks, and how PA-AI integration can advance semantic communications, and integrated sensing and communication. This synergy paves the way for adaptive, resilient, and self-optimizing NG networks.

</details>


### [262] [Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.07466)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 论文提出一种MILP-based调度方法，优化边缘-中心-云系统中的工作流延迟，实验显示延迟显著低于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 边缘-中心-云范式下的设备异构性、资源限制及能力差异导致延迟关键型应用面临挑战，需优化调度以最小化工作流延迟。

Method: 采用连续时间混合整数线性编程（MILP）全面建模，考虑了多设备异构性、计算/通信/能源限制及传感/执行能力差异等约束条件。

Result: 实验证明该方法在真实用例中平均延迟降低13.54%，在合成工作流中延迟减少33.03%，且具有良好可扩展性。

Conclusion: 该论文提出了一种基于连续时间混合整数线性规划的优化调度方法，显著降低了边缘-中心-云系统中的工作流应用延迟，相比现有启发式方法平均延迟改善13.54%，并在合成工作流中实现33.03%的平均延迟降低。

Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.

</details>


### [263] [A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices](https://arxiv.org/abs/2511.07770)
*Zewei Guo,Zhen Jia,JinXiao Zhu,Wenhao Huang,Yin Chen*

Main category: cs.NI

TL;DR: 论文提出大规模同型号设备数据集和开源框架，通过随机森林算法实现89.06%的射频指纹识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有射频指纹识别数据集中设备规模小、模型异构的问题，以支持稳健的机器学习模型训练和公平评估。

Method: 采用123台相同型号的商业IEEE 802.11g设备收集数据，包含3542万原始I/Q样本和185万射频特征，并提出了基于随机森林的算法。

Result: 在提出的数据集上，随机森林算法实现了89.06%的识别准确率，实验评估进一步验证了特征提取的相关性。

Conclusion: 该论文通过引入大规模同型号设备数据集和开源实验框架，显著提升了射频指纹识别在相同型号设备间的区分能力，并验证了特征提取方法的有效性。

Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.

</details>


### [264] [Argo: An efficient verification framework for distributed in-network computing](https://arxiv.org/abs/2511.08189)
*Mingyuan Song,Huan Shen,Jinghui Jiang,Qiang Su,Qingyu Song,Lu Tang,Wanjian Feng,Fei Yuan,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: Procurator 是一种新型验证框架，通过角色模型和CSP捕捉分布式网络程序的交互行为，显著提升验证效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 分布式网络内程序因性能优势被广泛部署，但其交互行为可能导致状态不一致和系统故障，现有验证框架无法有效检测此类问题。

Method: Procurator 结合了角色模型和通信顺序进程（CSP），将流水线执行转化为反应式、事件驱动的角色，并通过消息传递统一交互行为。此外，它还引入了语义感知状态修剪器以降低验证复杂度。

Result: Procurator 在五个真实网络系统中发现了10个由交互行为引起的错误，并将验证时间和内存消耗分别减少了913.2倍和1.9倍。

Conclusion: Procurator 是一种高效的验证框架，能够捕捉分布式网络内程序的交互行为，显著提升验证效率和系统可靠性。

Abstract: Distributed in-network programs are increasingly deployed in data centers for their performance benefits, but shifting application logic to switches also enlarges the failure domain. Ensuring their correctness before deployment is thus critical for reliability. While prior verification frameworks can efficiently detect bugs for programs running on a single switch, they overlook the common interactive behaviors in distributed settings, thereby missing related bugs that can cause state inconsistencies and system failures. This paper presents Procurator, a verification framework that efficiently captures interactive behaviors in distributed in-network programs. Procurator introduces a formal model combining the actor paradigm with Communicating Sequential Processes (CSP), translating pipeline execution into reactive, event-driven actors and unifying their interactions as message passing. To support flexible specification of distributed properties, it provides a unified intent language. Additionally, it incorporates a semantic-aware state pruner to reduce verification complexity, thus ensuring system scalability. Evaluation results show that Procurator efficiently uncovers 10 distinct bugs caused by interactive behaviors across five real-world in-network systems. It also reduces verification time by up to 913.2x and memory consumption by up to 1.9x compared to the state-of-the-art verifier.

</details>


### [265] [SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services](https://arxiv.org/abs/2511.08282)
*Eranga Bandara,Safdar H. Bouk,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Peter Foytik,Ross Gore,Xueping Liang,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: SRE-Llama是一个结合AI与区块链的SRE平台，自动化生成和管理SLI/SLO，提升云原生服务的可靠性和开发者效率。


<details>
  <summary>Details</summary>
Motivation: 解决开发者在定义SLI/SLO时对工具和流程理解不足的问题，提升云原生服务的可靠性和性能。

Method: 平台从云原生服务捕获指标并存储在时间序列数据库中，利用Federated Learning模型识别关键SLI指标，再通过微调的Llama-3 LLM生成SLI/SLO、错误预算及告警机制，并将生成的SLI/SLO编码为NFT存储在区块链上。

Result: 实现了基于定制化Open5GS 5G Core的SRE-Llama平台原型，验证了自动化SLI/SLO生成与管理的可行性。

Conclusion: SRE-Llama平台通过结合Generative-AI、Federated Learning、Blockchain和NFT技术，成功简化了云原生环境中SLI/SLO的生成与监控流程，提升了开发者的可访问性和效率。

Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.

</details>


### [266] [Demystifying QUIC from the Specifications](https://arxiv.org/abs/2511.08375)
*Darius Saif,Ashraf Matrawy*

Main category: cs.NI

TL;DR: 本文全面介绍QUIC协议，解决学习过程中的挑战，帮助读者理解其复杂规范。


<details>
  <summary>Details</summary>
Motivation: QUIC作为推动HTTP/3的关键协议，其复杂性和快速演变使得学习过程充满挑战。本文旨在解决读者在学习过程中遇到的困难，如标准文档的复杂性、协议的快速演变等。

Method: 通过分析QUIC的标准文档、比较其与TCP/TLS的差异，并解释其跨层和隐私导向的设计，本文提供了对QUIC的深入解读。

Result: 本文成功地为读者提供了一个清晰且全面的QUIC概述，帮助理解其规范和实践中的复杂性。

Conclusion: 本文旨在以全面且易于理解的方式介绍QUIC协议，帮助读者从规范中理解其复杂性。

Abstract: QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.

</details>


### [267] [Fault Tolerant Reconfigurable ML Multiprocessor](https://arxiv.org/abs/2511.08381)
*Tangrui Li,Justin Y. Shi,Matteo Spatola,Hongzheng Wang*

Main category: cs.NI

TL;DR: 论文通过实验验证了可重构多处理器架构在NN训练中的适应性，并探讨了与MLIR编译器的集成可能性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在证明该架构在神经网络训练工作流中的适应性和鲁棒性，并探索其与现有应用的硬件集成。

Method: 通过三个计算实验验证了受冯·诺伊曼启发的可重构容错多处理器架构。

Result: 实验证明了该架构的可行性，并提出了与MLIR编译器的潜在集成方案。

Conclusion: 论文展示了可重构多处理器架构在非规则工作流中的适应性鲁棒性可行性，并探讨了与MLIR编译器集成以支持多样化加速器硬件。

Abstract: This paper reports three computational experiments for a von Neumann inspired reconfigurable fault tolerant multiprocessor for neural network (NN) training workflows. The experiments are intended to prove the feasibility of the proposed reconfigurable multiprocessor architecture for non-regular workflows on robustness of adaptability. A potential integration with MLIR compilers is also discussed for integrating diverse accelerator hardware for existing practical applications.

</details>


### [268] [Adaptive Reallocation of RAN Functions for Resilient 6G Networks](https://arxiv.org/abs/2511.08467)
*Gabriel M. Almeida,Jacek Kibiłda,Joao F. Santos,Kleber Vieira Cardoso*

Main category: cs.NI

TL;DR: 提出首个利用RAN功能自适应放置的弹性机制，通过优化问题恢复服务连续性，在真实网络拓扑中验证其性能优于传统方法70%。


<details>
  <summary>Details</summary>
Motivation: 基站解耦为离散的RAN功能引入了新的威胁，一个功能的故障可能引发级联故障，破坏整个功能链，影响网络性能并导致中断。

Method: 模型通过检测受影响的RU，在替代云位置重新实例化CU和DU，并重新建立功能链来恢复服务连续性。将恢复过程建模为一个优化问题，以最大化故障后的网络性能，同时考虑基础设施的计算和通信约束。

Result: 在真实移动网络拓扑的多个故障场景下进行数值评估，结果显示该解决方案比传统弹性机制恢复了高达70%的吞吐量。

Conclusion: 提出的弹性机制通过自适应放置RAN功能，有效缓解了级联故障对移动网络的影响，恢复了服务连续性，并在多个故障场景下验证了其优于传统方法的性能。

Abstract: The disaggregation of base stations into discrete RAN functions introduces new threats to mobile networks, as failures in one RAN function can trigger cascading failures and disrupt the entire functional chain, impacting network performance and leading to outages. In this paper, we propose the first resilience mechanism leveraging the adaptive placement of RAN functions to mitigate disruptions and recover service continuity in the presence of compromised infrastructure. Our model detects disrupted RUs due to cascading failures, reacts by re-instantiating CU and DU in alternative cloud locations, and recovers service continuity by reestablishing functional chains. We formulate this recovery process as an optimization problem that maximizes post-failure network performance while considering computational and communication constraints of the infrastructure. We numerically evaluated our approach on a real-world mobile network topology under multiple failure scenarios, and demonstrated that our solution recovers up to 70% higher throughput compared to conventional resilience mechanisms.

</details>
