<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising](https://arxiv.org/abs/2511.08633)
*Assaf Singer,Noam Rotstein,Amir Mann,Ron Kimmel,Or Litany*

Main category: cs.CV

TL;DR: TTM是一种无需训练的视频生成框架，通过粗糙动画和双时钟去噪实现精确运动控制，兼容性强且表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像和文本的视频生成方法缺乏精确运动控制，且需要模型微调，计算成本高且限制性强。TTM旨在提供一种轻量级、通用性强的解决方案。

Method: 提出Time-to-Move (TTM)，一种无需训练、即插即用的框架，通过粗糙参考动画（如剪切-拖动或深度重投影）作为运动线索，结合双时钟去噪策略实现区域依赖的生成控制。

Result: 实验表明，TTM在真实感和运动控制上匹配或超越现有基于训练的方法，并实现了像素级外观控制的独特能力。

Conclusion: TTM框架在视频生成中实现了高效的运动和外观控制，无需额外训练，兼容性强，且在实验中表现优于现有基于训练的方法。

Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.

</details>


### [2] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: OUGS通过物理参数不确定性建模和语义分割，优化了3D高斯溅射的主动视图选择，提升了目标对象重建效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有主动重建方法依赖于场景级不确定性指标，这些指标常被无关背景干扰，导致对象中心任务的视图选择效率低下。

Method: 提出了一种基于3D高斯原语物理参数（如位置、尺度、旋转）的不确定性模型，通过渲染雅可比传播协方差，并结合语义分割掩码生成目标感知的不确定性评分。

Result: 在公共数据集上的实验表明，OUGS在3DGS重建效率和目标对象质量上显著优于现有最先进方法。

Conclusion: OUGS框架通过基于物理参数的不确定性建模和语义分割掩码的集成，显著提升了3DGS重建过程的效率，并在目标对象上实现了比现有方法更高的质量。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [3] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 论文提出了一种AI驱动的解决方案，用于自动化提取工厂结构信息，准确率高达93%。


<details>
  <summary>Details</summary>
Motivation: 为了应对能源转型中的数字化挑战，自动化工厂信息的提取，减轻MGM用户的日常工作量。

Method: 采用了OCR、Vision LLM、目标检测、关系推理和优化算法，并引入了一种新的Transformer架构来增强场景图生成模型。

Result: 在提取设计数据文本信息方面达到91%的准确率，组件识别准确率为93%，层次结构提取准确率约为80%。

Conclusion: 该论文提出了一种基于生成式人工智能的解决方案，成功实现了对SNAM能源基础设施工厂结构的自动化信息提取，显著提高了数字化流程的效率。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [4] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 本文提出了一种系统性评估方法，用于分析和量化基于修复的说话人脸生成中的嘴唇泄漏问题，包括三种互补的测试设置和衍生指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中因身份参考图像导致的嘴唇泄漏问题，该问题难以通过标准指标和常规测试检测。

Method: 采用三种测试设置（静默输入生成、不匹配的音频-视频配对、匹配的音频-视频合成）及衍生指标（唇同步差异和静默音频唇同步分数）。

Result: 提出了一种模型无关的方法，为未来研究建立了更可靠的基准，并研究了不同身份参考选择对泄漏的影响。

Conclusion: 该研究方法为说话人脸生成领域提供了更可靠的评估框架，并有助于优化身份参考设计。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [5] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: MATRIX是一个多无人机跟踪数据集和框架，用于复杂环境中的动态多视角监控，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多无人机监控系统在动态相机位置和复杂遮挡情况下表现不佳，需要更全面的数据集和框架来应对这些挑战。

Method: 提出了一种新颖的深度学习框架，包括实时相机校准、基于特征的图像配准和多视角特征融合的鸟瞰图（BEV）表示。

Result: 提出的方法在复杂环境中保持了约90%的检测和跟踪准确率，并在挑战性条件下成功跟踪了约80%的轨迹。

Conclusion: MATRIX数据集和框架为动态多视角监控系统的发展提供了重要基准，展示了在复杂环境中鲁棒的多无人机跟踪能力。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [6] [Learning Topology-Driven Multi-Subspace Fusion for Grassmannian Deep Network](https://arxiv.org/abs/2511.08628)
*Xuan Yu,Tianyang Xu*

Main category: cs.CV

TL;DR: 提出了一种Grassmannian流形上的多子空间融合框架，通过自适应子空间协作和拓扑驱动方法，显著提升了复杂几何结构的建模能力，并在多个任务中实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态单子空间表示，忽略了多子空间之间的动态交互，无法有效捕捉复杂几何结构。

Method: 1. 提出了一种基于Kolmogorov-Arnold表示定理的自适应多子空间建模机制；2. 设计了多子空间交互块，通过Fréchet均值优化在流形上融合异构几何表示。

Result: 在3D动作识别（HDM05、FPHA）、EEG分类（MAMEM-SSVEPII）和图任务中展示了最先进的性能。

Conclusion: 该论文提出了一种拓扑驱动的多子空间融合框架，通过Grassmannian流形上的自适应子空间协作，显著提升了复杂几何结构的捕捉能力，并在多个任务中实现了最先进的性能。

Abstract: Grassmannian manifold offers a powerful carrier for geometric representation learning by modelling high-dimensional data as low-dimensional subspaces. However, existing approaches predominantly rely on static single-subspace representations, neglecting the dynamic interplay between multiple subspaces critical for capturing complex geometric structures. To address this limitation, we propose a topology-driven multi-subspace fusion framework that enables adaptive subspace collaboration on the Grassmannian. Our solution introduces two key innovations: (1) Inspired by the Kolmogorov-Arnold representation theorem, an adaptive multi-subspace modelling mechanism is proposed that dynamically selects and weights task-relevant subspaces via topological convergence analysis, and (2) a multi-subspace interaction block that fuses heterogeneous geometric representations through Fréchet mean optimisation on the manifold. Theoretically, we establish the convergence guarantees of adaptive subspaces under a projection metric topology, ensuring stable gradient-based optimisation. Practically, we integrate Riemannian batch normalisation and mutual information regularisation to enhance discriminability and robustness. Extensive experiments on 3D action recognition (HDM05, FPHA), EEG classification (MAMEM-SSVEPII), and graph tasks demonstrate state-of-the-art performance. Our work not only advances geometric deep learning but also successfully adapts the proven multi-channel interaction philosophy of Euclidean networks to non-Euclidean domains, achieving superior discriminability and interpretability.

</details>


### [7] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 提出统一内存库的CAD框架，避免任务特定内存碎片化，实验显示在多个数据集上优于现有方法，AUROC分数最高达0.972。


<details>
  <summary>Details</summary>
Motivation: 现有的基于嵌入的CAD方法需要为每个任务构建特定类别的子内存库，限制了灵活性和可扩展性。

Method: 提出了一种新型的CAD框架，所有任务共享统一的内存库，通过固定大小的核心集增量更新嵌入，避免任务特定的内存碎片化。推理阶段通过最近邻匹配机制计算异常分数。

Result: 在MVTec AD和Visa数据集上的实验表明，该方法优于现有基线，AUROC分数分别为0.972和0.891，且在电子纸数据集上实现了100%的异常检测准确率。

Conclusion: 该方法在MVTec AD和Visa数据集上表现优异，AUROC分数分别达到0.972和0.891，在实际电子纸数据集上实现了100%的异常检测准确率，验证了其鲁棒性。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [8] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 论文提出了一个结合扩散去噪和时间感知actor-critic模型的统一框架，用于自动驾驶中的事故预测，旨在解决噪声输入和及时可靠预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中事故预测的实时性和可靠性至关重要，但噪声输入和准确预测时机是两大挑战。

Method: 框架整合了扩散去噪模块（迭代优化噪声鲁棒特征）和时间感知actor-critic模型（通过时间加权奖励优化预测时机）。

Result: 在三个基准数据集（DAD、CCD、A3D）上取得了最先进准确率和显著的平均事故时间提升，且在噪声环境下表现稳健。

Conclusion: 该模型在复杂交通场景中能生成更早、更稳定且与人类判断一致的预测，展现了实际部署的潜力。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [9] [RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation](https://arxiv.org/abs/2511.08651)
*Hae-Won Jo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: RS-Net通过空间和时间上下文评分机制，提升了动态场景图生成中关系预测的性能，尤其在长尾分布关系上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景图生成方法仅针对标注对象对进行训练，缺乏对非相关对的指导，导致推理时难以识别有意义的关系。

Method: 提出了Relation Scoring Network (RS-Net)，包含空间上下文编码器和时间编码器，通过评分机制整合空间交互和长期时间上下文信息。

Result: 在Action Genome数据集上，RS-Net显著提升了召回率和精确率，尤其在平均召回率上有明显增益，且保持了较高的效率。

Conclusion: RS-Net通过评分机制增强了动态场景图生成中的关系预测能力，尤其在处理长尾分布关系时表现优异，且在保持高效的同时优于现有方法。

Abstract: Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.

</details>


### [10] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种在潜在空间中操作的视频隐私保护方法，通过轻量级AAM模块减少隐私泄漏并保持任务效用。


<details>
  <summary>Details</summary>
Motivation: 解决现有隐私保护方法在视频基础模型中不适合的问题，这些方法需要重新训练整个实用视频模型并导致任务特定的匿名化。

Method: 引入了一个轻量级的Anonymizing Adapter Module（AAM），采用三种新设计的训练目标：剪辑级自监督隐私目标、共同训练目标和潜在一致性损失。

Result: 评估显示隐私泄漏显著减少了35%，同时在各种下游任务中保持了接近基准的效用性能。

Conclusion: 该论文提出了一种轻量级的匿名适配器模块（AAM），能够在视频特征中去除隐私信息同时保持通用任务效用，显著减少了隐私泄漏并保持了接近基准的效用性能。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [11] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 自回归逐像素预测的扩展策略依赖任务类型和高分辨率，计算资源是瓶颈，未来五年有望实现逐像素建模。


<details>
  <summary>Details</summary>
Motivation: 探索自回归逐像素预测这一简单、端到端但尚未充分研究的统一视觉模型框架的扩展性质。

Method: 通过在不同计算预算（最高达7e19 FLOPs）下训练一系列Transformer模型，并评估三种目标指标：逐像素预测目标、ImageNet分类准确率和生成质量（Fr'echet Distance）。

Result: 1. 扩展策略高度任务依赖。2. 高分辨率下模型规模需比数据规模增长更快。3. 计算资源是主要瓶颈。4. 预计未来五年可实现逐像素建模。

Conclusion: 研究发现，自回归逐像素预测的扩展策略高度依赖具体任务，且随着图像分辨率提高，模型规模需比数据规模增长更快。计算资源而非训练数据量成为主要瓶颈，预计未来五年内逐像素建模图像将变得可行。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [12] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 论文探讨了通过LoRA和DreamBooth等扩散微调技术生成更准确的平衡训练数据，以解决图像分类中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 图像分类系统常因训练数据中群体分布不均而继承偏见，例如头发颜色分类中金发与女性的刻板关联。现有方法如Stable Diffusion生成的平衡数据难以保持原始分布。

Method: 采用多种扩散微调技术（如LoRA、DreamBooth），并在每个群体内聚类图像以训练多个DreamBooth模型，生成平衡数据用于预训练，再在真实数据上微调。

Result: 在多个基准测试中，所研究的微调方法平均优于原始Stable Diffusion，与SOTA去偏见技术（如Group-DRO）效果相当，且在偏见严重时表现更优。

Conclusion: 扩散微调技术能有效生成平衡训练数据，缓解图像分类中的偏见问题，尤其在偏见严重时表现突出。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [13] [WiCV at CVPR 2025: The Women in Computer Vision Workshop](https://arxiv.org/abs/2511.08748)
*Estefania Talavera,Deblina Bhattacharjee,Himangi Mittal,Mengwei Ren,Karen Sanchez,Carla Muntean,JungEun Kim,Mona Jalal*

Main category: cs.CV

TL;DR: WiCV@CVPR 2025 highlighted diversity efforts in computer vision, featuring papers, mentorship, and networking, with significant participation and support.


<details>
  <summary>Details</summary>
Motivation: To increase the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community.

Method: The workshop included paper presentations, mentorship programs, and networking opportunities, supported by sponsors and travel grants.

Result: The workshop featured 14 accepted papers, 80 mentees matched with 37 mentors, and attracted over 100 onsite participants.

Conclusion: WiCV@CVPR 2025 successfully documented its impact and evolution, serving as a valuable reference for future initiatives aimed at promoting diversity, equity, and inclusion in AI and computer vision.

Abstract: The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.

</details>


### [14] [Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation](https://arxiv.org/abs/2511.08809)
*Abu Taib Mohammed Shahjahan,A. Ben Hamza*

Main category: cs.CV

TL;DR: PoseKAN是一种自适应图KAN框架，通过可学习的边函数和多跳特征聚合提升3D姿态估计性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法因局部感受野和频谱偏差限制了长距离依赖和高频细节的捕捉，尤其在遮挡和深度模糊性场景下表现不足。

Method: 提出PoseKAN，一种基于图Kolmogorov-Arnold Networks（KAN）的自适应图学习框架，通过可学习的边函数和多跳特征聚合增强模型表达能力，并结合残差PoseKAN块和全局响应归一化进一步优化特征。

Result: 在基准数据集上的实验表明，PoseKAN在3D人体姿态估计任务中优于现有最先进方法。

Conclusion: PoseKAN框架通过引入可学习的边函数和多跳特征聚合，有效提升了3D人体姿态估计的性能，特别是在处理遮挡和深度模糊性时表现优异。

Abstract: Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.

</details>


### [15] [SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph](https://arxiv.org/abs/2511.08810)
*Jingjie He,Weijie Liang,Zihan Shan,Matthew Caesar*

Main category: cs.CV

TL;DR: SIFT-Graph通过结合SIFT和GAT提取鲁棒特征，增强视觉模型对抗对抗攻击的能力，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现代深度视觉模型对密集的像素级表示高度敏感，容易受到对抗攻击的影响，传统防御策略缺乏对鲁棒视觉特征的利用。

Method: 整合了尺度不变特征变换（SIFT）关键点和图注意力网络（GAT），提取对扰动具有鲁棒性的局部结构特征，并与传统视觉模型（如Vision Transformer和CNN）融合。

Result: 初步结果表明，该方法有效提升了模型对基于梯度的白盒对抗攻击的鲁棒性，且对干净数据的准确率影响较小。

Conclusion: SIFT-Graph框架通过结合手工特征和深度学习特征，显著提升了视觉模型对抗对抗攻击的鲁棒性，同时保持了较高的干净数据准确率。

Abstract: Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.

</details>


### [16] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: 本文提出DT-NVS，一种基于transformer的3D扩散模型，用于广义新视角合成，解决了现有方法在真实场景中的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型仅关注小范围相机移动或对象中心场景，限制了在真实世界中的应用。本文旨在解决这一局限性，推广到多类别、未对齐的日常场景。

Method: 提出了一种基于transformer架构的3D感知扩散模型DT-NVS，采用图像损失在大规模真实世界视频数据集上训练，并引入了创新的相机条件策略和训练范式。

Result: 在单张输入图像的广义新视角合成任务上，DT-NVS表现优于现有方法，并展示了多样化的输出能力。

Conclusion: 该论文提出的DT-NVS模型在单张输入图像的广义新视角合成任务上表现出色，超越了当前最先进的3D感知扩散模型和确定性方法，并能生成多样化的输出。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [17] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 提出SiPF和RIAttnConv，通过全局一致的参考点增强旋转不变性，显著提升模型在任意旋转下的空间区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变性方法因受限的感知野导致全局姿态信息丢失，无法区分几何相似但空间不同的结构。

Method: 引入Shadow-informed Pose Feature (SiPF)和Rotation-invariant Attention Convolution (RIAttnConv)，结合基于Bingham分布的任务自适应阴影定位模块。

Result: 在3D分类和部件分割基准测试中，该方法显著优于现有旋转不变性方法，尤其是在需要细粒度空间区分的任务中。

Conclusion: 本文提出的SiPF和RIAttnConv方法有效解决了现有旋转不变性学习中的全局姿态信息丢失问题，显著提升了模型在任意旋转下的细粒度空间区分能力。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [18] [SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation](https://arxiv.org/abs/2511.08872)
*Hu Cui,Wenqiang Hua,Renjing Huang,Shurui Jia,Tessai Hayama*

Main category: cs.CV

TL;DR: SAS-SSM通过结构感知的时空卷积和跨步扫描策略，有效结合局部与全局姿态信息，提升3D姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SSM的方法通过手动设计的扫描操作破坏人体姿态的固有空间结构，难以捕捉复杂的姿态依赖关系。

Method: 提出Skeleton Structure-Aware Stride SSM（SAS-SSM），结合结构感知的时空卷积和跨步扫描策略，动态捕获关节间的局部交互并构建多尺度全局结构表示。

Result: SasMamba在3D姿态估计任务中表现优异，参数量显著少于现有混合模型。

Conclusion: SasMamba模型通过SAS-SSM架构，在保持线性计算复杂度的同时，显著提升了3D人体姿态估计的性能，且参数量较少。

Abstract: Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.

</details>


### [19] [Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks](https://arxiv.org/abs/2511.08883)
*Cheng Wang,Shuisheng Zhou,Fengjiao Peng,Jin Sheng,Feng Ye,Yinli Dong*

Main category: cs.CV

TL;DR: 提出了一种基于ViT的多重融合增强块（MFAVBs），通过显式融合正对特征提升聚类性能，实验证明其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习网络通过参数共享或动量更新隐式交互，可能未充分利用正对的互补性和相似性来提取聚类特征。

Method: 设计了一种基于ViT的多重融合增强块（MFAVBs），通过显式融合正对的学习特征，并利用CLIP预训练模型提取的特征进行数据预处理。

Result: MFAVBs在聚类性能上优于现有技术。

Conclusion: MFAVBs作为对比聚类的骨干网络，在七个公共数据集上的实验表明，其在聚类性能上优于现有技术。

Abstract: In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.

</details>


### [20] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 研究提出基于EfficientNet的深度学习方法，成功分类GBM六种病理区域，但泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 自动化、准确识别GBM组织病理学子区域有助于大规模形态学理解疾病。

Method: 采用四步深度学习策略，基于EfficientNet架构（B0-B4）对六种GBM组织病理学区域进行分类。

Result: EfficientNet-B1和B4在训练集上F1得分达0.98，但在验证和测试集上表现下降（F1分别为0.546和0.517）。

Conclusion: 该研究通过深度学习方法在GBM组织病理学区域分类上取得了显著成果，但模型在新数据上的泛化能力仍需改进。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [21] [Improving VisNet for Object Recognition](https://arxiv.org/abs/2511.08897)
*Mehdi Fatan Serj,C. Alejandro Parraga,Xavier Otazu*

Main category: cs.CV

TL;DR: 研究探索了生物启发的VisNet模型及其增强版本在物体识别和对称分类中的表现，结果显示其准确率显著提升，验证了模型的适应性和生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管人类视觉系统在物体识别上表现出色，但人工系统复制类似能力仍具挑战性。本研究旨在探索生物启发的神经网络模型VisNet及其增强版本在物体识别和对称分类中的表现。

Method: 研究采用了VisNet，一种生物启发的神经网络模型，并加入了径向基函数神经元、基于马氏距离的学习和类似视网膜的预处理方法，结合赫布学习和时间连续性原则来构建不变表示。

Result: 实验结果显示，增强的VisNet变体在多个数据集（如MNIST、CIFAR10和自定义对称物体集）上的识别准确率显著优于基线模型。

Conclusion: 研究强调了VisNet及其增强版本在视觉识别中的适应性和生物学相关性，为神经科学和人工智能提供了一个强大且可解释的框架。

Abstract: Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.
  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations

</details>


### [22] [Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency](https://arxiv.org/abs/2511.08901)
*Riling Wei,Kelu Yao,Chuanguang Yang,Jin Wang,Zhuoyan Gao,Chao Li*

Main category: cs.CV

TL;DR: 论文提出SemBridge框架，解决不对称跨模态知识蒸馏中的知识传输问题，通过动态选择和语义对齐模块，在多个数据集上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中由于配对模态的有限可用性，对称跨模态知识蒸馏（SCKD）的应用受到限制。因此，研究在弱语义一致性下的不对称跨模态知识蒸馏（ACKD）以桥接语义重叠有限的模态。

Method: 论文提出了SemBridge框架，包括学生友好匹配模块（利用自监督学习获取基于语义的知识并为每个学生样本提供个性化指导）和语义感知知识对齐模块（通过拉格朗日优化寻找最优传输路径）。

Result: SemBridge框架在6种不同模型架构和多个数据集上相比7种现有方法取得了最先进的性能。

Conclusion: 该论文提出了一个名为SemBridge的框架，通过整合学生友好匹配模块和语义感知知识对齐模块，有效解决了不对称跨模态知识蒸馏中的知识传输成本问题，并在多个数据集和模型架构上实现了最先进的性能。

Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.

</details>


### [23] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出了一种融合视觉预测和LLM结构先验的半监督检测框架，显著提升了文档布局理解的性能，尤其在轻量级和预训练模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管半监督学习取得了进展，文档布局理解仍需要大量数据。本文旨在通过融合视觉预测和文本预训练LLM的结构先验来提升性能。

Method: 通过OCR-LLM流程推断层次化区域，并结合教师检测器输出，通过逆方差融合生成优化的伪标签。

Result: 在轻量级SwiftFormer骨干网络（26M参数）上，仅使用5%的标签实现了88.2±0.3 AP；应用于预训练的LayoutLMv3（133M参数）时，达到89.7±0.4 AP，超越了标准半监督学习的LayoutLMv3（89.1±0.4 AP）。

Conclusion: LLM结构先验信息对于轻量级和预训练架构均具有互补性，能够显著提升半监督学习的性能，且开源LLM可实现隐私保护的部署。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [24] [Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection](https://arxiv.org/abs/2511.08904)
*Yating Liu,Yan Lu*

Main category: cs.CV

TL;DR: 提出CCDF框架，结合CC和SC模块，有效解决生成器过拟合问题，提升无监督遥感变化检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法通过生成器网络进行重建，但易因生成器过拟合导致性能不佳。

Method: 提出了一种新颖的一致性变化检测框架（CCDF），包含Cycle Consistency（CC）模块和Semantic Consistency（SC）模块，分别用于减少生成器过拟合和实现细节重建。

Result: 实验表明，CCDF框架在无监督遥感变化检测任务中优于其他先进方法。

Conclusion: 本文提出的CCDF框架通过引入Cycle Consistency（CC）模块和Semantic Consistency（SC）模块，有效解决了生成器过拟合问题，并在无监督遥感变化检测任务中取得了优于现有方法的性能。

Abstract: Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.

</details>


### [25] [HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing](https://arxiv.org/abs/2511.08908)
*Shuji Ono*

Main category: cs.CV

TL;DR: HitoMi-Cam是一种基于光谱反射的轻量级人体检测方法，适用于边缘设备，在形状不可预测的场景（如灾难救援）中表现优于CNN，且能实时运行。


<details>
  <summary>Details</summary>
Motivation: 基于CNN的目标检测存在形状依赖性，对于未包含在训练数据中的姿势性能下降。本研究旨在通过光谱方法解决这一局限性。

Method: 本研究实现了HitoMi-Cam，一种轻量级且形状无关的人体检测方法，利用服装的光谱反射特性。系统在无GPU的资源受限边缘设备上实现，以评估其实际可行性。

Result: 在物理硬件上评估表明，处理速度可达23.2 fps（253x190像素），适用于实时应用。在模拟搜救场景中，HitoMi-Cam的平均精度（AP）达93.5%，优于对比的CNN模型（最佳AP为53.8%），且误报率极低。

Conclusion: HitoMi-Cam方法并非旨在替代基于CNN的检测器，而是在特定条件下作为补充工具。结果表明，在形状不可预测的真实环境中（如灾难救援），基于光谱的人体检测可以成为边缘设备上实时操作的可行选择。

Abstract: While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.

</details>


### [26] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: NES通过合成图像、过滤负实体和注意力抑制，有效减少零样本图像描述中的幻觉，提升跨领域性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本训练在零样本图像描述（ZIC）中跨领域泛化能力差及幻觉内容多的问题，特别是在检索方法可能加剧幻觉的情况下。

Method: 提出了负实体抑制（NES）方法，包括三个步骤：使用合成图像确保训练和推理中的一致性图像到文本检索；过滤检索内容中的负实体以提高准确性；利用识别的负实体进行注意力级抑制以减少幻觉特征的影响。

Result: 在多个基准测试中，NES在保持领域内性能的同时，显著提升了跨领域迁移能力并降低了幻觉率，实现了ZIC的新最先进成果。

Conclusion: NES方法在保持领域内竞争力的同时，显著提升了跨领域泛化能力并减少了幻觉内容，为零样本图像描述（ZIC）设定了新的最先进效果。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [27] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: SPEED-Q是一种针对小型视觉语言模型低比特量化的新框架，通过分阶段处理和蒸馏增强策略，显著提升了量化精度和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，量化是部署视觉语言模型的有效方法，但现有研究很少探索对1B到2B参数模型的激进量化。

Method: 提出了SPEED-Q框架，包括分阶段敏感性自适应机制和蒸馏增强量化策略，以协调不同模态的性能并稳定训练过程。

Result: SPEED-Q在2比特设置下比现有量化方法精度提升高达6倍，并在2比特和4比特设置下均优于现有方法。

Conclusion: SPEED-Q成功解决了视觉语言模型（VLM）在边缘设备上低比特量化中的两大关键障碍，显著提升了量化精度和训练稳定性，并在多个基准测试中优于现有方法。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [28] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 提出Diff-FCHM，一种基于机器视觉导向的协作压缩方法，显著提升压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有协作压缩方法主要基于人类视觉压缩流程，在结合机器视觉压缩时存在复杂度和比特率不足的问题，而机器视觉仅关注核心区域，所需信息量远少于人类视觉。

Method: 提出了一种基于机器视觉导向的协作压缩方法Diff-FCHM，采用即插即用的可变比特率策略，并逐步聚合机器视觉压缩的语义，同时利用扩散先验恢复高保真细节。

Result: 实验结果表明Diff-FCHM在机器视觉和人类视觉压缩方面均显著优于现有方法。

Conclusion: Diff-FCHM在机器视觉和人类视觉压缩方面均表现出色，验证了其优越性能，代码将在接受后发布。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [29] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 本文提出分层蒸馏框架，结合轨迹和基于分布的蒸馏方法，优化训练过程，实现高保真单步扩散模型，性能优越。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的推理延迟是其实时应用的主要障碍。尽管轨迹和基于分布的步蒸馏方法提供了解决方案，但它们存在固有的权衡问题，需要一种更综合的方法来克服各自的局限性。

Method: 该方法提出了分层蒸馏框架，首先利用轨迹蒸馏建立结构‘草图’，然后通过基于分布的细化阶段进行优化。此外，引入了自适应加权判别器（AWD）以动态分配权重，专注于局部缺陷，实现高效细节优化。

Result: 在ImageNet 256×256数据集上，单步模型的FID达到2.26，与250步教师模型相媲美。在高分辨率文本到图像MJHQ基准测试中也取得了有希望的结果，证明了其通用性。

Conclusion: 本文提出的分层蒸馏（HD）框架通过结合轨迹蒸馏和基于分布的蒸馏方法，为高保真单步扩散模型建立了新的范式，展示了在多种任务上的最先进性能。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [30] [Boosting Adversarial Transferability via Ensemble Non-Attention](https://arxiv.org/abs/2511.08937)
*Yipeng Zou,Qin Liu,Jie Wu,Yu Peng,Guo Chen,Hui Zhou,Guanghui Ye*

Main category: cs.CV

TL;DR: NAMEA通过整合非注意力区域梯度，显著提升跨架构对抗迁移性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异构模型的梯度更新方向差异大，导致集成模型梯度方差难以降低，影响攻击性能。

Method: 设计了一种新颖的集成攻击方法NAMEA，首次将非注意力区域的梯度融入迭代优化过程，并通过元学习合并梯度。

Result: 在ImageNet数据集上，NAMEA平均比AdaEA和SMER分别高出15.0%和9.6%。

Conclusion: NAMEA通过整合异构模型的非注意力区域梯度，显著提升了跨架构对抗迁移性，为集成攻击提供了新思路。

Abstract: Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.

</details>


### [31] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: BRHVC通过BMC和BCF优化双向参考帧利用，性能超越现有神经视频压缩方法和传统编码VTM-RA。


<details>
  <summary>Details</summary>
Motivation: 虽然神经B帧视频压缩（NBVC）能利用双向参考帧提升性能，但其分层编码可能导致连续时间预测复杂化，尤其是帧跨度较大时，双向参考帧的贡献不平衡。

Method: 提出的BRHVC方法包括双向运动收敛（BMC）和双向上下文融合（BCF），前者优化运动补偿，后者根据运动补偿精度显式建模参考上下文的权重。

Result: 实验结果显示BRHVC在HEVC数据集上优于现有神经视频压缩方法，甚至超越了传统编码VTM-RA（随机访问配置）。

Conclusion: BRHVC方法通过BMC和BCF有效平衡双向参考帧的利用，显著提升了神经B帧视频压缩的性能，甚至超越了传统编码方法VTM-RA。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [32] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++：一种基于八叉树Transformer的LiDAR地点识别框架，在复杂环境中实现高精度和高效率定位。


<details>
  <summary>Details</summary>
Motivation: 解决森林和城市环境中因杂波、自相似性和视角变化导致的LiDAR地点识别和定位挑战。

Method: 采用基于八叉树的Transformer提取多粒度层次局部描述符，并结合可学习的多尺度几何验证模块优化重排序。

Result: 在公开数据集上表现优于现有方法，Recall@1平均达90.7%，定位误差低于2米和5度的比例为97.2%。

Conclusion: HOTFLoc++在LiDAR地点识别和6-DoF度量定位方面表现出色，尤其在复杂环境中具有高鲁棒性和高效率。

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [33] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: FGM-HD框架通过引入Hausdorff维度和优化策略，显著提升了生成图像的多样性而不牺牲质量。


<details>
  <summary>Details</summary>
Motivation: FGMs在生成高质量图像方面表现优异，但其固有的自相似性限制了输出图像的多样性。为了在保持视觉质量的同时提升多样性，本文探索了HD这一量化结构复杂度的概念。

Method: 提出了一种基于Hausdorff维度（HD）的新方法，包括可学习的HD估计方法、基于HD的损失函数和单调动量驱动的调度策略，以及HD引导的拒绝采样策略。

Result: 在ImageNet数据集上的实验表明，FGM-HD框架相比原始FGMs实现了39%的多样性提升，同时保持了相当的图像质量。

Conclusion: 本文提出的FGM-HD框架通过引入Hausdorff维度（HD）并采用基于HD的损失函数和拒绝采样策略，显著提升了Fractal Generative Models（FGMs）生成图像的多样性，同时保持了高视觉质量。这是首次将HD概念引入FGM的研究，为FGM的发展提供了理论贡献。

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [34] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: AuthSig是一种基于生成模型和水印的静态电子签名框架，通过风格嵌入调制隐式编码水印位，解决了静态签名易被复制的问题，实验验证了其高准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着无纸化办公的普及，静态扫描签名因其便利性仍广泛使用，但其几乎丧失了认证属性，无法可靠验证且易被恶意复制和重用。

Method: AuthSig采用生成模型和水印技术，通过精细调节风格嵌入来隐式编码水印位，并引入关键点驱动的数据增强策略以提高风格多样性。

Result: 实验结果显示，AuthSig在数字域失真和签名特定退化下提取准确率超过98%，且在打印扫描场景下仍保持有效。

Conclusion: AuthSig成功解决了静态电子签名易被恶意复制和重用的安全问题，通过生成模型和水印技术实现了签名与认证信息的绑定，实验验证了其高提取准确率和在打印扫描场景下的有效性。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [35] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: CoDR框架通过核心集和双检索优化演示选择，提升大型视觉语言模型的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统演示选择策略在效率和效果上难以平衡，需解决NP-hard的演示选择问题。

Method: 提出了一种名为核心集双检索（CoDR）的新框架，采用聚类剪枝方法构建多样化核心集，并开发双检索机制以平衡全局选择和效率。

Result: 实验结果表明，CoDR方法在ICL性能上显著优于现有策略。

Conclusion: CoDR框架通过核心集构建和双检索机制，显著提升了ICL的性能，为高效且有效的演示选择提供了稳健解决方案。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [36] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: WDT-MD框架通过创新设计解决了扩散基方法在微动脉瘤检测中的问题，实验证明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决扩散基异常检测在微动脉瘤筛查中的三个主要限制：身份映射、高误报率和正常特征重建不佳。

Method: 提出了一种基于小波扩散Transformer的框架（WDT-MD），包含噪声编码图像条件机制、伪正常模式合成和多尺度小波分析。

Result: 在IDRiD和e-ophtha MA数据集上的实验表明，WDT-MD在检测性能上超越了现有最先进方法。

Conclusion: WDT-MD框架在像素级和图像级微动脉瘤检测中均优于现有方法，显著提升了早期糖尿病视网膜病变筛查的潜力。

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [37] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 提出了一种结合去噪功能的变分分割模型，适用于噪声和强度不均匀的图像，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对噪声严重和强度不均匀的图像，传统分割方法表现不佳，因此需要一种更鲁棒的模型。

Method: 结合了去噪项的变分分割模型，包括I-散度项和自适应TV正则化器，使用RMSAV方案进行高效优化。

Result: 在多种噪声类型和强度不均匀的图像上，模型显著提高了分割准确性。

Conclusion: 该论文提出的方法在合成和真实世界图像上表现出优越的准确性和鲁棒性，优于现有方法。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [38] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: T-Rex-Omni通过引入负视觉提示和动态抑制负响应，提升了开放集物体检测性能，尤其在长尾场景表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有开放集物体检测器仅依赖正提示（如文本描述或视觉示例），易受视觉相似但语义不同的干扰物影响。

Method: 提出了一种统一的视觉提示编码器，联合处理正负视觉提示；设计了训练自由的负否定计算（NNC）模块和负否定铰链（NNH）损失函数。

Result: 实验表明T-Rex-Omni在零样本检测中表现优异，显著缩小了视觉提示与文本提示方法的性能差距，在长尾场景中达到51.2 AP_r。

Conclusion: T-Rex-Omni通过引入负视觉提示和动态抑制负响应的方法，显著提升了开放集物体检测系统的性能，特别是在长尾场景中表现突出。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [39] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: Owl通过双模态注意力重加权和对比解码策略，有效减少LVLMs中的对象幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立调节视觉或文本注意力，忽视了二者交互对幻觉的影响。

Method: 提出Owl框架，基于因果图建模视觉和文本注意力的交互，引入VTACR指标量化模态贡献不平衡，并设计细粒度注意力干预机制。

Result: 在POPE和CHAIR基准上，Owl显著减少幻觉并实现最先进的忠实性表现。

Conclusion: Owl框架通过双模态注意力重加权层缓解幻觉，显著减少LVLMs中的对象幻觉，同时在POPE和CHAIR基准上保持视觉语言理解能力。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [40] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出密集跨尺度对齐模型，结合全空间相关性和最小可觉差异，显著提升无监督图像对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像对齐方法存在精度不足和计算复杂度高的问题，亟需改进。

Method: 提出了一种密集跨尺度图像对齐模型，结合全空间相关性模块和最小可觉差异技术。

Result: 实验表明，该方法在精度和效率上均优于现有先进技术。

Conclusion: 该方法通过密集跨尺度特征对齐和全空间相关性模块，显著提升了无监督图像对齐的精度和效率，并通过引入最小可觉差异进一步优化了对失真敏感区域的关注。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [41] [USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation](https://arxiv.org/abs/2511.09045)
*Penghui Niu,Taotao Cai,Jiashuai She,Yajuan Zhang,Junhua Gua,Ping Zhanga,Jungong Hane,Jianxin Li*

Main category: cs.CV

TL;DR: USF-Net提出自适应大核卷积与低复杂度注意力机制，解决云图像序列外推中的特征提取、时空依赖建模及效率问题，实验表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征提取、时空依赖建模及计算效率方面存在局限，需提升地面遥感云图像序列外推的准确性与效率。

Method: 提出USF-Net，结合自适应大核卷积、低复杂度注意力机制及编码器-解码器框架，包含USTM（SiB和TiB）和DSM模块，解码器使用DUM解决“重影效应”。

Result: 在ASI-CIS数据集上，USF-Net显著优于现有方法，平衡了预测精度与计算效率。

Conclusion: USF-Net通过自适应大核卷积和低复杂度注意力机制，在云图像序列外推任务中实现了预测精度与计算效率的优越平衡，并发布了ASI-CIS数据集。

Abstract: Ground-based remote sensing cloud image sequence extrapolation is a key research area in the development of photovoltaic power systems. However, existing approaches exhibit several limitations:(1)they primarily rely on static kernels to augment feature information, lacking adaptive mechanisms to extract features at varying resolutions dynamically;(2)temporal guidance is insufficient, leading to suboptimal modeling of long-range spatiotemporal dependencies; and(3)the quadratic computational cost of attention mechanisms is often overlooked, limiting efficiency in practical deployment. To address these challenges, we propose USF-Net, a Unified Spatiotemporal Fusion Network that integrates adaptive large-kernel convolutions and a low-complexity attention mechanism, combining temporal flow information within an encoder-decoder framework. Specifically, the encoder employs three basic layers to extract features. Followed by the USTM, which comprises:(1)a SiB equipped with a SSM that dynamically captures multi-scale contextual information, and(2)a TiB featuring a TAM that effectively models long-range temporal dependencies while maintaining computational efficiency. In addition, a DSM with a TGM is introduced to enable unified modeling of temporally guided spatiotemporal dependencies. On the decoder side, a DUM is employed to address the common "ghosting effect." It utilizes the initial temporal state as an attention operator to preserve critical motion signatures. As a key contribution, we also introduce and release the ASI-CIS dataset. Extensive experiments on ASI-CIS demonstrate that USF-Net significantly outperforms state-of-the-art methods, establishing a superior balance between prediction accuracy and computational efficiency for ground-based cloud extrapolation. The dataset and source code will be available at https://github.com/she1110/ASI-CIS.

</details>


### [42] [4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching](https://arxiv.org/abs/2511.09055)
*Xingchi Chen,Pu Wang,Xuerui Li,Chaopeng Li,Juxiang Zhou,Jianhou Gan,Dianjie Lu,Guijuan Zhang,Wenqi Ren,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 4KDehazeFlow：基于流匹配和雾感知向量场的高效UHD图像去雾方法，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决超高清晰度（UHD）图像去雾中先验方法场景适应性有限和深度学习方法计算复杂度高、颜色失真的问题。

Method: 提出4KDehazeFlow方法，基于流匹配和雾感知向量场，结合可学习3D查找表（LUT）和四阶Runge-Kutta（RK4）ODE求解器，实现高效稳定的去雾。

Result: 实验表明，4KDehazeFlow在PSNR上提升2dB，在浓雾和颜色保真度上表现优异，超越七种先进方法。

Conclusion: 4KDehazeFlow通过流匹配和雾感知向量场建模去雾过程，提供高效的数据驱动自适应非线性颜色变换，显著提升了去雾质量和计算效率，优于现有七种先进方法。

Abstract: Ultra-High-Definition (UHD) image dehazing faces challenges such as limited scene adaptability in prior-based methods and high computational complexity with color distortion in deep learning approaches. To address these issues, we propose 4KDehazeFlow, a novel method based on Flow Matching and the Haze-Aware vector field. This method models the dehazing process as a progressive optimization of continuous vector field flow, providing efficient data-driven adaptive nonlinear color transformation for high-quality dehazing. Specifically, our method has the following advantages: 1) 4KDehazeFlow is a general method compatible with various deep learning networks, without relying on any specific network architecture. 2) We propose a learnable 3D lookup table (LUT) that encodes haze transformation parameters into a compact 3D mapping matrix, enabling efficient inference through precomputed mappings. 3) We utilize a fourth-order Runge-Kutta (RK4) ordinary differential equation (ODE) solver to stably solve the dehazing flow field through an accurate step-by-step iterative method, effectively suppressing artifacts. Extensive experiments show that 4KDehazeFlow exceeds seven state-of-the-art methods. It delivers a 2dB PSNR increase and better performance in dense haze and color fidelity.

</details>


### [43] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: PAN是一种通用、交互式、长时程的世界模型，通过GLP架构结合语言模型和视频扩散，实现高质量的未来世界状态预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型缺乏因果控制、交互性和长时程一致性，而世界建模方法通常在受限领域中表现有限。PAN旨在解决这些问题，实现通用的世界模拟。

Method: PAN采用生成潜在预测（GLP）架构，结合基于大型语言模型的自回归潜在动态骨干和视频扩散解码器，支持多样领域的开放域、动作条件化模拟。

Result: 实验表明，PAN在动作条件化世界模拟、长时程预测和模拟推理方面表现优异，优于其他视频生成器和世界模型。

Conclusion: PAN作为一种通用、可交互且长时程的世界模型，通过高质量的视频模拟预测未来世界状态，结合了潜在空间推理与可实现世界动态的统一，为实现通用世界模型迈出了重要一步。

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [44] [VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering](https://arxiv.org/abs/2511.09058)
*Hai-Dang Nguyen,Minh-Anh Dang,Minh-Tan Le,Minh-Tuan Le*

Main category: cs.CV

TL;DR: 本文提出VietMEAgent，一个可解释的多模态框架，用于越南文化理解的VQA系统，结合文化对象检测和结构化程序生成，提供透明解释，支持文化教育与保护。


<details>
  <summary>Details</summary>
Motivation: 当代VQA系统在面对文化特定内容时受限，因为文化知识在训练语料中代表性不足且推理过程缺乏可解释性。

Method: 方法整合了文化对象检测主干和结构化程序生成层，构建了一个紧密耦合答案预测与解释的流程，并利用越南文化实体的知识库和双模态解释模块。

Result: 构建了越南文化VQA数据集，展示了基于编程的方法在文化AI中的实用性，系统提供了透明的解释，揭示了计算理据和文化背景。

Conclusion: VietMEAgent框架通过结合文化对象检测和结构化程序生成层，为越南文化理解的VQA系统提供了可解释的解决方案，支持教育和文化保护。

Abstract: Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.

</details>


### [45] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: DOC通过正交梯度和动量更新提升对抗防御的多样性和覆盖范围，显著增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御方法（如TTC）因优化目标单一，生成的对抗样本多样性不足，难以全面中和多种扰动。

Method: 提出Directional Orthogonal Counterattack (DOC)，结合正交梯度方向和动量更新，扩展对抗空间探索，增加扰动多样性。

Result: 在16个数据集上的实验表明，DOC在多种攻击下提高了对抗鲁棒性，同时保持较高的干净准确率。

Conclusion: DOC通过引入正交梯度方向和基于动量的更新，增强了对抗样本的多样性，从而提高了对抗鲁棒性，同时在多种攻击下保持竞争力。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [46] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 论文探索了组合增量学习（CompIL）在组合零样本学习任务中的应用，提出了伪重放框架并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据不断涌现，组合可能性近乎无限、长尾且不完全可见，因此需要模型能够逐步提升组合泛化能力。

Method: 提出了一个伪重放框架，利用视觉合成器合成学习过的组合的视觉表示，并通过语言基元蒸馏机制在学习过程中保持对齐的基元表示。

Result: 通过大量实验验证了所提出框架的有效性。

Conclusion: 论文提出了一个伪重放框架，通过视觉合成器和语言基元蒸馏机制，有效地在增量学习中保持和提高组合泛化能力。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [47] [Ultra-Light Test-Time Adaptation for Vision--Language Models](https://arxiv.org/abs/2511.09101)
*Byunghyun Kim*

Main category: cs.CV

TL;DR: UL-TTA是一种无需训练和反向传播的轻量级测试时适应框架，通过调整logit级别参数，显著提升了VLMs在领域偏移下的性能和校准效果。


<details>
  <summary>Details</summary>
Motivation: 针对领域偏移下Vision-Language Models（VLMs）的特征漂移、类别先验不匹配和严重校准不足问题，现有测试时适应（TTA）方法通常需要大量计算资源，不适用于流式或边缘场景。

Method: UL-TTA采用在线EM风格的方法，包括选择性样本过滤、封闭式贝叶斯更新原型和先验、解耦的温度参数以及轻量级防护措施。

Result: UL-TTA在多个大规模跨域和OOD基准测试中显著提高了top-1准确性（平均比零样本CLIP高4.7个百分点），同时将ECE降低了20-30%，且延迟开销小于8%。

Conclusion: UL-TTA通过仅调整logit级别的参数（如类别原型、类别先验和温度），在保持骨干网络冻结的情况下，实现了在领域偏移下最先进的准确性与校准平衡。

Abstract: Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.

</details>


### [48] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 研究引入DKDS数据集以解决OCR在处理带噪音的古日语草书体文献时的不足，提供了文本/印章检测和文档二值化的基线结果。


<details>
  <summary>Details</summary>
Motivation: 由于现有OCR方法在处理带有噪音（如文献退化、印章）的古日语草书体文献时效果不佳，且缺乏专门的数据集，因此需要新的数据集和基准测试。

Method: 研究团队构建了DKDS数据集，并采用YOLO模型进行文本和印章检测，同时结合传统算法、K-means聚类和GAN方法进行文档二值化。

Result: 研究提供了DKDS数据集及其构建过程，并在两种基准测试中展示了基线结果，包括YOLO模型的检测效果和多种二值化方法的性能。

Conclusion: 该研究通过引入DKDS数据集填补了现有OCR技术在处理带有噪音的古日语草书体文献时的空白，并通过两种基准测试（文本和印章检测、文档二值化）提供了基线结果。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [49] [PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping](https://arxiv.org/abs/2511.09130)
*ChunLiang Wu,Tsunhua Yang,Hungying Chen*

Main category: cs.CV

TL;DR: PIFF是一种物理信息、基于流的生成神经网络，用于实时洪水深度估计，结合数据驱动学习和物理约束，替代传统模拟方法。


<details>
  <summary>Details</summary>
Motivation: 传统洪水测绘方法在效率和可靠性上存在局限，需要一种更高效、可靠的方法来实时估计洪水深度。

Method: 基于图像到图像的生成框架，结合简化的淹没模型（SPM）和基于变压器的降雨编码器，PIFF整合了物理约束和数据驱动学习。

Result: 在台湾台南26公里研究区域的182种降雨情景下，PIFF展现了其在洪水预测和响应中的有效性。

Conclusion: PIFF提供了一种高效、数据驱动的方法，用于实时洪水深度估计，成功替代了传统的高成本模拟方法。

Abstract: Flood mapping is crucial for assessing and mitigating flood impacts, yet traditional methods like numerical modeling and aerial photography face limitations in efficiency and reliability. To address these challenges, we propose PIFF, a physics-informed, flow-based generative neural network for near real-time flood depth estimation. Built on an image-to-image generative framework, it efficiently maps Digital Elevation Models (DEM) to flood depth predictions. The model is conditioned on a simplified inundation model (SPM) that embeds hydrodynamic priors into the training process. Additionally, a transformer-based rainfall encoder captures temporal dependencies in precipitation. Integrating physics-informed constraints with data-driven learning, PIFF captures the causal relationships between rainfall, topography, SPM, and flooding, replacing costly simulations with accurate, real-time flood maps. Using a 26 km study area in Tainan, Taiwan, with 182 rainfall scenarios ranging from 24 mm to 720 mm over 24 hours, our results demonstrate that PIFF offers an effective, data-driven alternative for flood prediction and response.

</details>


### [50] [MACEval: A Multi-Agent Continual Evaluation Network for Large Models](https://arxiv.org/abs/2511.09139)
*Zijian Chen,Yuze Sun,Yuan Tian,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: MACEval是一种动态评估大型模型的多代理网络，解决了现有基准的局限，实验证明其自动化、高效和经济的特点。


<details>
  <summary>Details</summary>
Motivation: 现有的大型模型评估基准存在封闭性、数据污染、维护困难等问题，MACEval旨在解决这些问题，提高评估的可信度和适应性。

Method: MACEval采用交互式和自主评估模式，通过角色分配、过程中数据生成和评估路由的级联代理网络进行动态评估。

Result: 在9个开放式任务和23个大型模型上的实验表明，MACEval具有自动化、高效经济、灵活可扩展等优势。

Conclusion: MACEval提出了一种新的动态评估大型模型的方法，通过多代理网络实现自动化和持续评估，具有高效、经济和灵活的特点，为未来大型模型评估提供了新方向。

Abstract: Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.

</details>


### [51] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: PressTrack-HMR is a pipeline for multi-person human mesh recovery from pressure signals, addressing occlusion and privacy issues. It segments individual signals and achieves competitive accuracy, with a new dataset (MIP) released.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-based HMR methods face limitations in real-world scenarios due to occlusions, lighting, and privacy concerns. Tactile mats offer an occlusion-free and privacy-friendly alternative, but distinguishing intermingled pressure signals in multi-person scenarios remains a challenge.

Method: The paper presents a top-down pipeline called PressTrack-HMR, which uses a tracking-by-detection strategy to segment individual pressure signals from raw data and performs human mesh recovery (HMR) for each signal.

Result: The method achieves 89.2 mm MPJPE and 112.6 mm WA-MPJPE100, showcasing its effectiveness in multi-person HMR using pressure data.

Conclusion: PressTrack-HMR demonstrates the potential of tactile mats for privacy-preserving multi-person action recognition, achieving competitive performance metrics (89.2 mm MPJPE and 112.6 mm WA-MPJPE100). The dataset and code are publicly available for further research.

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [52] [DBINDS -- Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: DBINDS通过扩散模型反演分析潜在空间动态，显著提升AI生成视频检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有检测器依赖像素级视觉线索且泛化能力差，面对快速发展的AI生成视频技术，亟需更高效的检测方法。

Method: 提出DBINDS检测器，基于扩散模型反演分析初始噪声序列差异，构建INDS并提取多域多尺度特征，结合特征优化和贝叶斯搜索调优的LightGBM分类器。

Result: 在GenVidBench上，DBINDS（仅用单一生成器训练）展现出强跨生成器性能，验证了其泛化性和鲁棒性。

Conclusion: DBINDS通过分析潜在空间动态而非像素级视觉线索，显著提升了AI生成视频检测的泛化能力和鲁棒性，在有限数据设置下表现出色。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [53] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 提出了DermBench和DermEval框架，用于可靠评估多模态大语言模型在皮肤病诊断中的表现，实验显示其与专家评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在皮肤病诊断中可靠评估的瓶颈，确保临床部署的负责任性。

Method: 通过构建DermBench（包含4000张皮肤病图像与专家认证诊断叙述的基准）和DermEval（无参考的多模态评估器），结合LLM评分和结构化批评，实现了对多模态模型的全面评估。

Result: 在4500个案例的实验中，DermBench和DermEval与专家评分的平均偏差分别为0.251和0.117（满分5分），验证了其评估的准确性和一致性。

Conclusion: DermBench和DermEval评估框架在多种多模态大语言模型上实现了与专家评分的高度一致，为临床部署提供了可靠且可扩展的评估手段。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [54] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: TACO框架通过自验证和置信度校准，有效减少MLLMs的幻觉问题，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）常出现幻觉问题，尤其是对象存在性、属性或关系错误，影响其可靠性。

Method: TACO将响应分解为原子查询，通过改写减少对措辞的敏感性，并使用自一致性（黑盒）或自置信度（灰盒）聚合估计置信度，最后用语言模型精炼答案。

Result: 在五个基准测试（POPE、MME、HallusionBench、AMBER和MM-Hal Bench）和两种MLLM（LLaVA-1.5-7B和CogVLM2）上的实验表明，TACO优于直接提示和视觉对比解码，减少了系统性偏差并改善了置信度校准。

Conclusion: TACO框架通过自验证和置信度校准，显著减少了MLLMs中的幻觉问题，提升了模型的可靠性和置信度校准效果。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [55] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: DensiCrafter通过优化密度场生成轻量自支撑3D空心结构，材料减重43%，实际打印验证可行。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D生成模型忽略物理约束和可制造性的问题，专注于生成轻量且自支撑的3D结构。

Method: 采用连续密度场优化方法，结合三个可微分、物理约束且无需模拟的损失项，以及质量正则化和受限优化域。

Result: 在文本到3D任务中实现材料质量减少43%，同时保持高几何保真度和稳定性。

Conclusion: DensiCrafter successfully生成轻量且自支撑的3D空心结构，通过优化密度场实现材料减重43%，并在实际3D打印中验证了其可制造性和稳定性。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [56] [Spatial Information Bottleneck for Interpretable Visual Recognition](https://arxiv.org/abs/2511.09239)
*Kaixiang Shu,Kai Meng,Junqin Luo*

Main category: cs.CV

TL;DR: 论文提出空间信息瓶颈（S-IB），通过信息论视角优化梯度归因，显著提升模型解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络通常学习空间纠缠的表示，混淆了区分性前景特征与虚假背景相关性，从而削弱了模型的解释性和鲁棒性。为了解决这一问题，作者从信息论的角度提出了对梯度归因的新理解框架。

Method: 作者提出了一种编码-解码视角：前向传播将输入编码到类别空间，而反向传播中的VJP将这种编码解码回特征空间。基于此，设计了空间信息瓶颈（S-IB），通过最大化前景VJP与输入之间的互信息，同时最小化背景区域的互信息，促使网络仅在类别相关的空间区域编码信息。

Result: 在五个基准测试上的实验表明，S-IB在六种解释方法上均实现了普遍改进，实现了更好的前景集中和背景抑制，同时保持了分类准确性的提升。

Conclusion: 论文提出了一种基于信息论的梯度归因理解框架，并证明了在温和条件下，反向传播中的向量-雅可比积（VJP）形成了输入特征相对于类标签的最小充分统计量。通过这种视角，作者提出了空间信息瓶颈（S-IB）方法，以空间解耦信息流，从而提升模型的解释性和鲁棒性。实验结果表明，S-IB在多种解释方法上均实现了显著的性能提升。

Abstract: Deep neural networks typically learn spatially entangled representations that conflate discriminative foreground features with spurious background correlations, thereby undermining model interpretability and robustness. We propose a novel understanding framework for gradient-based attribution from an information-theoretic perspective. We prove that, under mild conditions, the Vector-Jacobian Products (VJP) computed during backpropagation form minimal sufficient statistics of input features with respect to class labels. Motivated by this finding, we propose an encoding-decoding perspective : forward propagation encodes inputs into class space, while VJP in backpropagation decodes this encoding back to feature space. Therefore, we propose Spatial Information Bottleneck (S-IB) to spatially disentangle information flow. By maximizing mutual information between foreground VJP and inputs while minimizing mutual information in background regions, S-IB encourages networks to encode information only in class-relevant spatial regions. Since post-hoc explanation methods fundamentally derive from VJP computations, directly optimizing VJP's spatial structure during training improves visualization quality across diverse explanation paradigms. Experiments on five benchmarks demonstrate universal improvements across six explanation methods, achieving better foreground concentration and background suppression without method-specific tuning, alongside consistent classification accuracy gains.

</details>


### [57] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的基于视觉的支气管镜导航框架和新的合成基准数据集，实现了跨领域的精确定位，无需领域特定适应。


<details>
  <summary>Details</summary>
Motivation: 由于呼吸运动、解剖变异和CT与身体的偏差导致的变形和不对齐，支气管镜尖端在术中的精确定位仍然具有挑战性。现有的基于视觉的方法往往无法跨领域和患者泛化，导致残余对齐误差。

Method: 提出了一个基于视觉的姿态优化框架，用于术中内窥镜视图与术前CT解剖之间的帧级2D-3D配准。通过微调的模态和领域不变编码器直接计算真实内窥镜RGB帧与CT渲染深度图之间的相似性，而可微分渲染模块通过深度一致性迭代优化相机姿态。

Result: 仅在不同于基准的合成数据上训练，模型实现了平均2.65毫米的平移误差和0.19弧度的旋转误差，展示了准确且稳定的定位。在真实患者数据上的定性结果进一步确认了强大的跨领域泛化能力，无需领域特定适应即可实现一致的帧级2D-3D对齐。

Conclusion: 该框架通过基于视觉的迭代优化实现了鲁棒、领域不变的定位，同时新的基准数据集为基于视觉的支气管镜导航的标准化进展提供了基础。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [58] [GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow](https://arxiv.org/abs/2511.09272)
*Rui Wan,Qi Zheng,Ruoyu Zhang,Bu Chen,Jiaming Liu,Min Li,Minge Jing,Jinjia Zhou,Yibo Fan*

Main category: cs.CV

TL;DR: 提出FPGA-oriented AGC部署方案，通过软硬件协同设计和优化策略，显著提升边缘设备能效。


<details>
  <summary>Details</summary>
Motivation: AGC在资源受限的边缘设备上部署面临参数多、算法动态适应性差、以及高功耗等挑战。

Method: 通过分析AGC算法，采用网络压缩方法（如后训练静态量化和层融合技术），并设计了一个基于协处理器范式的重叠加速器，通过软硬件协同设计执行计算。硬件处理单元包括卷积、网格采样、上采样等引擎，并采用了双缓冲流水线和循环展开等并行化优化策略。

Result: 在PYNQ-Z1平台上建立的AGC FPGA原型，能效分别比商用CPU和GPU高24.9倍和4.1倍，每像素重建仅需11.7微焦耳。

Conclusion: 本研究首次提出了一种面向FPGA的AGC部署方案，显著提升了边缘计算视频服务的能效。

Abstract: The Animation-based Generative Codec (AGC) is an emerging paradigm for talking-face video compression. However, deploying its intricate decoder on resource and power-constrained edge devices presents challenges due to numerous parameters, the inflexibility to adapt to dynamically evolving algorithms, and the high power consumption induced by extensive computations and data transmission. This paper for the first time proposes a novel field programmable gate arrays (FPGAs)-oriented AGC deployment scheme for edge-computing video services. Initially, we analyze the AGC algorithm and employ network compression methods including post-training static quantization and layer fusion techniques. Subsequently, we design an overlapped accelerator utilizing the co-processor paradigm to perform computations through software-hardware co-design. The hardware processing unit comprises engines such as convolution, grid sampling, upsample, etc. Parallelization optimization strategies like double-buffered pipelines and loop unrolling are employed to fully exploit the resources of FPGA. Ultimately, we establish an AGC FPGA prototype on the PYNQ-Z1 platform using the proposed scheme, achieving \textbf{24.9$\times$} and \textbf{4.1$\times$} higher energy efficiency against commercial Central Processing Unit (CPU) and Graphic Processing Unit (GPU), respectively. Specifically, only \textbf{11.7} microjoules ($\upmu$J) are required for one pixel reconstructed by this FPGA system.

</details>


### [59] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: DreamPose3D是一个基于扩散的3D姿态估计框架，结合动作感知和时间想象力，显著提升性能并有效处理模糊运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖几何线索且独立预测3D姿态，难以处理模糊运动并泛化到真实场景。受人类理解和预测运动的启发，提出新框架。

Method: 提出了一个基于扩散的框架DreamPose3D，包含动作提示动态条件去噪、表示编码器（引入运动关节亲和力到注意力机制）和幻觉姿态解码器。

Result: 在Human3.6M和MPI-3DHP数据集上实现了最先进的性能，广播棒球数据集上也表现出强鲁棒性。

Conclusion: DreamPose3D通过结合动作感知推理和时间想象力，显著提升了3D人体姿态估计的准确性和鲁棒性，尤其在处理模糊运动和真实场景时表现优异。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>


### [60] [Deep Learning for Metabolic Rate Estimation from Biosignals: A Comparative Study of Architectures and Signal Selection](https://arxiv.org/abs/2511.09276)
*Sarvenaz Babakhani,David Remy,Alina Roitberg*

Main category: cs.CV

TL;DR: 本研究比较了经典与深度学习模型在能量消耗估计中的表现，发现分钟通气量是最佳单个信号，Transformer表现最优，但个体差异显著需自适应建模。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法很少区分神经架构与信号选择的作用。本研究旨在系统评估这两方面，以改进能量消耗估计的准确性。

Method: 本研究系统评估了经典基线方法与新型神经架构在不同信号（单个信号、信号对和分组传感器输入）上的表现。采用了Transformer、CNN和ResNet等模型，并对比了它们在多样体力活动中的表现。

Result: Transformer模型在分钟通气量信号上实现了最低的RMSE（0.87 W/kg）。低强度活动的RMSE最低（0.29 W/kg；NRMSE=0.04），而高强度任务的RMSE较大但归一化误差更具可比性。

Conclusion: 研究表明，分钟通气量是最具预测性的单个信号，而Transformer模型在所有活动中实现了最低的均方根误差（RMSE）。此外，成对和分组信号（如Hexoskin智能衬衫的五种信号）为CNN和ResNet等快速模型提供了良好替代方案。然而，高强度任务的RMSE较高，但归一化误差更具可比性。个体间差异显著，强调了自适应建模策略的必要性。

Abstract: Energy expenditure estimation aims to infer human metabolic rate from physiological signals such as heart rate, respiration, or accelerometer data, and has been studied primarily with classical regression methods. The few existing deep learning approaches rarely disentangle the role of neural architecture from that of signal choice. In this work, we systematically evaluate both aspects. We compare classical baselines with newer neural architectures across single signals, signal pairs, and grouped sensor inputs for diverse physical activities. Our results show that minute ventilation is the most predictive individual signal, with a transformer model achieving the lowest root mean square error (RMSE) of 0.87 W/kg across all activities. Paired and grouped signals, such as those from the Hexoskin smart shirt (five signals), offer good alternatives for faster models like CNN and ResNet with attention. Per-activity evaluation revealed mixed outcomes: notably better results in low-intensity activities (RMSE down to 0.29 W/kg; NRMSE = 0.04), while higher-intensity tasks showed larger RMSE but more comparable normalized errors. Finally, subject-level analysis highlights strong inter-individual variability, motivating the need for adaptive modeling strategies. Our code and models will be publicly available at https://github.com/Sarvibabakhani/deeplearning-biosignals-ee .

</details>


### [61] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: RichKD通过融合传统教师模型与CLIP的多模态知识，提升了知识蒸馏的效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法多依赖单一视觉模态，忽视了跨模态表征的潜力。本研究探索CLIP的视觉-语言知识作为补充监督源，以提升知识多样性。

Method: RichKD框架结合了传统教师的logits和特征与CLIP的多提示文本指导，通过多模态监督融合，捕捉数据集特定和语义丰富的视觉线索。

Result: RichKD在多个基准测试中表现优异，不仅提高了预测的置信度和可靠性，还优化了整个logit分布，增强了类间一致性和蒸馏质量。

Conclusion: RichKD通过融合传统教师模型与CLIP的多模态知识，显著提升了知识蒸馏的效果，不仅在准确性上超越现有基线，还增强了模型的鲁棒性。

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [62] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: A feedback-enhanced teacher-student framework with a dual-teacher model improves error correction in semi-supervised medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: To counteract the self-reinforcing bias caused by erroneous supervision in the teacher-student paradigm for medical image segmentation, which is overlooked by existing methods.

Method: Introduces a feedback mechanism into the teacher-student framework, including a feedback attributor and receiver, and further proposes a dual-teacher feedback model for dynamic error correction.

Result: Demonstrates effectiveness in mitigating error propagation through evaluations on three medical image benchmarks.

Conclusion: The proposed feedback mechanism and dual-teacher model effectively address error propagation in semi-supervised medical image segmentation, as validated by comprehensive evaluations on three benchmarks.

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [63] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: FQ-PETR是一种针对PETRs的全量化框架，通过创新技术解决了量化中的精度和效率问题，实现了高性能部署。


<details>
  <summary>Details</summary>
Motivation: PETR及其变体在基准测试中表现出色，但因高计算成本和内存占用面临部署挑战。直接应用现有量化方法会导致严重精度下降。

Method: 提出了FQ-PETR框架，包含三个关键创新：Quantization-Friendly LiDAR-ray Position Embedding (QFPE)、Dual-Lookup Table (DULUT)和Quantization After Numerical Stabilization (QANS)。

Result: FQ-PETR在PETRs（如PETR、StreamPETR、PETRv2、MV2d）上实现了高效量化，显著降低延迟并保持高精度。

Conclusion: FQ-PETR在W8A8量化下实现了接近浮点精度（仅1%退化），同时将延迟降低高达75%，显著优于现有的PTQ和QAT基线。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [64] [Spatio-Temporal Context Learning with Temporal Difference Convolution for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.09352)
*Houzhang Fang,Shukai Guo,Qiuhuan Chen,Yi Chang,Luxin Yan*

Main category: cs.CV

TL;DR: TDCNet结合时间差分和3D卷积，通过TDC模块和时空注意力机制，显著提升移动红外小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 移动红外小目标检测在无人机监控等应用中至关重要，但由于目标特征弱和背景干扰复杂，检测仍具有挑战性。现有方法在时空特征建模上各有局限，需一种更有效的方法。

Method: 提出了一种新颖的TDC重参数化模块，包含三个并行TDC块，用于捕获不同时间范围的上下文依赖，并融合时间差分和3D卷积为统一的时空卷积表示。此外，还提出了TDC引导的时空注意力机制，通过跨注意力建模全局语义依赖。

Result: 在IRSTD-UAV和公共红外数据集上的广泛实验表明，TDCNet在移动目标检测中达到了最先进的性能。

Conclusion: TDCNet通过结合时间差分和3D卷积的时空卷积表示，显著提升了移动红外小目标检测的性能，并在复杂背景中有效抑制伪运动干扰。

Abstract: Moving infrared small target detection (IRSTD) plays a critical role in practical applications, such as surveillance of unmanned aerial vehicles (UAVs) and UAV-based search system. Moving IRSTD still remains highly challenging due to weak target features and complex background interference. Accurate spatio-temporal feature modeling is crucial for moving target detection, typically achieved through either temporal differences or spatio-temporal (3D) convolutions. Temporal difference can explicitly leverage motion cues but exhibits limited capability in extracting spatial features, whereas 3D convolution effectively represents spatio-temporal features yet lacks explicit awareness of motion dynamics along the temporal dimension. In this paper, we propose a novel moving IRSTD network (TDCNet), which effectively extracts and enhances spatio-temporal features for accurate target detection. Specifically, we introduce a novel temporal difference convolution (TDC) re-parameterization module that comprises three parallel TDC blocks designed to capture contextual dependencies across different temporal ranges. Each TDC block fuses temporal difference and 3D convolution into a unified spatio-temporal convolution representation. This re-parameterized module can effectively capture multi-scale motion contextual features while suppressing pseudo-motion clutter in complex backgrounds, significantly improving detection performance. Moreover, we propose a TDC-guided spatio-temporal attention mechanism that performs cross-attention between the spatio-temporal features from the TDC-based backbone and a parallel 3D backbone. This mechanism models their global semantic dependencies to refine the current frame's features. Extensive experiments on IRSTD-UAV and public infrared datasets demonstrate that our TDCNet achieves state-of-the-art detection performance in moving target detection.

</details>


### [65] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: Flora是一种新颖的零样本骨架动作识别方法，通过语义调整和流分类器解决现有方法的对齐和分类问题，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在点对点对齐的脆弱性和分类器的刚性边界问题，Flora旨在解决这些限制。

Method: Flora方法结合了灵活邻域感知的语义调整和开放形式的分布感知流分类器，通过跨模态几何一致性目标和噪声无关的流匹配来优化对齐和分类。

Result: 在三个基准数据集上的大量实验证明了Flora的有效性。

Conclusion: 论文提出的Flora方法在零样本骨架动作识别中表现优异，尤其在仅使用10%可见数据训练时仍能保持卓越性能。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [66] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 首个美式足球运动员检测与跟踪数据集构建，微调检测与重新识别模型显著提升跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪方法多针对日常场景或特定运动（如足球、篮球），缺乏适用于美式足球的标准数据集，导致方法间难以公平比较。

Method: 通过构建首个美式足球运动员检测与跟踪数据集，对比评估了多种检测与跟踪方法，包括微调检测模型和整合重新识别模型到跟踪系统中。

Result: 微调检测模型提升了性能，整合微调检测器和重新识别模型到跟踪系统中，相比现有方法显著提高了跟踪精度。

Conclusion: 本研究通过构建首个专用于美式足球运动员的检测与跟踪数据集，并对比评估多种方法，证明了即使在拥挤场景中也能实现准确的检测与跟踪，显著提升了跟踪精度。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [67] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: 双教师知识蒸馏框架结合ViT和CNN教师，提升轻量级CNN性能，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有CAKD方法在架构不匹配和忽视同构CNN教师价值方面的不足。

Method: 通过Discrepancy-Aware Teacher Weighting和Structure Discrepancy-Aware Distillation策略，动态融合ViT和CNN教师的预测，并学习其结构差异。

Result: 在多个基准测试中表现优于现有方法，最高在HMDB51上提升了5.95%的准确率。

Conclusion: 提出的双教师知识蒸馏框架通过结合ViT和CNN教师的优势，显著提升了轻量级CNN学生的性能，实验验证了其有效性。

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


### [68] [vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs](https://arxiv.org/abs/2511.09540)
*Minye Shao,Sihan Guo,Xinrun Li,Xingyu Miao,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: vMFCoOp通过超球流形优化对齐LLM与CLIP的语义偏差，提升生物医学小样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM和CLIP变体之间的语义错位问题，以及传统欧几里得空间优化在多模态对齐中的局限性，尤其是在复杂生物医学影像中放大模态差距和不稳定小样本适应的问题。

Method: 提出vMFCoOp框架，利用超球流形上的vMF分布逆向估计，通过统一语义锚点对齐LLM和CLIP的语义偏差。

Result: 在14个医学数据集、12种医学影像模态和13个解剖区域上表现一致优于现有方法。

Conclusion: vMFCoOp框架通过在共享超球流形上逆向估计vMF分布，统一任意LLM和CLIP骨干网络的语义偏差，实现了稳健的生物医学提示学习和卓越的小样本分类性能，展现出在准确性、泛化性和临床适用性上的优势。

Abstract: Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work will be continuously expanded to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.

</details>


### [69] [RF-DETR: Neural Architecture Search for Real-Time Detection Transformers](https://arxiv.org/abs/2511.09554)
*Isaac Robinson,Peter Robicheaux,Matvei Popov,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: RF-DETR通过轻量级专家检测变压器和NAS技术，显著提升开放词汇检测器在COCO和Roboflow100-VL上的性能，成为首个实时检测超过60 AP的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇检测器在COCO上表现优异，但在真实世界中遇到分布外类别时泛化能力不足，且直接微调大型视觉语言模型（VLM）效率低下。

Method: 采用轻量级专家检测变压器（RF-DETR）和权重共享神经架构搜索（NAS）技术，优化DETR模型在不同目标域中的迁移能力。

Result: RF-DETR在COCO和Roboflow100-VL数据集上显著优于现有方法，例如RF-DETR（nano）在COCO上比D-FINE（nano）高出5.3 AP，RF-DETR（2x-large）在Roboflow100-VL上比GroundingDINO（tiny）高出1.2 AP且速度快20倍。

Conclusion: RF-DETR通过轻量级专家检测变压器和神经架构搜索（NAS）技术，显著提升了在COCO和Roboflow100-VL数据集上的性能，成为首个在COCO上实时检测超过60 AP的模型。

Abstract: Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the "tunable knobs" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [70] [Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments](https://arxiv.org/abs/2511.08851)
*Po-Heng Chou,Da-Chih Lin,Hung-Yu Wei,Walid Saad,Yu Tsao*

Main category: cs.NI

TL;DR: 提出了一种基于测量的5G铁路RLF预测框架，测试了六种模型，TimesNet在3秒窗口表现最佳，CNN在2秒窗口下平衡了准确性与延迟。


<details>
  <summary>Details</summary>
Motivation: 为5G非独立组网（NSA）铁路环境中的早期无线链路故障（RLF）预测提出了一种基于测量的框架。

Method: 使用10Hz地铁列车轨迹数据，结合服务和邻小区指标，对CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST和TimesNet六种模型在不同观察窗口和预测时间范围下进行了基准测试。

Result: 当观察窗口为3秒时，TimesNet在3秒预测时间范围内获得最高F1分数；而CNN在2秒预测时间范围内提供了较好的准确性与延迟权衡，支持冗余和自适应切换等主动措施。

Conclusion: 深度时序模型能够利用商用设备上的轻量级特征提前几秒预测可靠性下降，为5G铁路系统提供了实用的早期预警控制路径。

Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.

</details>


### [71] [Content-based Fine-grained Flow Management Supporting Out-of-Path Transparent Add-ons](https://arxiv.org/abs/2511.08976)
*Anna Ishizaki,Takuma Fukui,Hiroaki Nishi*

Main category: cs.NI

TL;DR: 提出基于内容的细粒度流管理和路径外透明附加架构，实现灵活流控制和网络透明性，评估显示效果良好且延迟低。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于每包的流控制缺乏灵活性的问题，并适应智能社区中多样化的服务需求。

Method: 提出了一种基于内容的细粒度流管理方法，包括选择性内容掩码和路径外匿名化两种方法，以及动态重写Ack和Seq数以维护TCP会话完整性的机制。

Result: 在Mininet上实现的评估结果表明，所提方法能以最小网络延迟影响实现有效的流管理，同时保持网络透明性。

Conclusion: 研究提出了一种基于内容的细粒度流管理方法和路径外透明附加架构，有效实现了网络透明性和灵活的流控制，同时保持了TCP会话完整性。

Abstract: This study aims to realize a mechanism for packet processing in the edge domain while maintaining network transparency, in order to accommodate diverse service requirements in smart communities. Since conventional flow control, which operates on a per-packet basis, lacks flexibility, we propose a content-based fine-grained flow management method that enables control at the level of individual content segments within packets. In addition, we introduce an out-of-path transparent add-on architecture to address the limitations of conventional transparent add-ons, which assume the presence of processing resources on the main path. The proposed system implements one approach for selective content masking and two approaches for out-of-path anonymization. Furthermore, we develop a mechanism for dynamically rewriting Ack and Seq numbers to preserve TCP session integrity. The proposed approaches were implemented and evaluated on Mininet, and the results demonstrate that effective flow management can be achieved with minimal impact on network delay while maintaining network transparency.

</details>


### [72] [Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems](https://arxiv.org/abs/2511.09006)
*Ameneh Zarei,Mahmood Ahmadi,Farhad Mardukhi*

Main category: cs.NI

TL;DR: 论文提出HIPA框架，结合云、雾和边缘计算，优化IoT性能，满足低延迟和隐私需求。


<details>
  <summary>Details</summary>
Motivation: IoT应用的数据量大、实时性要求高，传统云计算架构难以满足需求，需探索云、雾、边缘计算的互补作用。

Method: 通过机器学习动态分配云计算、雾计算和边缘计算层的任务，提出HIPA框架，整合现有研究成果。

Result: HIPA框架有效减少了延迟，提高了可扩展性，并确保了数据隐私。

Conclusion: 该论文提出了一种新的分层IoT处理架构（HIPA），结合云、雾和边缘计算的优势，以优化IoT性能，满足低延迟、高可扩展性和数据隐私的需求。

Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.

</details>


### [73] [Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks](https://arxiv.org/abs/2511.09087)
*Vijay K Shah,Cong Shen*

Main category: cs.NI

TL;DR: Tele-LLM-Hub 是一个低代码平台，用于快速设计和部署面向 5G 及更高级网络的上下文感知多代理 LLM 系统，核心是 TeleMCP 协议和工具集。


<details>
  <summary>Details</summary>
Motivation: 随着电信无线网络日益复杂，智能 LLM 应用需要具备对网络状态的领域特定理解，Tele-LLM-Hub 旨在解决这一问题并推动创新。

Method: 通过 TeleMCP（Telecom Model Context Protocol）实现结构化且丰富的上下文通信，提供低代码界面支持代理创建、工作流组合以及与软件栈（如 srsRAN）的交互。关键组件包括直接聊天界面、预建系统库、基于 RANSTRUCT 框架微调的 Agent Maker 和用于组合多代理工作流的 MA-Maker。

Result: Tele-LLM-Hub 实现了 TeleMCP，提供了一套工具和界面，支持快速原型设计和部署上下文感知多代理 LLM 系统。

Conclusion: Tele-LLM-Hub 是一个低代码解决方案，旨在简化和加速上下文感知多代理 LLM 系统的设计和部署，推动下一代无线网络的创新。

Abstract: This paper introduces Tele-LLM-Hub, a user friendly low-code solution for rapid prototyping and deployment of context aware multi-agent (MA) Large Language Model (LLM) systems tailored for 5G and beyond. As telecom wireless networks become increasingly complex, intelligent LLM applications must share a domainspecific understanding of network state. We propose TeleMCP, the Telecom Model Context Protocol, to enable structured and context-rich communication between agents in telecom environments. Tele-LLM-Hub actualizes TeleMCP through a low-code interface that supports agent creation, workflow composition, and interaction with software stacks such as srsRAN. Key components include a direct chat interface, a repository of pre-built systems, an Agent Maker leveraging finetuning with our RANSTRUCT framework, and an MA-Maker for composing MA workflows. The goal of Tele-LLM-Hub is to democratize the design of contextaware MA systems and accelerate innovation in next-generation wireless networks.

</details>


### [74] [Experimenting with Energy-Awareness in Edge-Cloud Containerized Application Orchestration](https://arxiv.org/abs/2511.09116)
*Dalal Ali,Rute C. Sofia*

Main category: cs.NI

TL;DR: 论文提出了一种能源感知调度方法，通过在异构边缘-云基础设施中注入能源指标，显著提高了能源效率，尤其在高负载场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索能源感知策略在异构边缘-云基础设施应用部署中的作用，以实现更可持续的云原生计算。

Method: 论文提出了一种方法，将能源指标（计算和网络层面）注入现有的调度方法中，以优化资源分配并降低能耗。

Result: 通过基于ARM设备的真实测试平台实验评估，与标准Kubernetes调度相比，能源消耗和负载分布均有所改善。

Conclusion: 该论文表明，在异构边缘-云基础设施中采用能源感知策略可以显著提高能源效率，尤其是在高负载场景下，为可持续的云原生计算提供了潜力。

Abstract: This paper explores the role of energy-awareness strategies into the deployment of applications across heterogeneous Edge-Cloud infrastructures. It proposes methods to inject into existing scheduling approaches energy metrics at a computational and network level, to optimize resource allocation and reduce energy consumption. The proposed approach is experimentally evaluated using a real-world testbed based on ARM devices, comparing energy consumption and workload distribution against standard Kubernetes scheduling. Results demonstrate consistent improvements in energy efficiency, particularly under high-load scenarios, highlighting the potential of incorporating energy-awareness into orchestration processes for more sustainable cloud-native computing.

</details>


### [75] [LLP-V2X: Low Latency-Power Vehicular Networking Towards 6G V2X](https://arxiv.org/abs/2511.09182)
*Zhaoyu Liu,Liu Cao,Lyutianyang Zhang,Dongyu Wei,Ye Hu,Weizheng Wang*

Main category: cs.NI

TL;DR: 提出多跳多路径车载网络方案，优化流量与功率以平衡延迟和能耗，设计可切换模式的调度器，满足6G V2X需求。


<details>
  <summary>Details</summary>
Motivation: 6G车载网络中能耗与延迟的权衡因更严格的QoS要求变得至关重要，但现有研究对具有新V2X特性的网络尚未深入探索。

Method: 提出多跳多路径车载网络架构，联合优化流量分配和链路功率，并形式化最小延迟和最小功率两个互补问题。基于理论设计LLP MHMP调度器，支持按需切换工作模式。

Result: 通过精心设计的仿真验证了所提方案在降低延迟和功耗方面的性能。

Conclusion: 本文提出了一种新型多跳多路径车载网络方案，通过联合优化车辆流量分配和链路传输功率，实现了低延迟和低功耗通信。设计的LLP MHMP调度器能够根据需要切换工作模式，满足6G V2X网络的严格QoS要求。

Abstract: The trade-off between energy and latency budgets is becoming significant due to the more stringent QoS requirements in 6G vehicular networks. However, comprehensively studying the trade-off between energy and latency budgets for 6G vehicular network with new Vehicle-to-Everything (V2X) features is still under-explored. This paper proposes a novel multi-hop, multi-path vehicular networking that jointly optimizes vehicular traffic splitting across candidate routes and per-link transmit power to achieve low-latency and low-power communications. Afterwards, we formalize two complementary problem formulations (minimum latency and minimum power) based on the proposed 6G V2X architecture and provide sufficient conditions. The performance of the proposed scheme is evaluated via well-designed simulations. Based on these theories, we design algorithm (LLP MHMP Scheduler) that switches on demand between a fixed-power minimum-latency mode and a fixed-latency minimum-power mode.

</details>


### [76] [Digital Co-Founders: Transforming Imagination into Viable Solo Business via Agentic AI](https://arxiv.org/abs/2511.09533)
*Farhad Rezazadeh,Pegah Bonehgazy*

Main category: cs.NI

TL;DR: 论文提出了一个三阶段框架，帮助个体创业者利用AI代理将创意转化为成功的单人生意，强调了人机协作和适应性规划的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在AI日益普及的时代，个体创业者如何利用AI代理作为数字联合创始人，克服资源有限等挑战，将创意转化为可持续的商业价值。

Method: 基于创业、创造力和创新研究，论文提出了一个三阶段框架，详细描述了每个阶段中AI代理的具体支持作用，包括市场扫描、原型设计、内容创作等。

Result: 研究结果展示了一个清晰的框架，帮助创业者通过AI支持实现从创意到规模化商业模式的转化，并识别了关键的成功因素和挑战。

Conclusion: 该论文总结了个体创业者如何在AI时代将创意转化为成功的单人生意，提出了一个包含三个阶段（想象塑造、现实测试、现实扩展）的框架，并强调了心理适应性、有效规划和成功的人机协作等关键因素。

Abstract: This paper investigates how individual entrepreneurs can turn creative ideas into successful solo businesses in an era increasingly shaped by Artificial Intelligence (AI) agents. It highlights the key steps that connect personal vision, structured experimentation, and lasting value creation, and shows how AI agents can act as digital co-founders throughout this journey. Building on research in entrepreneurship, creativity, and innovation, we present a framework with three key stages: (1) Imagination shaping, where vague goals become clear value propositions, supported by AI agents that help with market scanning, idea refinement, and rapid concept generation; (2) Reality testing, where these ideas are tested through low-cost experiments, structured feedback loops, and efficient execution, with AI agents automating tasks such as prototyping, content creation, customer interaction, and data analysis; and (3) Reality scaling, where successful ideas are transformed into repeatable processes, scalable market strategies, and long-term business models, increasingly operated and optimized by autonomous or semi-autonomous AI workflows. We focus on the specific context of solopreneurship, characterized by limited human resources, complete accountability for decision-making, and a strong association between the founder's identity and the business. The framework clearly identifies key enabling factors such as mental adaptability, effective planning, and successful human-AI collaboration within digital ecosystems. It also thoughtfully addresses ongoing challenges, like uncertainty and cognitive overload, which are heightened by our constant connectivity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [77] [A Simple Analysis of Ranking in General Graphs](https://arxiv.org/abs/2511.08801)
*Mahsa Derakhshan,Mohammad Roghani,Mohammad Saneian,Tao Yu*

Main category: cs.DS

TL;DR: 论文通过组合分析证明Ranking算法在一般图中能实现$(1/2 + c)$-近似匹配，$c \geq 0.005$。


<details>
  <summary>Details</summary>
Motivation: 研究Ranking算法在一般图中的近似匹配性能，进一步验证其在理论上的有效性。

Method: 采用了简单的组合分析方法。

Result: 证明了Ranking算法可以达到$(1/2 + c)$-近似匹配，$c \geq 0.005$。

Conclusion: 该论文通过组合分析证明了Ranking算法在一般图中能实现$(1/2 + c)$-近似匹配，其中$c \geq 0.005$。

Abstract: We provide a simple combinatorial analysis of the Ranking algorithm, originally introduced in the seminal work by Karp, Vazirani, and Vazirani [KVV90], demonstrating that it achieves a $(1/2 + c)$-approximate matching for general graphs for $c \geq 0.005$.

</details>


### [78] [Space-Efficient and Output-Sensitive Algorithms for the Longest Common Bitonic Subsequence](https://arxiv.org/abs/2511.08958)
*Md. Tanzeem Rahat,Md. Manzurul Hasan*

Main category: cs.DS

TL;DR: 本文提出了一种更高效的最长公共双调子序列算法，解决了原有方法的时间和空间效率问题，适用于大规模输入。


<details>
  <summary>Details</summary>
Motivation: LCBS问题在生物信息学、金融和信号分析中有广泛应用，但原有解决方案存在时间和空间效率低下的问题，限制了其实际应用。

Method: 通过改进的滚动行实现和将LCBS问题转化为稀疏有向无环图中的最长路径问题，实现了时间和空间复杂度的优化。

Result: 新算法在时间上达到O((n + m) log n + M log M)，空间上为O(M)，显著优于原有θ(nm)的复杂度。

Conclusion: 本文提出了针对最长公共双调子序列（LCBS）问题的更高效算法，突破了原有二次动态规划的空间限制，并提供了更优的时间和空间复杂度解决方案。

Abstract: The longest common bitonic subsequence (LCBS) of two sequences A and B is the longest subsequence that increases to a single peak and then decreases while appearing, in order, in both inputs. Although LCBS naturally models rise-fall patterns in bioinformatics, finance, and signal analysis, the only previously documented solution was a quadratic dynamic program that needs θ(nm) time and space. We show that this space barrier is not inherent: a refined rolling-row implementation evaluates the same recurrence in θ(nm) time with only θ(min(n, m)) additional memory. By isolating the M symbol matches and their C bitonic-compatible pairs, we cast LCBS as a longest-path problem in a sparse DAG and solve it in O((n + m) log n + M log M) time and O(M) space, which is asymptotically faster than the quadratic baseline whenever M << n m. These results make exact LCBS computation practical for inputs that were previously out of reach and expose a new fine-grained complexity landscape that invites further exploration.

</details>


### [79] [Prophet and Secretary at the Same Time](https://arxiv.org/abs/2511.09531)
*Gregory Kehne,Thomas Kesselheim*

Main category: cs.DS

TL;DR: 本文研究了在线最优停止问题中算法对不同分布假设的鲁棒性，确定了停止规则在先知不等式和秘书问题中同时竞争的可能性边界，并提出了最优算法和不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 研究在线算法对给定分布错误指定的鲁棒性，以及何时能同时在不同分布假设下保持良好的竞争比。

Method: 本文引入了适用于极端i.i.d.先知和秘书问题的算法族，并通过证明不可能性结果，确定了任何自适应停止规则都无法达到的(α, β)组合。

Result: 本文在固定到达次数n和泊松过程到达的两种设置下，确定了停止规则可以同时达到的(α, β)近似比边界，并提供了两种设置之间的归约方法。

Conclusion: 本文确定了停止规则在i.i.d.先知不等式问题和秘书问题中同时获得竞争性保证的可行性边界，并引入了一系列算法来提供非平凡的联合保证。

Abstract: Many online problems are studied in stochastic settings for which inputs are samples from a known distribution, given in advance, or from an unknown distribution. Such distributions model both beyond-worst-case inputs and, when given, partial foreknowledge for the online algorithm. But how robust can such algorithms be to misspecification of the given distribution? When is this detectable, and when does it matter? When can algorithms give good competitive ratios both when the input distribution is as specified, and when it is not?
  We consider these questions in the setting of optimal stopping, where the cases of known and unknown distributions correspond to the well-known prophet inequality and to the secretary problem, respectively. Here we ask: Can a stopping rule be competitive for the i.i.d. prophet inequality problem and the secretary problem at the same time? We constrain the Pareto frontier of simultaneous approximation ratios $(α, β)$ that a stopping rule can attain.
  We introduce a family of algorithms that give nontrivial joint guarantees and are optimal for the extremal i.i.d. prophet and secretary problems. We also prove impossibilities, identifying $(α, β)$ unattainable by any adaptive stopping rule. Our results hold for both $n$ fixed arrivals and for arrivals from a Poisson process with rate $n$. We work primarily in the Poisson setting, and provide reductions between the Poisson and $n$-arrival settings that may be of broader interest.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [80] [A Finite Difference Approximation of Second Order Regularization of Neural-SDFs](https://arxiv.org/abs/2511.08980)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种基于有限差分的轻量级曲率正则化方法，用于神经SDF学习，显著降低了计算成本且保持了高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然准确但计算成本高，另一些方法虽降低了开销但仍需高阶微分，因此需要一种更高效且通用的曲率正则化方法。

Method: 通过轻量级的有限差分模板替代传统的全Hessian矩阵计算，利用泰勒展开的二阶近似（截断误差为O(h^2)），避免了高阶微分的计算负担。

Result: 实验表明，有限差分变体在重建保真度上与自动微分版本相当，同时GPU内存使用和训练时间减少了高达两倍。

Conclusion: 该论文提出的有限差分框架为神经符号距离场（SDF）学习中的曲率正则化提供了一种高效且可扩展的替代方案，实验证明其在保持重建保真度的同时显著降低了计算开销。

Abstract: We introduce a finite-difference framework for curvature regularization in neural signed distance field (SDF) learning. Existing approaches enforce curvature priors using full Hessian information obtained via second-order automatic differentiation, which is accurate but computationally expensive. Others reduced this overhead by avoiding explicit Hessian assembly, but still required higher-order differentiation. In contrast, our method replaces these operations with lightweight finite-difference stencils that approximate second derivatives using the well known Taylor expansion with a truncation error of O(h^2), and can serve as drop-in replacements for Gaussian curvature and rank-deficiency losses. Experiments demonstrate that our finite-difference variants achieve reconstruction fidelity comparable to their automatic-differentiation counterparts, while reducing GPU memory usage and training time by up to a factor of two. Additional tests on sparse, incomplete, and non-CAD data confirm that the proposed formulation is robust and general, offering an efficient and scalable alternative for curvature-aware SDF learning.

</details>


### [81] [Computational Caustic Design for Surface Light Source](https://arxiv.org/abs/2511.09361)
*Sizhuo Zhou,Yuou Sun,Bailin Deng,Juyong Zhang*

Main category: cs.GR

TL;DR: 提出了一种基于优化点光源和可微分渲染的自由曲面设计方法，能更准确地模拟真实光源并生成匹配目标光分布的焦散透镜。


<details>
  <summary>Details</summary>
Motivation: 现有焦散透镜设计通常假设过度简化的点或平行光源，难以应对真实世界复杂光照模式，因此需要更准确的光源表示方法。

Method: 提出了一种基于可微分渲染框架的优化点光源集表示方法，结合通量模拟的光传输物理渲染，无需光源强度分布先验知识，并应用收缩映射将约束问题转化为无约束问题以高效探索光源参数空间。

Result: 仿真和物理实验表明，该方法比点光源近似更准确地表示真实表面光源，设计的焦散透镜能生成与目标光分布高度匹配的图像。

Conclusion: 通过优化点光源模型和考虑通量一致性与法线可积性的自由曲面设计，该方法比传统点光源近似更准确地模拟真实表面光源，产生与目标光分布高度匹配的焦散透镜效果。

Abstract: Designing freeform surfaces to control light based on real-world illumination patterns is challenging, as existing caustic lens designs often assume oversimplified point or parallel light sources. We propose representing surface light sources using an optimized set of point sources, whose parameters are fitted to the real light source's illumination using a novel differentiable rendering framework. Our physically-based rendering approach simulates light transmission using flux, without requiring prior knowledge of the light source's intensity distribution. To efficiently explore the light source parameter space during optimization, we apply a contraction mapping that converts the constrained problem into an unconstrained one. Using the optimized light source model, we then design the freeform lens shape considering flux consistency and normal integrability. Simulations and physical experiments show our method more accurately represents real surface light sources compared to point-source approximations, yielding caustic lenses that produce images closely matching the target light distributions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [82] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 论文改进了MAC图稀疏化算法，通过专用求解器、步长策略优化和自动连通性方案，使其更适合实时估计应用。


<details>
  <summary>Details</summary>
Motivation: 现有图稀疏化算法在大规模、长期图上表现不佳，MAC算法虽能保持估计性能但计算成本高且需手动预设边集。

Method: 开发了专用求解器以提高计算效率，研究了先进的步长策略以优化收敛速度和解决方案质量，并提出了自动保证图连通性的方案。

Result: 实现了平均2倍运行速度提升，优化了收敛速度和解决方案质量，并自动保证了图连通性。

Conclusion: 通过改进MAC算法，提高了其在实时估计应用中的可扩展性和可靠性。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [83] [IFG: Internet-Scale Guidance for Functional Grasping Generation](https://arxiv.org/abs/2511.09558)
*Ray Muxin Liu,Mingxuan Li,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 结合互联网规模模型的语义理解与模拟驱动的局部几何感知，实现高性能语义抓取。


<details>
  <summary>Details</summary>
Motivation: 解决大视觉模型在精确控制灵巧手进行3D抓取时缺乏几何理解的问题。

Method: 利用模拟与力闭合抓取生成管道理解手和物体的局部几何，并通过扩散模型在点云上实时操作。

Result: 实现了无需手动训练数据的高性能语义抓取。

Conclusion: 结合互联网规模模型的全局语义理解与模拟驱动的局部几何感知力闭合方法，实现了高性能的语义抓取，无需手动收集训练数据。

Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/

</details>


### [84] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文回顾了实现人机协同合作的关键组件，包括信息交换、自适应规划和反馈机制，并指出了未来研究的趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人和AI在复杂任务和环境中取得了显著成就，但在人类密集环境中，机器人若不能有效建模人类状态和意图并调整行为，将无法发挥其全部潜力。

Method: 通过审查人机交互的全过程，包括从多模态输入到机器人可理解表示的转换、自适应规划和角色分配，再到控制层和反馈机制。

Result: 确定了实现直观信息交换和技能转移的关键组件，并展望了更适应性和可访问性的人机协同合作的未来趋势。

Conclusion: 本文强调了实现人机协同合作（HRC）的关键组成部分，并指出了未来研究的方向，以促进更适应性和可访问性的人机交互。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [85] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: ATOM-CBF通过自适应误差边界和实时安全过滤器，解决了学习感知模块在OoD测量下的安全问题，无需真实标签，仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决学习感知模块在遇到训练数据分布外的测量（OoD）时因认知不确定性导致的安全问题，避免依赖真实标签或分布信息。

Method: 提出了ATOM-CBF框架，包含两个核心组件：OoD感知的自适应感知误差边界和集成此边界的实时安全过滤器。该方法无需真实标签或分布偏移信息，直接计算并适应OoD测量带来的认知不确定性。

Result: 仿真实验验证了ATOM-CBF在F1Tenth车辆（LiDAR数据）和四足机器人（RGB图像）上的安全性，表明其能有效维持系统安全。

Conclusion: ATOM-CBF框架通过自适应感知误差边界和实时调整的安全过滤器，有效应对了学习感知模块中的认知不确定性，显著提升了系统安全性，尤其在面对OoD测量时。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [86] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: CENIC是一种新型连续时间积分器，结合凸时间步进和误差控制积分，解决了现有方法的不足，实现了快速实时运行和准确性保证。


<details>
  <summary>Details</summary>
Motivation: 现有误差控制积分器难以处理接触的刚性动力学，无法满足现代机器人工作流程的速度和可扩展性要求。

Method: CENIC是一种新的连续时间积分器，结合了凸时间步进和误差控制积分的最新进展。

Result: CENIC能够以与离散时间机器人模拟器相当的速度运行，同时提供准确性和收敛性保证。

Conclusion: CENIC结合了凸时间步进和误差控制积分的最新进展，继承了连续积分和离散时间步进的优点，能够以与MuJoCo、Drake和Isaac Sim等离散时间机器人模拟器相当的速度运行，同时提供准确性和收敛性保证。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [87] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 提出了一种利用共享关节结构的双机械臂运动规划方法，实现了高效且高成功率的实时规划。


<details>
  <summary>Details</summary>
Motivation: 高自由度双机械臂机器人在未知、变化环境中的实时运动规划面临高维配置空间和复杂避障约束的挑战。

Method: 首先为每个运动链（左臂+躯干，右臂+躯干）构建动态路标图（DRM），然后利用共享关节的结构高效搜索两个路标图的组合，从而绕过维度灾难。

Result: 在19自由度的移动操作机器人上执行杂货配送任务时，平均规划时间为0.4秒，成功率高达99.9%，测试了超过2000次运动规划。

Conclusion: 本研究提出了一种通过利用双机械臂机器人中共享关节的结构来缓解高维空间运动规划问题的方法，并在实际杂货店场景中验证了其高效性和高成功率。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [88] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: CoUGARs 是一个低成本、可配置的AUV平台，支持多智能体研究，已通过仿真和实地测试验证。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中多智能体自主性水下测试的高成本和工程挑战。

Method: 基于商用和3D打印部件设计低成本AUV平台，配备DVL和USBL声学阵列/传感器，开发容器化软件栈并与HoloOcean模拟器集成。

Result: 系统在仿真和犹他州湖泊及水库的实地测试中验证了其有效性。

Conclusion: CoUGARs 是一个低成本、可配置的自主水下机器人平台，成功支持了多智能体自主性研究的实验和测试。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [89] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 针对X波段雷达在自主导航中的局限性，本文提出了一种新地点识别算法，通过对象密度规则和故意降级检测提升性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: X波段雷达虽为海事船只主要传感器，但由于分辨率低和信息不足，其在自主导航中的应用受限。

Method: 提出了一种专为X波段雷达设计的地点识别算法，结合了基于对象密度的候选选择规则和故意降级雷达检测以提高检索性能。

Result: 在公共海事雷达数据集和自采数据集上评估，算法性能优于现有雷达地点识别方法，并通过消融研究验证了关键参数的敏感性。

Conclusion: 本文提出的X波段雷达专用地点识别算法在自主导航中表现出色，尤其在处理低分辨率和信息不足的雷达数据时，通过对象密度规则和故意降级雷达检测实现了鲁棒性。

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [90] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 提出了一种低成本、实时的PICO机器人远程操作框架，优于主流方案，支持VLA数据集建设和实时远程操作。


<details>
  <summary>Details</summary>
Motivation: 降低上肢机器人操作研究的技术门槛，促进Vision-Language-Action（VLA）数据集的建设，并加速VLA相关研究的进展。

Method: 提出了一种基于PICO的机器人远程操作框架，支持实时获取手部运动和姿态数据，并在成本效益上优于主流视觉跟踪和运动捕捉解决方案。该框架与RealMirror生态系统原生兼容，提供即用功能，用于在Isaac仿真环境中稳定且精确地记录机器人轨迹。

Result: 该框架在成本效益上优于主流解决方案，支持实时远程操作多种末端执行器装备的机器人，包括灵巧手和机器人夹爪。

Conclusion: 该研究旨在降低上肢机器人操作研究的技术门槛，加速VLA相关研究的进展。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [91] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 本文提出一种基于规划级意图预测的共享控制框架，通过深度强化学习联合优化意图预测与路径重规划，显著提升人机协作的安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人共享控制中，有效理解人类运动意图对于无缝人机协作至关重要。

Method: 采用马尔可夫决策过程联合建模意图域预测和路径重规划问题，并通过深度强化学习求解。此外，开发了基于Voronoi图的人类轨迹生成算法，使模型能在无人类参与或演示数据的情况下完全在仿真中训练。

Result: 广泛的仿真和真实用户研究表明，该方法在操作员工作负荷和安全性方面显著优于现有辅助遥操作方法。

Conclusion: 本文提出的共享控制框架通过规划级意图预测显著降低了操作员的工作负荷并提高了安全性，同时不牺牲任务效率。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [92] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: SCOPE是一个零样本框架，通过利用前沿信息和自省机制提升视觉导航性能，实验显示其准确率提升4.6%。


<details>
  <summary>Details</summary>
Motivation: 现有零样本研究忽视了视觉前沿边界对导航的关键影响，且未能有效关联部分视觉观察与导航目标。

Method: SCOPE利用视觉语言模型估计探索潜力，构建时空潜力图以捕捉边界动态，并通过自省机制重新审视决策。

Result: 在两个不同的具身导航任务中，SCOPE的准确率比现有最佳基线高出4.6%。

Conclusion: SCOPE框架通过结合前沿信息和自省机制，显著提升了零样本视觉导航的性能，表现出更好的校准性、泛化能力和决策质量。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [93] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，将VLM转变为主动导航策略制定者，通过结构化推理和增强空间意识的技术，显著提升了导航效率和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用视觉语言模型（VLM）的推理能力，限制了其在机器人导航中的潜力。

Method: 框架将高级规划外包给VLM，利用其上下文理解能力引导基于前沿的探索代理。通过三种技术实现智能引导：结构化思维链提示、动态包含代理最近动作历史以防止循环，以及VLM解释俯视障碍地图与第一人称视图的新能力。

Result: 在HM3D、Gibson和MP3D等挑战性基准测试中，该方法生成的轨迹异常直接且逻辑性强，导航效率显著优于现有方法。

Conclusion: 该方法通过将视觉语言模型（VLM）从被动观察者转变为主动策略制定者，显著提升了机器人导航的效率和逻辑性，为更具能力的实体代理开辟了新路径。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [94] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: UniMM-V2X是一个端到端多智能体框架，通过多层次融合和MoE架构显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动驾驶中感知与决策孤立、多智能体协作仅关注感知层面而忽视与下游规划控制对齐的问题。

Method: 提出了一种新颖的端到端多智能体框架UniMM-V2X，采用多层次融合策略统一感知与预测协作，并结合MoE架构动态增强BEV表示。

Result: 在DAIR-V2X数据集上，UniMM-V2X实现了感知准确率提升39.7%、预测误差降低7.2%、规划性能提升33.2%的SOTA性能。

Conclusion: UniMM-V2X框架通过多层次融合策略和MoE架构，显著提升了自动驾驶的感知、预测和规划性能，展示了多智能体协同的强大潜力。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [95] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: EDMO算法通过三种新策略优化无人机三维路径规划，在复杂环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法在复杂场景中存在早熟收敛和缺乏解多样性的问题，影响了无人机路径规划的性能。

Method: 提出了EDMO算法，结合了动态量子隧道优化策略（DQTOS）、生物光趋动态聚焦搜索策略（BDFSS）和正交透镜对立学习策略（OLOBL）。

Result: EDMO在39个标准测试函数上表现优异，优于14种先进算法，并在实际无人机路径规划和工程设计任务中验证了其有效性。

Conclusion: EDMO算法在复杂动态环境中表现出色，为无人机三维路径规划提供了高效、智能的解决方案。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [96] [SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields](https://arxiv.org/abs/2511.09072)
*Sangheon Yang,Yeongin Yoon,Hong Mo Jung,Jongwoo Lim*

Main category: cs.RO

TL;DR: SMF-VO是一种轻量级、高效的视觉里程计框架，通过直接估计运动速度绕过传统姿态估计，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 传统VO和VIO方法的‘姿态中心’范式计算成本高，限制了在资源受限设备上的实时性能，因此需要一种更高效的替代方案。

Method: 采用稀疏光流直接估计瞬时线速度和角速度，绕过显式姿态估计或昂贵的地标跟踪，并使用适用于各种相机模型的广义3D射线运动场公式。

Result: 在基准数据集上表现出卓越的效率和竞争性精度，仅使用CPU在Raspberry Pi 5上实现超过100 FPS。

Conclusion: SMF-VO作为一种轻量级的‘运动中心’框架，通过直接估计瞬时线速度和角速度，提供了一种高效且准确的视觉里程计替代方案，特别适用于移动机器人和可穿戴设备。

Abstract: Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.

</details>


### [97] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: D-AWSIM是一个分布式模拟器，解决大规模城市交通模拟的高成本和单机限制问题，显著提升性能并适用于自动驾驶研究。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界实验成本高昂且面临监管挑战，而传统单机模拟器无法处理大规模城市交通场景，因此需要一种分布式解决方案。

Method: 本文提出了D-AWSIM，一个将工作负载分配到多台机器上的分布式模拟器，支持大规模传感器部署和密集交通环境的模拟。

Result: 评估显示，D-AWSIM在车辆数量和LiDAR传感器处理方面的吞吐量显著高于单机设置，并与Autoware集成证明了其在自动驾驶研究中的实用性。

Conclusion: D-AWSIM是一个分布式模拟器，能够有效支持大规模城市交通场景的模拟，显著提升了车辆数量和LiDAR传感器处理的吞吐量，为自动驾驶研究提供了实用工具。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [98] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: APEX是一种无需部署时参考数据的插件式方法，通过衰减动作先验和多批评框架提升步态机器人的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖部署期间的参考数据和大量调参，限制了适应性，APEX旨在消除这些限制。

Method: APEX结合了衰减动作先验和多批评框架，前者初始时将探索偏向专家演示，随后逐步允许策略独立探索，后者平衡任务性能与运动风格。

Result: APEX在仿真和Unitree Go2机器人上的实验验证了其有效性，能够学习多样化动作并在不同地形和速度间迁移参考风格。

Conclusion: APEX方法通过将专家演示直接整合到强化学习中，利用衰减动作先验和多批评框架，提高了步态机器人的学习稳定性、效率和泛化能力，为自然技能获取开辟了新途径。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [99] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出统一框架，实现软肌肉拮抗系统的独立扭矩和刚度实时控制，提升人机交互安全性和动态性能。


<details>
  <summary>Details</summary>
Motivation: 现有软肌肉控制器难以在动态接触瞬变中维持独立控制，因此需要开发一个统一框架以实现实时独立扭矩和刚度控制。

Method: 采用统一力定律捕捉多样软肌肉物理特性，结合级联控制器及解析逆动力学，通过共收缩/偏置坐标独立调节扭矩和刚度。

Result: 仿真验证显示，该框架在软表面上实现了200倍更快的稳定速度，在刚性表面上减少了81%的力，并保持了稳定的交互性能。

Conclusion: 该框架为肌肉骨骼拮抗系统实现自适应阻抗控制提供了基础，以实现安全的人机交互。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [100] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了两种数据驱动工具来评估数据集的信息量和可学习性，无需训练即可高效量化可学习性，为设计更高质量的数据集提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集评估方法主要关注多样性，而对可学习性的评估通常缺乏效率与可解释性，这限制了数据集质量的提升。

Method: 提出了两种基于数据的工具：一是构建统一的多模态表示并提出多样性熵，二是引入首个可解释、数据驱动的算法来量化数据集的可学习性。

Result: 在模拟和真实世界的具身数据集上验证了算法的有效性，表明它能提供可靠、可操作的见解。

Conclusion: 本研究为设计更高质量的数据集奠定了基础，推动了具身智能的发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [101] [RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation](https://arxiv.org/abs/2511.09141)
*Xuetao Li,Wenke Huang,Nengyuan Pan,Kaiyan Zhao,Songhua Yang,Yiming Wang,Mengde Li,Mang Ye,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: RGMP框架通过几何语义推理和递归高斯自适应，显著提升了机器人技能泛化和数据效率，任务成功率87%，数据效率提升5倍。


<details>
  <summary>Details</summary>
Motivation: 解决当前数据驱动方法在未见场景中几何推理不足和机器人-目标关系建模效率低下的问题。

Method: 提出了端到端的RGMP框架，结合了几何语义技能推理和数据高效的视觉运动控制。包括Geometric-prior Skill Selector用于感知能力，以及Adaptive Recursive Gaussian Network用于机器人运动合成。

Result: 在类人机器人和桌面双臂机器人上测试，任务成功率达87%，数据效率比现有最佳模型高5倍。

Conclusion: RGMP框架通过几何语义推理和递归高斯自适应，在跨领域泛化方面表现出色，任务成功率达到87%，数据效率提升5倍。

Abstract: Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

</details>


### [102] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: LODESTAR是一种新型LiDAR惯性里程计方法，通过DA-ASKF和DA-DE模块解决退化环境中的测量稀疏和不平衡问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在长走廊和高空飞行等退化环境中，LiDAR测量不平衡或稀疏，导致状态估计不准确。现有LiDAR惯性里程计（LIO）方法在这些环境中性能下降，亟需解决方案。

Method: LODESTAR采用退化感知自适应Schmidt-Kalman滤波器（DA-ASKF）和退化感知数据利用（DA-DE）模块。DA-ASKF通过滑动窗口利用过去状态和测量作为额外约束，并根据退化水平自适应分类状态为活跃或固定。DA-DE则根据局部化贡献和Jacobian矩阵条件数，修剪活跃状态中信息较少的测量，并选择性利用固定状态的测量。

Result: 实验结果表明，LODESTAR在各种退化条件下，在精度和鲁棒性方面优于现有的基于LiDAR的里程计方法和退化感知模块。

Conclusion: LODESTAR通过DA-ASKF和DA-DE两个模块有效解决了LiDAR测量在退化环境中的性能下降问题，显著提升了LiDAR惯性里程计的精度和鲁棒性。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [103] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: Humanoid-Union数据集和SCHUR框架通过大规模数据显著提升人形机器人的运动生成和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人学习中数据扩展的瓶颈问题，利用丰富的人类视频和运动数据作为免费且大规模的数据源。

Method: 通过自主流水线生成Humanoid-Union数据集，并基于此提出SCHUR学习框架，探索大规模数据对人形机器人高层次控制的影响。

Result: SCHUR在MPJPE下实现了37%的重建改进，在FID下实现了25%的对齐改进，实验验证了其在真实人形机器人中的有效性。

Conclusion: Humanoid-Union数据集和SCHUR框架显著提升了人形机器人的运动生成质量和文本-运动对齐效果，为大规模数据在高层次控制中的应用提供了有效解决方案。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [104] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: UMIGen通过手持设备和优化机制，高效生成跨机器人体现的数据，提升数据收集和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的机器人学习面临大规模高质量演示数据收集的挑战，包括高成本、专用硬件依赖和空间泛化能力有限。UMIGen旨在解决这些问题。

Method: UMIGen框架包含Cloud-UMI手持数据采集设备和可见性感知优化机制。前者无需视觉SLAM即可记录点云观察-动作对，后者扩展了DemoGen管道以生成符合真实自我中心观察的数据。

Result: 实验证明UMIGen在模拟和真实环境中均支持强跨体现泛化，并加速了多种操作任务的数据收集。

Conclusion: UMIGen通过Cloud-UMI设备和无视觉SLAM的点云记录，以及可见性感知优化机制，实现了高效的跨机器人体现的数据生成，显著提升了数据收集效率和泛化能力。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [105] [CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance](https://arxiv.org/abs/2511.09331)
*Stepan Dergachev,Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: CoRL-MPPI结合合作强化学习和MPPI，优化多机器人避障导航，实验验证其高效性与安全性。


<details>
  <summary>Details</summary>
Motivation: MPPI控制器在实践中可能因依赖随机采样而生成次优轨迹，需改进以提升导航性能。

Method: 提出CoRL-MPPI方法，将基于深度神经网络的合作强化学习策略嵌入MPPI框架，优化其采样分布。

Result: 实验表明，CoRL-MPPI在密集动态环境中优于现有基线（如ORCA、BVC等），显著提高了成功率和任务完成时间。

Conclusion: CoRL-MPPI通过结合合作强化学习和MPPI，显著提升了多机器人系统的导航效率和安全性，同时保留了MPPI的理论保证。

Abstract: Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

</details>


### [106] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: SPIDER框架通过物理重定向将人类运动数据转化为机器人可执行的动态轨迹，解决了数据稀缺问题，提高了策略学习的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 由于收集机器人特定数据成本高昂，而丰富的人类运动数据（如动作捕捉、视频和虚拟现实）可以弥补数据稀缺问题，但因体现差距和缺失动态信息（如力和扭矩），这些演示无法直接在机器人上执行。

Method: SPIDER是一个基于物理的重新定位框架，通过全局任务结构和目标提供的人类演示，结合大规模基于物理的采样和课程式虚拟接触指导，将仅包含运动学信息的人类演示转化为动态可行的机器人轨迹。

Result: SPIDER在9种人形/灵巧手体现和6个数据集中表现出色，成功率比标准采样提高18%，比强化学习基线快10倍，并生成了一个包含2.4M帧的动态可行机器人数据集。

Conclusion: SPIDER作为一种通用的基于物理的重新定位方法，能够处理多样化的数据质量，生成多样且高质量的数据，从而支持如强化学习等方法的高效策略学习。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [107] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: WMPO框架通过像素预测和对齐VLA特征，解决了VLA模型和RL的局限，实现了高效、自适应的机器人操控。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖专家演示，无法从失败中学习；RL在真实机器人上样本复杂度高。

Method: WMPO采用基于像素的预测方法，将"想象"轨迹与预训练的VLA特征对齐，并支持在策略GRPO。

Result: WMPO在仿真和真实机器人实验中均表现出色，包括样本效率提升、性能增强及新兴行为的展现。

Conclusion: WMPO是一个高效的VLA RL框架，显著提升了样本效率、整体性能，并展现出自我纠正、泛化和终身学习等新兴行为。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [108] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: MAP-VLA通过记忆提示增强预训练VLA模型，显著提升长时程机器人操作任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因缺乏记忆机制，仅依赖即时感知输入，难以处理长时程任务。

Method: MAP-VLA通过构建记忆库（包含任务各阶段的记忆单元）和实时检索匹配，动态整合记忆提示到冻结的VLA模型中。

Result: MAP-VLA在仿真基准和真实机器人评估中分别实现了7.0%和25.0%的绝对性能提升，超越现有方法。

Conclusion: MAP-VLA框架通过引入记忆提示机制，显著提升了预训练VLA模型在长时程机器人操作任务中的性能，实验证明了其优越性。

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>


### [109] [SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation](https://arxiv.org/abs/2511.09555)
*Hao Shi,Bin Xie,Yingfei Liu,Yang Yue,Tiancai Wang,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: SpatialActor解耦语义与几何，提升机器人操作的鲁棒性和泛化能力，噪声条件下性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的点基和图像基方法在机器人操作中存在语义丢失或对深度噪声敏感的问题，且忽视了对精确交互至关重要的低层空间线索。

Method: 提出SpatialActor框架，包含语义引导的几何模块和空间变换器，分别处理几何信息和低层空间线索。

Result: 在RLBench上达到87.4%的SOTA性能，噪声条件下提升13.9%至19.4%，显著增强少样本泛化能力。

Conclusion: SpatialActor通过解耦语义和几何信息，提出了一种鲁棒的机器人操作框架，显著提高了在噪声条件下的性能，并增强了少样本泛化能力。

Abstract: Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [110] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 本文首次通过MLIR中的OpenMP目标指令实现了选择性代码卸载到FPGA，展示了MLIR生态系统的灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律的放缓，FPGA等异构计算平台在加速HPC工作负载方面受到越来越多的关注。本文旨在填补选择性代码卸载到FPGA的开源实现的空白。

Method: 该方法结合了MLIR OpenMP方言和高级综合（HLS）方言，提供了一个面向FPGA的可移植编译流程。与之前依赖自定义编译器的OpenMP FPGA工作不同，本文通过MLIR集成支持任何兼容MLIR的前端（如Flang）。

Result: 本文方法通过标准OpenMP指令支持手动优化卸载的内核，显著减少了所需工作量，并展示了MLIR生态系统的可组合性优势。

Conclusion: 本文提出了一种通过MLIR中的OpenMP目标指令实现选择性代码卸载到FPGA的创新方法，展示了MLIR生态系统的可组合性优势，并为基于指令的FPGA加速提供了一条灵活且可扩展的路径。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [111] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: Using energy resources to decouple datacenter power from grid load reduces carbon emissions effectively, especially with optimized distribution and DC-grid cooperation, though some sites may need grid support.


<details>
  <summary>Details</summary>
Motivation: Address the carbon footprint concerns of AI and cloud datacenters by leveraging DC flexibility to improve renewable energy absorption in the grid.

Method: Defined and computed the power and energy needs of datacenter load decoupling, then evaluated distribution and management approaches, including DC-grid cooperation.

Result: Optimized distribution achieves >98% potential grid carbon reduction with 70% decoupling need; DC-grid cooperation enhances carbon reduction by 1.4x; economic benefits generally outweigh local decoupling costs.

Conclusion: Decoupling datacenter power capacity from grid load through optimized energy resource distribution and management can significantly reduce carbon emissions, with economic viability on average. However, site-specific variations may necessitate grid intervention.

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [112] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 评估四大云服务提供商的HPC性能与成本，AWS最快但贵，OCI最经济但慢，Azure中等，GCP在AMD上表现佳但ARM表现差。实例选择需根据速度或成本优先级决定。


<details>
  <summary>Details</summary>
Motivation: 评估虚拟化云基础设施中HPC风格的CPU性能和成本，以帮助用户根据工作负载需求选择最优的云服务提供商和实例类型。

Method: 使用SPEC ACCEL套件中的OpenMP工作负载子集，评估了四大云服务提供商（AWS、Azure、GCP、OCI）在Intel、AMD和ARM通用实例类型下的性能和成本，比较了按需和一年折扣定价。

Result: AWS在所有三种实例类型中提供最短的运行时间，但收费较高，尤其是按需使用；OCI是所有CPU家族中最经济的选择，尽管运行速度较慢；Azure表现中等；GCP在Intel到AMD的转换中表现显著提升，但其ARM实例性能较差且价格较高。AWS的ARM实例在某些情况下比其Intel和AMD实例快49%。

Conclusion: 实例选择和云服务提供商的选择会在运行时和价格上产生显著差异，建议根据工作负载的优先级（原始速度或成本最小化）来指导实例类型的选择。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [113] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 本文提出企业级隐私保护联邦学习框架愿景，强调可扩展性、无缝部署和全面隐私保护，旨在弥合研究与实际部署的差距。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在科学领域中因数据隐私、所有权和合规性约束而至关重要，但构建既用户友好又兼具可扩展性和隐私保护的企业级框架仍具挑战性。

Method: 基于构建高级隐私保护联邦学习（APPFL）框架的经验，提出了包括可扩展本地模拟、无缝过渡到部署、分布式部署、多级抽象以及全面的隐私和安全技术在内的关键能力。

Result: 提出了一个旨在跨计算环境无缝扩展的企业级隐私保护联邦学习框架，并讨论了实现这些目标的架构设计。

Conclusion: 本文提出了一个企业级、隐私保护的联邦学习框架愿景，旨在弥合研究原型与企业规模部署之间的差距，实现可扩展、可靠且保护隐私的科学AI。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [114] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG通过软件框架优化MIG的分配模型，减少碎片化并提升集群效率。


<details>
  <summary>Details</summary>
Motivation: GPU集群在多租户环境中常因利用率不足而导致资源浪费，现有的NVIDIA MIG技术虽然提供了硬件级隔离，但其硬件刚性和传统的一对一分配模型共同导致了严重的碎片化和集群范围的利用率不足。

Method: Flex-MIG是一个纯软件框架，采用一对多的分配模型，并支持跨MIG实例的主机共享内存集体操作，无需硬件修改。

Result: Flex-MIG消除了需要排水的重新配置，减少了碎片化，并在多样化的跟踪中提高了17%的完成时间。

Conclusion: Flex-MIG通过软件协调层重新思考MIG的操作模型，显著提高了集群效率，减少了碎片化，并将完成时间缩短了最高17%。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [115] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 论文提出CMP技术，解决了高并发队列的复杂性问题，恢复了队列的简单性并保持严格FIFO语义，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有无锁队列在并发环境下为保证正确性引入了大量复杂性，且往往牺牲了严格FIFO、无界容量或无锁进展。AI时代的高并发需求（数百至数千线程）使得这些协调开销成为瓶颈。

Method: 提出了循环内存保护（CMP）技术，通过有限保护窗口提供实际的重用保证。通过线性化分析和有限重用分析证明了严格FIFO和安全性。

Result: CMP在高竞争环境下性能优于现有无锁队列1.72-4倍，并保持对数百线程的可扩展性。

Conclusion: 论文通过引入循环内存保护（CMP）技术，成功地在保持严格FIFO语义、无界容量和无锁进展的同时，恢复了队列的简单性。CMP在高并发环境下表现出色，性能优于现有无锁队列1.72-4倍，并支持数百线程的扩展。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [116] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: CES是一种针对用户空间任务（如协程）的新型调度方法，通过避免内核级调度视角的同步原语，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言支持协程等用户空间任务，但现有同步原语从内核调度视角设计，导致不必要的延迟和吞吐量限制。

Method: 开发了Combine-and-Exchange Scheduling (CES)，一种确保竞争临界区保留在同一执行线程的新调度方法。

Result: CES在应用基准测试中实现了3倍性能提升，微基准测试中实现了8倍性能提升。

Conclusion: CES作为一种新颖的调度方法，显著提升了用户空间任务的同步性能，适用于多种现有语言和库。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [117] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA是一种针对空间数据流架构的编程语言，显著简化了编程复杂性，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 空间数据流架构在AI和科学应用中表现出色，但由于需要显式编排数据移动和异步计算，编程这些架构仍然具有挑战性。

Method: SPADA是一种编程语言，提供了对数据放置、数据流模式和异步操作的精确控制，同时抽象了架构特定的低级细节。

Result: SPADA使开发者能够用比CSL少6-8倍的代码表达复杂的并行模式，并在三个数量级范围内实现了接近理想的弱扩展。

Conclusion: SPADA通过统一空间数据流架构的编程模型，提升了这些高性能计算平台的理论基础和实践可用性。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [118] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 通过CSP和PAT验证Python联邦学习框架中的TDM通信算法，确保其无死锁且能成功终止。


<details>
  <summary>Details</summary>
Motivation: 为边缘系统中的联邦学习提供一个简单框架，并确保其通用算法（尤其是TDM通信）的可靠性与正确性。

Method: 采用CSP过程代数构建模型，分两阶段验证：第一阶段将Python代码忠实转换为CSP模型；第二阶段通过模型检查器PAT自动验证算法的无死锁性和成功终止性。

Result: 成功验证了TDM通信算法的安全性和活性，扩展了先前对集中式和分散式联邦学习算法的验证工作。

Conclusion: 本文通过CSP过程代数方法正式验证了Python联邦学习算法测试床中的第三种通用算法（TDM通信），证明了其无死锁（安全性）和成功终止（活性）的正确性。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>


### [119] [LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication](https://arxiv.org/abs/2511.09557)
*Prajwal Singhania,Siddharth Singh,Lannie Dalton Hough,Akarsh Srivastava,Harshitha Menon,Charles Fredrick Jekel,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 研究多节点分布式推理性能瓶颈，提出NVRAR算法优化all-reduce操作，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的增大，分布式推理在多GPU和多节点上的高效扩展变得至关重要。

Method: 使用多种先进推理引擎及研究原型YALIS进行实验，分析不同模型并行方案的强扩展性，并开发NVRAR算法优化all-reduce操作。

Result: NVRAR在HPE Slingshot和InfiniBand互连上比NCCL降低1.9x-3.6x延迟，集成到YALIS后使Llama 3.1 405B模型在多节点解码密集型工作负载中的端到端批量延迟降低1.72x。

Conclusion: 本文通过实验分析多节点分布式推理的性能瓶颈，并提出了一种基于NVSHMEM的层次化all-reduce算法NVRAR，显著降低了延迟。

Abstract: As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM和AMR图的方法，将英语自然语言转换为ASP程序，用于解决逻辑谜题，减少了LLM的依赖，提高了系统的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多不熟悉编程语言的用户需要与代码交互，ASP作为一种强大的组合问题解决工具，需要更易用的方法来降低学习门槛。

Method: 使用LLM简化自然语言句子、识别关键词并生成简单事实，然后通过AMR图解析简化语言并系统生成ASP约束。

Result: 系统成功生成了完整的ASP程序，能够解决组合逻辑问题，展示了该方法的有效性。

Conclusion: 该论文提出了一种新颖的方法，通过结合LLM和AMR图，将无约束英语转换为ASP程序，有效解决了逻辑谜题的组合问题，为创建更轻量级、可解释的自然语言到逻辑问题的转换系统迈出了重要一步。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [121] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 提出一种基于VSA的ARC-AGI求解器，结合认知合理性方法，在多个基准测试中表现优异且高效。


<details>
  <summary>Details</summary>
Motivation: 受人类智能建模方法的启发，旨在解决当前人工智能系统在ARC-AGI基准测试中表现不佳的问题。

Method: 通过对象中心程序合成方法，结合向量符号代数（VSA）表示抽象对象、指导解决方案搜索，并实现高效的神经学习。

Result: 在ARC-AGI-1-Train和ARC-AGI-1-Eval上分别获得10.8%和3.0%的得分，在Sort-of-ARC和1D-ARC上表现优异，且计算成本远低于GPT-4。

Conclusion: 该论文提出了一种基于向量符号代数（VSA）的认知合理ARC-AGI求解器，其在ARC-AGI基准测试中表现优于现有方法，且计算成本极低。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [122] [Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning](https://arxiv.org/abs/2511.08749)
*Mehrdad Zakershahrak*

Main category: cs.AI

TL;DR: QDIN提出将RL系统设计为多查询推理引擎，专用模块实现高精度推理（99%可达性），与传统控制性能（31%回报）解耦。


<details>
  <summary>Details</summary>
Motivation: 挑战传统RL单一目标范式，探索RL系统作为可回答多样环境查询的推理引擎的潜力。

Method: 引入了Query Conditioned Deterministic Inference Networks (QDIN)，一种统一架构，针对不同查询类型（策略、可达性、路径、比较）优化专用神经模块。

Result: 实验表明，专用查询架构在推理精度上（如99%可达性IoU）显著优于统一模型和后处理方法，同时保持竞争力的控制性能（31%回报）。

Conclusion: QDIN架构提出了将RL系统设计为可查询知识库的研究方向，对可解释性、验证和人类-AI协作具有深远影响。

Abstract: Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.

</details>


### [123] [Neural Value Iteration](https://arxiv.org/abs/2511.08825)
*Yang You,Ufuk Çakır,Alex Schutz,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 利用神经网络表示POMDP值函数，提出神经值迭代算法，解决大规模POMDP问题，效果接近最优。


<details>
  <summary>Details</summary>
Motivation: 传统的基于点值迭代的POMDP求解器在处理大规模问题时，由于α向量的高维性导致Bellman备份计算成本过高，难以实现。

Method: 采用神经网络的泛化能力与经典值迭代框架相结合的方法，提出了一种新的POMDP规划算法——神经值迭代。

Result: 神经值迭代算法在传统方法无法处理的大规模POMDP中，仍能实现接近最优的解。

Conclusion: 本文提出了一种名为“神经值迭代”的新算法，通过神经网络表示POMDP的值函数，成功解决了大规模POMDP问题中传统方法计算成本过高的问题，实现了接近最优的解决方案。

Abstract: The value function of a POMDP exhibits the piecewise-linear-convex (PWLC) property and can be represented as a finite set of hyperplanes, known as $α$-vectors. Most state-of-the-art POMDP solvers (offline planners) follow the point-based value iteration scheme, which performs Bellman backups on $α$-vectors at reachable belief points until convergence. However, since each $α$-vector is $|S|$-dimensional, these methods quickly become intractable for large-scale problems due to the prohibitive computational cost of Bellman backups. In this work, we demonstrate that the PWLC property allows a POMDP's value function to be alternatively represented as a finite set of neural networks. This insight enables a novel POMDP planning algorithm called \emph{Neural Value Iteration}, which combines the generalization capability of neural networks with the classical value iteration framework. Our approach achieves near-optimal solutions even in extremely large POMDPs that are intractable for existing offline solvers.

</details>


### [124] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: UCO通过多轮交互强化学习，结合进步奖励和支架奖励，动态适应学生认知水平，显著提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有监督微调方法缺乏动态适应能力，以及强化学习方法无法区分学生真实理解与简单模仿的问题。

Method: 提出单向认知优化（UCO）方法，采用多轮交互式强化学习范式，结合两个协同奖励函数：进步奖励和支架奖励。

Result: UCO模型在基准测试中表现优于所有同等规模的基线模型，性能接近先进闭源模型。

Conclusion: UCO模型在BigMath和MathTutorBench基准测试中表现优异，超越同等规模的所有基线模型，性能接近先进的闭源模型。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [125] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是首个能在复杂3D开放世界中实时完成长时间任务的通用代理，通过端到端的人机交互范式实现高效任务执行，并展示出强大的跨游戏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂3D开放世界中实时完成长时间任务的通用代理，以推动开放环境通用代理的发展。

Method: Lumine采用端到端的人机交互范式，结合感知、推理和行动，通过视觉语言模型驱动。它以5Hz处理原始像素，生成精确的30Hz键鼠动作，并在需要时自适应调用推理。

Result: Lumine在《原神》中成功完成了五小时的主线任务，效率与人类相当，并在零样本跨游戏泛化中表现优异，如在《鸣潮》和《崩坏：星穹铁道》中完成任务。

Conclusion: Lumine展示了在开放世界中开发通用代理的有效性，标志着在开放环境通用代理领域的实质性进展。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [126] [The Double Contingency Problem: AI Recursion and the Limits of Interspecies Understanding](https://arxiv.org/abs/2511.08927)
*Graham L. Bishop*

Main category: cs.AI

TL;DR: 本文探讨AI系统与物种递归交流的相互作用，提出双重偶然性问题，并主张将生物声学AI重新构想为递归认知形式之间的外交接触。


<details>
  <summary>Details</summary>
Motivation: 当前生物声学AI系统虽在跨物种性能上表现出色，但忽视了AI系统自身的信息处理可能系统性模糊或扭曲其他物种的交流结构这一根本问题。

Method: 通过借鉴哲学家Yuk Hui关于递归性和偶然性的研究，分析AI系统与物种间递归交流过程的相遇问题。

Result: 作者提出了双重偶然性问题，指出AI系统与物种间交流的相互作用需要新的概念框架。

Conclusion: 本文提出需要重新构想生物声学AI，从通用模式识别转向不同递归认知形式之间的外交接触，这对模型设计、评估框架和研究方法具有深远影响。

Abstract: Current bioacoustic AI systems achieve impressive cross-species performance by processing animal communication through transformer architectures, foundation model paradigms, and other computational approaches. However, these approaches overlook a fundamental question: what happens when one form of recursive cognition--AI systems with their attention mechanisms, iterative processing, and feedback loops--encounters the recursive communicative processes of other species? Drawing on philosopher Yuk Hui's work on recursivity and contingency, I argue that AI systems are not neutral pattern detectors but recursive cognitive agents whose own information processing may systematically obscure or distort other species' communicative structures. This creates a double contingency problem: each species' communication emerges through contingent ecological and evolutionary conditions, while AI systems process these signals through their own contingent architectural and training conditions. I propose that addressing this challenge requires reconceptualizing bioacoustic AI from universal pattern recognition toward diplomatic encounter between different forms of recursive cognition, with implications for model design, evaluation frameworks, and research methodologies.

</details>


### [127] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 该研究构建了AI与大数据集成的业务流程优化模型，通过三层架构实现实时监控，显著提升效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的深入，业务流程优化成为提升企业竞争力的关键。

Method: 采用三层架构（数据处理、AI算法、业务逻辑）结合分布式计算与深度学习技术，实现实时流程监控与优化。

Result: 实验验证显示，模型缩短流程处理时间42%，资源利用率提升28%，运营成本降低35%，高并发下系统可用性达99.9%。

Conclusion: 本研究通过整合人工智能与大数据的业务流程优化模型，显著提升了企业运营效率，为数字化转提供了重要理论与实践价值。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [128] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast是一种结合人类智慧与LLM智能的互动式时间序列预测框架，通过两阶段（认知基础构建与推理优化）显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法缺乏人类专家的交互、推理和适应性能力，限制其在复杂现实环境中的实用性。

Method: AlphaCast框架分为两个阶段：自动化预测准备（构建多源认知基础）和生成推理与反思优化（触发元推理循环进行自我修正与策略优化）。

Result: 在短期和长期数据集上的广泛实验表明，AlphaCast在预测准确性上 consistently 优于现有最先进的基线方法。

Conclusion: AlphaCast通过结合人类智慧与大型语言模型智能，重新定义了时间序列预测为互动过程，显著提升了预测准确性，并在实验中优于现有基线方法。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [129] [Heterogeneous Graph Neural Networks for Assumption-Based Argumentation](https://arxiv.org/abs/2511.08982)
*Preesha Gehlot,Anna Rapberger,Fabrizio Russo,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出两种GNN模型（ABAGCN和ABAGAT）近似计算ABA中的可信接受，实验显示其优于现有方法，并开发了高效的扩展重构算法。


<details>
  <summary>Details</summary>
Motivation: 由于在大型框架下精确计算稳定语义下的扩展是难以处理的，因此需要一种近似方法来提高计算效率。

Method: 提出了两种GNN架构——ABAGCN和ABAGAT，分别通过堆叠残差异构卷积层或注意力层来学习节点嵌入，并开发了一个多项式时间的扩展重构算法。

Result: 实验结果表明，ABAGCN和ABAGAT在ICCMA 2023基准测试中优于现有GNN基线，节点级F1分数最高达0.71，扩展重构算法在小型和大型ABA框架中分别达到0.85和0.58的F1分数。

Conclusion: 本文展示了首个利用图神经网络（GNN）近似计算基于假设的论证（ABA）中可信接受的方法，并通过实验验证了其有效性，为结构化论证的可扩展近似推理开辟了新途径。

Abstract: Assumption-Based Argumentation (ABA) is a powerful structured argumentation formalism, but exact computation of extensions under stable semantics is intractable for large frameworks. We present the first Graph Neural Network (GNN) approach to approximate credulous acceptance in ABA. To leverage GNNs, we model ABA frameworks via a dependency graph representation encoding assumptions, claims and rules as nodes, with heterogeneous edge labels distinguishing support, derive and attack relations. We propose two GNN architectures - ABAGCN and ABAGAT - that stack residual heterogeneous convolution or attention layers, respectively, to learn node embeddings. Our models are trained on the ICCMA 2023 benchmark, augmented with synthetic ABAFs, with hyperparameters optimised via Bayesian search. Empirically, both ABAGCN and ABAGAT outperform a state-of-the-art GNN baseline that we adapt from the abstract argumentation literature, achieving a node-level F1 score of up to 0.71 on the ICCMA instances. Finally, we develop a sound polynomial time extension-reconstruction algorithm driven by our predictor: it reconstructs stable extensions with F1 above 0.85 on small ABAFs and maintains an F1 of about 0.58 on large frameworks. Our work opens new avenues for scalable approximate reasoning in structured argumentation.

</details>


### [130] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 通过多代理递归优化（RR）实现渐进式搜索（GIS），显著提升LLM推理能力，复杂模型表现优于简单模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）表现出卓越的流畅性，但仍需提升其推理能力，本文提出通过结构化多代理管道实现GIS搜索以优化推理。

Method: 设计实验比较简单线性管道与包含递归优化层的复杂结构化管道，利用RAG驱动的语料库构建多代理模型，模拟美国三位开国元勋的历史人物形象，生成对当代政治问题的回答。

Result: 复杂模型在所有九个测试案例中均优于简单模型，平均仲裁分数为88.3对71.7，论证在分析深度、结构细节和战略框架上更优。

Conclusion: 递归优化（RR）是通过渐进、增量、顺序（GIS）搜索增强LLM推理能力的强大架构特征。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [131] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: Argus是一个提升自动驾驶系统韧性的运行时框架，通过持续监控和危险缓解显著提高安全性和驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在公共道路上部署时会面临多样化的驾驶危险，需要具备持续监控和自适应响应的能力以保障安全。

Method: 提出运行时韧性导向框架Argus，持续监控ADS生成的轨迹，通过危险缓解器在必要时接管控制。

Result: Argus将ADS的驾驶评分平均提升高达150.30%，并预防了高达64.38%的违规行为。

Conclusion: Argus框架有效提升了端到端自动驾驶系统的韧性，显著提高了驾驶评分并减少了安全违规，且额外时间开销极小。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [132] [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)
*Elliot Meyerson,Giuseppe Paolo,Roberto Dailey,Hormoz Shahrzad,Olivier Francon,Conor F. Hayes,Xin Qiu,Babak Hodjat,Risto Miikkulainen*

Main category: cs.AI

TL;DR: MAKER通过极端任务分解和多智能体投票纠错，首次实现零错误完成超百万步LLM任务，为大规模代理流程（MDAPs）提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理、洞察和工具使用方面取得了显著突破，但在执行类似人类、组织和社会规模的扩展任务时，由于持续的错误率而无法实现规模化。

Method: 采用极端任务分解，将任务拆分为可由微代理处理的子任务，并通过高效的多智能体投票方案在每一步进行纠错。

Result: MAKER系统成功解决了超百万步LLM任务且零错误，展示了其在更大规模任务上的潜在扩展能力。

Conclusion: MAKER系统通过极端任务分解和多智能体投票纠错机制，首次实现了零错误完成超百万步LLM任务，并展示了大规模分解代理流程（MDAPs）在解决组织和社会层面问题上的潜力。

Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.

</details>


### [133] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文探讨了AI在自主应急车辆中的应用，分析了扩散模型和大型语言模型如何提升传统强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在动态应急场景中样本效率低且缺乏适应性，本文旨在解决这些限制。

Method: 本文回顾了从传统强化学习到扩散模型增强的强化学习，再到大型语言模型辅助的上下文学习的转变。

Result: 扩散模型增强的强化学习通过合成数据生成增强了策略鲁棒性，而大型语言模型辅助的上下文学习提供了轻量级且可解释的替代方案。

Conclusion: 本文通过分析扩散模型增强的强化学习和大型语言模型辅助的上下文学习，为下一代自主应急响应系统提供了关键框架。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [134] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1是一个数据高效的训练框架，通过两阶段设计（SFT+TGRPO）显著提升自动化OR优化建模与求解的准确率和效率，降低数据需求。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动化方面提供了新机会，但现有方法的泛化能力和数据效率有限，需要大量标注或合成数据，导致高成本和可扩展性障碍。

Method: OR-R1采用两阶段设计：首先通过监督微调（SFT）从有限标注数据中学习问题表述和代码生成的基本推理模式；然后通过测试时组相对策略优化（TGRPO）提升能力和一致性。

Result: OR-R1在仅使用1/10合成数据的情况下，平均求解准确率达到67.7%，比ORLM提升4.2%。使用仅100个合成样本时，仍优于ORLM 2.4%。TGRPO进一步带来3.1%-6.4%的准确率提升，显著缩小单次与多次尝试的性能差距。

Conclusion: OR-R1提供了一个稳健、可扩展且成本效益高的解决方案，用于自动化OR优化问题的建模与求解，降低了工业OR应用的专业知识和数据门槛。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [135] [History-Aware Reasoning for GUI Agents](https://arxiv.org/abs/2511.09127)
*Ziwei Wang,Leyang Yang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: HAR框架通过增强短期记忆和反思学习，显著提升了GUI代理的长时序任务表现，HAR-GUI-3B模型在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在显式推理中短期记忆薄弱，无法感知历史交互，导致长时序任务表现不佳。

Method: 提出历史感知推理（HAR）框架，包括构建反思学习场景、合成定制纠正指南和设计混合RL奖励函数。

Result: HAR-GUI-3B模型成功将推理模式从历史无关转变为历史感知，提升了GUI自动化性能。

Conclusion: HAR-GUI-3B模型通过历史感知推理框架显著提升了GUI代理的短期记忆和屏幕细节感知能力，在多个基准测试中表现出色。

Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.

</details>


### [136] [ProBench: Benchmarking GUI Agents with Accurate Process Information](https://arxiv.org/abs/2511.09157)
*Leyang Yang,Ziwei Wang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: ProBench 是一个新基准测试，通过过程相关任务评估 GUI 代理性能，揭示其在实际应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI 代理评估仅关注最终屏幕状态，忽视了任务中多步链式操作及中间关键信息的重要性，导致评估不全面。

Method: 引入 ProBench 基准测试，包含 200 多个挑战性 GUI 任务，涵盖广泛场景。除传统的状态相关任务评估外，还扩展了过程相关任务，并设计专门评估方法，通过过程提供者自动提供准确过程信息。

Result: 评估显示，当前先进 GUI 代理在真实场景中存在显著局限性，普遍存在于大型通用模型和小型专用模型中。详细错误分析揭示了多个普遍问题。

Conclusion: ProBench 是一个全面的移动基准测试，通过引入过程相关任务和专门设计的评估方法，揭示了当前 GUI 代理在真实场景中的显著局限性，为未来改进指明了方向。

Abstract: With the deep integration of artificial intelligence and interactive technology, Graphical User Interface (GUI) Agent, as the carrier connecting goal-oriented natural language and real-world devices, has received widespread attention from the community. Contemporary benchmarks aim to evaluate the comprehensive capabilities of GUI agents in GUI operation tasks, generally determining task completion solely by inspecting the final screen state. However, GUI operation tasks consist of multiple chained steps while not all critical information is presented in the final few pages. Although a few research has begun to incorporate intermediate steps into evaluation, accurately and automatically capturing this process information still remains an open challenge. To address this weakness, we introduce ProBench, a comprehensive mobile benchmark with over 200 challenging GUI tasks covering widely-used scenarios. Remaining the traditional State-related Task evaluation, we extend our dataset to include Process-related Task and design a specialized evaluation method. A newly introduced Process Provider automatically supplies accurate process information, enabling presice assessment of agent's performance. Our evaluation of advanced GUI agents reveals significant limitations for real-world GUI scenarios. These shortcomings are prevalent across diverse models, including both large-scale generalist models and smaller, GUI-specific models. A detailed error analysis further exposes several universal problems, outlining concrete directions for future improvements.

</details>


### [137] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 论文提出CRM和CRF解决LRMs的过度思考问题，实验显示其提升准确率并减少令牌长度，且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如DeepSeek-R1和OpenAI o1）常因过度思考生成冗余推理步骤，增加计算成本。现有方法（如长度惩罚）存在长度崩溃和训练崩溃问题。

Method: 通过训练一个简洁性奖励模型（CRM）并引入新颖的奖励函数（CRF），建立了结果奖励与简洁性分数之间的显式依赖关系。

Result: 在五个数学基准数据集上的实验表明，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应令牌长度减少，且能泛化到其他LLMs（如Llama和Mistral）。

Conclusion: 该论文提出了一种新的奖励模型（CRM）和奖励函数（CRF），有效解决了大型推理模型（LRMs）中常见的过度思考问题，并在理论和实验上验证了其优越性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [138] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 论文提出双层监控框架（离群检测+透明度层）以提升代理型AI系统的操作可靠性，为高风险领域提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统在高风险领域（如医疗和工业）中因可靠性不足而存在潜在风险，需要缓解技术。

Method: 通过分析代理型AI系统的特性，作者提出了一个包含离群检测层和AI透明度层的双层监控框架。

Result: 提出的双层监控框架能够为人类操作员提供决策支持，以判断输出是否可靠并采取干预措施。

Conclusion: 该论文提出了一个双层的可靠性监控框架，为降低代理型AI系统在操作中的不可靠性风险提供了基础。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [139] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: MedFuse通过乘法融合模块处理不规则临床时间序列，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的临床时间序列存在不规则性（异步采样、缺失值、异质性），现有嵌入策略未能有效捕捉值依赖的特征交互。

Method: 提出MedFuse框架，核心是MuFuse模块，通过乘法调制融合特征和值的嵌入，保留特征特定信息并建模高阶依赖。

Result: 在三个真实世界数据集上，MedFuse在关键预测任务上均优于现有基线，乘法融合增强了表示的表达能力。

Conclusion: MedFuse通过乘法融合模块（MuFuse）在建模不规则临床时间序列方面表现出色，其泛化能力通过跨数据集预训练得到验证。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [140] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: HyperD通过解耦周期性和非周期性成分，结合新颖的模块和对齐损失，显著提升了交通预测性能。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临复杂的空间依赖性和多尺度周期性模式与不规则波动的共存问题，现有方法难以有效处理这些挑战。

Method: 提出HyperD框架，包含混合周期性表示模块（处理周期性成分）和频率感知残差表示模块（处理非周期性成分），并引入双视角对齐损失以确保语义分离。

Result: HyperD在四个真实交通数据集上实现了最先进的预测精度，并展现出更好的抗干扰能力和计算效率。

Conclusion: HyperD框架通过解耦交通数据的周期性和残差成分，结合空间-时间注意力和频率感知模块，显著提升了交通预测的准确性和鲁棒性，并在四个真实数据集上验证了其优越性。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


### [141] [From Model Training to Model Raising -- A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 论文提出'模型培养'新范式，通过重构训练语料库，从第一训练标记起即融入价值观对齐，以解决现有方法导致的模型易失联问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法在模型核心能力确立后才进行价值观对齐，导致模型容易失联且缺乏深层次价值观系统，这在大型语言模型能力逐渐超越人类的背景下尤为关键。

Method: 通过重新设计训练语料库，包括从第一人称视角重构训练数据、将信息重新情境化为生活体验、模拟社交互动以及搭建训练数据的顺序框架。

Result: 预期通过训练语料库的重新设计，模型从第一个训练标记开始就能早期承诺价值观，使知识、技能和价值观内在更难以分离。

Conclusion: 论文提出了一种从'模型训练'转向'模型培养'的范式转变，强调从一开始就将价值观对齐融入模型开发过程中，以构建根深蒂固的价值观系统。

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [142] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 文章呼吁开发透明、可重复、隐私友好的定性AI系统，以弥补当前AI在定性研究中的不足，促进多学科和混合方法研究的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在定量方法上取得了显著进展，但定性维度在科学理解中的整合仍然不足，这促使了开发专门定性AI系统的需求。

Method: 通过回顾近期文献，展示了如何通过增强定性能力来改进现有的自动化发现流程。

Result: 指出了安全定性AI在多学科和混合方法研究中的关键机遇。

Conclusion: 本文主张开发专门为解释性研究设计的定性人工智能系统，这些系统需具备透明性、可重复性和隐私友好性。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [143] [BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems](https://arxiv.org/abs/2511.09363)
*Ali Taheri,Alireza Taban,Sadegh Soudjani,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 论文提出基于LLM的框架，成功实现高成功率屏障证书合成，并发布BarrierBench基准测试，推动语言推理与形式化验证的整合。


<details>
  <summary>Details</summary>
Motivation: 当前屏障证书合成方法存在可扩展性差、依赖精心设计的模板、需大量手动专业知识等问题，论文旨在探索语言模型是否能捕获并操作这种专家推理。

Method: 论文提出了一种基于LLM的代理框架，利用自然语言推理提出、精化和验证候选证书，结合LLM驱动的模板发现和基于SMT的验证，支持屏障-控制器协同合成。

Result: 实验表明，框架在生成有效证书方面成功率超过90%，并通过BarrierBench基准测试验证了检索增强生成和代理协调策略的有效性。

Conclusion: 该论文通过引入基于LLM的代理框架和BarrierBench基准测试，展示了语言模型在捕获和操作专家推理方面的潜力，成功实现了90%以上的有效证书生成率，并推动了语言推理与形式化验证在动态系统中的整合。

Abstract: Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.
  This motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.
  To evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.
  The benchmark is publicly available at https://hycodev.com/dataset/barrierbench

</details>


### [144] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: GPT-5在PDDL规划任务中表现媲美专业规划器LAMA，纯推理测试中LLMs性能下降较少，显示显著进步。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在推理能力方面的进展，特别是前沿模型在端到端规划任务中的表现。

Method: 通过从PDDL领域和任务描述生成计划，评估了DeepSeek R1、Gemini 2.5 Pro、GPT-5以及LAMA在最新国际规划竞赛学习赛道子集中的表现。

Result: GPT-5在标准PDDL领域中的任务解决能力与LAMA相当；所有LLMs在纯推理测试中性能下降，但较之前模型表现更好。

Conclusion: 研究表明，最新的LLMs（特别是GPT-5）在标准PDDL领域中的表现与专业规划器LAMA相当，且在纯推理测试中性能下降较少，显示出较前代模型的显著改进。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [145] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 提出了一种基于潜在流匹配与无分类器指导的新方法，用于解耦潜在子空间，实验验证了其在高维数据特征访问中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在高维领域中，访问学习表示中的信息对科学发现至关重要。

Method: 基于潜在流匹配与无分类器指导的新方法，通过明确分离条件信息与剩余表示信息来解耦潜在子空间。

Result: 在合成2D高斯玩具问题、彩色MNIST和Galaxy10天文数据集的三个实验中，该方法能够访问高维数据的有意义特征。

Conclusion: 该方法为分析和控制潜在表示提供了一种简单而强大的机制，为利用生成模型进行科学探索开辟了途径。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [146] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: CrochetBench是一个评估多模态模型在钩针领域程序推理能力的基准，揭示了表面理解与实际执行的差距。


<details>
  <summary>Details</summary>
Motivation: 填补现有基准在高层次描述或视觉问答方面的不足，强调从描述到操作的转变，评估模型在钩针领域的实际执行能力。

Method: 采用CrochetPARADE DSL作为中间表示，进行结构验证和功能评估，涵盖针脚分类、指令接地、自然语言和图像到DSL翻译等任务。

Result: 模型性能从表面相似性到可执行正确性急剧下降，暴露了长距离符号推理和3D感知程序合成的局限性。

Conclusion: CrochetBench为评估多模态大语言模型在钩针领域的精细、低级程序推理能力提供了新的视角，揭示了表面理解与可执行精度之间的差距。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [147] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 提出了一种通过聚合多个生成模型来增强AI安全性的方法，利用共识采样算法实现与最安全子集相当的风险水平，并在缺乏一致时弃权。


<details>
  <summary>Details</summary>
Motivation: 现有的AI安全方法大多依赖于检查模型输出或激活，但某些风险无法通过这种方式检测到。因此，需要一种补充性的、与架构无关的方法来增强安全性。

Method: 论文提出了一种共识采样算法，该算法利用多个模型计算输出概率的能力，通过评估模型间的一致性和最安全子集的风险水平，实现安全性的增强。算法受到Vyas等人（2023）的可证明版权保护算法的启发。

Result: 共识采样算法能够在给定k个模型和提示的情况下，实现与最安全的s个模型平均风险相当的安全性，并在模型间缺乏足够一致时选择弃权。论文还证明了当足够多的模型是安全且表现出足够一致性时，弃权的概率可以被限制。

Conclusion: 该论文提出了一种新的、与架构无关的AI安全方法，通过聚合多个生成模型来增强安全性，即使某些风险无法通过检查单独检测到。该方法通过共识采样算法实现了与最安全的子集相当的风险水平，并在模型间缺乏足够一致时选择弃权。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [148] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: Physical AI 提出智能是身体与环境交互的涌现现象，通过六项原则构建闭环模型，并以辅助机器人实例验证。


<details>
  <summary>Details</summary>
Motivation: 旨在为智能系统的物理体现、感知、行动、学习和情境敏感性提供一个科学且系统的理论基础，区别于传统AI的数据驱动模型。

Method: 通过六个基本原则（体现、感知、动作、学习、自主性和情境敏感性）构建了一个闭环控制模型，展示了能量、信息、控制和情境的持续交互。

Result: 理论模型通过康复诊所中的自适应辅助机器人实例得到验证，展示了六项原则在实际系统中的交互作用。

Conclusion: Physical AI 提出了一种新的智能理论框架，强调智能是身体、环境和经验之间真实交互的涌现现象，而非单纯的数据处理或符号操作。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>


### [149] [Robust and Diverse Multi-Agent Learning via Rational Policy Gradient](https://arxiv.org/abs/2511.09535)
*Niklas Lauffer,Ameesh Shah,Micah Carroll,Sanjit A. Seshia,Stuart Russell,Michael Dennis*

Main category: cs.AI

TL;DR: 提出RPO和RPG方法，解决合作环境中对抗优化的自毁问题，并在多个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在合作环境中对抗优化导致的自毁行为，保持代理的理性。

Method: 引入了Rationality-preserving Policy Optimization (RPO)和Rational Policy Gradient (RPG)，通过对手塑造技术优化对抗目标。

Result: RPG扩展了现有对抗优化算法，成功应用于多个合作和一般和环境中。

Conclusion: RPO和RPG的创新方法有效解决了在合作环境中对抗优化导致的自毁行为，扩展了对抗优化算法的应用范围，并在多个合作和一般和环境中验证了其性能。

Abstract: Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io.

</details>


### [150] [Breadth-First Search vs. Restarting Random Walks for Escaping Uninformed Heuristic Regions](https://arxiv.org/abs/2511.09549)
*Daniel Platnick,Dawson Tomasz,Eamon Earl,Sourena Khanzadeh,Richard Valenzano*

Main category: cs.AI

TL;DR: 研究比较了BrFS和RRWs逃离UHRs的效果，发现EHC-RRW在理论和实验中均优于传统EHC，尤其适用于处理启发式局部最小值或平台。


<details>
  <summary>Details</summary>
Motivation: 贪婪搜索方法（如GBFS和EHC）在处理无启发式区域（如启发式局部最小值或平台）时表现不佳，因此需要研究更有效的逃离方法。

Method: 本研究首先从理论上推导了广度优先搜索（BrFS）和重启随机游走（RRWs）逃离UHRs的预期运行时间，然后通过实验比较了标准EHC和EHC-RRW在PDDL规划基准上的表现。

Result: 理论分析表明，RRWs在某些情况下预期运行时间更短；实验证实EHC-RRW在EHC有效的场景中具有更强的运行时保证。

Conclusion: EHC-RRW（使用重启随机游走的强制爬山算法）在理论上和实验中都显示出优于传统EHC（使用广度优先搜索）的性能，特别是在处理无启发式区域（UHRs）时。

Abstract: Greedy search methods like Greedy Best-First Search (GBFS) and Enforced Hill-Climbing (EHC) often struggle when faced with Uninformed Heuristic Regions (UHRs) like heuristic local minima or plateaus. In this work, we theoretically and empirically compare two popular methods for escaping UHRs in breadth-first search (BrFS) and restarting random walks (RRWs). We first derive the expected runtime of escaping a UHR using BrFS and RRWs, based on properties of the UHR and the random walk procedure, and then use these results to identify when RRWs will be faster in expectation than BrFS. We then evaluate these methods for escaping UHRs by comparing standard EHC, which uses BrFS to escape UHRs, to variants of EHC called EHC-RRW, which use RRWs for that purpose. EHC-RRW is shown to have strong expected runtime guarantees in cases where EHC has previously been shown to be effective. We also run experiments with these approaches on PDDL planning benchmarks to better understand their relative effectiveness for escaping UHRs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [151] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 综述2004年至今234篇故障分诊论文，分析挑战与进展，总结数据集与指标，展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的复杂性增加，故障分诊成为系统运维的关键流程，但异构数据量大，自动化分诊面临挑战。

Method: 对2004年至今的234篇论文进行系统综述，分析基本概念、系统架构及问题陈述，比较学术与工业研究目标，并总结开源数据集和评估指标。

Result: 识别了限制分诊系统实际部署的主要障碍，总结了广泛采用的开源数据集和评估指标，并提出了未来研究方向。

Conclusion: 本文通过综述234篇论文，总结了故障分诊领域的研究进展、挑战及未来方向，旨在促进学术创新与工业应用的更紧密结合。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [152] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 本文比较了Pandas、Polars和Dask在深度学习流程中的性能，填补了现有研究空白，发现它们在关键阶段表现各异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有文献中关于Python数据操作库在GPU密集型深度学习流程中性能评估的不足。

Method: 作者通过测量运行时、内存使用、磁盘占用及CPU/GPU能耗等关键指标，对比了三种Python数据操作库在不同模型和数据集中的表现。

Result: 研究结果显示三种库在数据加载、预处理和批量喂入等关键阶段的性能差异显著。

Conclusion: 该研究填补了现有文献的空白，通过评估Pandas、Polars和Dask在深度学习流程中的性能差异，为实际应用提供了实用建议。

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [153] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 研究分析回溯提交中的技术债务，发现4.3%的提交引入新债务，不同生态系统和开发者状态影响债务积累模式。


<details>
  <summary>Details</summary>
Motivation: 探讨在稳定源代码的回溯过程中，技术债务何时及为何产生，以帮助开发者更好地管理软件维护中的技术债务。

Method: 本研究分析了来自87个仓库的105,396次提交，涵盖三个软件生态系统（Apache、Eclipse、Python），通过回溯来源（31,076个）识别技术债务的产生时机和原因。

Result: 约4.3%的回溯提交引入了新的技术债务。Apache在绝对数量上贡献最多，而Python和Eclipse的债务提交比是Apache的近三倍。不同生态系统在发布周期的不同阶段积累技术债务的模式也不同。

Conclusion: 研究表明，约4.3%的回溯提交会引入新的技术债务，且不同生态系统（Apache、Eclipse、Python）在不同阶段的技术债务积累模式存在差异。开发者的经验、工作负荷和所有权状态也会影响技术债务的引入。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [154] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 本文提出自动化测试计划生成方法，以减少生产环境测试对服务的干扰，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 手动设计测试计划既繁琐又容易出错，尤其是在系统庞大复杂时更为困难，因此需要自动化方法来提高效率和准确性。

Method: 通过自动化方法生成测试计划，包括测试配置选择/生成、部署规划、测试运行调度以及选择降低干扰风险的策略等。

Result: 通过案例研究展示了自动化测试计划生成方法的不同方面，验证了其在减少生产环境干扰中的潜力。

Conclusion: 本文提出了一种自动化生成测试计划的方法，旨在减少生产环境中测试活动可能引起的服务中断，并通过案例研究验证了该方法的有效性。

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [155] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 提出了一种低数据域工业编码助手解决方案，通过微调小型本地模型和RAG技术，实现高质量代码生成，适用于边缘设备且无需依赖云服务。


<details>
  <summary>Details</summary>
Motivation: 由于PLC使用专有代码方言，训练编码助手具有挑战性；现有LLM虽能生成IEC 61131-3兼容代码，但不了解特定功能块或相关项目代码，且企业不信任云提供商。

Method: 使用小型本地模型进行微调，适用于边缘设备；多个AI模型相互竞争，利用推理、自动纠正错误，并通过在聊天界面中直接编译代码来检查有效性。

Result: 实现了高质量的代码生成，无需微调大型模型，支持边缘设备使用，并通过广泛评估（包括代码编译统计和用户评分）验证了方法的有效性。

Conclusion: 研究发现，通过检索增强生成（RAG）支持的编码助手可以在低数据域中工作，利用广泛的提示工程和定向检索。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [156] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: SPLVD通过动态选择训练数据和自步学习，显著提升漏洞检测准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在漏洞检测中准确率有限，主要原因是训练数据质量低。SPLVD旨在通过模拟人类学习过程解决这一问题。

Method: SPLVD采用自步学习方法，动态选择训练源代码，并设计了专门的数据选择器来优先学习简单代码。每个训练周期前重新计算代码难度并更新选择器。

Result: 在三个基准数据集上，SPLVD的F1得分分别为89.2%、68.7%和43.5%，优于现有方法。在OpenHarmony项目上，其精确度达到90.9%。

Conclusion: SPLVD通过动态选择训练数据并模拟人类学习过程，显著提高了漏洞检测的准确性和实用性。

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [157] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: AILINKPREVIEWER利用LLM生成PR链接预览，上下文摘要在指标上更优，但用户偏好非上下文摘要，显示性能与体验的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决PR中链接信息在自动化任务（如PR摘要生成和代码评审评论生成）中被丢弃的问题，以减少开发者的认知负荷和上下文切换。

Method: 通过分析50个GitHub仓库，比较了三种方法：上下文LLM摘要、非上下文LLM摘要和基于元数据的预览。评估指标包括BLEU、BERTScore和压缩比。

Result: 上下文摘要方法在指标上表现最佳，但用户研究显示多数参与者更偏好非上下文摘要。

Conclusion: 研究展示了LLM驱动的链接预览工具（AILINKPREVIEWER）在提升代码评审效率和丰富开发者上下文信息方面的潜力。尽管上下文摘要方法在指标上表现更优，但用户倾向于非上下文摘要，揭示了指标性能与用户体验之间的权衡。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [158] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）辅助用例建模能减少60%时间并保持质量，为软件工程师提供高效指导。


<details>
  <summary>Details</summary>
Motivation: 由于手动创建用例模型耗时且费力，实践中常被忽略，本研究探索了大型语言模型（LLMs）在这一繁琐过程中的辅助潜力。

Method: 本研究提出了一种方法，通过集成开放权重的大型语言模型（LLM）和高级提示工程技术，从软件需求中系统提取参与者和用例。

Result: 探索性研究结果显示，与传统手动建模相比，LLM方法将建模时间减少了60%，同时模型质量保持相当水平。

Conclusion: 研究表明，使用大型语言模型（LLMs）辅助用例建模可以显著提高效率，减少建模时间60%，同时保持模型质量，并为软件工程师提供有价值的指导。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [159] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 本文对Claude Code的配置生态系统进行了实证研究，分析了328个配置文件，发现明确架构等广泛关注点对代理编码系统的有效性至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然这些代理编码系统承诺带来前所未有的生产力提升，但其行为和效果高度依赖于定义架构约束、编码实践和工具使用策略的配置文件。然而，人们对这些配置工件的结构和内容知之甚少。

Method: 收集并分析了来自公开Claude Code项目的328个配置文件，以识别(i)它们指定的软件工程关注点和实践，以及(ii)这些关注点在单个文件中的共现情况。

Result: 结果强调了在代理配置文件中定义广泛的关注点和实践的重要性，特别是指定代理应遵循的架构。

Conclusion: 研究表明，在代理配置文件中定义广泛的关注点和实践至关重要，尤其是明确代理应遵循的架构。

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [160] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain是一种基于人类可解释概念的LLM路由器，用于软件相关任务，表现优于单个模型和黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件相关任务中表现广泛，但其性能在不同任务和同一任务内部存在显著差异，因此需要一种能够根据查询特性智能路由的方法。

Method: Routesplain首先从每个查询中提取人类可解释的概念（如任务、领域、推理复杂度），然后仅基于这些概念进行路由。

Result: Routesplain在16个先进LLMs和8个软件相关任务上评估，其准确性和成本均优于单个模型，且与黑盒基线相当或更优。

Conclusion: Routesplain通过提取人类可解释的概念进行路由，在多个软件相关任务中表现优异，不仅提升了响应质量，还降低了成本，同时为路由器的进一步改进提供了方向。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>
