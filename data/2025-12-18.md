<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 16]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Physically-Based Sky-Modeling For Image Based Lighting](https://arxiv.org/abs/2512.15632)
*Ian J. Maquignaz*

Main category: cs.CV

TL;DR: AllSky是一个直接从物理捕获的HDRI学习的天空模型，扩展了用户控制功能，展示了现有DNN模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在忠实再现自然天空方面存在不足，DNN生成的HDR图像在重新照明场景时无法达到与物理捕获HDR图像相同的色调、阴影和照明效果。

Method: 提出了AllSky，一个直接从物理捕获的HDRI学习的灵活全天候天空模型，用于研究输入模态、色调映射、条件设置和天空模型评估。

Result: AllSky模型在用户控制的环境地图和天空模型性能方面实现了最先进的表现，但现有DNN天空模型在可扩展性和下游应用中的准确照明方面存在限制。

Conclusion: AllSky模型在用户控制的太阳和云层位置方面扩展了当前功能，实现了最先进的天空模型性能，并展示了现有DNN天空模型与物理捕获的HDRI或参数化天空模型不可互换的局限性。

Abstract: Accurate environment maps are a key component for rendering photorealistic outdoor scenes with coherent illumination. They enable captivating visual arts, immersive virtual reality, and a wide range of engineering and scientific applications. Recent works have extended sky-models to be more comprehensive and inclusive of cloud formations but, as we demonstrate, existing methods fall short in faithfully recreating natural skies. Though in recent years the visual quality of DNN-generated High Dynamic Range Imagery (HDRI) has greatly improved, the environment maps generated by DNN sky-models do not re-light scenes with the same tones, shadows, and illumination as physically captured HDR imagery. In this work, we demonstrate progress in HDR literature to be tangential to sky-modelling as current works cannot support both photorealism and the 22 f-stops required for the Full Dynamic Range (FDR) of outdoor illumination. We achieve this by proposing AllSky, a flexible all-weather sky-model learned directly from physically captured HDRI which we leverage to study the input modalities, tonemapping, conditioning, and evaluation of sky-models. Per user-controlled positioning of the sun and cloud formations, AllSky expands on current functionality by allowing for intuitive user control over environment maps and achieves state-of-the-art sky-model performance. Through our proposed evaluation, we demonstrate existing DNN sky-models are not interchangeable with physically captured HDRI or parametric sky-models, with current limitations being prohibitive of scalability and accurate illumination in downstream applications

</details>


### [2] [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/abs/2512.15711)
*Divam Gupta,Anuj Pahuja,Nemanja Bartolovic,Tomas Simon,Forrest Iandola,Giljoo Nam*

Main category: cs.CV

TL;DR: GPiCA结合网格与3D高斯，实现移动设备高效渲染的逼真虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟形象在移动设备上渲染效率与真实感难以兼顾的问题。

Method: 采用混合表示方法，结合三角形网格和3D高斯，开发统一的微分渲染管线，训练神经网络解码面部表情代码为3D面部网格、RGBA纹理和3D高斯集合。

Result: GPiCA在保持高斯基虚拟形象的真实感的同时，达到了网格基虚拟形象的渲染性能。

Conclusion: GPiCA通过结合三角形网格和各向异性3D高斯，实现了在移动设备上高效渲染的逼真头部虚拟形象，同时保持了高真实感和渲染效率。

Abstract: We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.

</details>


### [3] [SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation](https://arxiv.org/abs/2512.14755)
*Paul Weinmann,Ferdinand Schenck,Martin Šiklar*

Main category: cs.CV

TL;DR: SkyCap数据集结合光学和SAR影像，通过标签转移实现SAR变化检测，光学基础模型在特定预处理下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决光学VHR影像因云层中断采集和SAR影像难以标注的问题，探索基础模型在VHR SAR变化检测中的应用。

Method: 通过档案匹配和共配准构建SkyCap数据集，利用光学到SAR的标签转移获取SAR变化检测标签，并对SARATR-X进行持续预训练，评估不同预处理下的模型性能。

Result: MTP(ViT-B+RVSA)光学基础模型在dB+Z-score预处理下取得最佳结果(F1$_c$=45.06)，优于直接预训练的SAR专用模型。

Conclusion: SkyCap数据集通过光学-SAR标签转移实现了无需SAR专家标注的SAR变化检测，展示了光学基础模型在SAR变化检测中的潜力，尽管预处理对齐对性能影响显著。

Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.

</details>


### [4] [SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.14757)
*Tomohito Kawabata,Xinyu Zhang,Ling Xiao*

Main category: cs.CV

TL;DR: 提出了 SocialNav-MoE，一种高效的小型视觉语言模型，用于社会合规导航，通过强化微调和语义相似性奖励提升性能，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在人类密集环境中，机器人导航的安全性和社会合规性同样重要，但现有研究多关注安全性。社会合规导航（考虑人类舒适度、社会规范和情境适当性）尚未充分探索。

Method: 研究了小型 VLM 的有效性，提出了 SocialNav-MoE，一种高效的混合专家视觉语言模型，结合强化微调（RFT）和语义相似性奖励（SSR）。还探讨了不同小型语言模型类型、路由策略和视觉编码器的效果。

Result: 在 SNEI 数据集上的实验表明，SocialNav-MoE 在导航准确性和效率之间表现优异，SSR 奖励函数效果显著。

Conclusion: SocialNav-MoE 在导航准确性和效率之间取得了良好平衡，提出的 SSR 奖励函数比硬级别和字符级别奖励更有效。

Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.

</details>


### [5] [The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics](https://arxiv.org/abs/2512.14758)
*Fan Bu,Rongfeng Li,Zijin Li,Ya Li,Linfeng Fan,Pei Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种混合专家系统管道，无需大量标注数据即可高精度识别中文简谱与歌词，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 中文简谱及其丰富的歌词资源在大规模光学音乐识别研究中未被充分探索，现有方法依赖大量标注数据，亟需一种无需大规模训练数据的解决方案。

Method: 采用自上而下的专家系统设计，结合传统计算机视觉技术（如短语相关性、骨架分析）和无监督深度学习模块，平衡了可解释性与准确性。

Result: 在《中国民歌选集》上测试，系统实现了高精度识别：旋律（音符级F1=0.951）和对齐歌词（字符级F1=0.931），成功数字化了5,000多首纯旋律歌曲和1,400多首带歌词歌曲。

Conclusion: 该论文提出的混合专家系统管道成功实现了高精度的简谱与歌词识别，填补了大规模中文简谱光学音乐识别的空白，为传统音乐的数字化提供了有效工具。

Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).

</details>


### [6] [AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion](https://arxiv.org/abs/2512.14760)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

TL;DR: AquaDiff是一种基于扩散的水下图像增强框架，通过色度先导和条件扩散过程有效校正色彩失真，同时在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 水下图像因波长依赖的光吸收和散射而严重退化，导致色彩失真、低对比度和细节丢失，阻碍了基于视觉的水下应用。

Method: AquaDiff结合了色度先导的颜色补偿策略和条件扩散过程，通过交叉注意力动态融合退化输入和噪声潜在状态。增强的去噪主干网络采用残差密集块和多分辨率注意力机制，捕捉全局色彩上下文和局部细节。

Result: AquaDiff在多样化的水下条件下实现了卓越的色彩校正和具有竞争力的整体图像质量。

Conclusion: AquaDiff在多个水下图像增强基准测试中表现出色，优于现有的传统、CNN、GAN和扩散方法，尤其在色彩校正和整体图像质量方面。

Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.

</details>


### [7] [Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification](https://arxiv.org/abs/2512.14770)
*Xixian Wu,Yang Ou,Pengchao Tian,Zian Yang,Jielei Zhang,Peiyi Li,Longwen Gao*

Main category: cs.CV

TL;DR: DAVR框架通过双路径评估和跨模型验证，有效减少视觉语言模型的幻觉问题，显著提升VQA任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在VQA中表现出潜力，但幻觉问题导致过度自信的错误答案，严重影响了答案的可靠性。

Method: DAVR采用双路径架构，一条路径通过融合VLM潜在特征与QA嵌入来评估响应可靠性，另一条路径利用外部参考模型进行事实交叉检查以减少幻觉。

Result: DAVR在ICCV-CLVL 2025的Reliable VQA Challenge中以$Φ_{100}$得分39.64和100-AUC 97.22的成绩排名第一。

Conclusion: DAVR框架通过自我反思和跨模型验证显著提升了视觉语言模型在VQA任务中的可靠性，并在ICCV-CLVL 2025的Reliable VQA Challenge中取得了领先成绩。

Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.

</details>


### [8] [HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering](https://arxiv.org/abs/2512.14870)
*Dan Ben-Ami,Gabriele Serussi,Kobi Cohen,Chaim Baskin*

Main category: cs.CV

TL;DR: HERBench是一个新的VideoQA基准，强制要求整合多个时间分离的证据，评估显示当前模型在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答（VideoQA）基准常依赖单一显著线索，未能充分测试需整合多个时间分离视觉证据的推理能力。

Method: 提出了HERBench基准，包含26K个五选一多选题，分为12个组合任务，并引入最小必需帧集（MRFS）来量化证据需求。

Result: 评估13种最先进的Video-LLMs显示普遍失败，准确率仅略高于随机猜测基线（31-42% vs. 20%），揭示了检索和融合两大瓶颈。

Conclusion: HERBench通过量化跨时间证据的需求，为提升视频理解的鲁棒性和组合性设定了明确目标。

Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.

</details>


### [9] [Isolated Sign Language Recognition with Segmentation and Pose Estimation](https://arxiv.org/abs/2512.14876)
*Daniel Perkins,Davis Hunter,Dhrumil Patel,Galen Flanagan*

Main category: cs.CV

TL;DR: 提出了一种高效的手语识别模型，结合姿态估计和Transformer架构，解决了数据稀缺和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在口语和书面语言翻译方面取得了进展，但美国手语（ASL）用户仍难以受益，因为手语依赖复杂的视觉线索。孤立手语识别（ISLR）因数据稀缺、用户差异大和计算成本高而受限。

Method: 模型整合了姿态估计管道提取手和面部关节坐标、分割模块隔离相关信息，以及ResNet-Transformer主干网络共同建模空间和时间依赖性。

Result: 提出的模型在减少计算需求的同时，保持了对不同手语使用者的鲁棒性。

Conclusion: 该论文提出了一种结合姿态估计、分割模块和ResNet-Transformer架构的模型，旨在降低计算需求并保持对手语使用者变化的鲁棒性。

Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.

</details>


### [10] [Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris](https://arxiv.org/abs/2512.14878)
*Wenshuo Li,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 论文提出了一种结合皮肤纹路文本描述符的跨模态动物重新识别方法，通过生成虚拟个体提升AI准确性，实现了文本到视觉的身份恢复，推动了生态监测中描述模态的统一。


<details>
  <summary>Details</summary>
Motivation: 生物学家长期以来结合视觉与文本现场笔记重新识别动物，但现有AI工具主要依赖图像。论文旨在通过引入皮肤纹路文本描述符（生态学中首次使用）扩展重新识别方法，以解决纯视觉方法的局限性。

Method: 论文提出了一种结合精确皮肤纹路文本描述符的方法，开发了文本-图像共同合成流程，生成包含数十个逼真视觉图像与皮肤纹路文本配对的‘虚拟个体’，以优化跨模态检索性能。

Result: 基于84,264个手动标记的细节和3,355张老虎图像，该方法在跨模态身份检索中展现了新能力，并通过虚拟个体生成显著提升了AI在跨模态检索中的准确性，缓解了数据稀缺问题。

Conclusion: 该论文得出结论，基于皮肤纹路的语言引导生物识别技术可以克服纯视觉方法的局限性，实现文本到视觉的身份恢复，并通过人类可验证的匹配支持，这在生态监测中实现了描述模态的语言驱动统一。

Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.

</details>


### [11] [Vibe Spaces for Creatively Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2512.14884)
*Huzheng Yang,Katherine Xu,Andrew Lu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: Vibe Blending 是一种新任务，通过 Vibe Space 层次图流形学习低维测地线，生成更具创造性和连贯性的视觉概念混合体。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以识别和遍历潜在空间中连接远距离概念的非线性路径，因此需要一种新方法来生成连贯且有意义的视觉概念混合体。

Method: 提出了 Vibe Space，一种层次图流形，学习 CLIP 等特征空间中的低维测地线，实现概念间的平滑过渡。

Result: Vibe Space 生成的混合体在人类评估中被认为比现有方法更具创造性和连贯性。

Conclusion: Vibe Space 能够生成比现有方法更具创造性和连贯性的视觉概念混合体。

Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.

</details>


### [12] [PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis](https://arxiv.org/abs/2512.14922)
*Joshua L. Ebbert,Dennis Della Corte*

Main category: cs.CV

TL;DR: PANDA-PLUS-Bench是一个专为评估前列腺癌Gleason分级基础模型鲁棒性而设计的基准数据集，结果显示模型间鲁棒性差异显著，组织特异性训练可能提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI基础模型在前列腺癌Gleason分级中可能通过学习标本特异性伪影而非可泛化的生物特征来获得高验证准确性，限制了其临床实用性。

Method: 通过专家注释的前列腺活检数据构建的PANDA-PLUS-Bench基准数据集，包含九张全切片图像，提取不同分辨率和增强条件下的组织块，评估七种基础模型分离生物信号与切片级混杂因素的能力。

Result: 不同模型在鲁棒性上表现出显著差异：Virchow2切片级编码最低（81.0%），但跨切片准确性第二低（47.2%）；HistoEncoder（前列腺组织特异性训练）表现最佳，跨切片准确性最高（59.7%），切片级编码最强（90.3%）。所有模型均存在切片内与跨切片准确性差距（19.9-26.9个百分点）。

Conclusion: PANDA-PLUS-Bench填补了基础模型评估的关键空白，为Gleason分级的临床重要背景下提供了一个专门构建的鲁棒性评估资源。

Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.

</details>


### [13] [Improving Pre-trained Segmentation Models using Post-Processing](https://arxiv.org/abs/2512.14937)
*Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,Nishad Kulkarni,Krithika Iyer,Austin Tapp,Syed Muhammad Anwar,María J. Ledesma-Carbayo,Marius George Linguraru*

Main category: cs.CV

TL;DR: 提出自适应后处理技术优化胶质瘤分割，在BraTS 2025挑战中显著提升性能，倡导转向高效、可持续的临床后处理策略。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型提高了自动分割的准确性，但大规模预训练模型泛化能力差且常表现不佳，产生系统性错误，如假阳性、标签交换和切片不连续性。此外，GPU资源分配不均和大规模模型训练的环境成本加剧了这些限制。

Method: 提出自适应后处理技术，用于优化针对多种肿瘤类型的大规模预训练模型生成的胶质瘤分割质量。

Result: 在多个BraTS 2025分割挑战任务中验证了该技术，其中撒哈拉以南非洲挑战的排名指标提高了14.9%，成人胶质瘤挑战提高了0.9%。

Conclusion: 本研究提倡将脑肿瘤分割研究重点从复杂模型架构转向高效、临床对齐的后处理策略，这些策略更精确、计算公平且可持续。

Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.

</details>


### [14] [TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation](https://arxiv.org/abs/2512.14938)
*Zhenzhi Wang,Jian Wang,Ke Ma,Dahua Lin,Bing Zhou*

Main category: cs.CV

TL;DR: TalkVerse是一个大规模开放的音频-视频同步语料库，支持公平、可重复的比较，并提供了一个高效、低成本的5B DiT基线模型，用于音频驱动的人类视频生成。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前最先进系统依赖封闭数据或计算密集型模型的问题，TalkVerse提供了一个大规模、开放的音频-视频同步语料库，以实现公平、可重复的方法比较。

Method: 利用视频VAE和高下采样率，结合滑动窗口机制与运动帧上下文，构建了一个可重复的5B DiT基线模型，并整合了MLLM导演以增强长视频的故事叙述。

Result: 模型在低推理成本下实现了分钟级生成，具有与14B Wan-S2V模型相当的唇同步和视觉质量，并支持通过受控潜在噪声注入进行零样本视频配音。

Conclusion: TalkVerse通过开源数据集、训练配方和5B检查点，降低了音频驱动人类视频生成研究的门槛，促进了公平、可重复的比较。

Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/

</details>


### [15] [Puzzle Curriculum GRPO for Vision-Centric Reasoning](https://arxiv.org/abs/2512.14944)
*Ahmadreza Jeddi,Hakki Can Karaimer,Hue Nguyen,Zhongling Wang,Ke Zhao,Javad Rajabi,Ran Zhang,Raghav Goyal,Babak Taati,Radek Grzeszczuk*

Main category: cs.CV

TL;DR: PC-GRPO是一种无需监督的RL方法，通过自监督谜题和动态课程提升VLMs的推理能力，解决了标注依赖和奖励稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有RL方法（如GRPO）依赖昂贵且嘈杂的人工标注、奖励稀疏及推理与答案逻辑不一致的问题。

Method: PC-GRPO采用三种自监督谜题环境（PatchFit、Rotation和Jigsaw）替代人工标注，并引入难度感知课程动态加权样本，同时监控推理-答案一致性（RAC）。

Result: PC-GRPO在多样化基准测试和Qwen-7B/Qwen-3B模型上，显著提升了推理质量、训练稳定性和下游任务准确性。

Conclusion: PC-GRPO通过自监督谜题环境和难度感知课程，显著提升了VLMs的推理质量、训练稳定性和下游任务准确性，为可扩展、可验证和可解释的RL后训练提供了实用路径。

Abstract: Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.

</details>


### [16] [Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities](https://arxiv.org/abs/2512.14961)
*Aref Farhadipour,Teodora Vukovic,Volker Dellwo,Petr Motlicek,Srikanth Madikeri*

Main category: cs.CV

TL;DR: 提出一种三模态人识别框架，通过多任务学习和动态融合策略，在模态缺失时仍保持高准确率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中因模态缺失或质量下降导致的人识别系统性能下降问题。

Method: 采用多任务学习独立处理各模态，结合交叉注意力和门控融合机制促进模态间交互，并使用置信度加权融合策略动态适应数据缺失。

Result: 在CANDOR数据集上达到99.18%的Top-1准确率，在VoxCeleb1数据集的双模态模式下达到99.92%准确率，且在模态缺失时仍保持高性能。

Conclusion: 提出的三模态人识别框架在模态缺失或质量低的情况下仍保持高准确率，适用于现实世界应用。

Abstract: Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.

</details>


### [17] [Where is the Watermark? Interpretable Watermark Detection at the Block Level](https://arxiv.org/abs/2512.14994)
*Maria Bulychev,Neil G. Marchant,Benjamin I. P. Rubinstein*

Main category: cs.CV

TL;DR: 本文提出了一种结合局部嵌入和区域级可解释性的后处理图像水印方法，通过离散小波变换和统计块策略生成检测图，在保持高不可感知性和鲁棒性的同时提供更透明的检测结果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步使得高度逼真的数字内容创作成为可能，引发了关于真实性、所有权和滥用的担忧。现有图像水印方案多为黑盒操作，缺乏透明度，影响用户信任。

Method: 该方法在离散小波变换域中使用统计块策略嵌入水印信号，生成检测图以揭示图像中可能被水印或篡改的区域。

Result: 该方法在常见图像变换下表现出强鲁棒性（例如对图像裁剪至一半仍保持鲁棒），同时对语义操作保持敏感性，且水印高度不可感知。

Conclusion: 本文提出的后处理图像水印方法结合了局部嵌入和区域级可解释性，在保持高不可感知性的同时，提供了更可解释的检测结果，并在常见图像变换下表现出强鲁棒性。

Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.

</details>


### [18] [Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle](https://arxiv.org/abs/2512.14998)
*Sibi Parivendan,Kashfia Sailunaz,Suresh Neethirajan*

Main category: cs.CV

TL;DR: 提出了一种基于姿态的框架，通过建模解剖关键点的时空几何特征来区分社会互动行为，显著优于传统接近度方法，适用于实时监测。


<details>
  <summary>Details</summary>
Motivation: 精准畜牧业需要客观评估社会行为以支持群体福利监测，但现有方法大多使用静态接近阈值推断互动，无法在复杂环境中区分亲和与对抗行为。

Method: 该框架整合了YOLOv11进行目标检测、监督个体识别、ByteTrack进行多目标跟踪、ZebraPose进行27点解剖关键点估计，以及支持向量机分类器对姿态衍生的距离动态进行训练。

Result: 在商业奶牛场的标注互动片段上，仅使用姿态信息的分类器在区分亲和与对抗行为时达到了77.51%的准确率，相比仅基于接近度的基线方法有显著提升。

Conclusion: 该研究提出了一个基于姿态的计算框架，用于区分社会互动中的亲和与对抗行为，为构建互动感知的社会网络提供了概念验证，并在商用硬件上实现了接近实时的性能。

Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.

</details>


### [19] [Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation](https://arxiv.org/abs/2512.15006)
*Huaying Zhang,Atsushi Hashimoto,Tosho Hirasawa*

Main category: cs.CV

TL;DR: 论文提出了一种评估VQG模型问题生成质量的新协议，并构建了EgoExoAsk数据集。实验证明该协议能有效区分模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQG模型的评估通常关注问题回答能力，而非生成问题的质量。本文旨在填补这一空白，专注于评估问题在从专家中引出新知识方面的质量。

Method: 论文提出了一种基于问题-答案检索的协议，用于评估VQG模型生成的问题质量。通过构建EgoExoAsk数据集（包含27,666个QA对）来训练检索器，并在验证集上进行基准测试。

Result: 实验结果显示，该评估协议能够合理区分不同模型的性能，尤其是在模型访问更丰富上下文时表现更好，验证了协议的有效性。

Conclusion: 该论文提出了一种评估视频问题生成（VQG）模型的新协议，通过模拟与专家的问答交流来评估问题生成的质量，并构建了EgoExoAsk数据集以支持这一评估。实验结果表明，该协议能够有效区分不同模型的性能。

Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .

</details>


### [20] [Model Agnostic Preference Optimization for Medical Image Segmentation](https://arxiv.org/abs/2512.15009)
*Yunseong Nam,Jiwon Jang,Dongkyu Won,Sang Hyun Park,Soopil Kim*

Main category: cs.CV

TL;DR: MAPO是一种模型无关的偏好优化框架，通过Dropout驱动的随机假设提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学图像分割方法在偏好优化中模型特定性和预测样本多样性不足的问题。

Method: 利用Dropout驱动的随机分割假设构建偏好一致的梯度，无需直接依赖真实标签监督。

Result: 在多类医学数据集上的评估表明，MAPO相比传统监督训练能显著提升分割性能。

Conclusion: MAPO作为一种模型无关的偏好优化框架，在医学图像分割中表现出色，能够提升边界一致性、减少过拟合并优化训练稳定性。

Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.

</details>


### [21] [Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving](https://arxiv.org/abs/2512.15181)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文首次深入分析了用于对象检测系统安全评估的关键性指标，提出了双向评分和多指标聚合策略，显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的首要目标是确保安全，而现有的性能评估指标缺乏针对安全性的专门考量，因此需要引入安全特定的指标来可靠评估对象检测系统。

Method: 通过全面回顾现有文献，识别并评估了一系列适用的关键性指标，并利用DeepAccident数据集进行了实证验证。

Result: 提出的方法在关键性分类准确率上实现了高达100%的提升。

Conclusion: 本文通过深入分析关键性指标，提出了双向关键性评分和多指标聚合策略，显著提高了对象检测系统的安全评估准确性，为自动驾驶的安全性评估提供了重要进展。

Abstract: Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.

</details>


### [22] [MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance](https://arxiv.org/abs/2512.15048)
*Kaizhe Zhang,Shinan Chen,Qian Zhao,Weizhan Zhang,Caixia Yan,Yudeng Xin*

Main category: cs.CV

TL;DR: MVGSR是一种多视图一致的3D高斯溅射超分辨率框架，通过相机姿态辅助视图选择和极线约束注意力机制，解决了现有方法缺乏跨视图一致性的问题，在多个基准测试中表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS SR方法（如单图像SR网络）缺乏跨视图一致性，无法融合多视图的互补信息；视频SR方法虽尝试解决，但需严格顺序帧，限制了在非结构化多视图数据集的应用。因此，需要一种能整合多视图信息、提升3DGS渲染细节和一致性的方法。

Method: MVGSR框架包括两个核心创新：1）基于相机姿态的辅助视图选择方法，适用于任意组织的多视图数据集；2）极线约束的多视图注意力机制，作为多视图SR网络的核心，选择性聚合辅助视图的一致信息。

Result: 实验表明，MVGSR在物体中心和场景级3DGS SR基准测试中均达到最先进性能。

Conclusion: MVGSR框架通过引入基于相机姿态的辅助视图选择方法和极线约束的多视图注意力机制，显著提升了3D高斯溅射超分辨率（3DGS SR）的性能，实现了高频细节和增强一致性的多视图信息整合。

Abstract: Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.

</details>


### [23] [Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry](https://arxiv.org/abs/2512.15423)
*Hoang Nguyen,Xiaohao Xu,Xiaonan Huang*

Main category: cs.CV

TL;DR: 本文提出首个端到端框架，通过3D-Mirage基准测试和Grounded Self-Distillation策略，量化并缓解单目深度模型中的3D幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 单目深度基础模型通过学习大规模语义先验实现了显著的泛化能力，但也存在一个关键漏洞：它们会从几何平面但感知模糊的输入中幻觉出虚假的3D结构（即3D Mirage现象）。本文旨在量化并解决这一未被充分认识的安全风险。

Method: 提出了3D-Mirage基准测试、基于拉普拉斯算子的评估框架（包括DCS和CCS两个指标）以及Grounded Self-Distillation策略。

Result: 提出了首个针对真实世界幻觉（如街头艺术）的基准测试3D-Mirage，并开发了量化框架和参数高效的Grounded Self-Distillation策略，有效缓解了3D Mirage现象。

Conclusion: 本文提出了首个端到端框架来探测、量化和缓解单目深度基础模型中的3D Mirage现象，并提供了诊断和减轻这一现象的基本工具，推动了MDE评估从像素级精度向结构和上下文鲁棒性的必要转变。

Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.

</details>


### [24] [Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement](https://arxiv.org/abs/2512.15055)
*Yifei Bian,Banglei Guan,Zibin Liu,Ang Su,Shiyao Zhu,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种利用事件相机和LED标记测量高频平面变形的方法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模结构因复杂载荷导致高频变形，但恶劣光照条件和高设备成本限制了传统高速相机的测量方法。

Method: 利用事件相机和LED标记，通过过滤观察噪声、区分运动引起的事件和LED闪烁事件，提取高速移动的LED标记，最终测量高频平面变形。

Result: 实验结果表明，该方法在测量高频平面变形方面具有准确性。

Conclusion: 该方法通过事件相机和LED标记成功测量了高频平面变形，实验验证了其准确性。

Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.

</details>


### [25] [Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank](https://arxiv.org/abs/2512.15066)
*Chenxiao Zhang,Runshi Zhang,Junchen Wang*

Main category: cs.CV

TL;DR: 提出一种记忆库小波网络，有效解决超声视频分割中的边界错误和小型对象丢失问题，实验证明其在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声视频的低对比度和噪声背景导致器官边界分割错误，小型对象丢失和长视频对象跟踪是主要挑战。

Method: 提出了一种基于记忆库的小波滤波与融合网络，采用编码器-解码器结构，结合记忆小波卷积、级联小波压缩和长短期记忆库机制，有效提取细粒度空间特征并整合高频信息。

Result: 在甲状腺结节、甲状腺和心脏数据集上的实验表明，该方法显著优于现有技术，尤其在小型甲状腺结节分割上表现突出。

Conclusion: 该方法在四个超声视频数据集上显著提升了分割指标，尤其在小型甲状腺结节分割上表现优异，验证了其在长视频中小型超声对象分割的有效性。

Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.

</details>


### [26] [PMMD: A pose-guided multi-view multi-modal diffusion for person generation](https://arxiv.org/abs/2512.15069)
*Ziyu Shang,Haoran Liu,Rongchao Zhang,Zhiqian Wei,Tongtong Feng*

Main category: cs.CV

TL;DR: PMMD是一种扩散框架，通过多模态编码和模块创新，解决了人像生成中的遮挡和风格漂移问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、服装风格漂移和姿态对齐方面的不足，提升虚拟试穿、图像编辑和数字人创建中的人像生成质量。

Method: 提出Pose-guided Multi-view Multimodal Diffusion (PMMD)框架，结合多视角参考、姿态图和文本提示，通过多模态编码器联合建模视觉、姿态和语义特征，并设计了ResCVA模块和跨模态融合模块。

Result: 在DeepFashion MultiModal数据集上，PMMD在一致性、细节保留和可控性方面优于代表性基线方法。

Conclusion: PMMD框架通过多模态编码器和创新的模块设计，显著提升了人像生成的一致性和细节保留能力，在DeepFashion MultiModal数据集上表现优于现有方法。

Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.

</details>


### [27] [Uni-Parser Technical Report](https://arxiv.org/abs/2512.15098)
*Xi Fang,Haoyi Tao,Shuwen Yang,Suyang Zhong,Haocheng Lu,Han Lyu,Chaozheng Huang,Xinyu Li,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: Uni-Parser是一个高效的跨模态文档解析引擎，适用于科学文献和专利，支持大规模云部署和多样下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统管道式文档解析方法在跨模态对齐和扩展性方面的不足，满足科学文献和专利解析的高效需求。

Method: 采用模块化、松散耦合的多专家架构，保持跨模态的细粒度对齐，支持自适应GPU负载均衡、分布式推理和动态模块编排。

Result: 在8 x NVIDIA RTX 4090D GPU上实现每秒20页PDF的处理速度，支持大规模应用。

Conclusion: Uni-Parser是一个针对科学文献和专利的工业级文档解析引擎，具有高吞吐量、高准确性和成本效益，适用于大规模云部署和广泛的下游应用。

Abstract: This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.

</details>


### [28] [Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets](https://arxiv.org/abs/2512.15110)
*Jialong Zuo,Haoyou Deng,Hanyu Zhou,Jiaxin Zhu,Yicheng Zhang,Yiwei Zhang,Yongxin Yan,Kaixing Huang,Weisen Chen,Yongtai Deng,Rui Jin,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: Nano Banana Pro在低级别视觉任务中展现了零样本潜力，主观质量优秀但定量指标不足，与专业模型仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探讨Nano Banana Pro是否能够作为低级别视觉任务的通用解决方案，填补商业产品在这一领域的未充分探索的潜力。

Method: 通过简单的文本提示（无需微调），在14个不同的低级别任务和40个数据集中进行了全面的零样本评估。

Result: Nano Banana Pro在主观视觉质量上表现出色，常能生成超越专业模型的高频细节，但在传统的基于参考的定量指标上表现不佳。

Conclusion: Nano Banana Pro被视为低级别视觉任务中的零样本竞争者，但达到领域专家的高保真度仍是一个重大挑战。

Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.

</details>


### [29] [3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding](https://arxiv.org/abs/2512.15126)
*Yupeng Zhu,Xiongzhen Zhang,Ye Chen,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出一种轻量级3D动画框架，通过解耦几何控制与外观合成，实现高效生成且优于现有视频方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D动画生产流程劳动密集、依赖专业知识且计算成本高，现有AIGC方法要么继承完整3D流程的高成本，要么牺牲3D可控性和交互性。本文旨在解决渲染质量与3D控制之间的权衡问题。

Method: 提出了一种轻量级的3D动画框架，将几何控制与外观合成解耦，利用2D-3D对齐的代理表示，将高保真外观和视角合成委托给学习到的图像空间生成先验。

Result: 实验表明，该方法在低功耗平台上实现了高效动画生成，并在多个方面优于基于视频的3D动画生成方法。

Conclusion: 该方法在低功耗平台上实现了高效的动画生成，并在身份保持、几何与纹理一致性以及精确交互控制方面优于基于视频的3D动画生成方法。

Abstract: 3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.
  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.

</details>


### [30] [Borrowing from anything: A generalizable framework for reference-guided instance editing](https://arxiv.org/abs/2512.15138)
*Shengxiao Zhou,Chenghua Li,Jianhao Huang,Qinghao Hu,Yifan Zhang*

Main category: cs.CV

TL;DR: GENIE是一种通用实例编辑框架，通过空间对齐、自适应残差缩放和渐进注意力融合，解决了语义纠缠问题，实现了高效的实例编辑。


<details>
  <summary>Details</summary>
Motivation: 参考引导的实例编辑受限于语义纠缠问题，即参考的内在外观与外在属性相互交织。如何解耦并正确应用参考信息是核心挑战。

Method: GENIE框架包含三个核心模块：空间对齐模块（SAM）用于纠正空间错位，自适应残差缩放模块（ARSM）学习放大显著内在线索并抑制外在属性，渐进注意力融合（PAF）机制学习如何将外观渲染到目标上并保持其结构。

Result: 在AnyInsertion数据集上的大量实验表明，GENIE在保真度和鲁棒性方面达到了最先进的水平。

Conclusion: GENIE通过空间对齐模块、自适应残差缩放模块和渐进注意力融合机制，成功实现了实例编辑中的语义解耦，并在AnyInsertion数据集上展现了卓越的保真度和鲁棒性，为基于解耦的实例编辑设立了新标准。

Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.

</details>


### [31] [Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning](https://arxiv.org/abs/2512.15153)
*Mengshi Qi,Yeteng Wu,Xianlin Zhang,Huadong Ma*

Main category: cs.CV

TL;DR: 论文提出新任务AFA和数据集CoT-AFA，并开发Explainable Fitness Assessor框架，通过多级注释和链式思维解释提升动作标准化评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解方法主要关注动作的识别和定位，无法满足评估动作标准化程度的需求，且现有数据集缺乏动作标准化程度的标签和详细反馈。

Method: 论文提出了一个并行处理流和动态门控机制的框架，融合视觉和语义信息以增强分析能力。

Result: 实验结果显示，该方法在解释生成（如CIDEr提升16.0%）、动作分类（准确率提升2.7%）和质量评估（准确率提升2.1%）方面均有显著改进。

Conclusion: 该论文提出了一个名为Explainable Fitness Assessor的框架，能够评估人类动作的标准化程度并提供解释和解决方案。实验结果表明，该方法在解释生成、动作分类和质量评估方面均有显著提升。

Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.

</details>


### [32] [EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence](https://arxiv.org/abs/2512.15160)
*Jiaxu Wan,Xu Wang,Mengwei Xie,Hang Zhang,Mu Xu,Yang Han,Hong Zhang,Ding Yuan,Yifan Yang*

Main category: cs.CV

TL;DR: EagleVision是一个双阶段框架，通过宏观感知和微观验证解决空间CoT的挑战，在VSI-Bench上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在空间一致性和视角多样性上存在不足，且证据链无法追溯。EagleVision旨在解决空间CoT中的三个关键挑战：全局空间感知、3D假设与视频帧的关联验证，以及空间奖励设计。

Method: EagleVision采用双阶段框架，包括宏观感知阶段（使用SPF-DPP选择关键帧）和微观验证阶段（通过BEV平面上的姿态查询实现空间CoT）。

Result: EagleVision在VSI-Bench上表现优异，实现了最先进的性能。

Conclusion: EagleVision在VSI-Bench上实现了开源视觉语言模型中最先进的性能，展示了强大且可泛化的空间理解能力。

Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.

</details>


### [33] [Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis](https://arxiv.org/abs/2512.15171)
*Kaixing Long,Danyi Weng,Yun Mi,Zhentai Zhang,Yanmeng Lu,Jian Geng,Zhitao Zhou,Liming Zhong,Qianjin Feng,Wei Yang,Lei Cao*

Main category: cs.CV

TL;DR: CMUS-Net通过跨模态超尺度学习网络，解决了肾活检图像中纳米与微米级特征融合问题，显著提升了多肾小球疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态和多尺度模型难以有效融合纳米级TEM图像与微米级OM/IM图像的特征，限制了多肾小球疾病分类的准确性。

Method: 提出CMUS-Net网络，结合稀疏多实例学习模块和跨模态尺度注意力模块，通过多损失函数优化模态间特征权重。

Result: 在内部数据集上，CMUS-Net达到ACC 95.37±2.41%、AUC 99.05±0.53%、F1-score 95.32±2.41%，优于其他多模态/多尺度方法。

Conclusion: CMUS-Net通过跨模态超尺度学习网络有效解决了肾活检图像中纳米级与微米级特征融合的难题，显著提升了多肾小球疾病的分类准确率，并在MN分期中展示了良好的泛化能力。

Abstract: Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.

</details>


### [34] [Robust and Calibrated Detection of Authentic Multimedia Content](https://arxiv.org/abs/2512.15182)
*Sarim Hashmi,Abdelrahman Elsayed,Mohammed Talha Alam,Samuele Poppi,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出一种重合成框架，用于验证样本真实性并对抗高效攻击者，在高精度、低召回率设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成模型合成的深度伪造内容（deepfakes）已大规模滥用，现有检测方法不可靠，存在高假阳性率和缺乏对抗鲁棒性的问题。

Method: 提出了一种重合成框架，通过校准的重合成方法来验证样本的真实性，并确保低假阳性率。

Result: 校准的重合成方法在高精度、低召回率设置下表现最优，且在相同计算预算下对抗高效攻击者时具有鲁棒性。

Conclusion: 本文提出的重合成框架在验证真实样本和对抗高效攻击者方面表现出色，支持多种模态并利用先进的逆变换技术。

Abstract: Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.

</details>


### [35] [ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment](https://arxiv.org/abs/2512.15186)
*Jianan Wang,Yang Hong,Hesong Li,Tao Wang,Songrong Liu,Ying Fu*

Main category: cs.CV

TL;DR: ERIENet 是一种高效的RAW图像增强网络，通过并行多尺度处理和绿色通道引导，实现了高效率和实时速度。


<details>
  <summary>Details</summary>
Motivation: 现有RAW图像增强方法通常顺序处理多尺度信息，难以实现轻量模型和高处理速度，且忽视绿色通道的优势。

Method: 提出了一种高效的多尺度全并行架构和通道感知残差密集块，以及绿色通道引导分支，以提取特征并指导图像重建。

Result: 在常用低光图像增强数据集上，ERIENet 在效率和效果上均优于现有方法，处理4K分辨率图像速度超过146 FPS。

Conclusion: ERIENet 通过并行处理多尺度信息和利用绿色通道的丰富信息，显著提升了低光RAW图像增强的效果和效率，实现了实时处理速度。

Abstract: RAW images have shown superior performance than sRGB images in many image processing tasks, especially for low-light image enhancement. However, most existing methods for RAW-based low-light enhancement usually sequentially process multi-scale information, which makes it difficult to achieve lightweight models and high processing speeds. Besides, they usually ignore the green channel superiority of RAW images, and fail to achieve better reconstruction performance with good use of green channel information. In this work, we propose an efficient RAW Image Enhancement Network (ERIENet), which parallelly processes multi-scale information with efficient convolution modules, and takes advantage of rich information in green channels to guide the reconstruction of images. Firstly, we introduce an efficient multi-scale fully-parallel architecture with a novel channel-aware residual dense block to extract feature maps, which reduces computational costs and achieves real-time processing speed. Secondly, we introduce a green channel guidance branch to exploit the rich information within the green channels of the input RAW image. It increases the quality of reconstruction results with few parameters and computations. Experiments on commonly used low-light image enhancement datasets show that ERIENet outperforms state-of-the-art methods in enhancing low-light RAW images with higher effiency. It also achieves an optimal speed of over 146 frame-per-second (FPS) for 4K-resolution images on a single NVIDIA GeForce RTX 3090 with 24G memory.

</details>


### [36] [TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion](https://arxiv.org/abs/2512.15211)
*Yufeng Xie*

Main category: cs.CV

TL;DR: 针对传统指标在低光环境中的噪声误判问题，提出TBC指标，通过目标-背景对比度提升融合图像质量，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统无参考指标（如EN和AG）在复杂低光环境中易将高频传感器噪声误认为有效细节，形成“噪声陷阱”，误导融合算法。

Method: 受韦伯定律启发，设计了目标-背景对比度（TBC）指标，专注于显著目标的相对对比度而非全局统计。

Result: 在DroneVehicle数据集上的实验表明，TBC与人眼感知更一致，为低空场景提供了可靠标准。

Conclusion: 提出的TBC指标在低空无人机侦察任务中优于传统无参考指标，能更准确地反映目标可见性并减少背景噪声的干扰。

Abstract: Infrared and visible image fusion is a pivotal technology in low-altitude UAV reconnaissance missions, providing high-quality data support for downstream tasks such as target detection and tracking by integrating thermal saliency with background texture details.However, traditional no-reference metrics fail(Specifically,like Entropy (EN) and Average Gradient (AG)) in complex low-light environments. They often misinterpret high-frequency sensor noise as valid detail. This creates a "Noise Trap," paradoxically assigning higher scores to noisy images and misguiding fusion algorithms.To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Experiments on the DroneVehicle dataset demonstrate that TBC aligns better with human perception and provides a reliable standard for low-altitude scenarios.

</details>


### [37] [From Camera to World: A Plug-and-Play Module for Human Mesh Transformation](https://arxiv.org/abs/2512.15212)
*Changhai Ma,Ziyu Wu,Yunkang Zhang,Qijun Ying,Boyan Liu,Xiaohui Cai*

Main category: cs.CV

TL;DR: Mesh-Plug通过结合RGB和深度图估计相机旋转，优化人体网格从相机到世界坐标系的转换，显著提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在相机坐标系下假设相机旋转为零，导致转换到世界坐标系时产生显著误差，因此需要一种更准确的转换方法。

Method: 提出了一种以人为中心的方法，结合RGB图像和深度图来估计相机旋转参数，并设计了网格调整模块以优化根关节方向和身体姿态。

Result: 在SPEC-SYN和SPEC-MTP基准数据集上，Mesh-Plug优于现有最先进方法。

Conclusion: Mesh-Plug 是一个即插即用的模块，能够准确地将人体网格从相机坐标系转换到世界坐标系，显著提升了重建精度。

Abstract: Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.

</details>


### [38] [SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal](https://arxiv.org/abs/2512.15221)
*Xiyu Zhu,Wei Wang,Xin Yuan,Xiao Wang*

Main category: cs.CV

TL;DR: SLCFormer是一种新型的频域-局部上下文变换器框架，通过FFEM和DESM模块有效去除夜间镜头眩光，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常无法有效解决非均匀散射眩光问题，这严重降低了它们在复杂现实场景中的适用性。

Method: SLCFormer整合了两个关键模块：FFEM（频率傅里叶和激励模块）用于在频域中捕捉全局上下文表示以建模眩光特性，以及DESM（方向增强空间模块）用于空间域中的局部结构增强和方向特征以实现精确的眩光去除。此外，还引入了基于ZernikeVAE的散射眩光生成管道。

Result: 在Flare7K++数据集上的大量实验表明，该方法在定量指标和感知视觉质量上均优于现有方法，并能鲁棒地泛化到真实夜间场景。

Conclusion: SLCFormer在夜间镜头眩光去除方面表现出色，不仅在定量指标上优于现有方法，还在感知视觉质量上有所提升，并能鲁棒地泛化到具有复杂眩光伪影的真实夜间场景。

Abstract: Lens flare is a common nighttime artifact caused by strong light sources scattering within camera lenses, leading to hazy streaks, halos, and glare that degrade visual quality. However, existing methods usually fail to effectively address nonuniform scattered flares, which severely reduces their applicability to complex real-world scenarios with diverse lighting conditions. To address this issue, we propose SLCFormer, a novel spectral-local context transformer framework for effective nighttime lens flare removal. SLCFormer integrates two key modules: the Frequency Fourier and Excitation Module (FFEM), which captures efficient global contextual representations in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM) for local structural enhancement and directional features in the spatial domain for precise flare removal. Furthermore, we introduce a ZernikeVAE-based scatter flare generation pipeline to synthesize physically realistic scatter flares with spatially varying PSFs, bridging optical physics and data-driven training. Extensive experiments on the Flare7K++ dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in both quantitative metrics and perceptual visual quality, and generalizing robustly to real nighttime scenes with complex flare artifacts.

</details>


### [39] [Null-LoRA: Low-Rank Adaptation on Null Space](https://arxiv.org/abs/2512.15233)
*Yi Zhang,Yulei Kang,Haoxuan Chen,Jinxuan Li,ian-Fang Hu*

Main category: cs.CV

TL;DR: Null-LoRA通过零空间约束和冻结部分低秩矩阵，以更少参数在多项任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练模型存在非平凡零空间，现有全参数空间的低秩适配方法存在冗余，通过子空间微调可达到类似效果。

Method: 提出基于零空间的低秩适配方法（Null-LoRA），通过冻结部分低秩矩阵并约束增量更新在零空间内，减少冗余并提升有效秩。

Result: Null-LoRA在图像-文本检索和视觉问答任务中，以更少参数超越了现有方法。

Conclusion: Null-LoRA通过冻结部分低秩矩阵并在零空间内约束增量更新，显著提升了参数效率，并在图像-文本检索和视觉问答任务中超越了现有方法。

Abstract: Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.

</details>


### [40] [Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification](https://arxiv.org/abs/2512.15249)
*Yupeng Zhang,Adam G. Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TL;DR: CMAC-MMD框架通过标准化诊断确定性，减少医疗AI在边缘化患者亚组中的偏见，提升诊断公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统（尤其是多模态视觉语言模型）存在交叉性偏见，导致对边缘化患者亚组的诊断信心不足，进而增加误诊和漏诊率。当前公平性干预措施往往无法解决这些问题或牺牲整体诊断性能。

Method: 开发了Cross-Modal Alignment Consistency (CMAC-MMD)训练框架，无需在临床推理中使用敏感人口统计数据，即可均衡模型的决策置信度。

Result: 在皮肤病学队列中，该方法将整体交叉性漏诊差距（ΔTPR）从0.50降至0.26，同时将AUC从0.94提升至0.97。在青光眼筛查中，ΔTPR从0.41降至0.31，AUC提升至0.72（基线为0.71）。

Conclusion: 该研究提出的Cross-Modal Alignment Consistency (CMAC-MMD)框架，通过标准化诊断确定性，显著减少了医疗AI系统在边缘化患者亚组中的诊断偏差，同时提升了整体诊断性能。

Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.

</details>


### [41] [Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models](https://arxiv.org/abs/2512.15254)
*Kuinan Hou,Jing Mi,Marco Zorzi,Lamberto Ballan,Alberto Testolin*

Main category: cs.CV

TL;DR: VLM在开放集对象计数中表现接近或优于专用架构，尤其在生成中间表示时更准确，但在复杂场景中仍不可靠，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 传统计数方法依赖特定领域的架构和预定义对象类别的数据集，而大型多模态视觉语言模型（VLM）可能为开放集对象计数提供灵活替代方案。

Method: 本研究系统地比较了最先进的专用计数架构与VLM在两种流行计数数据集及一个新基准上的性能，该基准专门用于更精细控制测试图像的视觉属性。

Result: 大多数VLM能近似枚举视觉场景中的物品数量，性能匹配或超越专用架构；当提示生成待计数对象的中间表示（如位置和语言标签）时，枚举准确性显著提高。

Conclusion: 尽管大多数视觉语言模型（VLM）在枚举视觉场景中的物品数量时表现接近或优于专用计算机视觉架构，但在复杂场景中仍无法可靠计数，表明仍需进一步研究以实现AI系统在现实环境中的可靠计数。

Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.

</details>


### [42] [MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement](https://arxiv.org/abs/2512.15261)
*Yingying Wang,Xuanhua He,Chen Wu,Jialing Huang,Suiyun Zhang,Rui Liu,Xinghao Ding,Haoxuan Che*

Main category: cs.CV

TL;DR: MMMamba是一种基于Mamba架构的跨模态上下文融合框架，用于全色锐化和图像超分辨率，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在适应多样空间和光谱变化方面有限，而交叉注意力机制效率低下且可能削弱细粒度对应关系。

Method: 基于Mamba架构设计，采用多模态交错扫描机制，实现高效的跨模态信息交换。

Result: 实验证明MMMamba在保持线性计算复杂度的同时，具有强大的跨模态交互能力。

Conclusion: MMMamba框架在多个任务和基准测试中表现出色，优于现有最先进技术。

Abstract: Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.

</details>


### [43] [SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2512.15310)
*Wangyu Wu,Zhenhong Chen,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: 提出ZSWSSS任务和SynthSeg Agents框架，利用LLM驱动代理生成合成训练数据，实验显示其竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决现有弱监督语义分割方法依赖真实训练样本的问题，探索零样本弱监督语义分割（ZSWSSS）的新方向。

Method: 提出了SynthSeg Agents框架，包含Self Refine Prompt Agent和Image Generation Agent两个模块，通过迭代优化提示、生成合成图像，并利用CLIP评分和ViT分类器提升数据质量。

Result: 在PASCAL VOC 2012和COCO 2014上验证了SynthSeg Agents的竞争力，无需真实图像即可生成高质量训练数据。

Conclusion: SynthSeg Agents展示了LLM驱动代理在无需真实图像监督下生成高质量训练数据的潜力，为成本高效和可扩展的语义分割提供了新方向。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.

</details>


### [44] [KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation](https://arxiv.org/abs/2512.15311)
*Wenke E,Yixin Sun,Jiaxu Liu,Hubert P. H. Shum,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.CV

TL;DR: 提出首个针对单全景相机BEV分割的跨模态蒸馏框架，通过LiDAR和相机融合的教师网络提升轻量级学生网络性能，显著降低传感器成本，并在多个数据集上验证了其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决单全景相机BEV分割中的传感器复杂性和高成本问题，提出了一种跨模态蒸馏框架，旨在通过知识蒸馏提升轻量级学生网络的性能。

Method: 利用新颖的LiDAR图像表示（融合了距离、强度和环境通道）以及体素对齐的视图变换器，保持空间保真度同时实现高效BEV处理。通过高容量LiDAR和相机融合的教师网络提取丰富的空间和语义特征，用于跨模态知识蒸馏到仅依赖单张360度全景相机图像的轻量级学生网络。

Result: 在Dur360BEV数据集上，教师模型显著优于现有基于相机的BEV分割方法，IoU提升了25.6%。蒸馏后的学生网络实现了8.5%的IoU增益，并以31.2 FPS的推理速度达到最先进水平。在KITTI-360上的评估进一步验证了框架的泛化能力。

Conclusion: 该论文提出了一种跨模态蒸馏框架，显著降低了传感器复杂性和部署成本，为实际自动驾驶中的高效、低成本BEV分割提供了实用解决方案。

Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.

</details>


### [45] [Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment](https://arxiv.org/abs/2512.15315)
*Antony Jerald,Dattesh Shanbhag,Sudhanya Chatterjee*

Main category: cs.CV

TL;DR: AutoMAC-MRI 是一种可解释的运动伪影分级框架，通过对比学习和亲和力评分实现透明且准确的 MRI 质量控制。


<details>
  <summary>Details</summary>
Motivation: 运动伪影会降低 MRI 图像质量并增加患者召回率，现有的自动化质量评估方法大多局限于二元决策且缺乏可解释性。

Method: 该方法利用监督对比学习来学习运动严重程度的判别性表示，并在该特征空间中计算特定等级的亲和力分数，以量化图像与每个运动等级的接近程度。

Result: 在超过 5000 张专家注释的脑 MRI 切片上评估显示，亲和力分数与专家判断高度一致，支持其作为运动严重程度的可解释性度量。

Conclusion: AutoMAC-MRI 通过结合准确的运动伪影分级和每级亲和力评分，实现了内联 MRI 质量控制，有望减少不必要的重新扫描并提高工作流程效率。

Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.

</details>


### [46] [Vision-based module for accurately reading linear scales in a laboratory](https://arxiv.org/abs/2512.15327)
*Parvesh Saini,Soumyadipta Maiti,Beena Rai*

Main category: cs.CV

TL;DR: 该研究开发了一种模仿人类读取线性刻度的方法，通过校正方向、提取特征，实现了从注射器和量筒中准确读取测量值，结果与人工读取一致。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉模型在物体检测、图像分类等任务上表现优异，但能像人类一样从图像中获取精确定量测量的模型仍较少。为提升实验室环境中机器人的自主性，需使其具备读取仪器测量值等基础技能。

Method: 对随机方向的注射器进行方向校正，缩小感兴趣区域至线性刻度部分，提取主要标记、对应数字和水平指示器位置等特征，最终计算读数。

Result: 系统提取的特征成功计算出最终读数，与人工读取值对比显示高度准确性。

Conclusion: 该系统通过模仿人类读取线性刻度的方法，实现了从注射器和量筒中准确读取测量值，并与人工读取结果高度一致。

Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.

</details>


### [47] [Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2512.15319)
*Yuxin Jiang,Yunkang Cao,Weiming Shen*

Main category: cs.CV

TL;DR: PCSNet通过原型特征适应和上下文感知分割网络，解决了预训练特征与目标场景的领域差距问题，显著提升了小样本异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有FSAD方法依赖预训练特征，但忽略了预训练特征与目标场景之间的领域差距，导致性能受限。

Method: PCSNet包含原型特征适应（PFA）子网络和上下文感知分割（CAS）子网络。PFA提取原型特征以增强正常数据的特征紧凑性并与异常分离，同时设计了像素级差异分类损失以提高异常区分度。CAS子网络用于像素级异常定位，利用伪异常辅助训练。

Result: 在MVTec和MPDD数据集上，PCSNet在8-shot场景下分别达到94.9%和80.2%的图像级AUROC，实际汽车塑料零件检测中也表现优异。

Conclusion: PCSNet通过原型特征适应和上下文感知分割网络有效解决了预训练特征与目标FSAD场景之间的领域差距问题，显著提升了小样本异常检测的性能。

Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.

</details>


### [48] [Emotion Recognition in Signers](https://arxiv.org/abs/2512.15376)
*Kotaro Funakoshi,Yaoxiong Zhu*

Main category: cs.CV

TL;DR: 本文通过跨语言数据集eJSL和BOBSL，结合文本情感识别和时间片段选择，提升了手语情感识别效果，并建立了更强基线。


<details>
  <summary>Details</summary>
Motivation: 解决手语情感识别中语法与情感面部表情重叠的理论挑战，以及模型训练数据稀缺的实践挑战。

Method: 使用eJSL（日本手语）和BOBSL（英国手语）数据集，结合文本情感识别、时间片段选择和手部动作特征，进行情感识别模型的训练和评估。

Result: 实证表明文本情感识别缓解了数据稀缺问题，时间片段选择和手部动作显著提升了情感识别效果，建立了优于口语语言模型的基线。

Conclusion: 本文通过跨语言设置解决了手语情感识别中的理论和实践挑战，利用eJSL和BOBSL数据集，证明了文本情感识别、时间片段选择和手部动作对手语情感识别的积极影响，并建立了优于口语语言模型的基线。

Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.

</details>


### [49] [MECAD: A multi-expert architecture for continual anomaly detection](https://arxiv.org/abs/2512.15323)
*Malihe Dahmardeh,Francesco Setti*

Main category: cs.CV

TL;DR: MECAD是一种持续异常检测的多专家架构，通过动态专家分配和高效内存管理实现增量学习，实验显示其优于单专家方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决单专家方法在持续异常检测中知识退化的问题，并适应工业环境中产品类型的不断变化。

Method: 采用多专家架构，基于特征相似性动态分配专家到对象类别，并利用优化的核心集选择和专用重放缓冲机制实现增量学习，无需完整模型重新训练。

Result: 在MVTec AD数据集上，5专家配置的平均AUROC达到0.8259，显著减少了知识退化。

Conclusion: MECAD框架在计算效率、专业知识保留和适应性之间取得了平衡，非常适合产品类型不断变化的工业环境。

Abstract: In this paper we propose MECAD, a novel approach for continual anomaly detection using a multi-expert architecture. Our system dynamically assigns experts to object classes based on feature similarity and employs efficient memory management to preserve the knowledge of previously seen classes. By leveraging an optimized coreset selection and a specialized replay buffer mechanism, we enable incremental learning without requiring full model retraining. Our experimental evaluation on the MVTec AD dataset demonstrates that the optimal 5-expert configuration achieves an average AUROC of 0.8259 across 15 diverse object categories while significantly reducing knowledge degradation compared to single-expert approaches. This framework balances computational efficiency, specialized knowledge retention, and adaptability, making it well-suited for industrial environments with evolving product types.

</details>


### [50] [SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering](https://arxiv.org/abs/2512.15396)
*Liang Peng,Yixuan Ye,Cheng Liu,Hangjun Che,Fei Wang,Zhiwen Yu,Si Wu,Hau-San Wong*

Main category: cs.CV

TL;DR: SMART模型通过语义匹配对比学习解决部分视图对齐聚类问题，有效利用未对齐数据并缓解分布偏移，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中严格对齐的多视图数据难以获取，且现有PVC方法未能充分利用未对齐数据中的共享语义，同时多视图数据的异质性导致表示分布偏移，影响跨视图潜在特征的对应关系建立。

Method: 提出了一种语义匹配对比学习模型（SMART），旨在通过语义匹配对比学习充分利用对齐和未对齐数据中的语义关系。

Result: 在八个基准数据集上的广泛实验表明，SMART方法在PVC问题上 consistently 优于现有方法。

Conclusion: SMART模型通过缓解跨视图分布偏移的影响，成功提升了部分视图对齐聚类（PVC）的性能，并在多个基准数据集上优于现有方法。

Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.

</details>


### [51] [A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection](https://arxiv.org/abs/2512.15326)
*Yuxin Jiang,Yunkang Can,Weiming Shen*

Main category: cs.CV

TL;DR: MRKD通过ILM和FLM策略改进了知识蒸馏，显著提升了图像异常检测的性能并减少了过度泛化。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在图像异常检测中存在过度泛化的问题，主要原因是输入信号和监督信号的相似性。

Method: 提出了一种名为MRKD的新技术，结合了图像级掩码（ILM）和特征级掩码（FLM），将图像重建任务转化为图像修复任务。

Result: 在MVTec数据集上，MRKD实现了图像级98.9% AU-ROC、像素级98.4% AU-ROC和95.3% AU-PRO的优异性能。

Conclusion: MRKD通过引入ILM和FLM策略，有效解决了知识蒸馏在图像异常检测中的过度泛化问题，并在MVTec数据集上取得了优异的性能表现。

Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.

</details>


### [52] [Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics](https://arxiv.org/abs/2512.15340)
*Junjie Chen,Fei Wang,Zhihao Huang,Qing Zhou,Kun Li,Dan Guo,Linfeng Zhang,Xun Yang*

Main category: cs.CV

TL;DR: TIMAR是一种因果框架，通过多模态融合和轻量级扩散头生成连贯且富有表现力的3D对话头部动态，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将说话和听视为独立过程或依赖非因果的全序列建模，导致对话连贯性不足。TIMAR旨在解决这一问题。

Method: 提出了TIMAR（Turn-level Interleaved Masked AutoRegression）框架，采用因果注意力机制和多模态融合，结合轻量级扩散头预测连续的3D头部动态。

Result: 在DualTalk基准测试中，TIMAR将Fréchet距离和MSE降低了15-30%，并在分布外数据上取得类似提升。

Conclusion: TIMAR框架通过因果建模和多模态信息融合，显著提升了3D对话头部生成的连贯性和表现力，并在实验中验证了其优越性能。

Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.

</details>


### [53] [IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning](https://arxiv.org/abs/2512.15635)
*Yuanhang Li,Yiren Song,Junzhe Bai,Xinran Liang,Hu Yang,Libiao Jin,Qi Mao*

Main category: cs.CV

TL;DR: IC-Effect 是一种基于DiT的指令引导框架，通过两阶段训练和时空稀疏标记化，实现少量数据下的高质量视频特效编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型无法满足特效编辑中对背景严格保留和特效自然注入的需求，尤其是在少量配对数据下。

Method: 采用基于DiT的框架，结合两阶段训练策略（通用编辑适应和Effect-LoRA特效学习），并引入时空稀疏标记化以提高效率。

Result: 实验表明，IC-Effect 能够实现高质量、可控且时间一致的视频特效编辑。

Conclusion: IC-Effect 提供了一种高质量、可控且时间一致的视频特效编辑方法，为视频创作开辟了新可能性。

Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.

</details>


### [54] [Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models](https://arxiv.org/abs/2512.15347)
*Shiran Ge,Chenyi Huang,Yuang Ai,Qihang Fan,Huaibo Huang,Ran He*

Main category: cs.CV

TL;DR: Pro-GRPO通过动态轨迹修剪和'扩展-修剪'策略，优化GRPO的计算效率，适用于多种生成模型。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO技术中大规模组与计算成本之间的冲突，提升生成模型对齐效率。

Method: 提出Pro-GRPO框架，采用'扩展-修剪'策略，结合多步OVF对潜在特征进行筛选。

Result: 实验表明Pro-GRPO能有效减少计算开销，同时保持或提升性能。

Conclusion: Pro-GRPO通过动态框架整合潜在特征轨迹修剪，显著减少了计算开销，并在扩散模型和流模型中验证了其通用性和有效性。

Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.

</details>


### [55] [VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?](https://arxiv.org/abs/2512.15649)
*Hongbo Zhao,Meng Wang,Fei Zhu,Wenzhuo Liu,Bolin Ni,Fanhu Zeng,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 研究评估了视觉文本压缩（VTC）对视觉语言模型（VLMs）长上下文能力的影响，发现多数模型在压缩信息下表现不佳，为未来VLM设计提供了参考。


<details>
  <summary>Details</summary>
Motivation: 视觉文本压缩（VTC）虽能实现3x-20x的令牌压缩比，但其高信息密度对视觉语言模型（VLMs）核心长上下文能力的影响尚未充分研究。

Method: 我们引入了首个VTC基准测试，并系统评估了VLMs在三种长上下文理解场景中的表现：VTC-Retrieval、VTC-Reasoning和VTC-Memory。此外，还建立了VTCBench-Wild以模拟多样化的输入场景。

Result: 尽管大多数VLMs能很好地解码文本信息（如OCR），但在VTC压缩信息下表现出令人惊讶的差的长上下文理解能力，无法捕捉上下文中的长关联或依赖关系。

Conclusion: 本研究为视觉文本压缩（VTC）提供了深入理解，并为设计更高效、可扩展的视觉语言模型（VLMs）奠定了基础。

Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.

</details>


### [56] [SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis](https://arxiv.org/abs/2512.15369)
*Maximilian Kellner,Mariana Ferrandon Cervantes,Yuandong Pan,Ruodan Lu,Ioannis Brilakis,Alexander Reiterer*

Main category: cs.CV

TL;DR: 研究提出了一个桥梁3D语义分割数据集，评估了三种模型并发现传感器差异可能导致性能下降11.4%。


<details>
  <summary>Details</summary>
Motivation: 基础设施检测和维护对现代社会至关重要，但缺乏专门用于桥梁3D语义分割的数据集，且传感器差异导致的领域差距尚未被充分研究。

Method: 研究提出了一个包含多国桥梁结构高分辨率3D扫描的数据集，并提供了详细的语义标签。通过评估三种最先进的3D深度学习架构，分析了它们在数据集上的表现，并量化了传感器差异导致的领域差距。

Result: 所有评估的3D深度学习架构在桥梁语义分割任务上表现稳健，但传感器差异可能导致性能下降高达11.4% mIoU。

Conclusion: 该研究通过提出一个专门用于桥梁3D语义分割的数据集，并分析不同传感器导致的领域差距，为基础设施检测和维护领域提供了重要支持。研究发现，尽管现有3D深度学习模型在任务上表现稳健，但传感器差异可能导致性能下降高达11.4% mIoU。

Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.

</details>


### [57] [Spatia: Video Generation with Updatable Spatial Memory](https://arxiv.org/abs/2512.15716)
*Jinjing Zhao,Fangyun Wei,Zhening Liu,Hongyang Zhang,Chang Xu,Yan Lu*

Main category: cs.CV

TL;DR: Spatia是一种新型视频生成框架，通过3D点云记忆提升时空一致性，支持相机控制和3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以保持长时空一致性，Spatia旨在通过空间记忆解决这一问题。

Method: 提出Spatia框架，利用3D场景点云作为持久空间记忆，迭代生成视频片段并持续更新记忆。

Result: Spatia不仅提升了生成视频的空间一致性，还实现了显式相机控制和3D感知编辑功能。

Conclusion: Spatia通过结合空间记忆和视觉SLAM技术，显著提升了视频生成的长时空一致性，并支持3D感知的交互式编辑。

Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>


### [58] [See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball](https://arxiv.org/abs/2512.15386)
*Arnau Barrera Roy,Albert Clapés Sintes*

Main category: cs.CV

TL;DR: 该论文提出篮球视频中篮板球归属预测任务，创建新数据集并使用深度学习方法进行基准测试，展示了预测的可行性和挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉和视频理解在体育分析中取得了显著进展，但对运动视频中动作发生前的预测研究较少。该工作旨在填补这一空白，特别是在篮球比赛中的篮板球预测。

Method: 研究引入了篮球广播视频中的动作预测任务，并创建了一个包含10万个视频片段、300多小时素材和2000多个手动标注篮板事件的新数据集。使用了最先进的行动预测方法进行基准测试。

Result: 实验结果表明，预测篮板球的归属是可行的，但也存在挑战。数据集支持广泛的视频理解应用，且目前没有类似的数据集存在。

Conclusion: 该研究通过预测篮球比赛中投篮后的篮板球归属，展示了在动态多智能体运动场景中预测建模的可行性和挑战，为实时自动广播和赛后分析工具提供了支持。

Abstract: Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.

</details>


### [59] [Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning](https://arxiv.org/abs/2512.15410)
*Simon Gutwein,Arthur Longuefosse,Jun Seita,Sabine Taschner-Mandl,Roxane Licandro*

Main category: cs.CV

TL;DR: 研究发现浅层通道独立模型（CIM-S）在多重成像数据的自监督学习中表现优于传统深度早期融合CNN，尤其在罕见细胞识别上。


<details>
  <summary>Details</summary>
Motivation: 探讨在多重成像数据中，保留标记独立性并结合浅层架构是否为自监督表示学习提供更合适的归纳偏置，而非增加模型规模。

Method: 研究比较了标准的早期融合CNN与通道分离架构（包括标记感知基线和新型浅层通道独立模型CIM-S），在Hodgkin淋巴瘤CODEX数据集上进行对比预训练和线性评估。

Result: 通道独立架构（尤其是CIM-S）尽管参数较少（5.5K），但在表示学习上表现显著优于早期融合模型，特别是在罕见细胞识别和标记特异性信息保留方面。

Conclusion: 轻量级、通道独立的架构（如CIM-S）在多重成像数据的自监督表示学习中，能够匹配甚至超越深度早期融合CNN和基础模型。

Abstract: Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.

</details>


### [60] [Step-GUI Technical Report](https://arxiv.org/abs/2512.15431)
*Haolong Yan,Jia Wang,Xin Huang,Yeqing Shen,Ziyang Meng,Zhimin Fan,Kaijun Tan,Jin Gao,Lieyu Shi,Mi Yang,Shiliang Yang,Zhirui Wang,Brian Li,Kang An,Chenyang Li,Lei Lei,Mengmeng Duan,Danxun Liang,Guodong Liu,Hang Cheng,Hao Wu,Jie Dong,Junhao Huang,Mei Chen,Renjie Yu,Shunshan Li,Xu Zhou,Yiting Dai,Yineng Deng,Yingdan Liang,Zelin Chen,Wen Sun,Chengxu Yan,Chunqin Xu,Dong Li,Fengqiong Xiao,Guanghao Fan,Guopeng Li,Guozhen Peng,Hongbing Li,Hang Li,Hongming Chen,Jingjing Xie,Jianyong Li,Jingyang Zhang,Jiaju Ren,Jiayu Yuan,Jianpeng Yin,Kai Cao,Liang Zhao,Liguo Tan,Liying Shi,Mengqiang Ren,Min Xu,Manjiao Liu,Mao Luo,Mingxin Wan,Na Wang,Nan Wu,Ning Wang,Peiyao Ma,Qingzhou Zhang,Qiao Wang,Qinlin Zeng,Qiong Gao,Qiongyao Li,Shangwu Zhong,Shuli Gao,Shaofan Liu,Shisi Gao,Shuang Luo,Xingbin Liu,Xiaojia Liu,Xiaojie Hou,Xin Liu,Xuanti Feng,Xuedan Cai,Xuan Wen,Xianwei Zhu,Xin Liang,Xin Liu,Xin Zhou,Yingxiu Zhao,Yukang Shi,Yunfang Xu,Yuqing Zeng,Yixun Zhang,Zejia Weng,Zhonghao Yan,Zhiguo Huang,Zhuoyu Wang,Zheng Ge,Jing Li,Yibo Zhu,Binxing Jiao,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 提出自进化训练管道和校准步奖励系统，显著降低标注成本并提升质量；Step-GUI模型在多个GUI任务中表现优异；GUI-MCP协议实现高隐私执行；AndroidDaily基准测试验证实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在GUI自动化中高质量训练数据获取和标注可靠性的挑战，推动实用GUI代理的发展。

Method: 采用自进化训练管道和校准步奖励系统，将模型生成的轨迹转化为可靠的训练信号，并通过轨迹级校准实现高精度标注。开发了Step-GUI模型家族和GUI-MCP协议，结合低级原子操作和高级任务委派，实现高隐私执行。

Result: Step-GUI模型在多个基准测试中达到最先进性能（如8B模型在AndroidWorld上80.2%），GUI-MCP协议实现了高隐私执行，AndroidDaily基准测试验证了代理在真实场景中的表现。

Conclusion: 本研究通过自进化训练管道和校准步奖励系统，显著提升了GUI自动化的数据质量和效率，同时通过GUI-MCP协议和AndroidDaily基准测试，为GUI代理的实际部署和评估提供了标准化解决方案，展示了其在日常数字交互中的强大潜力。

Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.

</details>


### [61] [CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning](https://arxiv.org/abs/2512.15433)
*Longchen Dai,Zixuan Shen,Zhiheng Zhou,Peipeng Yu,Zhihua Xia*

Main category: cs.CV

TL;DR: CLIP-FTI利用CLIP和StyleGAN实现更精细的人脸模板反演，提升识别准确率和跨模型攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸模板反演方法生成的图像存在面部部分属性过度平滑和可转移性有限的问题，CLIP-FTI旨在通过引入CLIP模型的语义信息来解决这些问题。

Method: 利用CLIP模型提取面部特征属性的语义嵌入，通过跨模态特征交互网络与泄露的模板融合，并投影到预训练StyleGAN的潜在空间中，生成具有更精细面部特征的人脸图像。

Result: 实验表明，CLIP-FTI在多个数据集和识别模型上实现了更高的识别准确率和属性相似度，恢复了更清晰的组件级属性语义，并提升了跨模型攻击的可转移性。

Conclusion: CLIP-FTI通过结合CLIP模型的语义嵌入和StyleGAN的生成能力，成功实现了更精细的人脸模板反演，提高了识别准确率和属性相似度，同时增强了跨模型攻击的可转移性。

Abstract: Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.

</details>


### [62] [ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence](https://arxiv.org/abs/2512.15445)
*Yueqianji Chen,Kevin Williams,John H. Doonan,Paolo Remagnino,Jo Hepworth*

Main category: cs.CV

TL;DR: ST-DETrack 是一种时空融合双解码器网络，通过自适应门控和生物约束，有效解决植物分支跟踪问题，BMA 达 93.6%。


<details>
  <summary>Details</summary>
Motivation: 解决植物分支在时间序列图像中因非刚性生长动态和严重身份碎片化而导致的跟踪难题。

Method: 提出了一种时空融合双解码器网络（ST-DETrack），结合空间解码器（利用几何先验）和时间解码器（利用运动一致性），并引入自适应门控机制和基于负向重力性的生物约束。

Result: 在 Brassica napus 数据集上，ST-DETrack 实现了 93.6% 的分支匹配准确率（BMA），显著优于空间和时间基线方法。

Conclusion: ST-DETrack 通过结合空间和时间解码器，以及自适应门控机制和生物约束，显著提高了在复杂动态植物架构中保持长期身份一致性的能力，验证了其高效性。

Abstract: Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.

</details>


### [63] [Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception](https://arxiv.org/abs/2512.15480)
*Malach Obisa Amonga,Benard Osero,Edna Too*

Main category: cs.CV

TL;DR: 研究评估ResNet-101和Inception v3在野生动物检测中的表现，两者均表现优异，但相似物种或复杂环境下的检测仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 野生动物目标检测对生物多样性保护、生态监测和栖息地保护至关重要，但面临环境变化、物种视觉相似性和类内多样性等挑战。

Method: 研究采用ResNet-101和Inception v3两种深度学习架构，通过标准化预处理（如图像调整至最大800像素、转换为RGB格式及PyTorch张量）训练和评估野生动物图像数据集，训练与验证比例为70:30。

Result: ResNet-101分类准确率为94%，mAP为0.91；Inception v3表现略优，分类准确率为95%，mAP为0.92。两种模型在视觉相似物种或光线差、遮挡情况下检测仍有挑战。

Conclusion: 研究确认ResNet-101和Inception v3在野生动物目标检测任务中表现优异，为保护性计算机视觉应用提供了可靠基础。

Abstract: Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.

</details>


### [64] [RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting](https://arxiv.org/abs/2512.15488)
*Seyed Abolfazl Ghasemzadeh,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: RUMPL是一种基于Transformer的3D姿态提升框架，通过3D射线表示和View Fusion Transformer，显著提升了多视角3D人体姿态估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有多视角学习方法在真实场景中泛化能力不足的问题，因缺乏大规模多视角数据集且数据采集条件受限。

Method: 提出RUMPL，一种基于Transformer的3D姿态提升器，采用3D射线表示2D关键点，并结合View Fusion Transformer聚合多视角信息。

Result: 实验表明，RUMPL相比三角测量和基于Transformer的图像表示基线，分别降低了53%和60%以上的MPJPE。在新基准测试中验证了其鲁棒性和可扩展性。

Conclusion: RUMPL框架通过引入基于3D射线的2D关键点表示，实现了对任意多视角配置的通用部署，无需重新训练或微调，显著提升了3D人体姿态估计的准确性和鲁棒性。

Abstract: Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL

</details>


### [65] [The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge](https://arxiv.org/abs/2512.15505)
*Rohit Jena,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: 研究重新评估了深度学习方法在医学图像配准中的零样本泛化能力，发现其在特定条件下表现良好，但在不同对比度和分辨率上性能显著下降，且对预处理敏感，建议更贴近实际的评估协议。


<details>
  <summary>Details</summary>
Motivation: 动机在于对LUMIR挑战中关于深度学习方法在未见对比度和分辨率上具有卓越零样本泛化能力的声明进行独立验证，这些声明与深度学习领域中对域偏移的已有理解相矛盾。

Method: 研究采用了严格的评估协议，对深度学习方法的零样本泛化能力进行了独立重新评估，并考虑了潜在的仪器偏差来源。

Result: 研究发现：(1)深度学习方法在分布内T1w图像和相近物种（猕猴）上表现与迭代优化方法相当；(2)在不同对比度（T2、T2*、FLAIR）上性能显著下降；(3)在高分辨率数据上存在可扩展性限制；(4)对预处理选择高度敏感。

Conclusion: 研究结论强调了深度学习在医学图像配准中的局限性，尤其是在面对不同对比度和分辨率时的性能下降，以及对预处理选择的高敏感性。研究建议评估协议应更贴近实际的临床和研究工作流程。

Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.

</details>


### [66] [Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting](https://arxiv.org/abs/2512.15508)
*Arthur Moreau,Richard Shaw,Michal Nazarczuk,Jisu Shin,Thomas Tanay,Zhensong Zhang,Songcen Xu,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 提出一种自适应分布基元的3D高斯抛雪球模型，显著提升生成效率和质量，同时减少基元数量并改善相机姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统基于密集刚性网格的3D高斯抛雪球模型在基元放置上存在局限，影响生成质量和效率。

Method: 采用多分辨率解码器学习在图像块中分布基元，结合自监督学习的3D重建主干网络进行端到端训练。

Result: 新方法在秒级内生成逼真场景，实现了前馈模型中最先进的新视角合成，且基元使用量更少，细节更丰富，伪影更少。

Conclusion: 该论文提出的新架构通过自适应分布替代传统密集网格，显著提升了3D高斯抛雪球模型的生成质量和效率，同时减少了所需基元数量。此外，该方法还改善了相机姿态估计，展示了无标签训练基础模型的潜力。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.

</details>


### [67] [VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics](https://arxiv.org/abs/2512.15512)
*Opeyemi Bamigbade,Mark Scanlon,John Sheppard*

Main category: cs.CV

TL;DR: VAAS框架结合ViT和SegFormer，提供连续可解释的异常评分，提升伪造图像检测的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型能产生视觉一致的伪造图像，传统基于像素或压缩伪影的检测方法难以应对，且缺乏异常强度的明确度量。

Method: 提出Vision-Attention Anomaly Scoring (VAAS)框架，整合Vision Transformers (ViT)的全局注意力异常估计和SegFormer嵌入的补丁级自一致性评分。

Result: 在DF2023和CASIA v2.0数据集上，VAAS实现了竞争性的F1和IoU性能，并通过注意力引导的异常图增强了视觉可解释性。

Conclusion: VAAS框架通过结合全局注意力异常估计和补丁级自一致性评分，提供了一个连续且可解释的异常评分，增强了图像完整性评估的透明度和可靠性。

Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.

</details>


### [68] [DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations](https://arxiv.org/abs/2512.15524)
*Yuxiang Shi,Zhe Li,Yanwen Wang,Hao Zhu,Xun Cao,Ligang Liu*

Main category: cs.CV

TL;DR: DeX-Portrait 是一种新型肖像动画方法，通过解耦姿态和表情信号实现高保真控制，实验证明其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型无法实现姿态与表情的高保真解耦控制，限制了如仅表情或仅姿态编辑的应用。

Method: 提出 DeX-Portrait，通过显式全局变换表示姿态，隐式潜在编码表示表情；设计运动训练器学习解耦驱动信号，采用双分支条件机制注入姿态变换，并通过交叉注意力注入表情潜在编码；最后设计渐进式混合无分类器引导以保持身份一致性。

Result: 实验表明，DeX-Portrait 在动画质量和解耦可控性上优于现有基线方法。

Conclusion: DeX-Portrait 在肖像动画质量和解耦控制能力上优于现有方法，实现了高保真的姿态和表情分离控制。

Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.

</details>


### [69] [EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration](https://arxiv.org/abs/2512.15528)
*Daiqing Wu,Dongbao Yang,Can Ma. Yu Zhou*

Main category: cs.CV

TL;DR: EmoCaliber 是一个自信感知的 MLLM，通过三阶段训练框架提升情感预测的可靠性，并在 VECBench 中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLMs 在 VEC 任务中通常将情感预测视为确定性任务，忽略了情感感知的主观性，因此需要增强模型对自信度的表达能力。

Method: 提出了一个三阶段训练框架，逐步赋予模型结构化推理、自信表达和校准能力，最终形成 EmoCaliber。

Result: EmoCaliber 在情感预测和自信度估计方面均优于现有方法。

Conclusion: EmoCaliber 在 VECBench 基准测试中表现出色，验证了其方法的有效性，标志着向更可靠的 VEC 系统迈出了可行的一步。

Abstract: Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.

</details>


### [70] [An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain](https://arxiv.org/abs/2512.15531)
*João Daniel Silva,Joao Magalhaes,Devis Tuia,Bruno Martins*

Main category: cs.CV

TL;DR: GeoMELT模型通过编码器专用架构实现多任务学习，解决了LVLMs的高成本问题，在遥感图像文本生成和跨模态检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型视觉与语言模型（LVLMs）在遥感领域的多任务处理潜力巨大，但其高昂的训练和使用成本限制了广泛应用。

Method: 提出了一种基于编码器专用架构的GeoMELT模型，专注于多任务学习，尤其是遥感图像文本生成和跨模态检索。

Result: GeoMELT模型在基准测试中表现出色，证实了其在多任务学习中的高效性和参数紧凑性。

Conclusion: GeoMELT模型通过采用编码器专用架构，在多任务学习中实现了高效性和参数紧凑性，验证了其在遥感图像文本生成和跨模态检索任务中的有效性。

Abstract: The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.

</details>


### [71] [BLANKET: Anonymizing Faces in Infant Video Recordings](https://arxiv.org/abs/2512.15542)
*Ditmar Hadera,Jan Cech,Miroslav Purkrabek,Matej Hoffmann*

Main category: cs.CV

TL;DR: BLANKET是一种新颖的婴儿面部匿名化方法，通过生成兼容随机面部和时间一致交换，在多个指标上优于DeepPrivacy2。


<details>
  <summary>Details</summary>
Motivation: 确保涉及人类受试者（尤其是婴儿）的视频数据的伦理使用，需要强大的匿名化方法。

Method: BLANKET采用两阶段方法：首先通过扩散模型生成与原身份兼容的随机面部，然后通过时间一致的面部交换和真实表情转移将新身份无缝融入视频帧。

Result: BLANKET在身份去识别、面部属性保留、下游任务（如人体姿态估计）影响及伪影存在方面均优于DeepPrivacy2。

Conclusion: BLANKET方法在婴儿面部匿名化方面优于DeepPrivacy2，尤其在保持面部属性和减少伪影方面表现更佳。

Abstract: Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.

</details>


### [72] [GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models](https://arxiv.org/abs/2512.15560)
*Bozhou Li,Sihan Yang,Yushuo Guan,Ruichuan An,Xinlong Chen,Yang Shi,Pengfei Wan,Wentao Zhang,Yuanxing zhang*

Main category: cs.CV

TL;DR: GRAN-TED 通过新基准和两阶段训练，优化了扩散模型的文本编码器，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本编码器在视觉合成中缺乏高效评估框架和适应预训练语言模型的难题。

Method: 提出 TED-6K 文本基准和两阶段训练范式（先微调多模态大语言模型，再通过层加权提取文本特征）。

Result: GRAN-TED 在 TED-6K 上达到最优性能，并在下游生成任务中带来显著提升。

Conclusion: GRAN-TED 提出了一种新的文本编码器范式，通过 TED-6K 基准和两阶段训练方法，显著提升了文本到图像和视频生成的语义保真度。

Abstract: The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.

</details>


### [73] [On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation](https://arxiv.org/abs/2512.15564)
*Roni Blushtein-Livnon,Osher Rafaeli,David Ioffe,Amir Boger,Karen Sandberg Esquenazi,Tal Svoray*

Main category: cs.CV

TL;DR: SAM3框架通过文本提示生成掩码，混合提示策略表现最佳，轻量级微调适用于规则目标，几何标注适度即可有效适应，但分割不足和边界错误仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分割面临标注数据有限及基础模型训练图像与航拍图像差异的问题，需要研究有限监督下的有效适应方法。

Method: 提出了SAM3概念驱动框架，通过文本提示生成掩码，无需任务特定修改。评估了文本、几何及混合提示策略，并在轻量级微调和零样本推理下进行了比较。

Result: 混合提示策略在所有目标和指标上表现最佳。文本提示对不规则形状目标效果较差，但轻量级微调在规则目标上表现实用。性能随监督规模增加而提升，但边际效益递减。

Conclusion: 在遥感图像分割任务中，结合语义和几何提示的策略表现最佳，而仅使用文本提示的效果最差。轻量级微调在几何规则且视觉显著的目标上提供了实用的性能-努力平衡。几何标注的适度投入足以实现有效适应，但分割不足和边界不准确仍是主要错误模式。

Abstract: Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.

</details>


### [74] [MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors](https://arxiv.org/abs/2512.15577)
*Zhipeng Du,Duolikun Danier,Jan Eric Lenssen,Hakan Bilen*

Main category: cs.CV

TL;DR: MoonSeg3R enables online monocular 3D segmentation via CUT3R geometric priors, outperforming RGB-D-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail in online zero-shot monocular 3D instance segmentation due to reliance on posed RGB-D sequences.

Method: Introduces three components: self-supervised query refinement, 3D query index memory, and state-distribution token from CUT3R.

Result: Achieves competitive performance on ScanNet200 and SceneNN datasets.

Conclusion: MoonSeg3R is the first method enabling online monocular 3D segmentation, achieving performance competitive with RGB-D-based systems.

Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.

</details>


### [75] [IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion](https://arxiv.org/abs/2512.15581)
*Shashank Mishra,Karan Patil,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: IMKD通过多级知识蒸馏提升雷达-相机3D检测性能，保留传感器特性并增强互补性，达到SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法直接传递模态特定特征，可能扭曲传感器特性并削弱其优势。IMKD旨在保留各传感器固有特性的同时增强其互补性。

Method: IMKD采用三阶段强度感知蒸馏策略：（1）LiDAR到雷达的强度感知特征蒸馏；（2）LiDAR到融合特征的强度引导蒸馏；（3）相机-雷达强度引导融合机制。

Result: 在nuScenes基准测试中，IMKD以67.0% NDS和61.0% mAP超越所有基于蒸馏的雷达-相机融合方法。

Conclusion: IMKD通过多级知识蒸馏框架显著提升了雷达-相机融合的3D目标检测性能，且无需在推理时使用LiDAR，达到了67.0% NDS和61.0% mAP的优异表现。

Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.

</details>


### [76] [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/abs/2512.15599)
*Tobias Kirschstein,Simon Giebenhain,Matthias Nießner*

Main category: cs.CV

TL;DR: FlexAvatar通过Transformer模型和可学习数据源标记，从单张图像生成高质量3D头部虚拟形象，解决了单目训练的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频学习时驱动信号和目标视角之间的纠缠问题，以及多视图数据有限导致的3D头部重建不完整挑战。

Method: 提出了一种基于Transformer的3D肖像动画模型，引入了可学习的数据源标记（bias sinks），实现了单目和多视图数据集的统一训练。

Result: FlexAvatar在评估中表现优异，能够生成完整的3D头部虚拟形象并实现逼真的面部动画，优于许多现有方法。

Conclusion: FlexAvatar成功地从单张图像创建了高质量且完整的3D头部虚拟形象，解决了单目训练导致的3D重建不完整问题，并在单视图、少样本和单目虚拟形象创建任务中表现出色。

Abstract: We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/

</details>


### [77] [Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition](https://arxiv.org/abs/2512.15603)
*Shengming Yin,Zekai Zhang,Zecheng Tang,Kaiyuan Gao,Xiao Xu,Kun Yan,Jiahao Li,Yilei Chen,Yuxiang Chen,Heung-Yeung Shum,Lionel M. Ni,Jingren Zhou,Junyang Lin,Chenfei Wu*

Main category: cs.CV

TL;DR: Qwen-Image-Layered是一种扩散模型，通过分解图像为可独立编辑的RGBA层，解决了编辑一致性问题，并提出了新的训练策略和数据管道。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在图像编辑中因栅格图像的纠缠性而难以保持一致性，而专业设计工具的分层表示提供了灵感。

Method: 提出了三个关键组件：RGBA-VAE、VLD-MMDiT架构和多阶段训练策略，并构建了从PSD文件中提取多层图像的训练数据管道。

Result: 实验表明，该方法在分解质量上显著超越现有方法，为一致性图像编辑建立了新范式。

Conclusion: Qwen-Image-Layered提出了一种端到端的扩散模型，通过分解RGB图像为多个语义解耦的RGBA层，实现了内在可编辑性，显著提升了分解质量和图像编辑的一致性。

Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}

</details>


### [78] [Robust Multi-view Camera Calibration from Dense Matches](https://arxiv.org/abs/2512.15608)
*Johannes Hägerlind,Bao-Long Tran,Urs Waldmann,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 论文提出了一种改进的SfM方法，通过优化对应关系子采样和视图增量添加策略，显著提高了相机姿态估计的准确性，尤其在径向畸变严重的场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 相机内参和外参估计是计算机视觉中的基本问题，尽管结构从运动（SfM）的进展提高了准确性和鲁棒性，但仍存在开放挑战。本文旨在提出一种鲁棒的姿态估计和校准方法，特别适用于动物行为研究和监控视频的法医分析。

Method: 论文分析了结构从运动（SfM）流程中的各个组件，并确定了提高准确性的设计选择。主要贡献包括：（1）研究如何最佳地子采样密集匹配器预测的对应关系以在估计过程中利用它们；（2）研究如何增量添加视图的选择标准。

Result: 在严格的定量评估中，论文展示了其改进的有效性，特别是在具有强烈径向畸变的相机上（79.9% vs. 40.4%）。此外，论文还在全局SfM设置中演示了对应关系子采样的效果。

Conclusion: 该论文提出的方法在相机姿态估计和校准中表现出色，尤其在具有强烈径向畸变的相机上效果显著（79.9% vs. 40.4%），并适用于广泛的相机设置，可能成为动物行为和法医分析的有用工具。

Abstract: Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.

</details>


### [79] [Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images](https://arxiv.org/abs/2512.15618)
*Morgan Coe,Gruffudd Jones,Leah-Nani Alconcel,Marina Gashinova*

Main category: cs.CV

TL;DR: 提出了一种基于ISAR图像序列的特征跟踪方法，用于提高空间物体外部结构的检测和分类信心，并通过阴影检测验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着近地空间环境中居民空间物体（RSOs）数量的快速增长，需要详细的信息来提供空间域感知（SDA）。

Method: 采用Hough变换检测线性特征，并通过梯度比方法进行边缘检测，结合双加权Hough变换提高特征检测精度。ISAR图像通过元启发式模拟器生成，初始帧对齐通过仿射变换实现。

Result: 展示了在ISAR图像序列中通过特征跟踪提高检测和分类信心的能力，并通过阴影检测用例验证了方法的有效性。

Conclusion: 通过提出的方法，利用特征跟踪在序列中增加了特征检测和分类的信心，并通过阴影检测的用例展示了其鲁棒性。

Abstract: With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented.

</details>


### [80] [OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence](https://arxiv.org/abs/2512.15621)
*Yu Zheng,Jie Hu,Kailun Yang,Jiaming Zhang*

Main category: cs.CV

TL;DR: 论文提出OccSTeP-WM模型，通过线性注意力机制和状态空间模块，显著提升自动驾驶场景理解的时空持续性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要鲁棒的3D场景理解，以应对时间干扰和未来潜在动作。

Method: 提出了一种无分词器的世界模型OccSTeP-WM，采用线性复杂度注意力机制和循环状态空间模块，逐步融合时空上下文。

Result: 实验显示，OccSTeP-WM在语义mIoU和占据IoU上分别提升了6.56%和9.26%。

Conclusion: 论文提出了OccSTeP-WM模型，有效解决了自动驾驶中4D时空持续性的挑战，并在实验中显著提升了语义和占据IoU指标。

Abstract: Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.

</details>


### [81] [InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization](https://arxiv.org/abs/2512.15644)
*Qirui Li,Yizhe Tang,Ran Yi,Guangben Lu,Fangyuan Zou,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CV

TL;DR: InpaintDPO是一个基于DPO的框架，通过多种优化方法解决前景条件修复中的空间关系幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在前景条件修复中存在空间关系幻觉问题（如不合理的比例、位置关系和视角），且空间合理性的主观性使其难以量化，阻碍了传统RLHF方法的应用。

Method: 提出了InpaintDPO框架，包括MaskDPO（限制偏好优化于背景区域）、Conditional Asymmetric Preference Optimization（通过差异化裁剪操作增强边界一致性）和Shared Commonality Preference Optimization（利用高质量样本的空间共性增强模型理解）。

Result: InpaintDPO框架显著提升了前景与背景的空间合理性，解决了梯度冲突和边界一致性问题。

Conclusion: InpaintDPO通过MaskDPO、Conditional Asymmetric Preference Optimization和Shared Commonality Preference Optimization等方法，有效解决了前景条件修复中的空间关系幻觉问题，提升了前景与背景的空间合理性。

Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.

</details>


### [82] [Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift](https://arxiv.org/abs/2512.15647)
*Jiacheng Cui,Bingkui Tong,Xinyue Bi,Xiaohan Zhao,Jiacheng Liu,Zhiqiang shen*

Main category: cs.CV

TL;DR: 本文发现软标签在有限裁剪下易产生局部语义漂移，提出结合硬标签的HALD方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 观察到在每张图像仅使用有限数量的裁剪时，软标签容易出现局部语义漂移，导致视觉内容与全局语义不匹配，从而引入系统误差和训练测试分布不对齐。

Method: 提出了一种新的训练范式HALD，结合软标签和硬标签的优势，利用硬标签作为中间纠正信号，同时保留软标签的细粒度优势。

Result: 在ImageNet-1K上仅使用285M存储的软标签，实现了42.7%的准确率，比之前的LPLD方法提升了9.0%。

Conclusion: 本文通过理论分析和实验验证，重新确立了硬标签在软标签主导训练中的互补作用，提出了HALD方法，有效缓解了局部语义漂移问题，并在多个基准测试中取得了显著的性能提升。

Abstract: Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.

</details>


### [83] [Stylized Synthetic Augmentation further improves Corruption Robustness](https://arxiv.org/abs/2512.15675)
*Georg Siedel,Rojan Regmi,Abhirami Anand,Weijia Shao,Silvia Vock,Andrey Morozov*

Main category: cs.CV

TL;DR: 结合合成数据和风格迁移的数据增强方法，显著提升模型鲁棒性，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对深度视觉模型对常见损坏的脆弱性，探索如何通过数据增强提升模型的鲁棒性。

Method: 通过系统实证分析，研究了风格迁移和合成数据增强及其关键超参数对图像分类器性能的影响。

Result: 在CIFAR-10-C、CIFAR-100-C和TinyImageNet-C上分别达到了93.54%、74.9%和50.86%的鲁棒准确率。

Conclusion: 本文提出的结合合成图像数据和神经风格迁移的训练数据增强流程，显著提升了深度视觉模型对常见损坏的鲁棒性，并在多个小规模图像分类基准上实现了最先进的性能。

Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively

</details>


### [84] [Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning](https://arxiv.org/abs/2512.15693)
*Yifei Li,Wenzhao Zheng,Yanran Zhang,Runze Sun,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Skyra是一种多模态大语言模型，通过识别AI生成视频中的视觉伪影并提供解释，显著提升检测性能。ViF-CoT-4K数据集和ViF-Bench基准测试支持其开发与评估。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术的滥用引发社会担忧，现有方法多为二元分类且缺乏可解释性，亟需可靠且可解释的检测方法。

Method: 采用两阶段训练策略，结合ViF-CoT-4K数据集进行监督微调，增强模型的时空伪影感知、解释能力和检测精度。

Result: Skyra在多个基准测试中表现优于现有方法，ViF-Bench的评估为可解释AI生成视频检测提供了重要见解。

Conclusion: Skyra模型通过多模态大语言模型（MLLM）识别AI生成视频中的视觉伪影，并结合人类可理解的解释，显著提升了检测的准确性和可解释性。ViF-Bench的引入为未来研究提供了全面的评估标准。

Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.

</details>


### [85] [VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression](https://arxiv.org/abs/2512.15701)
*Kyle Sargent,Ruiqi Gao,Philipp Henzler,Charles Herrmann,Aleksander Holynski,Li Fei-Fei,Jiajun Wu,Jason Zhang*

Main category: cs.CV

TL;DR: VLIC利用视觉语言模型的零样本推理能力改进图像压缩，直接使用VLM判断作为训练信号，在多个数据集上实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩模型中的失真函数（如MSE）与人类感知不一致，而VLM的零样本推理能力可以复现人类视觉偏好，这为改进图像压缩提供了新思路。

Method: 利用扩散模型的后训练技术，直接使用VLM的二元判断作为偏好信号，而非将其蒸馏为单独的感知损失网络。

Result: VLIC系统在人类对齐的视觉压缩任务中表现出色，根据感知指标和大规模用户研究，其性能在多个数据集上达到竞争性或最优水平。

Conclusion: 本文提出了一种基于视觉语言模型（VLM）的图像压缩系统VLIC，该系统通过零样本推理能力复现人类视觉偏好，并在多个数据集上实现了竞争性或最先进的性能。

Abstract: Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic

</details>


### [86] [End-to-End Training for Autoregressive Video Diffusion via Self-Resampling](https://arxiv.org/abs/2512.15702)
*Yuwei Guo,Ceyuan Yang,Hao He,Yang Zhao,Meng Wei,Zhenheng Yang,Weilin Huang,Dahua Lin*

Main category: cs.CV

TL;DR: Resampling Forcing是一种无需教师的端到端框架，通过自采样和稀疏因果掩码训练自回归视频模型，优于传统方法在长视频生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型在训练与测试不匹配时的暴露偏差问题，避免依赖双向教师模型或在线判别器。

Method: 提出了Resampling Forcing框架，包括自采样方案模拟推理时误差、稀疏因果掩码确保时间因果性，以及历史路由机制动态检索相关历史帧。

Result: 实验表明，该方法在性能上与基于蒸馏的基线相当，且在长视频中因原生长度训练而表现出更优的时间一致性。

Conclusion: Resampling Forcing框架通过自采样方案和稀疏因果掩码，实现了端到端的自回归视频模型训练，且在长视频生成中展现出优越的时间一致性。

Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.

</details>


### [87] [GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection](https://arxiv.org/abs/2512.15707)
*Yu Wang,Juhyung Ha,Frangil M. Ramirez,Yuchen Wang,David J. Crandall*

Main category: cs.CV

TL;DR: GateFusion通过分层门控融合和辅助目标，显著提升了ASD性能，并在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的ASD方法多依赖后期融合，难以捕捉细粒度的跨模态交互，影响在无约束场景中的鲁棒性。

Method: 提出了GateFusion架构，结合预训练的单模态编码器和分层门控融合解码器（HiGate），通过可学习的双模态门控在Transformer骨干的多层进行渐进式多深度融合。此外，提出了掩码对齐损失（MAL）和过正惩罚（OPP）来增强多模态学习。

Result: 在Ego4D-ASD、UniTalk和WASD基准测试中分别达到77.8% mAP（+9.4%）、86.1% mAP（+2.9%）和96.1% mAP（+0.5%），并在AVA-ActiveSpeaker上表现优异。

Conclusion: GateFusion通过引入HiGate解码器和两种辅助目标（MAL和OPP），在多个ASD基准测试中取得了新的最先进性能，并展示了良好的泛化能力。

Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.

</details>


### [88] [Multi-View Foundation Models](https://arxiv.org/abs/2512.15708)
*Leo Segre,Or Hirschorn,Shai Avidan*

Main category: cs.CV

TL;DR: 提出一种将基础模型转化为多视图基础模型的方法，通过3D感知注意力层提升跨视图特征一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基础模型在多视图场景中无法保证特征一致性的问题。

Method: 在基于Transformer的基础模型（如DINO、SAM、CLIP）中引入中间3D感知注意力层，以实现跨视图特征匹配。

Result: 定量实验表明，该方法在特征匹配方面相比现有基础模型有显著提升。

Conclusion: 通过引入中间3D感知注意力层，成功将基础模型转化为多视图基础模型，显著提高了特征匹配的一致性。

Abstract: Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.

</details>


### [89] [DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models](https://arxiv.org/abs/2512.15713)
*Lunbin Zeng,Jingfeng Yao,Bencheng Liao,Hongyuan Tao,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DiffusionVL通过微调自回归模型构建扩散视觉语言模型，性能显著提升且推理速度翻倍。


<details>
  <summary>Details</summary>
Motivation: 探索是否能够基于现有强大的自回归模型构建扩散视觉语言模型（dVLM），以弥补当前dVLM性能不足的缺陷。

Method: 通过简单微调，将自回归预训练模型适应到扩散范式，并引入块解码设计以支持任意长度生成和KV缓存重用。

Result: DiffusionVL在少量数据训练下实现了全面性能提升（MMMU-Pro视觉基准提升34.4%，MME认知基准提升37.5%）和2倍推理加速。

Conclusion: DiffusionVL展示了从现有强大的自回归模型转换到扩散范式的可行性，并在性能和推理速度上实现了显著提升。

Abstract: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.

</details>


### [90] [In Pursuit of Pixel Supervision for Visual Pre-training](https://arxiv.org/abs/2512.15715)
*Lihe Yang,Shang-Wen Li,Yang Li,Xinjie Lei,Dong Wang,Abdelrahman Mohamed,Hengshuang Zhao,Hu Xu*

Main category: cs.CV

TL;DR: Pixio是一种增强型MAE模型，证明自编码器在自监督学习中仍具竞争力，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 探索自编码器在自监督学习中的竞争力，证明其能生成强大的下游任务表示，同时保持简单、稳定和高效。

Method: 增强型掩码自编码器（MAE）模型'Pixio'，具有更具挑战性的预训练任务和更强大的架构，在20亿张网络爬取图像上训练，采用自筛选策略。

Result: Pixio在多种下游任务中表现优异，包括单目深度估计、前馈3D重建、语义分割和机器人学习，性能与DINOv3相当或更优。

Conclusion: 像素空间的自监督学习可以作为潜在空间方法的有前途的替代和补充。

Abstract: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [91] [EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving](https://arxiv.org/abs/2512.14946)
*Shaoting Feng,Yuhan Liu,Hanchen Li,Xiaokun Chen,Samuel Shen,Kuntai Du,Zhuohan Gu,Rui Zhang,Yuyang Huang,Yihua Cheng,Jiayi Yao,Qizheng Zhang,Ganesh Ananthanarayanan,Junchen Jiang*

Main category: cs.OS

TL;DR: EVICPRESS通过联合优化KV缓存的驱逐和压缩决策，显著提升LLM推理效率，实现更快的首次令牌时间且保持质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM用户增多，KV缓存占用可能超过GPU内存容量，现有方法未能联合优化驱逐和压缩决策以最小化平均生成延迟且不损害质量。

Method: EVICPRESS采用了一种统一的效用函数，量化了无损压缩或驱逐对质量和延迟的影响，并通过周期性更新所有可能的驱逐-压缩配置的效用函数分数，使用快速启发式方法重新安排所有存储层上的KV缓存。

Result: 在12个数据集和5个模型上的评估显示，EVICPRESS在同等生成质量下，首次令牌时间（TTFT）最高提升2.19倍。

Conclusion: EVICPRESS通过联合优化KV缓存的驱逐和压缩决策，显著提高了LLM推理系统的效率，实现了更快的首次令牌时间（TTFT）且保持生成质量。

Abstract: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.
  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [92] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究发现，实时同步用户的呼吸信号与机器人运动可以增强身体所有权感，为人类-机器人交互提供了新的内感受途径。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过实时体现用户的呼吸信号（称为“呼吸体现”）来增强用户在机器人系统中的身体所有权感。

Method: 采用组内实验设计，参与者控制机械臂，机械臂的运动与他们的呼吸同步或不同步。

Result: 同步呼吸信号显著增加了身体所有权感，并且大多数参与者更喜欢同步条件。

Conclusion: 研究表明，通过将用户的呼吸信号实时体现在机器人系统中，可以显著增强用户的身体所有权感，并受到大多数参与者的青睐。这为人类-机器人交互提供了一种新的内感受途径，对远程存在、假肢、机器人协作和共享自主等领域具有潜在影响。

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [93] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: ISS Policy是一种3D视觉运动扩散策略，通过隐式场景监督提升模仿学习的效率和泛化能力，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的模仿学习因依赖物体外观而忽略3D场景结构，导致训练效率低且泛化能力差。ISS Policy旨在解决这些问题。

Method: 提出了一种基于DiT的扩散策略ISS Policy，通过点云观测预测连续动作序列，并扩展了DiT模型，加入隐式场景监督模块以确保输出与场景几何演化一致。

Result: ISS Policy在单臂操作（MetaWorld）和灵巧手操作（Adroit）任务中均达到最先进性能，并在真实实验中展现出强泛化性和鲁棒性。

Conclusion: ISS Policy通过引入隐式场景监督模块，显著提升了基于视觉的模仿学习在3D场景中的性能和鲁棒性，并在单臂和灵巧手操作任务中达到了最先进水平。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [94] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: HERO框架通过层次化可通行3D场景图，显著提升导航效率与可达性。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D场景图在静态世界假设下的局限性，即仅基于静态空间布局定义可通行空间，导致在现实场景中效率低下、可达性有限的问题。

Method: 提出HERO框架，构建层次化可通行3D场景图（3DSGs），将可操作障碍物建模为通路，捕捉其物理交互性、功能语义和场景的关系层次。

Result: 与基线相比，HERO在部分阻塞环境中减少路径长度（PL）35.1%，在完全阻塞环境中提高成功率（SR）79.4%。

Conclusion: HERO框架通过重新定义可通行性，显著提升了在部分和完全阻塞环境中的导航效率与可达性。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [95] [NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles](https://arxiv.org/abs/2512.15080)
*Gaurav Bansal*

Main category: cs.RO

TL;DR: NAP3D利用3D-3D点对齐优化姿态估计，无需依赖闭环检测，在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传感器噪声和时间漂移导致的长期环境中姿态估计误差问题，尤其在无法依赖视觉闭环检测的情况下。

Method: NAP3D通过将当前深度图像的3D点与预训练的NeRF合成的3D点直接对齐，实现姿态估计的优化。

Result: 在自定义数据集上，NAP3D实现了5厘米内的相机姿态校正，并在TUM RGB-D上比基线方法提升了约6厘米的3D对齐RMSE。

Conclusion: NAP3D提供了一种轻量级、数据集无关的工具，在传统闭环检测不可用时，补充现有的SLAM和定位流程。

Abstract: Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.
  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.
  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.

</details>


### [96] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: BEV-Patch-PF 是一种无需GPS的实时地理定位系统，通过结合粒子滤波与BEV/航空特征图匹配，在越野环境中显著降低定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决无GPS环境下的机器人地理定位问题，特别是在复杂地形和光照条件下保持高精度和实时性。

Method: 通过机载RGB和深度图像构建BEV特征图，并为每个3-DoF粒子姿态假设从查询的局部航空图像中裁剪对应的航空特征图补丁，通过匹配BEV特征与航空补丁特征计算每个粒子的对数似然。

Result: 在两个真实世界越野数据集上，BEV-Patch-PF 在已见和未见路线上的绝对轨迹误差（ATE）分别比基线方法低7.5倍和7.0倍，且在密集树冠和阴影下仍保持准确性，系统实时运行频率为10 Hz。

Conclusion: BEV-Patch-PF 系统在无需GPS的情况下，通过结合粒子滤波与学习的鸟瞰图（BEV）和航空特征图，实现了高效的序列地理定位。在真实世界的越野数据集上，其性能显著优于基于检索的基线方法，且在密集树冠和阴影条件下仍保持高精度。

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [97] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出了一种新的安全度量框架，用于评估自动驾驶感知系统的安全性，弥补了传统指标在安全评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的性能指标如精确率、召回率和F1分数未能考虑感知的安全相关方面，可能导致严重事故的误检。

Method: 引入了一种轻量级对象安全度量，量化了与对象检测错误相关的潜在风险，以及包括两项任务之间相互依存关系的车道安全度量。

Result: 使用DeepAccident数据集验证了该方法能识别传统性能指标无法捕捉的安全关键感知错误。

Conclusion: 本文提出了一种新的安全度量框架，用于联合评估对象和车道检测任务，强调了对感知系统进行安全中心评估的重要性。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [98] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 本文综述了基于基础设施的AMR系统，提出了结合基础设施感知和车载自主的参考架构，并在实际工业环境中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管AMR在室内环境（如工厂）中可以得到基础设施的支持，但涉及外部传感器和计算资源的系统在文献中仍未被充分探索。

Method: 提出了一种结合基础设施感知、本地云计算和车载自主的参考架构，并基于该架构回顾了定位、感知和规划等核心技术。

Result: 在重型车辆制造环境中进行了实际部署，并总结了用户体验评估的结果。

Conclusion: 本文为复杂工业环境中可扩展、稳健且与人兼容的AMR系统提供了全面的基础，旨在促进未来开发。

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [99] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: VLA-AN 是一种高效的机载视觉-语言-动作框架，通过高保真数据集、渐进式训练和轻量级安全模块，显著提升了无人机在复杂环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型空中导航模型的四大局限性：数据域差距、时间导航与推理不足、生成动作策略的安全问题以及机载部署限制。

Method: 通过构建高保真数据集（使用3D高斯泼溅技术）、渐进式三阶段训练框架（依次强化场景理解、核心飞行技能和复杂导航能力），以及轻量级实时动作模块（结合几何安全校正），解决了现有模型的四大限制。

Result: 在资源受限的无人机上实现了实时推理吞吐量提升8.3倍，单任务最高成功率达98.1%。

Conclusion: VLA-AN 提供了一种高效、实用的解决方案，实现了轻量级空中机器人的全链闭环自主导航，显著提升了空间定位、场景推理和长时导航能力。

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [100] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 本文提出了一种整合功能建模与图论的新框架，用于分析人机系统中的联合工作策略，并通过案例研究展示了其在早期设计阶段的应用价值。


<details>
  <summary>Details</summary>
Motivation: 随着更先进的机器人能力部署，支持与人类问题持有者协作的合作能力需求增加，而现有的人机交互框架无法充分支持协调动态的早期设计阶段推理。

Method: 文章提出了一种新颖的计算框架，通过将功能建模技术与图论表示相结合，分析人机系统中的联合工作策略。

Result: 通过灾难机器人案例研究展示了该框架在概念设计中的应用，支持早期权衡探索人机协调策略，并识别支持灵活管理协调开销的合作能力。

Conclusion: 该框架通过明确协调需求及其时间演变，支持在设计阶段对合作能力要求和工作需求进行推理，为实施前的早期权衡探索提供了支持。

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [101] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: GuangMing-Explorer是一个全集成自主探索平台，适用于多样化环境，实验证明其在复杂和非结构化环境中高效有效。


<details>
  <summary>Details</summary>
Motivation: 尽管在自主探索的各个组件上取得了显著进展，但一个涵盖硬件和软件的全集成自主探索系统的全面实践描述仍然稀缺。

Method: 论文详细介绍了GuangMing-Explorer平台的系统架构，包括硬件设计、软件栈、算法部署和实验配置。

Result: 广泛的现实世界实验验证了平台在执行自主探索任务时的有效性和效率。

Conclusion: GuangMing-Explorer平台展示了在复杂和非结构化环境中进行自主探索任务的有效性和高效性，具备实际部署的潜力。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [102] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: CoNoCo是一种专为远程检测设计的水印策略，通过嵌入频谱信号到机器人运动中，有效验证策略所有权，且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在机器人系统中的成功应用，训练好的策略成为一种新型知识产权，亟需验证所有权和检测未经授权使用的方法。现有方法依赖于访问机器人内部状态，而审计者通常只能通过外部观测（如视频）进行检测，存在“物理观测鸿沟”。

Method: CoNoCo通过利用策略固有的随机性，将频谱信号嵌入机器人的运动中，且不降低性能。

Result: 实验表明，CoNoCo在模拟和真实机器人实验中，通过运动捕捉和不同视角视频等多种远程观测方式，实现了强健的检测效果。

Conclusion: 本文提出了一种名为Colored Noise Coherency（CoNoCo）的水印策略，旨在通过远程观测验证机器人策略的所有权，为非侵入式保护机器人知识产权提供了首个可行方法。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [103] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: MiVLA通过人类-机器人相互模仿预训练，提升VLA泛化能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLA模型因相机视角、视觉外观和形态差异导致的泛化能力受限问题。

Method: 利用人类手和机器人手臂的行为相似性，通过左右手坐标系的双向对齐方法，训练模型预测一种实体的行为轨迹并模仿另一种未见实体的行为。

Result: 在仿真和真实机器人平台上，MiVLA表现优于现有VLA模型（如π₀、π₀.₅和H-RDT）。

Conclusion: MiVLA通过人类-机器人相互模仿预训练，显著提升了视觉-语言-动作模型（VLA）的泛化能力，在仿真和真实机器人控制任务中分别优于现有方法25%和14%。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [104] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: LBVT机制通过预紧弹簧和四连杆被动调整传动比，仿真显示其能在预设扭矩下提升40%传动比，并在外力超过18N时放大扭矩，适用于腿式机器人。


<details>
  <summary>Details</summary>
Motivation: 现有可变传动系统需要额外执行器进行主动控制，增加了系统复杂性。LBVT旨在通过被动机制实现传动比动态调整，简化设计并提高效率。

Method: LBVT机制利用预紧弹簧和四连杆机构被动调整传动比，减少了机器人关节驱动系统的复杂性。通过仿真分析验证了其有效性。

Result: 仿真结果表明，LBVT在达到预设扭矩阈值时可实现高达40%的传动比增加，并在外力超过18N时触发扭矩放大效应，自主响应负载变化。

Conclusion: 本研究提出了一种基于负载的可变传动（LBVT）机制，通过动态调整传动比来增强机器人驱动性能，尤其适用于需要动态扭矩适应的腿式机器人。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [105] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 通过视觉-语言特征增强蒙特卡洛定位，实现多模态数据鲁棒关联，并在室内外场景中验证了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多传感器创建的环境地图中机器人测量与地图特征鲁棒关联的问题，提升定位的准确性和适应性。

Method: 扩展蒙特卡洛定位方法，利用视觉-语言特征计算视觉观测的似然度，支持多模态数据关联，并通过自然语言描述初始化全局定位。

Result: 在Matterport3D、Replica（室内）和SemanticKITTI（室外）数据集上验证了方法的有效性，展示了良好的泛化性能。

Conclusion: 该论文通过引入视觉-语言特征扩展了蒙特卡洛定位方法，显著提升了机器人定位的鲁棒性，并在室内外场景中验证了其泛化能力。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [106] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 本研究开发了一个开源的水下操纵研究工具包，旨在降低成本和提高可重复性，已在实验室和实地测试中验证其可靠性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 水下机器人技术在海洋科学、环境监测和海底工业操作中变得越来越重要，但目前水下操纵和驱动系统的开发仍受限于高成本、专有设计和模块化、研究导向硬件的有限获取。

Method: 研究团队开发了一个开放的、成本效益高的硬件和软件工具包，包括深度评级的Underwater Robotic Joint（URJ）、紧凑的控制和电源管理电子设备，以及基于ROS2的软件堆栈。所有CAD模型、制造文件、PCB源、固件和ROS2包都已公开发布。

Result: 工具包经过了广泛的实验室测试和多次实地部署，展示了在40米深度内可靠运行的能力，适用于多种应用，包括3-DoF水下操纵器、肌腱驱动的软夹持器和欠驱动沉积物采样器。

Conclusion: 本研究通过提供一个完全开源、经过实地测试的平台，旨在降低水下操作研究的入门门槛，提高可重复性，并加速水下现场机器人技术的创新。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>


### [107] [mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs](https://arxiv.org/abs/2512.15692)
*Jonas Pai,Liam Achenbach,Victoriano Montesinos,Benedek Forrai,Oier Mees,Elvis Nava*

Main category: cs.RO

TL;DR: 论文提出了一种视频-动作模型（VAM），通过结合预训练视频模型和动作解码器，显著提升了机器人操作的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（VLA）依赖于静态网络数据的预训练，缺乏对物理因果关系的理解，导致需要大量专家数据来补偿。论文提出通过视频联合捕获语义和视觉动态，以解决这一问题。

Method: 论文引入了一个视频-动作模型（VAM），该模型结合了预训练的互联网规模视频模型和一个基于流匹配的动作解码器。解码器作为逆动力学模型（IDM），从视频空间动作计划的潜在表示中生成低级机器人动作。

Result: 实验表明，该方法在模拟和真实世界的机器人操作任务中实现了最先进的性能，样本效率提高了10倍，收敛速度提高了2倍。

Conclusion: 该论文提出了一种新的视频-动作模型（VAM），通过结合预训练的互联网规模视频模型和基于流匹配的动作解码器，显著提升了机器人操作的性能和效率。

Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [108] [Probabilistic Inclusion Depth for Fuzzy Contour Ensemble Visualization](https://arxiv.org/abs/2512.15187)
*Cenyang Wu,Daniel Klötzl,Qinhan Yu,Shudan Guo,Runhao Lin,Daniel Weiskopf,Liang Zhou*

Main category: cs.GR

TL;DR: 提出PID方法，支持模糊和传统轮廓的集成可视化，通过高效近似和GPU并行算法显著提升计算效率，适用于多种网格和大规模3D集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理概率掩模集成、多种网格类型和大规模3D集成，因此需要一种通用的数据深度模型来支持这些需求。

Method: 提出概率包含深度（PID）模型，支持模糊轮廓和传统轮廓的集成可视化；采用均值概率轮廓进行高效近似计算，并设计GPU并行算法大幅降低计算时间。

Result: PID方法能够计算概率掩模集成、多种网格类型和大规模3D集成的轮廓箱线图，计算效率提升了一个数量级。

Conclusion: PID方法通过引入概率包含操作符和高效近似计算，成功支持了模糊轮廓和传统二进制轮廓的集成可视化，并在合成数据集和真实数据集上验证了其有效性。

Abstract: We propose Probabilistic Inclusion Depth (PID) for the ensemble visualization of scalar fields. By introducing a probabilistic inclusion operator $\subset_{\!p}$, our method is a general data depth model supporting ensembles of fuzzy contours, such as soft masks from modern segmentation methods, and conventional ensembles of binary contours. We also advocate to extend contour extraction in scalar field ensembles to become a fuzzy decision by considering the probabilistic distribution of an isovalue to encode the sensitivity information. To reduce the complexity of the data depth computation, an efficient approximation using the mean probabilistic contour is devised. Furthermore, an order of magnitude reduction in computational time is achieved with an efficient parallel algorithm on the GPU. Our new method enables the computation of contour boxplots for ensembles of probabilistic masks, ensembles defined on various types of grids, and large 3D ensembles that are not studied by existing methods. The effectiveness of our method is evaluated with numerical comparisons to existing techniques on synthetic datasets, through examples of real-world ensemble datasets, and expert feedback.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [109] [Label-consistent clustering for evolving data](https://arxiv.org/abs/2512.15210)
*Ameet Gadekar,Aristides Gionis,Thibault Marette*

Main category: cs.DS

TL;DR: 本文研究了标签一致k中心问题，提出了两种常数因子近似算法，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 在数据分析的迭代过程中，需要确保解决方案在更新时既能保持高质量，又能最小化与先前解决方案的剧烈变化，以实现平滑演进。

Method: 提出了两种常数因子近似算法，用于解决标签一致k中心问题。

Result: 理论分析表明所提算法具有常数因子近似性能，实验评估验证了其在真实数据集上的有效性。

Conclusion: 本文提出了两种针对标签一致k中心问题的常数因子近似算法，并通过实验验证了其在真实数据集上的有效性。

Abstract: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.
  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.

</details>


### [110] [A Constant-Factor Approximation for Directed Latency](https://arxiv.org/abs/2512.15473)
*Jannis Blauth,Ramin Mousavi*

Main category: cs.DS

TL;DR: 本文通过新分桶方法和增强LP松弛，首次在多项式时间内解决了Directed Latency问题的常数因子近似。


<details>
  <summary>Details</summary>
Motivation: Directed Latency问题在非对称度量下的近似算法研究存在显著空白，现有最佳近似因子为O(log n)且运行时间较长。本文旨在填补这一空白，提出多项式时间的常数因子近似算法。

Method: 引入了一种全新的分桶策略，并通过限制LP的可行性区域来增强标准LP松弛。随后提出了针对该LP分数解的舍入算法。

Result: 首次在多项式时间内实现了Directed Latency问题的常数因子近似。

Conclusion: 本文提出了一种新的分桶方法，通过增强标准线性规划（LP）松弛并减少猜测的激进性，首次在多项式时间内实现了Directed Latency问题的常数因子近似。

Abstract: In the Directed Latency problem, we are given an asymmetric metric on a set of vertices (or clients), and a given depot $s$. We seek a path $P$ starting at $s$ and visiting all the clients so as to minimize the sum of client waiting times (also known as latency) before being visited on the path.
  In contrast to the symmetric version of this problem (also known as the Deliveryperson problem and the Repairperson problem in the literature), there are significant gaps in our understanding of Directed Latency. The best approximation factor has remained at $O(\log n)$, where $n$ is the number of clients, for more than a decade [Friggstad, Salavatipour, and Svitkina, '13]. Only recently, [Friggstad and Swamy, '22] presented a constant-factor approximation but in quasi-polynomial time. Both results follow similar ideas: they consider buckets with geometrically-increasing distances, build paths in each bucket, and then stitch together all these paths to get a feasible solution. [Friggstad and Swamy, '22] showed if we guess a vertex from each bucket and augment a standard LP relaxation with these guesses, then one can reduce the stitching cost. Unfortunately, there are logarithmically many buckets so the running time of their algorithm is quasi-polynomial.
  In this paper, we present the first constant-factor approximation for Directed Latency in polynomial time by introducing a completely new way of bucketing which helps us strengthen a standard LP relaxation with less aggressive guessing. Although the resulting LP is no longer a relaxation of Directed Latency, it still admits a good solution. We present a rounding algorithm for fractional solutions of our LP, crucially exploiting the way we restricted the feasibility region of the LP formulation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 论文通过向量符号架构（VSA）视角分析Transformer，提出改进架构和训练目标，以增强逻辑可靠性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在需要稳定符号操作的任务上表现脆弱，需要一种统一的理论视角来解释其推理行为。

Method: 通过将自注意力和残差流解释为近似的向量符号架构（VSA），提出VSA启发的架构偏置和训练目标。

Result: 提出了一种代数视角，将Transformer内部机制与思维链、程序推理等联系起来，并解释了典型失败模式。

Conclusion: 该论文认为，将注意力机制视为软向量符号计算，为构建更可解释和逻辑可靠的推理系统提供了一条有原则的路径。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [112] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 提出了一种针对不完整知识图谱的问答基准构建方法，并开发了GR-Agent，其在完整和不完整设置下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答基准假设知识图谱完整，忽视了现实中知识图谱不完整的情况，导致评估过于依赖浅层检索而忽略推理能力。

Method: 提出了一种构建知识图谱不完整性基准的方法，并开发了自适应图推理代理（GR-Agent），该代理通过构建交互环境并利用图推理工具进行问答。

Result: 实验表明，现有方法在不完整知识图谱下性能下降，而GR-Agent在两种设置下均表现优异。

Conclusion: GR-Agent在完整和不完整的知识图谱设置下均表现出色，与非训练基线相比具有优势，并与基于训练的方法表现相当。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [113] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 通过结构化知识注入技术，LLM生成Terraform IaC的技术正确性显著提升，但意图对齐仍受限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成正确且意图对齐的基础设施即代码（IaC）方面成功率较低，本研究旨在通过系统注入结构化配置知识来改进这一现状。

Method: 从基础的Naive RAG逐步升级到更复杂的Graph RAG方法，包括语义增强和图组件间的依赖关系建模。

Result: 实验结果显示，注入结构化配置知识后，技术验证成功率提升至75.3%，整体成功率提升至62.6%。

Conclusion: 尽管知识注入技术显著提高了LLM生成IaC的技术正确性（从27.1%提升至62.6%），但在意图对齐方面仍存在瓶颈，揭示了‘正确性-一致性鸿沟’。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [114] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个多代理AI系统，专为农业气候适应决策设计，通过模块化架构和实时数据整合，提供实用且包容的解决方案，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 农业地区面临气候相关风险的损害，需要适应性风险管理和决策支持。代理式AI（特别是多代理框架）能解决复杂动态任务，但现有系统多为单代理或静态功能，缺乏动态协作推理和情境感知能力。

Method: AgroAskAI采用模块化、角色专业化的架构，通过责任链方法协调自主代理，整合实时工具和数据集，并内置治理机制以减少幻觉和提供内部反馈。

Result: 实验表明，通过额外工具和提示优化，AgroAskAI在农业气候适应查询中能生成更实用、接地气和包容的决策支持输出。

Conclusion: AgroAskAI展示了代理式AI在农业气候适应决策支持中的潜力，提供了更具可操作性、接地气和包容性的输出。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [115] [Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation](https://arxiv.org/abs/2512.15033)
*Xidan Song,Weiqi Wang,Ruifeng Cao,Qingya Hu*

Main category: cs.AI

TL;DR: 论文提出几何稳定性框架评估LLMs在象棋领域的推理能力，发现高准确性未必代表稳健理解，揭示了模型对模式匹配的依赖与几何推理的不足。


<details>
  <summary>Details</summary>
Motivation: 标准准确性指标无法区分真实的几何推理与对标准棋盘状态的表面记忆，需要新的评估方法来测试模型的稳健性。

Method: 提出几何稳定性框架（Geometric Stability Framework），通过棋盘旋转、镜像对称、颜色反转和格式转换等不变性变换，严格测试模型的一致性。

Result: 发现准确性-稳定性悖论（Accuracy-Stability Paradox），部分模型在标准位置上表现优异但在几何扰动下表现崩溃，而另一些模型则展现出双重稳健性。

Conclusion: 几何稳定性为AI评估提供了一个正交且关键的指标，能够有效区分推理能力与数据污染及过拟合问题。

Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

</details>


### [116] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: 本文探讨了代理AI在ISAC系统中的应用价值和前景，提出了一个基于GenAI的代理ISAC框架，并通过案例验证其性能优势，同时明确了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无线环境日益动态和复杂，ISAC系统需要更智能的处理和更自主的操作以保持效率和适应性。代理AI通过支持动态环境中的连续感知-推理-行动循环，为解决这些挑战提供了可行方案。

Method: 首先综述了代理AI和ISAC系统的关键特性，然后展示了ISAC系统的常见优化方法，并提出了一个基于生成人工智能（GenAI）的代理ISAC框架。

Result: 通过案例研究验证了所提出的代理ISAC框架在优化ISAC性能方面的优越性。

Conclusion: 本文总结了代理人工智能（AI）在集成传感与通信（ISAC）系统中的潜在应用价值，并提出了未来研究方向。

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [117] [LADY: Linear Attention for Autonomous Driving Efficiency without Transformers](https://arxiv.org/abs/2512.15038)
*Jihao Huang,Xi Xia,Zhiyuan Li,Tianle Liu,Jingke Wang,Junbo Chen,Tengju Ye*

Main category: cs.AI

TL;DR: LADY是一种基于线性注意力的端到端自动驾驶生成模型，解决了Transformer二次成本问题，支持高效跨模态交互，并在边缘设备上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构因二次注意力成本难以高效建模长时空序列，而线性注意力机制缺乏跨模态和跨时间交互支持，限制了自动驾驶部署。

Method: 提出LADY模型，采用完全基于线性注意力的生成模型，支持跨模态和跨时间交互，并引入轻量级线性交叉注意力机制。

Result: 在NAVSIM和Bench2Drive基准测试中，LADY实现了最先进性能，计算和内存成本恒定，且在实际边缘设备上验证了可行性。

Conclusion: LADY模型通过线性注意力机制在自动驾驶任务中实现了高效的时空建模，并在资源受限的边缘设备上验证了其实用性。

Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.

</details>


### [118] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER框架通过动态策略选择和外部工具辅助，显著提升LLM推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略难以平衡效率和准确性，尤其是面对不同难度查询时。

Method: CogER首先评估查询复杂度并分配至预定义层级，采用马尔可夫决策过程建模并通过强化学习训练CogER-Agent。

Result: CogER在领域内和领域外任务中分别实现了13%和8%的相对提升。

Conclusion: CogER框架通过动态选择最适合的推理策略，显著提升了大型语言模型在不同难度查询中的效率和准确性。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [119] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 通过聚类框架优化变量排序，减少计算开销，并在MWISP中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 动态变量排序虽能有效提升对偶边界紧密度，但全局应用时计算开销较大，因此需一种平衡效率与质量的解决方案。

Method: 论文引入了两种策略：Cluster-to-Cluster（按集群顺序处理）和Pick-and-Sort（迭代选择并排序代表性变量），并结合理论分析设定了集群数量的策略。

Result: 在MWISP基准测试中，所提方法相比标准动态变量排序基线，显著降低了计算成本。

Conclusion: 该论文提出了一种基于聚类的变量排序框架，显著降低了动态变量排序的搜索空间，并在MWISP基准实例中验证了其计算效率的提升。

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [120] [CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications](https://arxiv.org/abs/2512.15231)
*Zhengchao Chen,Haoran Wang,Jing Yao,Pedram Ghamisi,Jun Zhou,Peter M. Atkinson,Bing Zhang*

Main category: cs.AI

TL;DR: CangLing-KnowFlow是一个统一智能代理框架，整合专家知识库和动态学习模块，显著提升遥感数据处理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化系统缺乏统一框架、难以管理多样化端到端工作流的问题。

Method: 该框架结合了过程知识库（PKB）、动态工作流调整和进化记忆模块，通过专家验证的工作流案例和实时学习优化性能。

Result: 在KnowFlow-Bench基准测试中，CangLing-KnowFlow在所有复杂任务中的任务成功率比Reflexion基线至少高出4%。

Conclusion: CangLing-KnowFlow框架通过整合专家知识、动态工作流调整和进化记忆模块，为复杂地球观测任务提供了高效、可扩展的自动化解决方案，显著提升了任务成功率。

Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).

</details>


### [121] [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)
*Toshihide Ubukata,Enhong Mu,Takuto Yamauchi,Mingyue Zhang,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: GCRL通过GNNs增强RL方法，在多数领域提升控制器合成效率，但对称性高的领域例外。


<details>
  <summary>Details</summary>
Motivation: 现有的控制器合成方法依赖于固定规则或仅考虑有限当前特征的强化学习策略，限制了效率。

Method: GCRL利用图神经网络（GNNs）将LTS探索的历史编码为图结构，以捕获更广泛的非当前上下文。

Result: 在五个基准领域中的四个，GCRL表现出优于现有方法的学习效率和泛化能力，但在高度对称和严格局部交互的特定领域表现不佳。

Conclusion: GCRL通过集成图神经网络（GNNs）提升了基于强化学习（RL）的控制器合成方法，在大多数基准领域中表现出更高的学习效率和泛化能力，但在高度对称和严格局部交互的特定领域中表现不佳。

Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.

</details>


### [122] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 研究通过分析LLMs在科学推理中的表现，揭示了其认知弱点（如感知错误），为设计抗AI问题提供了依据，以维护评估公平性。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的普及，学术诚信和评估有效性受到挑战。本研究旨在揭示AI在多模态科学推理中的认知弱点，为教育者提供设计抗AI问题的依据。

Method: 研究利用2025年韩国高考地球科学I部分，设计了三种实验条件（全页输入、单题输入和优化多模态输入），定量和定性评估了GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro的表现。

Result: 定量结果显示，非结构化输入导致性能显著下降；定性分析揭示了‘感知-认知鸿沟’、‘计算-概念化差异’和‘过程幻觉’等认知缺陷。

Conclusion: 本研究通过分析大型语言模型（LLMs）在多模态科学推理中的认知局限性，为设计‘抗AI问题’提供了实用建议，以区分学生真实能力与AI生成答案，确保评估公平性。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [123] [SCOPE: Prompt Evolution for Enhancing Agent Effectiveness](https://arxiv.org/abs/2512.15374)
*Zehua Pei,Hui-Ling Zhen,Shixiong Kai,Sinno Jialin Pan,Yunhe Wang,Mingxuan Yuan,Bei Yu*

Main category: cs.AI

TL;DR: SCOPE通过自动优化提示机制，显著提升LLM代理在动态上下文中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理因静态提示无法有效管理动态上下文而导致的纠正和增强失败问题。

Method: 提出了双流机制（Dual-Stream）和视角驱动探索（Perspective-Driven Exploration），将上下文管理建模为在线优化问题。

Result: 在HLE基准测试中，任务成功率从14.23%提升至38.64%。

Conclusion: SCOPE通过自动优化提示机制，显著提升了LLM代理在动态上下文环境中的任务成功率，无需人工干预。

Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.

</details>


### [124] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 本文通过定性空间关系增强LLM的行人导航能力。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在行人导航中的实用性和准确性。

Method: 利用定性空间关系改进大型语言模型（LLM）。

Result: 改进后的LLM能更有效地为行人提供路线指引。

Conclusion: 通过定性空间关系增强大型语言模型（LLM）为行人提供路线指引的能力是可行的。

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [125] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: The paper presents an AI-driven framework that enhances early decision-making in card games by combining human and AI-generated game statistics, validated in Skat.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical early stages of multi-player card games (e.g., bidding, game selection) where human-derived statistical data is limited, by leveraging AI to enhance prediction accuracy and decision-making.

Method: The method involves deriving a general bootstrapping outer-learning framework that expands the database of human expert games with AI-generated games, using perfect feature hash functions to manage compacted tables and enable continuous self-improvement.

Result: The results show that the framework improves prediction accuracy by merging statistics from human and AI-generated games, with the case study in Skat validating its applicability to various game decisions.

Conclusion: The paper concludes that the automated approach, utilizing a bootstrapping outer-learning framework and perfect feature hash functions, effectively supports various decisions in card games like Skat, demonstrating the potential of AI-enhanced statistical methods in early game stages.

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [126] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 结合ASP和MILP的框架，解决UAM中资源受限的动态调度问题，支持模糊用户输入。


<details>
  <summary>Details</summary>
Motivation: 由于资源受限，高效调度在UAM领域备受关注，需处理动态操作需求和模糊的重新调度请求。

Method: 使用混合整数线性规划（MILP）和三值逻辑解释模糊用户意图，结合决策树和答案集编程（ASP）。

Result: 提出的系统优化了调度并透明支持人工输入。

Conclusion: 本文提出了一个结合ASP和MILP的集成框架，为可解释、自适应的UAM调度提供了稳健结构。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [127] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个大规模数学推理数据集，结合AoPS和StackExchange-Math问题，通过工具集成推理和高效训练策略，显著提升了数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在多样性、长形式轨迹和工具集成方面存在局限，需要更高质量的数据集来支持数学推理任务。

Method: 利用gpt-oss-120b的多模式生成能力，创建包含7.5M解决方案轨迹的Nemotron-Math数据集，结合85K AoPS问题和262K StackExchange-Math问题，采用顺序分桶策略加速长上下文微调。

Result: Nemotron-Math在匹配的AoPS问题上表现优于OpenMathReasoning，StackExchange-Math的加入提高了鲁棒性和泛化能力，同时在AIME 2024和2025上实现了100% maj@16准确率。

Conclusion: Nemotron-Math数据集通过整合多样化的数学问题和工具集成推理，显著提升了数学推理任务的性能，支持高效的长上下文训练，并在多个数学竞赛基准上实现了最先进的性能。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [128] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: SDE框架评估LLMs在科学发现中的表现，发现当前模型存在性能差距和系统性弱点，但仍显示出在多种科学项目中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准主要测试去上下文化的知识，忽视了推动科学发现的迭代推理、假设生成和观察解释。

Method: 引入了一个基于场景的基准，评估LLMs在生物学、化学、材料和物理学中的表现，通过专家定义的研究项目分解为模块化研究场景，从中抽样问题。

Result: 应用SDE框架发现，当前顶级LLMs在科学发现项目上存在性能差距、规模扩展的收益递减以及跨提供商的系统性弱点。

Conclusion: SDE框架为LLMs的科学发现相关评估提供了可复现的基准，并为其在科学发现中的发展指明了实用路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [129] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 本文提出了一个决策理论框架，分析AI委托的价值对齐、准确性和覆盖范围之间的权衡，区分了通用和情境特定委托的条件。


<details>
  <summary>Details</summary>
Motivation: 探讨在不确定性下，如何确定不完全对齐是否足够合理以证明委托决策的正当性。

Method: 引入了一个正式的决策理论框架，精确分析价值对齐、认知准确性和覆盖范围之间的权衡。

Result: 分析揭示了两种委托情景的鲜明区别：通用委托需要近乎完美的价值对齐和完全的认知信任，而情境特定委托即使在显著不对齐的情况下也可能是最优的。

Conclusion: 本文提出了一个原则性的方法，用于确定在特定情境下AI是否足够对齐，将焦点从实现完美对齐转移到在不确定性下管理委托的风险和回报。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [130] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC是一个在单一模型中交替进行推理和自我批判的框架，通过混合强化学习优化推理和自我评估，实验显示其在数学推理中表现优异。


<details>
  <summary>Details</summary>
Motivation: 受人类批判性思维的启发，旨在解决现有大型语言模型(LLMs)将推理与验证分离的问题，前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。

Method: 提出了Stepwise Think-Critique (STC)框架，该框架在单一模型中交替进行推理和自我批判，采用混合强化学习目标结合推理奖励和批判一致性奖励来联合优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展示了强大的批判性思维能力，并产生了更易理解的推理轨迹。

Conclusion: STC框架通过将推理与自我批判结合在单一模型中，展示了强大的批判性思维能力，并产生了更易理解的推理轨迹，标志着向具备内置批判性思维的LLMs迈进了一步。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [131] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过属性图改进LLM的上下文归因，提升解释忠实性40%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理过程不透明，引发安全和信任问题，现有上下文归因方法忽略代际间影响，导致解释不完整。

Method: 引入Context Attribution via Graph Explanations (CAGE)框架，构建一个有向图（属性图），量化每个生成对提示和所有先前生成的影响。

Result: CAGE在多个模型、数据集、指标和方法中显著提升上下文归因的忠实性。

Conclusion: CAGE框架通过引入属性图，改进了上下文归因的忠实性，平均提升高达40%。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


### [132] [Artism: AI-Driven Dual-Engine System for Art Generation and Critique](https://arxiv.org/abs/2512.15710)
*Shuai Liu,Yiqing Tian,Yang Chen,Mar Canet Sola*

Main category: cs.AI

TL;DR: 论文提出双引擎AI架构（AIDA和Ismism Machine），通过深度学习和多智能体协作模拟艺术发展，探索智能交互式批判实践，为艺术计算分析提供新方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决探索艺术演变潜在轨迹这一复杂问题，探索从传统单向批判向智能、交互式反思实践的转变。

Method: 论文提出了一种双引擎AI架构方法，包含两个相互连接的组件：AIDA（人工艺术家社交网络）和Ismism Machine（批判分析系统），利用深度学习和多智能体协作实现艺术历史发展和概念创新模式的多维模拟。

Result: 目前已在当代艺术概念的实验研究中应用该方法。

Conclusion: 本研究提出了一种基于AI驱动批判循环的通用方法，为艺术的计算分析提供了新的可能性。

Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.

</details>


### [133] [Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants](https://arxiv.org/abs/2512.15712)
*Vincent Huang,Dami Choi,Daniel D. Johnson,Sarah Schwettmann,Jacob Steinhardt*

Main category: cs.AI

TL;DR: 提出了一种端到端训练的Predictive Concept Decoder（PCD），通过编码-解码架构压缩激活并回答问题，展示了良好的扩展性和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的可扩展解释性方法依赖手工设计的代理，难以处理激活空间的复杂性，因此提出通过端到端训练目标来解决这一问题。

Method: 通过编码器将激活压缩为稀疏概念列表，解码器读取该列表并回答关于模型的自然语言问题，实现了从激活到模型行为的端到端训练。

Result: PCDs在大型非结构化数据上预训练后，通过微调能够准确回答问题，其瓶颈概念的自动解释评分随数据增加而提升，下游应用性能也相应提高。

Conclusion: PCDs（Predictive Concept Decoders）展示了良好的扩展性，能够有效检测越狱、秘密提示和植入的潜在概念，并准确揭示潜在用户属性。

Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [134] [Reexamining Paradigms of End-to-End Data Movement](https://arxiv.org/abs/2512.15028)
*Chin Fang,Timothy Stitt,Michael J. McManus,Toshio Moriya*

Main category: cs.DC

TL;DR: 本文探讨了高性能数据传输中的六个常见范式，指出网络带宽并非唯一关键因素，通过硬件-软件协同设计确保一致性能，缩小了基准测试与生产环境的差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示高性能数据传输中，网络带宽并非唯一关键因素，而是需要综合考虑网络延迟、TCP拥塞控制算法、主机端CPU性能和虚拟化等多方面因素。

Method: 通过一个支持延迟仿真的高速广域网性能预测测试平台，以及从资源受限的边缘环境到连接瑞士和美国加利福尼亚的100 Gbps运营链路的广泛生产测量，验证了研究结果。

Result: 结果表明，主要瓶颈往往不在网络核心，而是存在于边缘到核心的整个频谱中。

Conclusion: 本文提出了一种硬件-软件协同设计的整体方法，确保在1 Gbps到100 Gbps及以上速度的数据传输中保持一致的性能，有效缩小了基准测试结果与多样化复杂生产环境之间的差距。

Abstract: The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.

</details>


### [135] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ 是一个针对消费级 GPU 优化的语言模型训练框架，支持高效训练中等规模模型，性能接近云级 GPU。


<details>
  <summary>Details</summary>
Motivation: 解决消费级 GPU 内存有限且通信速度慢的问题，使其能够高效训练中等规模的语言模型。

Method: 采用了激活检查点、卸载和基于复制引擎的集合操作等优化技术，针对内存和通信瓶颈进行优化。

Result: LLMQ 能够在单张 16GB 中端游戏显卡上训练 7B 模型，或在配备 4 张 RTX 4090 的工作站上训练 32B 模型，同时保持约 50% 的 FLOP 利用率。

Conclusion: LLMQ 是一个高效的端到端 CUDA/C++ 实现，能够在中等规模的消费级 GPU 上训练 3B 至 32B 参数的语言模型，其性能媲美更昂贵的云级 GPU 生产系统。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [136] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 本文优化了GPU上的Bloom过滤器设计，通过向量化、线程协作和计算延迟优化，显著提升性能，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: GPU因其大规模线程级并行性和高带宽内存，适合加速Bloom过滤器变体，但相关设计尚未充分探索。本文旨在填补这一空白。

Method: 通过探索GPU设计空间的三个维度（向量化、线程协作和计算延迟），并结合硬件响应分析，提出了一种优化的GPU Bloom过滤器实现。

Result: 优化设计在B200 GPU上实现了比现有技术高11.35倍（查找）和15.4倍（构建）的性能，同时保持高精度，达到实际速度极限的92%以上。

Conclusion: 本文提出了一种优化的GPU Bloom过滤器设计，克服了速度与精度之间的传统权衡，实现了高吞吐量同时保持高精度，性能显著优于现有技术。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [137] [LeaseGuard: Raft Leases Done Right](https://arxiv.org/abs/2512.15659)
*A. Jesse Jiryu Davis,Murat Demirbas,Lingzhi Deng*

Main category: cs.DC

TL;DR: LeaseGuard是一种基于Raft的新型租约算法，通过优化显著提升分布式数据库的读写性能和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有Raft系统中的租约协议模糊且影响可用性，导致多数系统实现不正确或完全不使用。

Method: LeaseGuard算法基于Raft选举的保证，通过TLA+严格指定，并包含两项优化：快速恢复写入吞吐量和提高读取可用性。

Result: LeaseGuard在Python模拟和LogCabin实现中，将一致读取的网络往返次数从1降为0，写入吞吐量从约1000提升至约10,000次/秒，新领导者可立即允许99%的读取成功。

Conclusion: LeaseGuard通过Raft选举的特定保证，提供了一种简单且严格指定的租约算法，显著提高了读写性能和可用性。

Abstract: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.

</details>


### [138] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX是一种动态重批处理的EE LLM推理系统，通过优化批处理和调度，提升吞吐量并保证输出质量。


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合EE LLMs，无法灵活处理不同请求的退出时机，导致吞吐量下降或输出质量受损。

Method: 提出了Dynamic Rebatching解决方案，包括动态重组批次、无拷贝重批处理缓冲区和EE/SLA感知调度器，以及高效处理跳过的层的KV缓存。

Result: DREX相比基线方法提升了2-12%的吞吐量，且完全消除了非自愿退出。

Conclusion: DREX通过动态重批处理和优化调度，显著提升了EE LLMs的推理吞吐量，同时完全避免了非自愿退出，确保了输出质量。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [139] [UAV-enabled Computing Power Networks: Task Completion Probability Analysis](https://arxiv.org/abs/2512.15173)
*Yiqin Deng,Zhengru Fang,Senkang Hu,Yanan Ma,Haixia Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: 论文提出无人机驱动的计算能力网络（UAV-CPNs），通过动态调整无人机位置优化计算节点访问，理论分析和数值结果表明其能有效缓解传统网络的通信瓶颈和“孤岛效应”。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统计算能力网络中的通信瓶颈和多接入边缘计算中的“孤岛效应”，论文提出通过无人机动态定位增强计算能力分布和节点可访问性。

Method: 利用随机过程和随机几何理论，推导出任务完成概率的解析表达式，作为评估UAV-CPNs在指定端到端延迟要求下完成地面用户任务能力的主要性能指标。

Result: 数值结果表明，通信和计算能力之间的微妙平衡对提升UAV-CPNs性能至关重要，且计算节点的广泛分布带来了显著的性能提升。

Conclusion: 论文提出了一种通过无人机定位动态调整计算节点可访问性的创新框架，即无人机驱动的计算能力网络（UAV-CPNs），并通过理论分析和数值结果验证了其在缓解传统计算网络通信瓶颈和克服多接入边缘计算中的“孤岛效应”方面的潜力。

Abstract: This paper presents an innovative framework that synergistically enhances computing performance through ubiquitous computing power distribution and dynamic computing node accessibility control via adaptive unmanned aerial vehicle (UAV) positioning, establishing UAV-enabled Computing Power Networks (UAV-CPNs). In UAV-CPNs, UAVs function as dynamic aerial relays, outsourcing tasks generated in the request zone to an expanded service zone, consisting of a diverse range of computing devices, from vehicles with onboard computational capabilities and edge servers to dedicated computing nodes. This approach has the potential to alleviate communication bottlenecks in traditional computing power networks and overcome the "island effect" observed in multi-access edge computing. However, how to quantify the network performance under the complex spatio-temporal dynamics of both communication and computing power is a significant challenge, which introduces intricacies beyond those found in conventional networks. To address this, in this paper, we introduce task completion probability as the primary performance metric for evaluating the ability of UAV-CPNs to complete ground users' tasks within specified end-to-end latency requirements. Utilizing theories from stochastic processes and stochastic geometry, we derive analytical expressions that facilitate the assessment of this metric. Our numerical results emphasize that striking a delicate balance between communication and computational capabilities is essential for enhancing the performance of UAV-CPNs. Moreover, our findings show significant performance gains from the widespread distribution of computing nodes.

</details>


### [140] [More Capacity from Less Spectrum: Tapping into Optical-layer Intelligence in Optical Computing-Communication Integrated Network](https://arxiv.org/abs/2512.15190)
*Dao Thanh Hai,Shuo Li,Isaac Woungang*

Main category: cs.NI

TL;DR: 本文提出光计算-通信集成网络架构，通过光学层智能实现光路径尺度的计算与通信双重服务，性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 近年来光计算和全光信号处理技术的巨大投资与进展，推动了在光层同时提供计算与通信服务的需求。

Method: 引入光学层智能概念，提出光计算-通信集成网络架构，并通过数学建模优化设计，以NSFNET拓扑为例与传统光旁路模型进行性能对比。

Result: 在NSFNET拓扑上的性能对比显示，光计算-通信集成网络在频谱和计算效率上优于传统光旁路模型。

Conclusion: 本文提出了一种新型的光计算-通信集成网络架构，通过光学层智能概念实现光路径尺度的计算与通信双重服务，显著提升了频谱和计算效率。

Abstract: Driven by massive investments and consequently significant progresses in optical computing and all-optical signal processing technologies lately, this paper presents a new architectural paradigm for next-generation optical transport network, entitled \textit{optical computing-communication integrated network}, which is capable of providing dual services at the optical layer, namely, computing and communication. This approach seeks to exploit the potential for performing optical computing operations among lightpaths that traverse the same intermediate node. \textit{Optical-layer intelligence concept} is thus introduced as the capability to perform computing / processing at the lightpath scale to achieve greater spectral and/or computing efficiency. A case study focusing on optical aggregation operation is introduced, highlighting the key differences between optical computing-communication integrated network and its current counterpart, optical-bypass ones. A mathematical formulation for optimal designs of optical-aggregation-enabled network is then provided and performance comparison with traditional optical-bypass model is drawn on the realistic NSFNET topology.

</details>


### [141] [DNS-based dynamic context resolution for SCHC](https://arxiv.org/abs/2512.15217)
*Antoine Bernard,Sandoche Balakrichenan,Michel Marot,Benoit Ampeau*

Main category: cs.NI

TL;DR: 本文提出了一种基于DNS的动态获取LPWAN终端设备上下文规则的机制，实验验证了其可行性，但会引入额外通信延迟。


<details>
  <summary>Details</summary>
Motivation: LPWAN（低功耗广域网）的无线电资源和有效载荷大小有限，LoRaWAN提供了一种开放、易于部署的长距离网络解决方案。为了高效使用IPv6通信，需要动态获取终端设备的上下文规则。

Method: 利用DNS系统动态检索终端设备的上下文规则，并通过HTTP服务器下载这些规则，实验测量了通信延迟。

Result: 实验测量表明，提出的机制在真实测试平台上增加了通信延迟，但实现了动态获取上下文规则的目标。

Conclusion: 本文提出了一种基于DNS的机制，用于动态查找与终端设备相关的上下文规则，并通过HTTP服务器下载这些规则，从而优化LPWAN中的IPv6通信效率。

Abstract: LPWANs are networks characterised by the scarcity of their radio resources and their limited payload size. LoRaWAN offers an open, easy-to-deploy and efficient solution to operate a long-range network. To efficiently communicate using IPv6, the LPWAN working group from the IETF developed a solution called Static Context Header Compression (SCHC). It uses context rules, which are linked to a given End Device, to compress the IPv6 and UDP header. Since there may be a huge variety of End Devices profile, it makes sense to store the rules remotely and use a system to retrieve the profiles dynamically. In this paper we propose a mechanism based on DNS to find the context rules associated with an End Device, allowing it to be downloaded from an HTTP Server. We evaluate the corresponding delay added to the communications using experimental measurements from a real testbed.

</details>


### [142] [Packet-Level Traffic Modeling with Heavy-Tailed Payload and Inter-Arrival Distributions for Digital Twins](https://arxiv.org/abs/2512.15432)
*Enes Koktas,Peter Rost*

Main category: cs.NI

TL;DR: 提出了一种高效的混合流量生成器，适用于数字孪生，性能优于现有方法且参数极少。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要紧凑且易于重新校准的包级流量生成器，以重现数据包的大小和时间特征。

Method: 采用隐马尔可夫模型捕捉缓冲、流媒体和空闲状态，结合混合密度网络建模负载长度和到达间隔时间的联合分布，使用Student-t混合分布处理重尾特性。

Result: 在多个公共流量数据集上，该生成器在多数情况下最接近真实流量，且参数数量显著减少（仅约0.2 MB）。

Conclusion: 该论文提出了一种混合生成器，结合了小型隐马尔可夫模型和混合密度网络，能够高效生成符合真实流量特征的包级流量，适用于数字孪生环境。

Abstract: Digital twins of radio access networks require packet-level traffic generators that reproduce the size and timing of packets while remaining compact and easy to recalibrate as traffic changes. We address this need with a hybrid generator that combines a small hidden Markov model, which captures buffering, streaming, and idle states, with a mixture density network that models the joint distribution of payload length and inter-arrival time (IAT) in each state using Student-t mixtures. The state space and emission family are designed to handle heavy-tailed IAT by anchoring an explicit idle state in the tail and allowing each component to adapt its tail thickness. We evaluate the model on public traces of web, smart home, and encrypted media traffic and compare it with recent neural network and transformer based generators as well as hidden Markov baselines. Across most datasets and metrics, including average per-flow cumulative distribution functions, autocorrelation based measures of temporal structure, and Wasserstein distances between flow descriptors, the proposed generator matches the real traffic most closely in the majority of cases while using orders of magnitude fewer parameters. The full model occupies around 0.2 MB in our experiments, which makes it suitable for deployment inside digital twins where memory footprint and low-overhead adaptation are critical.

</details>


### [143] [GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication](https://arxiv.org/abs/2512.15481)
*Shavbo Salehi,Pedro Enrique Iturria-Rivera,Medhat Elsayed,Majid Bavand,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: PENME是一种新型视频语义通信方法，通过智能运动估计和选择性细化，大幅降低资源消耗并提升传输效率。


<details>
  <summary>Details</summary>
Motivation: 传统Shannon范式的语义通信在视频传输中存在高延迟、高比特率和功耗问题，PENME旨在通过语义通信减少资源消耗并保持质量。

Method: 提出了一种可预测性感知和熵自适应的神经运动估计方法（PENME），通过五步策略选择最佳运动提取模型，并结合选择性扩散细化（LCM-4）和资源块分配优化。

Result: 在Vimeo90K数据集上，PENME相比传统方法降低了40%延迟、减少了90%传输数据，提升了35%吞吐量，并在PSNR、MS-SSIM和LPIPS指标上显著优于基线方法。

Conclusion: PENME方法通过结合可预测性感知和熵自适应策略，显著提升了视频语义通信的效率，降低了延迟和资源消耗，同时保持了高语义相似度。

Abstract: Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [144] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 研究比较了十个生态系统的依赖放大模式，发现Maven放大倍数最高，建议针对不同生态系统采取特定安全策略。


<details>
  <summary>Details</summary>
Motivation: 比较不同生态系统中依赖放大的模式，以了解其对软件供应链安全的影响。

Method: 对十个主要生态系统中的500个项目进行实证研究，分析其依赖放大模式。

Result: Maven的平均放大倍数最高（24.70倍），而CocoaPods最低（0.32倍）。不同生态系统之间存在显著差异，表明生态系统设计选择（如依赖解析行为、标准库完整性等）是关键因素。

Conclusion: 研究发现不同生态系统的依赖放大模式存在显著差异，建议根据生态系统特性采取特定的安全策略。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [145] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 本研究提出VDMN图形化表示法，系统化指导VDT建模，并通过案例和专家访谈验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管价值驱动树（VDTs）在管理决策中的应用日益增多，但缺乏系统化的建模指南。

Method: 研究开发了Value Driver Modelling Notation（VDMN），并通过两个案例研究和专家访谈评估其实用性。

Result: VDMN支持一致且易于理解的VDT建模，验证了其实用性。

Conclusion: VDMN作为一种图形化表示法，为价值驱动树（VDTs）的建模提供了系统化和标准化的方法，显著提升了建模的一致性和可理解性。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [146] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 研究指出当前LLM在细微差别提示下的可靠性不足，提出新指标和评估方法，性能下降显著，强调这是未来改进的关键方向。


<details>
  <summary>Details</summary>
Motivation: 尽管高级LLM在基准测试如IFEval上表现出色，但其在现实世界中的可靠性仍受用户表达方式、上下文框架和任务表述变化的影响。

Method: 引入新指标reliable@k，开发自动化流水线通过数据增强生成高质量相似提示，并构建IFEval++进行系统评估。

Result: 评估了20个专有和26个开源LLM，发现它们在细微差别导向的可靠性方面表现不足，性能下降显著。

Conclusion: 当前的大型语言模型在细微差别导向的可靠性方面存在显著不足，性能可能因提示的细微修改而下降高达61.8%。研究强调了这一可靠性作为实现更可靠和可信赖LLM行为的关键下一步。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [147] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: Survey of 68 developers reveals a need for automated privacy compliance tools, with experienced developers showing higher concern for such tools.


<details>
  <summary>Details</summary>
Motivation: To understand emerging developer needs in privacy law compliance, which can guide the development of automation tools like Generative AI to streamline the compliance process.

Method: A survey was conducted with the participation of 68 developers to examine their needs regarding privacy law compliance.

Result: Most developers expressed a need for more automated tools, and privacy experience was found to increase concerns for privacy tools.

Conclusion: The study highlights the urgent need for privacy facilitators and automated tools to aid developers in privacy law compliance, emphasizing that privacy experience heightens concerns for such tools.

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [148] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: CAPE通过将需求转化为可执行规范并训练模型默认满足它们，显著降低违规率和成本，同时提升部署可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统缺乏表达和执行需求的方式，导致智能模型在部署中频繁失败，尽管基准测试表现优异。

Method: 通过CAPE协议实现‘Specify -> Verify -> Correct -> Train’循环，结合上下文客观性和验证-保真度扩展两大实证发现。

Result: 在六个领域的109,500个示例中，CAPE相对于DPO将违规率降低了81%（标准差小于0.3%），成本降低5至20倍，时间线从数月缩短至数周。

Conclusion: CAPE协议、PredicateGraph模式、CPL规范语言和政策包的发布，以及CapabilityBench的推出，标志着从智能基准测试向能力测量的转变。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [149] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 代理式方法在MATLAB到HDL的语法修复中表现更优，尤其在中小型模型上效果显著。


<details>
  <summary>Details</summary>
Motivation: 将高级语言（如MATLAB）算法转换为硬件描述语言（HDL）是一个资源密集型但必要的步骤，而大型语言模型（LLMs）的有限训练使得端到端转换容易出错。

Method: 比较了两种LLM驱动的方法：一种是结构化的专家设计流程，另一种是使用模型上下文协议（MCP）的动态自主代理方法。

Result: 代理方法在解决初始语法错误方面更有效，尤其是在中小型模型上，仿真成功率提高了20个百分点以上。

Conclusion: 代理式框架在适当设计时，能有效补偿中小型模型的能力限制。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [150] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: AI驱动的系统研究（ADRS）通过生成、评估和优化循环，在多项案例中展示了超越人类设计的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI正通过自动化新解决方案的发现改变研究流程，而系统性能研究尤其适合这种范式，因其天然具备验证候选方案的能力。

Method: 通过多个开源ADRS实例（如OpenEvolve、GEPA和ShinkaEvolve）进行十项案例研究，展示ADRS生成方案的效果。

Result: ADRS生成的解决方案在多项案例中匹配甚至超越人类设计的先进方案。

Conclusion: 尽管尚未找到适用于所有系统研究的通用ADRS应用方法，但初步研究结果与挑战为未来研究方向提供了有价值的指导。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [151] [Industry Expectations and Skill Demands in Quantum Software Testing](https://arxiv.org/abs/2512.14861)
*Ronnie de Souza Santos,Teresa Baldassarre,Cesar França*

Main category: cs.SE

TL;DR: 量子软件测试结合传统QA与实验验证，需跨学科技能，行业仍处早期发展阶段。


<details>
  <summary>Details</summary>
Motivation: 研究量子软件行业如何定义测试角色以及这些职位对专业人士的技能期望。

Method: 分析了来自量子软件和硬件开发组织的110个职位发布，识别了与测试相关的活动、能力和技能要求。

Result: 发现量子环境中的测试结合了传统软件质量保证与实验验证，强调校准、控制和混合量子-经典验证。雇主寻求具备编程与自动化专业知识、量子特定技术知识及跨学科协作技能的专业人士。

Conclusion: 量子软件测试仍处于早期但快速发展的阶段，连接了软件工程和实验物理学，强调了教育与研究需与工业实践保持一致。

Abstract: Quantum software testing introduces new challenges that differ fundamentally from those in classical software engineering. Aims: This study investigates how the quantum software industry defines testing roles and what skills are expected from professionals in these positions. Method: We analyzed 110 job postings from organizations involved in quantum software and hardware development, identifying activities, competencies, and skill requirements related to testing. Results: The findings show that testing in quantum contexts combines traditional software quality assurance with experimental validation, emphasizing calibration, control, and hybrid quantum-classical verification. Employers seek professionals who integrate programming and automation expertise with quantum-specific technical knowledge and interdisciplinary collaboration skills. Conclusions: Quantum software testing remains at an early but rapidly evolving stage that bridges software engineering and experimental physics, highlighting the need for educational and research efforts that align testing practices with industrial realities.

</details>


### [152] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: RE2-Bench是一个包含1,101个推理问题的基准，用于更真实地评估LLMs的代码推理能力，结果显示现有评估高估了LLMs的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅涉及简单程序，无法反映真实世界的复杂性，导致对LLMs在实际中泛化能力的假设存在重大威胁。

Method: 利用静态和动态程序分析自动序列化和反序列化真实世界代码中的复合、复杂和自定义类型，并基于九种可解释的代码复杂度指标通过多数投票机制将问题分类为Easy或Hard。

Result: 在RE2-Bench上对六种通用和面向推理的LLMs进行评估，结果显示从Easy到Hard问题的性能显著下降（输入预测下降51.50%，输出预测下降42.15%）。

Conclusion: RE2-Bench揭示了现有评估方法显著高估了LLMs的推理能力，尤其是在处理复杂代码时的表现。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [153] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: RepGen是一种自动化工具，通过LLM和迭代机制显著提升DL错误再现率，减少开发者负担。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型的固有非确定性和与硬件、软件环境的紧密耦合，手动方法只能可靠地再现约3%的DL错误，因此需要一种更高效的自动化方法。

Method: RepGen通过构建学习增强的上下文、制定全面的错误再现计划，并采用迭代的生成-验证-优化机制，利用LLM生成能够再现错误的代码。

Result: 在106个真实世界的DL错误上，RepGen实现了80.19%的再现率，比现有技术提高了19.81%。开发者研究表明，RepGen将错误再现的成功率提高了23.35%，时间减少了56.8%，并降低了认知负担。

Conclusion: RepGen是一种自动化且智能的方法，显著提高了深度学习（DL）错误的再现率，并减少了开发者的认知负担和时间成本。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [154] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 研究提出了一种基于LLM的两步提示框架，能高效预测GitHub对话失控，支持开源社区的主动管理。


<details>
  <summary>Details</summary>
Motivation: 开源社区中的有害互动会降低贡献者参与度并威胁项目可持续性，需要更高效且可扩展的主动管理方法。

Method: 提出了一种基于大型语言模型的两步提示框架：首先生成对话动态摘要（SCDs），然后利用这些摘要评估失控可能性。

Result: 在Qwen和Llama模型上，该方法在决策阈值为0.3时分别达到F1分数0.901和0.852，优于现有NLP基线。外部验证数据集上F1分数最高达0.797。

Conclusion: 研究发现，结构化的大型语言模型提示策略能有效早期检测开源软件中的对话失控，支持主动且可解释的社区管理。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [155] [An Exploratory Study of Bayesian Prompt Optimization for Test-Driven Code Generation with Large Language Models](https://arxiv.org/abs/2512.15076)
*Shlok Tomar,Aryan Deshwal,Ethan Villalovoz,Mattia Fazzini,Haipeng Cai,Janardhan Rao Doppa*

Main category: cs.SE

TL;DR: BODE-GEN通过贝叶斯优化在嵌入空间搜索最优提示，提升LLM代码生成准确性，实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）生成功能正确代码时提示选择对准确性的影响问题。

Method: 提出BODE-GEN方法，利用贝叶斯优化在连续嵌入空间中进行自适应数据驱动搜索，结合随机投影和维度缩放先验构建高斯过程代理模型。

Result: 在HumanEval+基准测试中，BODE-GEN相比固定提示和手动提示工程，显著提高了代码生成准确性，且样本效率高。

Conclusion: BODE-GEN通过贝叶斯优化在连续嵌入空间中高效搜索最优提示，显著提升了代码生成的准确性，且具有较高的样本效率。

Abstract: We consider the task of generating functionally correct code using large language models (LLMs). The correctness of generated code is influenced by the prompt used to query the given base LLM. We formulate the problem of finding the appropriate prompt as combinatorial search process and propose a Bayesian optimization (BO) approach referred to as {\em BO for Code GENeration (BODE-GEN)}. BODE-GEN performs an adaptive data-driven search over prompts guided by training data in the form of prompts tried and the functional accuracy of the generated code over a set of given test cases. The key insight is to perform BO in continuous embedding space by using an auxiliary LLM to bridge the gap between discrete prompt space and continuous embedding space. We leverage two synergistic ideas, namely, random projections and dimensionality scaled priors, to build effective Gaussian process based surrogate models over the high-dimensional embedding space. Our experiments on the HumanEval+ benchmark using multiple base LLMs show that BODE-GEN can improve performance in terms of code generation accuracy compared to fixed prompts and manual prompt engineering. Additionally, we demonstrate that BODE-GEN is sample-efficient, requiring relatively few iterations of BO to demonstrate improvements in code accuracy.

</details>


### [156] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 研究发现学术与工业需求存在差距，提出了七个关键启示以指导未来研究更具工业影响力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展正在从根本上重塑软件工程（SE），但学术进展与工业需求的契合度尚不明确。

Method: 首先对2022至2025年间在FSE、ASE和ICSE上发表的1,367篇论文进行了系统分析，随后对17个组织进行了实证调查，收集了282份关于六个主要主题的反馈。

Result: 通过对比学术能力与工业反馈，得出了七个关键启示，突出了软件需求和架构、智能SE方法的可靠性和可解释性等方面的未充分解决的挑战。

Conclusion: 本研究旨在重新聚焦学术界对这些重要但未被充分探索的问题的关注，并指导未来的软件工程研究朝着更大的工业影响力方向发展。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [157] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: 本文提出BDTransTest工具，用于自动化验证BPMN+DMN流程的语义正确性，解决了现有工具仅能检测语法错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前BPMN+DMN模型验证工具仅能检测语法错误，无法识别语义错误，导致设计师需手动执行流程以检测故障。

Method: 设计了BDTransTest工具，提供从BPMN+DMN流程到Java程序的翻译、测试计划的合成与执行，以及覆盖率的分析。

Result: BDTransTest工具能够有效翻译BPMN+DMN流程为Java程序，并生成测试计划，提高了流程验证的自动化程度和覆盖率。

Conclusion: 本文通过设计BDTransTest工具，解决了BPMN+DMN流程验证中的语义错误检测问题，并通过实验验证了其有效性。

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [158] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 提出一种异构模型对齐框架，结合自适应机制和LLM验证，解决数字孪生中多层级模型对齐的语义和同步问题，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多层级、模型驱动的数字孪生中，异构模型的对齐问题导致语义不匹配、不一致性和同步困难，现有静态映射方法不灵活且易出错。

Method: 采用自适应一致性机制连接元模型与演化模型，并引入LLM验证的对齐流程，确保元模型在领域知识中的结构保真和概念一致性。

Result: 通过空气质量用例和OAEI测试案例验证，该方法能自动发现语义对应关系，减少手动映射，提升跨模型类型的可扩展性。

Conclusion: 提出的异构模型对齐方法通过自适应一致性机制和LLM验证的流程，有效解决了多层级数字孪生中的语义不匹配和同步问题，提升了模型的自动化程度和可扩展性。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [159] [Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods](https://arxiv.org/abs/2512.15422)
*Ji Zhou,Yongqi Zhao,Yixian Hu,Hexuan Li,Zhengguo Gu,Nan Xu,Arno Eichberger*

Main category: cs.SE

TL;DR: 该综述系统分析了场景测试的最新进展，提出了改进分类法和实用工具，以解决现有挑战并支持ADS的安全部署。


<details>
  <summary>Details</summary>
Motivation: 解决传统验证方法（如大规模道路测试）成本高、耗时长的挑战，以及现有综述对最新方法和技术进展覆盖不全的问题。

Method: 系统分析了31项主要研究和10项综述，重点关注2023~2025年的最新框架，包括生成模型（如大语言模型、生成对抗网络、扩散模型和强化学习框架）的应用。

Result: 识别出三个持续存在的差距：缺乏标准化评估指标、伦理和人为因素整合不足、多模态和特定ODD场景覆盖不足。

Conclusion: 该综述提出了一个改进的分类法，包含多模态扩展、伦理和安全检查表以及ODD覆盖图，为研究人员提供了方法论的清晰性，并为行业提供了实践指导，支持可重复的评估并加速更高级别ADS的安全部署。

Abstract: Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.

</details>


### [160] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: Aletheia通过抄袭检测算法识别JavaScript捆绑包中的包版本，优于现有方法，并揭示捆绑包更新更快、漏洞更少，但少数供应商可能是更新的主要推动力。


<details>
  <summary>Details</summary>
Motivation: JavaScript生态系统中数百万包的下载统计显示高流行度的易受攻击版本，但生产网站上的实际流行度未知，现有检测机制无法全面分析现代Web应用的依赖更新行为。

Method: Aletheia，一种包无关的方法，通过源自抄袭检测领域的算法剖析JavaScript捆绑包以识别包版本。

Result: Aletheia在实用场景中明显优于现有方法；爬取Tranco前10万域名显示5%-20%的域名在16周内更新依赖，捆绑包更新速度显著快于CDN包含的包，漏洞版本减少多达10倍。

Conclusion: 尽管定量指标显示捆绑包更新速度更快且漏洞更少，但少数广泛供应商可能是及时更新的主要推动力，这表明单纯依赖数据可能无法全面反映实际情况。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [161] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 提出基于子模型的AAS架构，支持动态服务集成，通过案例验证其作为主动接口的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将AAS视为静态信息模型，缺乏对动态服务集成和系统适应的支持，因此探索将可执行行为（如容器化服务）集成到AAS中的潜力。

Method: 通过扩展子模型的行为定义，实现模块化事件驱动架构，能够基于嵌入的触发条件部署容器化服务。

Result: 通过3轴铣床的案例研究展示了该方法的有效性，使AAS成为主动执行增值服务的接口。

Conclusion: 该论文提出的基于子模型的架构使AAS不仅能作为被动的数字表示，还能作为执行增值服务的主动接口，为未来AI驱动的适应和系统级智能奠定了基础。

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [162] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 提出多主观排名方法评估ChatGPT在代码审查中的表现，结果显示其评论质量优于人类，同时警示未经检查集成的风险。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查生成评估方法要么依赖与单一标准答案的自动比较（无法捕捉人类观点的多样性），要么基于主观的‘有用性’评估（概念模糊），因此需要更有效的评估方法。

Method: 提出了一种基于多主观排名的评估方法，使用CodeReview StackExchange上的280个自包含代码审查请求及对应评论，由多位人类评委对ChatGPT生成的评论与平台上最佳人类回答进行质量排名。

Result: ChatGPT的评论在质量排名上显著优于人类评论，甚至超过了StackExchange的采纳答案。

Conclusion: ChatGPT生成的代码审查评论在质量上显著优于人类评论，甚至超过了StackExchange上的采纳答案。该方法不仅推动了对生成式AI在代码审查中性能的更有效评估，还提醒人们注意未经检查集成到审查流程中的潜在风险。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [163] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究发现语义等价代码转换（如变量重命名）能显著削弱MI检测，暴露了许可证合规漏洞，且组合转换无额外效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码领域的成功依赖于大量代码数据，但涉及知识产权合规和未经授权使用受限代码的问题。现有MI检测技术可能被语义等价代码转换技术削弱。

Method: 系统研究了语义等价代码转换规则是否可用于规避MI检测，包括变量重命名等规则，并通过因果分析验证了这些规则的影响。

Result: 模型准确率在最坏情况下仅下降1.5%，变量重命名规则使MI成功率降低10.19%，且组合多种转换不会进一步降低MI有效性。

Conclusion: 研究揭示了基于语义等价代码转换的技术可以显著削弱成员推理（MI）检测的有效性，尤其是变量重命名规则。这暴露了在训练大型代码语言模型时许可证合规性执行的关键漏洞。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [164] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: WuppieFuzz是一个自动化REST API模糊测试工具，通过覆盖率引导和自动化减少手动工作，评估显示其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为减少因暴露REST API端点带来的安全风险，需要高效的自动化测试技术。

Method: 利用OpenAPI规范生成初始输入语料库，结合REST特定和LibAFL提供的变异器，通过覆盖率引导选择请求序列以探索复杂状态。

Result: 在Petstore API上的评估表明白盒方法的鲁棒性及不同功率调度的有效性，同时监测了端点和代码覆盖率。

Conclusion: WuppieFuzz是一个基于LibAFL的开源REST API模糊测试工具，支持白盒、灰盒和黑盒测试，能有效减少手动工作并提高测试覆盖率。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


### [165] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 提出基于MLIR的编译器工具链，将Julia代码自动编译为SystemVerilog，解决FPGA开发中的双语言问题，实现高效硬件加速。


<details>
  <summary>Details</summary>
Motivation: 解决特定问题加速器开发中的“双语言问题”，即算法开发与内核实现语言不一致的问题。

Method: 提出了一种基于MLIR的编译器工具链，能够自动将Julia语言编写的内核编译为SystemVerilog，支持动态和静态调度，并与AXI4-Stream协议直接集成。

Result: 原型工具链能够合成一组信号处理/数学基准测试，在真实FPGA设备上以100MHz运行，吞吐量达到仅支持低级语言（如C/C++）的最先进工具链的59.71%至82.6%。

Conclusion: 该工具链允许领域专家以常规方式在Julia中编写计算内核，并将其重定向到FPGA而无需额外的编译指示或修改，实现了与现有最先进工具链相当的吞吐量。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>
