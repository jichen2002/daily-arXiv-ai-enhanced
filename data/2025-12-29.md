<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation](https://arxiv.org/abs/2512.21402)
*Arnav Gupta,Gurekas Singh Sahney,Hardik Rathi,Abhishek Chandwani,Ishaan Gupta,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 提出基于VLMs的短视频评估框架，通过无监督特征提取和回归预测参与度，实验证明其优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架（如VideoScore-2）仅关注视觉和语义保真度，无法捕捉具体视听属性如何驱动真实观众参与。

Method: 使用VLMs提取无监督视听特征，聚类为可解释因素，并训练回归评估器预测短视频的观众参与度。

Result: 实验显示预测参与度与实际参与度强相关，验证了该评估器的有效性和优越性。

Conclusion: 该研究提出了一种基于视觉语言模型（VLMs）的数据驱动评估框架，通过无监督提取视听特征并聚类为可解释因素，训练回归评估器预测短视频的观众参与度。实验证明其预测与实际参与度高度相关，相比传统指标（如SSIM、FID），该轻量级评估器更具可解释性和可扩展性。

Abstract: Evaluating short-form video content requires moving beyond surface-level quality metrics toward human-aligned, multimodal reasoning. While existing frameworks like VideoScore-2 assess visual and semantic fidelity, they do not capture how specific audiovisual attributes drive real audience engagement. In this work, we propose a data-driven evaluation framework that uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features, clusters them into interpretable factors, and trains a regression-based evaluator to predict engagement on short-form edutainment videos. Our curated YouTube Shorts dataset enables systematic analysis of how VLM-derived features relate to human engagement behavior. Experiments show strong correlations between predicted and actual engagement, demonstrating that our lightweight, feature-based evaluator provides interpretable and scalable assessments compared to traditional metrics (e.g., SSIM, FID). By grounding evaluation in both multimodal feature importance and human-centered engagement signals, our approach advances toward robust and explainable video understanding.

</details>


### [2] [ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields](https://arxiv.org/abs/2512.21692)
*Albert Barreiro,Roger Marí,Rafael Redondo,Gloria Haro,Carles Bosch*

Main category: cs.CV

TL;DR: ShinyNeRF是一种新型框架，能有效处理各向同性和各向异性反射，通过vMF分布近似出射辐射，实现高精度的3D数字化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确建模各向异性镜面表面（如刷金属），因此需要一种能同时处理各向同性和各向异性反射的新框架。

Method: 通过将出射辐射近似为各向同性von Mises-Fisher（vMF）分布的编码混合，联合估计表面法线、切线、镜面反射浓度和各向异性球面高斯（ASG）分布的各向异性幅度。

Result: 实验结果表明，ShinyNeRF在数字化各向异性镜面反射方面表现优异，优于现有方法。

Conclusion: ShinyNeRF在数字化各向异性镜面反射方面表现出色，不仅达到了最先进的性能，还提供了合理的物理解释和材料属性编辑能力。

Abstract: Recent advances in digitization technologies have transformed the preservation and dissemination of cultural heritage. In this vein, Neural Radiance Fields (NeRF) have emerged as a leading technology for 3D digitization, delivering representations with exceptional realism. However, existing methods struggle to accurately model anisotropic specular surfaces, typically observed, for example, on brushed metals. In this work, we introduce ShinyNeRF, a novel framework capable of handling both isotropic and anisotropic reflections. Our method is capable of jointly estimating surface normals, tangents, specular concentration, and anisotropy magnitudes of an Anisotropic Spherical Gaussian (ASG) distribution, by learning an approximation of the outgoing radiance as an encoded mixture of isotropic von Mises-Fisher (vMF) distributions. Experimental results show that ShinyNeRF not only achieves state-of-the-art performance on digitizing anisotropic specular reflections, but also offers plausible physical interpretations and editing of material properties compared to existing methods.

</details>


### [3] [A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding](https://arxiv.org/abs/2512.21414)
*Christina Liu,Alan Q. Wang,Joy Hsu,Jiajun Wu,Ehsan Adeli*

Main category: cs.CV

TL;DR: TBF框架通过神经网络融合工具输出，显著提升医学图像理解的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于文本的工具组合方法在医学图像理解中表现不佳的问题，因医学图像的关键信息通常为空间局部特征，难以通过文本有效融合。

Method: 提出Tool Bottleneck Framework (TBF)，通过Tool Bottleneck Model (TBM)以神经网络方式融合工具输出，替代传统的文本组合方法。

Result: 在组织病理学和皮肤病学任务中，TBF表现优于或与现有方法相当，尤其在数据有限时优势明显。

Conclusion: TBF框架在医学图像理解任务中表现优异，尤其在数据有限的情况下，其性能与深度学习分类器、VLMs及现有工具使用框架相当或更优。

Abstract: Recent tool-use frameworks powered by vision-language models (VLMs) improve image understanding by grounding model predictions with specialized tools. Broadly, these frameworks leverage VLMs and a pre-specified toolbox to decompose the prediction task into multiple tool calls (often deep learning models) which are composed to make a prediction. The dominant approach to composing tools is using text, via function calls embedded in VLM-generated code or natural language. However, these methods often perform poorly on medical image understanding, where salient information is encoded as spatially-localized features that are difficult to compose or fuse via text alone. To address this, we propose a tool-use framework for medical image understanding called the Tool Bottleneck Framework (TBF), which composes VLM-selected tools using a learned Tool Bottleneck Model (TBM). For a given image and task, TBF leverages an off-the-shelf medical VLM to select tools from a toolbox that each extract clinically-relevant features. Instead of text-based composition, these tools are composed by the TBM, which computes and fuses the tool outputs using a neural network before outputting the final prediction. We propose a simple and effective strategy for TBMs to make predictions with any arbitrary VLM tool selection. Overall, our framework not only improves tool-use in medical imaging contexts, but also yields more interpretable, clinically-grounded predictors. We evaluate TBF on tasks in histopathology and dermatology and find that these advantages enable our framework to perform on par with or better than deep learning-based classifiers, VLMs, and state-of-the-art tool-use frameworks, with particular gains in data-limited regimes. Our code is available at https://github.com/christinaliu2020/tool-bottleneck-framework.

</details>


### [4] [Scalable Deep Subspace Clustering Network](https://arxiv.org/abs/2512.21434)
*Nairouz Mrabah,Mohamed Bouguessa,Sihem Sami*

Main category: cs.CV

TL;DR: SDSNet是一种可扩展的深度子空间聚类框架，通过地标近似和联合优化，实现了O(n)复杂度，同时保持高聚类质量。


<details>
  <summary>Details</summary>
Motivation: 现有的子空间聚类方法由于构建完整亲和矩阵和进行谱分解的O(n^3)成本，存在可扩展性限制。深度学习方法虽改进了特征提取，但仍保持这一计算瓶颈。

Method: SDSNet采用基于地标的近似方法避免完整亲和矩阵，联合优化自编码器重建与自表达目标，并在因子化表示上直接进行谱聚类。

Result: 实验结果表明，SDSNet在显著提高计算效率的同时，达到了与最先进方法相当的聚类质量。

Conclusion: SDSNet通过结合卷积自编码器与子空间保持约束，显著提高了计算效率，同时保持了与最先进方法相当的聚类质量。

Abstract: Subspace clustering methods face inherent scalability limits due to the $O(n^3)$ cost (with $n$ denoting the number of data samples) of constructing full $n\times n$ affinities and performing spectral decomposition. While deep learning-based approaches improve feature extraction, they maintain this computational bottleneck through exhaustive pairwise similarity computations. We propose SDSNet (Scalable Deep Subspace Network), a deep subspace clustering framework that achieves $\mathcal{O}(n)$ complexity through (1) landmark-based approximation, avoiding full affinity matrices, (2) joint optimization of auto-encoder reconstruction with self-expression objectives, and (3) direct spectral clustering on factorized representations. The framework combines convolutional auto-encoders with subspace-preserving constraints. Experimental results demonstrate that SDSNet achieves comparable clustering quality to state-of-the-art methods with significantly improved computational efficiency.

</details>


### [5] [Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism](https://arxiv.org/abs/2512.21452)
*Haotian Lv,Yuhui Zhang,Jiangbo Dai,Hanli Wu,Jiaji Wang,Dawei Wang*

Main category: cs.CV

TL;DR: 该论文提出MCGA-Net框架，通过数据增强和多模态注意力网络提升GPR缺陷检测的自动化水平和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统GPR图像解释依赖主观经验，导致效率低下和准确性不足，亟需自动化解决方案。

Method: 提出了一个综合框架：(1) 使用DCGAN进行数据增强；(2) 设计MCGA-Net网络，结合MCFF和GAM；(3) 采用MS COCO迁移学习优化主干网络。

Result: MCGA-Net在精度（92.8%）、召回率（92.5%）和mAP@50（95.9%）上表现优异，且在噪声、弱信号和小目标检测中保持鲁棒性。

Conclusion: 该研究为基于GPR的自动化缺陷检测建立了新范式，在复杂地下环境中平衡了计算效率与高准确性。

Abstract: Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.

</details>


### [6] [CCAD: Compressed Global Feature Conditioned Anomaly Detection](https://arxiv.org/abs/2512.21459)
*Xiao Jin,Liang Diao,Qixin Xiao,Yifan Hu,Ziqi Zhang,Yuchen Liu,Haisong Gu*

Main category: cs.CV

TL;DR: CCAD结合全局特征与重构模型，提升异常检测性能与效率，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决无监督表示方法在域偏移下特征提取不足和重构方法训练效率低下的问题。

Method: 提出了一种名为CCAD的新方法，通过将全局特征作为重构模型的新模态条件，并设计了自适应压缩机制。

Result: CCAD在AUC指标上优于现有方法，并实现了更快的收敛速度。

Conclusion: CCAD通过结合全局特征和重构模型，显著提升了异常检测的性能和训练效率，并在实验中验证了其优越性。

Abstract: Anomaly detection holds considerable industrial significance, especially in scenarios with limited anomalous data. Currently, reconstruction-based and unsupervised representation-based approaches are the primary focus. However, unsupervised representation-based methods struggle to extract robust features under domain shift, whereas reconstruction-based methods often suffer from low training efficiency and performance degradation due to insufficient constraints. To address these challenges, we propose a novel method named Compressed Global Feature Conditioned Anomaly Detection (CCAD). CCAD synergizes the strengths of both paradigms by adapting global features as a new modality condition for the reconstruction model. Furthermore, we design an adaptive compression mechanism to enhance both generalization and training efficiency. Extensive experiments demonstrate that CCAD consistently outperforms state-of-the-art methods in terms of AUC while achieving faster convergence. In addition, we contribute a reorganized and re-annotated version of the DAGM 2007 dataset with new annotations to further validate our method's effectiveness. The code for reproducing main results is available at https://github.com/chloeqxq/CCAD.

</details>


### [7] [IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset](https://arxiv.org/abs/2512.21472)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: ISIC MultiAnnot++是最大的公开多标注者皮肤病变分割数据集，包含14,967张图像和17,684个分割掩码，支持标注者偏好和元数据分析。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模公开多标注者皮肤病变分割数据集，限制了相关研究的进展。

Method: 收集并整理来自ISIC Archive的14,967张皮肤镜图像，包含17,684个分割掩码，其中2,394张图像有2-5个标注。

Result: 创建了ISIC MultiAnnot++数据集，包含14,967张图像和17,684个分割掩码，标注者技能水平和分割工具等元数据丰富。

Conclusion: ISIC MultiAnnot++是目前最大的公开多标注者皮肤病变分割数据集，为研究标注者偏好建模和元数据分析提供了丰富资源。

Abstract: Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.

</details>


### [8] [GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification](https://arxiv.org/abs/2512.21476)
*Suncheng Xiang,Xiaoyang Wang,Junjie Jiang,Hejia Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 提出Gated Progressive Fusion网络，通过门控多级特征融合提升结肠镜息肉再识别性能，实验显示其优于现有单模态模型。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉再识别在计算机辅助诊断中至关重要，但现有方法因高层特征分辨率不足导致小物体识别效果不佳。

Method: 提出了一种名为Gated Progressive Fusion network的新架构，通过门控机制选择性融合多级特征，并采用逐步融合策略实现语义信息的逐层细化。

Result: 在标准基准测试中，多模态设置结合专用融合策略显著优于现有单模态ReID模型。

Conclusion: 论文提出的Gated Progressive Fusion网络在结肠镜息肉再识别任务中表现优异，尤其在多模态融合策略下显著优于现有单模态ReID模型。

Abstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, the coarse resolution of high-level features of a specific polyp often leads to inferior results for small objects where detailed information is important. To address this challenge, we propose a novel architecture, named Gated Progressive Fusion network, to selectively fuse features from multiple levels using gates in a fully connected way for polyp ReID. On the basis of it, a gated progressive fusion strategy is introduced to achieve layer-wise refinement of semantic information through multi-level feature interactions. Experiments on standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy.

</details>


### [9] [Generative Multi-Focus Image Fusion](https://arxiv.org/abs/2512.21495)
*Xinzhe Xie,Buyu Guo,Bolin Li,Shuangyan He,Yanzhen Gu,Qingyan Jiang,Peiliang Li*

Main category: cs.CV

TL;DR: GMFF提出两阶段（确定性融合+生成修复）多焦点图像融合框架，解决现有算法的焦点缺失和边缘伪影问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有融合算法假设每个空间位置至少有一张输入图像对其聚焦，且常因焦点估计不确定或硬选择操作产生边缘伪影。GMFF旨在解决这些限制。

Method: 第一阶段使用StackMFF V4进行确定性融合，第二阶段通过IFControlNet（基于潜在扩散模型）生成修复缺失焦点内容并消除边缘伪影。

Result: 实验表明GMFF在多焦点图像融合中达到最先进性能，尤其在复杂多焦点场景中表现优异。

Conclusion: GMFF通过两阶段（确定性融合和生成修复）实现了多焦点图像融合的最先进性能，并在复杂多焦点内容场景中展现出实际应用潜力。

Abstract: Multi-focus image fusion aims to generate an all-in-focus image from a sequence of partially focused input images. Existing fusion algorithms generally assume that, for every spatial location in the scene, there is at least one input image in which that location is in focus. Furthermore, current fusion models often suffer from edge artifacts caused by uncertain focus estimation or hard-selection operations in complex real-world scenarios. To address these limitations, we propose a generative multi-focus image fusion framework, termed GMFF, which operates in two sequential stages. In the first stage, deterministic fusion is implemented using StackMFF V4, the latest version of the StackMFF series, and integrates the available focal plane information to produce an initial fused image. The second stage, generative restoration, is realized through IFControlNet, which leverages the generative capabilities of latent diffusion models to reconstruct content from missing focal planes, restore fine details, and eliminate edge artifacts. Each stage is independently developed and functions seamlessly in a cascaded manner. Extensive experiments demonstrate that GMFF achieves state-of-the-art fusion performance and exhibits significant potential for practical applications, particularly in scenarios involving complex multi-focal content. The implementation is publicly available at https://github.com/Xinzhe99/StackMFF-Series.

</details>


### [10] [SVBench: Evaluation of Video Generation Models on Social Reasoning](https://arxiv.org/abs/2512.21507)
*Wenshuo Peng,Gongxuan Wang,Tianmeng Yang,Chuanhao Li,Xiaojie Xu,Hui He,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 当前文本到视频生成模型在社会一致行为生成方面存在显著不足，研究通过构建首个社会推理基准揭示了现代模型在深层社会推理任务上的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 当前模型在生成社会一致行为方面存在显著不足，无法像人类一样从简短视觉线索中推断意图、信念、情感和社会规范。

Method: 通过基于发育和社会心理学的研究成果，构建了首个视频生成中的社会推理基准，将三十个经典社会认知范式组织为七个核心维度，并开发了一个完全无需训练的基于代理的流程来操作这些范式。

Result: 研究结果显示，现代模型在表面合理性方面表现出色，但在意图识别、信念推理、共同注意和亲社会推理等方面系统性失败。

Conclusion: 当前文本到视频生成模型在视觉真实感、运动保真度和文本视频对齐方面取得了显著进展，但在生成社会一致行为方面仍存在根本性限制。

Abstract: Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.

</details>


### [11] [Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification](https://arxiv.org/abs/2512.21508)
*Md Ashik Khan,Md Nahid Siddique*

Main category: cs.CV

TL;DR: PET methods for chest X-Ray analysis outperform full fine-tuning with significantly fewer parameters, though calibration needs improvement for clinical use.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost of fine-tuning large vision-language models for multimodal chest X-Ray analysis.

Method: Parameter-efficient training (PET) strategies including frozen encoders, BitFit, LoRA, and adapters were tested for multi-label classification on chest X-Ray datasets.

Result: All PET variants achieved AUROC between 0.892 and 0.908 under a fixed parameter budget, outperforming full fine-tuning. External validation confirmed scalability, with Adapter achieving the best performance (0.7214 AUROC).

Conclusion: Frozen encoder strategies offer superior discrimination at significantly reduced computational cost, though calibration correction is crucial for clinical use.

Abstract: Multimodal chest X-Ray analysis often fine-tunes large vision-language models, which is computationally costly. We study parameter-efficient training (PET) strategies, including frozen encoders, BitFit, LoRA, and adapters for multi-label classification on the Indiana University Chest X-Ray dataset (3,851 image-report pairs; 579 test samples). To mitigate data leakage, we redact pathology terms from reports used as text inputs while retaining clinical context. Under a fixed parameter budget (2.37M parameters, 2.51% of total), all PET variants achieve AUROC between 0.892 and 0.908, outperforming full fine-tuning (0.770 AUROC), which uses 94.3M trainable parameters, a 40x reduction. External validation on CheXpert (224,316 images, 58x larger) confirms scalability: all PET methods achieve >0.69 AUROC with <9% trainable parameters, with Adapter achieving best performance (0.7214 AUROC). Budget-matched comparisons reveal that vision-only models (0.653 AUROC, 1.06M parameters) outperform budget-matched multimodal models (0.641 AUROC, 1.06M parameters), indicating improvements arise primarily from parameter allocation rather than cross-modal synergy. While PET methods show degraded calibration (ECE: 0.29-0.34) compared to simpler models (ECE: 0.049), this represents a tractable limitation addressable through post-hoc calibration methods. These findings demonstrate that frozen encoder strategies provide superior discrimination at substantially reduced computational cost, though calibration correction is essential for clinical deployment.

</details>


### [12] [Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art](https://arxiv.org/abs/2512.21512)
*Md Ashik Khan,Arafat Alam Jion*

Main category: cs.CV

TL;DR: 该论文提出固定阈值评估协议，揭示现有方法在鲁棒性估计上的误导，通过CNN-ViT混合模型展示不同场景下的性能差异，并提供部署指导。


<details>
  <summary>Details</summary>
Motivation: 现有的方法优化单一指标，未解决部署关键因素如操作点选择和固定阈值鲁棒性，导致鲁棒性估计误导。

Method: 引入了一个固定阈值评估协议，使用轻量级CNN-ViT混合模型，结合门控融合和可选频率增强，在三个操作点（低FPR、ROC最优、最佳F1）下进行系统退化测试。

Result: 评估揭示了一个统计验证的取证-语义谱：频率辅助CNN在原始照片上表现优异但在压缩下崩溃（93.33%至61.49%），而ViT通过鲁棒语义模式识别最小化退化（92.86%至88.36%）。多种子实验显示所有架构在艺术内容上的AUROC比逼真图像高15%（0.901-0.907 vs. 0.747-0.759）。混合方法在跨领域性能上表现平衡：tiny-genimage照片91.4%准确率，AiArtData艺术/图形89.7%，CIFAKE 98.3%。

Conclusion: 固定阈值评估协议消除了调整膨胀，揭示了真实的鲁棒性差距，并提供了可操作的部署指导：在干净照片验证中优先使用CNN，在压缩内容中使用ViT，在艺术/图形筛选中使用混合方法。

Abstract: AI image generators create both photorealistic images and stylized art, necessitating robust detectors that maintain performance under common post-processing transformations (JPEG compression, blur, downscaling). Existing methods optimize single metrics without addressing deployment-critical factors such as operating point selection and fixed-threshold robustness. This work addresses misleading robustness estimates by introducing a fixed-threshold evaluation protocol that holds decision thresholds, selected once on clean validation data, fixed across all post-processing transformations. Traditional methods retune thresholds per condition, artificially inflating robustness estimates and masking deployment failures. We report deployment-relevant performance at three operating points (Low-FPR, ROC-optimal, Best-F1) under systematic degradation testing using a lightweight CNN-ViT hybrid with gated fusion and optional frequency enhancement. Our evaluation exposes a statistically validated forensic-semantic spectrum: frequency-aided CNNs excel on pristine photos but collapse under compression (93.33% to 61.49%), whereas ViTs degrade minimally (92.86% to 88.36%) through robust semantic pattern recognition. Multi-seed experiments demonstrate that all architectures achieve 15% higher AUROC on artistic content (0.901-0.907) versus photorealistic images (0.747-0.759), confirming that semantic patterns provide fundamentally more reliable detection cues than forensic artifacts. Our hybrid approach achieves balanced cross-domain performance: 91.4% accuracy on tiny-genimage photos, 89.7% on AiArtData art/graphics, and 98.3% (competitive) on CIFAKE. Fixed-threshold evaluation eliminates retuning inflation, reveals genuine robustness gaps, and yields actionable deployment guidance: prefer CNNs for clean photo verification, ViTs for compressed content, and hybrids for art/graphics screening.

</details>


### [13] [MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions](https://arxiv.org/abs/2512.21513)
*Puyun Wang,Kaimin Yu,Huayang He,Xianyu Wu*

Main category: cs.CV

TL;DR: 构建了MuS-Polar3D数据集，支持偏振水下3D成像的多种任务，并提出两阶段重建方法，显著提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集在散射和观测条件多样性不足，阻碍了不同方法（如单视角和多视角偏振成像）的公平比较。

Method: 通过计算成像的视角，将水下散射条件下的3D重建解耦为两个阶段：去散射和3D重建。

Result: 在复杂散射条件下，使用多种基线方法进行广泛评估，最佳平均角度误差为15.49度。

Conclusion: MuS-Polar3D是首个公开的基于偏振的水下3D成像基准数据集，支持在可控散射条件下进行精确重建和公平算法评估。

Abstract: Polarization-based underwater 3D imaging exploits polarization cues to suppress background scattering, exhibiting distinct advantages in turbid water. Although data-driven polarization-based underwater 3D reconstruction methods show great potential, existing public datasets lack sufficient diversity in scattering and observation conditions, hindering fair comparisons among different approaches, including single-view and multi-view polarization imaging methods.
  To address this limitation, we construct MuS-Polar3D, a benchmark dataset comprising polarization images of 42 objects captured under seven quantitatively controlled scattering conditions and five viewpoints, together with high-precision 3D models (+/- 0.05 mm accuracy), normal maps, and foreground masks. The dataset supports multiple vision tasks, including normal estimation, object segmentation, descattering, and 3D reconstruction.
  Inspired by computational imaging, we further decouple underwater 3D reconstruction under scattering into a two-stage pipeline, namely descattering followed by 3D reconstruction, from an imaging-chain perspective. Extensive evaluations using multiple baseline methods under complex scattering conditions demonstrate the effectiveness of the proposed benchmark, achieving a best mean angular error of 15.49 degrees. To the best of our knowledge, MuS-Polar3D is the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, enabling accurate reconstruction and fair algorithm evaluation under controllable scattering conditions. The dataset and code are publicly available at https://github.com/WangPuyun/MuS-Polar3D.

</details>


### [14] [DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO](https://arxiv.org/abs/2512.21514)
*Henglin Liu,Huijuan Huang,Jing Wang,Chang Liu,Xiu Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出基于语义分组的分布创造力奖励和结构感知正则化方法，显著提升GRPO图像生成的多样性（13%-18%），解决了传统方法在训练后期输出同质化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO在训练后期倾向于产生同质化输出，缺乏创造力和视觉多样性，限制了其应用场景。这一问题可从奖励建模和生成动态两个角度分析：传统GRPO依赖单样本质量作为奖励信号，忽视了分布级多样性；常规GRPO正则化忽略了早期去噪在保持多样性中的主导作用。

Method: 在奖励层面，提出基于语义分组的分布创造力奖励，通过谱聚类构建分布级表示，并根据组大小自适应分配探索奖励；在生成层面，引入结构感知正则化，加强早期阶段的约束以保持多样性。

Result: 实验表明，本文方法在匹配质量分数下实现了13%-18%的语义多样性提升，为GRPO图像生成建立了新的质量-多样性权衡帕累托前沿。

Conclusion: 通过从奖励建模和生成动态两个角度重新审视多样性退化问题，并提出基于语义分组的分布创造力奖励和结构感知正则化方法，本文的方法在保持图像质量的同时显著提升了语义多样性（13%-18%），为基于GRPO的图像生成建立了新的质量-多样性帕累托前沿。

Abstract: Reinforcement learning (RL), particularly GRPO, improves image generation quality significantly by comparing the relative performance of images generated within the same group. However, in the later stages of training, the model tends to produce homogenized outputs, lacking creativity and visual diversity, which restricts its application scenarios. This issue can be analyzed from both reward modeling and generation dynamics perspectives. First, traditional GRPO relies on single-sample quality as the reward signal, driving the model to converge toward a few high-reward generation modes while neglecting distribution-level diversity. Second, conventional GRPO regularization neglects the dominant role of early-stage denoising in preserving diversity, causing a misaligned regularization budget that limits the achievable quality--diversity trade-off. Motivated by these insights, we revisit the diversity degradation problem from both reward modeling and generation dynamics. At the reward level, we propose a distributional creativity bonus based on semantic grouping. Specifically, we construct a distribution-level representation via spectral clustering over samples generated from the same caption, and adaptively allocate exploratory rewards according to group sizes to encourage the discovery of novel visual modes. At the generation level, we introduce a structure-aware regularization, which enforces stronger early-stage constraints to preserve diversity without compromising reward optimization efficiency. Experiments demonstrate that our method achieves a 13\%--18\% improvement in semantic diversity under matched quality scores, establishing a new Pareto frontier between image quality and diversity for GRPO-based image generation.

</details>


### [15] [Hierarchy-Aware Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2512.21529)
*Jiayu Li,Rajesh Gangireddy,Samet Akcay,Wei Cheng,Juhua Hu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的层次感知微调框架，通过结合两种损失函数和轻量级适应，显著提升了VLMs在层次分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在处理层次分类时，通常将标签视为扁平类别并进行全微调，这种方法成本高且预测结果在分类层次上不一致。

Method: 提出了一种层次感知的微调框架，结合TP-KL和HiSCE两种损失函数，并与轻量级LoRA适应集成。

Result: 在多个基准测试中，该方法在Full-Path Accuracy和Tree-based Inconsistency Error方面表现一致优于基线，且参数开销极小。

Conclusion: 本文提出了一种高效的层次感知微调框架，通过结合Tree-Path KL Divergence和Hierarchy-Sibling Smoothed Cross-Entropy两种目标，在共享嵌入空间中实现结构一致性，为适应结构化分类提供了一种高效策略。

Abstract: Vision-Language Models (VLMs) learn powerful multimodal representations through large-scale image-text pretraining, but adapting them to hierarchical classification is underexplored. Standard approaches treat labels as flat categories and require full fine-tuning, which is expensive and produces inconsistent predictions across taxonomy levels. We propose an efficient hierarchy-aware fine-tuning framework that updates a few parameters while enforcing structural consistency. We combine two objectives: Tree-Path KL Divergence (TP-KL) aligns predictions along the ground-truth label path for vertical coherence, while Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) encourages consistent predictions among sibling classes. Both losses work in the VLM's shared embedding space and integrate with lightweight LoRA adaptation. Experiments across multiple benchmarks show consistent improvements in Full-Path Accuracy and Tree-based Inconsistency Error with minimal parameter overhead. Our approach provides an efficient strategy for adapting VLMs to structured taxonomies.

</details>


### [16] [Vision Transformers are Circulant Attention Learners](https://arxiv.org/abs/2512.21542)
*Dongchen Han,Tianyu Li,Ziyi Wang,Gao Huang*

Main category: cs.CV

TL;DR: 提出 Circulant Attention，利用自注意力矩阵的 BCCB 特性实现高效计算（O(NlogN)），在保持模型容量的同时显著降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制在高分辨率场景下的计算复杂度问题，同时避免手工设计的局部性或稀疏性模式对模型容量的影响。

Method: 通过识别自注意力矩阵近似于 BCCB 矩阵的特性，提出一种显式建模和高效计算算法。

Result: 在多种视觉任务中验证了方法的有效性，计算复杂度降低至 O(NlogN)，同时保持了标准自注意力的性能。

Conclusion: Circulant Attention 是一种高效且保持模型容量的自注意力替代方案，适用于视觉 Transformer 架构。

Abstract: The self-attention mechanism has been a key factor in the advancement of vision Transformers. However, its quadratic complexity imposes a heavy computational burden in high-resolution scenarios, restricting the practical application. Previous methods attempt to mitigate this issue by introducing handcrafted patterns such as locality or sparsity, which inevitably compromise model capacity. In this paper, we present a novel attention paradigm termed \textbf{Circulant Attention} by exploiting the inherent efficient pattern of self-attention. Specifically, we first identify that the self-attention matrix in vision Transformers often approximates the Block Circulant matrix with Circulant Blocks (BCCB), a kind of structured matrix whose multiplication with other matrices can be performed in $\mathcal{O}(N\log N)$ time. Leveraging this interesting pattern, we explicitly model the attention map as its nearest BCCB matrix and propose an efficient computation algorithm for fast calculation. The resulting approach closely mirrors vanilla self-attention, differing only in its use of BCCB matrices. Since our design is inspired by the inherent efficient paradigm, it not only delivers $\mathcal{O}(N\log N)$ computation complexity, but also largely maintains the capacity of standard self-attention. Extensive experiments on diverse visual tasks demonstrate the effectiveness of our approach, establishing circulant attention as a promising alternative to self-attention for vision Transformer architectures. Code is available at https://github.com/LeapLabTHU/Circulant-Attention.

</details>


### [17] [EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal](https://arxiv.org/abs/2512.21545)
*Sanghyun Jo,Donghwan Lee,Eunji Jung,Seong Je Oh,Kyungsu Kim*

Main category: cs.CV

TL;DR: EraseLoRA 是一种无数据集框架，通过背景感知推理和测试时适应改进对象移除任务，优于现有无数据集方法并与依赖数据集的方法竞争。


<details>
  <summary>Details</summary>
Motivation: 现有的无数据集方法在对象移除任务中存在两个主要问题：非目标前景常被误判为背景导致不需要的对象重现，以及直接注意力操作破坏细节并阻碍背景线索的连贯整合。

Method: EraseLoRA 通过背景感知的前景排除（BFE）和多模态大语言模型分离目标前景、非目标前景和干净背景，再通过背景感知重建与子类型聚合（BRSA）进行测试时优化，实现背景子类型的互补整合。

Result: EraseLoRA 在对象移除任务中表现优于无数据集基线方法，并与依赖数据集的方法竞争，验证了其有效性。

Conclusion: EraseLoRA 作为一个插件，显著提升了预训练扩散模型在对象移除任务中的表现，不仅在无数据集基准上优于现有方法，还与依赖数据集的方法竞争。

Abstract: Object removal differs from common inpainting, since it must prevent the masked target from reappearing and reconstruct the occluded background with structural and contextual fidelity, rather than merely filling a hole plausibly. Recent dataset-free approaches that redirect self-attention inside the mask fail in two ways: non-target foregrounds are often misinterpreted as background, which regenerates unwanted objects, and direct attention manipulation disrupts fine details and hinders coherent integration of background cues. We propose EraseLoRA, a novel dataset-free framework that replaces attention surgery with background-aware reasoning and test-time adaptation. First, Background-aware Foreground Exclusion (BFE), uses a multimodal large-language models to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without paired supervision, producing reliable background cues while excluding distractors. Second, Background-aware Reconstruction with Subtype Aggregation (BRSA), performs test-time optimization that treats inferred background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives, preserving local detail and global structure without explicit attention intervention. We validate EraseLoRA as a plug-in to pretrained diffusion models and across benchmarks for object removal, demonstrating consistent improvements over dataset-free baselines and competitive results against dataset-driven methods. The code will be made available upon publication.

</details>


### [18] [Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration](https://arxiv.org/abs/2512.21560)
*Unnati Saraswat,Tarun Rao,Namah Gupta,Shweta Swami,Shikhar Sharma,Prateek Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 论文提出两个新任务（上下文感知对象插入和赞助商产品标志增强），构建数据集支持任务，结合VLMs和扩散模型提升智能图像编辑的上下文适应性。


<details>
  <summary>Details</summary>
Motivation: 现有工作很少确保插入的对象在上下文中是合适的，因此需要改进智能图像编辑的上下文适应性和准确性。

Method: 通过结合视觉语言模型（VLMs）和扩散模型，预测合适的对象类别、生成对象，并在场景中合理放置。同时检测产品并插入正确的品牌标志。

Result: 构建了两个带有类别注释、放置区域和赞助商产品标签的新数据集，以支持上下文感知对象插入和赞助商产品标志增强任务。

Conclusion: 该论文提出了两个新任务（上下文感知对象插入和赞助商产品标志增强），并构建了两个新数据集来支持这些任务，推动了智能图像编辑的发展。

Abstract: Intelligent image editing increasingly relies on advances in computer vision, multimodal reasoning, and generative modeling. While vision-language models (VLMs) and diffusion models enable guided visual manipulation, existing work rarely ensures that inserted objects are \emph{contextually appropriate}. We introduce two new tasks for advertising and digital media: (1) \emph{context-aware object insertion}, which requires predicting suitable object categories, generating them, and placing them plausibly within the scene; and (2) \emph{sponsor-product logo augmentation}, which involves detecting products and inserting correct brand logos, even when items are unbranded or incorrectly branded. To support these tasks, we build two new datasets with category annotations, placement regions, and sponsor-product labels.

</details>


### [19] [Exploration of Reproducible Generated Image Detection](https://arxiv.org/abs/2512.21562)
*Yihang Duan*

Main category: cs.CV

TL;DR: 研究通过复现实验揭示AIGC检测领域可复现性问题的根源，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 解决AIGC检测领域的两大核心问题：可复现性差和泛化能力不足。

Method: 回顾7篇AIGC检测关键论文，构建轻量级测试数据集，复现代表性检测方法。

Result: 严格遵循论文核心步骤可复现基本性能，但预处理破坏关键特征或跨生成器测试时性能显著下降。

Conclusion: 本研究为提升AIGC检测技术的可复现性提供了实证依据，并建议研究者更全面地披露实验细节及验证方法的泛化能力。

Abstract: While the technology for detecting AI-Generated Content (AIGC) images has advanced rapidly, the field still faces two core issues: poor reproducibility and insufficient gen eralizability, which hinder the practical application of such technologies. This study addresses these challenges by re viewing 7 key papers on AIGC detection, constructing a lightweight test dataset, and reproducing a representative detection method. Through this process, we identify the root causes of the reproducibility dilemma in the field: firstly, papers often omit implicit details such as prepro cessing steps and parameter settings; secondly, most detec tion methods overfit to exclusive features of specific gener ators rather than learning universal intrinsic features of AIGC images. Experimental results show that basic perfor mance can be reproduced when strictly following the core procedures described in the original papers. However, de tection performance drops sharply when preprocessing dis rupts key features or when testing across different genera tors. This research provides empirical evidence for improv ing the reproducibility of AIGC detection technologies and offers reference directions for researchers to disclose ex perimental details more comprehensively and verify the generalizability of their proposed methods.

</details>


### [20] [Towards Long-window Anchoring in Vision-Language Model Distillation](https://arxiv.org/abs/2512.21576)
*Haoyi Zhou,Shuo Li,Tianyu Chen,Qi Song,Chonghan Gao,Jianxin Li*

Main category: cs.CV

TL;DR: LAid enhances small VLMs' long-context understanding via distillation, extending context windows 3.2x without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of small VLMs in linguistics-photography alignment due to limited window size, leveraging knowledge distillation and RoPE.

Method: Proposes LAid with two components: progressive distance-weighted attention matching and learnable RoPE response gain modulation.

Result: Achieves up to 3.2 times longer effective context windows, with preserved or improved performance on VL benchmarks.

Conclusion: LAid-distilled models significantly extend effective context windows while maintaining performance, offering both practical techniques and theoretical insights into positional understanding transfer.

Abstract: While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students' capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.

</details>


### [21] [SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration](https://arxiv.org/abs/2512.21618)
*Zhiyuan Liu,Daocheng Fu,Pinlong Cai,Lening Wang,Ying Liu,Yilong Ren,Botian Shi,Jianqiang Wang*

Main category: cs.CV

TL;DR: SymDrive 是一种基于扩散的框架，通过对称自回归在线恢复和上下文感知修复，实现了高质量的3D渲染和场景编辑，解决了自动驾驶模拟中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以同时实现照片级真实感渲染和交互式交通编辑，尤其是在大角度新视角合成和资产操作时容易出现几何或光照伪影。

Method: 引入了对称自回归在线恢复范式，通过构建对称视图来恢复细粒度细节，并利用自回归策略生成一致的侧视图。此外，利用这种恢复能力实现无需训练的统一机制，将车辆插入视为上下文感知的修复，确保光照和阴影的一致性。

Result: 大量实验表明，SymDrive 在新视角增强和真实3D车辆插入方面均达到了最先进的性能。

Conclusion: SymDrive 提出了一种统一的基于扩散的框架，能够同时实现高质量的渲染和场景编辑，解决了自动驾驶领域3D模拟中高保真和可控性的挑战。

Abstract: High-fidelity and controllable 3D simulation is essential for addressing the long-tail data scarcity in Autonomous Driving (AD), yet existing methods struggle to simultaneously achieve photorealistic rendering and interactive traffic editing. Current approaches often falter in large-angle novel view synthesis and suffer from geometric or lighting artifacts during asset manipulation. To address these challenges, we propose SymDrive, a unified diffusion-based framework capable of joint high-quality rendering and scene editing. We introduce a Symmetric Auto-regressive Online Restoration paradigm, which constructs paired symmetric views to recover fine-grained details via a ground-truth-guided dual-view formulation and utilizes an auto-regressive strategy for consistent lateral view generation. Furthermore, we leverage this restoration capability to enable a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure seamless lighting and shadow consistency. Extensive experiments demonstrate that SymDrive achieves state-of-the-art performance in both novel-view enhancement and realistic 3D vehicle insertion.

</details>


### [22] [LLM-Free Image Captioning Evaluation in Reference-Flexible Settings](https://arxiv.org/abs/2512.21582)
*Shinnosuke Hirano,Yuiga Wada,Kazuki Matsuda,Seitaro Otsuki,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出Pearl，一种无LLM的图像字幕评估指标，通过新颖相似性学习机制和大型人类标注数据集，在多种数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的指标倾向于自身生成内容，中立性存疑；而无LLM指标虽无此问题，但性能不高。

Method: 提出了一种新颖的机制，学习图像-字幕和字幕-字幕相似性的表示，并构建了一个包含约333k人类标注的数据集。

Result: Pearl在Composite、Flickr8K-Expert、Flickr8K-CF、Nebula和FOIL数据集上均优于其他无LLM指标。

Conclusion: Pearl作为一种无LLM的监督式图像字幕评估指标，在参考和无参考设置下均表现出色，优于现有其他无LLM指标。

Abstract: We focus on the automatic evaluation of image captions in both reference-based and reference-free settings. Existing metrics based on large language models (LLMs) favor their own generations; therefore, the neutrality is in question. Most LLM-free metrics do not suffer from such an issue, whereas they do not always demonstrate high performance. To address these issues, we propose Pearl, an LLM-free supervised metric for image captioning, which is applicable to both reference-based and reference-free settings. We introduce a novel mechanism that learns the representations of image--caption and caption--caption similarities. Furthermore, we construct a human-annotated dataset for image captioning metrics, that comprises approximately 333k human judgments collected from 2,360 annotators across over 75k images. Pearl outperformed other existing LLM-free metrics on the Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings. Our project page is available at https://pearl.kinsta.page/.

</details>


### [23] [UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation](https://arxiv.org/abs/2512.21584)
*Linxuan Fan,Juntao Jiang,Weixuan Liu,Zhucun Xue,Jiajun Lv,Jiangning Zhang,Yong Liu*

Main category: cs.CV

TL;DR: UltraLBM-UNet 是一种轻量级 U-Net 变体，结合 Mamba 机制和多分支局部特征感知，在皮肤病变分割中实现高效且准确的结果，适用于临床部署。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病变分割方法存在性能低、计算复杂度高的问题，需要一种更高效且准确的解决方案。

Method: 提出 UltraLBM-UNet，结合双向 Mamba 全局建模和多分支局部特征感知，实现高效的局部特征注入和上下文交互。

Result: 在 ISIC 2017、ISIC 2018 和 PH2 数据集上表现优异，仅需 0.034M 参数和 0.060 GFLOPs，同时通过知识蒸馏训练的超紧凑模型 UltraLBM-UNet-T 也表现良好。

Conclusion: UltraLBM-UNet 是一种轻量级的 U-Net 变体，通过双向 Mamba 机制和多分支局部特征感知，实现了高效且准确的皮肤病变分割，适用于临床决策支持。

Abstract: Skin lesion segmentation is a crucial step in dermatology for guiding clinical decision-making. However, existing methods for accurate, robust, and resource-efficient lesion analysis have limitations, including low performance and high computational complexity. To address these limitations, we propose UltraLBM-UNet, a lightweight U-Net variant that integrates a bidirectional Mamba-based global modeling mechanism with multi-branch local feature perception. The proposed architecture integrates efficient local feature injection with bidirectional state-space modeling, enabling richer contextual interaction across spatial dimensions while maintaining computational compactness suitable for point-of-care deployment. Extensive experiments on the ISIC 2017, ISIC 2018, and PH2 datasets demonstrate that our model consistently achieves state-of-the-art segmentation accuracy, outperforming existing lightweight and Mamba counterparts with only 0.034M parameters and 0.060 GFLOPs. In addition, we introduce a hybrid knowledge distillation strategy to train an ultra-compact student model, where the distilled variant UltraLBM-UNet-T, with only 0.011M parameters and 0.019 GFLOPs, achieves competitive segmentation performance. These results highlight the suitability of UltraLBM-UNet for point-of-care deployment, where accurate and robust lesion analyses are essential. The source code is publicly available at https://github.com/LinLinLin-X/UltraLBM-UNet.

</details>


### [24] [From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https://arxiv.org/abs/2512.21598)
*Jian Lang,Rongpei Hong,Ting Zhong,Leiting Chen,Qiang Gao,Fan Zhou*

Main category: cs.CV

TL;DR: ALARM是一种无需标注的有害模因检测框架，通过显式模因识别和代理自改进机制，有效适应新型有害内容，性能超越传统标注驱动方法。


<details>
  <summary>Details</summary>
Motivation: 现有有害模因检测方法依赖大规模标注数据，不仅需要大量人工标注，且难以适应内容快速演变的特性。ALARM旨在解决这些问题，提供一种无需标注的自适应检测框架。

Method: ALARM通过基于置信度的显式模因识别机制从原始数据中分离显式模因并赋予伪标签，同时引入成对学习引导的代理自改进范式，利用显式模因构建对比对（正负样本）来优化学习代理。

Result: 在三个多样化数据集上的实验表明，ALARM不仅性能优越，且对新演变的模因表现出强适应性，甚至超越依赖标注的方法。

Conclusion: ALARM框架展示了无需标注数据的强大能力，能够有效适应新型有害模因的检测，甚至在性能上超越依赖标注数据的方法，为动态在线环境中的有害内容检测提供了可扩展的解决方案。

Abstract: The proliferation of harmful memes on online media poses significant risks to public health and stability. Existing detection methods heavily rely on large-scale labeled data for training, which necessitates substantial manual annotation efforts and limits their adaptability to the continually evolving nature of harmful content. To address these challenges, we present ALARM, the first lAbeL-free hARmful Meme detection framework powered by Large Multimodal Model (LMM) agent self-improvement. The core innovation of ALARM lies in exploiting the expressive information from "shallow" memes to iteratively enhance its ability to tackle more complex and subtle ones. ALARM consists of a novel Confidence-based Explicit Meme Identification mechanism that isolates the explicit memes from the original dataset and assigns them pseudo-labels. Besides, a new Pairwise Learning Guided Agent Self-Improvement paradigm is introduced, where the explicit memes are reorganized into contrastive pairs (positive vs. negative) to refine a learner LMM agent. This agent autonomously derives high-level detection cues from these pairs, which in turn empower the agent itself to handle complex and challenging memes effectively. Experiments on three diverse datasets demonstrate the superior performance and strong adaptability of ALARM to newly evolved memes. Notably, our method even outperforms label-driven methods. These results highlight the potential of label-free frameworks as a scalable and promising solution for adapting to novel forms and topics of harmful memes in dynamic online environments.

</details>


### [25] [GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians](https://arxiv.org/abs/2512.21599)
*Bintao He,Yiran Cheng,Hongjia Li,Xiang Gao,Xin Gao,Fa Zhang,Renmin Han*

Main category: cs.CV

TL;DR: GaussianEM 是一种基于高斯伪原子框架的方法，用于从冷冻电镜图像中同时建模组成和构象异质性，有效描述构象变化并连接密度与原子模型。


<details>
  <summary>Details</summary>
Motivation: 理解蛋白质灵活性及其与其他分子的动态相互作用对蛋白质功能研究至关重要，但分析包含连续运动和离散状态的数据集仍具挑战性。

Method: GaussianEM 采用双编码器-单解码器架构，将图像映射到其高斯组件，并通过高斯参数的变化表示结构变异性。

Result: GaussianEM 在模拟和实验数据集上均展示了其有效性。

Conclusion: GaussianEM 提供了一种直观且可解释的方法来描述蛋白质构象变化，并有效弥合了基于密度的模型与原子模型之间的差距。

Abstract: Understanding protein flexibility and its dynamic interactions with other molecules is essential for protein function study. Cryogenic electron microscopy (cryo-EM) provides an opportunity to directly observe macromolecular dynamics. However, analyzing datasets that contain both continuous motions and discrete states remains highly challenging. Here we present GaussianEM, a Gaussian pseudo-atomic framework that simultaneously models compositional and conformational heterogeneity from experimental cryo-EM images. GaussianEM employs a two-encoder-one-decoder architecture to map an image to its individual Gaussian components, and represent structural variability through changes in Gaussian parameters. This approach provides an intuitive and interpretable description of conformational changes, preserves local structural consistency along the transition trajectories, and naturally bridges the gap between density-based models and corresponding atomic models. We demonstrate the effectiveness of GaussianEM on both simulated and experimental datasets.

</details>


### [26] [TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant](https://arxiv.org/abs/2512.21616)
*Rongpei Hong,Jian Lang,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: LCMP是首个评估MLLM长上下文个性化能力的基准，TAME框架通过双记忆和RA2G范式显著提升交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视长上下文对话能力，无法持续提升交互体验，需新的评估基准和方法来填补这一空白。

Method: 提出了TAME框架，采用双记忆机制管理个性化概念的时空变化，并引入RA2G范式进行检索对齐增强生成。

Result: 实验证明TAME在LCMP上表现最佳，展示了在长上下文场景中的卓越和进化交互体验。

Conclusion: LCMP和TAME框架为MLLM个性化研究提供了新的评估基准和方法，显著提升了长上下文对话中的个性化交互体验。

Abstract: Multimodal Large Language Model (MLLM) Personalization is a critical research problem that facilitates personalized dialogues with MLLMs targeting specific entities (known as personalized concepts). However, existing methods and benchmarks focus on the simple, context-agnostic visual identification and textual replacement of the personalized concept (e.g., "A yellow puppy" -> "Your puppy Mochi"), overlooking the ability to support long-context conversations. An ideal personalized MLLM assistant is capable of engaging in long-context dialogues with humans and continually improving its experience quality by learning from past dialogue histories. To bridge this gap, we propose LCMP, the first Long-Context MLLM Personalization evaluation benchmark. LCMP assesses the capability of MLLMs in perceiving variations of personalized concepts and generating contextually appropriate personalized responses that reflect these variations. As a strong baseline for LCMP, we introduce a novel training-free and state-aware framework TAME. TAME endows MLLMs with double memories to manage the temporal and persistent variations of each personalized concept in a differentiated manner. In addition, TAME incorporates a new training-free Retrieve-then-Align Augmented Generation (RA2G) paradigm. RA2G introduces an alignment step to extract the contextually fitted information from the multi-memory retrieved knowledge to the current questions, enabling better interactions for complex real-world user queries. Experiments on LCMP demonstrate that TAME achieves the best performance, showcasing remarkable and evolving interaction experiences in long-context scenarios.

</details>


### [27] [CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective](https://arxiv.org/abs/2512.21617)
*Zhiwen Yang,Jinglin Xu,Yuxin Pen*

Main category: cs.CV

TL;DR: CausalFSFG通过因果干预解决FS-FGVC中的偏差问题，结合IMSE和IMFR，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了支持样本集作为混淆变量带来的偏差数据分布问题，影响了性能。

Method: 提出了基于因果推断的CausalFSFG方法，包含样本级干预的IMSE和特征级干预的IMFR。

Result: 在CUB-200-2011、Stanford Dogs和Stanford Cars等数据集上取得了最先进的性能。

Conclusion: CausalFSFG方法通过因果干预解决了FS-FGVC中的偏差数据分布问题，实现了最先进的性能。

Abstract: Few-shot fine-grained visual categorization (FS-FGVC) focuses on identifying various subcategories within a common superclass given just one or few support examples. Most existing methods aim to boost classification accuracy by enriching the extracted features with discriminative part-level details. However, they often overlook the fact that the set of support samples acts as a confounding variable, which hampers the FS-FGVC performance by introducing biased data distribution and misguiding the extraction of discriminative features. To address this issue, we propose a new causal FS-FGVC (CausalFSFG) approach inspired by causal inference for addressing biased data distributions through causal intervention. Specifically, based on the structural causal model (SCM), we argue that FS-FGVC infers the subcategories (i.e., effect) from the inputs (i.e., cause), whereas both the few-shot condition disturbance and the inherent fine-grained nature (i.e., large intra-class variance and small inter-class variance) lead to unobservable variables that bring spurious correlations, compromising the final classification performance. To further eliminate the spurious correlations, our CausalFSFG approach incorporates two key components: (1) Interventional multi-scale encoder (IMSE) conducts sample-level interventions, (2) Interventional masked feature reconstruction (IMFR) conducts feature-level interventions, which together reveal real causalities from inputs to subcategories. Extensive experiments and thorough analyses on the widely-used public datasets, including CUB-200-2011, Stanford Dogs, and Stanford Cars, demonstrate that our CausalFSFG achieves new state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM.

</details>


### [28] [Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints](https://arxiv.org/abs/2512.21637)
*Mutiara Shabrina,Nova Kurnia Putri,Jefri Satria Ferdiansyah,Sabita Khansa Dewi,Novanto Yudistira*

Main category: cs.CV

TL;DR: PPE框架通过L1正则化稀疏约束改进属性解耦编辑，减少非目标属性变化，保持身份。


<details>
  <summary>Details</summary>
Motivation: 解决文本驱动图像编辑中的属性纠缠问题，避免修改目标属性时意外改变其他语义属性。

Method: 分析了PPE框架的架构组件，包括基于BERT的属性预测和基于StyleGAN2的图像生成，并引入了L1正则化稀疏约束。

Result: 实验结果表明，提出的方法实现了更集中和可控的编辑，有效减少了非目标属性的意外变化。

Conclusion: 引入基于稀疏性的L1正则化约束显著改善了属性解耦编辑，减少了非目标属性的意外变化，同时保持了面部身份。

Abstract: Text-driven image manipulation often suffers from attribute entanglement, where modifying a target attribute (e.g., adding bangs) unintentionally alters other semantic properties such as identity or appearance. The Predict, Prevent, and Evaluate (PPE) framework addresses this issue by leveraging pre-trained vision-language models for disentangled editing. In this work, we analyze the PPE framework, focusing on its architectural components, including BERT-based attribute prediction and StyleGAN2-based image generation on the CelebA-HQ dataset. Through empirical analysis, we identify a limitation in the original regularization strategy, where latent updates remain dense and prone to semantic leakage. To mitigate this issue, we introduce a sparsity-based constraint using L1 regularization on latent space manipulation. Experimental results demonstrate that the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.

</details>


### [29] [TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References](https://arxiv.org/abs/2512.21641)
*Jiahong Yu,Ziqi Wang,Hailiang Zhao,Wei Zhai,Xueqiang Yan,Shuiguang Deng*

Main category: cs.CV

TL;DR: TrackTeller是一个整合多模态和时间推理的框架，用于动态3D场景中的语言基础对象跟踪，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态3D驾驶场景中基于自然语言的对象引用问题，尤其是那些依赖近期运动或短期交互的描述，这些无法仅通过静态外观或几何特征解决。

Method: 提出了TrackTeller，一个时间多模态基础框架，通过构建共享的UniScene表示、生成语言感知的3D提案，并利用运动历史和短期动态优化基础决策。

Result: 在NuPrompt基准测试中，TrackTeller实现了70%的相对多目标跟踪准确率提升，并将误报频率降低了3.15-3.4倍。

Conclusion: TrackTeller框架通过整合LiDAR-图像融合、语言条件解码和时间推理，显著提升了动态3D驾驶场景中的语言基础跟踪性能，实验结果显示其在NuPrompt基准测试中表现优异。

Abstract: Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.

</details>


### [30] [Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding](https://arxiv.org/abs/2512.21643)
*Zhiwang Zhou,Yuandong Pu,Xuming He,Yidi Liu,Yixin Chen,Junchao Gong,Xiang Zhuang,Wanghan Xu,Qinglong Cao,Shixiang Tang,Yihao Liu,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: Omni-Weather是首个统一天气生成和理解的多模态基础模型，通过共享架构和因果推理数据集实现高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将天气预测的准确性和机制解释分离，导致生成和理解目标孤立。Omni-Weather旨在填补这一空白。

Method: Omni-Weather采用雷达编码器进行天气生成任务，并通过共享的自注意力机制进行统一处理，同时构建了用于因果推理的Chain-of-Thought数据集。

Result: Omni-Weather在天气生成和理解任务中均达到最先进性能，并展示了生成和理解任务的相互增强效果。

Conclusion: Omni-Weather展示了统一天气生成和理解任务的可行性和价值，且生成和理解任务在该领域可以相互促进。

Abstract: Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.

</details>


### [31] [The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds](https://arxiv.org/abs/2512.21670)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.CV

TL;DR: 提出机械解释性框架，结合SAE和法医流形分析，揭示深度伪造检测模型内部特征使用规律及流形几何特性，提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度伪造检测模型准确率高，但其决策过程不透明，需开发解释性框架以理解其内部机制。

Method: 结合稀疏自编码器（SAE）分析和新型法医流形分析，探究模型内部表示及其对受控法医伪影操作的响应。

Result: 发现每层仅少量潜在特征被激活，且特征流形的几何特性（如内在维度、曲率和特征选择性）随不同深度伪造伪影类型系统性变化。

Conclusion: 本文通过机械解释性框架揭示了深度伪造检测模型的内部工作机制，识别了特定法医伪影对应的学习特征，为开发更具可解释性和鲁棒性的模型提供了指导。

Abstract: Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model's features respond to controlled forensic artifact manipulations. We demonstrate that only a small fraction of latent features are actively used in each layer, and that the geometric properties of the model's feature manifold, including intrinsic dimensionality, curvature, and feature selectivity, vary systematically with different types of deepfake artifacts. These insights provide a first step toward opening the "black box" of deepfake detectors, allowing us to identify which learned features correspond to specific forensic artifacts and to guide the development of more interpretable and robust models.

</details>


### [32] [Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles](https://arxiv.org/abs/2512.21673)
*Jalal Khan*

Main category: cs.CV

TL;DR: YOLOv8s在训练时间和检测准确率上均优于YOLO-NAS，适合自动驾驶感知任务。


<details>
  <summary>Details</summary>
Motivation: 比较新兴深度学习模型在自动驾驶车辆感知任务中的性能，以指导研究社区选择更高效的模型。

Method: 通过自定义数据集对YOLO-NAS和YOLOv8两种深度学习模型进行性能比较。

Result: YOLOv8s模型比YOLO-NAS节省75%的训练时间，且在目标检测准确率上以83%优于YOLO-NAS的81%。

Conclusion: YOLOv8s模型在训练时间和检测准确率上均优于YOLO-NAS模型，为自动驾驶车辆感知系统提供了更高效的解决方案。

Abstract: Recently, a plethora of machine learning (ML) and deep learning (DL) algorithms have been proposed to achieve the efficiency, safety, and reliability of autonomous vehicles (AVs). The AVs use a perception system to detect, localize, and identify other vehicles, pedestrians, and road signs to perform safe navigation and decision-making. In this paper, we compare the performance of DL models, including YOLO-NAS and YOLOv8, for a detection-based perception task. We capture a custom dataset and experiment with both DL models using our custom dataset. Our analysis reveals that the YOLOv8s model saves 75% of training time compared to the YOLO-NAS model. In addition, the YOLOv8s model (83%) outperforms the YOLO-NAS model (81%) when the target is to achieve the highest object detection accuracy. These comparative analyses of these new emerging DL models will allow the relevant research community to understand the models' performance under real-world use case scenarios.

</details>


### [33] [UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture](https://arxiv.org/abs/2512.21675)
*Shuo Cao,Jiayang Li,Xiaohui Li,Yuandong Pu,Kaiwen Zhu,Yuanting Gao,Siqi Luo,Yi Xin,Qi Qin,Yu Zhou,Xiangyu Chen,Wenlong Zhang,Bin Fu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: UniPercept-Bench是一个统一的感知级图像理解框架，通过领域自适应预训练和任务对齐强化学习训练模型，在视觉评分和视觉问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉理解任务上取得了显著进展，但其感知级图像特征的识别能力仍有限。

Method: 提出了UniPercept-Bench统一框架，通过领域自适应预训练（Domain-Adaptive Pre-Training）和任务对齐强化学习（Task-Aligned RL）训练强基线模型UniPercept。

Result: UniPercept在感知级图像理解上优于现有MLLMs，并可作为文本到图像生成的即插即用奖励模型。

Conclusion: UniPercept-Bench为多模态大语言模型（MLLMs）时代定义了感知级图像理解，并通过引入全面的基准和强基线，为推进感知级多模态图像理解提供了坚实基础。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.

</details>


### [34] [Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2512.21683)
*Yuntian Bo,Tao Zhou,Zechao Li,Haofeng Zhang,Ling Shao*

Main category: cs.CV

TL;DR: C-Graph框架利用医学图像的结构一致性提升跨领域少样本分割性能，同时保持源领域准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在过滤领域特定信息时无意中限制跨领域性能和降低源领域准确性的问题。

Method: 提出Contrastive Graph Modeling (C-Graph)框架，包括Structural Prior Graph (SPG)层、Subgraph Matching Decoding (SMD)机制和Confusion-minimizing Node Contrast (CNC)损失函数。

Result: 在多个跨领域基准测试中显著优于现有CD-FSMIS方法，实现了最先进的性能。

Conclusion: C-Graph框架通过利用医学图像的结构一致性作为可靠的跨领域先验，显著提升了跨领域少样本医学图像分割的性能，同时在源领域保持了强大的分割准确性。

Abstract: Cross-domain few-shot medical image segmentation (CD-FSMIS) offers a promising and data-efficient solution for medical applications where annotations are severely scarce and multimodal analysis is required. However, existing methods typically filter out domain-specific information to improve generalization, which inadvertently limits cross-domain performance and degrades source-domain accuracy. To address this, we present Contrastive Graph Modeling (C-Graph), a framework that leverages the structural consistency of medical images as a reliable domain-transferable prior. We represent image features as graphs, with pixels as nodes and semantic affinities as edges. A Structural Prior Graph (SPG) layer is proposed to capture and transfer target-category node dependencies and enable global structure modeling through explicit node interactions. Building upon SPG layers, we introduce a Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction. Furthermore, we design a Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity by contrastively enhancing node discriminability in the graph space. Our method significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.

</details>


### [35] [BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks](https://arxiv.org/abs/2512.21694)
*Md. Rakibul Islam,Md. Kamrozzaman Bhuiyan,Safwan Muntasir,Arifur Rahman Jawad,Most. Sharmin Sultana Samu*

Main category: cs.CV

TL;DR: 本文提出了一种生成孟加拉语手写文本的方法，使用自建数据集，成功实现了多样化输出，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为全球第五大语言，其手写文本生成研究相对较少，而现有研究主要集中在英语和阿拉伯语上，因此填补这一空白具有重要意义。

Method: 开发并使用了自建的孟加拉语手写样本数据集，包含约五百名不同年龄和性别的贡献者，所有图像均经过预处理以确保一致性和质量。

Result: 提出的方法能够从输入纯文本生成多样化的手写输出。

Conclusion: 本研究通过自建数据集和预处理方法，成功实现了孟加拉语手写文本的多样化生成，为该领域的进一步研究提供了支持。

Abstract: Handwritten Text Recognition (HTR) is a well-established research area. In contrast, Handwritten Text Generation (HTG) is an emerging field with significant potential. This task is challenging due to the variation in individual handwriting styles. A large and diverse dataset is required to generate realistic handwritten text. However, such datasets are difficult to collect and are not readily available. Bengali is the fifth most spoken language in the world. While several studies exist for languages such as English and Arabic, Bengali handwritten text generation has received little attention. To address this gap, we propose a method for generating Bengali handwritten words. We developed and used a self-collected dataset of Bengali handwriting samples. The dataset includes contributions from approximately five hundred individuals across different ages and genders. All images were pre-processed to ensure consistency and quality. Our approach demonstrates the ability to produce diverse handwritten outputs from input plain text. We believe this work contributes to the advancement of Bengali handwriting generation and can support further research in this area.

</details>


### [36] [SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration](https://arxiv.org/abs/2512.21684)
*Md Motaleb Hossen Manik,Md Zabirul Islam,Ge Wang*

Main category: cs.CV

TL;DR: SlideChain是一个基于区块链的框架，旨在为多模态语义提取提供可验证的完整性，解决了VLM在教育内容中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现代视觉-语言模型（VLM）在教育内容解释和生成中的应用日益广泛，但其语义输出的验证、重现和审计存在挑战，尤其是在高风险的STEM领域。

Method: 利用SlideChain Slides Dataset（包含1,117张医学影像讲座幻灯片），从四种先进的VLM中提取概念和关系三元组，并在本地EVM兼容区块链上锚定这些记录的加密哈希。

Result: 揭示了多模型间的显著差异（如概念重叠率低、关系三元组一致性接近零），并展示了SlideChain在篡改检测、可扩展性和确定性重现方面的完美表现。

Conclusion: SlideChain通过区块链技术为多模态教育内容提供了可验证的完整性和长期可审计性，支持AI辅助教学系统的可信度。

Abstract: Modern vision--language models (VLMs) are increasingly used to interpret and generate educational content, yet their semantic outputs remain challenging to verify, reproduce, and audit over time. Inconsistencies across model families, inference settings, and computing environments undermine the reliability of AI-generated instructional material, particularly in high-stakes and quantitative STEM domains. This work introduces SlideChain, a blockchain-backed provenance framework designed to provide verifiable integrity for multimodal semantic extraction at scale. Using the SlideChain Slides Dataset-a curated corpus of 1,117 medical imaging lecture slides from a university course-we extract concepts and relational triples from four state-of-the-art VLMs and construct structured provenance records for every slide. SlideChain anchors cryptographic hashes of these records on a local EVM (Ethereum Virtual Machine)-compatible blockchain, providing tamper-evident auditability and persistent semantic baselines. Through the first systematic analysis of semantic disagreement, cross-model similarity, and lecture-level variability in multimodal educational content, we reveal pronounced cross-model discrepancies, including low concept overlap and near-zero agreement in relational triples on many slides. We further evaluate gas usage, throughput, and scalability under simulated deployment conditions, and demonstrate perfect tamper detection along with deterministic reproducibility across independent extraction runs. Together, these results show that SlideChain provides a practical and scalable step toward trustworthy, verifiable multimodal educational pipelines, supporting long-term auditability, reproducibility, and integrity for AI-assisted instructional systems.

</details>


### [37] [Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective](https://arxiv.org/abs/2512.21691)
*Huan Li,Longjun Luo,Yuling Shi,Xiaodong Gu*

Main category: cs.CV

TL;DR: VGGT的全局自注意力层在长序列输入时会出现崩溃现象，本文通过数学建模解释了这一现象，并提出了理论支持令牌合并的有效性。


<details>
  <summary>Details</summary>
Motivation: 解释VGGT中全局自注意力层在输入序列超过几百帧时出现的崩溃现象，包括注意力矩阵接近秩一、令牌几何退化等问题。

Method: 通过将全局注意力迭代视为退化扩散过程，建立数学模型，证明令牌特征流以$O(1/L)$速率收敛于Dirac型测度，并推导出闭式平均场偏微分方程。

Result: 理论精确预测了注意力热图演化及实验结果，并解释了令牌合并补救措施的有效性。

Conclusion: 该分析为未来可扩展的3D视觉Transformer提供了原则性的解释框架，并强调了其在多模态泛化中的潜力。

Abstract: Visual Geometry Grounded Transformer (VGGT) delivers state-of-the-art feed-forward 3D reconstruction, yet its global self-attention layer suffers from a drastic collapse phenomenon when the input sequence exceeds a few hundred frames: attention matrices rapidly become near rank-one, token geometry degenerates to an almost one-dimensional subspace, and reconstruction error accumulates super-linearly.In this report,we establish a rigorous mathematical explanation of the collapse by viewing the global-attention iteration as a degenerate diffusion process.We prove that,in VGGT, the token-feature flow converges toward a Dirac-type measure at a $O(1/L)$ rate, where $L$ is the layer index, yielding a closed-form mean-field partial differential equation that precisely predicts the empirically observed rank profile.The theory quantitatively matches the attention-heat-map evolution and a series of experiments outcomes reported in relevant works and explains why its token-merging remedy -- which periodically removes redundant tokens -- slows the effective diffusion coefficient and thereby delays collapse without additional training.We believe the analysis provides a principled lens for interpreting future scalable 3D-vision transformers,and we highlight its potential for multi-modal generalization.

</details>


### [38] [Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating](https://arxiv.org/abs/2512.21693)
*Li Yang,Yuting Liu*

Main category: cs.CV

TL;DR: Prior-AttUNet通过生成解剖先验和注意力机制，高效解决了OCT图像中液体区域分割的挑战，表现出色且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决光学相干断层扫描（OCT）图像中液体区域分割的挑战，如模糊边界和设备间异质性。

Method: 采用混合双路径架构，结合生成解剖先验和分割网络，利用变分自编码器提供多尺度解剖先验，并通过三重注意力机制动态调整解码阶段特征重要性。

Result: 在RETOUCH基准测试中，Prior-AttUNet在三种OCT设备上分别达到93.93%、95.18%和93.47%的平均Dice相似系数，计算成本仅为0.37 TFLOPs。

Conclusion: Prior-AttUNet展示了作为自动化临床分析可靠工具的潜力，实现了高精度的分割与计算效率的平衡。

Abstract: Accurate segmentation of macular edema, a hallmark pathological feature in vision-threatening conditions such as age-related macular degeneration and diabetic macular edema, is essential for clinical diagnosis and management. To overcome the challenges of segmenting fluid regions in optical coherence tomography (OCT) images-notably ambiguous boundaries and cross-device heterogeneity-this study introduces Prior-AttUNet, a segmentation model augmented with generative anatomical priors. The framework adopts a hybrid dual-path architecture that integrates a generative prior pathway with a segmentation network. A variational autoencoder supplies multi-scale normative anatomical priors, while the segmentation backbone incorporates densely connected blocks and spatial pyramid pooling modules to capture richer contextual information. Additionally, a novel triple-attention mechanism, guided by anatomical priors, dynamically modulates feature importance across decoding stages, substantially enhancing boundary delineation. Evaluated on the public RETOUCH benchmark, Prior-AttUNet achieves excellent performance across three OCT imaging devices (Cirrus, Spectralis, and Topcon), with mean Dice similarity coefficients of 93.93%, 95.18%, and 93.47%, respectively. The model maintains a low computational cost of 0.37 TFLOPs, striking an effective balance between segmentation precision and inference efficiency. These results demonstrate its potential as a reliable tool for automated clinical analysis.

</details>


### [39] [A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets](https://arxiv.org/abs/2512.21760)
*Arunkumar V,Firos V M,Senthilkumar S,Gangadharan G R*

Main category: cs.CV

TL;DR: A-QCF-Net通过未配对CT和MRI数据训练统一分割模型，显著提升性能，为医疗影像分析提供新方法。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像的互补信息对病理准确划分至关重要，但深度学习模型的发展受限于配对和对齐的多模态数据集的稀缺。

Method: 论文提出了一种自适应四元数交叉融合网络（A-QCF-Net），利用四元数神经网络的高效参数和表达能力构建共享特征空间，并通过自适应四元数交叉融合（A-QCF）块实现双向知识转移。

Result: 在未配对的LiTS（CT）和ATLAS（MRI）数据集上联合训练的模型，肿瘤Dice分数分别达到76.7%（CT）和78.3%（MRI），显著超过单模态nnU-Net基线。

Conclusion: 该论文提出了一种自适应四元数交叉融合网络（A-QCF-Net），通过利用未配对的CT和MRI数据集训练单一的统一分割模型，显著提高了分割性能，为医疗影像分析提供了一种新的范式。

Abstract: Multimodal medical imaging provides complementary information that is crucial for accurate delineation of pathology, but the development of deep learning models is limited by the scarcity of large datasets in which different modalities are paired and spatially aligned. This paper addresses this fundamental limitation by proposing an Adaptive Quaternion Cross-Fusion Network (A-QCF-Net) that learns a single unified segmentation model from completely separate and unpaired CT and MRI cohorts. The architecture exploits the parameter efficiency and expressive power of Quaternion Neural Networks to construct a shared feature space. At its core is the Adaptive Quaternion Cross-Fusion (A-QCF) block, a data driven attention module that enables bidirectional knowledge transfer between the two streams. By learning to modulate the flow of information dynamically, the A-QCF block allows the network to exchange abstract modality specific expertise, such as the sharp anatomical boundary information available in CT and the subtle soft tissue contrast provided by MRI. This mutual exchange regularizes and enriches the feature representations of both streams. We validate the framework by jointly training a single model on the unpaired LiTS (CT) and ATLAS (MRI) datasets. The jointly trained model achieves Tumor Dice scores of 76.7% on CT and 78.3% on MRI, significantly exceeding the strong unimodal nnU-Net baseline by margins of 5.4% and 4.7% respectively. Furthermore, comprehensive explainability analysis using Grad-CAM and Grad-CAM++ confirms that the model correctly focuses on relevant pathological structures, ensuring the learned representations are clinically meaningful. This provides a robust and clinically viable paradigm for unlocking the large unpaired imaging archives that are common in healthcare.

</details>


### [40] [Inference-based GAN Video Generation](https://arxiv.org/abs/2512.21776)
*Jingbo Yang,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出了一种基于VAE-GAN和马尔可夫链的长视频生成模型，解决了现有模型在时间扩展和视频质量上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成长视频时面临时间扩展和视频质量下降的挑战，需要一种能够保持时间连续性和动态一致性的新方法。

Method: 采用VAE-GAN混合结构，结合马尔可夫链框架和记忆机制，通过短序列生成器的顺序连接实现长视频生成。

Result: 提出的模型能够生成数百或数千帧的长视频，确保时间连续性、一致性和动态效果。

Conclusion: 本文提出了一种结合VAE-GAN混合结构的视频生成模型，并通过马尔可夫链框架和记忆机制，成功生成了具有时间连续性和动态一致性的长视频序列。

Abstract: Video generation has seen remarkable progresses thanks to advancements in generative deep learning. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Generating models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) and more recently Diffusion Networks have been used for generating short video sequences, usually of up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure, in order to enable the generation process with inference capabilities. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. In classical approaches when aiming to increase the generated video length, the resulting video quality degrades, particularly when considering generating significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, with each state representing a VAE-GAN short-length video generator. This setup allows for the sequential connection of generated video sub-sequences, enabling temporal dependencies, resulting in meaningful long video sequences.

</details>


### [41] [FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection](https://arxiv.org/abs/2512.21695)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Kamrozzaman Bhuiyan,Farhad Uz Zaman,Md. Rakibul Islam*

Main category: cs.CV

TL;DR: FUSE系统通过结合频谱和语义特征，分两阶段训练，在多个数据集上实现了高性能的AI生成图像检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展增加了对可靠检测AI生成图像的需求。

Method: 提出了FUSE系统，结合快速傅里叶变换提取的频谱特征和CLIP视觉编码器获取的语义特征，分两阶段进行渐进式训练。

Result: 在多个数据集上表现出强大的泛化能力，Stage 1模型在Chameleon基准测试中达到最先进水平，GenImage数据集平均准确率为91.36%，所有测试生成器平均准确率为88.71%，平均精确率为94.96%。Stage 2训练进一步提升了大多数生成器的性能。

Conclusion: 整合频谱和语义特征的方法在AI生成图像的广义检测中显示出显著优势。

Abstract: The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.

</details>


### [42] [InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation](https://arxiv.org/abs/2512.21788)
*Jinqi Xiao,Qing Yan,Liming Jiang,Zichuan Liu,Hao Kang,Shen Sang,Tiancheng Zhi,Jing Liu,Cheng Yang,Xin Lu,Bo Yuan*

Main category: cs.CV

TL;DR: InstructMoLE 通过全局路由和正交性损失优化多条件生成，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有 LoRA 适配器和 MoLE 变体在多条件生成任务中的任务干扰问题，特别是局部路由与全局用户指令冲突导致的语义漂移和空间碎片化。

Method: 引入了 InstructMoLE 框架，采用全局路由信号（Instruction-Guided Routing, IGR）和输出空间正交性损失，确保专家功能多样性。

Result: InstructMoLE 在多条件生成基准测试中显著优于现有 LoRA 适配器和 MoLE 变体。

Conclusion: InstructMoLE 提供了一个稳健且可推广的框架，用于生成模型的指令驱动微调，实现了卓越的组合控制和用户意图的忠实度。

Abstract: Parameter-Efficient Fine-Tuning of Diffusion Transformers (DiTs) for diverse, multi-conditional tasks often suffers from task interference when using monolithic adapters like LoRA. The Mixture of Low-rank Experts (MoLE) architecture offers a modular solution, but its potential is usually limited by routing policies that operate at a token level. Such local routing can conflict with the global nature of user instructions, leading to artifacts like spatial fragmentation and semantic drift in complex image generation tasks. To address these limitations, we introduce InstructMoLE, a novel framework that employs an Instruction-Guided Mixture of Low-Rank Experts. Instead of per-token routing, InstructMoLE utilizes a global routing signal, Instruction-Guided Routing (IGR), derived from the user's comprehensive instruction. This ensures that a single, coherently chosen expert council is applied uniformly across all input tokens, preserving the global semantics and structural integrity of the generation process. To complement this, we introduce an output-space orthogonality loss, which promotes expert functional diversity and mitigates representational collapse. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks. Our work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling superior compositional control and fidelity to user intent.

</details>


### [43] [Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction](https://arxiv.org/abs/2512.21707)
*Zheng Yin,Chengjian Li,Xiangbo Shu,Meiqi Cao,Rui Yan,Jinhui Tang*

Main category: cs.CV

TL;DR: ST-MoE模型通过灵活探索人体运动的复杂时空依赖性并引入双向时空Mamba专家，显著降低了计算成本，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕获人体运动的复杂时空依赖性时存在两大局限：一是依赖位置编码导致时空表示不灵活，二是传统注意力机制的二次时间复杂度导致计算成本高。

Method: 提出Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE)模型，包含四种不同类型的时空专家，专门捕获不同的空间或时间依赖性，并引入双向时空Mamba作为专家以减少计算开销。

Result: 在四个多人基准数据集上的实验表明，ST-MoE在准确率上优于现有技术，同时减少了41.38%的模型参数并实现了3.6倍的训练加速。

Conclusion: ST-MoE模型通过引入双向时空Mamba作为专家，显著降低了计算成本，并在多个人体运动预测基准数据集上表现出色，不仅准确率优于现有技术，还减少了41.38%的模型参数并实现了3.6倍的训练加速。

Abstract: Comprehensively and flexibly capturing the complex spatio-temporal dependencies of human motion is critical for multi-person motion prediction. Existing methods grapple with two primary limitations: i) Inflexible spatiotemporal representation due to reliance on positional encodings for capturing spatiotemporal information. ii) High computational costs stemming from the quadratic time complexity of conventional attention mechanisms. To overcome these limitations, we propose the Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE), which flexibly explores complex spatio-temporal dependencies in human motion and significantly reduces computational cost. To adaptively mine complex spatio-temporal patterns from human motion, our model incorporates four distinct types of spatiotemporal experts, each specializing in capturing different spatial or temporal dependencies. To reduce the potential computational overhead while integrating multiple experts, we introduce bidirectional spatiotemporal Mamba as experts, each sharing bidirectional temporal and spatial Mamba in distinct combinations to achieve model efficiency and parameter economy. Extensive experiments on four multi-person benchmark datasets demonstrate that our approach not only outperforms state-of-art in accuracy but also reduces model parameter by 41.38% and achieves a 3.6x speedup in training. The code is available at https://github.com/alanyz106/ST-MoE.

</details>


### [44] [CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection](https://arxiv.org/abs/2512.21803)
*Ruochen Liu,Yi Tian,Jiahao Wang,Hongbin Liu,Xianxu Hou,Jingxin Liu*

Main category: cs.CV

TL;DR: CellMamba是一种轻量级且准确的一阶段检测器，专为细粒度生物医学实例检测设计，通过TMAC模块和Adaptive Mamba Head提升性能，实验证明其在准确性和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 病理图像中的细胞检测面临密集排列对象、细微的类间差异和严重背景干扰等独特挑战。

Method: 基于VSSD骨干网络，CellMamba集成了CellMamba Blocks，结合NC-Mamba或Multi-Head Self-Attention (MSA)与新颖的Triple-Mapping Adaptive Coupling (TMAC)模块。TMAC通过将通道分为两个并行分支，配备双重特异性和一个共识注意力图，自适应融合以保持局部敏感性和全局一致性。此外，设计了Adaptive Mamba Head，通过可学习权重融合多尺度特征，以适应不同大小的对象。

Result: 在两个公共数据集CoNSeP和CytoDArk0上的广泛实验表明，CellMamba在准确性上优于基于CNN、Transformer和Mamba的基线方法，同时显著减少了模型大小和推理延迟。

Conclusion: CellMamba被验证为一种高效且有效的解决方案，适用于高分辨率细胞检测。

Abstract: Cell detection in pathological images presents unique challenges due to densely packed objects, subtle inter-class differences, and severe background clutter. In this paper, we propose CellMamba, a lightweight and accurate one-stage detector tailored for fine-grained biomedical instance detection. Built upon a VSSD backbone, CellMamba integrates CellMamba Blocks, which couple either NC-Mamba or Multi-Head Self-Attention (MSA) with a novel Triple-Mapping Adaptive Coupling (TMAC) module. TMAC enhances spatial discriminability by splitting channels into two parallel branches, equipped with dual idiosyncratic and one consensus attention map, adaptively fused to preserve local sensitivity and global consistency. Furthermore, we design an Adaptive Mamba Head that fuses multi-scale features via learnable weights for robust detection under varying object sizes. Extensive experiments on two public datasets-CoNSeP and CytoDArk0-demonstrate that CellMamba outperforms both CNN-based, Transformer-based, and Mamba-based baselines in accuracy, while significantly reducing model size and inference latency. Our results validate CellMamba as an efficient and effective solution for high-resolution cell detection.

</details>


### [45] [RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention](https://arxiv.org/abs/2512.21710)
*Zhan Chen,Zile Guo,Enze Zhu,Peirong Zhang,Xiaoxuan Liu,Lei Wang,Yidan Zhang*

Main category: cs.CV

TL;DR: RAPTOR是一种实时高分辨率视频预测架构，通过高效视频注意力模块和单次生成设计，突破了现有方法的性能限制，适用于无人机等边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有方法因依赖迭代生成或高复杂度注意力机制，无法满足边缘硬件对高分辨率视频实时预测的严格要求，而这对无人机在密集城市环境中的安全至关重要。

Method: RAPTOR采用单次生成设计，避免了迭代方法的误差累积和延迟。其核心创新是高效视频注意力（EVA）模块，通过沿空间（S）和时间（T）轴交替操作，将复杂度降低至O(S + T)，实现了全局上下文建模。

Result: RAPTOR在Jetson AGX Orin上实现了512^2分辨率视频的30 FPS以上预测，在UAVid、KTH等数据集上创下PSNR、SSIM和LPIPS的新纪录，并将真实无人机导航任务的成功率提高了18%。

Conclusion: RAPTOR突破了视频预测中的分辨率、质量和实时性之间的权衡，首次在边缘硬件上实现了高分辨率（512^2）视频的实时预测（超过30 FPS），并在多个数据集上刷新了性能记录。

Abstract: Video prediction is plagued by a fundamental trilemma: achieving high-resolution and perceptual quality typically comes at the cost of real-time speed, hindering its use in latency-critical applications. This challenge is most acute for autonomous UAVs in dense urban environments, where foreseeing events from high-resolution imagery is non-negotiable for safety. Existing methods, reliant on iterative generation (diffusion, autoregressive models) or quadratic-complexity attention, fail to meet these stringent demands on edge hardware. To break this long-standing trade-off, we introduce RAPTOR, a video prediction architecture that achieves real-time, high-resolution performance. RAPTOR's single-pass design avoids the error accumulation and latency of iterative approaches. Its core innovation is Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling. Instead of processing flattened spacetime tokens with $O((ST)^2)$ or $O(ST)$ complexity, EVA alternates operations along the spatial (S) and temporal (T) axes. This factorization reduces the time complexity to $O(S + T)$ and memory complexity to $O(max(S, T))$, enabling global context modeling at $512^2$ resolution and beyond, operating directly on dense feature maps with a patch-free design. Complementing this architecture is a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details. Experiments show RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for $512^2$ video, setting a new state-of-the-art on UAVid, KTH, and a custom high-resolution dataset in PSNR, SSIM, and LPIPS. Critically, RAPTOR boosts the mission success rate in a real-world UAV navigation task by 18/%, paving the way for safer and more anticipatory embodied agents.

</details>


### [46] [S&P 500 Stock's Movement Prediction using CNN](https://arxiv.org/abs/2512.21804)
*Rahul Gupta*

Main category: cs.CV

TL;DR: 本文使用CNN处理多维原始股票数据，模拟历史数据矩阵进行预测，为金融市场预测提供了新方法，取得了积极成果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在预测股票市场时多使用单维度数据，而本文旨在利用多维原始数据（包括股票分割/股息事件）来提升预测的准确性和实用性。

Method: 采用卷积神经网络（CNN）对多维股票数据进行处理，模拟历史数据矩阵（类似于图像），进行预测。

Result: 模型在预测单只股票、行业板块或股票组合时取得了有希望的结果。

Conclusion: 本文展示了使用卷积神经网络（CNN）处理多维原始股票数据的有效性，为金融市场的预测提供了新的视角和方法。

Abstract: This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].
  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.

</details>


### [47] [AstraNav-World: World Model for Foresight Control and Consistency](https://arxiv.org/abs/2512.21714)
*Junjun Hu,Jintao Chen,Haochen Bai,Minghua Luo,Shichao Xie,Ziyi Chen,Fei Liu,Zedong Chu,Xinda Xue,Botao Ren,Xiaolong Wu,Mu Xu,Shanghang Zhang*

Main category: cs.CV

TL;DR: AstraNav-World是一个端到端的世界模型，通过统一视觉预测和动作规划，提升了具身导航的性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 在开放、动态环境中进行具身导航需要准确预测世界如何演变以及动作如何随时间展开。

Method: AstraNav-World结合了基于扩散的视频生成器和视觉语言策略，在一个统一的概率框架中同步预测未来视觉状态和动作序列。

Result: 实验表明，AstraNav-World在多个具身导航基准测试中提高了轨迹准确性和成功率，并展示了出色的零样本能力。

Conclusion: AstraNav-World通过将前瞻性视觉和控制统一在一个生成模型中，实现了在开放、动态环境中更可靠、可解释和通用的具身智能体。

Abstract: Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled "envision-then-plan" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.

</details>


### [48] [Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation](https://arxiv.org/abs/2512.21734)
*Steven Xiao,XIndi Zhang,Dechao Meng,Qi Wang,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: 提出Knot Forcing框架，通过分块生成、时间节点模块和超前运行机制，实现实时高保真肖像动画。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型和因果自回归视频生成方法在实时肖像动画中的局限性，如非因果性、错误累积和运动不连续性。

Method: 1. 分块生成策略，通过缓存的参考图像KV状态和滑动窗口注意力进行局部时间建模；2. 时间节点模块，通过图像到视频的条件传递平滑块间运动过渡；3. "超前运行"机制，动态更新参考帧的时间坐标以保持长期一致性。

Result: Knot Forcing实现了高保真、时间一致且交互式的无限序列肖像动画，并在消费级GPU上达到实时性能。

Conclusion: Knot Forcing框架通过创新的设计解决了实时肖像动画中的关键挑战，实现了高保真、时间一致且交互式的动画效果，适用于消费级GPU。

Abstract: Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.

</details>


### [49] [Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening](https://arxiv.org/abs/2512.21861)
*Md Rafid Islam,Rafsan Jany,Akib Ahmed,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: 研究通过融合不同CNN模型的特征，提升了糖尿病视网膜病变筛查的准确性和效率，尤其适用于大规模异构数据集。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是可预防失明的主要原因，但大规模筛查受限于专家资源的不足以及设备和人群间图像质量的差异。

Method: 本研究将糖尿病视网膜病变检测作为二分类任务，比较了三种预训练模型（ResNet50、EfficientNet-B0和DenseNet121）与成对和三重融合变体。使用来自五个公共数据集的11,156张图像进行实验。

Result: 融合模型在五次独立运行中 consistently 优于单一模型。EfficientNet-B0 + DenseNet121 (Eff+Den) 融合模型实现了最佳整体性能（准确率：82.89%），且在正常（83.60%）和糖尿病（82.60%）病例中均表现平衡。

Conclusion: 轻量级特征融合可以提升异构数据集的泛化能力，支持在准确性和吞吐量都至关重要的可扩展二进制糖尿病视网膜病变筛查工作流程。

Abstract: Diabetic retinopathy (DR) remains a leading cause of preventable blindness, yet large-scale screening is constrained by limited specialist availability and variable image quality across devices and populations. This work investigates whether feature-level fusion of complementary convolutional neural network (CNN) backbones can deliver accurate and efficient binary DR screening on globally sourced fundus images. Using 11,156 images pooled from five public datasets (APTOS, EyePACS, IDRiD, Messidor, and ODIR), we frame DR detection as a binary classification task and compare three pretrained models (ResNet50, EfficientNet-B0, and DenseNet121) against pairwise and tri-fusion variants. Across five independent runs, fusion consistently outperforms single backbones. The EfficientNet-B0 + DenseNet121 (Eff+Den) fusion model achieves the best overall mean performance (accuracy: 82.89\%) with balanced class-wise F1-scores for normal (83.60\%) and diabetic (82.60\%) cases. While the tri-fusion is competitive, it incurs a substantially higher computational cost. Inference profiling highlights a practical trade-off: EfficientNet-B0 is the fastest (approximately 1.16 ms/image at batch size 1000), whereas the Eff+Den fusion offers a favorable accuracy--latency balance. These findings indicate that lightweight feature fusion can enhance generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.

</details>


### [50] [SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild](https://arxiv.org/abs/2512.21736)
*Xindi Zhang,Dechao Meng,Steven Xiao,Qi Wang,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: SyncAnyone通过两阶段框架解决现有AI视频配音方法在动态面部运动和背景一致性上的不足，实现了高精度唇同步和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的训练方法虽能提升唇同步准确性，但破坏了时空上下文，导致动态面部运动和背景一致性表现不佳。

Method: 提出SyncAnyone两阶段学习框架：第一阶段训练基于扩散的视频变换器进行掩码嘴部修复；第二阶段开发无掩码调优管道，通过合成数据优化模型。

Result: 实验表明，SyncAnyone在视觉质量、时间连贯性和身份保持方面达到最先进水平。

Conclusion: SyncAnyone通过两阶段学习框架，成功解决了现有基于掩码训练方法在动态面部运动和背景一致性上的不足，实现了高精度的唇同步和视觉保真度。

Abstract: High-quality AI-powered video dubbing demands precise audio-lip synchronization, high-fidelity visual generation, and faithful preservation of identity and background. Most existing methods rely on a mask-based training strategy, where the mouth region is masked in talking-head videos, and the model learns to synthesize lip movements from corrupted inputs and target audios. While this facilitates lip-sync accuracy, it disrupts spatiotemporal context, impairing performance on dynamic facial motions and causing instability in facial structure and background consistency. To overcome this limitation, we propose SyncAnyone, a novel two-stage learning framework that achieves accurate motion modeling and high visual fidelity simultaneously. In Stage 1, we train a diffusion-based video transformer for masked mouth inpainting, leveraging its strong spatiotemporal modeling to generate accurate, audio-driven lip movements. However, due to input corruption, minor artifacts may arise in the surrounding facial regions and the background. In Stage 2, we develop a mask-free tuning pipeline to address mask-induced artifacts. Specifically, on the basis of the Stage 1 model, we develop a data generation pipeline that creates pseudo-paired training samples by synthesizing lip-synced videos from the source video and random sampled audio. We further tune the stage 2 model on this synthetic data, achieving precise lip editing and better background consistency. Extensive experiments show that our method achieves state-of-the-art results in visual quality, temporal coherence, and identity preservation under in-the wild lip-syncing scenarios.

</details>


### [51] [BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization](https://arxiv.org/abs/2512.21769)
*Evgeny Alves Limarenko,Anastasiia Studenikina*

Main category: cs.CV

TL;DR: BertsWin结合BERT掩码和Swin窗口，解决了3D医学图像中MAE的空间关系捕捉问题，显著提升收敛速度和训练效率，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准的MAE在3D体积图像中难以捕捉三维空间关系，尤其是在预训练中丢弃75%令牌时。需要一种能保持三维空间拓扑并高效处理的方法。

Method: 提出BertsWin，一种结合BERT风格全令牌掩码和Swin Transformer窗口的混合架构，引入完整3D令牌网格（包括掩码和可见部分）以增强空间上下文学习，并使用单级局部Swin窗口降低ViT的二次复杂度。

Result: BertsWin在TMJ分割任务中表现优异，语义收敛速度提升5.8倍，训练周期大幅减少，且计算资源消耗与稀疏ViT基线相当。

Conclusion: BertsWin架构通过保持完整的三维空间拓扑结构，显著加速了语义收敛（5.8倍），并在结合GradientConductor优化器后，训练周期减少15倍（44 vs 660），且不增加计算负担。

Abstract: The application of self-supervised learning (SSL) and Vision Transformers (ViTs) approaches demonstrates promising results in the field of 2D medical imaging, but the use of these methods on 3D volumetric images is fraught with difficulties. Standard Masked Autoencoders (MAE), which are state-of-the-art solution for 2D, have a hard time capturing three-dimensional spatial relationships, especially when 75% of tokens are discarded during pre-training. We propose BertsWin, a hybrid architecture combining full BERT-style token masking using Swin Transformer windows, to enhance spatial context learning in 3D during SSL pre-training. Unlike the classic MAE, which processes only visible areas, BertsWin introduces a complete 3D grid of tokens (masked and visible), preserving the spatial topology. And to smooth out the quadratic complexity of ViT, single-level local Swin windows are used. We introduce a structural priority loss function and evaluate the results of cone beam computed tomography of the temporomandibular joints. The subsequent assessment includes TMJ segmentation on 3D CT scans. We demonstrate that the BertsWin architecture, by maintaining a complete three-dimensional spatial topology, inherently accelerates semantic convergence by a factor of 5.8x compared to standard ViT-MAE baselines. Furthermore, when coupled with our proposed GradientConductor optimizer, the full BertsWin framework achieves a 15-fold reduction in training epochs (44 vs 660) required to reach state-of-the-art reconstruction fidelity. Analysis reveals that BertsWin achieves this acceleration without the computational penalty typically associated with dense volumetric processing. At canonical input resolutions, the architecture maintains theoretical FLOP parity with sparse ViT baselines, resulting in a significant net reduction in total computational resources due to faster convergence.

</details>


### [52] [Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning](https://arxiv.org/abs/2512.21924)
*Tao Yang,Xiuying Wang,Hao Liu,Guanzhong Gong,Lian-Ming Wu,Yu-Ping Wang,Lisheng Wang*

Main category: cs.CV

TL;DR: 新PHI重建框架通过解耦表示和边缘恢复模块，显著提升MRI病变检测的泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前无监督学习方法在多模态和多中心MRI中泛化性有限，且由于输入图像中的异常残差传播到重建的PHI中，性能受限。

Method: 论文提出了两个新模块：解耦表示模块，用于将脑MRI分解为成像信息和解剖图像；边缘到图像恢复模块，通过从解剖图像的高频边缘信息恢复解剖表示来重建高质量的PHI。

Result: 在九个公共数据集（4,443名患者的MRI）上评估，该方法优于17种SOTA方法，AP和DSC分别绝对提高了+18.32%和+13.64%。

Conclusion: 该论文提出了一种新的伪健康图像（PHI）重建框架，通过解耦表示模块和边缘到图像恢复模块，显著提高了多模态和多中心MRI中病变检测的泛化性和性能。

Abstract: Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients' MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.

</details>


### [53] [Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models](https://arxiv.org/abs/2512.21778)
*Nimrod Berman,Adam Botach,Emanuel Ben-Baruch,Shunit Haviv Hakimi,Asaf Gendler,Ilan Naiman,Erez Yosef,Igor Kviatkovsky*

Main category: cs.CV

TL;DR: Scene-VLM是一种多模态视觉语言模型，通过结合视觉和文本信息实现视频场景分割，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于编码器的方法存在视觉中心偏差、孤立分类镜头而忽略序列依赖，且缺乏叙事理解和可解释性。Scene-VLM旨在解决这些问题。

Method: Scene-VLM是一个经过微调的视觉语言模型框架，通过联合处理视觉和文本线索（如帧、转录文本和可选元数据），实现了跨连续镜头的多模态推理。模型采用因果依赖和上下文聚焦窗口机制，确保每个镜头级决策有足够的时序上下文。

Result: Scene-VLM在标准场景分割基准测试中表现优异，例如在MovieNet上，AP和F1分数分别提升了+6和+13.7。

Conclusion: Scene-VLM通过结合视觉和文本线索，实现了视频场景分割的最先进性能，并在MovieNet等基准测试中显著超越了先前的方法。

Abstract: Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.

</details>


### [54] [LVLM-Aided Alignment of Task-Specific Vision Models](https://arxiv.org/abs/2512.21985)
*Alexander Koebler,Lukas Kuhn,Ingo Thon,Florian Buettner*

Main category: cs.CV

TL;DR: LVLM-VA方法通过双向接口利用LVLM泛化能力，有效对齐小视觉模型与人类知识，减少虚假依赖，无需细粒度反馈。


<details>
  <summary>Details</summary>
Motivation: 小任务特定视觉模型的解释常显示其与人类领域知识不符，依赖虚假相关性，可能导致实际部署中的脆弱行为。

Method: 利用大型视觉语言模型（LVLM）的泛化能力，提出LVLM-VA方法，提供双向接口，将模型行为转化为自然语言，并将人类类别级规范映射为图像级批评。

Result: 在合成和真实数据集上验证，LVLM-VA显著改善了模型行为与人类规范的对齐，减少了对虚假特征和群体偏见的依赖。

Conclusion: LVLM-VA方法通过双向接口有效对齐了小任务特定视觉模型与人类领域知识，显著减少了模型对虚假特征和群体特定偏见的依赖，且无需细粒度反馈。

Abstract: In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.

</details>


### [55] [LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration](https://arxiv.org/abs/2512.22010)
*Wen Jiang,Li Wang,Kangyao Huang,Wei Fan,Jinyuan Liu,Shaoyu Liu,Hongwei Duan,Bin Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: LongFly是一种用于无人机长时距视觉语言导航的时空上下文建模框架，通过历史感知策略和模块化设计显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 当前无人机视觉语言导航方法难以建模复杂环境中的长时距时空上下文，导致语义对齐不准确和路径规划不稳定。

Method: 提出了基于历史感知的时空建模策略，包括基于槽的历史图像压缩模块、时空轨迹编码模块和提示引导的多模态集成模块。

Result: LongFly在成功率和路径长度加权成功率上分别比现有最佳基线提高了7.89%和6.33%。

Conclusion: LongFly框架显著提升了无人机在复杂环境中的长时距视觉语言导航性能，实验结果表明其在成功率和路径长度加权成功率上均优于现有方法。

Abstract: Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN. LongFly proposes a history-aware spatiotemporal modeling strategy that transforms fragmented and redundant historical data into structured, compact, and expressive representations. First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations. Then, the spatiotemporal trajectory encoding module is introduced to capture the temporal dynamics and spatial structure of UAV trajectories. Finally, to integrate existing spatiotemporal context with current observations, we design the prompt-guided multimodal integration module to support time-based reasoning and robust waypoint prediction. Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\% in success rate and 6.33\% in success weighted by path length, consistently across both seen and unseen environments.

</details>


### [56] [AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge](https://arxiv.org/abs/2512.21792)
*Hyam Omar Ali,Sahar Alhesseen,Lamis Elkhair,Adrian Galdran,Ming Feng,Zhixiang Xiong,Zengming Lin,Kele Xu,Liang Hu,Benjamin Keel,Oliver Mills,James Battye,Akshay Kumar,Asra Aslam,Prasad Dutande,Ujjwal Baid,Bhakti Baheti,Suhas Gajre,Aravind Shrenivas Murali,Eung-Joo Lee,Ahmed Fahal,Rachid Jennane*

Main category: cs.CV

TL;DR: 论文介绍了mAIcetoma挑战，旨在通过AI模型提升足菌肿诊断效率，结果显示模型在分割和分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 足菌肿是一种被忽视的热带疾病，诊断困难，尤其在资源匮乏地区。通过AI解决方案提升诊断效率，减轻医疗和社会经济负担。

Method: 组织mAIcetoma挑战，邀请全球团队开发基于深度学习的自动模型，用于足菌肿颗粒的分割和类型分类。使用标准化数据集（MyData）评估模型性能。

Result: 所有模型在分割任务中表现优异，强调颗粒检测的重要性。表现最佳的模型在分类任务中也展现了显著性能。

Conclusion: 该论文强调了AI在解决足菌肿诊断挑战中的潜力，特别是在资源匮乏地区。通过mAIcetoma挑战，成功开发了高精度的自动分割和分类模型，为足菌肿的诊断提供了新工具。

Abstract: Mycetoma is a neglected tropical disease caused by fungi or bacteria leading to severe tissue damage and disabilities. It affects poor and rural communities and presents medical challenges and socioeconomic burdens on patients and healthcare systems in endemic regions worldwide. Mycetoma diagnosis is a major challenge in mycetoma management, particularly in low-resource settings where expert pathologists are limited. To address this challenge, this paper presents an overview of the Mycetoma MicroImage: Detect and Classify Challenge (mAIcetoma) which was organized to advance mycetoma diagnosis through AI solutions. mAIcetoma focused on developing automated models for segmenting mycetoma grains and classifying mycetoma types from histopathological images. The challenge attracted the attention of several teams worldwide to participate and five finalist teams fulfilled the challenge objectives. The teams proposed various deep learning architectures for the ultimate goal of this challenge. Mycetoma database (MyData) was provided to participants as a standardized dataset to run the proposed models. Those models were evaluated using evaluation metrics. Results showed that all the models achieved high segmentation accuracy, emphasizing the necessitate of grain detection as a critical step in mycetoma diagnosis. In addition, the top-performing models show a significant performance in classifying mycetoma types.

</details>


### [57] [Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise](https://arxiv.org/abs/2512.21797)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 该研究通过扩散后验采样（DPS）实现单图像超分辨率，最佳配置在特定参数下表现最优，强调了平衡先验和梯度强度的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究扩散后验采样（DPS）在已知退化模型下的单图像超分辨率（SISR）应用。

Method: 实现了基于梯度条件化的似然引导采样过程，结合无条件扩散先验，以增强测量一致性。

Result: 在PS尺度0.95和噪声标准差σ=0.01时获得最佳配置（得分1.45231），能够恢复更清晰的边缘和更连贯的面部细节。

Conclusion: 研究发现，平衡扩散先验和测量梯度强度对于在不重新训练扩散模型的情况下获得稳定、高质量的重建至关重要。

Abstract: This report studies diffusion posterior sampling (DPS) for single-image super-resolution (SISR) under a known degradation model. We implement a likelihood-guided sampling procedure that combines an unconditional diffusion prior with gradient-based conditioning to enforce measurement consistency for $4\times$ super-resolution with additive Gaussian noise. We evaluate posterior sampling (PS) conditioning across guidance scales and noise levels, using PSNR and SSIM as fidelity metrics and a combined selection score $(\mathrm{PSNR}/40)+\mathrm{SSIM}$. Our ablation shows that moderate guidance improves reconstruction quality, with the best configuration achieved at PS scale $0.95$ and noise standard deviation $σ=0.01$ (score $1.45231$). Qualitative results confirm that the selected PS setting restores sharper edges and more coherent facial details compared to the downsampled inputs, while alternative conditioning strategies (e.g., MCG and PS-annealed) exhibit different texture fidelity trade-offs. These findings highlight the importance of balancing diffusion priors and measurement-gradient strength to obtain stable, high-quality reconstructions without retraining the diffusion model for each operator.

</details>


### [58] [StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars](https://arxiv.org/abs/2512.22065)
*Zhiyao Sun,Ziqiao Peng,Yifeng Ma,Yi Chen,Zhengguang Zhou,Zixiang Zhou,Guozhen Zhang,Youliang Zhang,Yuan Zhou,Qinglin Lu,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 提出了一种实时、交互式人类化身生成方法，通过两阶段框架和三个关键组件，实现了高质量、高效率的交互效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散基人类化身生成方法的非因果架构和高计算成本问题，以及现有交互方法通常仅限于头部和肩部区域，无法生成手势和身体动作的局限性。

Method: 提出了一个两阶段的自回归适应和加速框架，包括自回归蒸馏和对抗性细化，以适配高保真人类视频扩散模型。引入了三个关键组件：参考接收器、参考锚定位置重新编码（RAPR）策略和一致性感知判别器。

Result: 开发了一个一次性交互式人类化身模型，能够生成自然的说话和倾听行为以及连贯的手势。实验表明，该方法在生成质量、实时效率和交互自然性方面均优于现有方法。

Conclusion: 该方法在生成质量、实时效率和交互自然性方面均优于现有方法，实现了最先进的性能。

Abstract: Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .

</details>


### [59] [Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models](https://arxiv.org/abs/2512.21815)
*Mengqi He,Xinyu Tian,Xin Shen,Jinhong Ni,Shu Zou,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: EGA方法通过攻击关键高熵令牌，高效破坏VLM输出，揭示其安全机制漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有熵基攻击假设所有令牌对生成不稳定性贡献均等，但实际上少数高熵令牌主导输出轨迹，这暴露了VLM的安全风险。

Method: 提出Entropy-bank Guided Adversarial attacks (EGA)，专注于攻击高熵令牌（约20%），这些令牌在自回归生成中起关键作用。

Result: EGA在多种代表性VLM上实现了93-95%的攻击成功率，并将35-49%的良性输出转为有害输出，展示了更高的安全风险。

Conclusion: EGA方法揭示了当前VLM安全机制的新弱点，通过针对性攻击高熵令牌，实现了高攻击成功率（93-95%）和有害转换率。

Abstract: Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.

</details>


### [60] [End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration](https://arxiv.org/abs/2512.21831)
*Zhenwei Yang,Yibo Ai,Weidong Zhang*

Main category: cs.CV

TL;DR: XET-V2X是一种多模态融合的端到端跟踪框架，通过双层空间交叉注意力模块高效对齐异构视角和模态，显著提升了复杂交通场景中的感知性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，多视角协同感知和多模态融合对于可靠的3D时空理解至关重要，尤其是在V2X场景下的遮挡、有限视角和通信延迟等问题中。

Method: 提出了一种基于多尺度可变形注意力的双层空间交叉注意力模块，用于高效对齐异构视角和模态，并通过多视图图像特征聚合增强语义一致性，随后通过更新的空间查询引导点云融合。

Result: 在真实世界的V2X-Seq-SPD数据集和模拟的V2X-Sim-V2V及V2X-Sim-V2I基准测试中，XET-V2X在不同通信延迟下均表现出检测和跟踪性能的持续提升。

Conclusion: XET-V2X在复杂交通场景中实现了鲁棒且时间稳定的感知能力，显著提升了检测和跟踪性能。

Abstract: Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.

</details>


### [61] [Scalable Class-Incremental Learning Based on Parametric Neural Collapse](https://arxiv.org/abs/2512.21845)
*Chuangxin Zhang,Guangfeng Lin,Enhui Zhao,Kaiyang Liao,Yajun Chen*

Main category: cs.CV

TL;DR: SCL-PNC通过动态ETF分类器和知识蒸馏解决类别增量学习中的类别不对齐和特征漂移，实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展模型时忽略了结构效率和类别分布演化导致的类别不对齐问题，SCL-PNC旨在解决这些挑战。

Method: 提出基于参数神经崩溃的可扩展类别增量学习（SCL-PNC），通过自适应层和动态参数ETF框架实现需求驱动的低成本骨干扩展，并结合并行扩展框架和知识蒸馏算法。

Result: 实验表明SCL-PNC能高效处理现实场景中类别增加的模型扩展问题，并确保特征一致性。

Conclusion: SCL-PNC通过动态可扩展的ETF分类器和知识蒸馏技术，有效解决了类别增量学习中的类别不对齐和特征漂移问题，实验证明了其高效性和有效性。

Abstract: Incremental learning often encounter challenges such as overfitting to new data and catastrophic forgetting of old data. Existing methods can effectively extend the model for new tasks while freezing the parameters of the old model, but ignore the necessity of structural efficiency to lead to the feature difference between modules and the class misalignment due to evolving class distributions. To address these issues, we propose scalable class-incremental learning based on parametric neural collapse (SCL-PNC) that enables demand-driven, minimal-cost backbone expansion by adapt-layer and refines the static into a dynamic parametric Equiangular Tight Frame (ETF) framework according to incremental class. This method can efficiently handle the model expansion question with the increasing number of categories in real-world scenarios. Additionally, to counteract feature drift in serial expansion models, the parallel expansion framework is presented with a knowledge distillation algorithm to align features across expansion modules. Therefore, SCL-PNC can not only design a dynamic and extensible ETF classifier to address class misalignment due to evolving class distributions, but also ensure feature consistency by an adapt-layer with knowledge distillation between extended modules. By leveraging neural collapse, SCL-PNC induces the convergence of the incremental expansion model through a structured combination of the expandable backbone, adapt-layer, and the parametric ETF classifier. Experiments on standard benchmarks demonstrate the effectiveness and efficiency of our proposed method. Our code is available at https://github.com/zhangchuangxin71-cyber/dynamic_ ETF2. Keywords: Class incremental learning; Catastrophic forgetting; Neural collapse;Knowledge distillation; Expanded model.

</details>


### [62] [Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection](https://arxiv.org/abs/2512.21856)
*Lupiao Hu,Fasheng Wang,Fangmei Chen,Fuming Sun,Haojie Li*

Main category: cs.CV

TL;DR: 提出TPS-SCL方法，通过双流MobileViT和多个专用模块，高效处理未对齐RGB-T图像对的显著目标检测，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在未对齐RGB-T图像对上性能下降的问题，应对跨模态差异如空间错位、尺度变化和视角偏移。

Method: 采用双流MobileViT编码器结合Mamba扫描机制，设计了语义相关性约束模块（SCCM）、薄板样条对齐模块（TPSAM）和跨模态相关性模块（CMCM）。

Result: 在多个数据集上达到轻量级SOD方法的SOTA性能，并超越主流RGB-T SOD方法。

Conclusion: TPS-SCL方法在未对齐的RGB-T图像对上实现了高效的显著目标检测，性能优于现有的轻量级和主流方法。

Abstract: Existing RGB-T salient object detection methods predominantly rely on manually aligned and annotated datasets, struggling to handle real-world scenarios with raw, unaligned RGB-T image pairs. In practical applications, due to significant cross-modal disparities such as spatial misalignment, scale variations, and viewpoint shifts, the performance of current methods drastically deteriorates on unaligned datasets. To address this issue, we propose an efficient RGB-T SOD method for real-world unaligned image pairs, termed Thin-Plate Spline-driven Semantic Correlation Learning Network (TPS-SCL). We employ a dual-stream MobileViT as the encoder, combined with efficient Mamba scanning mechanisms, to effectively model correlations between the two modalities while maintaining low parameter counts and computational overhead. To suppress interference from redundant background information during alignment, we design a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features. Furthermore, we introduce a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between modalities. Additionally, a Cross-Modal Correlation Module (CMCM) is incorporated to fully explore and integrate inter-modal dependencies, enhancing detection performance. Extensive experiments on various datasets demonstrate that TPS-SCL attains state-of-the-art (SOTA) performance among existing lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.

</details>


### [63] [Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees](https://arxiv.org/abs/2512.21857)
*Haodong Lei,Hongsong Wang,Xin Geng,Liang Wang,Pan Zhou*

Main category: cs.CV

TL;DR: ADT-Tree通过动态调整草案树结构，显著加速视觉AR模型推理。


<details>
  <summary>Details</summary>
Motivation: 解决视觉AR模型中草案树因空间变化的令牌预测难度导致接受率不一致的问题。

Method: 提出Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree)，利用相邻令牌状态和先前的接受率动态调整草案树的深度和宽度。

Result: 在MS-COCO 2017和PartiPrompts上分别实现了3.13倍和3.05倍的加速。

Conclusion: ADT-Tree通过动态调整草案树的深度和宽度，显著提升了视觉AR模型的推理速度，同时保持了生成质量，且能与松弛采样方法无缝集成。

Abstract: Autoregressive (AR) image models achieve diffusion-level quality but suffer from sequential inference, requiring approximately 2,000 steps for a 576x576 image. Speculative decoding with draft trees accelerates LLMs yet underperforms on visual AR models due to spatially varying token prediction difficulty. We identify a key obstacle in applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to varying prediction difficulties in different image regions. We propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree), an adjacency-adaptive dynamic draft tree that dynamically adjusts draft tree depth and width by leveraging adjacent token states and prior acceptance rates. ADT-Tree initializes via horizontal adjacency, then refines depth/width via bisectional adaptation, yielding deeper trees in simple regions and wider trees in complex ones. The empirical evaluations on MS-COCO 2017 and PartiPrompts demonstrate that ADT-Tree achieves speedups of 3.13xand 3.05x, respectively. Moreover, it integrates seamlessly with relaxed sampling methods such as LANTERN, enabling further acceleration. Code is available at https://github.com/Haodong-Lei-Ray/ADT-Tree.

</details>


### [64] [Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models](https://arxiv.org/abs/2512.21860)
*Masayuki Kawarada,Kosuke Yamada,Antonio Tejero-de-Pablos,Naoto Inoue*

Main category: cs.CV

TL;DR: DIOR是一种无需训练的方法，利用LVLM生成条件图像嵌入，在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉基础模型（如CLIP）无法专注于指定条件的问题，生成能够聚焦特定方面的条件图像嵌入。

Method: DIOR利用大型视觉语言模型（LVLM），通过提示模型用与给定条件相关的单个词描述图像，并提取最后一个令牌的隐藏状态向量作为条件图像嵌入。

Result: 在条件图像相似性任务中，DIOR表现优于现有的无需训练基线（包括CLIP），并且在多个设置中优于需要额外训练的方法。

Conclusion: DIOR提供了一种无需训练的通用解决方案，能够为任何图像和条件生成条件图像嵌入，且在多个设置中表现优于需要额外训练的方法。

Abstract: Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.

</details>


### [65] [EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition](https://arxiv.org/abs/2512.21865)
*Yihan Hu,Xuelin Chen,Xiaodong Cun*

Main category: cs.CV

TL;DR: EasyOmnimatte是一种端到端的视频omnimatte方法，通过双专家策略（Effect Expert和Quality Expert）高效分解前景层及关联效应，显著提升质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频omnimatte方法多阶段或优化效率低，未能充分利用生成先验，导致分解效果不佳。通过微调视频修复模型，探索其感知前景相关效应的潜力。

Method: 采用预训练的视频修复扩散模型，通过LoRA微调特定DiT块（Effect Expert）和全微调（Quality Expert）的双专家策略，实现端到端的视频omnimatte分解。

Result: EasyOmnimatte在视频omnimatte任务中实现了新的最优性能，显著优于基线方法，并支持多种下游任务。

Conclusion: EasyOmnimatte通过双专家策略（Effect Expert和Quality Expert）实现了视频omnimatte的高质量分解，显著提升了质量和效率，并在实验中验证了其有效性。

Abstract: Existing video omnimatte methods typically rely on slow, multi-stage, or inference-time optimization pipelines that fail to fully exploit powerful generative priors, producing suboptimal decompositions. Our key insight is that, if a video inpainting model can be finetuned to remove the foreground-associated effects, then it must be inherently capable of perceiving these effects, and hence can also be finetuned for the complementary task: foreground layer decomposition with associated effects. However, although naïvely finetuning the inpainting model with LoRA applied to all blocks can produce high-quality alpha mattes, it fails to capture associated effects. Our systematic analysis reveals this arises because effect-related cues are primarily encoded in specific DiT blocks and become suppressed when LoRA is applied across all blocks. To address this, we introduce EasyOmnimatte, the first unified, end-to-end video omnimatte method. Concretely, we finetune a pretrained video inpainting diffusion model to learn dual complementary experts while keeping its original weights intact: an Effect Expert, where LoRA is applied only to effect-sensitive DiT blocks to capture the coarse structure of the foreground and associated effects, and a fully LoRA-finetuned Quality Expert learns to refine the alpha matte. During sampling, Effect Expert is used for denoising at early, high-noise steps, while Quality Expert takes over at later, low-noise steps. This design eliminates the need for two full diffusion passes, significantly reducing computational cost without compromising output quality. Ablation studies validate the effectiveness of this Dual-Expert strategy. Experiments demonstrate that EasyOmnimatte sets a new state-of-the-art for video omnimatte and enables various downstream tasks, significantly outperforming baselines in both quality and efficiency.

</details>


### [66] [DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation](https://arxiv.org/abs/2512.21867)
*Divyansh Srivastava,Akshay Mehra,Pranav Maneriker,Debopam Sanyal,Vishnu Raj,Vijay Kamarshi,Fan Du,Joshua Kimball*

Main category: cs.CV

TL;DR: DPAR是一种动态令牌聚合的自回归模型，通过熵驱动合并减少计算开销，提升生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 固定长度的令牌化方案在高分辨率图像生成中计算和内存需求急剧增加，DPAR旨在通过动态令牌聚合解决这一问题。

Method: DPAR采用轻量级无监督自回归模型，利用下一令牌预测熵作为合并令牌的可靠标准，动态调整补丁大小，最小化对标准解码器架构的修改。

Result: DPAR在Imagenet 256和384分辨率下分别减少1.81倍和2.06倍的令牌数量，训练FLOPs降低40%，收敛速度更快，FID提升27.1%。

Conclusion: DPAR通过动态聚合图像令牌为可变数量的补丁，显著降低了计算和内存需求，同时保持了与多模态生成框架的兼容性，并在训练成本和生成质量上实现了显著改进。

Abstract: Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.

</details>


### [67] [SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis](https://arxiv.org/abs/2512.21881)
*Mo Wang,Junfeng Xia,Wenhao Ye,Enyu Liu,Kaining Peng,Jianfeng Feng,Quanying Liu,Hongkai Wen*

Main category: cs.CV

TL;DR: SLIM-Brain 是一种高效 fMRI 基础模型，通过两阶段设计解决数据与训练效率问题，性能优于现有方法且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 当前 fMRI 分析的基础模型面临数据和训练效率的双重瓶颈，atlas-based 方法丢弃了空间细节，而 atlas-free 方法则计算资源消耗过大。

Method: SLIM-Brain 采用两阶段自适应设计：轻量级时间提取器捕获全局上下文并筛选高显著性数据窗口，4D 分层编码器（Hiera-JEPA）仅从选定的窗口学习细粒度体素级表示，并删除约 70% 的掩码补丁。

Result: SLIM-Brain 在七个公共基准测试中表现优异，仅需 4 千次预训练会话和约 30% 的 GPU 内存（相比传统体素级方法）。

Conclusion: SLIM-Brain 作为一种新型的 atlas-free 基础模型，在数据效率和训练效率上均有显著提升，同时在多个任务上实现了最先进的性能。

Abstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.

</details>


### [68] [Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer](https://arxiv.org/abs/2512.21883)
*Tianchen Deng,Wenhua Wu,Kunzhen Wu,Guangming Wang,Siting Zhu,Shenghai Yuan,Xun Chen,Guole Shen,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: Reloc-VGGT通过早期融合和多视角整合，提升了视觉定位的鲁棒性和效率，适用于复杂环境。


<details>
  <summary>Details</summary>
Motivation: 传统视觉定位方法在复杂环境中因后期运动平均不足而导致精度下降，需更有效的空间信息整合方法。

Method: 基于VGGT骨干网络，引入姿态标记器和投影模块，结合稀疏掩码注意力策略降低计算成本。

Result: 在约800万对姿态图像上训练的Reloc-VGGT表现出强准确性和泛化能力，实时提供高质量相机姿态估计。

Conclusion: Reloc-VGGT通过早期融合机制和多视角空间整合，实现了在复杂环境中的鲁棒视觉定位，并在多个公开数据集上验证了其高效性和准确性。

Abstract: Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.

</details>


### [69] [CrownGen: Patient-customized Crown Generation via Point Diffusion Model](https://arxiv.org/abs/2512.21890)
*Juyoung Bae,Moo Hyun Son,Jiale Peng,Wanting Qu,Wener Chen,Zelin Qiu,Kaixin Li,Xiaojuan Chen,Yifan Lin,Hao Chen*

Main category: cs.CV

TL;DR: CrownGen是一个自动化牙冠设计的生成框架，通过去噪扩散模型提高效率和质量，临床验证显示其效果不逊于传统方法。


<details>
  <summary>Details</summary>
Motivation: 数字牙冠设计在修复牙科中仍然是一个劳动密集型的瓶颈。

Method: CrownGen采用了一个生成框架，利用去噪扩散模型在牙齿级点云表示上进行患者定制化牙冠设计。系统包含两个核心组件：边界预测模块用于建立空间先验，以及基于扩散的生成模块用于在一次推理过程中合成多个牙齿的高保真形态。

Result: 在496个外部扫描的定量基准测试和26个修复病例的临床研究中验证了CrownGen。结果表明，CrownGen在几何保真度上超越了现有技术，并显著减少了主动设计时间。经过训练的牙医的临床评估证实，CrownGen辅助设计的牙冠在质量上不逊于专家技术人员使用手动工作流程生产的牙冠。

Conclusion: CrownGen通过自动化复杂的修复建模，提供了一个可扩展的解决方案，以降低成本、缩短周转时间并提高患者获得高质量牙科护理的机会。

Abstract: Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present \textbf{CrownGen}, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.

</details>


### [70] [High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer](https://arxiv.org/abs/2512.21905)
*Shen Zheng,Jiaran Cai,Yuansheng Guan,Shenneng Huang,Xingpei Ma,Junjie Cao,Hanfeng Zhao,Qiang Zhang,Shunsi Zhang,Xiao-Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于DiT的框架，通过混合引导信号和位置偏移模块，解决了长时长视频生成和细粒度细节合成的挑战，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成长时长视频和细粒度面部及手部细节方面存在挑战，限制了高质量应用的实际适用性。

Method: 提出了一种基于扩散变换器（DiT）的框架，设计了混合隐式引导信号和锐度引导因子，结合了时间感知位置偏移融合模块（Position Shift Adaptive Module），并引入了数据增强策略和骨骼对齐模型。

Result: 实验结果表明，该方法在高保真和长时长人类图像动画方面表现优于现有方法。

Conclusion: 该方法在生成高保真和长时长的人类动画视频方面表现优异，超越了现有最先进方法。

Abstract: Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.

</details>


### [71] [Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition](https://arxiv.org/abs/2512.21916)
*Zeyu Liang,Hailun Xia,Naichuan Zheng*

Main category: cs.CV

TL;DR: PAN是一种人体中心图表示学习框架，通过时空图和注意力校准提升多模态动作识别性能，两种变体在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态动作识别方法因RGB和骨骼模态的异质性未能充分利用其互补潜力，需要一种更有效的融合方法。

Method: 提出了PAN框架，利用RGB补丁的令牌嵌入构建时空图，并通过注意力后校准减少对高质量骨骼数据的依赖。设计了两种变体：PAN-Ensemble（双路径图卷积网络加后期融合）和PAN-Unified（单网络统一图表示学习）。

Result: 在三个广泛使用的多模态动作识别数据集上，PAN-Ensemble和PAN-Unified分别在分离和统一建模设置中达到了最先进的性能。

Conclusion: PAN框架通过人体中心图表示学习方法，成功提升了多模态动作识别的性能，并在两种变体（PAN-Ensemble和PAN-Unified）中均实现了最先进的表现。

Abstract: While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.

</details>


### [72] [AutoPP: Towards Automated Product Poster Generation and Optimization](https://arxiv.org/abs/2512.21921)
*Jiahao Fan,Yuxin Qin,Wei Feng,Yanyin Chen,Yaoyu Li,Ao Ma,Yixiu Li,Li Zhuang,Haoyi Bian,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law*

Main category: cs.CV

TL;DR: AutoPP是一个自动化产品海报生成和优化系统，通过设计模块和优化器提升效率，实验证明其效果卓越。


<details>
  <summary>Details</summary>
Motivation: 手动设计和优化产品海报耗时且资源密集，AutoPP旨在通过自动化流程解决这一问题。

Method: AutoPP采用统一设计模块整合海报的三大关键元素（背景、文本和布局），并通过元素渲染模块编码为条件令牌，可控生成海报。优化器则基于在线反馈，通过元素替换和IDPO方法提升CTR。

Result: 实验表明，AutoPP在离线和在线环境中均表现优异，并依托AutoPP1M数据集（包含100万高质量海报和用户反馈）验证了其有效性。

Conclusion: AutoPP通过自动化流程显著提升了产品海报的生成和优化效率，无需人工干预，并在离线和在线环境中均取得了最先进的效果。

Abstract: Product posters blend striking visuals with informative text to highlight the product and capture customer attention. However, crafting appealing posters and manually optimizing them based on online performance is laborious and resource-consuming. To address this, we introduce AutoPP, an automated pipeline for product poster generation and optimization that eliminates the need for human intervention. Specifically, the generator, relying solely on basic product information, first uses a unified design module to integrate the three key elements of a poster (background, text, and layout) into a cohesive output. Then, an element rendering module encodes these elements into condition tokens, efficiently and controllably generating the product poster. Based on the generated poster, the optimizer enhances its Click-Through Rate (CTR) by leveraging online feedback. It systematically replaces elements to gather fine-grained CTR comparisons and utilizes Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements. Our work is supported by AutoPP1M, the largest dataset specifically designed for product poster generation and optimization, which contains one million high-quality posters and feedback collected from over one million users. Experiments demonstrate that AutoPP achieves state-of-the-art results in both offline and online settings. Our code and dataset are publicly available at: https://github.com/JD-GenX/AutoPP

</details>


### [73] [Data relativistic uncertainty framework for low-illumination anime scenery image enhancement](https://arxiv.org/abs/2512.21944)
*Yiquan Gao,John See*

Main category: cs.CV

TL;DR: 本研究提出DRU框架，利用数据不确定性增强动漫场景图像的低光照效果，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有低光照增强方法主要针对自然图像和视频，而动漫场景图像的领域差异尚未充分探索，本研究旨在填补这一空白。

Method: 研究首先构建了一个未配对的动漫场景数据集，并提出了数据相对不确定性（DRU）框架，通过动态调整目标函数来校准模型学习。

Result: 实验表明，DRU框架训练的EnlightenGANs版本在感知和美学质量上均超越了现有方法。

Conclusion: 本研究提出的DRU框架通过利用数据不确定性信息，显著提升了动漫场景图像的低光照增强效果，超越了现有方法，并展示了数据为中心学习的新范式在视觉和语言领域的潜力。

Abstract: By contrast with the prevailing works of low-light enhancement in natural images and videos, this study copes with the low-illumination quality degradation in anime scenery images to bridge the domain gap. For such an underexplored enhancement task, we first curate images from various sources and construct an unpaired anime scenery dataset with diverse environments and illumination conditions to address the data scarcity. To exploit the power of uncertainty information inherent with the diverse illumination conditions, we propose a Data Relativistic Uncertainty (DRU) framework, motivated by the idea from Relativistic GAN. By analogy with the wave-particle duality of light, our framework interpretably defines and quantifies the illumination uncertainty of dark/bright samples, which is leveraged to dynamically adjust the objective functions to recalibrate the model learning under data uncertainty. Extensive experiments demonstrate the effectiveness of DRU framework by training several versions of EnlightenGANs, yielding superior perceptual and aesthetic qualities beyond the state-of-the-art methods that are incapable of learning from data uncertainty perspective. We hope our framework can expose a novel paradigm of data-centric learning for potential visual and language domains. Code is available.

</details>


### [74] [Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials](https://arxiv.org/abs/2512.21948)
*Ali Lotfi,Adam Carter,Thuan Ha,Mohammad Meysami,Kwabena Nketia,Steve Shirtliffe*

Main category: cs.CV

TL;DR: 提出自动化方法生成紧凑光谱指数，用于植被分类，通过多项式组合和特征选择实现高精度（最高97.70%）且模型简单。


<details>
  <summary>Details</summary>
Motivation: 旨在自动化地找到紧凑的光谱指数，用于植被分类，同时保持遥感所需的照明不变性。

Method: 采用所有光谱波段的两两归一化差异，构建固定次数的多项式组合，形成结构化搜索空间。使用特征选择方法（ANOVA过滤、递归消除和L1正则化SVM）筛选出少量高精度指数。

Result: 在Kochia检测中，单个二次指数达到96.26%的准确率，使用八个指数提升至97.70%。所选特征均为b4至b8波段的二次乘积，表明判别信号来自光谱交互而非单一波段比率。

Conclusion: 该方法通过构建多项式组合的紧凑光谱指数，实现了高精度的植被分类，且模型简单易解释。

Abstract: We introduce an automated way to find compact spectral indices for vegetation classification. The idea is to take all pairwise normalized differences from the spectral bands and then build polynomial combinations up to a fixed degree, which gives a structured search space that still keeps the illumination invariance needed in remote sensing. For a sensor with $n$ bands this produces $\binom{n}{2}$ base normalized differences, and the degree-2 polynomial expansion gives 1,080 candidate features for the 10-band Sentinel-2 configuration we use here. Feature selection methods (ANOVA filtering, recursive elimination, and $L_1$-regularized SVM) then pick out small sets of indices that reach the desired accuracy, so the final models stay simple and easy to interpret. We test the framework on Kochia (\textit{Bassia scoparia}) detection using Sentinel-2 imagery from Saskatchewan, Canada ($N = 2{,}318$ samples, 2022--2024). A single degree-2 index, the product of two normalized differences from the red-edge bands, already reaches 96.26\% accuracy, and using eight indices only raises this to 97.70\%. In every case the chosen features are degree-2 products built from bands $b_4$ through $b_8$, which suggests that the discriminative signal comes from spectral \emph{interactions} rather than individual band ratios. Because the indices involve only simple arithmetic, they can be deployed directly in platforms like Google Earth Engine. The same approach works for other sensors and classification tasks, and an open-source implementation (\texttt{ndindex}) is available.

</details>


### [75] [Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2512.21964)
*Dunyuan XU,Xikai Yang,Yaoqian Li,Juzheng Miao,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 医疗MLLMs对噪声敏感，IMC框架通过PDC和SMS提升跨模态鲁棒性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用领域的MLLMs鲁棒性，且集中于文本模态，无法满足医学领域对复杂噪声模式和严格安全标准的需求。本研究旨在填补这一空白，系统分析噪声对医疗MLLMs的影响。

Method: 提出了无需微调的IMC框架，包括视觉模态的PDC（扰动感知去噪校准）和文本模态的SMS（自实例化多代理系统），通过原型引导特征校准和代理层次结构实现跨模态鲁棒性增强。

Result: 在包含11种噪声类型的基准测试中，IMC框架在多个模态上达到了最先进的性能，验证了其在真实临床场景中的潜力。

Conclusion: 本研究提出的IMC框架通过利用MLLMs固有的去噪能力，显著提升了医疗多模态大语言模型在真实临床场景中的鲁棒性，为未来的临床应用提供了潜在解决方案。

Abstract: Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.

</details>


### [76] [A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation](https://arxiv.org/abs/2512.21984)
*Qi Lai,JunYan Li,Qiang Cai,Lei Wang,Tao Yan,XiaoKun Liang*

Main category: cs.CV

TL;DR: LMSF-A 是一个轻量级实时脊柱内窥镜实例分割框架，通过多尺度注意力设计在保持高准确性的同时减少计算量，适用于手术场景。


<details>
  <summary>Details</summary>
Motivation: 脊柱内窥镜实例分割在手术中至关重要，但由于视野狭窄、镜面高光、烟雾/出血、边界模糊和大尺度变化等挑战，现有方法难以满足实时性和硬件限制需求。

Method: LMSF-A 结合了 C2f-Pro 模块（RepViT 风格的重参数化卷积与高效多尺度注意力）、SSFF 和 TFE 颈部模块（增强跨尺度一致性和边界细节），以及 LMSH 头部模块（共享卷积和 GroupNorm 以减少参数并支持 batch-1 稳定性）。

Result: LMSF-A 在各项评估指标中表现优异，仅需 1.8M 参数和 8.8 GFLOPs，比大多数实例分割方法更轻量，且在公开牙齿基准测试中泛化良好。

Conclusion: LMSF-A 是一个轻量级多尺度注意力框架，在保持高准确性的同时显著减少了参数和计算量，适用于实时脊柱内窥镜实例分割，并在小批量训练下保持稳定。

Abstract: Real-time instance segmentation for spinal endoscopy is important for identifying and protecting critical anatomy during surgery, but it is difficult because of the narrow field of view, specular highlights, smoke/bleeding, unclear boundaries, and large scale changes. Deployment is also constrained by limited surgical hardware, so the model must balance accuracy and speed and remain stable under small-batch (even batch-1) training. We propose LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head. The backbone uses a C2f-Pro module that combines RepViT-style re-parameterized convolution (RVB) with efficient multi-scale attention (EMA), enabling multi-branch training while collapsing into a single fast path for inference. The neck improves cross-scale consistency and boundary detail using Scale-Sequence Feature Fusion (SSFF) and Triple Feature Encoding (TFE), which strengthens high-resolution features. The head adopts a Lightweight Multi-task Shared Head (LMSH) with shared convolutions and GroupNorm to reduce parameters and support batch-1 stability. We also release the clinically reviewed PELD dataset (61 patients, 610 images) with instance masks for adipose tissue, bone, ligamentum flavum, and nerve. Experiments show that LMSF-A is highly competitive (or even better than) in all evaluation metrics and much lighter than most instance segmentation methods requiring only 1.8M parameters and 8.8 GFLOPs, and it generalizes well to a public teeth benchmark. Code and dataset: https://github.com/hhwmortal/PELD-Instance-segmentation.

</details>


### [77] [Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs](https://arxiv.org/abs/2512.21999)
*Jiayu Hu,Beibei Li,Jiangwei Xia,Yanjun Qin,Bing Ji,Zhongshi He*

Main category: cs.CV

TL;DR: ALEAHallu通过对抗性参数编辑框架有效缓解视觉语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型因过度依赖语言先验和视觉特征整合不足而产生幻觉问题，现有启发式解码校准策略因不可训练性而优化潜力有限。

Method: 采用激活-定位-编辑对抗性范式，首先构建包含正负样本的激活数据集，识别易产生幻觉的参数簇，并通过注入对抗性调优前缀的提示进行微调。

Result: 在生成和判别视觉语言模型任务中，ALEAHallu显著减轻了幻觉现象。

Conclusion: ALEAHallu框架通过对抗性参数编辑显著缓解了视觉语言模型中的幻觉问题，证明了其在生成和判别任务中的有效性。

Abstract: While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.

</details>


### [78] [iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception](https://arxiv.org/abs/2512.22009)
*Sarthak Mehrotra,Sairam V C Rebbapragada,Mani Hemanth Reddy Bonthu,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: iSHIFT 是一种轻量级 MLLM 代理，通过隐式思维和感知控制模块实现高效且高精度的 GUI 交互，性能媲美现有最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 在 GUI 交互中难以同时满足高效性和高精度需求，尤其是在需要精确视觉定位的任务中表现不佳。iSHIFT 旨在解决这一问题。

Method: iSHIFT 结合了隐式思维链（隐式 CoT）和感知控制模块，通过特殊感知令牌引导注意力到相关屏幕区域，支持模型在慢速模式（高精度）和快速模式（高效）之间切换。

Result: iSHIFT 在多个基准数据集上达到了与现有最先进模型相当的性能，尽管其模型规模仅为 2.5B。

Conclusion: iSHIFT 作为一种轻量级代理，通过整合隐式思维和感知控制模块，在保持高效的同时实现了高精度的 GUI 交互，其性能与现有最先进模型相当。

Abstract: Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.

</details>


### [79] [Patch-Discontinuity Mining for Generalized Deepfake Detection](https://arxiv.org/abs/2512.22027)
*Huanhuan Yuan,Yang Ping,Zhengqin Xu,Junyi Cao,Shuai Jia,Chao Ma*

Main category: cs.CV

TL;DR: GenDF是一个简单高效的框架，通过特定表示学习和特征增强策略，在深度伪造检测任务中实现了优异的跨域泛化性能，且参数极少。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展使得高度逼真的伪造面部图像成为可能，对个人隐私和在线信息完整性构成严重威胁。现有深度伪造检测方法依赖手工制作的取证线索和复杂架构，在域内设置中表现良好，但在面对未见伪造模式时性能显著下降。

Method: GenDF采用紧凑简洁的网络设计，将强大大规模视觉模型迁移至深度伪造检测任务，结合深度伪造特定表示学习、特征空间重分布和分类不变特征增强策略。

Result: GenDF在跨域和跨操作设置中实现了最先进的泛化性能，仅需0.28M可训练参数。

Conclusion: GenDF框架通过深度伪造特定表示学习、特征空间重分布和分类不变特征增强策略，在跨域和跨操作设置中实现了最先进的泛化性能，同时仅需0.28M可训练参数，验证了其有效性和高效性。

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.

</details>


### [80] [Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models](https://arxiv.org/abs/2512.22046)
*Zongmin Zhang,Zhen Sun,Yifan Liao,Wenhan Dong,Xinlei He,Xingshuo Han,Shengmin Xu,Xinyi Huang*

Main category: cs.CV

TL;DR: BadVSFM是针对VSFMs的首个后门框架，通过两阶段策略实现高效攻击，揭示了当前模型的漏洞。


<details>
  <summary>Details</summary>
Motivation: 经典后门攻击在VSFMs中效果不佳（ASR低于5%），研究发现传统训练中梯度对齐和注意力集中在真实对象上，导致无法学习触发器相关表示。

Method: BadVSFM采用两阶段策略：首先调整图像编码器，使触发帧映射到目标嵌入，而干净帧保持与参考编码器对齐；其次训练掩码解码器，使触发帧-提示对生成共享目标掩码，同时保持干净输出接近参考解码器。

Result: BadVSFM在两种数据集和五种VSFMs上表现出强大的可控后门效果，同时保持干净分割质量。梯度冲突分析和注意力可视化证实了其有效性，且四种代表性防御方法对其无效。

Conclusion: BadVSFM框架成功地在VSFMs中实现了高效且可控的后门攻击，揭示了当前VSFMs中未被充分探索的漏洞。

Abstract: Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.

</details>


### [81] [MAI-UI Technical Report: Real-World Centric Foundation GUI Agents](https://arxiv.org/abs/2512.22047)
*Hanzhang Zhou,Xu Zhang,Panrong Tong,Jianan Zhang,Liangyu Chen,Quyu Kong,Chenglin Cai,Chen Liu,Yue Wang,Jingren Zhou,Steven Hoi*

Main category: cs.CV

TL;DR: MAI-UI是一系列基础GUI代理，通过自演进数据管道和在线RL框架解决了GUI代理部署的四大挑战，并在多个基准测试中创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 开发GUI代理可能彻底改变下一代人机交互，但面临缺乏原生代理-用户交互、UI操作限制、缺乏实际部署架构和动态环境脆弱性等挑战。

Method: MAI-UI采用自演进数据管道、原生设备-云协作系统和在线强化学习框架，扩展了导航数据并优化了并行环境和上下文长度。

Result: MAI-UI在多个基准测试中表现优异，如ScreenSpot-Pro（73.5%）、MMBench GUI L2（91.3%），并显著提升了设备性能和用户隐私保护。

Conclusion: MAI-UI通过统一的方法论解决了GUI代理部署中的四大挑战，并在GUI基础和移动导航方面建立了新的最先进水平。

Abstract: The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.

</details>


### [82] [Yume-1.5: A Text-Controlled Interactive World Generation Model](https://arxiv.org/abs/2512.22096)
*Xiaofeng Mao,Zhen Li,Chuanhao Li,Xiaojie Xu,Kaining Ying,Tong He,Jiangmiao Pang,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: \method 是一种新框架，通过三个核心组件解决了现有扩散模型在生成交互式世界时的挑战，支持文本控制和实时探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在参数大小、推理步骤和历史上下文方面存在严重限制，影响了实时性能并缺乏文本控制生成能力。

Method: \method 框架包含三个核心组件：(1) 统一上下文压缩与线性注意力的长视频生成框架；(2) 双向注意力蒸馏和增强文本嵌入方案支持的实时流加速策略；(3) 用于生成世界事件的文本控制方法。

Result: \method 能够从单张图像或文本提示生成真实、交互和连续的世界，支持基于键盘的探索。

Conclusion: \method 提出了一种新颖的框架，通过精心设计的组件解决了现有方法在参数大小、推理步骤和历史上下文方面的挑战，实现了从单张图像或文本提示生成真实、交互和连续的世界。

Abstract: Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.

</details>


### [83] [Learning Association via Track-Detection Matching for Multi-Object Tracking](https://arxiv.org/abs/2512.22105)
*Momir Adžemović*

Main category: cs.CV

TL;DR: TDLP是一种基于检测的跟踪方法，通过链接预测进行帧间关联，结合了几何特征和额外线索，无需手工规则且计算高效，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标跟踪方法分为两类：基于检测的跟踪方法计算高效但依赖手工关联启发式规则，端到端方法虽然能从数据中学习关联但计算复杂度高。TDLP旨在结合两者的优点，既直接从数据中学习关联，又保持模块化和计算高效。

Method: TDLP是一种基于检测的跟踪方法，通过预测轨迹和检测之间的链接来进行帧间关联，主要利用几何特征（如边界框），并可选择性地结合姿态和外观等额外线索。

Result: TDLP在多个基准测试中 consistently 超越现有最先进的基于检测和端到端跟踪方法。

Conclusion: TDLP在多种基准测试中表现优于现有的跟踪方法，特别是在处理异构特征时，链接预测比基于度量学习的关联更有效。

Abstract: Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.

</details>


### [84] [ProEdit: Inversion-based Editing From Prompts Done Right](https://arxiv.org/abs/2512.22118)
*Zhi Ouyang,Dian Zheng,Xiao-Ming Wu,Jian-Jian Jiang,Kun-Yu Lin,Jingke Meng,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ProEdit通过混合KV特征和扰动潜在空间，解决了现有方法过度依赖源信息的问题，实现了高效的图像和视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在采样过程中过度依赖源图像信息，导致目标图像编辑效果不佳（如无法按指令改变主体属性）。

Method: 在注意力层面引入KV-mix混合源和目标KV特征，在潜在空间层面提出Latents-Shift扰动源潜在空间，消除反转潜在空间对采样的影响。

Result: 在多个图像和视频编辑基准测试中达到SOTA性能，并能无缝集成到现有方法中。

Conclusion: ProEdit通过KV-mix和Latents-Shift技术，在保持背景一致性的同时有效减少源图像对编辑区域的负面影响，实现了图像和视频编辑的SOTA性能，并具备即插即用的特性。

Abstract: Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.

</details>


### [85] [See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning](https://arxiv.org/abs/2512.22120)
*Shuoshuo Zhang,Yizhen Zhang,Jingjing Fu,Lei Song,Jiang Bian,Yujiu Yang,Rui Wang*

Main category: cs.CV

TL;DR: BiPS通过双向感知塑造提升视觉语言模型性能，解决了细粒度视觉证据和跨领域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在细粒度视觉证据利用、跨领域泛化和推理成本方面存在不足。

Method: BiPS采用KL一致性约束和KL分离约束，分别确保对支持像素的完整覆盖和防止仅依赖文本的捷径。

Result: 在八个基准测试中，BiPS平均提升Qwen2.5-VL-7B性能8.2%，并在未见过的数据集和图像类型上表现出色。

Conclusion: BiPS通过双向感知塑造显著提升了大型视觉语言模型的性能，尤其在跨领域泛化能力上表现突出。

Abstract: Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks](https://arxiv.org/abs/2512.21375)
*Yuanshuang Fu,Qianyao Wang,Qihao Wang,Bonan Zhang,Jiaxin Zhao,Yiming Cao,Zhijun Li*

Main category: cs.RO

TL;DR: 本文提出了一种无人机动态光影干扰避障的主动路径规划方法，通过改进IFDS算法和MPC框架，显著提高了避障成功率和数据质量。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，如阴影和镜面反射（太阳耀光）等变化的光照条件会导致严重的光谱失真，从而降低数据可用性。为了在确保飞行安全的同时最大限度地获取高质量数据，本文提出了一种动态光影干扰避障的主动路径规划方法。

Method: 首先构建动态预测模型，将时变的光影干扰区域转化为三维虚拟障碍物；其次引入改进的干扰流体动力学系统（IFDS）算法，通过构建排斥力场生成平滑的初始避障路径；随后采用模型预测控制（MPC）框架进行滚动时域路径优化，以处理飞行动力学约束并实现实时轨迹跟踪；此外设计了动态飞行高度调整（DFAA）机制，在可观测区域狭窄时主动降低飞行高度以提高空间分辨率。

Result: 仿真结果表明，与传统PID和单一避障算法相比，所提方法在密集干扰场景中的避障成功率达到98%，显著提高了路径平滑度，并将有效观测数据量提升了约27%。

Conclusion: 本研究为复杂光照环境下精确的无人机水质监测提供了有效的工程解决方案。

Abstract: Unmanned Aerial Vehicle (UAV) spectral remote sensing technology is widely used in water quality monitoring. However, in dynamic environments, varying illumination conditions, such as shadows and specular reflection (sun glint), can cause severe spectral distortion, thereby reducing data availability. To maximize the acquisition of high-quality data while ensuring flight safety, this paper proposes an active path planning method for dynamic light and shadow disturbance avoidance. First, a dynamic prediction model is constructed to transform the time-varying light and shadow disturbance areas into three-dimensional virtual obstacles. Second, an improved Interfered Fluid Dynamical System (IFDS) algorithm is introduced, which generates a smooth initial obstacle avoidance path by building a repulsive force field. Subsequently, a Model Predictive Control (MPC) framework is employed for rolling-horizon path optimization to handle flight dynamics constraints and achieve real-time trajectory tracking. Furthermore, a Dynamic Flight Altitude Adjustment (DFAA) mechanism is designed to actively reduce the flight altitude when the observable area is narrow, thereby enhancing spatial resolution. Simulation results show that, compared with traditional PID and single obstacle avoidance algorithms, the proposed method achieves an obstacle avoidance success rate of 98% in densely disturbed scenarios, significantly improves path smoothness, and increases the volume of effective observation data by approximately 27%. This research provides an effective engineering solution for precise UAV water quality monitoring in complex illumination environments.

</details>


### [87] [Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction](https://arxiv.org/abs/2512.21398)
*Rahul Moorthy Mahesh,Oguzhan Goktug Poyrazoglu,Yukang Cao,Volkan Isler*

Main category: cs.RO

TL;DR: PaceForecaster利用副驾驶指令和局部传感器数据预测环境并生成子目标，提升机器人导航性能36%。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，机器人导航常面临安全与速度的权衡，本研究探讨副驾驶指令是否能帮助机器人更果断且安全地规划路径。

Method: 研究提出了PaceForecaster方法，该方法结合局部传感器数据（Level-1）和副驾驶指令，预测可见区域地图（Level-2）并生成指令条件子目标，指导机器人导航。

Result: 实验表明，PaceForecaster结合Log-MPPI控制器，在多边形环境中导航性能提升了36%。

Conclusion: PaceForecaster通过整合副驾驶指令和局部传感器数据，显著提升了机器人在复杂环境中的导航性能，证明了语言条件预测和目标指导的有效性。

Abstract: In cluttered environments, motion planners often face a trade-off between safety and speed due to uncertainty caused by occlusions and limited sensor range. In this work, we investigate whether co-pilot instructions can help robots plan more decisively while remaining safe. We introduce PaceForecaster, as an approach that incorporates such co-pilot instructions into local planners. PaceForecaster takes the robot's local sensor footprint (Level-1) and the provided co-pilot instructions as input and predicts (i) a forecasted map with all regions visible from Level-1 (Level-2) and (ii) an instruction-conditioned subgoal within Level-2. The subgoal provides the planner with explicit guidance to exploit the forecasted environment in a goal-directed manner. We integrate PaceForecaster with a Log-MPPI controller and demonstrate that using language-conditioned forecasts and goals improves navigation performance by 36% over a local-map-only baseline while in polygonal environments.

</details>


### [88] [Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments](https://arxiv.org/abs/2512.21425)
*Hang Zhou,Yuhui Zhai,Shiyu Shen,Yanfeng Ouyang,Xiaowei Shi,Xiaopeng*

Main category: cs.RO

TL;DR: 本研究首次通过理论分析和物理实验构建了城市空中交通的基本图，验证了地面交通经典理论在UAM中的适用性，并强调了实验验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机密度增加，城市空中交通可能面临类似地面交通的拥堵问题，但目前对其交通流特性缺乏深入理解。

Method: 结合理论分析和物理实验，设计无人机避碰控制律，开发基于仿真的交通生成方法，并利用Edie定义构建基本图。

Result: 构建了UAM交通的基本图，发现仿真与物理实验结果存在偏差，验证了实验验证的必要性。

Conclusion: 研究表明，地面交通的经典基本图结构同样适用于城市空中交通系统，物理实验验证了仿真结果的偏差，强调了实验验证的重要性。

Abstract: Urban Air Mobility (UAM) is an emerging application of unmanned aerial vehicles (UAVs) that promises to reduce travel time and alleviate congestion in urban transportation systems. As drone density increases, UAM operations are expected to experience congestion similar to that in ground traffic. However, the fundamental characteristics of UAM traffic flow, particularly under real-world operating conditions, remain poorly understood. This study proposes a general framework for constructing the fundamental diagram (FD) of UAM traffic by integrating theoretical analysis with physical experiments. To the best of our knowledge, this is the first study to derive a UAM FD using real-world physical test data. On the theoretical side, we design two drone control laws for collision avoidance and develop simulation-based traffic generation methods to produce diverse UAM traffic scenarios. Based on Edie's definition, traffic flow theory is then applied to construct the FD and characterize the macroscopic properties of UAM traffic. To account for real-world disturbances and modeling uncertainties, we further conduct physical experiments on a reduced-scale testbed using Bitcraze Crazyflie drones. Both simulation and physical test trajectory data are collected and organized into the UAMTra2Flow dataset, which is analyzed using the proposed framework. Preliminary results indicate that classical FD structures for ground transportation are also applicable to UAM systems. Notably, FD curves obtained from physical experiments exhibit deviations from simulation-based results, highlighting the importance of experimental validation. Finally, results from the reduced-scale testbed are scaled to realistic operating conditions to provide practical insights for future UAM traffic systems. The dataset and code for this paper are publicly available at https://github.com/CATS-Lab/UAM-FD.

</details>


### [89] [EVE: A Generator-Verifier System for Generative Policies](https://arxiv.org/abs/2512.21430)
*Yusuf Ali,Gryphon Patlin,Karthik Kothuri,Muhammad Zubair Irshad,Wuwei Liang,Zsolt Kira*

Main category: cs.RO

TL;DR: EVE是一个生成-验证框架，通过零-shot VLM验证器提升预训练生成策略的性能，无需额外训练，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成策略在分布偏移下性能下降，而语言模型领域通过推理时计算提升性能的方法尚未在生成策略中系统探索。

Method: EVE框架通过将预训练生成策略与多个零-shot VLM验证器结合，验证器提出动作改进建议，并通过动作整合器融合这些建议以生成最终执行动作。

Result: EVE在多样化操作任务中持续提升任务成功率，验证了生成-验证框架的有效性。

Conclusion: EVE框架通过生成-验证机制显著提升了预训练生成策略的性能，无需额外训练，为具身控制提供了可扩展、模块化的解决方案。

Abstract: Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.

</details>


### [90] [Planetary Terrain Datasets and Benchmarks for Rover Path Planning](https://arxiv.org/abs/2512.21438)
*Marvin Chancán,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文提出两个行星路径规划基准数据集，评估经典与学习算法，发现经典算法在复杂地形表现优异，学习模型泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有行星任务数据在路径规划和自主导航研究中未被充分利用，缺乏基于空间任务的行星数据集、标准化基准和评估协议。

Method: 提出了两个大型平面基准数据集MarsPlanBench和MoonPlanBench，并在统一框架下设置了经典和学习的路径规划算法进行评估。

Result: 经典算法在挑战性地形上表现优异，而学习模型在行星领域泛化能力有限。

Conclusion: 经典算法在复杂地形（如月球南北极）上平均可实现100%的全局路径规划成功率，而基于学习的模型在行星领域泛化能力仍有不足。

Abstract: Planetary rover exploration is attracting renewed interest with several upcoming space missions to the Moon and Mars. However, a substantial amount of data from prior missions remain underutilized for path planning and autonomous navigation research. As a result, there is a lack of space mission-based planetary datasets, standardized benchmarks, and evaluation protocols. In this paper, we take a step towards coordinating these three research directions in the context of planetary rover path planning. We propose the first two large planar benchmark datasets, MarsPlanBench and MoonPlanBench, derived from high-resolution digital terrain images of Mars and the Moon. In addition, we set up classical and learned path planning algorithms, in a unified framework, and evaluate them on our proposed datasets and on a popular planning benchmark. Through comprehensive experiments, we report new insights on the performance of representative path planning algorithms on planetary terrains, for the first time to the best of our knowledge. Our results show that classical algorithms can achieve up to 100% global path planning success rates on average across challenging terrains such as Moon's north and south poles. This suggests, for instance, why these algorithms are used in practice by NASA. Conversely, learning-based models, although showing promising results in less complex environments, still struggle to generalize to planetary domains. To serve as a starting point for fundamental path planning research, our code and datasets will be released at: https://github.com/mchancan/PlanetaryPathBench.

</details>


### [91] [Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment](https://arxiv.org/abs/2512.21497)
*Siddhartha Upadhyay,Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 扩展STT框架解决PrT-RAS任务，提供实时管合成和概率安全保证，控制器高效且无需模型/优化，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中不确定障碍物的概率时间可达-避障-停留任务，提供实时且形式化的安全保证。

Method: 开发了一种实时管合成程序，明确考虑了时变不确定障碍物，并提供了形式化的概率安全保证。STT被建模为状态空间中的时变球，其中心和半径基于不确定的传感器信息在线演化。推导了一个闭式、无近似的控制律，将系统轨迹限制在管内。

Result: 该方法提供了概率避障和有限时间任务完成的正式保证，控制器无需模型、近似或优化，实现了高效实时执行并保证收敛到目标。仿真和硬件实验验证了框架的有效性和可扩展性。

Conclusion: 该研究通过扩展时空管（STT）框架，成功解决了动态环境中不确定障碍物的概率时间可达-避障-停留（PrT-RAS）任务，提供了实时管合成方法，并确保了概率安全性和任务完成。

Abstract: In this work, we extend the Spatiotemporal Tube (STT) framework to address Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in dynamic environments with uncertain obstacles. We develop a real-time tube synthesis procedure that explicitly accounts for time-varying uncertain obstacles and provides formal probabilistic safety guarantees. The STT is formulated as a time-varying ball in the state space whose center and radius evolve online based on uncertain sensory information. We derive a closed-form, approximation-free control law that confines the system trajectory within the tube, ensuring both probabilistic safety and task satisfaction. Our method offers a formal guarantee for probabilistic avoidance and finite-time task completion. The resulting controller is model-free, approximation-free, and optimization-free, enabling efficient real-time execution while guaranteeing convergence to the target. The effectiveness and scalability of the framework are demonstrated through simulation studies and hardware experiments on mobile robots, a UAV, and a 7-DOF manipulator navigating in cluttered and uncertain environments.

</details>


### [92] [A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming](https://arxiv.org/abs/2512.21534)
*Congrui Bai,Zhenting Du,Weibang Bai*

Main category: cs.RO

TL;DR: 论文提出了一种螺旋结构的静电层夹紧可变刚度机制（HWS-ELJ），通过实验验证其在机器人手指设计中的高效性和可行性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种新型可变刚度机制，以解决传统平面ELJ在刚度调节和空间占用方面的局限性。

Method: 提出了一种新型可变刚度机制HWS-ELJ，利用静电吸引力增强层间摩擦，抑制相对滑动，实现可调刚度。与传统平面ELJ相比，螺旋结构在相同电极接触面积下显著提升了刚度调节能力。

Result: 实验结果表明HWS-ELJ的设计符合理论推导趋势，且机器人手指原型成功实现了电压驱动的刚度调节。

Conclusion: 该论文通过实验验证了HWS-ELJ设计的合理性及其在机器人手指刚度调节中的有效性，展示了电压驱动刚度调节的可行性。

Abstract: This paper introduces a novel variable stiffness mechanism termed Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) and systematically investigates its potential applications in variable stiffness robotic finger design. The proposed method utilizes electrostatic attraction to enhance interlayer friction, thereby suppressing relative sliding and enabling tunable stiffness. Compared with conventional planar ELJ, the helical configuration of HWS-ELJ provides exponentially increasing stiffness adjustment with winding angle, achieving significantly greater stiffness enhancement for the same electrode contact area while reducing the required footprint under equivalent stiffness conditions. Considering the practical advantage of voltage-based control, a series of experimental tests under different initial force conditions were conducted to evaluate the stiffness modulation characteristics of HWS-ELJ. The results demonstrated its rational design and efficacy, with outcomes following the deduced theoretical trends. Furthermore, a robotic finger prototype integrating HWS-ELJ was developed, demonstrating voltage-driven stiffness modulation and confirming the feasibility of the proposed robotic variable stiffness mechanism.

</details>


### [93] [World-Coordinate Human Motion Retargeting via SAM 3D Body](https://arxiv.org/abs/2512.21573)
*Zhangzheng Tu,Kailun Su,Shaolong Zhu,Yukun Zheng*

Main category: cs.RO

TL;DR: 提出了一种轻量级框架，从单目视频中恢复适用于机器人的人体运动，通过结构化表示和物理约束实现稳定效果。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中恢复世界坐标系下的人体运动对于具身智能和机器人技术具有重要意义，但现有的SLAM流程或复杂的时间模型过于复杂和笨重。

Method: 提出了一种轻量级、工程导向的框架，利用SAM 3D Body（3DB）作为冻结感知骨干，并使用Momentum HumanRig（MHR）表示作为机器人友好的中间表示。方法包括锁定身份和骨骼尺度参数、在低维MHR潜在空间中进行滑动窗口优化，以及通过可微分软脚-地面接触模型恢复物理上合理的全局根轨迹。

Result: 在真实单目视频上的实验表明，我们的方法具有稳定的世界轨迹和可靠的机器人重定向效果。

Conclusion: 通过结构化人体表示和轻量级物理约束，我们的方法能够从单目视频输入中生成适用于机器人的运动。

Abstract: Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.

</details>


### [94] [AstraNav-Memory: Contexts Compression for Long Memory](https://arxiv.org/abs/2512.21627)
*Botao Ren,Junjun Hu,Xinda Xue,Minghua Luo,Jintao Chen,Haochen Bai,Liangliang You,Mu Xu*

Main category: cs.RO

TL;DR: 提出图像中心记忆框架，通过高效视觉压缩与导航策略结合，提升终身导航性能，实验证明其在陌生和熟悉环境中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 终身具身导航需要智能体积累、保留和利用跨任务的空间语义经验，以在陌生环境中高效探索并在熟悉环境中快速到达目标。虽然对象中心记忆可解释，但其依赖检测和重建管道，限制了鲁棒性和可扩展性。

Method: 提出了一种图像中心记忆框架，通过高效的视觉上下文压缩模块与基于Qwen2.5-VL的导航策略端到端耦合，实现长期隐式记忆。采用ViT骨干网络和轻量级PixelUnshuffle+Conv块，支持可配置的压缩率。

Result: 在GOAT-Bench和HM3D-OVON上的实验结果表明，该方法实现了最先进的导航性能，提高了陌生环境中的探索效率并缩短了熟悉环境中的路径。消融研究进一步表明，适度压缩在效率和准确性之间提供了最佳平衡。

Conclusion: 压缩的图像中心记忆框架为终身具身智能体提供了一种实用且可扩展的接口，使其能够推理长期的视觉历史并以类似人类的效率导航。

Abstract: Lifelong embodied navigation requires agents to accumulate, retain, and exploit spatial-semantic experience across tasks, enabling efficient exploration in novel environments and rapid goal reaching in familiar ones. While object-centric memory is interpretable, it depends on detection and reconstruction pipelines that limit robustness and scalability. We propose an image-centric memory framework that achieves long-term implicit memory via an efficient visual context compression module end-to-end coupled with a Qwen2.5-VL-based navigation policy. Built atop a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, our visual tokenizer supports configurable compression rates; for example, under a representative 16$\times$ compression setting, each image is encoded with about 30 tokens, expanding the effective context capacity from tens to hundreds of images. Experimental results on GOAT-Bench and HM3D-OVON show that our method achieves state-of-the-art navigation performance, improving exploration in unfamiliar environments and shortening paths in familiar ones. Ablation studies further reveal that moderate compression provides the best balance between efficiency and accuracy. These findings position compressed image-centric memory as a practical and scalable interface for lifelong embodied agents, enabling them to reason over long visual histories and navigate with human-like efficiency.

</details>


### [95] [Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning](https://arxiv.org/abs/2512.21654)
*Zikun Guo,Adeyinka P. Adedigba,Rammohan Mallipeddi,Heoncheol Lee*

Main category: cs.RO

TL;DR: 该论文提出了一种结合结构先验和负载感知目标的多机器人路径规划框架，通过改进的ACO算法在复杂环境中实现了更优的路径紧凑性和公平性。


<details>
  <summary>Details</summary>
Motivation: 多机器人路径规划因其组合复杂性和需平衡全局效率与公平任务分配的挑战而难以扩展，传统群智能方法在复杂环境中易早熟收敛，亟需新方法解决。

Method: 该方法将结构先验集成到蚁群优化（ACO）的搜索过程中，利用任务的空间分布初始化结构先验，并通过改进的信息素更新规则强调结构上有意义的连接，同时结合负载感知目标平衡总行程和个体机器人工作量。

Result: 在多种基准场景下的实验表明，该方法在路径紧凑性、稳定性和工作量分配上均优于代表性元启发式基线，适用于物流、监控和搜救等大规模协调场景。

Conclusion: 该论文提出的结构诱导探索框架通过整合空间结构先验和负载感知目标，显著提升了多机器人路径规划的效率和公平性，为大规模协调应用提供了可扩展且可解释的解决方案。

Abstract: Multi-robot path planning is a fundamental yet challenging problem due to its combinatorial complexity and the need to balance global efficiency with fair task allocation among robots. Traditional swarm intelligence methods, although effective on small instances, often converge prematurely and struggle to scale to complex environments. In this work, we present a structure-induced exploration framework that integrates structural priors into the search process of the ant colony optimization (ACO). The approach leverages the spatial distribution of the task to induce a structural prior at initialization, thereby constraining the search space. The pheromone update rule is then designed to emphasize structurally meaningful connections and incorporates a load-aware objective to reconcile the total travel distance with individual robot workload. An explicit overlap suppression strategy further ensures that tasks remain distinct and balanced across the team. The proposed framework was validated on diverse benchmark scenarios covering a wide range of instance sizes and robot team configurations. The results demonstrate consistent improvements in route compactness, stability, and workload distribution compared to representative metaheuristic baselines. Beyond performance gains, the method also provides a scalable and interpretable framework that can be readily applied to logistics, surveillance, and search-and-rescue applications where reliable large-scale coordination is essential.

</details>


### [96] [MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning](https://arxiv.org/abs/2512.21722)
*Zishuo Wang,Xinyu Zhang,Zhuonan Liu,Tomohito Kawabata,Daeun Song,Xuesu Xiao,Ling Xiao*

Main category: cs.RO

TL;DR: MAction-SocialNav是一个高效的视觉语言模型，通过元认知提示处理社交导航中的动作模糊性，实验显示其在决策质量和安全对齐上优于GPT-4o和Claude。


<details>
  <summary>Details</summary>
Motivation: 社交导航中规范模糊，现有方法假设单一正确动作，限制了处理现实世界社交不确定性的能力。

Method: 提出了MAction-SocialNav，一个高效的视觉语言模型，通过引入元认知提示（MCP）方法增强模型推理能力，并创建了一个多动作社交导航数据集进行评估。

Result: 与GPT-4o和Claude相比，MAction-SocialNav在决策质量（APG: 0.595 vs. 0.000/0.025）和安全对齐（ER: 0.264 vs. 0.642/0.668）上显著提升，同时保持实时效率（1.524 FPS）。

Conclusion: MAction-SocialNav在社交推理性能上表现优异，同时保持高效率，展现了其在现实世界人机导航中的潜力。

Abstract: Socially compliant navigation requires robots to move safely and appropriately in human-centered environments by respecting social norms. However, social norms are often ambiguous, and in a single scenario, multiple actions may be equally acceptable. Most existing methods simplify this problem by assuming a single correct action, which limits their ability to handle real-world social uncertainty. In this work, we propose MAction-SocialNav, an efficient vision language model for socially compliant navigation that explicitly addresses action ambiguity, enabling generating multiple plausible actions within one scenario. To enhance the model's reasoning capability, we introduce a novel meta-cognitive prompt (MCP) method. Furthermore, to evaluate the proposed method, we curate a multi-action socially compliant navigation dataset that accounts for diverse conditions, including crowd density, indoor and outdoor environments, and dual human annotations. The dataset contains 789 samples, each with three-turn conversation, split into 710 training samples and 79 test samples through random selection. We also design five evaluation metrics to assess high-level decision precision, safety, and diversity. Extensive experiments demonstrate that the proposed MAction-SocialNav achieves strong social reasoning performance while maintaining high efficiency, highlighting its potential for real-world human robot navigation. Compared with zero-shot GPT-4o and Claude, our model achieves substantially higher decision quality (APG: 0.595 vs. 0.000/0.025) and safety alignment (ER: 0.264 vs. 0.642/0.668), while maintaining real-time efficiency (1.524 FPS, over 3x faster).

</details>


### [97] [HELP: Hierarchical Embodied Language Planner for Household Tasks](https://arxiv.org/abs/2512.21723)
*Alexandr V. Korchemnyi,Anatoly O. Onishchenko,Eva A. Bakaeva,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.RO

TL;DR: HELP是一种分层LLM代理架构，用于复杂场景规划，尤其在家庭任务中表现优异，支持小参数开源模型。


<details>
  <summary>Details</summary>
Motivation: 利用LLM处理语言模糊性、环境信息检索及基于代理技能的能力，需设计合适架构。

Method: 提出了一种名为HELP的分层LLM代理架构，每个代理专注于解决不同子任务。

Result: 在家庭任务中进行了真实实验，验证了HELP的有效性。

Conclusion: HELP架构通过分层LLM代理有效解决了复杂场景中的规划问题，尤其在家庭任务中表现出色，且适用于开源小参数模型。

Abstract: Embodied agents tasked with complex scenarios, whether in real or simulated environments, rely heavily on robust planning capabilities. When instructions are formulated in natural language, large language models (LLMs) equipped with extensive linguistic knowledge can play this role. However, to effectively exploit the ability of such models to handle linguistic ambiguity, to retrieve information from the environment, and to be based on the available skills of an agent, an appropriate architecture must be designed. We propose a Hierarchical Embodied Language Planner, called HELP, consisting of a set of LLM-based agents, each dedicated to solving a different subtask. We evaluate the proposed approach on a household task and perform real-world experiments with an embodied agent. We also focus on the use of open source LLMs with a relatively small number of parameters, to enable autonomous deployment.

</details>


### [98] [MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction](https://arxiv.org/abs/2512.21853)
*Kentaro Uno,Elian Neppel,Gustavo H. Diaz,Ashutosh Mishra,Shamistan Karimov,A. Sejal Jain,Ayesha Habib,Pascal Pama,Hazal Gozbasi,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了模块化可重构机器人MoonBot的设计与开发，通过模拟月球任务验证其功能，并总结了连接器设计的经验教训。


<details>
  <summary>Details</summary>
Motivation: 月球表面探索与开发的吸引力日益增长，机器人对于探索未知地形、利用当地资源及建设未来人类栖息地至关重要。

Method: 介绍了MoonBot的设计与开发过程，并通过初步现场演示验证了概念证明，执行了模拟月球基础设施建立的里程碑任务。

Result: MoonBot成功执行了包括土木工程操作、基础设施组件运输与部署以及与充气模块的辅助操作等任务，验证了其功能性和适应性。

Conclusion: 本文总结了MoonBot在模拟月球基础设施建立任务中的表现，重点分析了连接器设计的经验教训，为未来月球任务中模块化机器人系统的发展提供了宝贵见解。

Abstract: The allure of lunar surface exploration and development has recently captured widespread global attention. Robots have proved to be indispensable for exploring uncharted terrains, uncovering and leveraging local resources, and facilitating the construction of future human habitats. In this article, we introduce the modular and on-demand reconfigurable robot (MoonBot), a modular and reconfigurable robotic system engineered to maximize functionality while operating within the stringent mass constraints of lunar payloads and adapting to varying environmental conditions and task requirements. This article details the design and development of MoonBot and presents a preliminary field demonstration that validates the proof of concept through the execution of milestone tasks simulating the establishment of lunar infrastructure. These tasks include essential civil engineering operations, infrastructural component transportation and deployment, and assistive operations with inflatable modules. Furthermore, we systematically summarize the lessons learned during testing, focusing on the connector design and providing valuable insights for the advancement of modular robotic systems in future lunar missions.

</details>


### [99] [Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking](https://arxiv.org/abs/2512.21882)
*Kenta Iizuka,Akiyoshi Uchida,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 论文提出了一种结合动态禁入球体和非线性优化的轨迹规划方法，用于安全接近自由旋转空间碎片，并开发了相应的控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决空间碎片清除任务中，机器人操纵器安全接近自由旋转目标的挑战。

Method: 采用非线性优化方法进行轨迹规划，引入动态禁入球体以适应不同接近条件，并开发了基于离散ON/OFF推进器的控制策略。

Result: 成功实现了对自由旋转目标的近距离安全接近，并验证了控制策略的可行性。

Conclusion: 该论文提出了一种基于非线性优化的轨迹规划方法，结合动态禁入球体和控制策略，实现了对自由旋转空间碎片的近距离安全接近，为后续捕获奠定了基础。

Abstract: Approaching a tumbling target safely is a critical challenge in space debris removal missions utilizing robotic manipulators onboard servicing satellites. In this work, we propose a trajectory planning method based on nonlinear optimization for a close-range rendezvous to bring a free-floating, rotating debris object in a two-dimensional plane into the manipulator's workspace, as a preliminary step for its capture. The proposed method introduces a dynamic keep-out sphere that adapts depending on the approach conditions, allowing for closer and safer access to the target. Furthermore, a control strategy is developed to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.

</details>


### [100] [Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications](https://arxiv.org/abs/2512.21886)
*Akiyoshi Uchida,Antonine Richard,Kentaro Uno,Miguel Olivares-Mendez,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 该论文提出了一种适用于自由浮动基座机器人的惯性参数在线识别方法，通过结合动量守恒原理，在数值模拟中验证了其准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 掌握被抓取物体的惯性参数对于动态感知操作至关重要，尤其是在自由浮动基座的空间机器人领域。

Method: 应用并扩展了一种现有的在线识别方法，通过结合动量守恒原理，使其适用于自由浮动基座的机器人。

Result: 数值模拟结果表明，该方法能够准确识别惯性参数，与真实值相比具有高精度。

Conclusion: 该方法在数值模拟中验证了其有效性，能够准确识别未知目标物体的惯性参数，适用于在轨服务和其他空间任务。

Abstract: Knowing the inertia parameters of a grasped object is crucial for dynamics-aware manipulation, especially in space robotics with free-floating bases. This work addresses the problem of estimating the inertia parameters of an unknown target object during manipulation. We apply and extend an existing online identification method by incorporating momentum conservation, enabling its use for the floating-base robots. The proposed method is validated through numerical simulations, and the estimated parameters are compared with ground-truth values. Results demonstrate accurate identification in the scenarios, highlighting the method's applicability to on-orbit servicing and other space missions.

</details>


### [101] [Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space](https://arxiv.org/abs/2512.21887)
*Weichen Zhang,Peizhi Tang,Xin Zeng,Fanhang Man,Shiquan Yu,Zichao Dai,Baining Zhao,Hongjin Chen,Yu Shang,Wei Wu,Chen Gao,Xinlei Chen,Xin Wang,Yong Li,Wenwu Zhu*

Main category: cs.RO

TL;DR: ANWM是一种结合高级语义和导航实用性的无人机导航模型，通过未来帧投影模块提升长距离视觉预测和导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有导航策略通常仅优化低级目标（如避障和轨迹平滑），缺乏将高级语义融入规划的能力，因此需要一种能结合语义和导航实用性的新方法。

Method: 提出ANWM（空中导航世界模型），利用4-DoF无人机轨迹训练，并引入物理启发的未来帧投影（FFP）模块，以提供几何先验并减少长距离视觉生成的不确定性。

Result: ANWM在长距离视觉预测和无人机导航成功率方面均显著优于现有模型。

Conclusion: ANWM通过结合高级语义和导航实用性，显著提升了无人机在大规模环境中的导航成功率，并在长距离视觉预测方面优于现有世界模型。

Abstract: Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.

</details>


### [102] [Flexible Multitask Learning with Factorized Diffusion Policy](https://arxiv.org/abs/2512.21898)
*Chaoqi Liu,Haonan Chen,Sigmund H. Høeg,Shaoxiong Yao,Yunzhu Li,Kris Hauser,Yilun Du*

Main category: cs.RO

TL;DR: 提出模块化扩散策略框架，通过分解动作分布为专门扩散模型，提升多任务学习效果并支持灵活适应新任务。


<details>
  <summary>Details</summary>
Motivation: 多任务学习面临机器人动作分布高度多模态和多样化的挑战，现有整体模型往往无法充分拟合动作分布且缺乏灵活适应性。

Method: 引入了一种新颖的模块化扩散策略框架，将复杂的动作分布分解为多个专门的扩散模型，每个模型捕获行为空间的一个独特子模式。

Result: 在模拟和真实世界的机器人操作环境中，该方法 consistently outperforms strong modular and monolithic baselines。

Conclusion: 该论文提出的模块化扩散策略框架在模拟和真实世界的机器人操作环境中均表现优异，超越了现有的模块化和整体化基线方法。

Abstract: Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution and lack the flexibility required for efficient adaptation. We introduce a novel modular diffusion policy framework that factorizes complex action distributions into a composition of specialized diffusion models, each capturing a distinct sub-mode of the behavior space for a more effective overall policy. In addition, this modular structure enables flexible policy adaptation to new tasks by adding or fine-tuning components, which inherently mitigates catastrophic forgetting. Empirically, across both simulation and real-world robotic manipulation settings, we illustrate how our method consistently outperforms strong modular and monolithic baselines.

</details>


### [103] [StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision](https://arxiv.org/abs/2512.21970)
*Shengliang Deng,Mi Yan,Yixin Zheng,Jiayi Su,Wenhao Zhang,Xiaoguang Zhao,Heming Cui,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: StereoVLA模型利用立体视觉的几何特征和单目视觉的语义特征，显著提升了视觉-语言-动作模型的性能，实验证明其优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 立体视觉能够提供丰富的空间线索，但在视觉-语言-动作模型中的应用尚未充分探索，因此提出StereoVLA模型以填补这一空白。

Method: 提出了一种新颖的几何-语义特征提取模块，结合立体视觉的几何特征和单目视觉的语义特征，并引入交互区域深度估计任务以增强空间感知和加速模型收敛。

Result: 实验表明，StereoVLA在立体设置下的多种任务中大幅超越基线模型，并对相机姿态变化表现出强鲁棒性。

Conclusion: StereoVLA模型通过结合立体视觉的几何特征和单目视觉的语义特征，显著提升了视觉-语言-动作模型在立体设置下的性能，并展现出对相机姿态变化的强鲁棒性。

Abstract: Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.

</details>


### [104] [Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation](https://arxiv.org/abs/2512.21983)
*Saksham Gupta,Sarthak Mishra,Arshad Ayub,Kamran Farooque,Spandan Roy,Babita Gupta*

Main category: cs.RO

TL;DR: BRIS 是一个紧凑的人机协作平台，整合了可操纵支气管镜和导管推进机制，通过实时深度感知提供安全导管定位，验证了其在困难气道中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人及远程操作插管系统主要关注气道导航，未提供导管推进的集成控制或导管深度相对于隆突的客观验证。

Method: BRIS 整合了四向可操纵纤维支气管镜、独立的导管推进机制以及与标准临床工作流程兼容的摄像头增强口器。学习型闭环控制框架利用实时形状感知将操纵杆输入映射到笛卡尔空间中的远端支气管镜尖端运动。

Result: 系统在高保真气道模型上验证，展示了可靠的导航和控制导管放置能力。

Conclusion: BRIS 系统作为迈向更安全、更一致且临床兼容的机器人气道管理的一步，展示了其在标准及困难气道配置下的可靠导航和控制导管放置能力。

Abstract: Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications. Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina. This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness. BRIS integrates a four-way steerable fiberoptic bronchoscope, an independent endotracheal tube advancement mechanism, and a camera-augmented mouthpiece compatible with standard clinical workflows. A learning-enabled closed-loop control framework leverages real-time shape sensing to map joystick inputs to distal bronchoscope tip motion in Cartesian space, providing stable and intuitive teleoperation under tendon nonlinearities and airway contact. Monocular endoscopic depth estimation is used to classify airway regions and provide interpretable, anatomy-aware guidance for safe tube positioning relative to the carina. The system is validated on high-fidelity airway mannequins under standard and difficult airway configurations, demonstrating reliable navigation and controlled tube placement. These results highlight BRIS as a step toward safer, more consistent, and clinically compatible robotic airway management.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [105] [LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems](https://arxiv.org/abs/2512.21701)
*Nan Chen,Xiaotian Dai,Tong Cheng,Alan Burns,Iain Bate,Shuai Zhao*

Main category: cs.OS

TL;DR: LEFT-RS协议通过并行访问和优化故障恢复，显著提升多核实时系统的资源利用效率和容错能力。


<details>
  <summary>Details</summary>
Motivation: 现有锁定协议无法有效处理关键区内的瞬时故障，传统容错技术会增加阻塞。

Method: 提出了LEFT-RS协议，支持任务并行访问全局资源，优化了故障恢复机制。

Result: 通过最坏情况响应时间分析验证了时序保证，实验结果显示调度性能显著提升。

Conclusion: LEFT-RS协议显著提升了多核实时系统的资源利用效率和容错能力，平均调度性能提高了84.5%。

Abstract: Emerging real-time applications have driven the transition to multicore embedded systems, where tasks must share resources due to functional demands and limited availability. These resources, whether local or global, are protected within critical sections to prevent race conditions, with locking protocols ensuring both exclusive access and timing requirements. However, transient faults occurring within critical sections can disrupt execution and propagate errors across multiple tasks. Conventional locking protocols fail to address such faults, and integrating traditional fault tolerance techniques often increases blocking. Recent approaches improve fault recovery through parallel replica execution; however, challenges remain due to sequential accessing, coordination overhead, and susceptibility to common-mode faults. In this paper, we propose a Lock-frEe Fault-Tolerant Resource Sharing (LEFT-RS) protocol for multicore real-time systems. LEFT-RS allows tasks to concurrently access and read global resources while entering their critical sections in parallel. Each task can complete its access earlier upon successful execution if other tasks experience faults, thereby improving the efficiency of resource usage. Our design also limits the overhead and enhances fault resilience. We present a comprehensive worst-case response time analysis to ensure timing guarantees. Extensive evaluation results demonstrate that our method significantly outperforms existing approaches, achieving up to an 84.5% improvement in schedulability on average.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [106] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 本研究通过调查46名行业专业人士，揭示了LLM在软件工程中的积极影响（如技术问题解决效率提升）和潜在风险（如安全问题和认知依赖），呼吁批判性使用并推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在开发者日常工作中的深入应用，理解其使用方式变得至关重要。

Method: 基于对46名行业专业人士的调查，涵盖了不同教育背景和经验水平的开发者。

Result: 调查结果显示，LLM在解决技术问题、文档支持和源代码标准化方面有积极影响，但也存在认知依赖、安全风险和技术自主性减弱的担忧。

Conclusion: 本研究强调了在软件工程中采用LLM时需要批判性和监督性使用的重要性，并提供了对开发者及研究者的实际建议，同时为未来关于LLM认知、伦理和组织影响的研究提供了动机。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [107] [Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software](https://arxiv.org/abs/2512.21348)
*Ying Xiao,Shangwen Wang,Sicen Liu,Dingyuan Xue,Xian Zhan,Yepang Liu,Jie M. Zhang*

Main category: cs.SE

TL;DR: 论文提出CoT方法，通过调整数据相关性显著提升公平性，效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统软件公平性研究忽视了公平性作为软件质量问题的核心地位，现有偏差缓解方法在广泛适用性和有效性之间存在矛盾。

Method: CoT引入Phi系数作为直观的相关性度量，并采用多目标优化来解决代理偏差。

Result: CoT将弱势群体的真正阳性率平均提高了17.5%，并将统计奇偶差异（SPD）、平均几率差异（AOD）和均等机会差异（EOD）等关键偏差指标平均减少了50%以上。

Conclusion: 论文提出了一种名为Correlation Tuning (CoT)的新型预处理方法，通过调整数据相关性来缓解偏差，显著提高了弱势群体的预测性能，并减少了关键偏差指标。

Abstract: Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.

</details>


### [108] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo通过进化算法提升代码生成的适应性和新颖性，在多项测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 受人类进化的自然选择和适应启发，旨在提升代理在分布变化环境中的表现。

Method: 结合进化算法，将RL轨迹视为“基因组”，在夜间回放阶段进行突变和选择，增强Dream Queue的进化操作。

Result: 在HumanEval变体、BigCodeBench和自定义PySpark管道模拟中，CosmoCore-Evo比原版和基线方法表现更优，解决方案新颖性提高35%，适应速度加快25%。

Conclusion: CosmoCore-Evo通过引入进化算法显著提升了代码生成任务的适应性和新颖性，验证了进化组件在缩小LLM代理感知差距中的关键作用。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [109] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 多智能体委员会框架通过协作显著提升软件测试效率和成功率，优于单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 手动软件测试成本高且耗时，而单智能体LLM方法存在幻觉和不一致行为的问题。

Method: 采用多智能体委员会框架，结合模型多样性、角色驱动的行为变化和视觉用户界面理解，通过三轮投票协议达成共识。

Result: 在84次实验运行中，多智能体委员会的任务成功率达到89.5%，配置2至4个智能体时成功率为91.7%至100%，显著优于单智能体基线的78.0%。在WebShop和OWASP基准测试中分别达到74.7%和82.0%的成功率。

Conclusion: 该论文提出了一个多智能体委员会框架，通过多样化的视觉支持大型语言模型（LLM）协作，显著提高了软件测试的成功率和效率，并在多个基准测试中表现优于单智能体基线。

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [110] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench 是一个评估 LLM 在真实科研软件生态中作为科学计算开发代理能力的大规模基准，通过多阶段筛选和专家评审确保任务质量，填补了端到端科学开发评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的科学推理基准侧重于概念知识，而软件工程基准强调通用功能实现和问题解决，缺乏对端到端科学开发环境的评估。

Method: 通过从六个广泛使用的科学代码库中提取维护者提交的拉取请求任务，并经过多阶段筛选和专家评审，确保任务的科学挑战性、充分测试覆盖率和难度校准。

Result: AInsteinBench 通过可执行环境评估、科学意义的失败模式及测试驱动验证，衡量模型从表面代码生成向计算科学研究核心能力的转变。

Conclusion: AInsteinBench 是一个针对大型语言模型（LLM）代理在真实科研软件生态系统中作为科学计算开发代理能力的评估基准，填补了现有基准在端到端科学开发环境评估上的空白。

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>


### [111] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 本文提出32项标准评估GitHub issue质量，构建可解释模型预测合并概率，发现明确、简洁的issue更易被AI代理成功实现。


<details>
  <summary>Details</summary>
Motivation: AI代理（如Copilot）的性能高度依赖输入质量，而现有最佳实践建议有限且高层面。本文旨在提供更具体的标准，帮助开发者编写适合AI代理的GitHub issue。

Method: 作者通过比较导致合并和关闭的pull request的GitHub issue，建立了32项详细标准来衡量issue的质量，并构建了一个可解释的机器学习模型来预测合并概率。

Result: 研究发现，合并率较高的issue通常较短、范围明确、提供清晰的实现指导和相关工件提示，而包含外部引用（如配置、依赖或API）的issue合并率较低。模型的AUC中位数为72%。

Conclusion: 本文构建了一个可解释的机器学习模型，帮助开发者优化GitHub issue以提高被AI代理（如Copilot）合并的概率，并揭示了影响合并率的关键质量指标。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [112] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一个无需执行的预测性测试框架，利用LLMs生成测试输入并检测错误，通过两阶段优化提升效率和效果。


<details>
  <summary>Details</summary>
Motivation: 在软件开发的某些场景中，需要在未实际执行代码的情况下检测运行时错误和异常，例如在线代码片段集成前的错误检测。

Method: Cerberus利用LLMs生成触发运行时错误的输入，并进行代码覆盖预测和错误检测，通过两阶段反馈循环优化测试过程。

Result: 实证评估表明，Cerberus在生成高覆盖率测试用例和发现更多运行时错误方面表现优于传统和基于学习的测试框架。

Conclusion: Cerberus框架通过两阶段反馈循环，在不执行代码的情况下，有效提高了代码覆盖率和运行时错误检测能力，优于传统和基于学习的测试框架。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [113] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise利用LLM多代理框架一体化生成优化初始种子库，减少测试用例数量同时提升覆盖率和错误检测效率。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试中，生成大规模初始种子库和后续优化是两个分离的步骤，效率低下且资源消耗大。FuzzWise旨在通过一体化流程解决这一问题。

Method: FuzzWise采用基于大型语言模型（LLMs）的多代理框架，一个代理生成测试用例，另一个代理预测代码覆盖率，实时评估新测试用例对整体覆盖率的贡献。

Result: 实验表明，FuzzWise生成的测试用例数量显著少于基线方法，但代码覆盖率和运行时错误触发率更高，且时间效率更优。

Conclusion: FuzzWise通过整合初始种子生成与优化过程，显著提高了模糊测试的效率和效果，尤其在减少测试用例数量的同时提升了代码覆盖率和运行时错误检测能力。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [114] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 提出了一种基于lambda表达式的C#代码克隆合并技术，评估显示35%的克隆对适合重构，其中28.9%成功。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在Java程序的行为参数化，对其他编程语言的研究较少，特别是C#。

Method: 使用lambda表达式分析并合并代码克隆，通过NiCad克隆检测器检测的克隆对进行评估。

Result: 在2,217对克隆中，35.0%被认为适合重构，其中28.9%成功重构。

Conclusion: 研究表明，提出的C#特定技术能有效利用lambda表达式来分析和合并代码克隆，尽管成功率有待提高。

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [115] [XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production](https://arxiv.org/abs/2512.21555)
*Qi Hu,Jiangchao Liu,Xin Yu,Lin Zhang,Edward Jiang*

Main category: cs.SE

TL;DR: XTrace 是一种新型动态追踪框架，通过非侵入式代理优化 Android ART 虚拟机的内置检测机制，解决了传统方法无法实时追踪“幽灵错误”的问题，并在实际应用中证明了其稳定性和高效性。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用复杂性和用户设备环境碎片化的加剧，传统静态日志和崩溃后分析方法缺乏实时上下文信息，无法有效应对特定场景下的“幽灵错误”。

Method: XTrace 通过非侵入式代理和优化 Android ART 虚拟机的内置检测机制，实现了高性能方法拦截。

Result: XTrace 在大规模在线 A/B 实验中表现出生产级稳定性和性能，诊断了超过 11 次严重在线崩溃和多个性能瓶颈，将根因定位效率提高了 90% 以上。

Conclusion: XTrace 提供了一个生产级的解决方案，解决了 Android 动态追踪中稳定性和全面覆盖之间的长期冲突。

Abstract: As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against "ghost bugs" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.

</details>


### [116] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: \methodName 是一种基于LLMs的新方法，通过类型和依赖关系的协同演化实现仓库级Python类型推断，显著优于现有工具，并大幅减少错误。


<details>
  <summary>Details</summary>
Motivation: Python的动态类型机制虽然灵活，但也是大规模软件运行时类型错误的主要来源，这激发了自动类型推断技术的研究。现有工具在孤立代码片段中的类型推断取得了进展，但仓库级类型推断仍面临重大挑战，主要由于复杂的跨过程依赖关系难以建模和解决。

Method: \methodName 基于LLMs，通过类型和依赖关系的协同演化实现仓库级类型推断。它构建了一个实体依赖图（EDG）来建模仓库中的对象和类型依赖关系，并在推断过程中迭代优化EDG中的类型和依赖关系。关键创新包括：(1) 设计EDG模型捕获仓库级类型依赖；(2) 迭代类型推断方法，每轮迭代中类型和依赖关系协同演化；(3) 类型检查器循环策略，实时验证和纠正推断，减少错误传播。

Result: 在12个复杂Python仓库上的评估显示，\methodName 显著优于现有工作，TypeSim得分0.89，TypeExact得分0.84，相对最强基线分别提高了27%和40%。此外，它减少了工具引入的新类型错误的92.7%。

Conclusion: \methodName 在12个复杂的Python仓库上显著优于现有工作，TypeSim得分0.89，TypeExact得分0.84，分别比最强基线提高了27%和40%。更重要的是，它减少了工具引入的新类型错误的92.7%，展示了在真实Python开发中自动化、可靠类型标注的重大进步。

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [117] [How Do Agents Perform Code Optimization? An Empirical Study](https://arxiv.org/abs/2512.21757)
*Huiyun Peng,Antonio Zhong,Ricardo Andrés Calvo Méndez,Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: AI coding agents produce performance optimizations with less validation than humans but similar patterns, suggesting room for improvement.


<details>
  <summary>Details</summary>
Motivation: To understand how AI coding agents perform on real-world performance optimization tasks, given the lack of existing empirical studies.

Method: Empirical study comparing 324 AI-generated and 83 human-authored performance optimization PRs from the AIDev dataset, analyzing adoption, maintainability, optimization patterns, and validation practices.

Result: AI-authored PRs are less likely to include explicit performance validation (45.7% vs. 63.6%, p=0.007) but use similar optimization patterns as humans.

Conclusion: AI-authored performance PRs exhibit limitations in performance validation compared to human-authored PRs, though they use similar optimization patterns. The study highlights opportunities for improving agentic code optimization.

Abstract: Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\% vs. 63.6\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.

</details>


### [118] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 比较了SPDX和CycloneDX两种SBOM格式的工具生态系统，发现CycloneDX在开发者参与度上更优，SPDX在生态系统成熟度和行业采用上更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 探讨SBOM（软件物料清单）的两种主要格式（SPDX和CycloneDX）在工具生态系统成熟度、工具支持和社区参与方面的差异。

Method: 定量比较了170个公开宣传的SBOM工具的使用案例，分析了36,990个开源工具的问题报告，并调查了使用每种工具生态系统的前250个开源项目的健康指标。

Result: CycloneDX工具显示出更高的开发者参与度和某些健康指标，而SPDX工具则因其更成熟的生态系统和更广泛的行业采用而具有优势。

Conclusion: 研究表明，CycloneDX工具在开发者参与度和某些健康指标上表现更优，而SPDX工具则因其更成熟的生态系统、更广泛的工具可用性和行业采用而具有优势。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [119] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: 该论文集收录了WACA 2025研讨会的论文，与DisCoTec 2025同期举行。


<details>
  <summary>Details</summary>
Motivation: 促进可适应云架构领域的研究与交流。

Method: 论文集形式，收录研讨会后的论文。

Result: 汇集了研讨会上的研究成果和讨论内容。

Conclusion: 该论文集收录了2025年6月20日在法国里尔举行的可适应云架构研讨会（WACA 2025）的会后论文，该研讨会与第20届分布式计算技术国际联合会议（DisCoTec 2025）同期举行。

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


### [120] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出无标签评估指标PMSS，解决依赖标记数据的问题，与现有指标相关性高，适用于标签缺失场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标依赖标记数据，限制了研究范围并导致性能结论不一致。

Method: 提出了一种无标签的模板级度量PMSS，通过medoid silhouette分析和Levenshtein距离评估日志解析器的性能。

Result: PMSS与FGA和FTA显著正相关，差异较小（FGA平均差异2.1%，FTA差异9.8%），且计算复杂度接近线性。

Conclusion: PMSS提供了一种在真实标签不一致或缺失情况下的有价值的日志解析器评估替代方案。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [121] [Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development](https://arxiv.org/abs/2512.21818)
*Brian Bowers,Smita Khapre,Jugal Kalita*

Main category: cs.SE

TL;DR: 多智能体系统在代码生成中高效但易受攻击，安全分析代理能提升韧性但自身也存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统在软件工程中的潜力及其安全漏洞，探索如何平衡效率与韧性。

Method: 提出了一种多智能体系统架构，用于软件工程过程的实施阶段，并构建了全面的威胁模型。比较了不同架构（coder、coder-tester、coder-reviewer-tester）的韧性和效率。

Result: coder-reviewer-tester架构比coder和coder-tester架构更韧性，但代码生成效率较低。增加安全分析代理可提高韧性并减少效率损失，但安全分析代理易受高级代码注入攻击。

Conclusion: 多智能体系统在代码生成方面表现出色，但易受攻击，尤其是代码注入。通过增加安全分析代理可以提高系统韧性，但安全分析代理本身也可能受到高级代码注入攻击。

Abstract: Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.

</details>


### [122] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 论文提出了一种新的二进制程序分析框架，通过内核模块和进程空心化技术提升细粒度分析的性能，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对细粒度分析（如动态污点分析）在部署性、内存使用和性能开销方面的问题，以适应新的分析场景（如内存破坏利用和沙箱逃逸恶意软件）。

Method: 主要采用内核模块扩展传统动态二进制插桩的分析能力，并通过进程空心化技术在容器进程中构建分析环境。

Result: 原型在Windows平台上实现，通过大量基准和实际程序实验验证了框架的有效性和性能，并在实际漏洞利用程序和恶意代码分析中展示了实用价值。

Conclusion: 该论文提出的二进制程序分析框架通过内核模块和进程空心化技术，有效提升了细粒度分析的可用性和性能，并在实际应用中验证了其有效性。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [123] [From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration](https://arxiv.org/abs/2512.21360)
*Shuide Wen,Yu Sun,Beier Ku,Zhi Gao,Lijun Ma,Yang Yang,Can Jiao*

Main category: cs.AI

TL;DR: 研究证实多模态大模型可作为标准化投射评估工具，多智能体框架通过角色划分提升解释一致性，为数字心理健康服务提供新思路。


<details>
  <summary>Details</summary>
Motivation: 解决House-Tree-Person（HTP）绘图测试在临床心理学中长期面临的挑战，如评分标准不统一、依赖检查者主观经验以及缺乏统一的定量编码系统。

Method: 通过定量实验和定性分析，评估多模态大语言模型（MLLM）与人类专家解释的语义相似性，并验证多智能体系统在纠正视觉幻觉和生成心理报告方面的效果。

Result: 定量实验显示，MLLM解释与人类专家解释的平均语义相似度约为0.75（标准差约0.05），在结构化专家数据集中相似度升至0.85。定性分析表明，多智能体系统通过整合社会心理学视角和去污名化叙事，有效纠正视觉幻觉并生成具有高生态效度和内部一致性的心理报告。

Conclusion: 研究结果证实了多模态大模型作为投射评估标准化工具的潜力。提出的多智能体框架通过角色划分，将特征识别与心理推理解耦，为数字心理健康服务提供了新范式。

Abstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.
  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.
  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.
  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence

</details>


### [124] [A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers](https://arxiv.org/abs/2512.21365)
*Chung-Chin Shih,Ti-Rong Wu,Ting Han Wei,Yu-Shan Hsu,Hung Guei,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文分析了基于RZS的围棋求解器在死活问题中的表现，发现了其识别关键区域和稀有模式的能力，但也指出了与人类策略的差异，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究当前最先进的计算机围棋求解器在解决死活问题时的行为，并与人类棋手的策略进行对比。

Method: 使用基于相关区域搜索（RZS）和相关区域模式表的技术分析围棋死活问题。

Result: 求解器成功识别了关键区域和稀有模式，并在两个问题上给出了与书中不同的答案，但也存在对稀有模式价值误判和偏好直接活棋而非最大化领土的问题。

Conclusion: 本文提出了解决围棋死活问题的改进方向，并指出了当前求解器的局限性。

Abstract: This paper analyzes the behavior of solving Life-and-Death (L&D) problems in the game of Go using current state-of-the-art computer Go solvers with two techniques: the Relevance-Zone Based Search (RZS) and the relevance-zone pattern table. We examined the solutions derived by relevance-zone based solvers on seven L&D problems from the renowned book "Life and Death Dictionary" written by Cho Chikun, a Go grandmaster, and found several interesting results. First, for each problem, the solvers identify a relevance-zone that highlights the critical areas for solving. Second, the solvers discover a series of patterns, including some that are rare. Finally, the solvers even find different answers compared to the given solutions for two problems. We also identified two issues with the solver: (a) it misjudges values of rare patterns, and (b) it tends to prioritize living directly rather than maximizing territory, which differs from the behavior of human Go players. We suggest possible approaches to address these issues in future work. Our code and data are available at https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ.

</details>


### [125] [Three-way conflict analysis based on alliance and conflict functions](https://arxiv.org/abs/2512.21419)
*Junfang Luo,Mengjun Hu,Guangming Lang,Xin Yang,Keyun Qin*

Main category: cs.AI

TL;DR: 论文提出分离联盟和冲突功能以澄清三向冲突分析中的语义，解决了传统聚合方法的解释挑战，并展示了其在实际应用中的价值。


<details>
  <summary>Details</summary>
Motivation: 传统方法在聚合一组问题或代理的辅助功能时存在语义解释的挑战，需要一种更清晰的方法来区分不同的态度。

Method: 论文将辅助功能中的两个对立方面分离为联盟和冲突功能，并研究了这些功能在三向冲突分析中的应用。

Result: 通过分离联盟和冲突功能，论文成功地澄清了语义，并探索了这些概念在解决冲突分析关键问题中的应用。

Conclusion: 该论文提出了一种新的方法来澄清三向冲突分析中的语义，通过将联盟和冲突功能分离，解决了传统聚合方法中的解释挑战。

Abstract: Trisecting agents, issues, and agent pairs are essential topics of three-way conflict analysis. They have been commonly studied based on either a rating or an auxiliary function. A rating function defines the positive, negative, or neutral ratings of agents on issues. An auxiliary function defines the alliance, conflict, and neutrality relations between agents. These functions measure two opposite aspects in a single function, leading to challenges in interpreting their aggregations over a group of issues or agents. For example, when studying agent relations regarding a set of issues, a standard aggregation takes the average of an auxiliary function concerning single issues. Therefore, a pair of alliance +1 and conflict -1 relations will produce the same result as a pair of neutrality 0 relations, although the attitudes represented by the two pairs are very different. To clarify semantics, we separate the two opposite aspects in an auxiliary function into a pair of alliance and conflict functions. Accordingly, we trisect the agents, issues, and agent pairs and investigate their applications in solving a few crucial questions in conflict analysis. Particularly, we explore the concepts of alliance sets and strategies. A real-world application is given to illustrate the proposed models.

</details>


### [126] [Feasible strategies in three-way conflict analysis with three-valued ratings](https://arxiv.org/abs/2512.21420)
*Jing Liu,Mengjun Hu,Guangming Lang*

Main category: cs.AI

TL;DR: 本文研究了三向冲突分析中的可行策略，提出加权一致性和非一致性度量及相应算法，案例验证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有三向冲突分析研究多关注冲突的本质理解，但缺乏解决策略的制定，本文旨在从一致性和非一致性两个角度研究可行策略。

Method: 基于代理和问题的权重，提出了加权一致性和非一致性度量，并开发了算法来识别可行策略、L阶可行策略及对应的最优策略。

Result: 通过NBA劳资谈判和甘肃省发展规划两个案例研究，验证了模型的实际性、有效性和优越性。

Conclusion: 本文提出的冲突解决模型通过结合加权代理-问题评估与一致性和非一致性度量，系统地识别了可行策略和最优解，优于传统方法。

Abstract: Most existing work on three-way conflict analysis has focused on trisecting agent pairs, agents, or issues, which contributes to understanding the nature of conflicts but falls short in addressing their resolution. Specifically, the formulation of feasible strategies, as an essential component of conflict resolution and mitigation, has received insufficient scholarly attention. Therefore, this paper aims to investigate feasible strategies from two perspectives of consistency and non-consistency. Particularly, we begin with computing the overall rating of a clique of agents based on positive and negative similarity degrees. Afterwards, considering the weights of both agents and issues, we propose weighted consistency and non-consistency measures, which are respectively used to identify the feasible strategies for a clique of agents. Algorithms are developed to identify feasible strategies, $L$-order feasible strategies, and the corresponding optimal ones. Finally, to demonstrate the practicality, effectiveness, and superiority of the proposed models, we apply them to two commonly used case studies on NBA labor negotiations and development plans for Gansu Province and conduct a sensitivity analysis on parameters and a comparative analysis with existing state-of-the-art conflict analysis approaches. The comparison results demonstrate that our conflict resolution models outperform the conventional approaches by unifying weighted agent-issue evaluation with consistency and non-consistency measures to enable the systematic identification of not only feasible strategies but also optimal solutions.

</details>


### [127] [Three-way decision with incomplete information based on similarity and satisfiability](https://arxiv.org/abs/2512.21421)
*Junfang Luo,Mengjun Hu,Keyun Qin*

Main category: cs.AI

TL;DR: 本文泛化了三支决策的两种现有方法（计算和概念），提出了适用于不完全信息的新相似度和满足度度量，并探讨了四种新方法。


<details>
  <summary>Details</summary>
Motivation: 现有三支决策方法主要针对完全信息，而实际应用中信息往往不完全，因此需要泛化现有方法以适应不完全信息场景。

Method: 1. 提出对象相似度度量作为等价关系的泛化；2. 提出公式满足度度量作为完全信息满足的定量泛化；3. 分别基于相似类和近似性、α-意义集和公式置信度探讨三支决策方法。

Result: 提出了基于相似度和满足度的两种三支决策新方法，为不完全信息处理提供了新方向。

Conclusion: 本文通过提出新的相似度和满足度度量，将三支决策推广到不完全信息场景，为实际应用提供了新的理论框架和方法。

Abstract: Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.

</details>


### [128] [LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis](https://arxiv.org/abs/2512.21482)
*Fanwei Zeng,Changtao Miao,Jing Huang,Zhiya Tan,Shutao Gong,Xiaoming Yu,Yang Wang,Huazhe Tan,Weibin Yao,Jianshu Li*

Main category: cs.AI

TL;DR: LogicLens是一个统一的视觉-文本协同推理框架，通过CCT机制和多任务优化，显著提升文本伪造分析性能，并在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本中心伪造分析方法局限于粗粒度视觉分析，缺乏复杂推理能力，且将检测、定位和解释视为独立任务，忽视其内在关联。

Method: 提出LogicLens框架，采用Cross-Cues-aware Chain of Thought (CCT)机制和GRPO优化的加权多任务奖励函数，结合PR$^2$标注流程和RealText数据集。

Result: 在T-IC13零样本评估中，LogicLens比专用框架和GPT-4o分别高出41.4%和23.4%的宏平均F1分数；在T-SROIE数据集上也显著领先其他MLLM方法。

Conclusion: LogicLens框架通过视觉-文本协同推理和多任务优化，显著提升了文本中心伪造分析的性能，并在多个基准测试中表现优异。

Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.

</details>


### [129] [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)
*Yanhao Li,Lu Ma,Jiaran Zhang,Lexiang Tang,Wentao Zhang,Guibo Luo*

Main category: cs.AI

TL;DR: Leash 是一种自适应长度惩罚框架，通过动态调整惩罚系数，显著减少推理长度而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定长度惩罚，难以调整且无法适应 LLMs 不断发展的推理能力，导致准确性和简洁性之间的次优权衡。

Method: 通过将长度控制建模为约束优化问题，并采用拉格朗日原始-对偶方法动态调整惩罚系数，Leash 实现了自适应长度惩罚。

Result: 在 Deepseek-R1-Distill-Qwen-1.5B 和 Qwen3-4B-Thinking-2507 上的实验表明，Leash 在多样化任务中平均推理长度减少了 60%，同时保持竞争力。

Conclusion: Leash 提出了一种实用的、有效的方法，用于开发可控且高效的大型语言模型（LLMs），平衡推理能力和计算预算。

Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.

</details>


### [130] [NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent](https://arxiv.org/abs/2512.21578)
*Ali Sahami,Sudhanshu Garg,Andrew Wang,Chaitanya Kulkarni,Farhad Farahani,Sean Yun-Shiuan Chuang,Jian Wan,Srinivasan Manoharan,Uma Kona,Nitin Sharma,Linsey Pang,Prakhar Mehrotra,Jessica Clark,Mark Moyou*

Main category: cs.AI

TL;DR: PayPal与NVIDIA合作，利用NeMo框架优化Commerce Agent，通过微调Nemotron SLM显著提升检索性能，同时降低成本并保持质量。


<details>
  <summary>Details</summary>
Motivation: 利用NVIDIA的NeMo框架优化PayPal的Commerce Agent，以提升代理性能和效率。

Method: 通过系统化的超参数扫描（学习率、优化器、余弦退火调度和LoRA秩）训练基于LoRA的模型，并采用NVIDIA的NeMo框架进行LLM模型微调。

Result: 显著降低了延迟和成本，同时保持了代理质量，解决了检索组件占代理响应时间50%以上的性能问题。

Conclusion: 优化后的Nemotron SLM有效解决了检索组件的关键性能问题，同时保持或提升了整体系统性能。

Abstract: We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM).
  We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\% of total agent response time, while maintaining or enhancing overall system performance.

</details>


### [131] [A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning](https://arxiv.org/abs/2512.21583)
*Zelin Zang,Wenyi Gu,Siqi Ma,Dan Yang,Yue Shen,Zhu Zhang,Guohui Fan,Wing-Kuen Ling,Fuji Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉语言对齐与逻辑正则化推理的诊断框架，显著提升了多模态医疗AI的诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和视觉语言模型（VLMs）在医学领域的快速发展，简单地整合临床文本和医学影像并不能保证可靠的推理。现有的多模态模型常产生幻觉或不一致的思维链，限制了临床信任。

Method: 该系统包括用于文本和图像的输入编码器、用于跨模态对齐的投影模块、将诊断任务分解为步骤的推理控制器，以及将逐步前提组装成可验证结论的逻辑树生成器。

Result: 在MedXpertQA等基准测试上的评估表明，该方法在多模态任务中提高了诊断准确性，并生成了更可解释的推理轨迹，同时在纯文本设置中保持竞争力。

Conclusion: 该论文提出了一种基于LLaVA的诊断框架，结合了视觉语言对齐与逻辑正则化推理，显著提高了诊断准确性并生成了更可解释的推理轨迹，为可信赖的多模态医疗AI迈出了有希望的一步。

Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

</details>


### [132] [AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design](https://arxiv.org/abs/2512.21613)
*Zhishuai Zhang,Xintian Li,Shilong Liu,Aodong Zhang,Lu Jie,Nan Sun*

Main category: cs.AI

TL;DR: AMS-IO-Agent是一个基于LLM的代理，专用于AMS IC的I/O子系统生成，通过结构化知识库和意图转换实现高效自动化设计，并在实际流片中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AMS IC设计中输入/输出子系统生成的自动化问题，连接自然语言设计意图与工业级设计交付。

Method: 提出AMS-IO-Agent框架，结合结构化领域知识库和设计意图结构化技术，使用JSON和Python作为中间格式转换模糊用户意图为可验证逻辑步骤。

Result: 在AMS-IO-Bench基准测试中，DRC+LVS通过率超过70%，设计周转时间从小时缩短至分钟，并通过28 nm CMOS流片验证。

Conclusion: AMS-IO-Agent通过整合结构化领域知识库和设计意图结构化能力，成功实现了从自然语言设计意图到工业级AMS IC设计交付的自动化，显著提高了设计效率和通过率。

Abstract: In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.

</details>


### [133] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA是一个多代理平台，整合生物学、化学和药理学，通过自主执行和人类指导的结合，将药物发现转变为可编程的工程学科。


<details>
  <summary>Details</summary>
Motivation: 治疗发现面临专业领域碎片化和计算设计与生理验证之间的执行差距的挑战，当前生成式AI模型多为被动助手而非自主执行者。

Method: OrchestRA是一个人在回路中的多代理平台，包括生物学家代理、化学家代理和药理学家代理，分别负责目标识别、结构设计和药代动力学评估。

Result: OrchestRA通过动态反馈循环，实现了药代动力学和毒性特征直接触发结构再优化，提升了药物发现的效率和准确性。

Conclusion: OrchestRA通过将生物学、化学和药理学整合到一个自主发现引擎中，实现了治疗设计的民主化，将药物发现从随机搜索转变为可编程的、基于证据的工程学科。

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [134] [Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing](https://arxiv.org/abs/2512.21626)
*Hong Xie,Haoran Gu,Yanying Huang,Tao Tan,Defu Lian*

Main category: cs.AI

TL;DR: 本文提出了一种多臂随机赌博机变体，用于资源分配问题，设计了算法并匹配了理论遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 解决LLM应用和边缘智能等场景中的资源分配问题，优化优先级资源分配机制下的性能。

Method: 设计了MSB-PRS-OffOpt算法和近似UCB算法，用于优化和学习优先级资源分配机制下的非线性组合效用函数。

Result: 证明了实例无关和实例相关的遗憾下界，并设计了算法，其遗憾上界与下界匹配。

Conclusion: 本文提出了一种针对资源分配问题的多臂随机赌博机变体，并设计了算法MSB-PRS-OffOpt和近似UCB算法，其遗憾上界与理论下界匹配。

Abstract: This paper proposes a variant of multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, edge intelligence, etc. The model is composed of $M$ arms and $K$ plays. Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function. Each play is associated with a priority weight. When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first manner. Instance independent and instance dependent regret lower bounds of $Ω( α_1 σ\sqrt{KM T} )$ and $Ω(α_1 σ^2 \frac{M}Δ \ln T)$ are proved, where $α_1$ is the largest priority weight and $σ$ characterizes the reward tail. When model parameters are given, we design an algorithm named \texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(MK^3)$. Utilizing \texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $ \sqrt{K \ln KT }$ and $α_1 K^2$ respectively. To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.

</details>


### [135] [Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning](https://arxiv.org/abs/2512.21699)
*Eranga Bandara,Tharaka Hewa,Ross Gore,Sachin Shetty,Ravi Mukkamala,Peter Foytik,Abdul Rahman,Safdar H. Bouk,Xueping Liang,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提出了一种负责任且可解释的AI代理架构，通过多模型共识和推理层治理解决自主AI系统的关键挑战，实际评估证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统功能的增强，其在可解释性、问责性、鲁棒性和治理方面的挑战日益突出，现有实现往往缺乏理解决策 rationale 或跨代理交互执行责任的机制。

Method: 采用异构LLM和VLM代理独立生成候选输出，并通过专用推理代理进行结构化整合，强制执行安全和策略约束，减少幻觉和偏见，生成可审计的、基于证据的决策。

Result: 评估表明，共识驱动的推理提升了多样应用领域的鲁棒性、透明度和操作信任。

Conclusion: 本文提出了一种基于多模型共识和推理层治理的负责任（RAI）和可解释（XAI）AI代理架构，旨在解决自主AI系统在可解释性、问责性、鲁棒性和治理方面的挑战。通过实际工作流评估，证明了该架构在提升鲁棒性、透明度和操作信任方面的有效性。

Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.

</details>


### [136] [Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets](https://arxiv.org/abs/2512.21775)
*Matyas Bohacek,Ignacio Vilanova Echavarri*

Main category: cs.AI

TL;DR: 论文提出CRS框架和开源工具，解决GAI数据集伦理问题，支持现有和未来数据集的合规评估与构建。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能数据集的伦理与法律问题常被忽视，尤其是数据来源、合法性和安全性信息在共享和编辑过程中易丢失。

Method: 通过数据溯源技术开发了一个开源Python库，用于评估现有数据集的合规性，并指导新数据集的负责任构建。

Result: CRS框架及开源工具能够无缝集成到现有数据处理和AI训练流程中，同时具备反应性和前瞻性。

Conclusion: 该论文提出了合规评级方案（CRS）框架及配套的开源Python库，旨在解决生成式人工智能数据集在透明度、问责制和安全性方面的伦理与法律问题。

Abstract: Generative Artificial Intelligence (GAI) has experienced exponential growth in recent years, partly facilitated by the abundance of large-scale open-source datasets. These datasets are often built using unrestricted and opaque data collection practices. While most literature focuses on the development and applications of GAI models, the ethical and legal considerations surrounding the creation of these datasets are often neglected. In addition, as datasets are shared, edited, and further reproduced online, information about their origin, legitimacy, and safety often gets lost. To address this gap, we introduce the Compliance Rating Scheme (CRS), a framework designed to evaluate dataset compliance with critical transparency, accountability, and security principles. We also release an open-source Python library built around data provenance technology to implement this framework, allowing for seamless integration into existing dataset-processing and AI training pipelines. The library is simultaneously reactive and proactive, as in addition to evaluating the CRS of existing datasets, it equally informs responsible scraping and construction of new datasets.

</details>


### [137] [Accelerating Scientific Discovery with Autonomous Goal-evolving Agents](https://arxiv.org/abs/2512.21782)
*Yuanqi Du,Botao Yu,Tianyu Liu,Tony Shen,Junwu Chen,Jan G. Rittig,Kunyang Sun,Yikun Zhang,Zhangde Song,Bo Zhou,Cassandra Masschelein,Yingze Wang,Haorui Wang,Haojun Jia,Chao Zhang,Hongyu Zhao,Martin Ester,Teresa Head-Gordon,Carla P. Gomes,Huan Sun,Chenru Duan,Philippe Schwaller,Wengong Jin*

Main category: cs.AI

TL;DR: SAGA框架通过自动化目标函数设计，提升科学发现代理在多领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学发现代理通常依赖于不完美的代理目标函数，自动化目标函数设计是未满足的核心需求。

Method: SAGA采用双层架构，外层LLM代理分析优化结果、提出新目标并转换为可计算评分函数，内层执行当前目标下的解决方案优化。

Result: SAGA框架在抗生素设计、无机材料设计等功能性DNA序列设计和化学过程设计等多个应用中显著提升了科学发现代理的有效性。

Conclusion: 自动化目标函数设计是科学发现代理的核心需求，SAGA框架通过双层架构成功实现了这一目标，并在多个应用领域展示了其有效性。

Abstract: There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.

</details>


### [138] [SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?](https://arxiv.org/abs/2512.21907)
*Kenny Workman,Zhen Yang,Harihara Muralidharan,Hannah Le*

Main category: cs.AI

TL;DR: SpatialBench是一个包含146个问题的基准测试，用于评估AI代理在复杂空间转录组学数据分析中的表现，结果显示当前模型准确率较低，工具设计对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学分析的规模和复杂性迅速增加，计算分析成为生物发现的主要瓶颈。尽管前沿AI代理在软件工程和通用数据分析方面取得了显著进步，但其是否能从混乱的真实空间数据集中提取生物学见解尚不明确。

Method: 引入了SpatialBench基准，包含146个可验证的问题，源自五种空间技术和七种任务类别的实际空间分析工作流程。

Result: 基准测试显示基础模型的准确率仍然较低（20-38%），模型与任务及平台之间存在强烈交互作用。工具设计对性能有显著影响。

Conclusion: SpatialBench作为一个测量工具和诊断镜头，有助于开发能够忠实、透明且可重复地与真实空间数据集交互的AI代理。

Abstract: Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.

</details>


### [139] [Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks](https://arxiv.org/abs/2512.22106)
*Zubair Shah,Noaman Khan*

Main category: cs.AI

TL;DR: 该论文提出了一种基于博弈论的剪枝方法，将模型组件视为博弈玩家，稀疏性作为均衡结果，实验验证了其竞争性和理论优势。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法通常将稀疏性作为外部约束，通过启发式重要性评分或训练时正则化强制执行，缺乏理论依据。

Method: 通过将参数组（如权重、神经元或滤波器）建模为连续非合作博弈中的玩家，每个玩家选择其在网络中的参与水平以平衡贡献与冗余和竞争。

Result: 实验表明，该方法在标准基准上实现了竞争性的稀疏性-准确性权衡，并提供了一种可解释的理论基础替代方案。

Conclusion: 该论文提出了一种基于博弈论的新视角，将剪枝视为模型组件间战略互动的均衡结果，为现有剪枝方法提供了理论解释和替代方案。

Abstract: Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [140] [Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries](https://arxiv.org/abs/2512.21499)
*Christian Janos Lebeda,Aleksandar Nikolov,Haohua Tang*

Main category: cs.DS

TL;DR: 论文提出了一种基于傅里叶基的因子化机制，用于差分隐私下的边际查询发布，证明其最优性并扩展到更广泛查询类型，运行效率更高。


<details>
  <summary>Details</summary>
Motivation: 重新审视在差分隐私下发布边际查询的任务，旨在提供一种更简单、更高效且最优的算法，以解决现有方法计算复杂度高的问题。

Method: 采用傅里叶基释放查询，通过独立噪声和逆傅里叶变换重建边际查询答案，构建了一种高效的因子化机制。

Result: 提出的算法在加权噪声方差和最大噪声方差方面达到最优，运行时间优于现有方法，并能扩展到更广泛的产品查询和扩展边际查询。

Conclusion: 该论文提出了一种基于傅里叶基的因子化机制，用于在差分隐私下发布边际查询，证明了其在加权噪声方差和最大噪声方差方面的最优性，并扩展到更广泛的产品查询和扩展边际查询，展示了几乎最优的性能。

Abstract: We revisit the task of releasing marginal queries under differential privacy with additive (correlated) Gaussian noise. We first give a construction for answering arbitrary workloads of weighted marginal queries, over arbitrary domains. Our technique is based on releasing queries in the Fourier basis with independent noise with carefully calibrated variances, and reconstructing the marginal query answers using the inverse Fourier transform. We show that our algorithm, which is a factorization mechanism, is exactly optimal among all factorization mechanisms, both for minimizing the sum of weighted noise variances, and for minimizing the maximum noise variance. Unlike algorithms based on optimizing over all factorization mechanisms via semidefinite programming, our mechanism runs in time polynomial in the dataset and the output size. This construction recovers results of Xiao et al. [Neurips 2023] with a simpler algorithm and optimality proof, and a better running time.
  We then extend our approach to a generalization of marginals which we refer to as product queries. We show that our algorithm is still exactly optimal for this more general class of queries. Finally, we show how to embed extended marginal queries, which allow using a threshold predicate on numerical attributes, into product queries. We show that our mechanism is almost optimal among all factorization mechanisms for extended marginals, in the sense that it achieves the optimal (maximum or average) noise variance up to lower order terms.

</details>


### [141] [Fully Dynamic Spectral Sparsification for Directed Hypergraphs](https://arxiv.org/abs/2512.21671)
*Sebastian Forster,Gramoz Goranci,Ali Momeni*

Main category: cs.DS

TL;DR: 提出动态有向超图谱稀疏化算法，实现近乎最优的稀疏化器大小和更新时间，并扩展至并行批量动态设置。


<details>
  <summary>Details</summary>
Motivation: 针对谱超图稀疏化这一图谱稀疏化的自然推广问题，研究如何高效维护动态有向超图的谱稀疏化器。

Method: 采用完全动态算法，实现了近乎最优的稀疏化器大小和摊销更新时间，并在并行批量动态设置中处理批量超边插入或删除。

Result: 算法实现了稀疏化器大小$O(n^2 / \varepsilon ^2 \log ^7 m)$和摊销更新时间$O(r^2 \log ^3 m)$，并在并行批量动态设置中处理$k$个超边操作的摊销工作为$O(kr^2 \log ^3 m)$，深度为$O(\log ^2 m)$。

Conclusion: 该论文提出了一种简单且高效的完全动态算法，用于维护有向超图的谱稀疏化，并在并行批量动态设置中扩展了该方法，填补了该领域基于谱的稀疏化算法的空白。

Abstract: There has been a surge of interest in spectral hypergraph sparsification, a natural generalization of spectral sparsification for graphs. In this paper, we present a simple fully dynamic algorithm for maintaining spectral hypergraph sparsifiers of \textit{directed} hypergraphs. Our algorithm achieves a near-optimal size of $O(n^2 / \varepsilon ^2 \log ^7 m)$ and amortized update time of $O(r^2 \log ^3 m)$, where $n$ is the number of vertices, and $m$ and $r$ respectively upper bound the number of hyperedges and the rank of the hypergraph at any time.
  We also extend our approach to the parallel batch-dynamic setting, where a batch of any $k$ hyperedge insertions or deletions can be processed with $O(kr^2 \log ^3 m)$ amortized work and $O(\log ^2 m)$ depth. This constitutes the first spectral-based sparsification algorithm in this setting.

</details>


### [142] [A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication](https://arxiv.org/abs/2512.21980)
*A. I. Perminov*

Main category: cs.DS

TL;DR: 提出一种新算法，通过自动化搜索优化3×3矩阵乘法，加法复杂度降至58次，操作总数减至81。


<details>
  <summary>Details</summary>
Motivation: 改进非交换环上3×3矩阵乘法的加法复杂度，确保高效性和跨任意域的可移植性。

Method: 结合三元限制翻转图探索与贪婪交集约简进行公共子表达式消除的自动化搜索。

Result: 实现了仅需58次标量加法的秩-23方案，总标量操作次数从83减少到81。

Conclusion: 本文提出了一种新的最先进算法，用于在一般非交换环上进行精确的3×3矩阵乘法，通过秩-23方案仅需58次标量加法，改进了先前的最佳加法复杂度。

Abstract: This paper presents a new state-of-the-art algorithm for exact $3\times3$ matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from $\{-1, 0, 1\}$, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST是一种利用GPU Tensor Cores加速BFS的框架，通过位图结构和优化布局，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现代GPU的Tensor Cores（TC）具有极高的吞吐量，但其针对密集操作设计，难以直接应用于不规则、非结构化的图计算（如BFS）。因此，需要一种高效的方法将边缘操作映射到TC上。

Method: BLEST采用了一种基于位图的结构和优化的执行布局，引入了Binarised Virtual Slice Sets (BVSS)来平衡负载，并应用了两种图重排序策略以提高内存效率。此外，还开发了批处理的SpMSpV乘法模式，利用位运算TC瓦片处理点积。

Result: 实验表明，BLEST在多种实际图数据集上平均比BerryBees、Gunrock和GSWITCH快3.58倍、4.64倍和4.9倍。

Conclusion: BLEST框架通过优化BFS算法在Tensor Cores上的执行，显著提升了性能，平均速度比现有解决方案快3.58至4.9倍。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [144] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: 智慧城市通过数据中心架构优化服务。


<details>
  <summary>Details</summary>
Motivation: 提升城市服务的效率、可持续性和韧性。

Method: 采用数据中心的架构设计。

Result: 智慧城市服务得到优化。

Conclusion: 智慧城市通过数据驱动的架构显著提升了城市服务的效率、可持续性和韧性。

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [145] [Demystifying ARM SME to Optimize General Matrix Multiplications](https://arxiv.org/abs/2512.21473)
*Chencheng Deng,Weiling Yang,Jianbin Fang,Dezun Dong*

Main category: cs.DC

TL;DR: MpGEMM利用ARM SME特性优化GEMM，实现1.23倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代架构如ARM SME虽为矩阵运算提供专用硬件，但现有线性代数库未能充分利用其潜力，尤其是对大矩阵的处理。

Method: 通过系统化分析SME架构，提出优化指南，设计实现了缓存感知分区、高效数据打包（含实时转置）及专用微内核（利用多向量加载和所有可用瓦片寄存器）。

Result: 在Apple M4 Pro上测试，MpGEMM相比厂商优化的Apple Accelerate库平均加速1.23倍，且显著优于其他开源方案。

Conclusion: MpGEMM是一个开源库，通过充分利用ARM SME架构的特性，在多种精度下优化GEMM操作，显著提升了性能。

Abstract: General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.

</details>


### [146] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: FinDEP是一种针对DEP的细粒度任务调度算法，通过优化任务重叠提升MoE推理吞吐量，实验显示最高提升1.61倍性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有DEP方法在共享专家支持和高效任务调度方面的不足，FinDEP旨在通过细粒度任务调度算法提升MoE推理的吞吐量。

Method: FinDEP通过将计算/通信划分为更小的任务以支持细粒度流水线、制定支持可变粒度和顺序的调度优化方案，以及开发高效的求解器来解决大规模搜索空间问题。

Result: 在四个GPU系统上的实验表明，FinDEP相比现有方法最高可提升1.61倍的吞吐量，并在32-GPU系统上实现了1.24倍的加速。

Conclusion: FinDEP显著提升了MoE架构在DEP环境下的推理吞吐量，通过细粒度任务调度和优化算法，实现了最高1.61倍的性能提升。

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [147] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: nncase是一个开源端到端编译框架，通过统一优化解决LLMs部署中的异构性问题，性能优于主流框架。


<details>
  <summary>Details</summary>
Motivation: 传统编译器在内存架构异构性下存在工作流碎片化和高适应成本问题，阻碍了LLMs的高效部署。

Method: nncase采用基于e-graph的术语重写引擎解决阶段排序问题，整合了Auto Vectorize、Auto Distribution和Auto Schedule三个关键模块，以及缓冲区感知的Codegen阶段。

Result: nncase在Qwen3系列模型上优于MLC LLM和Intel IPEX，CPU性能接近手工优化的llama.cpp。

Conclusion: nncase框架通过统一的编译优化方法，显著提升了大型语言模型（LLMs）的部署效率，性能优于主流框架，并接近手工优化的效果。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [148] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 研究分布式LLM推理的资源分配问题，提出性能模型、优化算法及轻量级模拟器，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）部署成本高，现有分布式系统PETALS虽降低了门槛，但资源分配对性能至关重要且优化方法未知。

Method: 通过实验验证的性能模型预测推理性能，将离线优化问题建模为混合整数线性规划问题，并提出了具有性能保证的多项式复杂度算法，以及在线设置的适应方案。

Result: 实验和模拟验证表明，所提解决方案在多样化地理分布式服务器设置中显著优于现有方案。

Conclusion: 本研究首次系统性地研究了分布式LLM推理中的资源分配问题，提出了性能预测模型、离线优化算法及其在线适应方案，显著降低了推理时间，并开发了一个轻量级CPU模拟器以促进未来研究。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [149] [Embedding Samples Dispatching for Recommendation Model Training in Edge Environments](https://arxiv.org/abs/2512.21615)
*Guopeng Li,Haisheng Tan,Chi Zhang,Hongqiu Ni,Zilong Wang,Xinyue Zhang,Yang Xu,Han Tian*

Main category: cs.DC

TL;DR: ESD机制优化嵌入样本分配，减少传输成本并加速DLRM训练。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中嵌入表传输成本高的问题，以提升DLRM训练效率。

Method: 开发了ESD机制，提出HybridDis作为分配决策方法，结合最优算法和启发式算法。

Result: 实验结果显示，ESD减少了高达36.76%的嵌入传输成本，并实现了1.74倍的端到端训练加速。

Conclusion: ESD机制通过优化输入嵌入样本的分配，显著减少了嵌入传输成本，并加速了DLRM训练过程。

Abstract: Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.

</details>


### [150] [Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference](https://arxiv.org/abs/2512.21730)
*Linyi Jiang,Yifei Zhu,Hao Yin,Bo Li*

Main category: cs.DC

TL;DR: Hyperion 是一种云设备协作框架，通过动态调度和加权集成，显著提升超高清视觉数据的处理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的变换器基础模型在处理超高清视频时面临计算或传输的高开销问题，需要一种高效的解决方案。

Method: Hyperion 整合了协作感知的重要性评分器、动态调度器和加权集成器，以解决计算和传输瓶颈。

Result: 实验表明，Hyperion 在各种网络环境下，帧处理速率最高提升1.61倍，准确性最高提升20.2%。

Conclusion: Hyperion 是一种云设备协作框架，通过动态网络利用现成的视觉变换器实现超高清视觉数据的低延迟推理，显著提升了处理速度和准确性。

Abstract: Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.

</details>


### [151] [LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices](https://arxiv.org/abs/2512.21835)
*Mingyu Sun,Xiao Zhang,Shen Qu,Yan Li,Mengbai Xiao,Yuan Yuan,Dongxiao Yu*

Main category: cs.DC

TL;DR: LIME系统通过协作技术，在资源受限的边缘设备上实现了无损且高效的大模型推理。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在边缘设备上因资源限制和网络带宽瓶颈导致的高效推理挑战。

Method: 采用交错流水线并行与模型卸载技术，结合细粒度离线分配调度器和在线内存适配策略。

Result: 在LLaMA3.3-70B-Instruct模型推理中，相比现有基线，LIME在零星和突发请求模式下分别实现了1.7倍和3.7倍的加速。

Conclusion: LIME系统在边缘设备上实现了无损的大模型推理，显著提升了推理速度，且不牺牲模型准确性。

Abstract: Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\times$ and 3.7$\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.

</details>


### [152] [Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View](https://arxiv.org/abs/2512.22035)
*Yanmeng Wang,Zhiwen Dai,Shuai Wang,Jian Zhou,Fu Xiao,Tony Q. S. Quek,Tsung-Hui Chang*

Main category: cs.DC

TL;DR: FedAuto是一种新型联邦微调框架，通过自适应聚合解决连接失败和数据异质性问题，无需先验知识，性能优越且理论保证强。


<details>
  <summary>Details</summary>
Motivation: 现有联邦微调方法在现实网络中的异构连接条件和数据分布下性能受限，缺乏理论保证。

Method: 提出FedAuto框架，采用自适应聚合策略，无需先验网络条件知识，支持即插即用部署。

Result: FedAuto在多种连接失败场景下优于现有方法，支持全参数和部分参数微调，且收敛性得到严格证明。

Conclusion: FedAuto通过自适应聚合有效解决了联邦微调中的连接失败和数据异质性挑战，无需先验网络知识或基础设施修改，提供了更强的理论保证和实际性能。

Abstract: Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.

</details>


### [153] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO是一种针对MoE优化的通信库，通过高效数据重排和轻量级规划，显著提升训练和推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有通信库在处理MoE模型的数据重排时效率低下，导致端到端运行时超过一半的开销。

Method: FUSCO通过捕获细粒度数据布局，并由流水线通信引擎高效执行数据重排，辅以轻量级规划和负载均衡机制，消除冗余通信并分散流量。

Result: 在代表性基准测试中，FUSCO相比NCCL和DeepEP分别实现了最高3.84倍和2.01倍的加速；在端到端任务中，训练延迟降低1.17-1.39倍和1.10-1.19倍，推理首令牌生成延迟降低1.09-1.25倍和1.06-1.16倍。

Conclusion: FUSCO是一种高效的MoE通信库，通过融合数据转换和通信操作，显著提升了大规模MoE模型的训练和推理效率。

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [154] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: PRAXIS是一个基于LLM和依赖图的云事故诊断工具，显著提升分析效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 云事故在生产环境中造成巨大损失，代码和配置问题是主要原因，需要高效诊断工具。

Method: PRAXIS采用LLM驱动的结构化遍历方法，结合服务依赖图（SDG）和程序依赖图（PDG）进行故障定位。

Result: 与现有技术相比，PRAXIS将RCA准确率提升至3.1倍，同时减少3.8倍的token消耗。

Conclusion: PRAXIS显著提升了云事故根本原因分析的准确性和效率，同时降低了资源消耗。

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [155] [Physics-informed Diffusion Models for Multi-scale Prediction of Reference Signal Received Power in Wireless Networks](https://arxiv.org/abs/2512.21475)
*Xiaoqian Qi,Haoye Chai,Yue Wang,Zhaocheng Wang,Yong Li*

Main category: cs.NI

TL;DR: 提出Channel-Diff框架，通过物理建模和扩散模型提升RSRP预测准确性，相比基线方法提升25.15%-37.19%。


<details>
  <summary>Details</summary>
Motivation: 现有RSRP预测研究在准确性和真实性方面存在局限，尤其是难以有效捕捉无线信道中大尺度（LS）和小尺度（SS）信号衰减的复杂特性，且缺乏物理先验知识。

Method: 提出了一种新颖的RSRP预测框架Channel-Diff，通过物理建模和多模态条件，结合物理信息条件扩散模型作为预测网络，并设计了物理先验引导的两阶段训练方案。

Result: Channel-Diff在RSRP预测中显著提升了准确性，并在可迁移性和训练效率方面表现突出。

Conclusion: Channel-Diff框架在RSRP预测中表现出色，准确率相比基线方法提升了25.15%-37.19%，同时在可迁移性和训练效率方面也表现优异。

Abstract: The Reference Signal Received Power (RSRP) is a crucial factor that determines communication performance in mobile networks. Accurately predicting the RSRP can help network operators perceive user experiences and maximize throughput by optimizing wireless resources. However, existing research into RSRP prediction has limitations in accuracy and verisimilitude. Theoretical derivations and existing data-driven methods consider only easily quantifiable Large-Scale (LS) information, and struggle to effectively capture the intertwined LS and Small-Scale (SS) signal attenuation characteristics of the wireless channel. Moreover, the lack of prior physical knowledge leads to weak accuracy, interpretability, and transferability. In this paper, we propose a novel RSRP prediction framework, Channel-Diff. This framework physically models LS and SS attenuation using multimodal conditions and employs physics-informed conditional diffusion models as the prediction network. Channel-Diff extracts prior physical information that characterises the signal propagation process from network parameters and multi-attribute maps of the urban spatial environment. It provides LS physical priors through large-scale propagation modelling and shadow-occlusion modelling, and SS physical priors through multipath propagation modelling and urban microenvironment feature extraction. We design a physical-prior-guided two-stage training scheme with a noise prior guidance mechanism, enabling effective fusion of multi-scale physical knowledge with the diffusion models. Evaluations demonstrate Channel-Diff exhibits excellent performance on RSRP prediction, achieving at least 25.15%-37.19% improvement in accuracy relative to baseline methods. Additionally, the model also demonstrated outstanding performance in terms of transferability and training efficiency.

</details>


### [156] [Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities](https://arxiv.org/abs/2512.21717)
*Abd Ullah Khan,Adnan Shahid,Haejoon Jung,Hyundong Shin*

Main category: cs.NI

TL;DR: 本文探讨了SAGIN-enabled MC的挑战及AI驱动的资源优化潜力，通过代理RL案例研究展示了学习型方法的有效性。


<details>
  <summary>Details</summary>
Motivation: SAGIN-enabled MC作为下一代网络的关键推动者，但其异构性（TN和NTN）引入了复杂的架构挑战，尤其是多链路类型的资源分配问题。

Method: 本文回顾了SAGIN-enabled MC的当前发展，并概述了实施中的关键挑战。提出了一个基于代理强化学习（RL）的资源分配优化案例研究。

Result: 结果表明，基于学习的方法能有效处理复杂场景，显著提升网络性能（如延迟和容量），同时以适度的功耗增加为可接受的权衡。

Conclusion: 学习型方法能有效处理复杂场景，显著提升网络性能（如延迟和容量），同时以适度的功耗增加为可接受的权衡。未来研究方向包括实现高效的SAGIN-enabled MC。

Abstract: Space-air-ground-integrated network (SAGIN)-enabled multiconnectivity (MC) is emerging as a key enabler for next-generation networks, enabling users to simultaneously utilize multiple links across multi-layer non-terrestrial networks (NTN) and multi-radio access technology (multi-RAT) terrestrial networks (TN). However, the heterogeneity of TN and NTN introduces complex architectural challenges that complicate MC implementation. Specifically, the diversity of link types, spanning air-to-air, air-to-space, space-to-space, space-to-ground, and ground-to-ground communications, renders optimal resource allocation highly complex. Recent advancements in reinforcement learning (RL) and agentic artificial intelligence (AI) have shown remarkable effectiveness in optimal decision-making in complex and dynamic environments. In this paper, we review the current developments in SAGIN-enabled MC and outline the key challenges associated with its implementation. We further highlight the transformative potential of AI-driven approaches for resource optimization in a heterogeneous SAGIN environment. To this end, we present a case study on resource allocation optimization enabled by agentic RL for SAGIN-enabled MC involving diverse radio access technologies (RATs). Results show that learning-based methods can effectively handle complex scenarios and substantially enhance network performance in terms of latency and capacity while incurring a moderate increase in power consumption as an acceptable tradeoff. Finally, open research problems and future directions are presented to realize efficient SAGIN-enabled MC.

</details>


### [157] [Meta-Learning-Based Handover Management in NextG O-RAN](https://arxiv.org/abs/2512.22022)
*Michail Kalntis,George Iosifidis,José Suárez-Varela,Andra Lutu,Fernando A. Kuipers*

Main category: cs.NI

TL;DR: 本文提出CONTRA框架，通过联合优化THO和CHO，在O-RAN中实现高效切换控制，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统切换和条件切换在信号、资源使用和可靠性方面存在复杂权衡，需要自适应和鲁棒的切换控制。

Method: 提出了CONTRA框架，包含两种变体：一种是用户预先分配HO类型，另一种是基于系统条件动态决定HO类型。采用元学习算法，确保性能接近完美未来信息的预言机。

Result: CONTRA在动态和真实场景中优于3GPP兼容和强化学习基线，提升了用户吞吐量并降低了切换成本。

Conclusion: CONTRA框架在O-RAN架构中联合优化了传统切换（THO）和条件切换（CHO），显著提升了用户吞吐量并降低了切换成本，为6G网络的灵活智能控制提供了可行方案。

Abstract: While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.

</details>


### [158] [Schwarz Information Criterion Aided Multi-Armed Bandit for Decentralized Resource Allocation in Dynamic LoRa Networks](https://arxiv.org/abs/2512.22089)
*Ryotai Ariyoshi,Aohan Li,Mikio Hasegawa,Tomoaki Ohtsuki,Miao Pan,Zhu Han*

Main category: cs.NI

TL;DR: 轻量级分布式学习方法结合UCB1-tuned和SIC，提升LoRa网络在动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统UCB1-tuned算法在动态环境中适应性不足，需一种轻量级方法优化传输成功率和能源效率。

Method: 每个LoRa终端设备采用UCB1-tuned算法选择传输参数（信道、功率、带宽），并结合SIC检测环境变化以重置学习历史。

Result: 实验证明，该方法在传输成功率、能源效率和适应性上优于传统UCB1-tuned算法。

Conclusion: 本文提出的结合UCB1-tuned算法和SIC的轻量级分布式学习方法，在动态通信环境中显著提升了LoRa网络的传输成功率、能源效率和适应性。

Abstract: This paper proposes a lightweight distributed learning method for transmission parameter selection in Long Range (LoRa) networks that can adapt to dynamic communication environments. In the proposed method, each LoRa End Device (ED) employs the Upper Confidence Bound (UCB)1-tuned algorithm to select transmission parameters including channel, transmission power, and bandwidth. The transmission parameters are selected based on the acknowledgment (ACK) feedback returned from the gateway after each transmission and the corresponding transmission energy consumption. Hence, it enables devices to simultaneously optimize transmission success rate and energy efficiency in a fully distributed manner. However, although UCB1-tuned based method is effective under stationary conditions, it suffers from slow adaptation in dynamic environments due to its strong reliance on historical observations. To address this limitation, we integrate the Schwarz Information Criterion (SIC) to our proposed method. SIC is adopted because it enables low-cost detection of changes in the communication environment, making it suitable for implementation on resource-constrained LoRa EDs. When a change is detected by SIC, the learning history of UCB1-tuned is reset, allowing rapid re-learning under the new conditions. Experimental results using real LoRa devices demonstrate that the proposed method achieves superior transmission success rate, energy efficiency, and adaptability compared with the conventional UCB1-tuned algorithm without SIC.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [159] [Graph Drawing Stress Model with Resistance Distances](https://arxiv.org/abs/2512.21901)
*Yosuke Onoue*

Main category: cs.GR

TL;DR: 本文提出基于电阻距离的新图绘制方法，克服传统限制，实验效果更优，算法复杂度低。


<details>
  <summary>Details</summary>
Motivation: 挑战传统基于图论最短距离的应力图绘制方法，提出电阻距离能更好地捕捉图的全局结构。

Method: 提出了一种基于电阻距离的新方法，并开发了线性时间算法Omega，结合快速电阻距离嵌入和随机节点对采样进行随机梯度下降（SGD）。

Result: 实验证明该方法在邻域保持和聚类忠实度上表现更优，算法复杂度保持在O(|E|)。

Conclusion: 本文通过引入基于电阻距离的新范式，成功克服了传统图绘制方法在理论和计算上的限制，提供了一种更实用且可扩展的网络可视化解决方案。

Abstract: This paper challenges the convention of using graph-theoretic shortest distance in stress-based graph drawing. We propose a new paradigm based on resistance distance, derived from the graph Laplacian's spectrum, which better captures global graph structure. This approach overcomes theoretical and computational limitations of traditional methods, as resistance distance admits a natural isometric embedding in Euclidean space. Our experiments demonstrate improved neighborhood preservation and cluster faithfulness. We introduce Omega, a linear-time graph drawing algorithm that integrates a fast resistance distance embedding with random node-pair sampling for Stochastic Gradient Descent (SGD). This comprehensive random sampling strategy, enabled by efficient pre-computation of resistance distance embeddings, is more effective and robust than pivot-based sampling used in prior algorithms, consistently achieving lower and more stable stress values. The algorithm maintains $O(|E|)$ complexity for both weighted and unweighted graphs. Our work establishes a connection between spectral graph theory and stress-based layouts, providing a practical and scalable solution for network visualization.

</details>
