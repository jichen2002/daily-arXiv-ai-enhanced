{"id": "2601.01031", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.01031", "abs": "https://arxiv.org/abs/2601.01031", "authors": ["Bharadwaj Veeravalli"], "title": "A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations", "comment": null, "summary": "We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.", "AI": {"tldr": "Developed an MPCC-DLT framework for DSS, optimizing load allocation and completion time, with practical extensions for real-time task handling.", "motivation": "To address the challenges of concurrent data dissemination, parallel computation, and result return in heterogeneous onboard processing and inter-satellite link conditions within distributed satellite systems.", "method": "The authors propose a formulation with closed-form expressions for optimal load allocation and completion time, considering computation speed, link bandwidth, and result-size overhead. They also derive deadline feasibility conditions and extend the framework with a real-time admission control mechanism.", "result": "Simulations show latency reduction for distributable tasks but diminishing returns for communication-heavy tasks due to overheads. The extended framework effectively handles stochastic task arrivals and deadlines.", "conclusion": "This work presents the first analytically tractable MPCC-DLT model for DSS, offering practical insights for application-aware scheduling and system-level design of future satellite constellations."}}
{"id": "2601.01125", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01125", "abs": "https://arxiv.org/abs/2601.01125", "authors": ["Mohammad Goudarzi", "Arash Shaghaghi", "Zhiyu Wang", "Rajkumar Buyya"], "title": "Performance and Security Aware Distributed Service Placement in Fog Computing", "comment": null, "summary": "The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.", "AI": {"tldr": "SPA-DDRL\u6846\u67b6\u901a\u8fc7\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u96fe\u8ba1\u7b97\u4e2d\u7684\u670d\u52a1\u90e8\u7f72\uff0c\u517c\u987e\u54cd\u5e94\u65f6\u95f4\u548c\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u96fe\u8ba1\u7b97\u4e2d\u5f02\u6784\u8d44\u6e90\u3001\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u591a\u6837\u5316\u5b89\u5168\u9700\u6c42\u5e26\u6765\u7684\u670d\u52a1\u90e8\u7f72\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u3001\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u79bb\u7b56\u7565\u6821\u6b63\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPA-DDRL\u5728\u54cd\u5e94\u65f6\u95f4\u548c\u5b89\u5168\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u6240\u6709\u7cfb\u7edf\u89c4\u6a21\u4e0b\u5747\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "SPA-DDRL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u96fe\u8ba1\u7b97\u4e2d\u7684\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u548c\u5b89\u5168\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u54cd\u5e94\u65f6\u95f4\u6539\u5584\u4e8616.3%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8633%\u3002"}}
{"id": "2601.01209", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01209", "abs": "https://arxiv.org/abs/2601.01209", "authors": ["Xin Tan", "Yicheng Feng", "Yu Zhou", "Yimin Jiang", "Yibo Zhu", "Hong Xu"], "title": "OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL", "comment": null, "summary": "Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.", "AI": {"tldr": "OrchestrRL\u548cRFabric\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u548c\u7f51\u7edc\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u5206\u89e3\u5f0f\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5206\u89e3\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u751f\u6210\u9636\u6bb5\u6210\u4e3a\u74f6\u9888\uff0c\u4e14\u7f51\u7edc\u6d41\u91cf\u6a21\u5f0f\u52a8\u6001\u53d8\u5316\uff0c\u4f20\u7edf\u7f51\u7edc\u67b6\u6784\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "OrchestrRL\u91c7\u7528\u81ea\u9002\u5e94\u8ba1\u7b97\u8c03\u5ea6\u5668\u52a8\u6001\u8c03\u6574\u5e76\u884c\u5ea6\uff0cRFabric\u5219\u901a\u8fc7\u53ef\u91cd\u6784\u6df7\u5408\u5149\u7535\u7f51\u7edc\u5b9e\u65f6\u8c03\u6574\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOrchestrRL\u572848\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e861.40\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0cRFabric\u5728\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u9759\u6001Fat-Tree\u7f51\u7edc\u3002", "conclusion": "OrchestrRL\u548cRFabric\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u5206\u89e3\u5f0f\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u4e2d\u3002"}}
{"id": "2601.01310", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01310", "abs": "https://arxiv.org/abs/2601.01310", "authors": ["Songyu Zhang", "Aaron Tam", "Myungjin Lee", "Shixiong Qi", "K. K. Ramakrishnan"], "title": "Making MoE based LLM inference resilient with Tarragon", "comment": null, "summary": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.", "AI": {"tldr": "Tarragon\u662f\u4e00\u4e2a\u5bb9\u9519\u7684MoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u6545\u969c\u57df\u548c\u81ea\u6108\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u6545\u969c\u5bfc\u81f4\u7684\u505c\u6ede\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728MoE\u6a21\u578b\u90e8\u7f72\u4e2d\uff0c\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9\u6545\u969c\u4f1a\u89e6\u53d1\u6574\u4e2a\u670d\u52a1\u7684\u91cd\u542f\uff0c\u5bfc\u81f4\u7d2f\u79ef\u8fdb\u5ea6\u4e22\u5931\u548c\u63a8\u7406\u7ba1\u9053\u505c\u6ede\uff0c\u8fd9\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684LLM\u670d\u52a1\u4e0d\u9002\u7528\u3002", "method": "Tarragon\u901a\u8fc7\u53ef\u91cd\u6784\u7684\u6570\u636e\u8def\u5f84\u548c\u81ea\u6108\u673a\u5236\uff0c\u5206\u79bb\u6ce8\u610f\u529b\u5de5\u4f5c\u8005\uff08AWs\uff09\u548c\u4e13\u5bb6\u5de5\u4f5c\u8005\uff08EWs\uff09\u7684\u6545\u969c\u57df\uff0c\u5e76\u91c7\u7528\u5f02\u6b65KV\u7f13\u5b58\u68c0\u67e5\u70b9\u548c\u5f71\u5b50\u4e13\u5bb6\u6280\u672f\u6765\u964d\u4f4e\u6062\u590d\u6210\u672c\u3002", "result": "\u4e0eMegaScale-Infer\u76f8\u6bd4\uff0cTarragon\u5c06\u6545\u969c\u5bfc\u81f4\u7684\u505c\u6ede\u65f6\u95f4\u51cf\u5c11\u4e86160-213\u500d\uff08\u4ece\u7ea664\u79d2\u964d\u81f30.3-0.4\u79d2\uff09\uff0c\u4e14\u5728\u65e0\u6545\u969c\u65f6\u6027\u80fd\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "Tarragon\u663e\u8457\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u5c06\u6545\u969c\u5f71\u54cd\u9650\u5236\u5728\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7ba1\u9053\u7684\u6301\u7eed\u8fdb\u5c55\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u6545\u969c\u5bfc\u81f4\u7684\u505c\u6ede\u65f6\u95f4\u3002"}}
{"id": "2601.01288", "categories": ["cs.GR", "cs.AI", "cs.PF", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01288", "abs": "https://arxiv.org/abs/2601.01288", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Benjamin Ultan Cowley"], "title": "PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS", "comment": null, "summary": "Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.", "AI": {"tldr": "PyBatchRender \u662f\u4e00\u4e2a\u9ad8\u6027\u80fd Python 3D \u6e32\u67d3\u5e93\uff0c\u4e13\u4e3a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u6279\u5904\u7406\u6e32\u67d3\uff0c\u6027\u80fd\u8fdc\u8d85\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709 3D \u6e32\u67d3\u73af\u5883\u5728\u6027\u80fd\u548c\u590d\u6742\u6027\u4e0a\u7684\u74f6\u9888\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8005\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5de5\u5177\u3002", "method": "\u57fa\u4e8e Panda3D \u6e38\u620f\u5f15\u64ce\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u6279\u5904\u7406\u6e32\u67d3\u6280\u672f\u5b9e\u73b0\u9ad8\u8fbe 1000 \u500d\u7684\u52a0\u901f\uff0c\u652f\u6301\u5b8c\u5168\u7528 Python \u5feb\u901f\u6784\u5efa\u81ea\u5b9a\u4e49\u573a\u666f\u3002", "result": "PyBatchRender \u5728\u7b80\u5355\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc7 100 \u4e07 FPS \u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u6027\u80fd\u5ab2\u7f8e\u6700\u5148\u8fdb\u7684 C++ \u5f15\u64ce\u3002", "conclusion": "PyBatchRender \u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684 Python 3D \u6e32\u67d3\u5e93\uff0c\u4e13\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u4e86\u9ad8\u541e\u5410\u91cf\u548c\u6613\u7528\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u6027\u80fd\u4e0e\u590d\u6742\u6027\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.00979", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.00979", "abs": "https://arxiv.org/abs/2601.00979", "authors": ["Valentin Blomer", "Kai-Uwe Bux"], "title": "The cost of cyclic permutations and remainder sums in the Euclidean algorithm", "comment": "32 pages, 7 figures", "summary": "We discuss a modification to the Gries-Mills block swapping scheme for in-place rotation with average costs of 1.85 moves per element and worst case performance still at 3 moves per element. Analysis of the average case relies on the asymptotic behavior of the sum of remainders in the Euclidean algorithm.", "AI": {"tldr": "\u8bba\u6587\u6539\u8fdb\u4e86Gries-Mills\u5757\u4ea4\u6362\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u539f\u5730\u65cb\u8f6c\u7684\u5e73\u5747\u6027\u80fd\u81f31.85\u6b21\u79fb\u52a8/\u5143\u7d20\uff0c\u6700\u574f\u60c5\u51b5\u4ecd\u4e3a3\u6b21\u79fb\u52a8/\u5143\u7d20\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684Gries-Mills\u5757\u4ea4\u6362\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u539f\u5730\u65cb\u8f6c\u7684\u5e73\u5747\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u51e0\u91cc\u5f97\u7b97\u6cd5\u4e2d\u4f59\u6570\u548c\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u7814\u7a76\u4e86\u5e73\u5747\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6539\u8fdb\u540e\u7684\u65b9\u6848\u5b9e\u73b0\u4e86\u5e73\u5747\u6bcf\u4e2a\u5143\u7d201.85\u6b21\u79fb\u52a8\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u574f\u60c5\u51b5\u4e0b3\u6b21\u79fb\u52a8\u7684\u4e0a\u9650\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Gries-Mills\u5757\u4ea4\u6362\u65b9\u6848\uff0c\u7528\u4e8e\u539f\u5730\u65cb\u8f6c\uff0c\u5e73\u5747\u6210\u672c\u4e3a\u6bcf\u4e2a\u5143\u7d201.85\u6b21\u79fb\u52a8\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u4ecd\u4e3a\u6bcf\u4e2a\u5143\u7d203\u6b21\u79fb\u52a8\u3002"}}
{"id": "2601.00974", "categories": ["cs.NI", "cs.DM", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00974", "abs": "https://arxiv.org/abs/2601.00974", "authors": ["Inna Voloshchuk", "Hayden Jananthan", "Chansup Byun", "Jeremy Kepner"], "title": "Improving the Graph Challenge Reference Implementation", "comment": "Presented at IEEE MIT URTC 2025", "summary": "The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u91cd\u6784\u548c\u5e76\u884c\u5316\u4f18\u5316\u4e86Graph Challenge\u7684\u53c2\u8003\u4ee3\u7801\uff0c\u4ee3\u7801\u91cf\u51cf\u5c1167%\uff0c\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u57fa\u7840\u3002", "motivation": "\u63d0\u9ad8Graph Challenge\u53c2\u8003\u4ee3\u7801\u7684\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u4ee5\u4fc3\u8fdb\u5927\u89c4\u6a21\u56fe\u548c\u7a00\u758f\u6570\u636e\u5206\u6790\u7684\u521b\u65b0\u3002", "method": "\u4f7f\u7528pMatlab\u548cpPython\u5206\u5e03\u5f0f\u6570\u7ec4\u7f16\u7a0b\u5e93\uff0c\u901a\u8fc7\u5e76\u884c\u6620\u5c04\u5bf9\u6570\u636e\u8fdb\u884c\u5e76\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f18\u5316\u4e86\u539f\u59cbPython\u5b9e\u73b0\u3002", "result": "\u4ee3\u7801\u4ece1000\u884c\u7f29\u51cf\u81f3325\u884c\uff0c\u529f\u80fd\u5b8c\u6574\uff0c\u5e76\u884c\u5904\u7406\u80fd\u529b\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u6d41\u91cf\u77e9\u9635\u6c42\u548c\u4e0e\u5206\u6790\u7684\u53ef\u6269\u5c55\u6027\u80fd\u3002", "conclusion": "\u91cd\u6784\u540e\u7684\u4ee3\u7801\u4e0d\u4ec5\u663e\u8457\u51cf\u5c11\u4e86\u4ee3\u7801\u91cf\uff0867%\uff09\uff0c\u8fd8\u4fdd\u6301\u4e86\u5b8c\u6574\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u6620\u5c04\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3aGraph Challenge\u53c2\u4e0e\u8005\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u9ad8\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2601.00812", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00812", "abs": "https://arxiv.org/abs/2601.00812", "authors": ["Takashi Ushio", "Kazuhiro Onishi", "Hideyoshi Yanagisawa"], "title": "Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements", "comment": "This article has been accepted for publication in IEEE Access and will be published shortly", "summary": "Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified \"pleasantness,\" \"surprise,\" and \"habituation\" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected \"pleasantness\" associated with brand presentation, BS has captured \"surprise\" arising from informational complexity, and UN has reflected \"surprise\" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u81ea\u7531\u80fd\u539f\u7406\u91cf\u5316\u5e7f\u544a\u89c6\u9891\u60c5\u611f\uff0cKLD\u3001BS\u3001UN\u5206\u522b\u5bf9\u5e94\u6109\u60a6\u3001\u60ca\u559c\u548c\u4e60\u60ef\u5316\uff0c\u65b9\u6cd5\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u3002", "motivation": "\u4e3a\u65e0\u9700\u4f9d\u8d56\u751f\u7406\u4fe1\u53f7\u6216\u4e3b\u89c2\u8bc4\u5206\u7684\u53ef\u89e3\u91ca\u60c5\u611f\u4f30\u8ba1\u5efa\u7acb\u65b9\u6cd5\u57fa\u7840\u3002", "method": "\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\uff0c\u4ece\u5e7f\u544a\u89c6\u9891\u7684\u573a\u666f\u7ea7\u8868\u8fbe\u7279\u5f81\u4e2d\u91cf\u5316\u2018\u6109\u60a6\u611f\u2019\u3001\u2018\u60ca\u559c\u2019\u548c\u2018\u4e60\u60ef\u5316\u2019\uff0c\u5229\u7528KLD\u3001BS\u548cUN\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660eKLD\u53cd\u6620\u54c1\u724c\u5448\u73b0\u7684\u2018\u6109\u60a6\u611f\u2019\uff0cBS\u6355\u6349\u4fe1\u606f\u590d\u6742\u6027\u5f15\u53d1\u7684\u2018\u60ca\u559c\u2019\uff0cUN\u53cd\u6620\u5143\u7d20\u7c7b\u578b\u548c\u7a7a\u95f4\u6392\u5217\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u2018\u60ca\u559c\u2019\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u79cd\u60c5\u611f\u6a21\u5f0f\u3002\u65b9\u6cd5\u5728\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u81ea\u7531\u80fd\u539f\u7406\u91cf\u5316\u4e86\u5e7f\u544a\u89c6\u9891\u4e2d\u7684\u60c5\u611f\u53cd\u5e94\uff0c\u8bc1\u660e\u4e86KLD\u3001BS\u548cUN\u5206\u522b\u4e0e\u2018\u6109\u60a6\u611f\u2019\u3001\u2018\u60ca\u559c\u2019\u548c\u2018\u4e60\u60ef\u5316\u2019\u76f8\u5173\uff0c\u65b9\u6cd5\u5177\u6709\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u6765\u53ef\u6269\u5c55\u66f4\u591a\u8868\u8fbe\u5143\u7d20\u5e76\u9a8c\u8bc1\u4e3b\u89c2\u8bc4\u5206\u3002"}}
{"id": "2601.01042", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01042", "abs": "https://arxiv.org/abs/2601.01042", "authors": ["Zixiao Zhao", "Yanjie Jiang", "Hui Liu", "Kui Liu", "Lu Zhang"], "title": "SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities", "comment": "Accepted by ICSE 2026", "summary": "Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \\textbf{SeRe}, a \\textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \\textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSeRe\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u6784\u5efa\u5b89\u5168\u76f8\u5173\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u9a8c\u8bc1\u5176\u4e0e\u5b9e\u9645\u5206\u5e03\u7684\u543b\u5408\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5b89\u5168\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6570\u636e\u96c6\u548c\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0c\u7f3a\u4e4f\u5b89\u5168\u7279\u5b9a\u6ce8\u91ca\u6216\u89c4\u6a21\u4e0d\u8db3\uff0c\u65e0\u6cd5\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u9884\u6d4b\uff0c\u4ee5\u9ad8\u7cbe\u5ea6\u548c\u5408\u7406\u53ec\u56de\u7387\u6784\u5efa\u4e86\u5b89\u5168\u76f8\u5173\u7684\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u96c6SeRe\u3002", "result": "\u901a\u8fc7\u7cbe\u7ec6\u8c03\u6574\u7684\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u4ece373,824\u4e2a\u539f\u59cb\u5ba1\u67e5\u5b9e\u4f8b\u4e2d\u63d0\u53d6\u4e866,732\u4e2a\u5b89\u5168\u76f8\u5173\u8bc4\u8bba\uff0c\u786e\u4fdd\u8de8\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u8868\u6027\u3002\u7edf\u8ba1\u5206\u6790\u8868\u660eSeRe\u4e0e\u5b9e\u9645\u5b89\u5168\u76f8\u5173\u5ba1\u67e5\u5206\u5e03\u4e00\u81f4\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165SeRe\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u65e8\u5728\u63a8\u52a8\u81ea\u52a8\u5316\u5b89\u5168\u4ee3\u7801\u5ba1\u67e5\u7684\u7814\u7a76\uff0c\u5e76\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u5b89\u5168\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.00969", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00969", "abs": "https://arxiv.org/abs/2601.00969", "authors": ["Ali Salamatian", "Ke", "Ren", "Kieran Pattison", "Cyrus Neary"], "title": "Value Vision-Language-Action Planning & Search", "comment": "10 pages, 3 figures", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.", "AI": {"tldr": "V-VLAPS\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u4ef7\u503c\u51fd\u6570\u548cMCTS\uff0c\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4f9d\u8d56\u884c\u4e3a\u514b\u9686\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u4e14\u73b0\u6709MCTS\u65b9\u6cd5\u4ec5\u4f9d\u8d56VLA\u5148\u9a8c\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u56de\u62a5\u7684\u51c6\u786e\u4f30\u8ba1\u3002", "method": "\u5728\u56fa\u5b9aVLA\u9aa8\u5e72\u7f51\u7edc\uff08Octo\uff09\u7684\u6f5c\u5728\u8868\u793a\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u4e3aMCTS\u63d0\u4f9b\u660e\u786e\u7684\u4ef7\u503c\u4fe1\u53f7\u3002", "result": "\u5728LIBERO\u673a\u5668\u4eba\u64cd\u4f5c\u5957\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cV-VLAPS\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u4e865\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u4e865-15%\u7684MCTS\u6a21\u62df\u6b21\u6570\u3002", "conclusion": "V-VLAPS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2601.00814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00814", "abs": "https://arxiv.org/abs/2601.00814", "authors": ["Abhishek Kumar"], "title": "Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections", "comment": null, "summary": "The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7cfb\u7edf\uff0c\u901a\u8fc7\u589e\u5f3a\u5b9e\u4f53\u63cf\u8ff0\u548c\u4f7f\u7528\u591a\u8bed\u8a00Transformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u63d0\u5347\u5bf9\u9f50\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\uff0c\u901a\u8fc7\u65b0\u9896\u6280\u672f\u751f\u6210\u63cf\u8ff0\u4ee5\u4e30\u5bcc\u672c\u4f53\u5b9e\u4f53\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u91c7\u7528\u5fae\u8c03\u540e\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\u751f\u6210\u66f4\u597d\u7684\u5d4c\u5165\u3002\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u627e\u5230\u6b63\u672c\u4f53\u5b9e\u4f53\u5bf9\uff0c\u518d\u5e94\u7528\u9608\u503c\u8fc7\u6ee4\u4fdd\u7559\u9ad8\u76f8\u4f3c\u5ea6\u5b9e\u4f53\u3002", "result": "\u5728OAEI-2022 multifarm track\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fbe\u523071%\u7684F1\u5206\u6570\uff0878%\u53ec\u56de\u7387\u548c65%\u7cbe\u786e\u7387\uff09\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7ba1\u9053\u80fd\u591f\u6709\u6548\u6355\u6349\u7ec6\u5fae\u7684\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\uff0cF1\u5206\u6570\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e8616%\u3002"}}
{"id": "2601.01500", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01500", "abs": "https://arxiv.org/abs/2601.01500", "authors": ["Jinxiao Zhang", "Yunpu Xu", "Xiyong Wu", "Runmin Dong", "Shenggan Cheng", "Yi Zhao", "Mengxuan Chen", "Qinrui Zheng", "Jianting Liu", "Haohuan Fu"], "title": "DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster", "comment": null, "summary": "Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.", "AI": {"tldr": "DiT-HC\u662f\u9996\u4e2a\u5728HPC CPU\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u548c\u6269\u5c55\u751f\u6210\u6a21\u578bDiT\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u9879\u5173\u952e\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86CPU\u96c6\u7fa4\u5728\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u65b0\u786c\u4ef6\u7279\u6027\uff08\u5982\u77e9\u9635\u52a0\u901f\u5355\u5143\u548c\u9ad8\u5e26\u5bbd\u5185\u5b58\uff09\u7684\u53d1\u5c55\uff0cCPU\u96c6\u7fa4\u4e3a\u52a0\u901f\u548c\u6269\u5c55\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4fc3\u8fdb\u4e86AI\u4e0e\u79d1\u5b66\u8ba1\u7b97\u7684\u7edf\u4e00\u3002", "method": "DiT-HC\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u5e26\u6709AutoMem\u7684\u901a\u4fe1\u81ea\u7531\u5f20\u91cf\u5e76\u884c(CFTP)\uff0c(2) HCOps\u4f18\u5316\u7684GEMM\u548c\u7b97\u5b50\u5185\u6838\uff0c(3) \u81ea\u5b9a\u4e49MPI\u540e\u7aef\uff0c\u91cd\u53e0\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u5185\u5b58\u79fb\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDiT-HC\u5728256\u4e2a\u8282\u70b9\u4e0a\u5b9e\u73b0\u4e868.2\u81f387.7\u500d\u7684\u52a0\u901f\uff0c\u5f31\u6269\u5c55\u6548\u7387\u8fbe\u523090.6%\u3002", "conclusion": "DiT-HC\u5c55\u793a\u4e86\u5728CPU\u96c6\u7fa4\u4e0a\u5927\u89c4\u6a21\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765HPC\u4e0eAI\u7684\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2601.01361", "categories": ["cs.GR", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01361", "abs": "https://arxiv.org/abs/2601.01361", "authors": ["Duosi Jin", "Jianqiu Xu", "Guidong Zhang"], "title": "VARTS: A Tool for the Visualization and Analysis of Representative Time Series Data", "comment": null, "summary": "Large-scale time series visualization often suffers from excessive visual clutter and redundant patterns, making it difficult for users to understand the main temporal trends. To address this challenge, we present VARTS, an interactive visual analytics tool for representative time series selection and visualization. Building upon our previous work M4-Greedy, VARTS integrates M4-based sampling, DTW-based similarity computation, and greedy selection into a unified workflow for the identification and visualization of representative series. The tool provides a responsive graphical interface that allows users to import time series datasets, perform representative selection, and visualize both raw and reduced data through multiple coordinated views. By reducing redundancy while preserving essential data patterns, VARTS effectively enhances visual clarity and interpretability for large-scale time series analysis. The demo video is available at https://youtu.be/mS9f12Rf0jo.", "AI": {"tldr": "VARTS\u662f\u4e00\u6b3e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u901a\u8fc7M4\u91c7\u6837\u3001DTW\u76f8\u4f3c\u5ea6\u548c\u8d2a\u5a6a\u9009\u62e9\uff0c\u5e2e\u52a9\u7528\u6237\u4ece\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u6570\u636e\u5e76\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u53ef\u89c6\u5316\u5e38\u56e0\u89c6\u89c9\u6df7\u4e71\u548c\u5197\u4f59\u6a21\u5f0f\u800c\u96be\u4ee5\u7406\u89e3\u4e3b\u8981\u65f6\u95f4\u8d8b\u52bf\u3002", "method": "VARTS\u7ed3\u5408\u4e86M4\u91c7\u6837\u3001DTW\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u8d2a\u5a6a\u9009\u62e9\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u54cd\u5e94\u5f0f\u56fe\u5f62\u754c\u9762\uff0c\u652f\u6301\u7528\u6237\u5bfc\u5165\u6570\u636e\u3001\u6267\u884c\u4ee3\u8868\u6027\u9009\u62e9\u5e76\u901a\u8fc7\u591a\u89c6\u56fe\u53ef\u89c6\u5316\u539f\u59cb\u548c\u7b80\u5316\u6570\u636e\u3002", "result": "VARTS\u6210\u529f\u51cf\u5c11\u4e86\u5197\u4f59\u5e76\u4fdd\u7559\u4e86\u5173\u952e\u6570\u636e\u6a21\u5f0f\uff0c\u589e\u5f3a\u4e86\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "VARTS\u901a\u8fc7\u6574\u5408M4\u91c7\u6837\u3001DTW\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u8d2a\u5a6a\u9009\u62e9\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.01388", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.01388", "abs": "https://arxiv.org/abs/2601.01388", "authors": ["Seoyong Lee", "Jinho Lee"], "title": "AGIS: Fast Approximate Graph Pattern Mining with Structure-Informed Sampling", "comment": "VLDB 2026", "summary": "Approximate Graph Pattern Mining (AGPM) is essential for analyzing large-scale graphs where exact counting is computationally prohibitive. While there exist numerous sampling-based AGPM systems, they all rely on uniform sampling and overlook the underlying probability distribution. This limitation restricts their scalability to a broader range of patterns.\n  In this paper, we introduce AGIS, an extremely fast AGPM system capable of counting arbitrary patterns from huge graphs. AGIS employs structure-informed neighbor sampling, a novel sampling technique that deviates from uniformness but allocates specific sampling probabilities based on the pattern structure. We first derive the ideal sampling distribution for AGPM and then present a practical method to approximate it. Furthermore, we develop a method that balances convergence speed and computational overhead, determining when to use the approximated distribution.\n  Experimental results demonstrate that AGIS significantly outperforms the state-of-the-art AGPM system, achieving 28.5x geometric mean speedup and more than 100,000x speedup in specific cases. Furthermore, AGIS is the only AGPM system that scales to graphs with tens of billions of edges and robustly handles diverse patterns, successfully providing accurate estimates within seconds. We will open-source AGIS to encourage further research in this field.", "AI": {"tldr": "AGIS\u901a\u8fc7\u975e\u5747\u5300\u91c7\u6837\u6280\u672f\u663e\u8457\u63d0\u5347\u56fe\u6a21\u5f0f\u6316\u6398\u6548\u7387\uff0c\u6210\u4e3a\u9996\u4e2a\u80fd\u5904\u7406\u8d85\u5927\u89c4\u6a21\u56fe\u6570\u636e\u7684AGPM\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684AGPM\u7cfb\u7edf\u4f9d\u8d56\u5747\u5300\u91c7\u6837\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u6982\u7387\u5206\u5e03\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u66f4\u5e7f\u6cdb\u6a21\u5f0f\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "AGIS\u91c7\u7528\u7ed3\u6784\u901a\u77e5\u7684\u90bb\u5c45\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u63a8\u5bfc\u7406\u60f3\u91c7\u6837\u5206\u5e03\u5e76\u63d0\u51fa\u5b9e\u7528\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e73\u8861\u6536\u655b\u901f\u5ea6\u4e0e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAGIS\u5728\u51e0\u4f55\u5e73\u5747\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u7279\u5b9a\u60c5\u51b5\u4e0b\u901f\u5ea6\u63d0\u5347\u8d85\u8fc7100,000\u500d\uff0c\u5e76\u80fd\u5904\u7406\u6570\u767e\u4ebf\u8fb9\u56fe\u3002", "conclusion": "AGIS\u662f\u4e00\u79cd\u6781\u5feb\u7684\u8fd1\u4f3c\u56fe\u6a21\u5f0f\u6316\u6398\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\uff0c\u5e76\u5728\u51e0\u79d2\u5185\u63d0\u4f9b\u51c6\u786e\u4f30\u8ba1\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u901a\u77e5\u7684\u90bb\u5c45\u91c7\u6837\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u662f\u552f\u4e00\u80fd\u6269\u5c55\u5230\u6570\u767e\u4ebf\u8fb9\u56fe\u7684AGPM\u7cfb\u7edf\u3002"}}
{"id": "2601.01086", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01086", "abs": "https://arxiv.org/abs/2601.01086", "authors": ["Jianpeng Qi", "Chao Liu", "Chengrui Wang", "Rui Wang", "Junyu Dong", "Yanwei Yu"], "title": "Decision-Aware Semantic State Synchronization in Compute-First Networking", "comment": "12 pages, 9 figures", "summary": "In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.", "AI": {"tldr": "SenseCFN\u901a\u8fc7\u51b3\u7b56\u611f\u77e5\u7684\u72b6\u6001\u540c\u6b65\uff0c\u5728CFN\u4e2d\u4f18\u5316\u4efb\u52a1\u5378\u8f7d\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u5e76\u51cf\u5c11\u66f4\u65b0\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5468\u671f\u6027\u66f4\u65b0\u6216\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u65b0\u9c9c\u5ea6\uff0c\u5e38\u5ffd\u7565\u72b6\u6001\u53d8\u5316\u5bf9\u5378\u8f7d\u51b3\u7b56\u7684\u5b9e\u9645\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5728\u9ad8\u7cfb\u7edf\u8d1f\u8f7d\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86SenseCFN\u6846\u67b6\uff0c\u5305\u62ec\u8f7b\u91cf\u7ea7\u8bed\u4e49\u72b6\u6001\u8868\u793a\u548c\u8bed\u4e49\u504f\u5dee\u6307\u6570\uff08SDI\uff09\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u5e03\u5f0f\u6267\u884c\uff08CTDE\uff09\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u66f4\u65b0\u548c\u5378\u8f7d\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cSenseCFN\u5728\u6613\u9971\u548c\u573a\u666f\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u9ad8\u8fbe99.6%\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc725%\uff0c\u540c\u65f6\u72b6\u6001\u66f4\u65b0\u9891\u7387\u964d\u4f4e\u7ea670%\u81f396%\u3002", "conclusion": "SenseCFN\u901a\u8fc7\u51b3\u7b56\u611f\u77e5\u7684\u72b6\u6001\u540c\u6b65\u6846\u67b6\uff0c\u5728\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\uff08CFN\uff09\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u66f4\u65b0\u5f00\u9500\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u72b6\u6001\u66f4\u65b0\u9891\u7387\u3002"}}
{"id": "2601.00829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00829", "abs": "https://arxiv.org/abs/2601.00829", "authors": ["Alexander Vinogradov"], "title": "Can Generative Models Actually Forge Realistic Identity Documents?", "comment": "11 pages, 16 figures", "summary": "Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u96be\u4ee5\u4f2a\u9020\u5177\u6709\u6cd5\u533b\u771f\u5b9e\u6027\u7684\u8eab\u4efd\u6587\u6863\uff0c\u98ce\u9669\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002", "motivation": "\u63a2\u8ba8\u5f53\u4ee3\u5f00\u6e90\u548c\u516c\u5f00\u53ef\u7528\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u751f\u6210\u8db3\u4ee5\u6b3a\u9a97\u4eba\u7c7b\u6216\u81ea\u52a8\u5316\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8eab\u4efd\u6587\u6863\u4f2a\u9020\u54c1\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u516c\u5f00\u53ef\u7528\u7684\u751f\u6210\u6a21\u578b\u5bb6\u65cf\uff08\u5982Stable Diffusion\u3001Qwen\u3001Flux\u3001Nano-Banana\u7b49\uff09\u7684\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u80fd\u6a21\u62df\u6587\u6863\u7684\u8868\u9762\u7f8e\u5b66\uff0c\u4f46\u65e0\u6cd5\u590d\u5236\u7ed3\u6784\u548c\u6cd5\u533b\u771f\u5b9e\u6027\u3002", "conclusion": "\u5f53\u524d\u7684\u5f00\u6e90\u548c\u516c\u5f00\u53ef\u7528\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u591f\u6a21\u62df\u8eab\u4efd\u6587\u6863\u7684\u8868\u9762\u7f8e\u5b66\uff0c\u4f46\u65e0\u6cd5\u590d\u5236\u7ed3\u6784\u548c\u6cd5\u533b\u771f\u5b9e\u6027\uff0c\u56e0\u6b64\u751f\u6210\u8eab\u4efd\u6587\u6863\u6df1\u5ea6\u4f2a\u9020\u8fbe\u5230\u6cd5\u533b\u7ea7\u771f\u5b9e\u6027\u7684\u98ce\u9669\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002"}}
{"id": "2601.01129", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01129", "abs": "https://arxiv.org/abs/2601.01129", "authors": ["Kla Tantithamthavorn", "Yaotian Zou", "Andy Wong", "Michael Gupta", "Zhe Wang", "Mike Buller", "Ryan Jiang", "Matthew Watson", "Minwoo Jeong", "Kun Chen", "Ming Wu"], "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian", "comment": "Accepted at the 48th International Conference on Software Engineering (ICSE'26), SEIP Track. 12 Pages", "summary": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?\n  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).", "AI": {"tldr": "RovoDev Code Reviewer \u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7 LLM \u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u8bba\uff0c\u663e\u8457\u63d0\u5347\u5ba1\u67e5\u6548\u7387\u548c\u8f6f\u4ef6\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u8bbe\u8ba1\u65e0\u9700\u5fae\u8c03\u3001\u5177\u5907\u5ba1\u67e5\u5f15\u5bfc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8d28\u91cf\u68c0\u67e5\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u751f\u6210\u5de5\u5177\u7684\u5b9e\u9645\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86 RovoDev Code Reviewer\uff0c\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u7684\u4f01\u4e1a\u7ea7 LLM \u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5e76\u96c6\u6210\u5230 Atlassian \u7684 Bitbucket \u4e2d\u3002", "result": "RovoDev Code Reviewer \u5728\u4e00\u5e74\u8bc4\u4f30\u671f\u5185\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\u548c\u8d28\u91cf\u3002", "conclusion": "RovoDev Code Reviewer \u5728 Atlassian \u7684\u5f00\u53d1\u751f\u6001\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u901a\u8fc7\u79bb\u7ebf\u3001\u5728\u7ebf\u548c\u7528\u6237\u53cd\u9988\u8bc4\u4f30\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u751f\u6210\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0838.70% \u89e6\u53d1\u540e\u7eed\u4ee3\u7801\u53d8\u66f4\uff09\uff0c\u52a0\u901f\u53cd\u9988\u5468\u671f\uff08\u51cf\u5c11 PR \u5468\u671f\u65f6\u95f4 30.8%\uff09\uff0c\u51cf\u8f7b\u5ba1\u67e5\u8005\u5de5\u4f5c\u91cf\uff08\u51cf\u5c11\u4eba\u5de5\u7f16\u5199\u8bc4\u8bba 35.6%\uff09\uff0c\u5e76\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\uff08\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5efa\u8bae\u7684\u9519\u8bef\u53d1\u73b0\uff09\u3002"}}
{"id": "2601.00978", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00978", "abs": "https://arxiv.org/abs/2601.00978", "authors": ["Yanyi Chen", "Min Deng"], "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly", "comment": null, "summary": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbe\u8ba1\u7684\u4eba\u673a\u534f\u4f5c\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u9a71\u52a8\u89c4\u5212\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u7ed3\u6784\u5316\u88c5\u914d\u4e2d\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u5728\u566a\u58f0\u611f\u77e5\u548c\u4eba\u7c7b\u5e72\u9884\u4e0b\u7684\u53ef\u9760\u72b6\u6001\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u8026\u5408\u6a21\u5757\uff1a\u6a21\u5757I\uff08PSS\uff09\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5c06RGB-D\u89c2\u6d4b\u4e0e\u8bbe\u8ba1\u89c4\u8303\u548c\u9886\u57df\u77e5\u8bc6\u5bf9\u9f50\uff0c\u5408\u6210\u53ef\u9a8c\u8bc1\u7684\u7b26\u53f7\u5316\u88c5\u914d\u72b6\u6001\uff1b\u6a21\u5757II\uff08HPR\uff09\u6267\u884c\u4efb\u52a1\u7ea7\u591a\u673a\u5668\u4eba\u5206\u914d\uff0c\u5e76\u5728\u89c2\u6d4b\u72b6\u6001\u504f\u79bb\u9884\u671f\u6267\u884c\u7ed3\u679c\u65f6\u66f4\u65b0\u8ba1\u5212\u3002", "result": "\u572827\u7ec4\u4ef6\u6728\u6846\u67b6\u88c5\u914d\u4e0a\u9a8c\u8bc1\uff0cPSS\u6a21\u5757\u8fbe\u523097%\u7684\u72b6\u6001\u5408\u6210\u51c6\u786e\u7387\uff0cHPR\u6a21\u5757\u5728\u591a\u6837\u5316HRC\u573a\u666f\u4e2d\u4fdd\u6301\u53ef\u884c\u4efb\u52a1\u8fdb\u5c55\u3002", "conclusion": "\u6574\u5408\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u4e0e\u77e5\u8bc6\u9a71\u52a8\u89c4\u5212\uff0c\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.00816", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00816", "abs": "https://arxiv.org/abs/2601.00816", "authors": ["Ismail Ahmad Abdullah"], "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback", "comment": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims", "summary": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.\n  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.\n  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance", "AI": {"tldr": "MathLedger\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684AI\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u5bc6\u7801\u5b66\u8ba4\u8bc1\uff0c\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5bfc\u81f4\u5728\u5b89\u5168\u5173\u952e\u90e8\u7f72\u4e2d\u4fe1\u4efb\u5371\u673a\u3002MathLedger\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53cd\u5c04\u5f62\u5f0f\u5b66\u4e60\uff08RFL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b26\u53f7\u5316\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u5176\u66f4\u65b0\u7531\u9a8c\u8bc1\u7ed3\u679c\u800c\u975e\u7edf\u8ba1\u635f\u5931\u9a71\u52a8\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u548c\u6cbb\u7406\u57fa\u7840\u8bbe\u65bd\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5305\u62ecDelta p\u8ba1\u7b97\u3001\u65b9\u5dee\u8ddf\u8e2a\u548c\u6545\u969c\u5173\u95ed\u6cbb\u7406\u89e6\u53d1\u673a\u5236\u3002", "conclusion": "MathLedger\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3001\u5bc6\u7801\u5b66\u8ba4\u8bc1\u548c\u5b66\u4e60\u52a8\u6001\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7684\u53ef\u5ba1\u8ba1\u6027\u3002"}}
{"id": "2601.01596", "categories": ["cs.DC", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.01596", "abs": "https://arxiv.org/abs/2601.01596", "authors": ["Congrong Ren", "Robert Underwood", "Sheng Di", "Emrecan Kutay", "Zarija Lukic", "Aylin Yener", "Franck Cappello", "Hanqi Guo"], "title": "FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data", "comment": null, "summary": "This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\u5728\u635f\u5931\u538b\u7f29\u4e2d\u4fdd\u7559\u9891\u8c31\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4e2a\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4ec5\u4fdd\u8bc1\u7a7a\u95f4\u57df\u7684\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u9891\u57df\uff0c\u8fd9\u5bf9\u4e8e\u4f9d\u8d56\u9891\u57df\u8868\u793a\u7684\u79d1\u5b66\u5e94\u7528\uff08\u5982\u5b87\u5b99\u5b66\u3001\u71c3\u70e7\u6a21\u62df\u7b49\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u9891\u57df\u8bef\u5dee\u8868\u793a\u4e3a\u7a7a\u95f4\u57df\u8bef\u5dee\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u63a8\u5bfc\u51fa\u8054\u5408\u7ea6\u675f\u4e24\u4e2a\u57df\u8bef\u5dee\u7684\u533a\u57df\uff0c\u5e76\u8fed\u4ee3\u6295\u5f71\u7a7a\u95f4\u8bef\u5dee\u5411\u91cf\u76f4\u81f3\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u5728\u5b87\u5b99\u5b66\u6a21\u62df\u3001X\u5c04\u7ebf\u884d\u5c04\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u7a7a\u95f4\u548c\u9891\u57df\u7684\u5173\u952e\u79d1\u5b66\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\u7684\u65b0\u6280\u672f\uff0c\u7528\u4e8e\u5728\u635f\u5931\u538b\u7f29\u4e2d\u4fdd\u7559\u9891\u8c31\u7279\u5f81\uff0c\u901a\u8fc7GPU\u5e76\u884c\u52a0\u901f\u5b9e\u73b0\u5b9e\u7528\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.02072", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02072", "abs": "https://arxiv.org/abs/2601.02072", "authors": ["Haato Watanabe", "Nobuyuki Umetani"], "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes", "comment": "Presented at SIGGRAPH Asia 2025 (Technical Communications). Best Technical Communications Award", "summary": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9ad8\u65af\u6cfc\u6e85\u4e2d\u63d0\u53d6\u7ec6\u957f\u7269\u4f53\u591a\u6bb5\u7ebf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u8349\u56fe\u548c\u52a8\u6001\u7f16\u7a0b\u5b9e\u73b0\u7a33\u5065\u6784\u5efa\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u7f3a\u4e4f\u8fde\u63a5\u4fe1\u606f\u4e14\u9ad8\u65af\u57fa\u5143\u914d\u7f6e\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u96be\u4ee5\u76f4\u63a5\u6784\u5efa\u591a\u6bb5\u7ebf\u8868\u793a\u3002", "method": "\u91c7\u7528\u52a8\u6001\u7f16\u7a0b\u9ad8\u6548\u89e3\u51b3\u7684\u5c4f\u5e55\u7a7a\u95f4\u6700\u77ed\u8def\u5f84\u5206\u6790\uff0c\u4ece\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u63d0\u53d6\u7ec6\u957f\u7269\u4f53\u7684\u591a\u6bb5\u7ebf\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u63d0\u53d6\u7ec6\u957f\u7269\u4f53\u591a\u6bb5\u7ebf\u8868\u793a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u8349\u56fe\u8f93\u5165\u548c\u5c4f\u5e55\u7a7a\u95f4\u6700\u77ed\u8def\u5f84\u5206\u6790\uff0c\u80fd\u591f\u7a33\u5065\u5730\u6784\u5efa\u4ee3\u8868\u7ec6\u957f\u90e8\u5206\u7684\u591a\u6bb5\u7ebf\u7f51\u683c\u3002"}}
{"id": "2601.01390", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.01390", "abs": "https://arxiv.org/abs/2601.01390", "authors": ["Timothy M. Chan"], "title": "Derandomizing Pseudopolynomial Algorithms for Subset Sum", "comment": "To appear in SODA 2026", "summary": "We reexamine the classical subset sum problem: given a set $X$ of $n$ positive integers and a number $t$, decide whether there exists a subset of $X$ that sums to $t$; or more generally, compute the set $\\mbox{out}$ of all numbers $y\\in\\{0,\\ldots,t\\}$ for which there exists a subset of $X$ that sums to $y$. Standard dynamic programming solves the problem in $O(tn)$ time. In SODA'17, two papers appeared giving the current best deterministic and randomized algorithms, ignoring polylogarithmic factors: Koiliaris and Xu's deterministic algorithm runs in $\\widetilde{O}(t\\sqrt{n})$ time, while Bringmann's randomized algorithm runs in $\\widetilde{O}(t)$ time. We present the first deterministic algorithm running in $\\widetilde{O}(t)$ time.\n  Our technique has a number of other applications: for example, we can also derandomize the more recent output-sensitive algorithms by Bringmann and Nakos [STOC'20] and Bringmann, Fischer, and Nakos [SODA'25] running in $\\widetilde{O}(|\\mbox{out}|^{4/3})$ and $\\widetilde{O}(|\\mbox{out}|\\sqrt{n})$ time, and we can derandomize a previous fine-grained reduction from 0-1 knapsack to min-plus convolution by Cygan et al. [ICALP'17].", "AI": {"conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a$\\widetilde{O}(t)$\uff0c\u89e3\u51b3\u4e86\u7ecf\u5178\u5b50\u96c6\u548c\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6280\u672f\u7684\u5176\u4ed6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u52a8\u6001\u7f16\u7a0b\u6280\u672f\uff0c\u7ed3\u5408\u6700\u65b0\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86$\\widetilde{O}(t)$\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178\u5b50\u96c6\u548c\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u73b0\u6709\u786e\u5b9a\u6027\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u8fbe\u5230\u4e0e\u968f\u673a\u7b97\u6cd5\u76f8\u5f53\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u8fd0\u884c\u65f6\u95f4\u4e3a$\\widetilde{O}(t)$\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6280\u672f\u5728\u591a\u4e2a\u5176\u4ed6\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a$\\widetilde{O}(t)$\u65f6\u95f4\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\u89e3\u51b3\u5b50\u96c6\u548c\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2601.01630", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01630", "abs": "https://arxiv.org/abs/2601.01630", "authors": ["Nicholas Jones", "Eytan Modiano"], "title": "Utility Maximization in Wireless Backhaul Networks with Service Guarantees", "comment": null, "summary": "We consider the problem of maximizing utility in wireless backhaul networks, where utility is a function of satisfied service level agreements (SLAs), defined in terms of end-to-end packet delays and instantaneous throughput. We model backhaul networks as a tree topology and show that SLAs can be satisfied by constructing link schedules with bounded inter-scheduling times, an NP-complete problem known as pinwheel scheduling. For symmetric tree topologies, we show that simple round-robin schedules can be optimal under certain conditions. In the general case, we develop a mixed-integer program that optimizes over the set of admission decisions and pinwheel schedules. We develop a novel pinwheel scheduling algorithm, which significantly expands the set of schedules that can be found in polynomial time over the state of the art. Using conditions from this algorithm, we develop a scalable, distributed approach to solve the utility-maximization problem, with complexity that is linear in the depth of the tree.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65e0\u7ebf\u56de\u7a0b\u7f51\u7edc\u6548\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7pinwheel\u8c03\u5ea6\u548c\u6df7\u5408\u6574\u6570\u89c4\u5212\u6ee1\u8db3SLA\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65e0\u7ebf\u56de\u7a0b\u7f51\u7edc\u4e2d\u6ee1\u8db3SLA\uff08\u7aef\u5230\u7aef\u6570\u636e\u5305\u5ef6\u8fdf\u548c\u77ac\u65f6\u541e\u5410\u91cf\uff09\u7684\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2aNP\u5b8c\u5168\u95ee\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u4f18\u5316\u51c6\u5165\u51b3\u7b56\u548cpinwheel\u8c03\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684pinwheel\u8c03\u5ea6\u7b97\u6cd5\uff0c\u663e\u8457\u6269\u5c55\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u627e\u5230\u7684\u8c03\u5ea6\u96c6\u5408\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u5176\u590d\u6742\u5ea6\u4e0e\u6811\u7684\u6df1\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65e0\u7ebf\u56de\u7a0b\u7f51\u7edc\u4e2d\u6700\u5927\u5316\u6548\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5177\u6709\u6709\u754c\u8c03\u5ea6\u95f4\u9694\u7684\u94fe\u8def\u8c03\u5ea6\u6765\u6ee1\u8db3SLA\u8981\u6c42\uff0c\u5e76\u5728\u5bf9\u79f0\u6811\u62d3\u6251\u4e0b\u8bc1\u660e\u4e86\u7b80\u5355\u8f6e\u8be2\u8c03\u5ea6\u7684\u6700\u4f18\u6027\u3002"}}
{"id": "2601.00837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00837", "abs": "https://arxiv.org/abs/2601.00837", "authors": ["Agniv Roy Choudhury"], "title": "Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs", "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.\n  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.\n  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.\n  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\\% accuracy, 99.61\\% F1-score, and 99.93\\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.\n  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.\n  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u81ea\u5b9a\u4e49CNN\u4e0e\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08\u5982ResNet50\uff09\u5728\u513f\u7ae5\u80ba\u708e\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684ResNet50\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u5177\u6709\u5728\u8d44\u6e90\u6709\u9650\u5730\u533a\u4f5c\u4e3a\u7b5b\u67e5\u5de5\u5177\u7684\u6f5c\u529b\u3002", "motivation": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.", "method": "A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.", "result": "Fine-tuned ResNet50 achieved the best performance: 99.43% accuracy, 99.61% F1-score, and 99.93% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.", "conclusion": "Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets."}}
{"id": "2601.01199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01199", "abs": "https://arxiv.org/abs/2601.01199", "authors": ["Logan Murphy", "Aren A. Babikian", "Marsha Chechik"], "title": "Abductive Vibe Coding (Extended Abstract)", "comment": null, "summary": "When software artifacts are generated by AI models (\"vibe coding\"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3aAI\u751f\u6210\u7684\u4ee3\u7801\u63d0\u4f9b\u534a\u6b63\u5f0f\u7684\u7406\u7531\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66ff\u4ee3\u96be\u4ee5\u5b9e\u73b0\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u7531\u4e8e\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684AI\u751f\u6210\u4ee3\u7801\u573a\u666f\u96be\u4ee5\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u9002\u5f53\u6027\u3002", "method": "\u7814\u7a76\u63cf\u8ff0\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u751f\u6210\u4e00\u7ec4\u6761\u4ef6\uff0c\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\uff0c\u751f\u6210\u7684\u4ee3\u7801\u53ef\u4ee5\u88ab\u89c6\u4e3a\u9002\u5f53\u3002", "result": "\u7814\u7a76\u6b63\u5728\u8fdb\u884c\u4e2d\uff0c\u63cf\u8ff0\u4e86\u5f53\u524d\u6846\u67b6\u7684\u5b9e\u65bd\u8fdb\u5c55\u548c\u9884\u671f\u7684\u7814\u7a76\u673a\u4f1a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u53ef\u5206\u6790\u7684\u534a\u6b63\u5f0f\u7406\u7531\uff0c\u4ee5\u8bc4\u4f30AI\u751f\u6210\u4ee3\u7801\u7684\u9002\u5f53\u6027\uff0c\u800c\u975e\u76f4\u63a5\u5224\u5b9a\u5176\u6b63\u786e\u6027\u3002"}}
{"id": "2601.00981", "categories": ["cs.RO", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00981", "abs": "https://arxiv.org/abs/2601.00981", "authors": ["Wenhui Chu", "Khang Tran", "Nikolaos V. Tsekos"], "title": "Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions", "comment": "9 pages, 8 figures, published in ICBBB 2022", "summary": "Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aMRI\u5f15\u5bfc\u8840\u7ba1\u5185\u624b\u672f\u7684\u672f\u524d\u89c4\u5212\u5e73\u53f0\uff0c\u901a\u8fc7\u865a\u62df\u5939\u5177\u548c\u78c1\u573a\u68af\u5ea6\u6ce2\u5f62\u786e\u4fdd\u624b\u672f\u5b89\u5168\uff0c\u652f\u6301\u5b9e\u65f6\u64cd\u4f5c\u3002", "motivation": "\u4e3aMRI\u5f15\u5bfc\u7684\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u63d0\u4f9b\u672f\u524d\u89c4\u5212\u548c\u5efa\u6a21\u5de5\u5177\uff0c\u786e\u4fdd\u624b\u672f\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002", "method": "\u5e73\u53f0\u91c7\u7528\u53cc\u5411\u6570\u636e\u548c\u547d\u4ee4\u7ba1\u9053\u8fde\u63a5MRI\u626b\u63cf\u4eea\u3001\u8ba1\u7b97\u6838\u5fc3\u548c\u64cd\u4f5c\u8005\uff0c\u5305\u62ec\u8840\u7ba1\u5e8a\u63d0\u53d6\u3001\u865a\u62df\u8d70\u5eca\u62df\u5408\u3001\u78c1\u573a\u68af\u5ea6\u6ce2\u5f62\u751f\u6210\u7b49\u6a21\u5757\uff0c\u652f\u6301\u591a\u7ebf\u7a0b\u5b9e\u65f6\u64cd\u4f5c\u3002", "result": "\u5e73\u53f0\u6210\u529f\u5b9e\u73b0\u4e86\u8840\u7ba1\u8def\u5f84\u7684\u865a\u62df\u5efa\u6a21\u548c\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u80fd\u591f\u751f\u6210\u78c1\u573a\u68af\u5ea6\u6ce2\u5f62\u5e76\u6a21\u62df\u4e0d\u540c\u8840\u6d41\u6761\u4ef6\u4e0b\u7684\u5668\u68b0\u64cd\u63a7\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eMRI\u5f15\u5bfc\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u7684\u672f\u524d\u89c4\u5212\u548c\u5efa\u6a21\u8ba1\u7b97\u5e73\u53f0\uff0c\u901a\u8fc7\u865a\u62df\u5939\u5177\u786e\u4fdd\u624b\u672f\u5b89\u5168\u6027\uff0c\u5e76\u4e3a\u5b9e\u65f6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2601.00818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00818", "abs": "https://arxiv.org/abs/2601.00818", "authors": ["Chandra Sekhar Kubam"], "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making", "comment": "8 pages", "summary": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3bAI\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u5347\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u7684\u5feb\u901f\u6570\u5b57\u5316\u8feb\u5207\u9700\u8981\u81ea\u4e3b\u3001\u900f\u660e\u3001\u5b9e\u65f6\u7684\u4fe1\u7528\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u73b0\u4ee3\u91d1\u878d\u8fd0\u8425\u6240\u9700\u7684\u9002\u5e94\u6027\u63a8\u7406\u3001\u60c5\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u53ef\u89e3\u91caAI\u6a21\u5757\u548c\u5b9e\u65f6\u6570\u636e\u5438\u6536\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30\u501f\u6b3e\u4eba\u7684\u98ce\u9669\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u4f18\u4e8e\u4f20\u7edf\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\uff0c\u4f46\u4ecd\u5b58\u5728\u6a21\u578b\u6f02\u79fb\u3001\u9ad8\u7ef4\u6570\u636e\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u53ca\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u7b49\u5b9e\u9645\u9650\u5236\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u6539\u53d8\u4fe1\u7528\u5206\u6790\u7684\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u52a8\u6001\u5408\u89c4\u6027\u3001\u65b0\u578b\u4ee3\u7406\u534f\u4f5c\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u53ca\u8de8\u56fd\u4fe1\u7528\u751f\u6001\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u3002"}}
{"id": "2601.01712", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01712", "abs": "https://arxiv.org/abs/2601.01712", "authors": ["Jiarui Wang", "Huichao Chai", "Yuanhang Zhang", "Zongjin Zhou", "Wei Guo", "Xingkun Yang", "Qiang Tang", "Bo Pan", "Jiawei Zhu", "Ke Cheng", "Yuting Yan", "Shulan Wang", "Yingjie Zhu", "Zhengfan Yuan", "Jiaqi Huang", "Yuhan Zhang", "Xiaosong Sun", "Zhinan Zhang", "Hong Zhu", "Yongsheng Zhang", "Tiantian Dong", "Zhong Xiao", "Deliang Liu", "Chengzhou Lu", "Yuan Sun", "Zhiyuan Chen", "Xinming Han", "Zaizhu Liu", "Yaoyuan Wang", "Ziyang Zhang", "Yong Liu", "Jinxin Xu", "Yajing Sun", "Zhoujun Yu", "Wenting Zhou", "Qidong Zhang", "Zhengyong Zhang", "Zhonghai Gu", "Yibo Jin", "Yongxiang Feng", "Pengfei Zuo"], "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference", "comment": null, "summary": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.", "AI": {"tldr": "RelayGR\u901a\u8fc7\u9884\u63a8\u65ad\u548c\u7f13\u5b58\u4f18\u5316\uff0c\u5728\u5de5\u4e1a\u7ea7\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u957f\u5e8f\u5217\u5904\u7406\u548c\u66f4\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u56e0\u5904\u7406\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u800c\u63d0\u5347\u63a8\u8350\u8d28\u91cf\uff0c\u4f46\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u53d7\u9650\u4e8e\u4e25\u683c\u7684\u5ef6\u8fdf\u9884\u7b97\uff0c\u5e8f\u5217\u957f\u5ea6\u88ab\u5927\u5e45\u538b\u7f29\u3002", "method": "RelayGR\u91c7\u7528\u5e8f\u5217\u611f\u77e5\u89e6\u53d1\u5668\u3001\u4eb2\u548c\u611f\u77e5\u8def\u7531\u5668\u548c\u5185\u5b58\u611f\u77e5\u6269\u5c55\u5668\u4e09\u9879\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7528\u6237\u884c\u4e3a\u524d\u7f00\u9884\u63a8\u65ad\u548c\u7f13\u5b58\u7ba1\u7406\u3002", "result": "\u5728\u56fa\u5b9aP99 SLO\u4e0b\uff0cRelayGR\u652f\u63011.5\u500d\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u5e76\u5c06\u7b26\u5408SLO\u7684\u541e\u5410\u91cf\u63d0\u5347\u81f33.6\u500d\u3002", "conclusion": "RelayGR\u7cfb\u7edf\u901a\u8fc7\u9009\u62e9\u6027\u9884\u63a8\u65ad\u3001KV\u7f13\u5b58\u9a7b\u7559\u548c\u672c\u5730\u5316\u8def\u7531\u7b49\u6280\u672f\uff0c\u5728\u4e25\u683c\u5ef6\u8fdfSLO\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5e8f\u5217\u957f\u5ea6\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2601.02096", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02096", "abs": "https://arxiv.org/abs/2601.02096", "authors": ["Peizhuo Li", "Sebastian Starke", "Yuting Ye", "Olga Sorkine-Hornung"], "title": "Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs", "comment": null, "summary": "Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.", "AI": {"tldr": "\u4f7f\u7528VR\u4e09\u70b9\u8f68\u8ff9\u7b80\u5316\u821e\u8005\u8fd0\u52a8\u5efa\u6a21\uff0c\u901a\u8fc7MLP\u9884\u6d4b\u8ddf\u968f\u8005\u8f68\u8ff9\uff0c\u5e76\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u821e\u8005\u95f4\u9ad8\u7ef4\u5ea6\u5168\u8eab\u8fd0\u52a8\u4ea4\u4e92\u5efa\u6a21\u7684\u6311\u6218\uff0c\u7b80\u5316\u8fd0\u52a8\u63cf\u8ff0\u4e0e\u5408\u6210\u3002", "method": "\u5229\u7528VR\u8bbe\u5907\u7684\u4e09\u70b9\u8f68\u8ff9\u4f5c\u4e3a\u8fd0\u52a8\u63cf\u8ff0\u7b26\uff0c\u91c7\u7528\u9ad8\u6548\u7684MLP\u7f51\u7edc\u9884\u6d4b\u8ddf\u968f\u8005\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u5c06\u8f68\u8ff9\u8f6c\u6362\u4e3a\u865a\u62df\u5316\u8eab\u3002", "result": "\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u821e\u8e48\u6570\u636e\u96c6\uff08\u5982ballroom dancing\uff09\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u5982LaFAN\uff09\u4e0a\u5747\u8868\u73b0\u7a33\u5065\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u548c\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528VR\u8bbe\u5907\u7684\u4e09\u70b9\u8f68\u8ff9\u4f5c\u4e3a\u821e\u8005\u8fd0\u52a8\u63cf\u8ff0\u7b26\u7684\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u821e\u8005\u95f4\u5168\u8eab\u8fd0\u52a8\u7684\u5efa\u6a21\u4e0e\u5408\u6210\u3002\u901a\u8fc7\u4f4e\u7ef4\u5ea6\u548c\u9ad8\u6548MLP\u7f51\u7edc\uff0c\u6210\u529f\u9884\u6d4b\u8ddf\u968f\u8005\u7684\u4e09\u70b9\u8f68\u8ff9\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u66f4\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01710", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.01710", "abs": "https://arxiv.org/abs/2601.01710", "authors": ["Kevin Pfisterer", "Quentin Hillebrand", "Vorapong Suppakitpaisarn"], "title": "Publishing Below-Threshold Triangle Counts under Local Weight Differential Privacy", "comment": null, "summary": "We propose an algorithm for counting below-threshold triangles in weighted graphs under local weight differential privacy. While prior work focused on unweighted graphs, many real-world networks naturally include edge weights. We study the setting where the graph topology is public known and the privacy of the influence of an individual on the edge weights is protected. This captures realistic scenarios such as road networks and telecommunication networks. Our approach consists of two rounds of communication. In the first round, each node publishes their incident weight information under local weight differential privacy while in the second round, the nodes locally count below-threshold triangles, for which we introduce a biased and unbiased variant. We further propose two different improvements. We present a pre-computation step that reduces the covariance and thereby lowers the expected error. Secondly, we develop an algorithm for computing the smooth-sensitivity, which significantly reduces the running time compared to a straightforward approach. Finally, we provide experimental results that demonstrate the differences between the biased and unbiased variants and the effectiveness of the proposed improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u52a0\u6743\u56fe\u4e2d\u4fdd\u62a4\u9690\u79c1\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e24\u8f6e\u901a\u4fe1\u8ba1\u7b97\u4f4e\u4e8e\u9608\u503c\u4e09\u89d2\u5f62\uff0c\u5e76\u5c55\u793a\u4e86\u6709\u504f\u548c\u65e0\u504f\u53d8\u4f53\u7684\u5dee\u5f02\u53ca\u6539\u8fdb\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7f51\u7edc\uff08\u5982\u9053\u8def\u7f51\u7edc\u548c\u7535\u4fe1\u7f51\u7edc\uff09\u901a\u5e38\u5305\u542b\u8fb9\u6743\u91cd\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u65e0\u6743\u56fe\u4e0a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7b97\u6cd5\u5728\u4fdd\u62a4\u4e2a\u4f53\u5bf9\u8fb9\u6743\u91cd\u9690\u79c1\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u52a0\u6743\u56fe\u4e2d\u7684\u4f4e\u4e8e\u9608\u503c\u4e09\u89d2\u5f62\u3002", "method": "\u91c7\u7528\u4e24\u8f6e\u901a\u4fe1\u65b9\u6cd5\uff1a\u7b2c\u4e00\u8f6e\uff0c\u6bcf\u4e2a\u8282\u70b9\u5728\u672c\u5730\u6743\u91cd\u5dee\u5206\u9690\u79c1\u4e0b\u53d1\u5e03\u5176\u5173\u8054\u7684\u6743\u91cd\u4fe1\u606f\uff1b\u7b2c\u4e8c\u8f6e\uff0c\u8282\u70b9\u672c\u5730\u8ba1\u7b97\u4f4e\u4e8e\u9608\u503c\u7684\u4e09\u89d2\u5f62\uff0c\u5e76\u5f15\u5165\u4e86\u6709\u504f\u548c\u65e0\u504f\u4e24\u79cd\u53d8\u4f53\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u9884\u8ba1\u7b97\u6b65\u9aa4\u548c\u8ba1\u7b97\u5e73\u6ed1\u654f\u611f\u5ea6\u7684\u7b97\u6cd5\u4ee5\u51cf\u5c11\u8bef\u5dee\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6709\u504f\u548c\u65e0\u504f\u53d8\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u9884\u8ba1\u7b97\u6b65\u9aa4\u548c\u5e73\u6ed1\u654f\u611f\u5ea6\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u52a0\u6743\u56fe\u4e2d\u8ba1\u7b97\u4f4e\u4e8e\u9608\u503c\u4e09\u89d2\u5f62\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u672c\u5730\u6743\u91cd\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e2a\u4f53\u5bf9\u8fb9\u6743\u91cd\u7684\u5f71\u54cd\u9690\u79c1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u6709\u504f\u548c\u65e0\u504f\u53d8\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u6240\u63d0\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.01645", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.01645", "abs": "https://arxiv.org/abs/2601.01645", "authors": ["Vipindev Adat Vasudevan", "Homa Esfahanizadeh", "Benjamin D. Kim", "Laura Landon", "Alejandro Cohen", "Muriel M\u00e9dard"], "title": "Revisiting the Interface between Error and Erasure Correction in Wireless Standards", "comment": null, "summary": "Modern 5G communication systems implement a combination of error correction and feedback-based erasure correction (HARQ/ARQ) as reliability mechanisms, which can introduce substantial delay and resource inefficiency. We propose forward erasure correction using network coding as a more delay-efficient alternative. We present a mathematical characterization of network delay for existing reliability mechanisms and network coding. Through simulations in a network slicing environment, we demonstrate that network coding not only improves the in-order delivery delay and goodput for the applications utilizing the slice, but also benefits other applications sharing the network by reducing resource utilization for the coded slice. Our analysis and characterization point towards ideas that require attention in the 6G standardization process. These findings highlight the need for greater modularity in protocol stack design that enables the integration of novel technologies in future wireless networks.", "AI": {"tldr": "\u7f51\u7edc\u7f16\u7801\u57285G\u4e2d\u4f5c\u4e3a\u66ff\u4ee3HARQ/ARQ\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u5728\u5ef6\u8fdf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u4e3a6G\u6807\u51c6\u5316\u63d0\u4f9b\u6a21\u5757\u5316\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u73b0\u4ee35G\u901a\u4fe1\u7cfb\u7edf\u91c7\u7528\u7684\u7ea0\u9519\u548c\u57fa\u4e8e\u53cd\u9988\u7684\u64e6\u9664\u6821\u6b63\u673a\u5236\uff08\u5982HARQ/ARQ\uff09\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u5ef6\u8fdf\u548c\u8d44\u6e90\u4f4e\u6548\uff0c\u7f51\u7edc\u7f16\u7801\u88ab\u63d0\u51fa\u4f5c\u4e3a\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u7f51\u7edc\u5207\u7247\u73af\u5883\u4e0b\u7684\u4eff\u771f\uff0c\u6bd4\u8f83\u4e86\u73b0\u6709\u53ef\u9760\u6027\u673a\u5236\u4e0e\u7f51\u7edc\u7f16\u7801\u7684\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7f51\u7edc\u7f16\u7801\u4e0d\u4ec5\u63d0\u5347\u4e86\u5207\u7247\u5e94\u7528\u7684\u987a\u5e8f\u4ea4\u4ed8\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff0c\u8fd8\u901a\u8fc7\u51cf\u5c11\u7f16\u7801\u5207\u7247\u7684\u8d44\u6e90\u5229\u7528\uff0c\u4f7f\u5171\u4eab\u7f51\u7edc\u7684\u5176\u4ed6\u5e94\u7528\u53d7\u76ca\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u534f\u8bae\u6808\u8bbe\u8ba1\u9700\u8981\u66f4\u9ad8\u7684\u6a21\u5757\u5316\uff0c\u4ee5\u6574\u5408\u65b0\u6280\u672f\u5982\u7f51\u7edc\u7f16\u7801\uff0c\u8fd9\u5bf96G\u6807\u51c6\u5316\u8fc7\u7a0b\u63d0\u51fa\u4e86\u65b0\u7684\u601d\u8003\u65b9\u5411\u3002"}}
{"id": "2601.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00839", "abs": "https://arxiv.org/abs/2601.00839", "authors": ["Zahid Ullah", "Muhammad Hilal", "Eunsoo Lee", "Dragan Pamucar", "Jihie Kim"], "title": "Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS", "comment": null, "summary": "Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u4e86U-Net\u3001Attention U-Net\u548cTransUNet\u5728\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u5c55\u793a\u4e86\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5c06\u5fc3\u810f\u5f71\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8fdb\u5c55\u76f8\u7ed3\u5408\u7684\u7efc\u8ff0\u4e0e\u7edf\u4e00\u5b9e\u9a8c\u57fa\u51c6\u7684\u7814\u7a76\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u6587\u732e\u7684\u7efc\u8ff0\uff0c\u5bf9U-Net\u3001Attention U-Net\u548cTransUNet\u4e09\u79cd\u67b6\u6784\u5728CAMUS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5305\u62ec\u591a\u79cd\u9884\u5904\u7406\u65b9\u6cd5\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "result": "U-Net\u5728NIfTI\u6570\u636e\u4e0a\u8fbe\u523094%\u7684\u5e73\u5747Dice\u5206\u6570\uff0cAttention U-Net\u5728\u5c0f\u533a\u57df\u6216\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u8868\u73b0\u66f4\u4f18\uff0cTransUNet\u5728\u5168\u5c40\u7a7a\u95f4\u5efa\u6a21\u4e0a\u8868\u73b0\u6700\u5f3a\u3002\u4f2a\u6807\u7b7e\u751f\u6210\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7edf\u4e00\u4e14\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u57fa\u51c6\uff0c\u6bd4\u8f83\u4e86U-Net\u3001Attention U-Net\u548cTransUNet\u5728\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u6570\u636e\u9884\u5904\u7406\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2601.01215", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01215", "abs": "https://arxiv.org/abs/2601.01215", "authors": ["Prateek Rajput", "Yewei Song", "Abdoul Aziz Bonkoungou", "Iyiola E. Olatunji", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code", "comment": "11 Pages, 11 figures, Accepted at ICSE SEIP", "summary": "Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u7a0b\u5e8f\u5373\u4f7f\u901a\u8fc7\u6d4b\u8bd5\u4e5f\u53ef\u80fd\u5b58\u5728\u8fd0\u884c\u65f6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u51faDMPD\u548cMIS\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u5728CI/CD\u4e2d\u4f18\u5148\u9009\u62e9\u7a33\u5b9a\u6027\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u751f\u6210\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u7684\u7a0b\u5e8f\uff0c\u4f46\u901a\u8fc7\u6d4b\u8bd5\u5e76\u4e0d\u4fdd\u8bc1\u53ef\u9760\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u540c\u4e00\u4efb\u52a1\u7684\u4e0d\u540c\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u8868\u73b0\u51fa\u975e\u5e38\u4e0d\u540c\u7684\u5185\u5b58\u548c\u6027\u80fd\u6a21\u5f0f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u9690\u85cf\u7684\u64cd\u4f5c\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u6d4b\u91cf\u591a\u4e2a\u6b63\u786e\u751f\u6210\u7a0b\u5e8f\u5728\u6267\u884c\u65f6\u7684\u5185\u5b58\u7a33\u5b9a\u6027\u3002\u5728\u89e3\u51b3\u65b9\u6848\u5c42\u9762\uff0c\u5f15\u5165\u4e86\u52a8\u6001\u5e73\u5747\u6210\u5bf9\u8ddd\u79bb\uff08DMPD\uff09\uff0c\u5229\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u6bd4\u8f83\u5185\u5b58\u4f7f\u7528\u8f68\u8ff9\u7684\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u8f6c\u6362\u4e3a\u5355\u8c03\u5cf0\u503c\u8f6e\u5ed3\uff08MPPs\uff09\u51cf\u5c11\u77ac\u6001\u566a\u58f0\u3002\u5728\u4efb\u52a1\u5c42\u9762\u805a\u5408DMPD\u5f97\u5230\u6a21\u578b\u4e0d\u7a33\u5b9a\u6027\u5206\u6570\uff08MIS\uff09\u3002", "result": "\u5728BigOBench\u548cCodeContests\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u8fd0\u884c\u65f6\u5dee\u5f02\u3002\u4e0d\u7a33\u5b9a\u6027\u901a\u5e38\u968f\u7740\u91c7\u6837\u6e29\u5ea6\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5373\u4f7fpass@1\u6709\u6240\u6539\u5584\u3002\u8fd8\u89c2\u5bdf\u5230\u7a33\u5b9a\u6027\u5ea6\u91cf\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u6307\u6807\uff08\u5982\u8ba4\u77e5\u548c\u5708\u590d\u6742\u5ea6\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u8868\u660e\u64cd\u4f5c\u884c\u4e3a\u4e0e\u53ef\u7ef4\u62a4\u6027\u4e4b\u95f4\u5b58\u5728\u8054\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5728\u6301\u7eed\u96c6\u6210/\u6301\u7eed\u4ea4\u4ed8\uff08CI/CD\uff09\u6d41\u7a0b\u4e2d\u91c7\u7528\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u5728\u4e0d\u727a\u7272\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u64cd\u4f5c\u98ce\u9669\u3002"}}
{"id": "2601.01067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01067", "abs": "https://arxiv.org/abs/2601.01067", "authors": ["Wenzheng Zhang", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Topological Mapping and Navigation using a Monocular Camera based on AnyLoc", "comment": "Published in Proc. IEEE CASE 2025. 7 pages, 11 figures", "summary": "This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u6444\u50cf\u5934\u7684\u62d3\u6251\u5730\u56fe\u6784\u5efa\u4e0e\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u8282\u70b9\u7b80\u5316\u8def\u5f84\u89c4\u5212\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u9ad8\u6548\u4e14\u8f7b\u91cf\u3002", "motivation": "\u62d3\u6251\u5730\u56fe\u901a\u8fc7\u5173\u952e\u8282\u70b9\u800c\u975e\u7cbe\u786e\u5750\u6807\u8868\u793a\u73af\u5883\uff0c\u7b80\u5316\u4e86\u8def\u5f84\u89c4\u5212\u548c\u5bfc\u822a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u5730\u56fe\u6784\u5efa\u548c\u5bfc\u822a\u7684\u573a\u666f\u3002", "method": "\u57fa\u4e8eAnyLoc\uff0c\u5c06\u5173\u952e\u5e27\u8f6c\u6362\u4e3a\u63cf\u8ff0\u7b26\u4ee5\u6784\u5efa\u62d3\u6251\u5173\u7cfb\uff0c\u5b9e\u73b0\u73af\u8def\u68c0\u6d4b\u548c\u5730\u56fe\u6784\u5efa\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u5272\u56fe\u50cf\u4e0e\u76ee\u6807\u8282\u70b9\u5173\u8054\u7684\u56fe\u50cf\u6765\u786e\u5b9a\u89c6\u89c9\u5bfc\u822a\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e2d\u65e0\u9700\u9884\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u8fdb\u884c\u73af\u8def\u68c0\u6d4b\u548c\u5bfc\u822a\uff0c\u76f8\u6bd4\u57fa\u4e8eResNet\u7684\u65b9\u6cd5\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8660.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u76ee\u6444\u50cf\u5934\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u6210\u672c\u3002"}}
{"id": "2601.00821", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.00821", "abs": "https://arxiv.org/abs/2601.00821", "authors": ["Tao An"], "title": "CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations", "comment": "15 pages, 5 figures", "summary": "Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.\n  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.\n  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.", "AI": {"tldr": "CogCanvas\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u7ec4\u7ec7\u5bf9\u8bdd\u4e2d\u7684\u8ba4\u77e5\u6784\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u957f\u5bf9\u8bdd\u4e2d\u7684\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e0e\u4fe1\u606f\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5bf9\u8bdd\u4e2d\u7684\u8ba4\u77e5\u6784\u4ef6\uff08\u51b3\u7b56\u3001\u4e8b\u5b9e\u3001\u63d0\u9192\uff09\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u65f6\u95f4\u611f\u77e5\u56fe\uff0c\u5b9e\u73b0\u6297\u538b\u7f29\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogCanvas\u6574\u4f53\u51c6\u786e\u7387\u8fbe34.7%\uff0c\u663e\u8457\u4f18\u4e8eRAG\u548cGraphRAG\uff0c\u5c24\u5176\u5728\u65f6\u95f4\u63a8\u7406\u548c\u591a\u8df3\u56e0\u679c\u63a8\u7406\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CogCanvas\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01787", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01787", "abs": "https://arxiv.org/abs/2601.01787", "authors": ["Yuxiao Li", "Mingze Xia", "Xin Liang", "Bei Wang", "Robert Underwood", "Sheng Di", "Hemant Sharma", "Dishant Beniwal", "Franck Cappello", "Hanqi Guo"], "title": "pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression", "comment": null, "summary": "Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u4fee\u6b63\u6709\u635f\u538b\u7f29\u540e\u7684\u62d3\u6251\u7279\u5f81\uff0c\u901a\u8fc7\u7b80\u5316MSz\u65b9\u6cd5\u5e76\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u6269\u5c55\u3002", "motivation": "\u6709\u635f\u538b\u7f29\u5728\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u5e94\u7528\u53ef\u80fd\u626d\u66f2\u5173\u952e\u7279\u5f81\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u548c\u79d1\u5b66\u7ed3\u8bba\u3002\u73b0\u6709\u7684\u5355GPU\u7b97\u6cd5\uff08MSz\uff09\u65e0\u6cd5\u6269\u5c55\u5230\u6781\u7aef\u89c4\u6a21\u6570\u636e\u3002", "method": "\u7b97\u6cd5\u901a\u8fc7\u4fdd\u7559\u6240\u6709\u4f4d\u7f6e\u7684\u6700\u9661\u4e0a\u5347\u548c\u4e0b\u964d\u65b9\u5411\uff0c\u7b80\u5316\u4e86MSz\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u540c\u65f6\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u989d\u5916\u5b58\u50a8\u5f00\u9500\u3002", "result": "\u5728Perlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86128\u4e2aGPU\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\u901a\u8fc7\u7b80\u5316MSz\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u8ba1\u7b97\u548c\u4fee\u6b63\u79ef\u5206\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u62d3\u6251\u7279\u5f81\u4fee\u6b63\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387\u3002"}}
{"id": "2601.01050", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.01050", "abs": "https://arxiv.org/abs/2601.01050", "authors": ["Hongming Fu", "Wenjia Wang", "Xiaozhen Qiao", "Shuo Yang", "Zheng Liu", "Bo Zhao"], "title": "EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos", "comment": null, "summary": "We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.", "AI": {"tldr": "EgoGrasp\u662f\u9996\u4e2a\u4ece\u52a8\u6001\u76f8\u673a\u62cd\u6444\u7684\u81ea\u6211\u4e2d\u5fc3\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u4e16\u754c\u7a7a\u95f4\u624b-\u7269\u4f53\u4ea4\u4e92\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6846\u67b6\u548cHOI\u5148\u9a8c\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u7684\u4e16\u754c\u7a7a\u95f4\u624b-\u7269\u4f53\u4ea4\u4e92\u91cd\u5efa\u5bf9\u4e8e\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u548c\u5728\u5177\u8eab\u667a\u80fd\u4e0e\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u5e45\u56fe\u50cf\u6216\u76f8\u673a\u5750\u6807\uff0c\u65e0\u6cd5\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\u6216\u4e00\u81f4\u7684\u5168\u5c40\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u65b0\u5f00\u53d1\u7684\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u7684\u9c81\u68d2\u9884\u5904\u7406\u6d41\u7a0b\u3001\u57fa\u4e8e\u89e3\u8026\u6269\u6563\u6a21\u578b\u7684\u5168\u8eabHOI\u5148\u9a8c\u6a21\u578b\uff0c\u4ee5\u53ca\u591a\u76ee\u6807\u6d4b\u8bd5\u65f6\u4f18\u5316\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e16\u754c\u7a7a\u95f4\u624b-\u7269\u4f53\u4ea4\u4e92\u91cd\u5efa\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EgoGrasp\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u76f8\u673a\u62cd\u6444\u7684\u81ea\u6211\u4e2d\u5fc3\u5355\u76ee\u89c6\u9891\u4e2d\u8fdb\u884c\u4e16\u754c\u7a7a\u95f4\u624b-\u7269\u4f53\u4ea4\u4e92\uff08W-HOI\uff09\u91cd\u5efa\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2601.01841", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.01841", "abs": "https://arxiv.org/abs/2601.01841", "authors": ["Jingyang Zhao", "Yonghang Su", "Mingyu Xiao"], "title": "Improved Approximation Algorithms for the Multiple-Depot Split Delivery Vehicle Routing Problem", "comment": null, "summary": "The Multiple-Depot Split Delivery Vehicle Routing Problem (MD-SDVRP) is a challenging problem with broad applications in logistics. The goal is to serve customers' demand using a fleet of capacitated vehicles located in multiple depots, where each customer's demand can be served by more than one vehicle, while minimizing the total travel cost of all vehicles. We study approximation algorithms for this problem. Previously, the only known result was a $6$-approximation algorithm for a constant number of depots (INFORMS J. Comput. 2023), and whether this ratio could be improved was left as an open question. In this paper, we resolve it by proposing a $(6-2\\cdot 10^{-36})$-approximation algorithm for this setting. Moreover, we develop constant-factor approximation algorithms that work beyond a constant number of depots, improved parameterized approximation algorithms related to the vehicle capacity and the number of depots, as well as bi-factor approximation algorithms.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u4ed3\u5e93\u5206\u5272\u914d\u9001\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "motivation": "MD-SDVRP\u662f\u7269\u6d41\u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u4ec5\u63d0\u4f9b\u4e86\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u76846-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4e14\u662f\u5426\u80fd\u591f\u6539\u8fdb\u8fd9\u4e00\u6bd4\u7387\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u6269\u5c55\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9MD-SDVRP\u95ee\u9898\u8bbe\u8ba1\u4e86\u591a\u79cd\u7b97\u6cd5\uff0c\u5305\u62ec\u6539\u8fdb\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3001\u53c2\u6570\u5316\u8fd1\u4f3c\u7b97\u6cd5\u548c\u53cc\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u672c\u6587\u6210\u529f\u63d0\u51fa\u4e86$(6-2\\cdot 10^{-36})$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u975e\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u53c2\u6570\u5316\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u53cc\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u591a\u4ed3\u5e93\u5206\u5272\u914d\u9001\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08MD-SDVRP\uff09\u7684\u6539\u8fdb\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5305\u62ec\u9488\u5bf9\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u7684$(6-2\\cdot 10^{-36})$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ee5\u53ca\u9002\u7528\u4e8e\u975e\u56fa\u5b9a\u6570\u91cf\u4ed3\u5e93\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8fd8\u6539\u8fdb\u4e86\u4e0e\u8f66\u8f86\u5bb9\u91cf\u548c\u4ed3\u5e93\u6570\u91cf\u76f8\u5173\u7684\u53c2\u6570\u5316\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u53cc\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2601.01838", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01838", "abs": "https://arxiv.org/abs/2601.01838", "authors": ["Henok Daniel", "Omar Alhussein", "Jie Liang", "Cheng Li", "Ernesto Damiani"], "title": "Enhanced Open-Source NWDAF for Event-Driven Analytics in 5G Networks", "comment": null, "summary": "The network data analytics function (NWDAF) has been introduced in the fifth-generation (5G) core standards to enable event-driven analytics and support intelligent network automation. However, existing implementations remain largely proprietary, and open-source alternatives lack comprehensive support for end-to-end event subscription and notification. In this paper, we present an open source NWDAF framework integrated into an existing Free5GC implementation, which serves as an open-source 5G core implementation. Our implementation extends the session management function to support standardized event exposure interfaces and introduces custom-built notification mechanisms into the SMF and the access and mobility management function for seamless data delivery. The NWDAF subscribes to events and generates analytics on user equipment (UE) behavior, session lifecycle, and handover dynamics. We validate our system through a two-week deployment involving four virtual next-generation NodeBs (gNBs) and multiple virtual UEs with dynamic mobility patterns. To demonstrate predictive capabilities, we incorporate a mobility-aware module that achieves 80.65\\% accuracy in forecasting the next gNB handover cell. The framework supports reliable UE registration, state tracking, and cross-cell handovers.", "AI": {"tldr": "\u5f00\u6e90NWDAF\u6846\u67b6\u96c6\u6210Free5GC\uff0c\u652f\u6301\u6807\u51c6\u5316\u4e8b\u4ef6\u8ba2\u9605\u4e0e\u9884\u6d4b\u5206\u6790\uff0c\u90e8\u7f72\u9a8c\u8bc1\u53ef\u9760\u6027\u548c80.65%\u5207\u6362\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709NWDAF\u5b9e\u73b0\u591a\u4e3a\u4e13\u6709\u65b9\u6848\uff0c\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u7f3a\u4e4f\u7aef\u5230\u7aef\u4e8b\u4ef6\u8ba2\u9605\u4e0e\u901a\u77e5\u7684\u5168\u9762\u652f\u6301\u3002", "method": "\u6269\u5c55\u4e86\u4f1a\u8bdd\u7ba1\u7406\u529f\u80fd\u4ee5\u652f\u6301\u6807\u51c6\u5316\u4e8b\u4ef6\u66b4\u9732\u63a5\u53e3\uff0c\u5e76\u5728SMF\u548cAMF\u4e2d\u5f15\u5165\u81ea\u5b9a\u4e49\u901a\u77e5\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u7f1d\u6570\u636e\u4ea4\u4ed8\u3002NWDAF\u8ba2\u9605\u4e8b\u4ef6\u5e76\u751f\u6210\u5173\u4e8eUE\u884c\u4e3a\u3001\u4f1a\u8bdd\u751f\u547d\u5468\u671f\u548c\u5207\u6362\u52a8\u6001\u7684\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e24\u5468\u90e8\u7f72\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u652f\u6301\u53ef\u9760\u7684UE\u6ce8\u518c\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u8de8\u5c0f\u533a\u5207\u6362\uff0c\u5e76\u5c55\u793a\u4e86\u9884\u6d4b\u80fd\u529b\uff08\u9884\u6d4b\u4e0b\u4e00gNB\u5207\u6362\u5c0f\u533a\u7684\u51c6\u786e\u7387\u8fbe80.65%\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5f00\u6e90NWDAF\u6846\u67b6\u6210\u529f\u96c6\u6210\u5230Free5GC\u4e2d\uff0c\u652f\u6301\u6807\u51c6\u5316\u4e8b\u4ef6\u8ba2\u9605\u4e0e\u901a\u77e5\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2601.00854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00854", "abs": "https://arxiv.org/abs/2601.00854", "authors": ["Igor Lodin", "Sergii Filatov", "Vira Filatova", "Dmytro Filatov"], "title": "Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge", "comment": "11 pages, 5 figures", "summary": "We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.", "AI": {"tldr": "MCLSC\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u548c\u6f5c\u5728\u8bed\u4e49\u753b\u5e03\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u89c6\u89c9\u60c5\u5883\u611f\u77e5\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u9ad8\u6548\u7684\u89c6\u89c9\u60c5\u5883\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u8865\u507f\u7684\u6f5c\u5728\u8bed\u4e49\u753b\u5e03\uff08MCLSC\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u9759\u6001\u548c\u52a8\u6001\u4e24\u5c42\u6f5c\u5728\u753b\u5e03\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u89e6\u53d1\u5206\u5272\u63a8\u7406\u3002", "result": "\u5728480p\u89c6\u9891\u7247\u6bb5\u4e0a\uff0c\u539f\u578b\u7cfb\u7edf\u51cf\u5c11\u4e8630\u500d\u4ee5\u4e0a\u7684\u5206\u5272\u8c03\u7528\uff0c\u5e76\u5c06\u7aef\u5230\u7aef\u5904\u7406\u65f6\u95f4\u964d\u4f4e\u4e8620\u500d\u4ee5\u4e0a\u3002", "conclusion": "MCLSC\u901a\u8fc7\u7ef4\u62a4\u9759\u6001\u548c\u52a8\u6001\u7684\u6f5c\u5728\u8bed\u4e49\u753b\u5e03\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5206\u5272\u8c03\u7528\u7684\u6b21\u6570\u548c\u5904\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u8986\u76d6\u7684\u8fde\u8d2f\u6027\u3002"}}
{"id": "2601.01219", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01219", "abs": "https://arxiv.org/abs/2601.01219", "authors": ["Hossein Amiri", "Joon-Seok Kim", "Hamdi Kavak", "Andrew Crooks", "Dieter Pfoser", "Carola Wenk", "Andreas Z\u00fcfle"], "title": "HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life", "comment": null, "summary": "Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u8f6f\u4ef6\u7ba1\u9053\uff0c\u7ed3\u5408\u7ecf\u9a8c\u6570\u636e\u548c\u6a21\u62df\u7684\u4f18\u52bf\uff0c\u751f\u6210\u5e76\u5904\u7406\u5927\u89c4\u6a21\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u8f68\u8ff9\u6570\u636e\u96c6\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u53c2\u4e0e\u8005\u504f\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u56db\u4e2a\u96c6\u6210\u7ec4\u4ef6\uff1a\u6570\u636e\u751f\u6210\u5f15\u64ce\u3001\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u6821\u51c6\u6a21\u5757\u3001\u6570\u636e\u5904\u7406\u5957\u4ef6\u548c\u53ef\u89c6\u5316\u6a21\u5757\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u5177\u6709\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\u7684\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\u5982\u6a21\u578b\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u8f6f\u4ef6\u7ba1\u9053\uff0c\u7528\u4e8e\u6821\u51c6\u3001\u751f\u6210\u3001\u5904\u7406\u548c\u53ef\u89c6\u5316\u5927\u89c4\u6a21\u4e2a\u4f53\u7ea7\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u7ecf\u9a8c\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u6a21\u62df\u7684\u63a7\u5236\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.01106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01106", "abs": "https://arxiv.org/abs/2601.01106", "authors": ["Michele Grimaldi", "Yosaku Maeda", "Hitoshi Kakami", "Ignacio Carlucho", "Yvan Petillot", "Tomoya Inoue"], "title": "Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector", "comment": null, "summary": "Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u9a8c\u8bc1\u4e86\u8d85\u6df1\u6e0a\u5e26\u81ea\u4e3b\u7269\u4f53\u56de\u6536\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u6a21\u62df\u5728\u6df1\u6d77\u5e72\u9884\u884c\u4e3a\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u6781\u7aef\u9759\u6c34\u538b\u529b\u3001\u80fd\u89c1\u5ea6\u4f4e\u3001\u6c34\u6d41\u4ee5\u53ca\u6df1\u6d77\u7cbe\u786e\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u81ea\u4e3b\u7269\u4f53\u56de\u6536\u5728\u8d85\u6df1\u6e0a\u5e26\u5177\u6709\u6311\u6218\u6027\uff0c\u5b9e\u5730\u5b9e\u9a8c\u6210\u672c\u9ad8\u3001\u98ce\u9669\u5927\u4e14\u8f66\u8f86\u53ef\u7528\u6027\u6709\u9650\u3002", "method": "\u4f7f\u7528Stonefish\u6a21\u62df\u5668\u6a21\u62df\u771f\u5b9e\u7684\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u6d41\u4f53\u52a8\u529b\u5e72\u6270\u3001\u611f\u77e5\u4ee5\u53ca\u4e0e\u76ee\u6807\u7269\u4f53\u7684\u4ea4\u4e92\uff0c\u63a7\u5236\u6846\u67b6\u7ed3\u5408\u4e86\u4e16\u754c\u5750\u6807\u7cfbPID\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u673a\u68b0\u81c2\u63a7\u5236\u5668\u3002", "result": "HSV\u5728\u6a21\u62df\u4e2d\u6210\u529f\u4ece\u6d77\u9762\u4e0b\u6f5c\u81f36000\u7c73\uff0c\u6267\u884c\u7ed3\u6784\u5316\u6d77\u5e95\u8986\u76d6\uff0c\u68c0\u6d4b\u76ee\u6807\u7269\u4f53\u5e76\u5b8c\u6210\u57fa\u4e8e\u5438\u529b\u7684\u56de\u6536\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u6a21\u62df\u4e3a\u6df1\u6d77\u5e72\u9884\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u4f4e\u98ce\u9669\u7684\u8bc4\u4f30\u624b\u6bb5\uff0c\u9002\u7528\u4e8e\u5b9e\u5730\u90e8\u7f72\u524d\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2601.00823", "categories": ["cs.AI", "cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00823", "abs": "https://arxiv.org/abs/2601.00823", "authors": ["Austin R. Ellis-Mohr", "Max Hartman", "Lav R. Varshney"], "title": "Energy-Aware Routing to Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\u4f18\u5316\u80fd\u6e90\u4f7f\u7528\uff0c\u4e3a\u80fd\u6e90\u611f\u77e5\u7684\u6a21\u578b\u8def\u7531\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5f02\u6784\u80fd\u8017\u7279\u6027\u5bfc\u81f4\u80fd\u6e90\u6d6a\u8d39\u6216\u8fc7\u5ea6\u4f9d\u8d56\u8f85\u52a9\u80fd\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u6a21\u578b\u9009\u62e9\u548c\u63a8\u7406\u65b9\u5f0f\u4ee5\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5f02\u6784\u63a8\u7406\u80fd\u8017\uff0c\u7814\u7a76\u4e86\u4efb\u52a1\u8c03\u5ea6\u7cfb\u7edf\u4e2d\u80fd\u6e90\u4f9b\u5e94\u4e0e\u968f\u673a\u6ce2\u52a8\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u4e34\u754c\u72b6\u6001\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e8c\u9636\u8868\u5f81\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e34\u754c\u72b6\u6001\u4e0b\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u53d7\u65f6\u95f4\u3001\u6a21\u578b\u548c\u6267\u884c\u9009\u62e9\u4e2d\u53d8\u5f02\u6027\u5438\u6536\u7684\u5f71\u54cd\uff0c\u65b9\u5dee\u611f\u77e5\u8def\u7531\u548c\u8c03\u5ea6\u6210\u4e3a\u5173\u952e\u8bbe\u8ba1\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u4e3a\u5f00\u53d1\u80fd\u6e90\u611f\u77e5\u7684\u6a21\u578b\u8def\u7531\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.01980", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01980", "abs": "https://arxiv.org/abs/2601.01980", "authors": ["Manuel Parra-Roy\u00f3n", "\u00c1lvaro Rodr\u00edguez-Gallardo", "Susana S\u00e1nchez-Exp\u00f3sito", "Laura Darriba-Pol", "Jes\u00fas S\u00e1nchez-Casta\u00f1eda", "M. \u00c1ngeles Mendoza", "Juli\u00e1n Garrido", "Javier Mold\u00f3n", "Lourdes Verdes-Montenegro"], "title": "Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet", "comment": "8 pages", "summary": "The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.\n  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.\n  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.", "AI": {"tldr": "Proposes FaaS + MOEAs for SKA's data processing, shifting to computation-to-data to overcome bottlenecks, optimizing workflows for time, energy, and cost.", "motivation": "The SKA's unprecedented data volumes make traditional data-centric computing models unviable due to network and storage bottlenecks, necessitating a shift towards distributed and in-situ computing.", "method": "The work proposes integrating Function-as-a-Service (FaaS) with Multi-Objective Evolutionary Algorithms (MOEAs) to optimize data-intensive workflows by moving computation closer to data sources.", "result": "The proposed framework explores near-optimal execution plans balancing execution time, energy consumption, and data transfer costs.", "conclusion": "The paper establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SKA Regional Centres Network (SRCNet) architecture, leveraging distributed and in-situ computing with FaaS and MOEAs."}}
{"id": "2601.01869", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.01869", "abs": "https://arxiv.org/abs/2601.01869", "authors": ["Yi Zhou", "Haoyu Jiang", "Chenghao Zhu", "Andr\u00e9 Rossi"], "title": "Exact Clique Number Manipulation via Edge Interdiction", "comment": null, "summary": "The Edge Interdiction Clique Problem (EICP) aims to remove at most $k$ edges from a graph so as to minimize the size of the largest clique in the remaining graph. This problem captures a fundamental question in graph manipulation: which edges are structurally critical for preserving large cliques? Such a problem is also motivated by practical applications including protein function maintenance and image matching. The EICP is computationally challenging and belongs to a complexity class beyond NP. Existing approaches rely on general mixed-integer bilevel programming solvers or reformulate the problem into a single-level mixed integer linear program. However, they are still not scalable when the graph size and interdiction budget $k$ grow. To overcome this, we investigate new mixed integer linear formulations, which recast the problem into a sequence of parameterized Edge Blocker Clique Problems (EBCP). This perspective decomposes the original problem into simpler subproblems and enables tighter modeling of clique-related inequalities. Furthermore, we propose a two-stage exact algorithm, \\textsc{RLCM}, which first applies problem-specific reduction techniques to shrink the graph and then solves the reduced problem using a tailored branch-and-cut framework. Extensive computational experiments on maximum clique benchmark graphs, large real-world sparse networks, and random graphs demonstrate that \\textsc{RLCM} consistently outperforms existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLCM\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u95ee\u9898\u548c\u4e24\u9636\u6bb5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86EICP\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u3002", "motivation": "EICP\u95ee\u9898\u5728\u86cb\u767d\u8d28\u529f\u80fd\u7ef4\u62a4\u548c\u56fe\u50cf\u5339\u914d\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u5f62\u89c4\u6a21\u548c\u9884\u7b97k\u589e\u957f\u65f6\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u901a\u8fc7\u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u53c2\u6570\u5316\u7684Edge Blocker Clique Problems\uff08EBCP\uff09\u5e8f\u5217\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u7b97\u6cd5RLCM\uff0c\u7ed3\u5408\u95ee\u9898\u7279\u5b9a\u7684\u7f29\u51cf\u6280\u672f\u548c\u5b9a\u5236\u7684\u5206\u652f\u5207\u5272\u6846\u67b6\u3002", "result": "RLCM\u7b97\u6cd5\u5728\u6700\u5927\u56e2\u57fa\u51c6\u56fe\u3001\u5927\u578b\u771f\u5b9e\u7a00\u758f\u7f51\u7edc\u548c\u968f\u673a\u56fe\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u516c\u5f0f\u548c\u4e24\u9636\u6bb5\u7cbe\u786e\u7b97\u6cd5RLCM\uff0c\u663e\u8457\u63d0\u5347\u4e86Edge Interdiction Clique Problem\uff08EICP\uff09\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cd\u56fe\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.01968", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01968", "abs": "https://arxiv.org/abs/2601.01968", "authors": ["Yuan Guo", "Yilong Chen", "Zixiang Ren", "Derrick Wing Kwan Ng", "Jie Xu"], "title": "Near-Field Multi-Cell ISCAP with Extremely Large-Scale Antenna Array", "comment": null, "summary": "This paper investigates a coordinated multi-cell integrated sensing, communication, and powering (ISCAP) system operating in the electromagnetic near field, where each base station (BS) employs an extremely large-scale antenna array (ELAA) to simultaneously support downlink communication, wireless power transfer (WPT), and environmental sensing. Three categories of communication users (CUs) with different interference cancellation capabilities are considered, and sensing is enabled through a distributed multiple-input multiple-output (MIMO) radar architecture. To address the resulting design challenges, a robust optimization framework is proposed by optimizing the beamforming strategy to maximize the worst-case detection probability over a prescribed sensing region, subject to per-user signal-to-interference-plus-noise ratio (SINR) constraints and energy harvesting requirements at energy receivers (ERs), while explicitly capturing the uncertainty in ER locations. By leveraging semidefinite relaxation (SDR), the original non-convex problem is reformulated as a convex semidefinite program with a provably tight relaxation. Furthermore, a low-complexity maximum ratio transmission (MRT)-based suboptimal scheme is developed, yielding a closed-form solution in the asymptotic regime as the number of antenna elements approaches infinity. Extensive numerical results reveal the fundamental trade-offs among sensing accuracy, communication reliability, and WPT efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5c0f\u533a\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u4f9b\u7535\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u7b56\u7565\u5e73\u8861\u611f\u77e5\u3001\u901a\u4fe1\u548c\u4f9b\u7535\u7684\u9700\u6c42\uff0c\u5e76\u5f00\u53d1\u4e86\u4f4e\u590d\u6742\u5ea6\u6b21\u4f18\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u591a\u5c0f\u533a\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u4f9b\u7535\uff08ISCAP\uff09\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5728\u7535\u78c1\u8fd1\u573a\u73af\u5883\u4e0b\u540c\u65f6\u652f\u6301\u4e0b\u884c\u901a\u4fe1\u3001\u65e0\u7ebf\u529f\u7387\u4f20\u8f93\uff08WPT\uff09\u548c\u73af\u5883\u611f\u77e5\u7684\u6311\u6218\u3002", "method": "\u8bba\u6587\u91c7\u7528\u534a\u5b9a\u677e\u5f1b\uff08SDR\uff09\u6280\u672f\u5c06\u539f\u59cb\u975e\u51f8\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u534a\u5b9a\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u677e\u5f1b\u7684\u7d27\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u6bd4\u4f20\u8f93\uff08MRT\uff09\u7684\u4f4e\u590d\u6742\u5ea6\u6b21\u4f18\u65b9\u6848\u3002", "result": "\u6570\u503c\u7ed3\u679c\u63ed\u793a\u4e86\u611f\u77e5\u7cbe\u5ea6\u3001\u901a\u4fe1\u53ef\u9760\u6027\u548cWPT\u6548\u7387\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u7b56\u7565\uff0c\u5728\u89c4\u5b9a\u7684\u611f\u77e5\u533a\u57df\u5185\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u6982\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u6bcf\u4e2a\u7528\u6237\u7684\u4fe1\u5e72\u566a\u6bd4\u7ea6\u675f\u548c\u80fd\u91cf\u63a5\u6536\u5668\u7684\u80fd\u91cf\u6536\u96c6\u8981\u6c42\uff0c\u5e76\u660e\u786e\u8003\u8651\u4e86\u80fd\u91cf\u63a5\u6536\u5668\u4f4d\u7f6e\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u6bd4\u4f20\u8f93\u7684\u4f4e\u590d\u6742\u5ea6\u6b21\u4f18\u65b9\u6848\uff0c\u5728\u65e0\u9650\u5929\u7ebf\u6570\u91cf\u7684\u6e10\u8fd1\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u95ed\u5f0f\u89e3\u3002"}}
{"id": "2601.00879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00879", "abs": "https://arxiv.org/abs/2601.00879", "authors": ["Zahid Ullah", "Jihie Kim"], "title": "VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading", "comment": null, "summary": "Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.", "AI": {"tldr": "VLOrdinalFormer\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u7684\u5e8f\u6570\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86KOA\u65e9\u671f\u5206\u7ea7\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3KOA\u65e9\u671f\u9636\u6bb5\uff08KL1\u548cKL2\uff09\u5728KL\u5206\u7ea7\u7cfb\u7edf\u4e2d\u56e0\u5f71\u50cf\u5b66\u5dee\u5f02\u7ec6\u5fae\u5bfc\u81f4\u7684\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408ViT L16\u9aa8\u5e72\u7f51\u7edc\u3001CORAL\u5e8f\u6570\u56de\u5f52\u548cCLIP\u9a71\u52a8\u7684\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u5206\u5c42\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3001\u7c7b\u522b\u611f\u77e5\u91cd\u52a0\u6743\u548c\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u4f18\u5316\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728OAI kneeKL224\u6570\u636e\u96c6\u4e0a\uff0cVLOrdinalFormer\u5728\u5b8f\u89c2F1\u5206\u6570\u548c\u603b\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eCNN\u548cViT\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728KL1\u548cKL2\u5206\u7c7b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684VLOrdinalFormer\u6846\u67b6\u5728KOA\u5206\u7ea7\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65e9\u671f\u9636\u6bb5\uff08KL1\u548cKL2\uff09\u7684\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.01233", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01233", "abs": "https://arxiv.org/abs/2601.01233", "authors": ["Kangchen Zhu", "Zhiliang Tian", "Shangwen Wang", "Mingyue Leng", "Xiaoguang Mao"], "title": "Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling", "comment": "Accepted by ICSE 2026", "summary": "Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.", "AI": {"tldr": "Atomizer\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u8bed\u4e49\u610f\u56fe\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u590d\u5408\u63d0\u4ea4\u89e3\u8026\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u89e3\u8026\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\u4e14\u7f3a\u4e4f\u53cd\u601d\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u5408\u63d0\u4ea4\u4e2d\u7684\u8bed\u4e49\u610f\u56fe\u3002", "method": "Atomizer\u91c7\u7528\u610f\u56fe\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\uff08IO-CoT\uff09\u7b56\u7565\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u65ad\u4ee3\u7801\u53d8\u66f4\u7684\u8bed\u4e49\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u5206\u7ec4-\u8bc4\u5ba1\u534f\u4f5c\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u5206\u7ec4\u3002", "result": "\u5728C#\u548cJava\u6570\u636e\u96c6\u4e0a\uff0cAtomizer\u5e73\u5747\u6027\u80fd\u5206\u522b\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd56.0%\u548c5.5%\uff0c\u590d\u6742\u63d0\u4ea4\u4e2d\u4f18\u52bf\u8fbe16%\u4ee5\u4e0a\u3002", "conclusion": "Atomizer\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u5728C#\u548cJava\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u4e866.0%\u548c5.5%\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u63d0\u4ea4\u4e2d\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\uff08\u8d85\u8fc716%\uff09\u3002"}}
{"id": "2601.01139", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01139", "abs": "https://arxiv.org/abs/2601.01139", "authors": ["Sriram Rajasekar", "Ashwini Ratnoo"], "title": "Latent Space Reinforcement Learning for Multi-Robot Exploration", "comment": null, "summary": "Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u52a8\u7f16\u7801\u5668\u548c\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u964d\u7ef4\u548c\u590d\u6742\u73af\u5883\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4ee5\u53ca\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u8f93\u5165\u5927\u5c0f\u9650\u5236\u800c\u4ec5\u9002\u7528\u4e8e\u79bb\u6563\u73af\u5883\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u964d\u7ef4\uff0c\u5c06\u9ad8\u4fdd\u771f\u5360\u7528\u5730\u56fe\u538b\u7f29\u4e3a\u6f5c\u5728\u72b6\u6001\u5411\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7a7a\u95f4\u4fe1\u606f\uff1b\u5f15\u5165\u57fa\u4e8ePerlin\u566a\u58f0\u7684\u7a0b\u5e8f\u751f\u6210\u7b97\u6cd5\u521b\u5efa\u590d\u6742\u8bad\u7ec3\u73af\u5883\uff1b\u91c7\u7528\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u5206\u6563\u534f\u8c03\uff1b\u5f15\u5165\u52a0\u6743\u5171\u8bc6\u673a\u5236\u901a\u8fc7\u53ef\u8c03\u4fe1\u4efb\u53c2\u6570\u8c03\u8282\u5bf9\u5171\u4eab\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u591a\u667a\u80fd\u4f53\u6570\u91cf\u548c\u964c\u751f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u591a\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u80fd\u6709\u6548\u6269\u5c55\uff0c\u5bf9\u964c\u751f\u4e14\u7ed3\u6784\u4e0d\u540c\u7684\u73af\u5883\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u901a\u4fe1\u53d7\u9650\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u97e7\u6027\u3002"}}
{"id": "2601.00828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00828", "abs": "https://arxiv.org/abs/2601.00828", "authors": ["Yin Li"], "title": "Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis", "comment": "9 pages, 2 figures, 3 tables. Code available at https://github.com/Kevin0304-li/llm-self-correction", "summary": "Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u7684\u5185\u5728\u81ea\u6821\u6b63\u80fd\u529b\u6709\u9650\uff0c\u8f83\u5f31\u6a21\u578b\u6821\u6b63\u7387\u66f4\u9ad8\uff0c\u9519\u8bef\u68c0\u6d4b\u4e0e\u6821\u6b63\u6210\u529f\u7387\u65e0\u5173\uff0c\u9519\u8bef\u4f4d\u7f6e\u63d0\u793a\u53cd\u800c\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u8ba4\u4e3a\u5177\u6709\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u5176\u5185\u5728\u81ea\u6821\u6b63\uff08\u65e0\u9700\u5916\u90e8\u53cd\u9988\u7684\u81ea\u6211\u8f93\u51fa\u4fee\u6b63\uff09\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7\u8de8\u6a21\u578b\u5b9e\u9a8c\uff08GSM8K-Complex\u6570\u636e\u96c6\uff0cn=500\u6bcf\u6a21\u578b\uff0c\u5171346\u4e2a\u9519\u8bef\uff09\u7cfb\u7edf\u5206\u89e3\u81ea\u6821\u6b63\u4e3a\u4e09\u4e2a\u5b50\u80fd\u529b\uff1a\u9519\u8bef\u68c0\u6d4b\u3001\u9519\u8bef\u5b9a\u4f4d\u548c\u9519\u8bef\u6821\u6b63\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u663e\u8457\u7684\u51c6\u786e\u6027-\u6821\u6b63\u6096\u8bba\uff1a\u8f83\u5f31\u6a21\u578b\uff08GPT-3.5\uff0c66%\u51c6\u786e\u7387\uff09\u7684\u5185\u5728\u6821\u6b63\u7387\u6bd4\u5f3a\u6a21\u578b\uff08DeepSeek\uff0c94%\u51c6\u786e\u7387\uff09\u9ad81.6\u500d\uff0826.8% vs 16.7%\uff09\u3002\u9519\u8bef\u68c0\u6d4b\u7387\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u5dee\u5f02\u5de8\u5927\uff0810%\u81f382%\uff09\uff0c\u4f46\u68c0\u6d4b\u80fd\u529b\u4e0d\u9884\u6d4b\u6821\u6b63\u6210\u529f\u3002\u63d0\u4f9b\u9519\u8bef\u4f4d\u7f6e\u63d0\u793a\u5bf9\u6240\u6709\u6a21\u578b\u5747\u6709\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5173\u4e8e\u6a21\u578b\u80fd\u529b\u548c\u81ea\u6211\u6539\u8fdb\u7684\u7ebf\u6027\u5047\u8bbe\uff0c\u5bf9\u81ea\u4f18\u5316\u6d41\u7a0b\u7684\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.02092", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02092", "abs": "https://arxiv.org/abs/2601.02092", "authors": ["Abdullah Al Asif", "Sixing Yu", "Juan Pablo Munoz", "Arya Mazaheri", "Ali Jannesari"], "title": "SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks", "comment": null, "summary": "SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \\textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\\times$ lower total communication cost and $13\\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.", "AI": {"tldr": "SuperSFL\u901a\u8fc7\u52a8\u6001\u5b50\u7f51\u7edc\u548cTPGF\u673a\u5236\uff0c\u5728\u5f02\u6784\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u6548\u7387\uff0c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3SplitFed Learning\u5728\u5f02\u6784\u73af\u5883\u4e2d\u56e0\u8bbe\u5907\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\u5dee\u5f02\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "SuperSFL\u91c7\u7528\u6743\u91cd\u5171\u4eab\u7684\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u5ba2\u6237\u7aef\u7279\u5b9a\u5b50\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u4e09\u9636\u6bb5\u68af\u5ea6\u878d\u5408\uff08TPGF\uff09\u673a\u5236\u4f18\u5316\u672c\u5730\u66f4\u65b0\u3001\u670d\u52a1\u5668\u7aef\u8ba1\u7b97\u548c\u68af\u5ea6\u878d\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u5bb9\u9519\u7684\u5ba2\u6237\u7aef\u5206\u7c7b\u5668\u548c\u534f\u4f5c\u5f0f\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u805a\u5408\u673a\u5236\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0cSuperSFL\u6bd4\u57fa\u7ebfSFL\u6536\u655b\u901f\u5ea6\u5feb2-5\u500d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e20\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed13\u500d\uff0c\u4e14\u80fd\u6548\u66f4\u9ad8\u3002", "conclusion": "SuperSFL\u901a\u8fc7\u52a8\u6001\u751f\u6210\u8d44\u6e90\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u7279\u5b9a\u5b50\u7f51\u7edc\u548c\u5f15\u5165\u4e09\u9636\u6bb5\u68af\u5ea6\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5f02\u6784\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u6548\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002"}}
{"id": "2601.02021", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.02021", "abs": "https://arxiv.org/abs/2601.02021", "authors": ["Runze Zheng", "Yuqing Zheng", "Zhengyi Cheng", "Long Luo", "Haoxiang Luo", "Gang Sun", "Hongfang Yu", "Dusit Niyato"], "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI", "comment": null, "summary": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.", "AI": {"tldr": "AgentVNE\u662f\u4e00\u4e2a\u4e91\u8fb9\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u548c\u62d3\u6251\u63a8\u7406\u89e3\u51b3\u4e86\u8fb9\u7f18\u667a\u80fd\u4e2d\u591a\u4ee3\u7406\u670d\u52a1\u90e8\u7f72\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u591a\u4ee3\u7406\u670d\u52a1\uff08MAS\uff09\u90e8\u7f72\u9762\u4e34\u8d44\u6e90\u53d7\u9650\u3001\u52a8\u6001\u5185\u5b58\u79ef\u7d2f\u548c\u590d\u6742\u8de8\u8282\u70b9\u4ea4\u4e92\u7b49\u6311\u6218\uff0c\u4f20\u7edfVNE\u7b97\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5176\u5b9e\u65f6\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5c42\u67b6\u6784\u7684AgentVNE\u6846\u67b6\uff0c\u7ed3\u5408LLM\u8bc6\u522b\u8bed\u4e49\u7ea6\u675f\u5e76\u751f\u6210\u8d44\u6e90\u589e\u5f3a\uff0c\u540c\u65f6\u6784\u5efa\u8d44\u6e90\u76f8\u4f3c\u6027\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548cPPO\u5fae\u8c03\u7b56\u7565\u6355\u83b7\u52a8\u6001\u5de5\u4f5c\u6d41\u4e0e\u5f02\u6784\u7f51\u7edc\u7684\u62d3\u6251\u76f8\u4f3c\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cAgentVNE\u5c06\u5de5\u4f5c\u6d41\u901a\u4fe1\u5ef6\u8fdf\u964d\u4f4e\u81f3\u57fa\u7ebf\u768440%\u4ee5\u4e0b\uff0c\u5e76\u5728\u9ad8\u8d1f\u8f7d\u573a\u666f\u4e0b\u5c06\u670d\u52a1\u63a5\u53d7\u7387\u63d0\u9ad8\u4e86\u7ea65%-10%\u3002", "conclusion": "AgentVNE\u4e3a\u8fb9\u7f18\u667a\u80fd\u4e2d\u7684\u591a\u4ee3\u7406\u670d\u52a1\u90e8\u7f72\u63d0\u4f9b\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u57fa\u7840\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u670d\u52a1\u63a5\u53d7\u7387\u3002"}}
{"id": "2601.00887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00887", "abs": "https://arxiv.org/abs/2601.00887", "authors": ["Hongbo Jin", "Kuanwei Lin", "Wenhao Zhang", "Yichen Jin", "Ge Li"], "title": "VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition", "comment": null, "summary": "Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.", "AI": {"tldr": "VideoCuRL\u901a\u8fc7\u5206\u89e3\u89c6\u89c9\u548c\u8ba4\u77e5\u96be\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u5f3a\u5316\u5b66\u4e60\u8bfe\u7a0b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u4e3b\u8981\u4f9d\u8d56\u968f\u673a\u6570\u636e\u6d17\u724c\u6216\u57fa\u4e8e\u6807\u91cf\u96be\u5ea6\u6307\u6807\u7684\u7b80\u5355\u8bfe\u7a0b\u7b56\u7565\uff0c\u65e0\u6cd5\u533a\u5206\u89c6\u89c9\u65f6\u95f4\u611f\u77e5\u8d1f\u8377\u548c\u8ba4\u77e5\u63a8\u7406\u6df1\u5ea6\u8fd9\u4e24\u4e2a\u6b63\u4ea4\u6311\u6218\u3002", "method": "\u63d0\u51faVideoCuRL\u6846\u67b6\uff0c\u4f7f\u7528\u5149\u6d41\u548c\u5173\u952e\u5e27\u71b5\u4f5c\u4e3a\u89c6\u89c9\u590d\u6742\u5ea6\u7684\u65e0\u8bad\u7ec3\u4ee3\u7406\uff0c\u6821\u51c6\u60ca\u8bb6\u5ea6\u4f5c\u4e3a\u8ba4\u77e5\u590d\u6742\u5ea6\u7684\u4ee3\u7406\uff0c\u5c06\u6570\u636e\u6620\u5c04\u5230\u4e8c\u7ef4\u8bfe\u7a0b\u7f51\u683c\uff0c\u5e76\u91c7\u7528\u80fd\u529b\u611f\u77e5\u5bf9\u89d2\u7ebf\u6ce2\u524d\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u8c03\u5ea6\u3002", "result": "VideoCuRL\u5728\u63a8\u7406\u4efb\u52a1\uff08VSI-Bench +2.5\uff09\u548c\u611f\u77e5\u4efb\u52a1\uff08VideoMME +2.9\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u4e14\u6d88\u9664\u4e86\u751f\u6210\u5f0f\u8bfe\u7a0b\u7684\u9ad8\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "VideoCuRL\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u89c6\u89c9\u65f6\u95f4\u611f\u77e5\u8d1f\u8377\u548c\u8ba4\u77e5\u63a8\u7406\u6df1\u5ea6\u4e24\u4e2a\u6b63\u4ea4\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89c6\u9891\u540e\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01271", "abs": "https://arxiv.org/abs/2601.01271", "authors": ["Qingxiao Tao", "Xiaodong Gu", "Hao Zhong", "Beijun Shen"], "title": "CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs", "comment": null, "summary": "Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.", "AI": {"tldr": "CatchAll\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4ed3\u5e93\u611f\u77e5\u5f02\u5e38\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u5c42\u77e5\u8bc6\u6574\u5408\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4ed3\u5e93\u7ea7\u5f02\u5e38\u5904\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\u3002", "method": "CatchAll\u4e3aLLM\u63d0\u4f9b\u4e86\u4e09\u5c42\u5f02\u5e38\u5904\u7406\u77e5\u8bc6\uff1aAPI\u7ea7\u5f02\u5e38\u77e5\u8bc6\u3001\u4ed3\u5e93\u7ea7\u6267\u884c\u4e0a\u4e0b\u6587\u548c\u8de8\u4ed3\u5e93\u5904\u7406\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u6307\u5bfcLLM\u751f\u6210\u4ee3\u7801\u3002", "result": "CatchAll\u5728RepoExEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cCodeBLEU\u5f97\u5206\u4e3a0.31\uff0c\u610f\u56fe\u9884\u6d4b\u51c6\u786e\u7387\u4e3a60.1%\uff0cPass@1\u4e3a29%\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CatchAll\u901a\u8fc7\u6574\u5408API\u7ea7\u5f02\u5e38\u77e5\u8bc6\u3001\u4ed3\u5e93\u7ea7\u6267\u884c\u4e0a\u4e0b\u6587\u548c\u8de8\u4ed3\u5e93\u5904\u7406\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4ed3\u5e93\u7ea7\u5f02\u5e38\u5904\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.01144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01144", "abs": "https://arxiv.org/abs/2601.01144", "authors": ["Shu Pan", "Simon Archieri", "Ahmet Cinar", "Jonatan Scharff Willners", "Ignacio Carlucho", "Yvan Petillot"], "title": "VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction", "comment": null, "summary": "Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.", "AI": {"tldr": "VISO\u662f\u4e00\u79cd\u878d\u5408\u7acb\u4f53\u76f8\u673a\u3001IMU\u548c3D\u58f0\u7eb3\u7684\u6c34\u4e0bSLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u6821\u51c6\u548c\u5149\u5ea6\u6e32\u67d3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u89c6\u89c9\u6311\u6218\u4e25\u91cd\u5f71\u54cd\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9ad8\u4fdd\u771f\u7a20\u5bc6\u91cd\u5efa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7cbe\u7684\u5728\u7ebf\u6821\u51c6\u65b9\u6cd5\u7528\u4e8e3D\u58f0\u7eb3\u4e0e\u76f8\u673a\u4e4b\u95f4\u7684\u5916\u53c2\u4f30\u8ba1\uff0c\u5e76\u8bbe\u8ba1\u4e863D\u58f0\u7eb3\u70b9\u4e91\u7684\u5149\u5ea6\u6e32\u67d3\u7b56\u7565\u4ee5\u589e\u5f3a\u58f0\u7eb3\u5730\u56fe\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u5ba4\u6c34\u7bb1\u548c\u5f00\u653e\u6e56\u6cca\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVISO\u5728\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6c34\u4e0b\u548c\u57fa\u4e8e\u89c6\u89c9\u7684SLAM\u7b97\u6cd5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4e0e\u79bb\u7ebf\u7a20\u5bc6\u5efa\u56fe\u65b9\u6cd5\u76f8\u5f53\u7684\u5b9e\u65f6\u7a20\u5bc63D\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "VISO\u7cfb\u7edf\u901a\u8fc7\u878d\u5408\u7acb\u4f53\u76f8\u673a\u3001IMU\u548c3D\u58f0\u7eb3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7a20\u5bc63D\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6c34\u4e0bSLAM\u7b97\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u63a5\u8fd1\u79bb\u7ebf\u7a20\u5bc6\u5efa\u56fe\u65b9\u6cd5\u7684\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2601.00830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00830", "abs": "https://arxiv.org/abs/2601.00830", "authors": ["Deep Pankajbhai Mehta"], "title": "Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning", "comment": "22 pages, 8 figures, 9 tables", "summary": "When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.", "AI": {"tldr": "AI\u6a21\u578b\u7684\u89e3\u91ca\u5f80\u5f80\u4e0d\u771f\u5b9e\u53cd\u6620\u5176\u63a8\u7406\u5f71\u54cd\u56e0\u7d20\uff0c\u5f3a\u5236\u62a5\u544a\u63d0\u793a\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u7528\u6237\u504f\u597d\u63d0\u793a\u5c24\u5176\u5371\u9669\u3002", "motivation": "\u9a8c\u8bc1AI\u7cfb\u7edf\u9010\u6b65\u89e3\u91ca\u5176\u63a8\u7406\u65f6\uff0c\u8fd9\u4e9b\u89e3\u91ca\u662f\u5426\u771f\u5b9e\u53cd\u6620\u4e86\u5f71\u54cdAI\u7b54\u6848\u7684\u56e0\u7d20\u3002", "method": "\u572811\u4e2a\u9886\u5148\u7684AI\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5d4c\u5165\u63d0\u793a\u5e76\u6d4b\u91cf\u6a21\u578b\u662f\u5426\u63d0\u53ca\u8fd9\u4e9b\u63d0\u793a\uff0c\u8fdb\u884c\u4e86\u8d85\u8fc79,000\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u7684\u7814\u7a76\u3002", "result": "\u6a21\u578b\u51e0\u4e4e\u4ece\u4e0d\u81ea\u53d1\u63d0\u53ca\u63d0\u793a\uff0c\u4f46\u5f53\u76f4\u63a5\u8be2\u95ee\u65f6\u4f1a\u627f\u8ba4\u6ce8\u610f\u5230\u5b83\u4eec\uff1b\u5f3a\u5236\u62a5\u544a\u63d0\u793a\u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u65e0\u63d0\u793a\u65f6\u4e5f\u62a5\u544a\uff0c\u5e76\u964d\u4f4e\u51c6\u786e\u6027\uff1b\u7279\u522b\u5371\u9669\u7684\u63d0\u793a\u662f\u90a3\u4e9b\u8fce\u5408\u7528\u6237\u504f\u597d\u7684\uff0c\u6a21\u578b\u6700\u5e38\u9075\u5faa\u4f46\u6700\u5c11\u62a5\u544a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u89c2\u5bdfAI\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0d\u8db3\u4ee5\u53d1\u73b0\u5176\u9690\u85cf\u7684\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2601.02286", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02286", "abs": "https://arxiv.org/abs/2601.02286", "authors": ["Rahul Sengupta", "Nooshin Yousefzadeh", "Manav Sanghvi", "Yash Ranjan", "Anand Rangarajan", "Sanjay Ranka", "Yashaswi Karnati", "Jeremy Dilmore", "Tushar Patel", "Ryan Casburn"], "title": "BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation", "comment": "6 pages, 10 figures", "summary": "With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO\", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.", "AI": {"tldr": "BigSUMO\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u901a\u6570\u636e\u5206\u6790\u3001\u4e2d\u65ad\u68c0\u6d4b\u548c\u5e76\u884c\u6a21\u62df\uff0c\u5e2e\u52a9\u4f18\u5316\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u3002", "motivation": "\u968f\u7740\u5168\u7403\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6548\u7ba1\u7406\u5bf9\u4ea4\u901a\u673a\u6784\u548c\u57ce\u5e02\u89c4\u5212\u8005\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5de5\u5177\u6765\u5206\u6790\u5927\u91cf\u5b58\u50a8\u7684\u4ea4\u901a\u6570\u636e\u5e76\u8fdb\u884c\u6709\u6548\u5e72\u9884\u3002", "method": "BigSUMO\u6574\u5408\u4e86\u9ad8\u5206\u8fa8\u7387\u73af\u8def\u68c0\u6d4b\u5668\u548c\u4fe1\u53f7\u72b6\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u7a00\u758f\u7684\u63a2\u6d4b\u8f68\u8ff9\u6570\u636e\uff0c\u8fdb\u884c\u63cf\u8ff0\u6027\u5206\u6790\u5e76\u68c0\u6d4b\u6f5c\u5728\u4e2d\u65ad\u3002\u7136\u540e\u4f7f\u7528SUMO\u5fae\u6a21\u62df\u5668\u8fdb\u884c\u9884\u6d4b\u6027\u5206\u6790\uff0c\u6d4b\u8bd5\u6570\u767e\u79cd\u5047\u8bbe\u573a\u666f\u4ee5\u4f18\u5316\u4ea4\u901a\u6027\u80fd\u3002", "result": "BigSUMO\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u6570\u636e\u5904\u7406\u548c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u96c6\u6210\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u3001\u53ef\u6269\u5c55\u6027\u4e14\u6613\u4e8e\u90e8\u7f72\u3002", "conclusion": "BigSUMO\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u652f\u6301\u3002"}}
{"id": "2601.02240", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.02240", "abs": "https://arxiv.org/abs/2601.02240", "authors": ["Matteo Bordin", "Andrea Lacava", "Michele Polese", "Francesca Cuomo", "Tommaso Melodia"], "title": "Enabling Deep Reinforcement Learning Research for Energy Saving in Open RAN", "comment": null, "summary": "The growing performance demands and higher deployment densities of next-generation wireless systems emphasize the importance of adopting strategies to manage the energy efficiency of mobile networks. In this demo, we showcase a framework that enables research on Deep Reinforcement Learning (DRL) techniques for improving the energy efficiency of intelligent and programmable Open Radio Access Network (RAN) systems. Using the open-source simulator ns-O-RAN and the reinforcement learning environment Gymnasium, the framework enables to train and evaluate DRL agents that dynamically control the activation and deactivation of cells in a 5G network. We show how to collect data for training and evaluate the impact of DRL on energy efficiency in a realistic 5G network scenario, including users' mobility and handovers, a full protocol stack, and 3rd Generation Partnership Project (3GPP)-compliant channel models. The tool will be open-sourced and a tutorial for energy efficiency testing in ns-O-RAN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDRL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f00\u6e90\u5de5\u5177ns-O-RAN\u548cGymnasium\u4f18\u53165G\u7f51\u7edc\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u5bf9\u6027\u80fd\u548c\u90e8\u7f72\u5bc6\u5ea6\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u6709\u6548\u7ba1\u7406\u79fb\u52a8\u7f51\u7edc\u7684\u80fd\u6e90\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u6a21\u62df\u5668ns-O-RAN\u548c\u5f3a\u5316\u5b66\u4e60\u73af\u5883Gymnasium\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30DRL\u4ee3\u7406\u52a8\u6001\u63a7\u52365G\u7f51\u7edc\u4e2d\u5c0f\u533a\u6fc0\u6d3b\u4e0e\u4f11\u7720\u3002", "result": "\u5c55\u793a\u4e86\u5728\u771f\u5b9e5G\u7f51\u7edc\u573a\u666f\u4e2d\uff08\u5305\u62ec\u7528\u6237\u79fb\u52a8\u6027\u3001\u5207\u6362\u3001\u5b8c\u6574\u534f\u8bae\u6808\u548c3GPP\u517c\u5bb9\u4fe1\u9053\u6a21\u578b\uff09DRL\u5bf9\u80fd\u6e90\u6548\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u6536\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u667a\u80fd\u53ef\u7f16\u7a0b\u5f00\u653e\u65e0\u7ebf\u63a5\u5165\u7f51\uff08O-RAN\uff09\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u6559\u7a0b\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2601.00888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00888", "abs": "https://arxiv.org/abs/2601.00888", "authors": ["Happy Gery Pangestu", "Andi Prademon Yunus", "Siti Khomsah"], "title": "Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study", "comment": "29 pages, 9 figures, submitted in VCIBA", "summary": "Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cdCNN\u9aa8\u5e72\u7f51\u7edc\u5728NST\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0ResNet\u5728\u6548\u7387\u548c\u7ed3\u6784\u4fdd\u7559\u4e0a\u4f18\u4e8eVGG\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u8721\u67d3\u751f\u6210\u5e94\u7528\u3002", "motivation": "\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff08NST\uff09\u4e3a\u5370\u5ea6\u5c3c\u897f\u4e9a\u8721\u67d3\u56fe\u6848\u7684\u6570\u5b57\u4fdd\u5b58\u548c\u751f\u6210\u63a2\u7d22\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eVGG\u67b6\u6784\uff0c\u5176\u5f3a\u5927\u7684\u98ce\u683c\u8868\u73b0\u529b\u4ee5\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4e3a\u4ee3\u4ef7\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4e94\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u9aa8\u5e72\u7f51\u7edc\uff08VGG16\u3001VGG19\u3001Inception V3\u3001ResNet50\u548cResNet101\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u57fa\u4e8e245\u4e2a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u7ed3\u5408\u5b9a\u91cf\u6307\u6807\u3001\u5b9a\u6027\u8bc4\u4f30\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u4ee5\u68c0\u9a8c\u7ed3\u6784\u4fdd\u7559\u3001\u98ce\u683c\u884c\u4e3a\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9aa8\u5e72\u7f51\u7edc\u9009\u62e9\u5728\u7ed3\u6784\u76f8\u4f3c\u6027\u4e0a\u6ca1\u6709\u4ea7\u751f\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff08ANOVA on SSIM\uff0cp=0.83\uff09\uff0c\u8868\u660e\u7ed3\u6784\u4fdd\u7559\u6c34\u5e73\u76f8\u5f53\u800c\u975e\u98ce\u683c\u8d28\u91cf\u76f8\u540c\u3002\u57fa\u4e8eResNet\u7684\u67b6\u6784\u6bd4VGG\u6a21\u578b\u5feb\u7ea65-6\u500d\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u611f\u77e5\u76f8\u4f3c\u6027\uff08LPIPS=0.53\uff09\uff0c\u4e14\u9700\u8981\u8d85\u8fc716\u500d\u7684FLOPs\uff080.63 vs 10.12 GFLOPs\uff09\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e00\u81f4\u7684\u98ce\u683c\u6743\u8861\uff0cVGG\u4ea7\u751f\u66f4\u5bc6\u96c6\u7684\u7ed8\u753b\u7eb9\u7406\uff0cResNet\u503e\u5411\u4e8e\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u8721\u67d3\u7b14\u89e6\u4fdd\u7559\uff0c\u98ce\u683c\u5316\u8f83\u6e29\u548c\uff0cInception V3\u8868\u73b0\u51fa\u4e2d\u95f4\u4f46\u66f4\u5608\u6742\u7684\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u91cd\u65b0\u5b9a\u4f4d\u4e86NST\u4e2d\u7684\u67b6\u6784\u9009\u62e9\uff0c\u4ece\u6700\u5927\u5316\u98ce\u683c\u5f3a\u5ea6\u8f6c\u5411\u6548\u7387\u611f\u77e5\u548c\u7ed3\u6784\u4fdd\u7559\u7684\u90e8\u7f72\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8eResNet\u7684\u9aa8\u5e72\u7f51\u7edc\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9762\u5411\u884c\u4e1a\u7684\u8721\u67d3\u751f\u6210\u7684\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2601.01320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01320", "abs": "https://arxiv.org/abs/2601.01320", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python", "comment": null, "summary": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.", "AI": {"tldr": "ALPHA \u662f\u9996\u4e2a\u5206\u5c42\u611f\u77e5\u7684 Python \u6f0f\u6d1e\u68c0\u6d4b\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86 LLMs \u548c SAST \u5de5\u5177\u7684\u6027\u80fd\uff0c\u53d1\u73b0 LLMs \u6574\u4f53\u8868\u73b0\u66f4\u597d\u4f46 SAST \u66f4\u7cbe\u786e\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u57fa\u51c6\u7f3a\u4e4f CWE \u7ea7\u522b\u7684\u7279\u5f02\u6027\uff0c\u65e0\u6cd5\u4e3a\u8fed\u4ee3\u4fee\u6b63\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002ALPHA \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "ALPHA \u91c7\u7528\u5206\u5c42\u611f\u77e5\u7684 CWE \u7279\u5b9a\u60e9\u7f5a\u673a\u5236\uff0c\u533a\u5206\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u89c4\u8303\u548c\u6a2a\u5411\u9519\u8bef\uff0c\u8bc4\u4f30\u4e86 7 \u4e2a LLMs \u548c 2 \u4e2a SAST \u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "LLMs \u663e\u8457\u4f18\u4e8e SAST \u5de5\u5177\uff0c\u4f46 SAST \u5728\u68c0\u6d4b\u53d1\u751f\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u786e\u5ea6\u3002\u6a21\u578b\u95f4\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u5dee\u5f02\u663e\u8457\uff088.26%-81.87% \u4e00\u81f4\u7387\uff09\uff0c\u5bf9\u53cd\u9988\u9a71\u52a8\u7cfb\u7edf\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "ALPHA \u662f\u9996\u4e2a\u9488\u5bf9 Python \u7684\u51fd\u6570\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5206\u5c42\u611f\u77e5\u7684 CWE \u7279\u5b9a\u60e9\u7f5a\u8bc4\u4f30 LLMs \u548c SAST \u5de5\u5177\uff0c\u4e3a\u53cd\u9988\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u5c06 ALPHA \u60e9\u7f5a\u7eb3\u5165\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u5c42\u6b21\u611f\u77e5\u7684\u6f0f\u6d1e\u68c0\u6d4b\u3002"}}
{"id": "2601.01155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01155", "abs": "https://arxiv.org/abs/2601.01155", "authors": ["Zhang Shizhe", "Liang Jingsong", "Zhou Zhitao", "Ye Shuhan", "Wang Yizhuo", "Tan Ming Siang Derek", "Chiun Jimmy", "Cao Yuhong", "Sartoretti Guillaume"], "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation", "comment": null, "summary": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.", "AI": {"tldr": "ORION\u662f\u4e00\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u5728\u7ebf\u5bfc\u822a\uff0c\u901a\u8fc7\u5206\u6563\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u534f\u4f5c\u5b9e\u73b0\u9ad8\u6548\u5408\u4f5c\u3002", "motivation": "\u89e3\u51b3\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5e73\u8861\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u4fe1\u606f\u5171\u4eab\u3002", "method": "ORION\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u56fe\u7f16\u7801\u5668\u548c\u9009\u9879-\u8bc4\u8bba\u5bb6\u6846\u67b6\uff0c\u5b9e\u73b0\u5206\u6563\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u534f\u4f5c\u3002", "result": "ORION\u5728\u4e0d\u540c\u56e2\u961f\u89c4\u6a21\u548c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ORION\u6846\u67b6\u5728\u7269\u7406\u673a\u5668\u4eba\u56e2\u961f\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5408\u4f5c\u5bfc\u822a\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.00843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00843", "abs": "https://arxiv.org/abs/2601.00843", "authors": ["Ayda Aghaei Nia"], "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification", "comment": "16 pages, 7 figures, 3 tables. Source code and implementation available at: https://github.com/ayda-aghaei/OmniNeuro. Highlights the use of LLMs (Gemini) and Quantum probability formalism for real-time BCI explainability", "summary": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.", "AI": {"tldr": "OmniNeuro \u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684 BCI \u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u5f15\u64ce\u63d0\u4f9b\u900f\u660e\u53cd\u9988\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u89e3\u7801\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8\u4e86 BCI \u7684\u89e3\u7801\u51c6\u786e\u6027\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\uff0c\u5bfc\u81f4\u7528\u6237\u632b\u8d25\u611f\u548c\u795e\u7ecf\u53ef\u5851\u6027\u6548\u679c\u4e0d\u4f73\u3002", "method": "OmniNeuro \u6574\u5408\u4e86\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\uff1a\u7269\u7406\u5b66\uff08\u80fd\u91cf\uff09\u3001\u6df7\u6c8c\uff08\u5206\u5f62\u590d\u6742\u6027\uff09\u548c\u91cf\u5b50\u542f\u53d1\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u901a\u8fc7\u5b9e\u65f6\u795e\u7ecf\u58f0\u5316\u548c\u751f\u6210\u5f0f AI \u4e34\u5e8a\u62a5\u544a\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u5728 PhysioNet \u6570\u636e\u96c6\uff08N=109\uff09\u4e0a\uff0c\u7cfb\u7edf\u5e73\u5747\u51c6\u786e\u7387\u4e3a 58.52%\uff0c\u5b9a\u6027\u8bd5\u70b9\u7814\u7a76\uff08N=3\uff09\u8bc1\u5b9e\u53ef\u89e3\u91ca\u53cd\u9988\u6709\u52a9\u4e8e\u7528\u6237\u8c03\u8282\u5fc3\u7406\u52aa\u529b\u5e76\u51cf\u5c11\u8bd5\u9519\u9636\u6bb5\u3002", "conclusion": "OmniNeuro \u4f5c\u4e3a\u4e00\u4e2a\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u6210\u529f\u5730\u5c06 BCI \u4ece\u9ed1\u76d2\u89e3\u7801\u5668\u8f6c\u53d8\u4e3a\u900f\u660e\u7684\u53cd\u9988\u4f19\u4f34\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u795e\u7ecf\u53ef\u5851\u6027\u6548\u679c\u3002"}}
{"id": "2601.02311", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02311", "abs": "https://arxiv.org/abs/2601.02311", "authors": ["Deep Pankajbhai Mehta"], "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies", "comment": "8 pages, 3 tables", "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u653e\u7f6e\u8bed\u4e49\u9884\u6d4b\u5e76\u884c\u7b56\u7565\u884c\u4e3a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u68af\u5ea6\u5b8c\u6574\u6027\u548c\u72b6\u6001\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ec4\u5408\u7b56\u7565\u7684\u89c4\u5219\u3002", "motivation": "\u5f53\u524d\u5b9e\u8df5\u4e2d\uff0c\u5e76\u884c\u7b56\u7565\u7684\u9009\u62e9\u4f9d\u8d56\u4e8e\u8bd5\u9519\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7cfb\u7edf\u6846\u67b6\u6765\u9884\u6d4b\u5176\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u653e\u7f6e\u8bed\u4e49\uff08\u4e94\u79cd\u6a21\u5f0f\u548c\u56db\u79cd\u8bad\u7ec3\u72b6\u6001\u7684\u7ec4\u5408\uff09\u6765\u9884\u6d4b\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u91cf\uff0c\u65e0\u9700\u5b9e\u73b0\u7ec6\u8282\u3002", "result": "\u9884\u6d4b\u7ed3\u679c\u4e0e\u5df2\u53d1\u8868\u7ed3\u679c\u5b8c\u5168\u5339\u914d\uff0c\u4f8b\u5982ZeRO-3\u7684\u5185\u5b58\u6d88\u8017\u6bd4\u6570\u636e\u5e76\u884c\u5c118\u500d\uff0c\u901a\u4fe1\u6210\u672c\u589e\u52a01.5\u500d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u653e\u7f6e\u8bed\u4e49\u6765\u9884\u6d4b\u5e76\u884c\u7b56\u7565\u7684\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u68af\u5ea6\u5b8c\u6574\u6027\u548c\u72b6\u6001\u4e00\u81f4\u6027\u662f\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u5355\u8bbe\u5907\u7ed3\u679c\u5339\u914d\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b89\u5168\u7ec4\u5408\u7b56\u7565\u7684\u89c4\u5219\u3002"}}
{"id": "2601.00897", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00897", "abs": "https://arxiv.org/abs/2601.00897", "authors": ["Sai Teja Erukude", "Jane Mascarenhas", "Lior Shamir"], "title": "CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis", "comment": "23 pages", "summary": "Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.", "AI": {"tldr": "CornViT\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5CvT\u6846\u67b6\uff0c\u7528\u4e8e\u7389\u7c73\u7c92\u5206\u7ea7\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u90e8\u7f72\u4fbf\u6377\u3002", "motivation": "\u7389\u7c73\u7c92\u7684\u51c6\u786e\u5206\u7ea7\u5bf9\u79cd\u5b50\u8ba4\u8bc1\u3001\u5b9a\u5411\u64ad\u79cd\u548c\u80b2\u79cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u68c0\u67e5\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5377\u79ef\u89c6\u89c9\u53d8\u6362\u5668\uff08CvT\uff09\u6846\u67b6\uff0c\u5206\u522b\u8fdb\u884c\u7eaf\u5ea6\u3001\u5f62\u6001\u548c\u80da\u80ce\u65b9\u5411\u5206\u7c7b\u3002", "result": "CornViT\u5728\u7eaf\u5ea6\u3001\u5f62\u72b6\u548c\u80da\u80ce\u65b9\u5411\u68c0\u6d4b\u4e0a\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u5206\u522b\u4e3a93.76%\u300194.11%\u548c91.12%\uff0c\u4f18\u4e8eResNet-50\u548cDenseNet-121\u3002", "conclusion": "CornViT\u6846\u67b6\u3001\u7cbe\u9009\u6570\u636e\u96c6\u548cWeb\u5e94\u7528\u4e3a\u79cd\u5b50\u8d28\u91cf\u5de5\u4f5c\u6d41\u4e2d\u7684\u7389\u7c73\u7c92\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01413", "categories": ["cs.SE", "cs.MS", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01413", "abs": "https://arxiv.org/abs/2601.01413", "authors": ["Yingjie Ma", "Jing Guo", "Richard D. Braatz"], "title": "GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python", "comment": null, "summary": "Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.", "AI": {"tldr": "GlycoPy\u662f\u4e00\u4e2aPython\u6846\u67b6\uff0c\u652f\u6301\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u5efa\u6a21\u4e0e\u7b97\u6cd5\u5b9a\u5236\uff0c\u586b\u8865\u4e86\u7b97\u6cd5\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5728\u975e\u7ebf\u6027\uff08\u751f\u7269\uff09\u5316\u5b66\u8fc7\u7a0b\u4e2d\u6027\u80fd\u53d7\u9650\uff0c\u7f3a\u4e4f\u5206\u5c42\u5efa\u6a21\u5de5\u5177\u548c\u9ad8\u6548NMPC\u7b97\u6cd5\u5b9e\u73b0\u3002", "method": "\u5f00\u53d1\u4e86GlycoPy\uff0c\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u65b9\u7a0b\u5bfc\u5411\u3001\u9762\u5411\u5bf9\u8c61\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u652f\u6301\u5206\u5c42\u5efa\u6a21\u3001\u53c2\u6570\u4f30\u8ba1\u3001\u52a8\u6001\u4f18\u5316\u548cNMPC\u7b97\u6cd5\u5b9a\u5236\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u4ece\u7b80\u5355\u5fae\u5206\u4ee3\u6570\u7cfb\u7edf\u5230\u591a\u5c3a\u5ea6\u751f\u7269\u8fc7\u7a0b\u6a21\u578b\uff09\u9a8c\u8bc1\u4e86GlycoPy\u7684\u5efa\u6a21\u3001\u4f18\u5316\u548cNMPC\u529f\u80fd\u3002", "conclusion": "GlycoPy\u6846\u67b6\u6709\u6548\u586b\u8865\u4e86\u9ad8\u7ea7\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7b97\u6cd5\u4e0e\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5efa\u6a21\u3001\u4f18\u5316\u548c\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2601.01188", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01188", "abs": "https://arxiv.org/abs/2601.01188", "authors": ["Zhiwei Huang", "Yanwei Fu", "Yi Zhou", "Xieyuanli Chen", "Qijun Chen", "Rui Fan"], "title": "DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network", "comment": null, "summary": "LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7279\u5b9a\u6807\u5b9a\u76ee\u6807\u7684\u81ea\u76d1\u7763LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u7f51\u7edc\uff0c\u901a\u8fc7\u53cc\u9762\u6570\u636e\u589e\u5f3a\u548c\u5dee\u5f02\u56fe\u6784\u5efa\u63d0\u5347\u7cbe\u5ea6\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6807\u5b9a\u76ee\u6807\u6216\u7279\u5b9a\u9759\u6001\u573a\u666f\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u548c\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u53cc\u9762\u6570\u636e\u589e\u5f3a\u6280\u672f\u751f\u6210\u591a\u89c6\u89d2\u76f8\u673a\u89c6\u56fe\uff0c\u5e76\u8bbe\u8ba1\u53cc\u8def\u5f84\u81ea\u76d1\u7763\u6807\u5b9a\u6846\u67b6\uff0c\u7ed3\u5408\u5dee\u5f02\u56fe\u6784\u5efa\u8fc7\u7a0b\u5173\u8054LiDAR\u548c\u76f8\u673a\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5f55\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u76d1\u7763LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u7f51\u7edc\u901a\u8fc7\u53cc\u9762\u6570\u636e\u589e\u5f3a\u548c\u5dee\u5f02\u56fe\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u5b9a\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2601.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00845", "abs": "https://arxiv.org/abs/2601.00845", "authors": ["Lili Chen", "Wensheng Gan", "Shuang Liang", "Philip S. Yu"], "title": "Enhancing Temporal Awareness in LLMs for Temporal Point Processes", "comment": "preprint", "summary": "Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL", "AI": {"tldr": "TPP-TAL\u662f\u4e00\u79cd\u589e\u5f3aLLMs\u65f6\u95f4\u611f\u77e5\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u4e0e\u8bed\u4e49\u63d0\u5347\u4e8b\u4ef6\u5efa\u6a21\u6548\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u65f6\u95f4\u4fe1\u606f\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u8fd9\u5728\u51c6\u786e\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "TPP-TAL\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u800c\u975e\u7b80\u5355\u62fc\u63a5\u4e8b\u4ef6\u65f6\u95f4\u548c\u7c7b\u578b\u5d4c\u5165\uff0c\u6765\u589e\u5f3aLLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTPP-TAL\u5728\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "TPP-TAL\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.00905", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00905", "abs": "https://arxiv.org/abs/2601.00905", "authors": ["Eliot Park", "Abhi Kumar", "Pranav Rajpurkar"], "title": "Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems", "comment": "x", "summary": "While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7269\u54c1\u53ef\u56de\u6536\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0a\u6709\u8fdb\u6b65\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u9ad8\u6548\u56de\u6536\u7684\u91cd\u8981\u6027\u88ab\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u4f46\u516c\u4f17\u51c6\u786e\u5224\u65ad\u7269\u54c1\u7684\u53ef\u56de\u6536\u6027\u53ca\u5176\u6b63\u786e\u5904\u7406\u65b9\u5f0f\u4ecd\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001GPT-4o-mini\u548cClaude 3.5\uff09\u5728\u9884\u6d4b\u5e38\u89c1\u4e22\u5f03\u7269\u54c1\u53ef\u56de\u6536\u6027\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u5339\u914d\u7269\u54c1\u5230\u5408\u9002\u56de\u6536\u7bb1\u4e2d\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u76f8\u6bd4\u4e4b\u524d\u7248\u672c\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u6301\u7eed\u4f18\u5316\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u5bf9\u4e8e\u63d0\u5347\u516c\u4f17\u56de\u6536\u5b9e\u8df5\u548c\u63a8\u52a8\u73af\u5883\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.01426", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01426", "abs": "https://arxiv.org/abs/2601.01426", "authors": ["Chaofan Tao", "Jierun Chen", "Yuxin Jiang", "Kaiqi Kou", "Shaowei Wang", "Ruoyu Wang", "Xiaohui Li", "Sidi Yang", "Yiming Du", "Jianbo Dai", "Zhiming Mao", "Xinyu Wang", "Lifeng Shang", "Haoli Bai"], "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving", "comment": "Project website: https://github.com/SWE-Lego/SWE-Lego", "summary": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.", "AI": {"tldr": "SWE-Lego\u901a\u8fc7\u8f7b\u91cf\u7ea7SFT\u548c\u4f18\u5316\u6570\u636e\u96c6\uff0c\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u5e76\u901a\u8fc7TTS\u8fdb\u4e00\u6b65\u4f18\u5316\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4efb\u52a1\u4e2d\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684SFT\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u907f\u514d\u590d\u6742\u8bad\u7ec3\u8303\u5f0f\u7684\u4f9d\u8d56\u3002", "method": "SWE-Lego\u91c7\u7528\u4e09\u4e2a\u6838\u5fc3\u6784\u5efa\u5757\uff1a1\uff09\u5305\u542b32k\u9ad8\u8d28\u91cf\u4efb\u52a1\u5b9e\u4f8b\u548c18k\u9a8c\u8bc1\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff1b2\uff09\u7ed3\u5408\u9519\u8bef\u63a9\u7801\u548c\u57fa\u4e8e\u96be\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\u7684\u4f18\u5316SFT\u6d41\u7a0b\uff1b3\uff09\u57fa\u4e8e\u8bad\u7ec3\u826f\u597d\u7684\u9a8c\u8bc1\u5668\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u3002", "result": "SWE-Lego-Qwen3-8B\u548c32B\u6a21\u578b\u5728SWE-bench Verified\u4e0a\u5206\u522b\u8fbe\u523042.2%\u548c52.6%\u7684\u51c6\u786e\u7387\uff0c\u901a\u8fc7TTS@16\u8fdb\u4e00\u6b65\u63d0\u5347\u81f349.6%\u548c58.8%\u3002", "conclusion": "SWE-Lego\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u4f18\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2601.01196", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01196", "abs": "https://arxiv.org/abs/2601.01196", "authors": ["Shenqi Lu", "Liangwei Zhang"], "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners", "comment": null, "summary": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.", "AI": {"tldr": "EduSim-LLM integrates LLMs with robot simulation to enable language-driven control, achieving over 88.9% accuracy in complex tasks.", "motivation": "The integration of natural language understanding into robotic control is a challenge hindering intuitive human control over complex robotic systems, limiting their educational and practical accessibility.", "method": "The study presents EduSim-LLM, an educational platform integrating LLMs with robot simulation, designing two human-robot interaction models (direct and autonomous control) and conducting systematic simulations based on multiple language models.", "result": "Experiential results show LLMs can reliably translate natural language instructions into executable robot behavior sequences, with improved accuracy through prompt-engineering templates.", "conclusion": "LLMs can reliably convert natural language into structured robot actions, with prompt-engineering templates significantly improving instruction-parsing accuracy. The highest complexity tests show an overall accuracy rate exceeding 88.9%."}}
{"id": "2601.00848", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00848", "abs": "https://arxiv.org/abs/2601.00848", "authors": ["Ron F. Del Rosario"], "title": "Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models", "comment": "26 pages, 3 figures, 7 tables. Datasets and code: https://huggingface.co/guerilla7/agentic-safety-gguf", "summary": "We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7OpenTelemetry\u8ddf\u8e2a\u5206\u6790\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u4ee3\u7406AI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u65f6\u95f4\u653b\u51fb\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u811a\u672c\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u4ee3\u7406AI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u65f6\u95f4\u653b\u51fb\u6a21\u5f0f\uff0c\u586b\u8865\u73b0\u6709\u6280\u672f\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3QLoRA\u5fae\u8c03\u5728\u8d44\u6e90\u53d7\u9650\u7684ARM64\u786c\u4ef6\uff08NVIDIA DGX Spark\uff09\u4e0a\u8fdb\u884c\u4e86\u4e09\u6b21\u8bad\u7ec3\u8fed\u4ee3\uff0c\u5e76\u91c7\u7528\u7b56\u7565\u6027\u589e\u5f3a\u3002\u6570\u636e\u96c6\u5305\u62ec\u6765\u81ea18\u4e2a\u516c\u5171\u7f51\u7edc\u5b89\u5168\u6e90\u768480,851\u4e2a\u793a\u4f8b\u548c35,026\u4e2a\u5408\u6210\u7684OpenTelemetry\u8ddf\u8e2a\u3002", "result": "\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece42.86%\u63d0\u5347\u81f374.29%\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u768431.4\u4e2a\u767e\u5206\u70b9\u63d0\u5347\u3002\u9488\u5bf9\u7279\u5b9a\u77e5\u8bc6\u7a7a\u767d\u7684\u793a\u4f8b\u8868\u73b0\u4f18\u4e8e\u65e0\u5dee\u522b\u6269\u5c55\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u6839\u636e\u5176\u5a01\u80c1\u73af\u5883\u6784\u5efa\u81ea\u5b9a\u4e49\u7684\u4ee3\u7406\u5b89\u5168\u6a21\u578b\u3002\u5c3d\u7ba1\u5b9e\u9645\u90e8\u7f72\u7531\u4e8e\u8bef\u62a5\u7387\u9700\u8981\u4eba\u5de5\u76d1\u7763\uff0c\u4f46\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u4ee3\u7406AI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u65f6\u95f4\u653b\u51fb\u6a21\u5f0f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.00913", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00913", "abs": "https://arxiv.org/abs/2601.00913", "authors": ["Subhankar Mishra"], "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs", "AI": {"tldr": "Clean-GS\u5229\u7528\u7a00\u758f\u8bed\u4e49\u63a9\u7801\u53bb\u96643D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u4e2d\u7684\u80cc\u666f\u6742\u6ce2\u548c\u6d6e\u52a8\u7269\uff0c\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u538b\u7f29\uff0c\u9002\u5408\u5e26\u5bbd\u53d7\u9650\u5e94\u7528\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u573a\u666f\u91cd\u5efa\uff0c\u4f46\u4f1a\u4ea7\u751f\u5927\u91cf\u4f2a\u9ad8\u65af\uff08\u6d6e\u52a8\u7269\uff09\uff0c\u4e0d\u4ec5\u906e\u6321\u76ee\u6807\u5bf9\u8c61\uff0c\u8fd8\u589e\u52a0\u6a21\u578b\u5927\u5c0f\uff0c\u9650\u5236\u4e86\u5728\u5e26\u5bbd\u53d7\u9650\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "Clean-GS\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u6295\u5f71\u5230\u63a9\u7801\u533a\u57df\u8fdb\u884c\u767d\u540d\u5355\u8fc7\u6ee4\uff1b2\uff09\u6df1\u5ea6\u7f13\u51b2\u989c\u8272\u9a8c\u8bc1\uff1b3\uff09\u57fa\u4e8e\u90bb\u5c45\u7684\u5f02\u5e38\u503c\u53bb\u9664\u3002\u8be5\u65b9\u6cd5\u4ec5\u97003\u4e2a\u5206\u5272\u63a9\u7801\uff081%\u7684\u89c6\u56fe\uff09\u5373\u53ef\u8bc6\u522b\u5e76\u79fb\u9664\u4e0d\u5c5e\u4e8e\u76ee\u6807\u5bf9\u8c61\u7684\u9ad8\u65af\u3002", "result": "\u5728Tanks and Temples\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cClean-GS\u5c06\u6587\u4ef6\u5927\u5c0f\u4ece125MB\u51cf\u5c11\u523047MB\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\uff0c\u4f7f3DGS\u6a21\u578b\u66f4\u9002\u5408\u7f51\u9875\u90e8\u7f72\u548cAR/VR\u5e94\u7528\u3002", "conclusion": "Clean-GS\u901a\u8fc7\u7ed3\u5408\u767d\u540d\u5355\u7a7a\u95f4\u8fc7\u6ee4\u3001\u989c\u8272\u5f15\u5bfc\u9a8c\u8bc1\u548c\u57fa\u4e8e\u90bb\u5c45\u7684\u5f02\u5e38\u503c\u53bb\u9664\uff0c\u6709\u6548\u51cf\u5c11\u4e863D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u4e2d\u7684\u4f2a\u9ad8\u65af\uff08\u6d6e\u52a8\u7269\uff09\uff0c\u5b9e\u73b0\u4e8660-80%\u7684\u6a21\u578b\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u8c61\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5e26\u5bbd\u53d7\u9650\u7684\u5e94\u7528\u3002"}}
{"id": "2601.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01514", "abs": "https://arxiv.org/abs/2601.01514", "authors": ["Matej Kucera", "Marco Castelluccio", "Daniel Feitosa", "Ayushi Rastogi"], "title": "Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox", "comment": "11 pages, 1 figure, 4 tables. To be published in ICSE-SEIP 2026 conference proceedings", "summary": "The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7fa4\u4f53\u4ee3\u7801\u8bc4\u5ba1\uff08\u5982GitHub\u7684\u7fa4\u4f53\u5206\u914d\uff09\u80fd\u63d0\u9ad8\u8d28\u91cf\uff08\u51cf\u5c11\u56de\u5f52\uff09\uff0c\u4f46\u5bf9\u901f\u5ea6\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u540c\u65f6\u5e26\u6765\u5de5\u4f5c\u5747\u8861\u548c\u57f9\u8bad\u65b0\u8bc4\u5ba1\u5458\u7684\u597d\u5904\u3002", "motivation": "\u63a2\u8ba8\u4ee3\u7801\u8bc4\u5ba1\u5206\u914d\u8fc7\u7a0b\u4e2d\u2018\u7fa4\u4f53\u8bc4\u5ba1\u8bf7\u6c42\u2019\u5bf9\u8bc4\u5ba1\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u8fd9\u4e00\u73af\u8282\u7684\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u7edf\u8ba1\u5efa\u6a21\uff08\u5206\u6790\u7ea666,000\u4e2aMozilla Firefox\u9879\u76ee\u7684\u4fee\u8ba2\uff09\u548c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u7684\u5b9e\u8df5\u8005\u89c2\u70b9\u3002", "result": "\u7fa4\u4f53\u8bc4\u5ba1\u4e0e\u8bc4\u5ba1\u8d28\u91cf\u63d0\u5347\u76f8\u5173\uff0c\u4f46\u5bf9\u901f\u5ea6\u5f71\u54cd\u4e0d\u660e\u663e\u3002", "conclusion": "\u7fa4\u4f53\u8bc4\u5ba1\u8bf7\u6c42\u4e0e\u66f4\u9ad8\u7684\u8bc4\u5ba1\u8d28\u91cf\u76f8\u5173\uff08\u8868\u73b0\u4e3a\u66f4\u5c11\u7684\u56de\u5f52\u95ee\u9898\uff09\uff0c\u4f46\u5bf9\u8bc4\u5ba1\u901f\u5ea6\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002\u6b64\u5916\uff0c\u7fa4\u4f53\u8bc4\u5ba1\u8fd8\u80fd\u5e26\u6765\u5de5\u4f5c\u8d1f\u8f7d\u5747\u8861\u548c\u65b0\u8bc4\u5ba1\u5458\u57f9\u8bad\u7684\u673a\u4f1a\u3002"}}
{"id": "2601.01282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01282", "abs": "https://arxiv.org/abs/2601.01282", "authors": ["Fang Nan", "Meher Malladi", "Qingqing Li", "Fan Yang", "Joonas Juola", "Tiziano Guadagnino", "Jens Behley", "Cesar Cadena", "Cyrill Stachniss", "Marco Hutter"], "title": "SAHA: Supervised Autonomous HArvester for selective forest thinning", "comment": null, "summary": "Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578b\u673a\u5668\u4eba\u6536\u5272\u673a\uff08SAHA\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u68ee\u6797\u9009\u62e9\u6027\u758f\u4f10\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u68ee\u6797\u7ba1\u7406\u4efb\u52a1\uff08\u5982\u9009\u62e9\u6027\u758f\u4f10\uff09\u9700\u8981\u719f\u7ec3\u64cd\u4f5c\u5458\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u673a\u5668\u4eba\u6280\u672f\u53ef\u63d0\u5347\u6548\u7387\u3002", "method": "\u57fa\u4e8e4.5\u5428\u6536\u5272\u673a\u5e73\u53f0\uff0c\u901a\u8fc7\u786c\u4ef6\u6539\u9020\u5b9e\u73b0\u611f\u77e5\u4e0e\u81ea\u52a8\u63a7\u5236\uff0c\u7ed3\u5408\u5b66\u4e60\u548c\u6a21\u578b\u65b9\u6cd5\u5b9e\u73b0\u6db2\u538b\u6267\u884c\u5668\u7cbe\u786e\u63a7\u5236\u3001\u590d\u6742\u73af\u5883\u5bfc\u822a\u3001\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\u53ca\u5730\u5f62\u53ef\u7a7f\u8d8a\u6027\u8bed\u4e49\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u5317\u6b27\u68ee\u6797\u7684\u516c\u91cc\u7ea7\u81ea\u4e3b\u4efb\u52a1\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u6536\u5272\u673a\u5728\u771f\u5b9e\u68ee\u6797\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5c0f\u578b\u673a\u5668\u4eba\u6536\u5272\u673a\uff08SAHA\uff09\u5728\u68ee\u6797\u9009\u62e9\u6027\u758f\u4f10\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u63a8\u8fdb\u673a\u5668\u4eba\u68ee\u6797\u7ba1\u7406\u7684\u7ecf\u9a8c\u6559\u8bad\u3002"}}
{"id": "2601.00856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00856", "abs": "https://arxiv.org/abs/2601.00856", "authors": ["Milos Stankovic", "Ella Hirche", "Sarah Kollatzsch", "Julia Nadine Doetsch"], "title": "Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks", "comment": "Comment on arXiv:2506.08872", "summary": "Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.", "AI": {"tldr": "A critique of Kosmyna et al.'s study on AI and human performance, pointing out design flaws, reproducibility, methodological issues, and transparency gaps, suggesting improvements for publication.", "motivation": "To provide constructive feedback on the study by Kosmyna et al. (2025) to enhance its readiness for peer-reviewed publication by addressing identified limitations.", "method": "The analysis focuses on reviewing the study design, sample size, reproducibility of analyses, EEG methodology, result reporting, and transparency.", "result": "Identified key concerns include limited sample size, reproducibility issues, methodological flaws in EEG analysis, inconsistent results reporting, and lack of transparency.", "conclusion": "The paper critiques the study by Kosmyna et al. (2025), highlighting concerns about study design, reproducibility, methodological issues, result inconsistencies, and transparency. It suggests more conservative interpretation of results and improvements for peer-reviewed publication."}}
{"id": "2601.00918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00918", "abs": "https://arxiv.org/abs/2601.00918", "authors": ["Faisal Ahmed"], "title": "Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning", "comment": "15 pages, 7 figures", "summary": "Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.\n  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.", "AI": {"tldr": "TDA-Alz \u662f\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u96c6\u6210\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u63d0\u53d6\u8111 MRI \u7684\u62d3\u6251\u63cf\u8ff0\u7b26\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u4fdd\u7559\u6700\u5177\u533a\u5206\u6027\u7684\u62d3\u6251\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u96c6\u6210\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728 OASIS-1 MRI \u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86 98.19% \u7684\u51c6\u786e\u7387\u548c 99.75% \u7684 AUC\uff0c\u4f18\u4e8e\u6216\u5339\u914d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TDA-Alz \u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u3001\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u57fa\u4e8e MRI \u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\uff0c\u5177\u6709\u5728\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01602", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01602", "abs": "https://arxiv.org/abs/2601.01602", "authors": ["Henry Ndou"], "title": "MTS-1: A Lightweight Delta-Encoded Telemetry Format optimised for Low-Resource Environments and Offline-First System Health Monitoring", "comment": "4 figures, 1 table. Technical report on the MTS-1 telemetry format. Direct correspondence to henry.ndou@nust.ac.zw", "summary": "System-level telemetry is fundamental to modern remote monitoring, predictive maintenance, and AI-driven infrastructure optimisation. Existing telemetry encodings such as JSON, JSON Lines, CBOR, and Protocol Buffers were designed for high-bandwidth, always-online environments. They impose significant overhead when deployed in bandwidth-constrained networks common across Sub-Saharan Africa, rural enterprise deployments, and unstable LAN environments. This paper introduces MTS-1 (Magenta Telemetry Standard v1), a novel delta-encoded binary telemetry format designed for offline-first system monitoring, LAN-assisted proxy delivery, and energy-efficient IoT-to-server transmission. We compare MTS-1 against JSON, JSON Lines, CBOR, MessagePack, and Protocol Buffers across payload size, encoding cost, network efficiency, and cost-latency performance. Synthetic benchmarking demonstrates preliminary compression improvements of up to 74.7% versus JSON and 5.4% versus MessagePack, with linear scaling characteristics across dataset sizes.", "AI": {"tldr": "MTS-1\u662f\u4e00\u79cd\u65b0\u578b\u4e8c\u8fdb\u5236\u9065\u6d4b\u683c\u5f0f\uff0c\u4e13\u4e3a\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u73b0\u6709\u683c\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9065\u6d4b\u7f16\u7801\u5982JSON\u3001CBOR\u7b49\u5728\u9ad8\u5e26\u5bbd\u3001\u59cb\u7ec8\u5728\u7ebf\u7684\u73af\u5883\u4e2d\u8bbe\u8ba1\uff0c\u4f46\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u7f51\u7edc\u4e2d\uff08\u5982\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u3001\u519c\u6751\u4f01\u4e1a\u90e8\u7f72\u548c\u4e0d\u7a33\u5b9a\u7684LAN\u73af\u5883\uff09\u4f1a\u4ea7\u751f\u663e\u8457\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83MTS-1\u4e0eJSON\u3001JSON Lines\u3001CBOR\u3001MessagePack\u548cProtocol Buffers\u5728\u6709\u6548\u8f7d\u8377\u5927\u5c0f\u3001\u7f16\u7801\u6210\u672c\u3001\u7f51\u7edc\u6548\u7387\u548c\u6210\u672c\u5ef6\u8fdf\u6027\u80fd\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "MTS-1\u5728\u521d\u6b65\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u538b\u7f29\u4f18\u52bf\uff0c\u5e76\u9002\u7528\u4e8e\u79bb\u7ebf\u4f18\u5148\u7684\u7cfb\u7edf\u76d1\u63a7\u3001LAN\u8f85\u52a9\u4ee3\u7406\u4f20\u8f93\u548c\u8282\u80fd\u7684IoT\u5230\u670d\u52a1\u5668\u4f20\u8f93\u3002", "conclusion": "MTS-1\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4JSON\u548cMessagePack\u5206\u522b\u5b9e\u73b0\u4e8674.7%\u548c5.4%\u7684\u538b\u7f29\u63d0\u5347\uff0c\u4e14\u5177\u6709\u7ebf\u6027\u6269\u5c55\u7279\u6027\u3002"}}
{"id": "2601.01438", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01438", "abs": "https://arxiv.org/abs/2601.01438", "authors": ["Russell Buchanan", "Adrian R\u00f6fer", "Jo\u00e3o Moura", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Online Estimation and Manipulation of Articulated Objects", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Autonomous Robots, and is available online at [Link will be updated when available]", "summary": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u672c\u4f53\u611f\u77e5\u7684\u94f0\u63a5\u7269\u4f53\u5728\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u64cd\u4f5c\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5bf9\u4efb\u610f\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5148\u9a8c\u6216\u64cd\u4f5c\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u56e0\u5b50\u56fe\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u672c\u4f53\u611f\u77e5\uff08\u5982\u8fd0\u52a8\u5b66\u548c\u529b\u4f20\u611f\uff09\uff0c\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u8fdb\u884c\u94f0\u63a5\u4f30\u8ba1\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u673a\u5668\u4eba\u80fd\u6210\u529f\u6253\u5f00\u672a\u89c1\u8fc7\u7684\u62bd\u5c49\uff0c\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u6210\u529f\u738775%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u672c\u4f53\u611f\u77e5\u7684\u5728\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5bf9\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.00869", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00869", "abs": "https://arxiv.org/abs/2601.00869", "authors": ["Huang Junyao", "Situ Ruimin", "Ye Renqin"], "title": "Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery", "comment": "19 pages, 5 tables. Dataset and code available at https://github.com/zhizibianjie-omniedge/geo-cultural-encoding", "summary": "As artificial intelligence systems increasingly mediate consumer information discovery,\n  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large\n  Language Models (LLMs) -- systematic differences in brand recommendations arising from\n  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,\n  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6\n  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,\n  p<.001). This disparity persists in identical English queries, indicating training data\n  geography -- not language -- drives the effect. We introduce the Existence Gap: brands\n  absent from LLM training corpora lack \"existence\" in AI responses regardless of quality.\n  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%\n  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how\n  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we\n  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic\n  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility\n  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization\n  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats\n  through semantic coverage, technical depth, and cultural localization. Our findings reveal\n  that in AI-mediated markets, the limits of a brand's \"Data Boundaries\" define the limits\n  of its \"Market Frontiers.\"", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u54c1\u724c\u5728AI\u63a8\u8350\u4e2d\u7684\u53ef\u89c1\u6027\u9ad8\u5ea6\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u6587\u5316\u7f16\u7801\uff0c\u4e2d\u56fdLLMs\u7684\u54c1\u724c\u63d0\u53ca\u7387\u663e\u8457\u9ad8\u4e8e\u56fd\u9645LLMs\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u62a4\u57ce\u6cb3\u6846\u67b6\u548c\u751f\u6210\u5f15\u64ce\u4f18\u5316\u6218\u7565\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u4e2d\u54c1\u724c\u63a8\u8350\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u5730\u7406\u5206\u5e03\u5bf9\u54c1\u724c\u53ef\u89c1\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e861,909\u4e2a\u7eaf\u82f1\u6587\u67e5\u8be2\uff0c\u8986\u76d66\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001Claude\u3001Gemini\u3001Qwen3\u3001DeepSeek\u3001Doubao\uff09\u548c30\u4e2a\u54c1\u724c\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u5982Zhizibianjie\uff09\u9a8c\u8bc1\u4e86\u8bed\u8a00\u8fb9\u754c\u969c\u788d\u7684\u5f71\u54cd\u3002", "result": "\u4e2d\u56fdLLMs\u7684\u54c1\u724c\u63d0\u53ca\u7387\u6bd4\u56fd\u9645LLMs\u9ad830.6\u4e2a\u767e\u5206\u70b9\uff0888.9% vs. 58.3%\uff09\uff0c\u4e14\u76f8\u540c\u82f1\u6587\u67e5\u8be2\u4e2d\u5dee\u5f02\u4ecd\u5b58\u5728\uff0c\u8868\u660e\u8bad\u7ec3\u6570\u636e\u5730\u7406\u5206\u5e03\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u54c1\u724c\u5728AI\u4e2d\u4ecb\u5e02\u573a\u4e2d\u7684\u53ef\u89c1\u6027\u53d7\u9650\u4e8e\u5176\u8bad\u7ec3\u6570\u636e\u7684\u6587\u5316\u7f16\u7801\uff0c\u63d0\u51fa\u4e86\u2018\u6570\u636e\u62a4\u57ce\u6cb3\u6846\u67b6\u2019\u548c\u2018\u7b97\u6cd5\u65e0\u5904\u4e0d\u5728\u2019\u4f5c\u4e3a\u751f\u6210\u5f15\u64ce\u4f18\u5316\u7684\u6218\u7565\u76ee\u6807\uff0c\u5e76\u4e3a\u54c1\u724c\u63d0\u4f9b\u4e8618\u4e2a\u6708\u7684\u5efa\u8bbe\u8def\u7ebf\u56fe\u3002"}}
{"id": "2601.00925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00925", "abs": "https://arxiv.org/abs/2601.00925", "authors": ["I-Hsien Ting", "Yi-Jun Tseng", "Yu-Sheng Lin"], "title": "Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis", "comment": null, "summary": "Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.\n  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9020\u5f71\u5242CT\u56fe\u50cf\u4e2d\u80ba\u6813\u585e\u7684\u81ea\u52a8\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe85%\uff0c\u4e3a\u80ba\u6813\u585e\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u80ba\u6813\u585e\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u53ef\u663e\u8457\u964d\u4f4e\u6b7b\u4ea1\u7387\u3002\u7136\u800c\uff0c\u9020\u5f71\u5242\u53ef\u80fd\u5bf9\u6162\u6027\u80be\u75c5\u60a3\u8005\u9020\u6210\u6025\u6027\u80be\u635f\u4f24\uff0c\u4e14\u9020\u5f71\u5242\u9700\u8981\u65f6\u95f4\u751f\u6548\uff0c\u53ef\u80fd\u5ef6\u8bef\u6025\u6027\u80ba\u6813\u585e\u60a3\u8005\u7684\u9ec4\u91d1\u6cbb\u7597\u65f6\u95f4\u3002", "method": "\u672c\u7814\u7a76\u91c7\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5bf9\u65e0\u9020\u5f71\u5242\u7684CT\u56fe\u50cf\u8fdb\u884c\u80ba\u6813\u585e\u81ea\u52a8\u5206\u7c7b\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u9020\u5f71\u5242CT\u56fe\u50cf\u7684\u80ba\u6813\u585e\u5206\u7c7b\u4e2d\u8868\u73b0\u663e\u8457\uff0c\u51c6\u786e\u7387\u4e3a85%\uff0cAUC\u4e3a0.84\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u9020\u5f71\u5242CT\u56fe\u50cf\u4e2d\u81ea\u52a8\u5206\u7c7b\u80ba\u6813\u585e\u7684\u53ef\u884c\u6027\uff0c\u51c6\u786e\u7387\u8fbe85%\uff0cAUC\u4e3a0.84\u3002"}}
{"id": "2601.01780", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01780", "abs": "https://arxiv.org/abs/2601.01780", "authors": ["Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan", "Mehdi Keshani", "Abbas Heydarnoori"], "title": "LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment", "comment": null, "summary": "Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.", "AI": {"tldr": "LIA\u5229\u7528LLM\u7684\u9884\u8bad\u7ec3\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u9ad8\u6548\u95ee\u9898\u5206\u914d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u95ee\u9898\u5206\u914d\u5728\u5927\u578b\u5f00\u6e90\u9879\u76ee\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u548c\u6613\u9519\u7684\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9879\u76ee\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u6216\u7a00\u758f\u5608\u6742\u7684\u5173\u7cfb\u4fe1\u606f\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "LIA\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528DeepSeek-R1-Distill-Llama-8B\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u76f4\u63a5\u4ece\u95ee\u9898\u6807\u9898\u548c\u63cf\u8ff0\u751f\u6210\u5f00\u53d1\u8005\u63a8\u8350\u6392\u540d\u3002", "result": "LIA\u5728Hit@1\u6307\u6807\u4e0a\u6bd4\u57fa\u7840\u9884\u8bad\u7ec3\u6a21\u578b\u9ad8\u51fa187.8%\uff0c\u5e76\u4f18\u4e8e\u56db\u79cd\u9886\u5148\u7684\u95ee\u9898\u5206\u914d\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u5347211.2%\u3002", "conclusion": "LIA\uff08\u57fa\u4e8eLLM\u7684\u95ee\u9898\u5206\u914d\u65b9\u6cd5\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u9002\u5e94LLM\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u9898\u5206\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6210\u4e3a\u8f6f\u4ef6\u7ef4\u62a4\u4efb\u52a1\u4e2d\u5b9e\u7528\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01561", "abs": "https://arxiv.org/abs/2601.01561", "authors": ["Yujian Qiu", "Yuqiu Mu", "Wen Yang", "Hao Zhu"], "title": "AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization", "comment": null, "summary": "This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.", "AI": {"tldr": "AIMS\u662f\u4e00\u79cd\u81ea\u9002\u5e94LiDAR-IMU-\u817f\u90e8\u91cc\u7a0b\u8ba1\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u72ed\u7a84\u96a7\u9053\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u72ed\u7a84\u96a7\u9053\u7c7b\u73af\u5883\u4e2d\u56e0LiDAR\u6d4b\u91cf\u51e0\u4f55\u7ea6\u675f\u5f31\u3001\u4f20\u7edf\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u6613\u7d2f\u79ef\u8fd0\u52a8\u4f30\u8ba1\u8bef\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAIMS\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u81ea\u9002\u5e94LiDAR-IMU-\u817f\u90e8\u91cc\u7a0b\u8ba1\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u9000\u5316\u611f\u77e5\u53ef\u9760\u6027\u8bc4\u4f30\u52a8\u6001\u8c03\u6574\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u77e9\u9635\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u72ed\u7a84\u8d70\u5eca\u73af\u5883\u4e2d\uff0cAIMS\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AIMS\u65b9\u6cd5\u5728\u72ed\u7a84\u96a7\u9053\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.00880", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.", "AI": {"tldr": "UCL\u6846\u67b6\u5c06\u63d0\u793a\u5de5\u7a0b\u7cfb\u7edf\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4ee4\u724c\u4f7f\u7528\uff0c\u6a21\u578b\u67b6\u6784\u5dee\u5f02\u5f71\u54cd\u4f18\u5316\u914d\u7f6e\u3002", "motivation": "\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u4f18\u5316\uff0c\u63d0\u51faUniversal Conditional Logic\uff08UCL\uff09\u6570\u5b66\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff08N=305\uff0c11\u4e2a\u6a21\u578b\uff0c4\u6b21\u8fed\u4ee3\uff09\uff0c\u9a8c\u8bc1\u4e86\u6838\u5fc3\u673a\u5236\u2014\u2014\u6307\u793a\u51fd\u6570\u3001\u7ed3\u6784\u5f00\u9500\u548c\u65e9\u671f\u7ed1\u5b9a\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\uff0829.8%\uff09\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5f00\u9500\u51fd\u6570\u89e3\u91ca\u4e86\u7248\u672c\u7279\u5b9a\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "UCL\u88ab\u786e\u7acb\u4e3a\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684LLM\u4ea4\u4e92\uff0c\u6a21\u578b\u5bb6\u65cf\u7279\u5b9a\u4f18\u5316\u6210\u4e3a\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.00928", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00928", "abs": "https://arxiv.org/abs/2601.00928", "authors": ["Luis Yoichi Morales", "Francesco Zanlungo", "David M. Woollard"], "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store", "comment": null, "summary": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u673a\u5668\u89c6\u89c9\u8ffd\u8e2a\u987e\u5ba2\u8f68\u8ff9\u5e76\u8bc6\u522b\u5176\u8d27\u67b6\u6d4f\u89c8\u884c\u4e3a\u7684\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u96f6\u552e\u89c4\u5212\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u96f6\u552e\u4e1a\u4e2d\u673a\u5668\u4eba\u90e8\u7f72\u7684\u6311\u6218\uff0c\u65e8\u5728\u901a\u8fc7\u7406\u89e3\u987e\u5ba2\u7684\u8d2d\u7269\u610f\u56fe\uff0c\u63d0\u5347\u96f6\u552e\u73af\u5883\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u673a\u5668\u89c6\u89c9\u76843D\u8ffd\u8e2a\u548c\u5934\u9876\u6444\u50cf\u5934\u83b7\u53d6\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u8ba1\u7b97\u987e\u5ba2\u7684\u201c\u8d27\u67b6\u8bbf\u95ee\u201d\u884c\u4e3a\u3002\u7b97\u6cd5\u7ecf\u8fc7\u4e24\u7ec4\u72ec\u7acb\u8f68\u8ff9\u6570\u636e\uff088138\u6761\u548c15129\u6761\uff09\u7684\u6821\u51c6\uff0c\u5e76\u5728\u4e0d\u540c\u5546\u5e97\u7684\u8f68\u8ff9\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u4e8e\u6821\u51c6\u73af\u5883\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u987e\u5ba2\u7684\u6d4f\u89c8\u6d3b\u52a8\uff0c\u5e76\u6210\u529f\u5206\u6790\u4e86\u987e\u5ba2\u6d4f\u89c8\u6a21\u5f0f\u4e0e\u5b9e\u9645\u8d2d\u4e70\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7b97\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u8bc6\u522b\u987e\u5ba2\u6d4f\u89c8\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u96f6\u552e\u89c4\u5212\u548c\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u7684\u5e94\u7528\u3002"}}
{"id": "2601.01839", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01839", "abs": "https://arxiv.org/abs/2601.01839", "authors": ["Martin Prause"], "title": "The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation", "comment": "Dataset available: https://ieee-dataport.org/documents/machine-learning-canvas-success-determinants", "summary": "Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (\u03b2= 0.432, p < 0.001), which improves work processes (\u03b2= 0.428, p < 0.001) and builds better infrastructure (\u03b2= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the \"how\" of coding but cannot replace the \"why\" and \"what\" of strategic thinking.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u2018\u673a\u5668\u5b66\u4e60\u753b\u5e03\u2019\u6846\u67b6\uff0c\u63ed\u793a\u6218\u7565\u3001\u6d41\u7a0b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u652f\u6301\u56db\u4e2a\u4e92\u8054\u56e0\u7d20\u4e3aML\u9879\u76ee\u6210\u529f\u5173\u952e\uff0cAI\u52a9\u624b\u867d\u52a0\u901f\u7f16\u7801\u4f46\u65e0\u6cd5\u66ff\u4ee3\u6218\u7565\u601d\u8003\u3002", "motivation": "\u5c3d\u7ba1AI\u7f16\u7801\u52a9\u624b\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u8d85\u8fc780%\u7684\u673a\u5668\u5b66\u4e60\u9879\u76ee\u672a\u80fd\u63d0\u4f9b\u5b9e\u9645\u5546\u4e1a\u4ef7\u503c\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6210\u529f\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u8c03\u67e5150\u540d\u6570\u636e\u79d1\u5b66\u5bb6\u5e76\u91c7\u7528\u7edf\u8ba1\u5efa\u6a21\u5206\u6790\u5176\u53cd\u9988\uff0c\u521b\u5efa\u5e76\u6d4b\u8bd5\u4e86\u2018\u673a\u5668\u5b66\u4e60\u753b\u5e03\u2019\u6846\u67b6\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u5173\u952e\u6210\u529f\u56e0\u7d20\uff1a\u6218\u7565\u3001\u6d41\u7a0b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u652f\u6301\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u76f8\u4e92\u5173\u8054\uff0c\u5171\u540c\u51b3\u5b9a\u9879\u76ee\u6210\u8d25\u3002", "conclusion": "AI\u7f16\u7801\u52a9\u624b\u867d\u7136\u80fd\u52a0\u901f\u7f16\u7801\u8fc7\u7a0b\uff0c\u4f46\u65e0\u6cd5\u66ff\u4ee3\u6218\u7565\u601d\u8003\u7684\u2018\u4e3a\u4ec0\u4e48\u2019\u548c\u2018\u505a\u4ec0\u4e48\u2019\uff0c\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u6218\u7565\u3001\u6d41\u7a0b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u652f\u6301\u56db\u4e2a\u56e0\u7d20\u7684\u6709\u673a\u7ed3\u5408\u3002"}}
{"id": "2601.01577", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01577", "abs": "https://arxiv.org/abs/2601.01577", "authors": ["Tran Tien Dat", "Nguyen Hai An", "Nguyen Khanh Viet Dung", "Nguyen Duy Duc"], "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller", "comment": null, "summary": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines", "AI": {"tldr": "Hanoi-World\u7ed3\u5408JEPA\u4e0eRNN\uff0c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u957f\u671f\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u78b0\u649e\u7387\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u4e2d\u5b58\u5728\u6570\u636e\u9700\u6c42\u9ad8\u3001\u6027\u80fd\u4e0d\u7a33\u5b9a\u3001\u5b89\u5168\u6027\u4e0d\u8db3\u53ca\u8fc7\u5ea6\u5173\u6ce8\u566a\u58f0\u7279\u5f81\u7684\u95ee\u9898\uff0c\u800c\u81ea\u76d1\u7763\u5b66\u4e60\u901a\u8fc7JEPA\u67b6\u6784\u6a21\u62df\u4eba\u8111\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eJEPA\u67b6\u6784\u7684Hanoi-World\u6a21\u578b\uff0c\u5229\u7528RNN\u8fdb\u884c\u957f\u671f\u6c34\u5e73\u89c4\u5212\uff0c\u5e76\u5728Highway-Env\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHanoi-World\u5728\u9ad8\u901f\u516c\u8def\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u4f4e\u78b0\u649e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u57fa\u51c6\u3002", "conclusion": "Hanoi-World\u6a21\u578b\u901a\u8fc7\u7ed3\u5408JEPA\u67b6\u6784\u548cRNN\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u957f\u671f\u89c4\u5212\u4e0e\u5b89\u5168\u610f\u8bc6\u7684\u5e73\u8861\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u78b0\u649e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2601.00885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00885", "abs": "https://arxiv.org/abs/2601.00885", "authors": ["Mandar Parab"], "title": "Counterfactual Self-Questioning for Stable Policy Optimization in Language Models", "comment": null, "summary": "Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.", "AI": {"tldr": "\u63d0\u51fa Counterfactual Self-Questioning \u6846\u67b6\uff0c\u5355\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u751f\u6210\u548c\u8bc4\u4f30\u53cd\u4e8b\u5b9e\u6279\u8bc4\u6539\u8fdb\u63a8\u7406\uff0c\u65e0\u9700\u5916\u90e8\u8f85\u52a9\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u63d0\u5347\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u591a\u4f9d\u8d56\u5916\u90e8\u6279\u8bc4\u8005\u3001\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u6216\u96c6\u6210\u91c7\u6837\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u751f\u6210\u521d\u59cb\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u9488\u5bf9\u6f5c\u5728\u5931\u8d25\u70b9\u63d0\u51fa\u9488\u5bf9\u6027\u95ee\u9898\uff0c\u5e76\u751f\u6210\u66ff\u4ee3\u63a8\u7406\u8f68\u8ff9\u4ee5\u66b4\u9732\u9519\u8bef\u5047\u8bbe\u6216\u65e0\u6548\u6b65\u9aa4\uff0c\u4ece\u800c\u63d0\u4f9b\u7ed3\u6784\u5316\u76f8\u5bf9\u53cd\u9988\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCounterfactual Self-Questioning \u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "Counterfactual Self-Questioning \u6846\u67b6\u901a\u8fc7\u5355\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u8bc4\u4f30\u81ea\u8eab\u63a8\u7406\u7684\u53cd\u4e8b\u5b9e\u6279\u8bc4\uff0c\u65e0\u9700\u5916\u90e8\u6279\u8bc4\u8005\u6216\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u578b\u6a21\u578b\u3002"}}
{"id": "2601.00939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00939", "abs": "https://arxiv.org/abs/2601.00939", "authors": ["Feng Luo", "Hongbo Pan", "Xiang Yang", "Baoyu Jiang", "Fengqing Liu", "Tao Huang"], "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.", "AI": {"tldr": "ShadowGS\u662f\u57fa\u4e8e3DGS\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6e32\u67d3\u548c\u5149\u7ebf\u884c\u8fdb\u6280\u672f\u89e3\u51b3\u591a\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u9634\u5f71\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u53473D\u91cd\u5efa\u7cbe\u5ea6\u548c\u6e32\u67d3\u6548\u7387\u3002", "motivation": "\u591a\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u4e2d\uff0c\u7531\u4e8e\u5149\u7167\u6761\u4ef6\u53d8\u5316\u5bfc\u81f4\u7684\u9634\u5f71\u4e0d\u4e00\u81f4\u95ee\u9898\u663e\u8457\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u5efa\u6a21\u51e0\u4f55\u4e00\u81f4\u9634\u5f71\u5e76\u4fdd\u6301\u9ad8\u6548\u6e32\u67d3\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684ShadowGS\u6846\u67b6\uff0c\u7ed3\u5408\u9065\u611f\u7269\u7406\u6e32\u67d3\u65b9\u7a0b\u548c\u9ad8\u6548\u5149\u7ebf\u884c\u8fdb\u6280\u672f\uff0c\u7cbe\u786e\u5efa\u6a21\u51e0\u4f55\u4e00\u81f4\u7684\u9634\u5f71\uff0c\u5e76\u6709\u6548\u89e3\u8026\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u5149\u7167\u6210\u5206\u548c\u8868\u89c2\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cShadowGS\u5728RGB\u3001\u5168\u8272\u9510\u5316\u548c\u7a00\u758f\u89c6\u56fe\u536b\u661f\u8f93\u5165\u7b49\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "ShadowGS\u5728\u9634\u5f71\u89e3\u8026\u7cbe\u5ea6\u30013D\u91cd\u5efa\u7cbe\u5ea6\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u4ec5\u9700\u51e0\u5206\u949f\u8bad\u7ec3\u65f6\u95f4\uff0c\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.01921", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01921", "abs": "https://arxiv.org/abs/2601.01921", "authors": ["Mikel Robredo", "Matteo Esposito", "Fabio Palomba", "Rafael Pe\u00f1aloza", "Valentina Lenarduzzi"], "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach", "comment": "ACCEPTED REGISTERED REPORT AT SANER (CORE A*) 2026", "summary": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.\n  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.\n  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.\n  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u65f6\u95f4\u654f\u611f\u6280\u672f\u5728\u7f3a\u9677\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc6\u522b\u65e9\u671f\u6307\u6807\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6301\u7eed\u6f14\u8fdb\uff0c\u9700\u8981\u80fd\u591f\u9884\u6d4b\u7f3a\u9677\u7684\u65f6\u95f4\u654f\u611f\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u591a\u79cd\u65f6\u95f4\u654f\u611f\u9884\u6d4b\u6280\u672f\uff0c\u9884\u6d4b\u8f6f\u4ef6\u9879\u76ee\u7684\u672a\u6765\u7f3a\u9677\u5bc6\u5ea6\uff0c\u5e76\u8bc6\u522b\u7f3a\u9677\u51fa\u73b0\u7684\u65e9\u671f\u75c7\u72b6\u3002", "result": "\u9884\u671f\u7ed3\u679c\u4e3a\u65e9\u671f\u4f30\u8ba1\u7f3a\u9677\u503e\u5411\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u63d0\u4f9b\u4e86\u65f6\u95f4\u654f\u611f\u6280\u672f\u7684\u6709\u6548\u6027\u8bc1\u636e\uff0c\u5e76\u8bc6\u522b\u4e86\u7f3a\u9677\u51fa\u73b0\u7684\u65e9\u671f\u6307\u6807\u3002"}}
{"id": "2601.01618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01618", "abs": "https://arxiv.org/abs/2601.01618", "authors": ["Huajie Tan", "Peterson Co", "Yijie Xu", "Shanyu Rong", "Yuheng Ji", "Cheng Chi", "Xiansheng Chen", "Qiongyu Zhang", "Zhongxia Zhao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation", "comment": "26 pages, 14 figures", "summary": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io", "AI": {"tldr": "Action-Sketcher\u901a\u8fc7\u89c6\u89c9\u8349\u56fe\u5916\u90e8\u5316\u7a7a\u95f4\u610f\u56fe\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5de5\u4f5c\u6d41\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u548c\u5206\u5c42\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7b56\u7565\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\u4e14\u610f\u56fe\u9690\u542b\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u4e2d\u53c2\u8003\u57fa\u7840\u8584\u5f31\u3001\u4efb\u52a1\u5206\u89e3\u4f4e\u6548\uff0c\u4e14\u52a8\u4f5c\u9009\u62e9\u7f3a\u4e4f\u56e0\u679c\u89e3\u91ca\u3002", "method": "\u63d0\u51faVisual Sketch\u4f5c\u4e3a\u89c6\u89c9\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaAction-Sketcher\u6846\u67b6\uff0c\u91c7\u7528See-Think-Sketch-Act\u5faa\u73af\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cAction-Sketcher\u5728\u6742\u4e71\u573a\u666f\u548c\u591a\u5bf9\u8c61\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u957f\u65f6\u7a0b\u6210\u529f\u7387\u3001\u66f4\u5f3a\u7684\u52a8\u6001\u573a\u666f\u53d8\u5316\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u53ef\u7f16\u8f91\u8349\u56fe\u548c\u5206\u6b65\u8ba1\u5212\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Action-Sketcher\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8349\u56fe\uff08Visual Sketch\uff09\u5916\u90e8\u5316\u7a7a\u95f4\u610f\u56fe\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u4ee4\u724c\u95e8\u63a7\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.00923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00923", "abs": "https://arxiv.org/abs/2601.00923", "authors": ["Josef Ott"], "title": "Context Collapse: In-Context Learning and Model Collapse", "comment": "Master's thesis", "summary": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76LLM\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\uff0c\u63ed\u793aICL\u7684\u76f8\u53d8\u548c\u68af\u5ea6\u65cb\u8f6c\u673a\u5236\uff0c\u8bc1\u660e\u6a21\u578b\u5d29\u6e83\u7684\u5fc5\u7136\u6027\uff0c\u5e76\u63d0\u51fa\u4e0a\u4e0b\u6587\u5d29\u6e83\u6982\u5ff5\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\u7684\u673a\u5236\u53ca\u5176\u5bf9\u751f\u6210\u6a21\u578b\u957f\u671f\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u53d8\u6362\u5668\u548c\u6743\u91cd\u7ed1\u5b9a\u6280\u672f\u7814\u7a76ICL\uff0c\u901a\u8fc7\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u5206\u6790\u6700\u4f18\u9884\u6761\u4ef6\u5668\uff1b\u5229\u7528\u9785\u548c\u968f\u673a\u6e38\u8d70\u7406\u8bba\u5206\u6790\u7ebf\u6027\u56de\u5f52\u548c\u9ad8\u65af\u62df\u5408\u4e0b\u7684\u6a21\u578b\u5d29\u6e83\u3002", "result": "\u53d1\u73b0ICL\u4e2d\u53c2\u6570\u76f8\u53d8\u548c\u68af\u5ea6\u65b9\u5411\u65cb\u8f6c\u73b0\u8c61\uff0c\u8bc1\u660e\u6a21\u578b\u5d29\u6e83\u51e0\u4e4e\u5fc5\u7136\u53d1\u751f\uff0c\u9664\u975e\u6570\u636e\u589e\u957f\u8db3\u591f\u5feb\u6216\u957f\u671f\u4fdd\u7559\uff0c\u5e76\u63d0\u51fa\u4e0a\u4e0b\u6587\u5d29\u6e83\u6982\u5ff5\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5668\u548c\u7b80\u5316\u8bbe\u7f6e\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u53c2\u6570\u76f8\u53d8\u548c\u68af\u5ea6\u65b9\u5411\u65cb\u8f6c\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u7684\u65b0\u6982\u5ff5\uff0c\u5c06ICL\u52a8\u6001\u4e0e\u751f\u6210\u6a21\u578b\u7684\u957f\u671f\u7a33\u5b9a\u6027\u6311\u6218\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2601.00940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00940", "abs": "https://arxiv.org/abs/2601.00940", "authors": ["Jonas Li", "Michelle Li", "Luke Liu", "Heng Fan"], "title": "Learning to Segment Liquids in Real-world Images", "comment": "9 pages, 7 figures", "summary": "Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.", "AI": {"tldr": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6db2\u4f53\u6570\u636e\u96c6LQDS\uff0c\u5e76\u63d0\u51faLQDM\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6db2\u4f53\u5206\u5272\u6548\u679c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u6db2\u4f53\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u904d\u5b58\u5728\u4e0e\u673a\u5668\u4eba\u5b89\u5168\u4ea4\u4e92\u7684\u9700\u6c42\uff0c\u4ee5\u53ca\u6db2\u4f53\u5916\u89c2\u591a\u6837\u6027\u5e26\u6765\u7684\u5206\u5272\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aLQDM\u7684\u65b0\u578b\u6db2\u4f53\u68c0\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u4e13\u7528\u8fb9\u754c\u5206\u652f\u4e0e\u4e3b\u5206\u5272\u5206\u652f\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u5206\u5272\u9884\u6d4b\u3002", "result": "LQDM\u5728LQDS\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LQDM\u6a21\u578b\u5728LQDS\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u6db2\u4f53\u8bed\u4e49\u5206\u5272\u5efa\u7acb\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u51c6\u3002"}}
{"id": "2601.01944", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.01944", "abs": "https://arxiv.org/abs/2601.01944", "authors": ["Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities", "comment": "ACCEPTED REGISTERED REPORT AT SANER (CORE A*) 2026", "summary": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.\n  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790AI\u5e93\u5728Python\u548cJava\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u91c7\u7528\u53ca\u5176\u5bf9\u5f00\u53d1\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83\u63ed\u793a\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u5f00\u6e90\u9879\u76ee\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u91c7\u7528\u548c\u5bf9\u9879\u76ee\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5bf9157.7k\u4e2a\u6f5c\u5728\u5f00\u6e90\u4ed3\u5e93\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\uff0c\u91c7\u7528\u4ed3\u5e93\u6307\u6807\u548c\u8f6f\u4ef6\u6307\u6807\uff0c\u6bd4\u8f83\u91c7\u7528AI\u5e93\u4e0e\u672a\u91c7\u7528AI\u5e93\u7684\u9879\u76ee\u3002", "result": "\u9884\u8ba1\u5c06\u8bc6\u522b\u91c7\u7528AI\u5e93\u4e0e\u672a\u91c7\u7528AI\u5e93\u7684\u5f00\u6e90\u9879\u76ee\u5728\u5f00\u53d1\u6d3b\u52a8\u3001\u793e\u533a\u53c2\u4e0e\u548c\u4ee3\u7801\u590d\u6742\u6027\u65b9\u9762\u7684\u53ef\u6d4b\u91cf\u5dee\u5f02\u3002", "conclusion": "\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u5206\u6790\u63ed\u793aAI\u5e93\u5728Python\u548cJava\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u91c7\u7528\u60c5\u51b5\u53ca\u5176\u5bf9\u5f00\u53d1\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u4e3aAI\u96c6\u6210\u5982\u4f55\u91cd\u5851\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.01651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01651", "abs": "https://arxiv.org/abs/2601.01651", "authors": ["Yucheng Xu", "Xiaofeng Mao", "Elle Miller", "Xinyu Yi", "Yang Li", "Zhibin Li", "Robert B. Fisher"], "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos", "comment": null, "summary": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.", "AI": {"tldr": "DemoBot\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9\u673a\u5668\u4eba\u4ece\u5355\u4e00\u672a\u6807\u6ce8\u89c6\u9891\u4e2d\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u6280\u80fd\uff0c\u6210\u529f\u5b8c\u6210\u53cc\u624b\u88c5\u914d\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u4e00\u672a\u6807\u6ce8\u89c6\u9891\u4e2d\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u6311\u6218\uff0c\u907f\u514d\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece\u539f\u59cb\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u53cc\u624b\u548c\u7269\u4f53\u7684\u7ed3\u6784\u5316\u8fd0\u52a8\u8f68\u8ff9\uff0c\u4f5c\u4e3a\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ba1\u9053\u8fdb\u884c\u7ec6\u5316\u3002\u5177\u4f53\u6280\u672f\u5305\u62ec\u57fa\u4e8e\u65f6\u95f4\u6bb5\u7684RL\u3001\u6210\u529f\u95e8\u63a7\u91cd\u7f6e\u7b56\u7565\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u5956\u52b1\u8bfe\u7a0b\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u540c\u6b65\u548c\u5f02\u6b65\u53cc\u624b\u88c5\u914d\u4efb\u52a1\u3002", "conclusion": "DemoBot\u6846\u67b6\u901a\u8fc7\u4ece\u5355\u4e00\u672a\u6807\u6ce8RGB-D\u89c6\u9891\u4e2d\u5b66\u4e60\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u53cc\u81c2\u591a\u6307\u673a\u5668\u4eba\u7cfb\u7edf\u5bf9\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u83b7\u53d6\uff0c\u4e3a\u4ece\u4eba\u7c7b\u89c6\u9891\u76f4\u63a5\u83b7\u53d6\u6280\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.00994", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit\u662f\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u4e92\u52a8\u7684\u6846\u67b6\uff0c\u7814\u7a76LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\uff0c\u53d1\u73b025\u79cd\u8bf4\u670d\u6280\u672f\u4f7f\u7528\u5e7f\u6cdb\uff0c\u6a21\u578b\u5dee\u5f02\u5f71\u54cd\u663e\u8457\uff0c\u5e76\u89c2\u5bdf\u5230\u72ec\u7279\u73b0\u8c61\u3002", "motivation": "\u65e8\u5728\u514b\u670d\u4ee5\u5f80\u7814\u7a76\u4e2d\u57fa\u4e8e\u6e38\u620f\u7684\u6a21\u62df\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u66f4\u771f\u5b9e\u7684\u73af\u5883\u7814\u7a76\u8bf4\u670d\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7684\u4e92\u52a8\uff0c\u7279\u522b\u662f\u5728\u653f\u6cbb\u9009\u4e3e\u671f\u95f4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aElecTwit\u7684\u4eff\u771f\u6846\u67b6\u3002", "result": "\u89c2\u5bdf\u523025\u79cd\u7279\u5b9a\u8bf4\u670d\u6280\u672f\u5728\u5927\u591a\u6570\u6d4b\u8bd5\u7684LLM\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8303\u56f4\u8d85\u8fc7\u4ee5\u5f80\u62a5\u544a\u3002\u4e0d\u540c\u6a21\u578b\u5728\u6280\u672f\u4f7f\u7528\u548c\u6574\u4f53\u8bf4\u670d\u8f93\u51fa\u4e0a\u7684\u5dee\u5f02\u7a81\u663e\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5bf9\u73b0\u5b9e\u793e\u4ea4\u6a21\u62df\u52a8\u6001\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u72ec\u7279\u73b0\u8c61\uff0c\u5982\u201c\u771f\u76f8\u5185\u6838\u201d\u4fe1\u606f\u548c\u81ea\u53d1\u7684\u201c\u58a8\u6c34\u201d\u8ff7\u604b\u73b0\u8c61\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u4e2d\u5177\u6709\u8bf4\u670d\u529b\u7684LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u786e\u4fdd\u5176\u4e00\u81f4\u6027\u5e76\u9884\u9632\u6f5c\u5728\u5371\u9669\u7ed3\u679c\u3002"}}
{"id": "2601.00943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00943", "abs": "https://arxiv.org/abs/2601.00943", "authors": ["Megha Mariam K. M", "Aditya Arun", "Zakaria Laskar", "C. V. Jawahar"], "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education", "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30T2V\u6a21\u578b\u751f\u6210\u7269\u7406\u6559\u5b66\u89c6\u9891\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u89c6\u89c9\u8d28\u91cf\u9ad8\u4f46\u6982\u5ff5\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u62bd\u8c61\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u65e8\u5728\u63a8\u52a8AI\u751f\u6210\u51c6\u786e\u7684\u6559\u5b66\u5185\u5bb9\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662fT2V\u7cfb\u7edf\uff09\u5728\u7269\u7406\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u76f4\u89c2\u7684\u6559\u5b66\u89c6\u9891\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u3001\u6613\u83b7\u53d6\u548c\u4e2a\u6027\u5316\u7684AI\u9a71\u52a8\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30T2V\u6a21\u578b\u751f\u6210\u7269\u7406\u6559\u5b66\u89c6\u9891\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u7269\u7406\u6982\u5ff5\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u6559\u5b66\u70b9\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u70b9\u8bbe\u8ba1\u63d0\u793a\u8bcd\u4ee5\u751f\u6210\u89c6\u89c9\u89e3\u91ca\u3002", "result": "\u5f53\u524d\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u3001\u8fd0\u52a8\u6d41\u7545\u7684\u89c6\u9891\uff0c\u4f46\u5728\u6982\u5ff5\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u529b\u5b66\u3001\u6d41\u4f53\u548c\u5149\u5b66\u9886\u57df\u8868\u73b0\u8f83\u597d\uff0c\u7535\u78c1\u5b66\u548c\u70ed\u529b\u5b66\u5219\u8f83\u5dee\u3002", "conclusion": "\u5f53\u524dT2V\u6a21\u578b\u5728\u751f\u6210\u6559\u80b2\u89c6\u9891\u65f6\uff0c\u89c6\u89c9\u8d28\u91cf\u8f83\u9ad8\u4f46\u6982\u5ff5\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u7535\u78c1\u5b66\u548c\u70ed\u529b\u5b66\u7b49\u62bd\u8c61\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002\u5e0c\u671b\u8be5\u57fa\u51c6\u6d4b\u8bd5\u80fd\u5e2e\u52a9\u793e\u533a\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63a8\u52a8T2V\u7cfb\u7edf\u751f\u6210\u51c6\u786e\u4e14\u7b26\u5408\u8bfe\u7a0b\u8981\u6c42\u7684\u7269\u7406\u6559\u5b66\u5185\u5bb9\u3002"}}
{"id": "2601.01952", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01952", "abs": "https://arxiv.org/abs/2601.01952", "authors": ["Max Unterbusch", "Andreas Vogelsang"], "title": "Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration", "comment": "Accepted at ICSE-NIER 2026", "summary": "Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a \"defect\" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.", "AI": {"tldr": "HLC\u901a\u8fc7LLM\u94fe\u5f0f\u63a8\u7406\u548c\u7528\u6237\u53cd\u9988\u5b9e\u73b0\u9700\u6c42\u7f3a\u9677\u7684\u81ea\u9002\u5e94\u9884\u6d4b\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\u5373\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9700\u6c42\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u901a\u7528\u6a21\u5f0f\uff0c\u4f46\u7f3a\u9677\u5b9a\u4e49\u5177\u6709\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9700\u9002\u5e94\u4e0d\u540c\u9879\u76ee\u548c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faHuman-LLM Collaboration (HLC)\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u7528\u6237\u9a8c\u8bc1\u53cd\u9988\u5faa\u73af\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u81ea\u9002\u5e94\u8c03\u6574\u9884\u6d4b\u3002", "result": "\u5728QuRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHLC\u4ec5\u970020\u4e2a\u9a8c\u8bc1\u6837\u672c\u5373\u53ef\u5feb\u901f\u63d0\u5347\u6027\u80fd\uff0c\u7ed3\u5408\u89e3\u91ca\u7684\u9a8c\u8bc1\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "HLC\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408LLM\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u7528\u6237\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9700\u6c42\u7f3a\u9677\u7684\u81ea\u9002\u5e94\u9884\u6d4b\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.01675", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01675", "abs": "https://arxiv.org/abs/2601.01675", "authors": ["Snehal s. Dikhale", "Karankumar Patel", "Daksh Dhingra", "Itoshi Naramura", "Akinobu Hayashi", "Soshi Iba", "Nawid Jamali"], "title": "VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data", "comment": "Accepted for publication in IEEE Robotics and Automation Letters (RA-L), January 2022. Presented at ICRA 2022. This is the author's version of the manuscript", "summary": "Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89e6\u89c9\u548c\u89c6\u89c9\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u4e91\u8868\u793a\u548c\u5bc6\u96c6\u878d\u5408\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e866D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u4ece\u5408\u6210\u6570\u636e\u6cdb\u5316\u5230\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5939\u6301\u5668\u5bfc\u81f4\u7684\u4e25\u91cd\u906e\u6321\u95ee\u9898\uff0c\u4ee5\u53ca\u89e6\u89c9\u6570\u636e\u7f3a\u4e4f\u6807\u51c6\u8868\u793a\u548c\u4f20\u611f\u5668\u878d\u5408\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u8868\u793a\u89e6\u89c9\u6570\u636e\u7684\u7269\u4f53\u8868\u9762\u63a5\u89e6\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u50cf\u7d20\u7ea7\u5bc6\u96c6\u878d\u5408\u7684\u7f51\u7edc\u67b6\u6784\u3002\u540c\u65f6\u6269\u5c55\u4e86NVIDIA\u7684\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\u5408\u6210\u5668\u4ee5\u751f\u6210\u5408\u6210\u89c6\u89c9\u6570\u636e\u548c\u5bf9\u5e94\u7684\u89e6\u89c9\u70b9\u4e91\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u89e6\u89c9\u6570\u636e\u663e\u8457\u6539\u5584\u4e866D\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e14\u7f51\u7edc\u80fd\u591f\u4ece\u5408\u6210\u6570\u636e\u6cdb\u5316\u5230\u5b9e\u9645\u673a\u5668\u4eba\u3002", "conclusion": "\u7ed3\u5408\u89e6\u89c9\u548c\u89c6\u89c9\u6570\u636e\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e866D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e14\u7f51\u7edc\u80fd\u591f\u6210\u529f\u4ece\u5408\u6210\u8bad\u7ec3\u6cdb\u5316\u5230\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2601.01195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01195", "abs": "https://arxiv.org/abs/2601.01195", "authors": ["Wuzhenghong Wen", "Chao Xue", "Su Pan", "Yuwei Sun", "Minlong Peng"], "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering", "comment": "11 pages, 2 figures", "summary": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.", "AI": {"tldr": "MRE\u6846\u67b6\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548cT-GRPO\u4f18\u5316\u591a\u8df3\u63a8\u7406\uff0c\u63d0\u5347TKGQA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3TKGQA\u4e2d\u591a\u8df3\u63a8\u7406\u65f6\u56e0\u65f6\u95f4\u76f8\u4f3c\u548c\u8bed\u4e49\u590d\u6742\u5173\u7cfb\u5bfc\u81f4\u7684\u5b50\u56fe\u68c0\u7d22\u95ee\u9898\uff0c\u51cf\u5c11\u6b21\u4f18\u51b3\u7b56\u548c\u9519\u8bef\u4f20\u64ad\u3002", "method": "\u63d0\u51faMRE\u6846\u67b6\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u3001\u76d1\u7763\u5fae\u8c03\u4f5c\u4e3a\u51b7\u542f\u52a8\u7b56\u7565\uff0c\u4ee5\u53caT-GRPO\uff08\u6811\u5f62\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u9012\u5f52\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u4e24\u4e2aTKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMRE\u6a21\u578b\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u5904\u7406\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u566a\u58f0\u65f6\u95f4\u6807\u6ce8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MRE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\uff0c\u7ed3\u5408T-GRPO\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86TKGQA\u4efb\u52a1\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u590d\u6742\u67e5\u8be2\u5904\u7406\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002"}}
{"id": "2601.00963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00963", "abs": "https://arxiv.org/abs/2601.00963", "authors": ["Bishwajit Saha", "Dmitry Krotov", "Mohammed J. Zaki", "Parikshit Ram"], "title": "Deep Clustering with Associative Memories", "comment": null, "summary": "Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).", "AI": {"tldr": "DCAM\u901a\u8fc7\u80fd\u91cf\u57fa\u52a8\u529b\u5b66\u548c\u5173\u8054\u8bb0\u5fc6\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u7edf\u4e00\u5728\u4e00\u4e2a\u76ee\u6807\u4e2d\uff0c\u63d0\u5347\u4e86\u805a\u7c7b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u4e2d\uff0c\u8868\u793a\u5b66\u4e60\u662f\u53ef\u5fae\u5206\u7684\uff0c\u800c\u805a\u7c7b\u662f\u4e00\u4e2a\u79bb\u6563\u4f18\u5316\u4efb\u52a1\uff0c\u9700\u8981\u5404\u79cd\u8fd1\u4f3c\u548c\u6b63\u5219\u5316\u6765\u9002\u5e94\u6807\u51c6\u7684\u53ef\u5fae\u5206\u6d41\u7a0b\uff0c\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u4e4b\u95f4\u5b58\u5728\u5272\u88c2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u80fd\u91cf\u57fa\u52a8\u529b\u5b66\u548c\u5173\u8054\u8bb0\u5fc6\uff08Associative Memories\uff09\u6765\u6784\u5efa\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5DCAM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCAM\u5728\u5404\u79cd\u67b6\u6784\u9009\u62e9\uff08\u5377\u79ef\u3001\u6b8b\u5dee\u6216\u5168\u8fde\u63a5\uff09\u548c\u6570\u636e\u6a21\u6001\uff08\u56fe\u50cf\u6216\u6587\u672c\uff09\u4e0b\u5747\u80fd\u63d0\u5347\u805a\u7c7b\u8d28\u91cf\u3002", "conclusion": "DCAM\u65b9\u6cd5\u901a\u8fc7\u80fd\u91cf\u57fa\u52a8\u529b\u5b66\u548c\u5173\u8054\u8bb0\u5fc6\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u5728\u4e00\u4e2a\u5355\u4e00\u76ee\u6807\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u8d28\u91cf\u3002"}}
{"id": "2601.01954", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01954", "abs": "https://arxiv.org/abs/2601.01954", "authors": ["Alexander Korn", "Lea Zaruchas", "Chetan Arora", "Andreas Metzger", "Sven Smolka", "Fanyu Wang", "Andreas Vogelsang"], "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations", "comment": "To be published at The 3rd ACM International Conference on AI Foundation Models and Software Engineering FORGE 2026", "summary": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u63d0\u793a\u62a5\u544a\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u7ed3\u6784\u5316\u6307\u5357\u4ee5\u6539\u5584\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u7531\u4e8e\u63d0\u793a\u76f8\u5173\u51b3\u7b56\u5728SE\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u900f\u660e\u6027\u7684\u8bb0\u5f55\uff0c\u5f71\u54cd\u4e86\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6bd4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u5206\u6790\u4e86\u8fd1300\u7bc7\u53d1\u8868\u5728\u9876\u7ea7SE\u4f1a\u8bae\u4e0a\u7684\u8bba\u6587\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u63d0\u793a\u8bbe\u8ba1\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u7684\u62a5\u544a\u60c5\u51b5\uff1b\u5176\u6b21\u8c03\u67e5\u4e86105\u4f4d\u7a0b\u5e8f\u59d4\u5458\u4f1a\u6210\u5458\uff0c\u4e86\u89e3\u4ed6\u4eec\u5bf9LLM\u9a71\u52a8\u7814\u7a76\u4e2d\u63d0\u793a\u62a5\u544a\u7684\u671f\u671b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5b9e\u8df5\u4e0e\u8bc4\u5ba1\u8005\u671f\u671b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u7248\u672c\u62ab\u9732\u3001\u63d0\u793a\u7406\u7531\u548c\u6709\u6548\u6027\u5a01\u80c1\u65b9\u9762\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6307\u5357\uff0c\u65e8\u5728\u63d0\u9ad8\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u65b9\u6cd5\u4e25\u8c28\u6027\u3002"}}
{"id": "2601.01705", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01705", "abs": "https://arxiv.org/abs/2601.01705", "authors": ["Kenneth Kwok", "Basura Fernando", "Qianli Xu", "Vigneshwaran Subbaraju", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Explicit World Models for Reliable Human-Robot Collaboration", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u901a\u8fc7\u6784\u5efa\u2018\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u2019\u6765\u5bf9\u9f50AI\u4e0e\u4eba\u7c7b\u671f\u671b\uff0c\u5f3a\u8c03\u52a8\u6001\u4ea4\u4e92\u800c\u975e\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5177\u4f53\u5316AI\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u5145\u6ee1\u4f20\u611f\u566a\u58f0\u3001\u6a21\u7cca\u6307\u4ee4\u548c\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0\u5177\u4f53\u5316AI\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f3a\u8c03\u52a8\u6001\u3001\u6a21\u7cca\u548c\u4e3b\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u7279\u6027\uff0c\u800c\u975e\u4f20\u7edf\u7684\u6a21\u578b\u53ef\u9884\u6d4b\u6027\u548c\u9c81\u68d2\u6027\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u2018\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u2019\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u53d8\u7684\u793e\u4f1a\u5316\u3001\u591a\u6a21\u6001\u548c\u6d41\u52a8\u7684\u4eba\u7c7b\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684AI\u4ea4\u4e92\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6784\u5efa\u548c\u66f4\u65b0\u2018\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u2019\u4e3a\u6838\u5fc3\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u9760\u7684\u5177\u4f53\u5316AI\uff0c\u901a\u8fc7\u8fd9\u4e00\u6a21\u578b\u6765\u5bf9\u9f50\u673a\u5668\u884c\u4e3a\u4e0e\u4eba\u7c7b\u671f\u671b\u3002"}}
{"id": "2601.01301", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01301", "abs": "https://arxiv.org/abs/2601.01301", "authors": ["Keith Frankston", "Benjamin Howard"], "title": "Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies", "comment": "11 pages; an efficient implementation is available at https://github.com/bhoward73/rmcts", "summary": "We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, \"RMCTS\". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.\n  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in \"Monte--Carlo tree search as regularized policy optimization\" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.\n  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.", "AI": {"tldr": "RMCTS\u662f\u4e00\u79cd\u5feb\u901f\u9012\u5f52\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u901f\u5ea6\u663e\u8457\u4f18\u4e8eMCTS-UCB\uff0c\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3AlphaZero\u7684MCTS-UCB\u7b97\u6cd5\u901f\u5ea6\u8f83\u6162\u7684\u95ee\u9898\uff0c\u63d0\u5347\u641c\u7d22\u6548\u7387\u3002", "method": "\u91c7\u7528\u9012\u5f52\u7684AlphaZero\u98ce\u683c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff08RMCTS\uff09\uff0c\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u5927\u6279\u91cf\u7f51\u7edc\u63a8\u65ad\u51cf\u5c11GPU\u5ef6\u8fdf\u3002", "result": "RMCTS\u5728\u5355\u6839\u72b6\u6001\u641c\u7d22\u65f6\u6bd4MCTS-UCB\u5feb40\u500d\uff0c\u5927\u6279\u91cf\u641c\u7d22\u65f6\u5feb3\u500d\uff0c\u4e14\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u81f3\u4e09\u5206\u4e4b\u4e00\u3002", "conclusion": "RMCTS\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u6027\u80fd\u4e0a\u4e0eMCTS-UCB\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002"}}
{"id": "2601.00964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00964", "abs": "https://arxiv.org/abs/2601.00964", "authors": ["Md. Maksudul Haque", "Rahnuma Akter", "A S M Ahsanul Sarkar Akib", "Abdul Hasib"], "title": "A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI", "comment": null, "summary": "Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\\% and micro-average AUC of 99.33\\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u589e\u5f3a\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff0c\u5728\u4e03\u7c7b\u75c5\u53d8\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u5168\u7403\u5e38\u89c1\u4e14\u5371\u9669\u7684\u764c\u75c7\u7c7b\u578b\uff0c\u9700\u8981\u53ca\u65f6\u51c6\u786e\u7684\u8bca\u65ad\u3002", "method": "\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u3001\u5927\u89c4\u6a21\u6570\u636e\u589e\u5f3a\u3001\u6df7\u5408EfficientNetV2-L\u6846\u67b6\u4e0e\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.15%\u7684\u603b\u51c6\u786e\u7387\u300185.45%\u7684\u5b8fF1\u5206\u6570\u548c99.33%\u7684\u5fae\u5e73\u5747AUC\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u523091.15%\uff0c\u5c24\u5176\u5728\u9ed1\u8272\u7d20\u7624\u548c\u9ed1\u8272\u7d20\u7ec6\u80de\u75e3\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2601.02066", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02066", "abs": "https://arxiv.org/abs/2601.02066", "authors": ["Al Muttakin", "Saikat Mondal", "Chanchal Roy"], "title": "The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts", "comment": "To appear in Proc. IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, 12-18 Apr 2026", "summary": "Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\\% of the 100 evaluated artifacts were executable, of which 32.5\\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\\% (7 out of 40) required low effort, while 82.5\\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86100\u4e2aSE\u590d\u5236\u5305\uff0c\u53d1\u73b0\u4ec540%\u53ef\u6267\u884c\uff0c35%\u91cd\u73b0\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u6307\u5357\u3002", "motivation": "\u586b\u8865\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u590d\u5236\u5305\u53ef\u6267\u884c\u6027\u548c\u53ef\u91cd\u73b0\u6027\u7efc\u5408\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u8fc7\u53bb\u5341\u5e74ICSE\u4f1a\u8bae\u4e2d\u53d1\u8868\u7684100\u4e2a\u590d\u5236\u5305\uff0c\u5305\u62ec\u53ef\u6267\u884c\u6027\u3001\u6240\u9700\u4fee\u6539\u3001\u6267\u884c\u6311\u6218\u53ca\u7ed3\u679c\u91cd\u73b0\u6027\u3002", "result": "\u4ec540%\u7684\u5de5\u4ef6\u53ef\u6267\u884c\uff0c\u5176\u4e2d32.5%\u65e0\u9700\u4fee\u6539\u5373\u53ef\u8fd0\u884c\uff1b35%\u7684\u53ef\u6267\u884c\u5de5\u4ef6\u91cd\u73b0\u4e86\u539f\u59cb\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u590d\u5236\u5305\u7684\u53ef\u7528\u6027\u3001\u53ef\u6267\u884c\u6027\u548c\u53ef\u91cd\u73b0\u6027\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u6761\u53ef\u64cd\u4f5c\u7684\u6307\u5357\u4ee5\u6539\u8fdb\u7814\u7a76\u5de5\u4ef6\u7684\u51c6\u5907\u3001\u6587\u6863\u548c\u5ba1\u67e5\u3002"}}
{"id": "2601.01726", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01726", "abs": "https://arxiv.org/abs/2601.01726", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions", "comment": "10 pages, 7 figures", "summary": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4e0eMRI\u626b\u63cf\u4eea\u96c6\u6210\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u5185\u4ecb\u5165\uff0c\u63d0\u5347\u4e86\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u589e\u5f3aMRI\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u8f85\u52a9\u8bbe\u5907\u5f15\u5bfc\u8840\u7ba1\u5185\u4ecb\u5165\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5305\u62ec\u8ba1\u7b97\u5355\u5143\u548c\u7528\u6237\u754c\u9762\uff0c\u7528\u4e8e\u4e0eMRI\u626b\u63cf\u4eea\u65e0\u7f1d\u96c6\u6210\u3002\u8be5\u7cfb\u7edf\u5904\u7406MR\u56fe\u50cf\u4ee5\u63cf\u7ed8\u8840\u7ba1\u7f51\u7edc\uff0c\u5efa\u7acb\u865a\u62df\u8def\u5f84\u548c\u8fb9\u754c\u4ee5\u9632\u6b62\u624b\u672f\u635f\u4f24\u3002", "result": "\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u548c\u5b89\u5168\u89c4\u8303\u521b\u5efa\u5b9a\u5236\u7684\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u8840\u6d41\u7279\u6027\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u5bfc\u822a\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u7684\u5efa\u6a21\u65b9\u9762\u8bc4\u4f30\u4e86\u9884\u8bbe\u8840\u7ba1\u8def\u5f84\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u57fa\u4e8eQt\u6846\u67b6\u548cC/C++\u5f00\u53d1\uff0c\u7ed3\u5408\u4e13\u7528\u8f6f\u4ef6\u6a21\u5757\uff0c\u4ee3\u8868\u4e86\u6210\u50cf\u6280\u672f\u4e0e\u673a\u5668\u4eba\u8f85\u52a9\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8840\u7ba1\u5185\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.01321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01321", "abs": "https://arxiv.org/abs/2601.01321", "authors": ["Rong Zhou", "Dongping Chen", "Zihan Jia", "Yao Su", "Yixin Liu", "Yiwen Lu", "Dongwei Shi", "Yue Huang", "Tianyang Xu", "Yi Pan", "Xinliang Li", "Yohannes Abate", "Qingyu Chen", "Zhengzhong Tu", "Yu Yang", "Yu Zhang", "Qingsong Wen", "Gengchen Mai", "Sunyang Fu", "Jiachen Li", "Xuyu Wang", "Ziran Wang", "Jing Huang", "Tianming Liu", "Yong Chen", "Lichao Sun", "Lifang He"], "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models", "comment": null, "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u9636\u6bb5\u6846\u67b6\uff0c\u7cfb\u7edf\u5316AI\u5728\u6570\u5b57\u5b6a\u751f\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\uff0c\u5f3a\u8c03\u7269\u7406\u5efa\u6a21\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u534f\u540c\uff0c\u5e76\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5982\u4f55\u63d0\u5347\u6570\u5b57\u5b6a\u751f\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u4f5c\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u7cbe\u786e\u6570\u5b57\u8868\u793a\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u96c6\u6210\uff0c\u5df2\u4ece\u88ab\u52a8\u4eff\u771f\u5de5\u5177\u6f14\u53d8\u4e3a\u667a\u80fd\u81ea\u4e3b\u5b9e\u4f53\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63cf\u8ff0AI\u5728\u6570\u5b57\u5b6a\u751f\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u73b0\u6709\u6280\u672f\u548c\u5b9e\u8df5\uff0c\u63d0\u70bc\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u56db\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u5efa\u6a21\u3001\u955c\u50cf\u3001\u5e72\u9884\u548c\u81ea\u4e3b\u7ba1\u7406\uff0c\u5e76\u5206\u6790\u4e86\u7269\u7406\u5efa\u6a21\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u8de8\u9886\u57df\u7efc\u8ff0\uff0c\u8bc6\u522b\u4e86\u4e0e\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u76f8\u5173\u7684\u5171\u540c\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u8d1f\u8d23\u4efbAI\u9a71\u52a8\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u56db\u9636\u6bb5\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u63cf\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u6570\u5b57\u5b6a\u751f\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\uff0c\u5f3a\u8c03\u4e86\u4ece\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5411\u7269\u7406\u4fe1\u606f\u548c\u57fa\u7840\u6a21\u578b\u7684\u8f6c\u53d8\uff0c\u5e76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u6280\u672f\u5982\u4f55\u5c06\u6570\u5b57\u5b6a\u751f\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u548c\u81ea\u6211\u6539\u8fdb\u7684\u8ba4\u77e5\u7cfb\u7edf\u3002"}}
{"id": "2601.00988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00988", "abs": "https://arxiv.org/abs/2601.00988", "authors": ["Lin Xi", "Yingliang Ma", "Xiahai Zhuang"], "title": "Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss", "comment": null, "summary": "We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bFSVOS\u6a21\u578b\uff0c\u901a\u8fc7\u65b9\u5411\u91c7\u6837\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u89c6\u9891\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u975eCUDA\u8bbe\u5907\u4e0a\u7684\u53ef\u79fb\u690d\u6027\u9650\u5236\u4ee5\u53ca\u53c2\u6570\u5316\u5c42\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u9002\u5e94\u591a\u6837\u5316\u7684\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u65b9\u5411\u7684\u91c7\u6837\u89c6\u89d2\u91cd\u65b0\u7ec4\u7ec7\u5c40\u90e8\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u975e\u53c2\u6570\u5316\u91c7\u6837\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76d1\u7763\u65f6\u7a7a\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\u4ee5\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728CADICA\u3001XACV\u548cMOSXAV\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFSVOS\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684FSVOS\u6a21\u578b\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\uff0c\u4e3a\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2601.02200", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02200", "abs": "https://arxiv.org/abs/2601.02200", "authors": ["Markus Borg", "Nadim Hagatulah", "Adam Tornhill", "Emma S\u00f6derberg"], "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics", "comment": "Accepted for the 3rd ACM International Conference on AI Foundation Models and Software Engineering (FORGE 2026)", "summary": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u7c7b\u53cb\u597d\u7684\u4ee3\u7801\u4e5f\u66f4\u80fd\u517c\u5bb9AI\u5de5\u5177\uff0cCodeHealth\u6307\u6807\u53ef\u7528\u4e8e\u8bc4\u4f30AI\u5e72\u9884\u7684\u98ce\u9669\uff0c\u4e3a\u5927\u89c4\u6a21AI\u91c7\u7528\u505a\u51c6\u5907\u3002", "motivation": "\u968f\u7740\u4eba\u7c7b\u5f00\u53d1\u8005\u4e0eAI\u7f16\u7801\u4ee3\u7406\u5728\u540c\u4e00\u4e2a\u4ee3\u7801\u5e93\u4e2d\u5de5\u4f5c\u7684\u6df7\u5408\u65f6\u4ee3\u7684\u5230\u6765\uff0c\u786e\u4fdd\u4e0d\u540c\u80fd\u529b\u7684LLM\u80fd\u53ef\u9760\u5730\u7f16\u8f91\u4ee3\u7801\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf9\u6765\u81ea\u7ade\u8d5b\u7f16\u7a0b\u76845,000\u4e2aPython\u6587\u4ef6\u8fdb\u884c\u57fa\u4e8eLLM\u7684\u91cd\u6784\uff0c\u7814\u7a76AI\u53cb\u597d\u4ee3\u7801\u7684\u6982\u5ff5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cCodeHealth\uff08\u4e00\u79cd\u9488\u5bf9\u4eba\u7c7b\u7406\u89e3\u6821\u51c6\u7684\u8d28\u91cf\u6307\u6807\uff09\u4e0eAI\u91cd\u6784\u540e\u7684\u8bed\u4e49\u4fdd\u7559\u4e4b\u95f4\u5b58\u5728\u6709\u610f\u4e49\u7684\u5173\u8054\u3002", "conclusion": "\u4eba\u7c7b\u53cb\u597d\u7684\u4ee3\u7801\u4e5f\u66f4\u80fd\u517c\u5bb9AI\u5de5\u5177\uff0c\u7ec4\u7ec7\u53ef\u4ee5\u5229\u7528CodeHealth\u6307\u6807\u6765\u8bc4\u4f30AI\u5e72\u9884\u7684\u98ce\u9669\uff0c\u5e76\u51b3\u5b9a\u662f\u5426\u9700\u8981\u989d\u5916\u7684\u4eba\u5de5\u76d1\u7763\u3002\u6295\u8d44\u4e8e\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u4e0d\u4ec5\u5bf9\u4eba\u7c7b\u6709\u76ca\uff0c\u4e5f\u4e3a\u5927\u89c4\u6a21AI\u91c7\u7528\u505a\u597d\u4e86\u51c6\u5907\u3002"}}
{"id": "2601.01762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01762", "abs": "https://arxiv.org/abs/2601.01762", "authors": ["Yanhao Wu", "Haoyang Zhang", "Fei He", "Rui Wu", "Congpei Qiu", "Liang Gao", "Wei Ke", "Tong Zhang"], "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving", "comment": "underreview", "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety", "AI": {"tldr": "\u63d0\u51fa\u7ea7\u8054\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u6761\u4ef6\u5316\u516c\u5f0f\u663e\u5f0f\u7ed3\u5408\u9a7e\u9a76\u8def\u5f84\u4e0e\u7eb5\u5411\u89c4\u5212\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u534f\u8c03\u6027\u4e0e\u5b89\u5168\u6027\uff0cBench2Drive\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709SOTA\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u89c4\u5212\u9636\u6bb5\u5c06\u6a2a\u5411\u548c\u7eb5\u5411\u9884\u6d4b\u89e3\u8026\uff0c\u53ef\u80fd\u5bfc\u81f4\u8def\u5f84\u4e0e\u901f\u5ea6\u534f\u8c03\u5931\u8d25\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u9a7e\u9a76\u8def\u5f84\u4f5c\u4e3a\u7eb5\u5411\u89c4\u5212\u7684\u5148\u9a8c\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ea7\u8054\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u6761\u4ef6\u5316\u516c\u5f0f\u5c06\u9a7e\u9a76\u8def\u5f84\u663e\u5f0f\u5730\u7eb3\u5165\u7eb5\u5411\u89c4\u5212\uff0c\u5e76\u57fa\u4e8e\u6b64\u9884\u6d4b\u6cbf\u9a7e\u9a76\u8def\u5f84\u7684\u7eb5\u5411\u4f4d\u79fb\u800c\u975e\u5b8c\u6574\u76842D\u8f68\u8ff9\u70b9\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u9762\u5411\u89c4\u5212\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6a21\u62df\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u521b\u4e0b\u4e86\u65b0\u7684SOTA\uff0c\u9a7e\u9a76\u5f97\u5206\u4e3a89.07\uff0c\u6210\u529f\u7387\u4e3a73.18%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u8c03\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea7\u8054\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u7eb5\u5411\u89c4\u5212\u5efa\u7acb\u5728\u9a7e\u9a76\u8def\u5f84\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6a2a\u5411\u548c\u7eb5\u5411\u89c4\u5212\u7684\u534f\u8c03\u4e0e\u78b0\u649e\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u534f\u8c03\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.01330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01330", "abs": "https://arxiv.org/abs/2601.01330", "authors": ["Shengji Tang", "Weihao Lin", "Jingqi Ye", "Hao Li", "Bo Zhang", "Shuyue Hu", "Tao Chen", "Wangli Ouyang", "Lei Bai", "Peng Ye"], "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale", "comment": "12 pages", "summary": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).", "AI": {"tldr": "JiSi\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u8def\u7531\u548c\u805a\u5408\u65b9\u6cd5\uff0c\u4f7f\u5f00\u6e90LLM\u534f\u4f5c\u8d85\u8d8aGemini-3-Pro\uff0c\u5c55\u793a\u4e86\u96c6\u4f53\u667a\u80fd\u7684AGI\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u96c6\u4f53\u667a\u80fd\u4f5c\u4e3a\u66ff\u4ee3\u5355\u4e00\u6a21\u578b\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5f53\u524d\u8def\u7531\u548c\u805a\u5408\u7684\u74f6\u9888\u3002", "method": "\u5f15\u5165JiSi\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a\u6df7\u5408\u67e5\u8be2-\u54cd\u5e94\u8def\u7531\u3001\u57fa\u4e8e\u652f\u6301\u96c6\u7684\u805a\u5408\u5668\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJiSi\u4ec5\u752847%\u7684\u6210\u672c\u5373\u8d85\u8d8aGemini-3-Pro\uff0c\u5e76\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u3002", "conclusion": "JiSi\u6846\u67b6\u901a\u8fc7\u96c6\u4f53\u667a\u80fd\u5c55\u793a\u4e86\u8d85\u8d8aGemini-3-Pro\u7684\u6f5c\u529b\uff0c\u4e3aAGI\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u8def\u5f84\u3002"}}
{"id": "2601.00991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00991", "abs": "https://arxiv.org/abs/2601.00991", "authors": ["Joshua Kawaguchi", "Saad Manzur", "Emily Gao Wang", "Maitreyi Sinha", "Bryan Vela", "Yunxi Wang", "Brandon Vela", "Wayne B. Hayes"], "title": "UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data", "comment": "CVPR 2026 submission. Introduces UnrealPose-1M dataset and UnrealPose-Gen pipeline", "summary": "Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted \"coherent\" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.", "AI": {"tldr": "\u63d0\u51faUnrealPose-Gen\u7ba1\u7ebf\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4eba\u4f53\u59ff\u6001\u6570\u636e\uff0c\u53d1\u5e03UnrealPose-1M\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u5176\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u91ce\u5916\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u6807\u6ce8\u548c\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u59ff\u6001\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Unreal Engine 5\u548cMovie Render Queue\u6784\u5efa\u7684UnrealPose-Gen\u7ba1\u7ebf\uff0c\u751f\u6210\u5305\u542b3D\u5173\u8282\u30012D\u6295\u5f71\u3001\u906e\u6321\u6807\u5fd7\u7b49\u4e30\u5bcc\u6807\u6ce8\u7684\u5408\u6210\u6570\u636e\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b\u7ea6100\u4e07\u5e27\u7684UnrealPose-1M\u6570\u636e\u96c6\uff0c\u5e76\u5728\u56db\u9879\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u771f\u5b9e\u6027\u3002", "conclusion": "UnrealPose-1M\u6570\u636e\u96c6\u548cUnrealPose-Gen\u7ba1\u7ebf\u7684\u53d1\u5e03\uff0c\u652f\u6301\u7b2c\u4e09\u65b9\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u4f53\u59ff\u6001\u6570\u636e\uff0c\u5f25\u8865\u4e86\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2601.02215", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02215", "abs": "https://arxiv.org/abs/2601.02215", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems", "comment": null, "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.", "AI": {"tldr": "LLM\u8d4b\u80fd\u7684\u5de5\u4f5c\u6d41\u7a0b\u652f\u6301SDV\u5f00\u53d1\uff0c\u6db5\u76d6\u5b89\u5168\u62d3\u6251\u8bbe\u8ba1\u548c\u4e8b\u4ef6\u9a71\u52a8\u4ee3\u7801\u5206\u6790\uff0c\u901a\u8fc7\u4e8b\u4ef6\u94fe\u6a21\u578b\u548cMDE\u65b9\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u5f00\u53d1\u4e2d\u7684\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u548c\u4e8b\u4ef6\u9a71\u52a8\u4ee3\u7801\u5206\u6790\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u94fe\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u5206\u6790\uff0c\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u65b9\u6cd5\u548c\u5bf9\u8c61\u7ea6\u675f\u8bed\u8a00\uff08OCL\uff09\u89c4\u5219\u8fdb\u884c\u5b89\u5168\u62d3\u6251\u5206\u6790\u3002", "result": "\u5728\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u76f8\u5173\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u672c\u5730\u53ef\u90e8\u7f72\u548c\u4e13\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6709\u6548\u652f\u6301\u4e86\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\uff08SDV\uff09\u7684\u8f6f\u4ef6\u5f00\u53d1\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u62d3\u6251\u8bbe\u8ba1\u548c\u4e8b\u4ef6\u9a71\u52a8\u51b3\u7b56\u4ee3\u7801\u5206\u6790\u65b9\u9762\u3002"}}
{"id": "2601.01822", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01822", "abs": "https://arxiv.org/abs/2601.01822", "authors": ["Shiyong Meng", "Tao Zou", "Bolei Chen", "Chaoxu Mu", "Jianxin Wang"], "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization", "comment": "7 pages, 4 figures", "summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.", "AI": {"tldr": "DisCo-FLoc\u901a\u8fc7\u53cc\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u89e3\u51b3\u5e73\u9762\u56fe\u5b9a\u4f4d\u6a21\u7cca\u95ee\u9898\uff0c\u65e0\u9700\u8bed\u4e49\u6807\u7b7e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u7b80\u7ea6\u5e73\u9762\u56fe\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\u5bfc\u81f4\u5b9a\u4f4d\u6a21\u7cca\uff0c\u4e14\u6602\u8d35\u7684\u8bed\u4e49\u6807\u6ce8\u9650\u5236\u4e86\u5176\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bed\u4e49\u6807\u7b7e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c04\u7ebf\u56de\u5f52\u9884\u6d4b\u5668\uff0c\u4ee5\u53ca\u5e26\u6709\u4f4d\u7f6e\u7ea7\u548c\u65b9\u5411\u7ea7\u7ea6\u675f\u7684\u65b0\u578b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e25\u683c\u5339\u914d\u6df1\u5ea6\u611f\u77e5\u89c6\u89c9\u7279\u5f81\u4e0e\u5e73\u9762\u56fe\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDisCo-FLoc\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u8bed\u4e49\u7684\u65b9\u6cd5\u3002", "conclusion": "DisCo-FLoc\u901a\u8fc7\u53cc\u7ea7\u89c6\u89c9-\u51e0\u4f55\u5bf9\u6bd4\u6709\u6548\u89e3\u51b3\u4e86\u7b80\u7ea6\u5316\u5e73\u9762\u56fe\u4e2d\u91cd\u590d\u7ed3\u6784\u5bfc\u81f4\u7684\u5b9a\u4f4d\u6a21\u7cca\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bed\u4e49\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2601.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01363", "abs": "https://arxiv.org/abs/2601.01363", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Xiaohui Zhong", "Mengping Yang", "Qiusheng Huang", "Lei Chen", "Libo Wu", "Hao Li"], "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research", "comment": null, "summary": "Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25\u00b0 resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.", "AI": {"tldr": "FuXi-Uni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u6807\u8bb0\u5bf9\u9f50\u548c\u79d1\u5b66\u89e3\u7801\u5668\uff0c\u5728\u5730\u7403\u79d1\u5b66\u548c\u751f\u7269\u533b\u5b66\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8de8\u5b66\u79d1\u79d1\u5b66\u95ee\u9898\u9700\u8981\u6574\u5408\u5f02\u6784\u3001\u9ad8\u7ef4\u6570\u636e\uff0c\u800c\u73b0\u6709AI\u6a21\u578b\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u7f3a\u4e4f\u540c\u65f6\u7406\u89e3\u548c\u751f\u6210\u591a\u6a21\u6001\u79d1\u5b66\u6570\u636e\u7684\u80fd\u529b\u3002", "method": "FuXi-Uni\u5c06\u8de8\u5b66\u79d1\u79d1\u5b66\u6807\u8bb0\u4e0e\u81ea\u7136\u8bed\u8a00\u6807\u8bb0\u5bf9\u9f50\uff0c\u5e76\u5229\u7528\u79d1\u5b66\u89e3\u7801\u5668\u91cd\u5efa\u79d1\u5b66\u6807\u8bb0\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u548c\u79d1\u5b66\u6570\u503c\u9884\u6d4b\u3002", "result": "\u5728\u5730\u7403\u79d1\u5b66\u4e2d\uff0cFuXi-Uni\u768410\u5929\u5168\u7403\u5929\u6c14\u9884\u62a5\u57280.25\u00b0\u5206\u8fa8\u7387\u4e0b\u4f18\u4e8eSOTA\u7269\u7406\u9884\u62a5\u7cfb\u7edf\uff0c\u5728\u70ed\u5e26\u6c14\u65cb\u9884\u6d4b\u548c\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u5929\u6c14\u573a\u751f\u6210\u4e2d\u8868\u73b0\u5353\u8d8a\uff1b\u5728\u751f\u7269\u533b\u5b66\u4e2d\uff0c\u5b83\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u9886\u5148\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "FuXi-Uni\u901a\u8fc7\u7edf\u4e00\u5f02\u6784\u79d1\u5b66\u6a21\u6001\u4e8e\u539f\u751f\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0c\u4e3a\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2601.00993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00993", "abs": "https://arxiv.org/abs/2601.00993", "authors": ["Julian D. Santamaria", "Claudia Isaza", "Jhony H. Giraldo"], "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift", "comment": null, "summary": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.", "AI": {"tldr": "WildIng\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u4e0e\u56fe\u50cf\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u91ce\u751f\u52a8\u7269\u8bc6\u522b\u6a21\u578b\u5728\u5730\u7406\u57df\u8f6c\u79fb\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u51c6\u786e\u7387\u63d0\u534730%\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5730\u7406\u57df\u8f6c\u79fb\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u8868\u793a\u4f7f\u5176\u5bf9\u80cc\u666f\u3001\u5149\u7167\u7b49\u53d8\u5316\u654f\u611f\u3002", "method": "WildIng\u6574\u5408\u4e86\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff0c\u521b\u5efa\u4e86\u5bf9\u5730\u7406\u57df\u8f6c\u79fb\u66f4\u9c81\u68d2\u7684\u8868\u793a\u65b9\u6cd5\u3002", "result": "WildIng\u5728\u7f8e\u6d32\u548c\u975e\u6d32\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5c06BioCLIP\u7b49\u57fa\u7840\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e8630%\u3002", "conclusion": "WildIng\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5730\u7406\u57df\u8f6c\u79fb\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02238", "abs": "https://arxiv.org/abs/2601.02238", "authors": ["Nils Bosbach", "Alwalid Salama", "Lukas J\u00fcnger", "Mark Burton", "Niko Zurstra\u00dfen", "Rebecca Pelke", "Rainer Leupers"], "title": "NQC2: A Non-Intrusive QEMU Code Coverage Plugin", "comment": "PREPRINT - accepted by the Rapid Simulation and Performance Evaluation for Design Workshop (RAPIDO '24)", "summary": "Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.\n  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.", "AI": {"tldr": "NQC2\u901a\u8fc7QEMU\u63d2\u4ef6\u5b9e\u73b0\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4ee3\u7801\u8986\u76d6\u7387\u5206\u6790\uff0c\u65e0\u9700\u63d2\u6869\u4e14\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8986\u76d6\u7387\u5206\u6790\u65b9\u6cd5\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff08\u5c24\u5176\u662f\u88f8\u673a\u7a0b\u5e8f\uff09\u4e2d\u56e0\u7f3a\u4e4f\u64cd\u4f5c\u7cfb\u7edf\u548c\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7QEMU\u63d2\u4ef6NQC2\u5728\u8fd0\u884c\u65f6\u63d0\u53d6\u8986\u76d6\u7387\u4fe1\u606f\uff0c\u5e76\u5c06\u6570\u636e\u5b58\u50a8\u5728\u4e3b\u673a\u6587\u4ef6\u4e2d\u3002", "result": "NQC2\u6027\u80fd\u8d85\u8d8aXilinx\u7c7b\u4f3c\u65b9\u6848\u8fbe8.5\u500d\u3002", "conclusion": "NQC2\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u8f6f\u4ef6\u63d2\u6869\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4ee3\u7801\u8986\u76d6\u7387\u5206\u6790\u7684\u6548\u7387\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2601.01872", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01872", "abs": "https://arxiv.org/abs/2601.01872", "authors": ["Hongbo Duan", "Shangyi Luo", "Zhiyuan Deng", "Yanbo Chen", "Yuanhao Chiang", "Yi Liu", "Fangming Liu", "Xueqian Wang"], "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios", "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.", "AI": {"tldr": "CausalNav\u662f\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u4e13\u4e3a\u52a8\u6001\u6237\u5916\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\u548c\u5b9e\u65f6\u611f\u77e5\u63d0\u5347\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u9c81\u68d2\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u81ea\u4e3b\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5305\u62ec\u8bed\u4e49\u63a8\u7406\u3001\u52a8\u6001\u6761\u4ef6\u548c\u957f\u671f\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u591a\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\uff08Embodied Graph\uff09\uff0c\u7ed3\u5408LLMs\u548c\u79bb\u7ebf\u5730\u56fe\u6570\u636e\uff0c\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4ee5\u5b9e\u73b0\u8bed\u4e49\u5bfc\u822a\u548c\u957f\u7a0b\u89c4\u5212\u3002\u52a8\u6001\u5bf9\u8c61\u5728\u573a\u666f\u56fe\u6784\u5efa\u548c\u5206\u5c42\u89c4\u5212\u6a21\u5757\u4e2d\u663e\u5f0f\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u7a97\u53e3\u6301\u7eed\u66f4\u65b0\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCausalNav\u5177\u6709\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "CausalNav\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\u548c\u5b9e\u65f6\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.01366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01366", "abs": "https://arxiv.org/abs/2601.01366", "authors": ["Zixian Liu", "Sihao Liu", "Yuqi Zhao"], "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models", "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.", "AI": {"tldr": "KGCE \u662f\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u5e93\u548c\u53cc\u56fe\u8bc4\u4f30\u7684\u6559\u80b2\u8de8\u5e73\u53f0\u4efb\u52a1\u57fa\u51c6\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u7684\u4e0d\u8db3\u5e76\u63d0\u5347\u4e86\u4ee3\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6846\u67b6\u5728\u6559\u80b2\u573a\u666f\u7684\u8de8\u5e73\u53f0\u4efb\u52a1\u652f\u6301\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u79c1\u6709\u9886\u57df\u8f6f\u4ef6\u7684\u7ed3\u6784\u7406\u89e3\u4e0d\u8db3\u5bfc\u81f4\u4ee3\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u7ec6\u8282\u6267\u884c\u6548\u7387\u3002", "method": "KGCE \u6574\u5408\u4e86\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5305\u542b104\u4e2a\u6559\u80b2\u76f8\u5173\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u5b66\u6821\u7279\u5b9a\u8f6f\u4ef6\u7684\u77e5\u8bc6\u5e93\u589e\u5f3a\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "KGCE \u901a\u8fc7\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u79c1\u6709\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u6267\u884c\u6548\u7387\u3002", "conclusion": "KGCE \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u7684\u65b0\u57fa\u51c6\u5e73\u53f0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6559\u80b2\u573a\u666f\u4e2d\u8de8\u5e73\u53f0\u4efb\u52a1\u6267\u884c\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u79c1\u6709\u9886\u57df\u8f6f\u4ef6\u4e2d\u7684\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2601.00998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00998", "abs": "https://arxiv.org/abs/2601.00998", "authors": ["Yue Zhou", "Jue Chen", "Zilun Zhang", "Penghui Huang", "Ran Ding", "Zhentao Zou", "PengFei Gao", "Yuchen Wei", "Ke Li", "Xue Yang", "Xue Jiang", "Hongxin Yang", "Jonathan Li"], "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models", "comment": "20 pages, 17 figures", "summary": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65e0\u4eba\u673a\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6DVGBench\u548cDroneVG-R1\u6a21\u578b\uff0c\u901a\u8fc7I2E-CoT\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u53c2\u8003\u8868\u8fbe\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u9886\u57df\u77e5\u8bc6\u7684\u9690\u5f0f\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86DroneVG-R1\u6a21\u578b\uff0c\u7ed3\u5408I2E-CoT\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06\u9690\u5f0f\u67e5\u8be2\u8f6c\u6362\u4e3a\u663e\u5f0f\u67e5\u8be2\u4ee5\u964d\u4f4e\u5b9a\u4f4d\u96be\u5ea6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u4e3b\u6d41\u6a21\u578b\u5728\u663e\u5f0f\u548c\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86DVGBench\u57fa\u51c6\u548cDroneVG-R1\u6a21\u578b\uff0c\u901a\u8fc7I2E-CoT\u65b9\u6cd5\u63d0\u5347\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u7684\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.02248", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02248", "abs": "https://arxiv.org/abs/2601.02248", "authors": ["Mohammad Reza Heidari Iman", "Giorgio Di Natale", "Katell Morin-Allory"], "title": "Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions", "comment": "6 pages", "summary": "Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u6700\u65b0\u7684\u65ad\u8a00\u6316\u6398\u5668\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u65b9\u6cd5\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002", "motivation": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u9a8c\u8bc1\u4ece\u4e1a\u8005\u63d0\u4f9b\u5bf9\u73b0\u6709\u6316\u6398\u5668\u80fd\u529b\u548c\u5c40\u9650\u6027\u7684\u6df1\u5165\u89c1\u89e3\u3002", "method": "\u56de\u987e\u4e86\u6700\u65b0\u3001\u5148\u8fdb\u4e14\u5e7f\u6cdb\u91c7\u7528\u7684\u65ad\u8a00\u6316\u6398\u5668\uff0c\u5e76\u5bf9\u5b83\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u65ad\u8a00\u6316\u6398\u5668\u65b9\u6cd5\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u8bc6\u522b\u4e86\u5b83\u4eec\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u65ad\u8a00\u6316\u6398\u5668\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u5148\u8fdb\u7684\u65ad\u8a00\u6316\u6398\u5668\u7684\u65b9\u5411\u3002"}}
{"id": "2601.01946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01946", "abs": "https://arxiv.org/abs/2601.01946", "authors": ["Sichao Song", "Yuki Okafuji", "Takuya Iwamoto", "Jun Baba", "Hiroshi Ishiguro"], "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment", "comment": null, "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u96f6\u552e\u673a\u5668\u4eba\u867d\u589e\u52a0\u987e\u5ba2\u505c\u7559\u7387\uff0c\u4f46\u51cf\u5c11\u4e86\u5e97\u5458\u4e3b\u5bfc\u7684\u540e\u7eed\u670d\u52a1\u6b65\u9aa4\uff0c\u63ed\u793a\u4e86\u5458\u5de5\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u90e8\u7f72\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u5728\u96f6\u552e\u73af\u5883\u4e2d\u90e8\u7f72\u5bf9\u8bdd\u670d\u52a1\u673a\u5668\u4eba\u5bf9\u987e\u5ba2\u884c\u4e3a\u548c\u670d\u52a1\u6d41\u7a0b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5458\u5de5\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u5730\u5b9e\u9a8c\uff0c\u5305\u62ec12\u5929\u7684\u4e09\u79cd\u6761\u4ef6\u4ea4\u66ff\uff08\u65e0\u673a\u5668\u4eba\u3001\u4ec5\u673a\u5668\u4eba\u3001\u673a\u5668\u4eba+\u56fa\u5b9a\u88c5\u7f6e\uff09\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u6ce8\u91ca\u670d\u52a1\u6f0f\u6597\uff0c\u540e\u7eed\u901a\u8fc7\u516d\u6b21\u5458\u5de5\u8bbf\u8c08\u89e3\u91ca\u5b9a\u91cf\u6a21\u5f0f\u3002", "result": "\u673a\u5668\u4eba\u589e\u52a0\u4e86\u987e\u5ba2\u505c\u7559\u7387\uff08\u5c24\u5176\u662f\u4e0e\u56fa\u5b9a\u88c5\u7f6e\u7ed3\u5408\u65f6\uff09\uff0c\u4f46\u51cf\u5c11\u4e86\u5e97\u5458\u4e3b\u5bfc\u7684\u4e0b\u6e38\u6b65\u9aa4\uff08\u5982\u63a5\u8fd1\u3001\u8fdb\u5e97\u3001\u534f\u52a9\u4f53\u9a8c\u548c\u8d2d\u4e70\uff09\u3002\u8bbf\u8c08\u63ed\u793a\u4e86\u5458\u5de5\u907f\u514d\u6253\u65ad\u673a\u5668\u4eba\u4e0e\u987e\u5ba2\u5bf9\u8bdd\u3001\u9762\u5bf9\u5bf9\u8bdd\u5ef6\u8fdf\u7684\u65f6\u673a\u6a21\u7cca\u6027\u4ee5\u53ca\u673a\u5668\u4eba\u5bf9\u513f\u7ae5\u7684\u5438\u5f15\u529b\u7b49\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7efc\u5408\u4e86\u987e\u5ba2\u3001\u5458\u5de5\u548c\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\u5173\u7cfb\uff0c\u5e76\u4e3a\u9ad8\u63a5\u89e6\u96f6\u552e\u73af\u5883\u4e2d\u670d\u52a1\u673a\u5668\u4eba\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2601.01378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01378", "abs": "https://arxiv.org/abs/2601.01378", "authors": ["Han Yuan", "Yilin Wu", "Li Zhang", "Zheng Ma"], "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification", "comment": null, "summary": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAAAI\u6d41\u7a0b\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u4fee\u6b63\u4e8b\u5b9e\u5e7b\u89c9\u63d0\u5347SLMs\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u56e0\u4e8b\u5b9e\u5e7b\u89c9\u5bfc\u81f4\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u63a2\u7d22\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6d41\u7a0bAAAI\uff0c\u5305\u62ec\u5173\u8054\u8bc6\u522b\u3001\u81ea\u52a8\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u4e8b\u5b9e\u5e7b\u89c9\u4e0e\u9519\u8bef\u5206\u7c7b\u6b63\u76f8\u5173\uff1b(2) \u57fa\u4e8e\u7f16\u7801\u5668\u7684\u9a8c\u8bc1\u5668\u80fd\u6709\u6548\u68c0\u6d4b\u4e8b\u5b9e\u5e7b\u89c9\uff1b(3) \u53cd\u9988\u673a\u5236\u53ef\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684AAAI\u6d41\u7a0b\uff08\u5173\u8054\u8bc6\u522b\u3001\u81ea\u52a8\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff09\u6709\u52a9\u4e8e\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2601.01002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01002", "abs": "https://arxiv.org/abs/2601.01002", "authors": ["Prem Babu Kanaparthi", "Tulasi Venkata Sri Varshini Padamata"], "title": "Lightweight Channel Attention for Efficient CNNs", "comment": "6 pages, 5 figures", "summary": "Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.", "AI": {"tldr": "LCA\u6a21\u5757\u901a\u8fc7\u81ea\u9002\u5e94\u4e00\u7ef4\u5377\u79ef\u548c\u5206\u7ec4\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u5c3d\u7ba1\u6ce8\u610f\u529b\u673a\u5236\u5728\u73b0\u4ee3CNN\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e0d\u540c\u901a\u9053\u6ce8\u610f\u529b\u8bbe\u8ba1\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u7ecf\u9a8c\u6027\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86SE\u3001ECA\u548c\u63d0\u51fa\u7684LCA\u6a21\u5757\u5728ResNet 18\u548cMobileNetV2\u67b6\u6784\u4e0a\u7684\u6027\u80fd\u3002LCA\u91c7\u7528\u81ea\u9002\u5e94\u4e00\u7ef4\u5377\u79ef\u548c\u5206\u7ec4\u64cd\u4f5c\u4ee5\u51cf\u5c11\u53c2\u6570\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLCA\u5728CIFAR 10\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728\u53c2\u6570\u6548\u7387\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LCA\u6a21\u5757\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0eECA\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff08ResNet 18\u4e0a94.68%\uff0cMobileNetV2\u4e0a93.10%\uff09\uff0c\u5e76\u5728\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6ce8\u610f\u529b\u589e\u5f3aCNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2601.02345", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02345", "abs": "https://arxiv.org/abs/2601.02345", "authors": ["Parham Khamsepour", "Mark Cole", "Ish Ashraf", "Sandeep Puri", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena", "comment": "Accepted for publication in SANER 2026", "summary": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.", "AI": {"tldr": "QAMR\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u7248\u672c\u7cfb\u7edf\u6587\u6863\u4f18\u5316\u7684\u95ee\u7b54\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6539\u8fdb\u7684RAG\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u6280\u672f\u5728\u591a\u7248\u672c\u7cfb\u7edf\u6587\u6863\u4e0a\u7684\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u4fc3\u4f7f\u5f00\u53d1QAMR\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "QAMR\u901a\u8fc7\u7ed3\u5408\u9884\u5904\u7406\u3001\u67e5\u8be2\u91cd\u5199\u548c\u4e0a\u4e0b\u6587\u9009\u62e9\uff0c\u91c7\u7528\u53cc\u5206\u5757\u7b56\u7565\u4f18\u5316\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u3002", "result": "QAMR\u5728\u7b54\u6848\u6b63\u786e\u6027\uff0888.5%\uff09\u548c\u68c0\u7d22\u51c6\u786e\u6027\uff0890%\uff09\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u534716.5%\u548c12%\uff0c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c118%\u3002", "conclusion": "QAMR\u5728\u8de8\u591a\u7248\u672c\u7cfb\u7edf\u6587\u6863\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u6b63\u786e\u6027\u548c\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u54cd\u5e94\u65f6\u95f4\u3002"}}
{"id": "2601.01948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01948", "abs": "https://arxiv.org/abs/2601.01948", "authors": ["Zhihao Gu", "Ming Yang", "Difan Zou", "Dong Xu"], "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation", "comment": "Accepted to AAAI2026", "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.", "AI": {"tldr": "SDP\u662f\u4e00\u79cd\u57fa\u4e8e\u6280\u80fd\u7684\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u4e3a\u57fa\u672c\u6280\u80fd\u5e76\u786e\u4fdd\u6280\u80fd\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u7b56\u7565\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u6307\u4ee4\u751f\u6210\u77ed\u671f\u63a7\u5236\u4fe1\u53f7\uff0c\u53ef\u80fd\u5bfc\u81f4\u52a8\u4f5c\u751f\u6210\u4e0d\u5bf9\u9f50\u3002\u57fa\u672c\u6280\u80fd\uff08\u5982\u201c\u5411\u4e0a\u79fb\u52a8\u201d\u548c\u201c\u6253\u5f00\u5939\u722a\u201d\uff09\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u6709\u6548\u7684\u63a5\u53e3\u3002", "method": "SDP\u6574\u5408\u4e86\u53ef\u89e3\u91ca\u7684\u6280\u80fd\u5b66\u4e60\u4e0e\u6761\u4ef6\u52a8\u4f5c\u89c4\u5212\uff0c\u62bd\u8c61\u51fa\u516b\u4e2a\u53ef\u91cd\u7528\u7684\u57fa\u672c\u6280\u80fd\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u89c6\u89c9\u89c2\u5bdf\u548c\u8bed\u8a00\u6307\u4ee4\u4e2d\u63d0\u53d6\u79bb\u6563\u8868\u793a\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u7f51\u7edc\uff0c\u4e3a\u6bcf\u4e2a\u72b6\u6001\u5206\u914d\u6240\u9700\u7684\u57fa\u672c\u6280\u80fd\uff0c\u6784\u5efa\u5355\u4e00\u6280\u80fd\u7b56\u7565\u4ee5\u751f\u6210\u6280\u80fd\u5bf9\u9f50\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\uff0cSDP\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SDP\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u57fa\u672c\u6280\u80fd\u5e76\u9009\u62e9\u5355\u4e00\u6280\u80fd\u7b56\u7565\uff0c\u786e\u4fdd\u4e86\u8de8\u4efb\u52a1\u7684\u6280\u80fd\u4e00\u81f4\u6027\u884c\u4e3a\uff0c\u4e3a\u57fa\u4e8e\u6280\u80fd\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.01467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01467", "abs": "https://arxiv.org/abs/2601.01467", "authors": ["Romuald Kwessy Mouona", "Blaise Bl\u00e9riot Koguep Njionou", "Etienne Romuald Temgoua Alomo", "Rokia Missaoui", "Leonard Kwuida"], "title": "A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts", "comment": "26 pages", "summary": "This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.", "AI": {"tldr": "\u7814\u7a76\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u7684\u6761\u4ef6\u5c5e\u6027\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u542b\uff0c\u5e76\u6784\u5efa\u5176\u6700\u4f18\u57fa\u3002", "motivation": "\u63a2\u8ba8\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u8fd9\u4e9b\u8574\u542b\u7684\u6f5c\u5728\u5e94\u7528\u548c\u7406\u8bba\u57fa\u7840\u3002", "method": "\u7814\u7a76\u805a\u7126\u4e8eGanter\u548cObiedkov\u5f15\u5165\u7684\u6761\u4ef6\u5c5e\u6027\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u542b\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u8fd9\u4e9b\u8574\u542b\u7684\u6700\u4f18\u57fa\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u7684\u6761\u4ef6\u5c5e\u6027\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u542b\u6784\u5efa\u4e86\u4e00\u4e2a\u6700\u4f18\u57fa\u3002"}}
{"id": "2601.01022", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01022", "abs": "https://arxiv.org/abs/2601.01022", "authors": ["Shiao Wang", "Xiao Wang", "Haonan Zhao", "Jiarui Xu", "Bo Jiang", "Lin Zhu", "Xin Zhao", "Yonghong Tian", "Jin Tang"], "title": "Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking", "comment": null, "summary": "Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u57df\u65e9\u671f\u878d\u5408\u548c\u8fd0\u52a8\u5f15\u5bfc\u7684RGB-Event\u8ddf\u8e2a\u6846\u67b6\uff0c\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7279\u6027\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709RGB-Event\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u7279\u5f81\u7ea7\u878d\u5408\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5982\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u8fd0\u52a8\u654f\u611f\u6027\uff0c\u4e14\u5bf9\u4f4e\u4fe1\u606f\u533a\u57df\u5904\u7406\u65b9\u5f0f\u5355\u4e00\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u8bba\u6587\u65b9\u6cd5\u5305\u62ec\u5c06RGB\u548c\u4e8b\u4ef6\u6a21\u6001\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u4ece\u7a7a\u95f4\u57df\u8f6c\u6362\u5230\u9891\u57df\uff0c\u5206\u79bb\u632f\u5e45\u548c\u76f8\u4f4d\u6210\u5206\uff0c\u5e76\u901a\u8fc7\u632f\u5e45\u548c\u76f8\u4f4d\u6ce8\u610f\u529b\u9009\u62e9\u6027\u878d\u5408\u9ad8\u9891\u4e8b\u4ef6\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5229\u7528\u8fd0\u52a8\u5f15\u5bfc\u7684\u7a7a\u95f4\u7a00\u758f\u5316\u6a21\u5757\u6355\u83b7\u76ee\u6807\u8fd0\u52a8\u7ebf\u7d22\u4e0e\u7a7a\u95f4\u6982\u7387\u5206\u5e03\u7684\u5173\u7cfb\uff0c\u8fc7\u6ee4\u4f4e\u4fe1\u606f\u533a\u57df\u3002", "result": "\u5728FE108\u3001FELT\u548cCOESOT\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684RGB-Event\u8ddf\u8e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684RGB-Event\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u65e9\u671f\u878d\u5408\u548c\u8fd0\u52a8\u5f15\u5bfc\u7684\u7a7a\u95f4\u7a00\u758f\u5316\u6a21\u5757\uff0c\u6709\u6548\u5229\u7528\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u8fd0\u52a8\u654f\u611f\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2601.01969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01969", "abs": "https://arxiv.org/abs/2601.01969", "authors": ["Sichao Song", "Yuki Okafuji", "Kaito Ariu", "Amy Koike"], "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI", "comment": null, "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u548c\u79bb\u7ebf\u8bc4\u4f30\uff0c\u4e3a\u516c\u5171HRI\u573a\u666f\u4e2d\u7684\u8bed\u97f3\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7ecf\u9a8c\u3002", "motivation": "\u8bbe\u8ba1\u5728\u5f00\u653e\u591a\u6837\u73af\u5883\u4e2d\u65e2\u9ad8\u6548\u53c8\u53ef\u63a5\u53d7\u7684\u5bf9\u8bdd\u670d\u52a1\u673a\u5668\u4eba\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\uff0c\u5728\u7ebf\u5b66\u4e60\u80fd\u9002\u5e94\u975e\u7a33\u6001\u6761\u4ef6\u3002", "method": "\u901a\u8fc712\u5929\u7684\u73b0\u573a\u90e8\u7f72\uff0c\u5c06\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u91c7\u7528Thompson\u91c7\u6837\u5728\u516d\u79cd\u52a8\u4f5c\uff08\u8bed\u901f\u6162/\u6b63\u5e38/\u5feb\uff0c\u7b80\u6d01/\u8be6\u7ec6\uff09\u4e2d\u9009\u62e9\u3002\u540c\u65f6\uff0c\u7ed3\u5408\u4e09\u79cd\u4e8c\u5143\u5956\u52b1\uff08\u7528\u6237\u8bc4\u5206\u3001\u5bf9\u8bdd\u7ed3\u675f\u3001\u81f3\u5c11\u4e24\u8f6e\u5bf9\u8bdd\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6bcf\u79cd\u5956\u52b1\u673a\u5236\u8bf1\u5bfc\u51fa\u4e0d\u540c\u7684\u52a8\u4f5c\u5206\u5e03\u548c\u4ea4\u4e92\u884c\u4e3a\uff0c\u79bb\u7ebf\u8bc4\u4f30\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u4e0a\u4e0b\u6587\u56e0\u7d20\uff08\u5982\u4eba\u7fa4\u5bc6\u5ea6\u3001\u5c0f\u7ec4\u89c4\u6a21\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u516c\u5171\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u573a\u666f\u4e2d\u90e8\u7f72\u5728\u7ebf\u8bed\u97f3\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u76f4\u63a5\u5e94\u7528\u7684\u8bbe\u8ba1\u7ecf\u9a8c\u3002"}}
{"id": "2601.01511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01511", "abs": "https://arxiv.org/abs/2601.01511", "authors": ["Ahmed Dawoud", "Osama El-Shamy"], "title": "Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning", "comment": null, "summary": "Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u7684DML\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u89e3\u51b3\u89c2\u6d4b\u6027\u7814\u7a76\u4e2d\u7684\u6df7\u6742\u504f\u5dee\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u89c2\u6d4b\u6027\u7814\u7a76\u4e2d\uff0c\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u5e38\u5bfc\u81f4\u9009\u62e9\u504f\u5dee\uff0c\u800c\u4f20\u7edf\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0e\u7ed3\u6784\u5316\u534f\u53d8\u91cf\u6b63\u4ea4\u7684\u6df7\u6742\u56e0\u7d20\u3002\u9ad8\u7ef4\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u53ef\u80fd\u5305\u542b\u8fd9\u4e9b\u6f5c\u5728\u53d8\u91cf\u7684\u4e30\u5bcc\u4ee3\u7406\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u7684\u53cc\u91cd\u673a\u5668\u5b66\u4e60\uff08DML\uff09\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u56e0\u679c\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u57fa\u51c6\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u975e\u7ed3\u6784\u5316\u6587\u672c\u5d4c\u5165\u80fd\u6355\u6349\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u4e2d\u7f3a\u5931\u7684\u5173\u952e\u6df7\u6742\u4fe1\u606f\u3002\u6807\u51c6\u7684\u57fa\u4e8e\u6811\u7684DML\u4f30\u8ba1\u5668\u56e0\u65e0\u6cd5\u5efa\u6a21\u5d4c\u5165\u6d41\u5f62\u7684\u8fde\u7eed\u62d3\u6251\u7ed3\u6784\u800c\u4fdd\u7559\u663e\u8457\u504f\u5dee\uff08+24%\uff09\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u5c06\u504f\u5dee\u964d\u81f3-0.86%\uff0c\u6709\u6548\u6062\u590d\u4e86\u771f\u5b9e\u56e0\u679c\u53c2\u6570\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u5229\u7528\u9ad8\u7ef4\u81ea\u7136\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u65f6\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u65e0\u6df7\u6742\u5047\u8bbe\u3002"}}
{"id": "2601.01024", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01024", "abs": "https://arxiv.org/abs/2601.01024", "authors": ["Tien-Huy Nguyen", "Huu-Loc Tran", "Thanh Duc Ngo"], "title": "ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval", "comment": "Accepted at WACV Main Track 2026", "summary": "Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself", "AI": {"tldr": "ITSELF\u662f\u4e00\u79cd\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u5f0f\u5c40\u90e8\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7GRAB\u3001MARS\u548cATS\u7ec4\u4ef6\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5373\u53ef\u5728TBPS\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c40\u90e8\u5bf9\u9f50\u5904\u7406TBPS\u4efb\u52a1\uff0c\u4f46\u5bb9\u6613\u9677\u5165\u6377\u5f84\u5b66\u4e60\u548c\u865a\u5047\u5173\u8054\uff0c\u5bfc\u81f4\u9519\u4f4d\uff1b\u540c\u65f6\u6ce8\u5165\u5148\u9a8c\u77e5\u8bc6\u53ef\u80fd\u626d\u66f2\u6a21\u6001\u5185\u7ed3\u6784\u3002\u7814\u7a76\u53d1\u73b0\u7f16\u7801\u5668\u6ce8\u610f\u529b\u4ece\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u5373\u53ef\u63d0\u4f9b\u7a7a\u95f4\u7cbe\u786e\u7684\u8bc1\u636e\uff0c\u56e0\u6b64\u63d0\u51faITSELF\u6846\u67b6\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ITSELF\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aGRAB\uff08\u901a\u8fc7\u6ce8\u610f\u529b\u94f6\u884c\u8f6c\u6362\u9ad8\u663e\u8457\u6027\u6807\u8bb0\u5e76\u5e94\u7528\u5c40\u90e8\u76ee\u6807\uff09\u3001MARS\uff08\u8de8\u5c42\u805a\u5408\u6ce8\u610f\u529b\u5e76\u6267\u884c\u591a\u6837\u6027\u611f\u77e5\u7684top-k\u9009\u62e9\uff09\u548cATS\uff08\u4ece\u7c97\u5230\u7ec6\u8c03\u5ea6\u4fdd\u7559\u9884\u7b97\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684TBPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cITSELF\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ITSELF\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u5f0f\u5c40\u90e8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u65e0\u9700\u989d\u5916\u5148\u9a8c\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6587\u672c\u57fa\u4e8e\u4eba\u7269\u641c\u7d22\uff08TBPS\uff09\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01971", "abs": "https://arxiv.org/abs/2601.01971", "authors": ["Aditya Singh", "Rajpal Singh", "Jishnu Keshavan"], "title": "Deep Robust Koopman Learning from Noisy Data", "comment": null, "summary": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60Koopman\u7b97\u5b50\uff0c\u663e\u8457\u51cf\u5c11\u566a\u58f0\u8bf1\u5bfc\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u901a\u5e38\u5e26\u6709\u566a\u58f0\uff0c\u5bfc\u81f4Koopman\u7b97\u5b50\u7684\u8fd1\u4f3c\u5b58\u5728\u566a\u58f0\u8bf1\u5bfc\u504f\u5dee\uff0c\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8054\u5408\u5b66\u4e60\u7cfb\u7edf\u7684\u6b63\u5411\u548c\u53cd\u5411\u65f6\u95f4\u52a8\u6001\u4e00\u81f4\u7684Koopman\u57fa\u51fd\u6570\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u52a8\u6001\u5408\u6210\u51cf\u5c11\u504f\u5dee\u7684Koopman\u7b97\u5b50\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u566a\u58f0\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u504f\u5dee\u3002\u591a\u4e2a\u4e32\u8054\u673a\u68b0\u81c2\u7684\u52a8\u6001\u9884\u6d4b\u548c\u8ddf\u8e2a\u63a7\u5236\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u9c81\u68d2\u6027\uff0cFranka FR3 7-DoF\u673a\u68b0\u81c2\u7684\u5b9e\u9a8c\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u8054\u5408\u5b66\u4e60\u9002\u5f53\u7684\u63d0\u5347\u51fd\u6570\u548c\u51cf\u5c11\u504f\u5dee\u7684Koopman\u7b97\u5b50\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9884\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2601.01522", "categories": ["cs.AI", "cs.CL", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.01522", "abs": "https://arxiv.org/abs/2601.01522", "authors": ["Danial Amin"], "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u591aLLM\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u548c\u7a33\u5065\u805a\u5408\u4f18\u5316\u4e0d\u5bf9\u79f0\u6210\u672c\u51b3\u7b56\uff0c\u5b9e\u9a8c\u663e\u793a\u6210\u672c\u964d\u4f4e34%\u3001\u516c\u5e73\u6027\u63d0\u534745%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5355\u4e00LLM\u540e\u9a8c\u9608\u503c\u51b3\u7b56\uff09\u5728\u6210\u672c\u4e0d\u5bf9\u79f0\u7684\u5e8f\u5217\u51b3\u7b56\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u63d0\u793a\u6cd5\u83b7\u53d6\u5404\u5019\u9009\u72b6\u6001\u7684\u4f3c\u7136\uff0c\u901a\u8fc7\u7a33\u5065\u7edf\u8ba1\u65b9\u6cd5\u805a\u5408\u591a\u6837\u6a21\u578b\uff0c\u5e76\u5728\u65b0\u8bc1\u636e\u5230\u6765\u65f6\u57fa\u4e8e\u663e\u5f0f\u5148\u9a8c\u4f7f\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u66f4\u65b0\u4fe1\u5ff5\u3002", "result": "\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\uff0c\u76f8\u6bd4\u6700\u4f73\u5355LLM\u57fa\u7ebf\uff0c\u603b\u6210\u672c\u964d\u4f4e34%\uff08294,000\u7f8e\u5143\uff09\uff0c\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u63d0\u534745%\uff08\u6700\u5927\u7fa4\u4f53\u5dee\u8ddd\u4ece22%\u964d\u81f35%\uff09\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u3001\u6210\u672c\u611f\u77e5\u7684\u591aLLM\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLMs\u89c6\u4e3a\u8fd1\u4f3c\u4f3c\u7136\u6a21\u578b\u800c\u975e\u5206\u7c7b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u51b3\u7b56\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u3002"}}
{"id": "2601.01026", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01026", "abs": "https://arxiv.org/abs/2601.01026", "authors": ["Douglas Costa Braga", "Daniel Oliveira Dantas"], "title": "Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation", "comment": "9 pages, 5 figures, 4 tables. Submitted to VISAPP 2025", "summary": "We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u767d\u8840\u75c5\u7ec6\u80de\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\uff08ALL\uff09\u662f\u6700\u5e38\u89c1\u7684\u513f\u7ae5\u764c\u75c7\uff0c\u4f9d\u8d56\u4e13\u5bb6\u663e\u5fae\u955c\u8bca\u65ad\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u548c\u65f6\u95f4\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408EfficientNetV2-B3\u4e0eSqueeze-and-Excitation\u673a\u5236\u7684\u6ce8\u610f\u529b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u3001\u7126\u70b9\u635f\u5931\u51fd\u6570\u548c\u60a3\u8005\u7ea7\u6570\u636e\u5206\u5272\u3002", "result": "\u5728C-NMC 2019\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e8697.89%\u7684F1\u5206\u6570\u548c\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08p < 0.001\uff09\uff0c\u53c2\u6570\u6570\u91cf\u6bd4VGG16\u5c1189%\u3002", "conclusion": "\u73b0\u4ee3\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u53ef\u4ee5\u63d0\u9ad8\u767d\u8840\u75c5\u7ec6\u80de\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u5408\u4e34\u5e8a\u90e8\u7f72\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.02078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02078", "abs": "https://arxiv.org/abs/2601.02078", "authors": ["Chenghao Yin", "Da Huang", "Di Yang", "Jichao Wang", "Nanshu Zhao", "Chen Xu", "Wenjun Sun", "Linjie Hou", "Zhijun Li", "Junhui Wu", "Zhaobo Liu", "Zhen Xiao", "Sheng Zhang", "Lei Bao", "Rui Feng", "Zhenquan Pang", "Jiayu Li", "Qian Wang", "Maoqing Yao"], "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot", "comment": null, "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.", "AI": {"tldr": "Genie Sim 3.0\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u62df\u5e73\u53f0\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u4fdd\u771f\u573a\u666f\u548c\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u6e90\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u7269\u7406\u4e16\u754c\u4e2d\u6570\u636e\u6536\u96c6\u7684\u9ad8\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u6a21\u62df\u57fa\u51c6\u7684\u788e\u7247\u5316\u3001\u8303\u56f4\u72ed\u7a84\u6216\u4fdd\u771f\u5ea6\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86Genie Sim 3.0\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u62df\u5e73\u53f0\uff0c\u4ee5\u53caGenie Sim Generator\uff0c\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6784\u5efa\u9ad8\u4fdd\u771f\u573a\u666f\u3002\u8fd8\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528LLM\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b200\u591a\u4e2a\u4efb\u52a1\u3001\u8d85\u8fc710,000\u5c0f\u65f6\u5408\u6210\u6570\u636e\u7684\u5f00\u6e90\u6570\u636e\u96c6\u3002", "result": "\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u53ef\u6269\u5c55\u7b56\u7565\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u5728\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u4e2d\u7684\u7a33\u5065\u80fd\u529b\uff0c\u8868\u660e\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6709\u6548\u5730\u66ff\u4ee3\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u7b56\u7565\u8bad\u7ec3\u3002"}}
{"id": "2601.01532", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01532", "abs": "https://arxiv.org/abs/2601.01532", "authors": ["Fanzhe Fu"], "title": "Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix", "comment": "6 pages, 2 figures", "summary": "In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify \"Cognitive Conviction\" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a \"cognitive buffer,\" they may exhibit \"Defensive OverThinking\" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProject Aletheia\u6846\u67b6\uff0c\u901a\u8fc7Tikhonov Regularization\u548cSynthetic Proxy Protocol\u91cf\u5316AI\u8ba4\u77e5\u4fe1\u5ff5\u6df1\u5ea6\uff0c\u5e76\u5f15\u5165S_aligned\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u4e3aAI\u79d1\u5b66\u8bda\u4fe1\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u8303\u5f0f\u5728AGI\u53d1\u5c55\u4e2d\u9762\u4e34\u8ba4\u8bc6\u8bba\u5371\u673a\uff0c\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u91cf\u5316\u4fe1\u5ff5\u6df1\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u6d4b\u91cfAI\u7684\u8ba4\u77e5\u4fe1\u5ff5\u3002", "method": "\u91c7\u7528Tikhonov Regularization\u53cd\u8f6c\u8bc4\u59d4\u7684\u6df7\u6dc6\u77e9\u9635\uff0c\u5e76\u901a\u8fc7Synthetic Proxy Protocol\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u907f\u514d\u4f9d\u8d56\u4e0d\u900f\u660e\u7684\u79c1\u6709\u6570\u636e\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u6a21\u578b\u5728\u5bf9\u6297\u538b\u529b\u4e0b\u53ef\u80fd\u8868\u73b0\u51fa\u201c\u9632\u5fa1\u6027\u8fc7\u5ea6\u601d\u8003\u201d\uff0c\u540c\u65f6\u63d0\u51fa\u4e86S_aligned\u6765\u9a8c\u8bc1\u4fe1\u5ff5\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316AI\u8ba4\u77e5\u4fe1\u5ff5\u6df1\u5ea6\u7684\u6846\u67b6Project Aletheia\uff0c\u5e76\u5f15\u5165Aligned Conviction Score\uff08S_aligned\uff09\u6765\u786e\u4fdd\u4fe1\u5ff5\u4e0e\u5b89\u5168\u6027\u4e0d\u51b2\u7a81\uff0c\u4e3a\u8861\u91cfAI\u79d1\u5b66\u8bda\u4fe1\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2601.01036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01036", "abs": "https://arxiv.org/abs/2601.01036", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Kien Nguyen Do Trung", "Duc Dung Nguyen"], "title": "Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising", "comment": null, "summary": "While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.", "AI": {"tldr": "Mono3DV\u901a\u8fc73D\u611f\u77e5\u5339\u914d\u548c\u53bb\u566a\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u3002", "motivation": "\u89e3\u51b3DETR\u7c7b\u67b6\u6784\u5728\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u56e0\u5ffd\u75653D\u5c5e\u6027\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Mono3DV\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a3D\u611f\u77e5\u7684\u4e8c\u5206\u5339\u914d\u7b56\u7565\u30013D\u53bb\u566a\u65b9\u6848\u548c\u53d8\u5206\u67e5\u8be2\u53bb\u566a\u673a\u5236\u3002", "result": "\u5728KITTI 3D\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u672a\u4f7f\u7528\u4efb\u4f55\u5916\u90e8\u6570\u636e\u3002", "conclusion": "Mono3DV\u901a\u8fc7\u5f15\u51653D\u611f\u77e5\u7684\u4e8c\u5206\u5339\u914d\u7b56\u7565\u30013D\u53bb\u566a\u65b9\u6848\u548c\u53d8\u5206\u67e5\u8be2\u53bb\u566a\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2601.02085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02085", "abs": "https://arxiv.org/abs/2601.02085", "authors": ["Meili Sun", "Chunjiang Zhao", "Lichao Yang", "Hao Liu", "Shimin Hu", "Ya Xiong"], "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots", "comment": null, "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.", "AI": {"tldr": "SRR-Net\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u611f\u77e5\u4e0e\u5b9e\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6210\u719f\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u56e0\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u3001\u5b9a\u4f4d\u504f\u5dee\u3001\u7a7a\u6293\u548c\u679c\u5b9e\u6ed1\u843d\u5bfc\u81f4\u7684\u91c7\u6458\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u6545\u969c\u8bca\u65ad\u4e0e\u81ea\u6062\u590d\u6846\u67b6\uff0c\u6838\u5fc3\u4e3aSRR-Net\u591a\u4efb\u52a1\u611f\u77e5\u6a21\u578b\uff0c\u7ed3\u5408\u76f8\u5bf9\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u548c\u65e9\u671f\u4e2d\u6b62\u7b56\u7565\uff0c\u5e76\u5229\u7528\u5fae\u5149\u5b66\u6444\u50cf\u5934\u5b9e\u65f6\u53cd\u9988\u3002", "result": "SRR-Net\u5728\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6210\u719f\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u8fbe0.895\uff08\u8349\u8393\uff09\u548c0.972\uff08\u624b\uff09\uff0c\u5206\u5272\u7cbe\u5ea6\u4e3a0.887\uff08\u8349\u8393\uff09\u548c0.974\uff08\u624b\uff09\uff0c\u6210\u719f\u5ea6\u4f30\u8ba1\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.035\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe163.35 FPS\u3002", "conclusion": "SRR-Net\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u611f\u77e5\u4e0e\u7ea0\u6b63\u63a7\u5236\u7b56\u7565\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u9762\u4e34\u7684\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u3001\u5b9a\u4f4d\u504f\u5dee\u3001\u7a7a\u6293\u548c\u679c\u5b9e\u6ed1\u843d\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6458\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.01546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01546", "abs": "https://arxiv.org/abs/2601.01546", "authors": ["Letian Kong", "Qianran", "Jin", "Renyu Zhang"], "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation", "comment": "39 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\u6539\u5584LLM\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\uff0c\u590d\u6742\u4efb\u52a1\u9700\u4e24\u9636\u6bb5\uff0c\u7b80\u5355\u4efb\u52a1\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65f6\uff0c\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u9700\u6539\u8fdb\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u4e0a\u4e0b\u6587\u5f62\u6210\u548c\u4e0a\u4e0b\u6587\u5bfc\u822a\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u5b9e\u9a8c\uff08\u987a\u5e8f\u8d2d\u4e70\u6e38\u620f\u3001\u4f17\u7b79\u6e38\u620f\u548c\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\uff09\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5728\u56db\u79cdSOTA\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u590d\u6742\u4efb\u52a1\u9700\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7b80\u5355\u4efb\u52a1\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\uff0c\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u4e0a\u4e0b\u6587\u5f62\u6210\u548c\u4e0a\u4e0b\u6587\u5bfc\u822a\uff09\u5bf9\u4e8e\u5b9e\u73b0LLM\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u662f\u5fc5\u8981\u7684\uff0c\u800c\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u548c\u8bca\u65adLLM\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2601.01041", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.01041", "abs": "https://arxiv.org/abs/2601.01041", "authors": ["Xiang Zhang", "Wenliang Weng", "Daoyong Fu", "Ziqiang Li", "Zhangjie Fu"], "title": "Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking", "comment": null, "summary": "Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.", "AI": {"tldr": "MASM\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u548c\u4f2a\u5f71\u8868\u793a\uff0c\u7ea6\u675f\u4f2a\u5f71\u5b50\u7a7a\u95f4\u62df\u5408\uff0c\u63d0\u5347\u8de8\u6570\u636e\u96c6\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5728\u8de8\u6570\u636e\u96c6\u548c\u73b0\u5b9e\u590d\u6742\u573a\u666f\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4e0d\u540c\u4f2a\u9020\u65b9\u6cd5\u5f15\u5165\u7684\u4f2a\u5f71\u5206\u5e03\u9ad8\u5ea6\u591a\u6837\u5316\uff0c\u800c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4f2a\u5f71\u65f6\u6613\u7834\u574f\u5176\u539f\u59cb\u901a\u7528\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u4f2a\u5f71\u5b50\u7a7a\u95f4\u548c\u9009\u62e9\u6027\u5c42\u63a9\u7801\uff08MASM\uff09\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u5212\u5206\u4e3a\u7a33\u5b9a\u7684\u8bed\u4e49\u4e3b\u5b50\u7a7a\u95f4\u548c\u591a\u4e2a\u53ef\u5b66\u4e60\u7684\u4f2a\u5f71\u5b50\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u9009\u62e9\u6027\u5c42\u63a9\u7801\u7b56\u7565\u81ea\u9002\u5e94\u8c03\u8282\u7f51\u7edc\u5c42\u66f4\u65b0\u884c\u4e3a\u3002", "result": "MASM\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u5efa\u6a21\u4e0d\u540c\u4f2a\u9020\u4f2a\u5f71\u6a21\u5f0f\u5e76\u4fdd\u7559\u901a\u7528\u8bed\u4e49\u5b50\u7a7a\u95f4\uff0c\u7ed3\u5408\u6b63\u4ea4\u6027\u548c\u8c31\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MASM\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u8868\u793a\u548c\u4f2a\u9020\u4f2a\u5f71\u8868\u793a\uff0c\u5e76\u7ea6\u675f\u4f2a\u5f71\u5b50\u7a7a\u95f4\u7684\u62df\u5408\u5f3a\u5ea6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u6570\u636e\u96c6\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01569", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01569", "abs": "https://arxiv.org/abs/2601.01569", "authors": ["Maohao Ran", "Zhenglin Wan", "Cooper Lin", "Yanting Zhang", "Hongyu Xin", "Hongwei Fan", "Yibo Xu", "Beier Luo", "Yaxin Zhou", "Wangbo Zhao", "Lijie Yang", "Lang Feng", "Fuchao Yang", "Jingxuan Wu", "Yiqiao Huang", "Chendong Ma", "Dailing Jiang", "Jianbo Deng", "Sihui Han", "Bo An", "Yike Guo", "Jun Song"], "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators", "comment": "32 pages, 14 Figures", "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.", "AI": {"tldr": "CaveAgent\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u548c\u72b6\u6001\u7ba1\u7406\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u4ee3\u7406\u5728\u591a\u8f6e\u4efb\u52a1\u548c\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u591a\u8f6e\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u6f02\u79fb\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Dual-stream Context Architecture\uff0c\u5c06\u72b6\u6001\u7ba1\u7406\u89e3\u8026\u4e3a\u8f7b\u91cf\u7ea7\u7684\u8bed\u4e49\u6d41\u548c\u6301\u4e45\u5316\u7684Python\u8fd0\u884c\u65f6\u6d41\uff0c\u5e76\u5f15\u5165\u4e86Stateful Runtime Management\u673a\u5236\uff0c\u652f\u6301\u590d\u6742Python\u5bf9\u8c61\u7684\u6ce8\u5165\u548c\u68c0\u7d22\u3002", "result": "\u5728Tau$^2$-bench\u548cBFCL\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCaveAgent\u5728\u96f6\u552e\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u63d0\u9ad8\u4e8610.5%\uff0c\u591a\u8f6e\u573a\u666f\u4e0b\u603btoken\u6d88\u8017\u51cf\u5c11\u4e8628.4%\uff0c\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2dtoken\u6d88\u8017\u51cf\u5c11\u4e8659%\u3002", "conclusion": "CaveAgent\u6846\u67b6\u901a\u8fc7\u5c06LLM\u4ece\u6587\u672c\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u64cd\u4f5c\u7b26\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u4ea4\u4e92\u548c\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2601.02125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02125", "abs": "https://arxiv.org/abs/2601.02125", "authors": ["Zhuoxiong Xu", "Xuanchen Li", "Yuhao Cheng", "Fei Xu", "Yichao Yan", "Xiaokang Yang"], "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance", "comment": null, "summary": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5934\u50cf\u9a71\u52a8\u7684\u673a\u5668\u4eba\u6b4c\u5531\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u6620\u5c04\u548c\u60c5\u611f\u52a8\u6001\u8303\u56f4\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b4c\u5531\u8868\u6f14\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5438\u5f15\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u9762\u90e8\u9a71\u52a8\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5bf9\u8bdd\u6216\u6a21\u4eff\u9759\u6001\u8868\u60c5\uff0c\u96be\u4ee5\u6ee1\u8db3\u6b4c\u5531\u4e2d\u8fde\u7eed\u60c5\u611f\u8868\u8fbe\u548c\u4e00\u81f4\u6027\u7684\u9ad8\u8981\u6c42\u3002", "method": "\u5229\u7528\u5d4c\u5165\u5e7f\u6cdb\u4eba\u7c7b\u5148\u9a8c\u7684\u8096\u50cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u751f\u52a8\u7684\u6b4c\u5531\u5934\u50cf\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u7684\u6620\u5c04\u51fd\u6570\u5c06\u8fd9\u4e9b\u9762\u90e8\u7279\u5f81\u4f20\u9012\u5230\u673a\u5668\u4eba\u4e0a\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u60c5\u611f\u52a8\u6001\u8303\u56f4\uff08Emotion Dynamic Range\uff09\u6307\u6807\u6765\u91cf\u5316\u6b4c\u5531\u4e2d\u7684\u60c5\u611f\u4e30\u5bcc\u5ea6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5507\u97f3\u540c\u6b65\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e30\u5bcc\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5934\u50cf\u9a71\u52a8\u7684\u673a\u5668\u4eba\u6b4c\u5531\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u7684\u6620\u5c04\u51fd\u6570\u5c06\u4e30\u5bcc\u7684\u9762\u90e8\u8868\u60c5\u7279\u5f81\u4f20\u9012\u5230\u673a\u5668\u4eba\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b4c\u5531\u8868\u6f14\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5438\u5f15\u529b\u3002"}}
{"id": "2601.01562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01562", "abs": "https://arxiv.org/abs/2601.01562", "authors": ["Mingyu Xu", "Cheng Fang", "Keyue Jiang", "Yuqian Zheng", "Yanghua Xiao", "Baojian Zhou", "Qifang Zhao", "Suhang Zheng", "Xiuwen Zhu", "Jiyang Tang", "Yongchi Zhao", "Yijia Luo", "Zhiqi Bai", "Yuchi Xu", "Wenbo Su", "Wei Wang", "Bing Zhao", "Lin Qu", "Xiaoxiao Xu"], "title": "Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement", "comment": null, "summary": "We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.", "AI": {"tldr": "Logics-STEM\u662f\u4e00\u4e2a\u9488\u5bf9STEM\u63a8\u7406\u4efb\u52a1\u7684\u5148\u8fdb\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9STEM\u9886\u57df\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "Logics-STEM\u901a\u8fc7\u6570\u636e-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5f15\u64ce\u4f18\u5316\uff0c\u5305\u62ec\u6570\u636e\u9636\u6bb5\u76845\u9636\u6bb5\u7cbe\u5fc3\u8bbe\u8ba1\uff08\u6807\u6ce8\u3001\u53bb\u91cd\u3001\u53bb\u6c61\u3001\u84b8\u998f\u548c\u5206\u5c42\u91c7\u6837\uff09\u548c\u7b97\u6cd5\u9636\u6bb5\u7684\u5931\u8d25\u9a71\u52a8\u540e\u8bad\u7ec3\u6846\u67b6\uff08\u5229\u7528\u76ee\u6807\u77e5\u8bc6\u68c0\u7d22\u548c\u6570\u636e\u5408\u6210\uff09\u3002", "result": "Logics-STEM\u5728STEM\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u63d0\u53474.68%\uff0c\u4f18\u4e8e\u5176\u4ed68B\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "Logics-STEM\u5c55\u793a\u4e86\u5c06\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u6570\u636e\u76f8\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u6570\u636e-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5728\u901a\u8fc7\u540e\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2601.01044", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01044", "abs": "https://arxiv.org/abs/2601.01044", "authors": ["Jin Wang", "Angelo De Castro", "Yuxi Zhang", "Lucas Basolli Borsatto", "Yuechen Guo", "Victoria Bastos Primo", "Ana Beatriz Montevecchio Bernardino", "Gota Morota", "Ricardo C Chebel", "Haipeng Yu"], "title": "Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data", "comment": null, "summary": "Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.", "AI": {"tldr": "\u8fc1\u79fb\u5b66\u4e60\u53ef\u6709\u6548\u63d0\u5347\u5c0f\u519c\u573a\u5976\u725b\u4f53\u91cd\u9884\u6d4b\u7cbe\u5ea6\uff0c\u6df1\u5ea6\u56fe\u50cf\u4e0e\u70b9\u4e91\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u5728\u755c\u7267\u4e1a\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u6bd4\u8f83\u6df1\u5ea6\u56fe\u50cf\u4e0e\u70b9\u4e91\u6570\u636e\u5728\u4f53\u91cd\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u89e3\u51b3\u5c0f\u89c4\u6a21\u519c\u573a\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08ConvNeXt\u3001MobileViT\u5904\u7406\u6df1\u5ea6\u56fe\u50cf\uff1bPointNet\u3001DGCNN\u5904\u7406\u70b9\u4e91\u6570\u636e\uff09\uff0c\u5728\u4e09\u4e2a\u89c4\u6a21\u4e0d\u540c\u7684\u519c\u573a\uff081,201\u3001215\u300158\u5934\u725b\uff09\u4e2d\u8bc4\u4f30\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u5728\u5c0f\u519c\u573a\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6e90\u5b66\u4e60\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u8d85\u8fc7\u8054\u5408\u5b66\u4e60\uff1b\u6df1\u5ea6\u56fe\u50cf\u4e0e\u70b9\u4e91\u6a21\u578b\u65e0\u4e00\u81f4\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u5728\u5c0f\u89c4\u6a21\u519c\u573a\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4f53\u91cd\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6df1\u5ea6\u56fe\u50cf\u4e0e\u70b9\u4e91\u6a21\u578b\u6027\u80fd\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u9002\u7528\u4e8e\u6570\u636e\u5171\u4eab\u53d7\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2601.01765", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01765", "abs": "https://arxiv.org/abs/2601.01765", "authors": ["Yao Lu", "Shang Liu", "Hangan Zhou", "Wenji Fang", "Qijun Zhang", "Zhiyao Xie"], "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization", "comment": null, "summary": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.", "AI": {"tldr": "RTL-OPT\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728RTL\u4f18\u5316\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u8bbe\u8ba1\uff0c\u5e76\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u91cf\u5316PPA\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u800c\u975e\u4f18\u5316\u8d28\u91cf\uff08PPA\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728RTL\u4f18\u5316\u4e2d\u7684\u80fd\u529b\u3002", "method": "RTL-OPT\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u8bbe\u8ba1\uff0c\u8986\u76d6\u7ec4\u5408\u903b\u8f91\u3001\u6d41\u6c34\u7ebf\u6570\u636e\u8def\u5f84\u3001\u6709\u9650\u72b6\u6001\u673a\u548c\u5185\u5b58\u63a5\u53e3\u7b49\u7c7b\u522b\uff0c\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u4e00\u5bf9RTL\u4ee3\u7801\uff08\u6b21\u4f18\u7248\u672c\u548c\u4eba\u5de5\u4f18\u5316\u53c2\u8003\u7248\u672c\uff09\uff0c\u5e76\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "RTL-OPT\u80fd\u591f\u6807\u51c6\u5316\u548c\u6709\u610f\u4e49\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u5e76\u91cf\u5316PPA\u6539\u8fdb\u3002", "conclusion": "RTL-OPT\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728RTL\u4f18\u5316\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u8bbe\u8ba1\uff0c\u8986\u76d6\u591a\u79cd\u5b9e\u73b0\u7c7b\u522b\uff0c\u5e76\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u548c\u91cf\u5316PPA\u6539\u8fdb\u3002"}}
{"id": "2601.02184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02184", "abs": "https://arxiv.org/abs/2601.02184", "authors": ["Yuhang Zhang", "S\u00f6ren Schwertfeger"], "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors", "comment": null, "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u6c14\u538b\u4f20\u611f\u7684\u4f4e\u6210\u672c\u5782\u76f4\u5b9a\u4f4d\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\u548c100%\u697c\u5c42\u8bc6\u522b\uff0c\u4f18\u4e8e\u4f20\u7edfSLAM\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86ROS\u517c\u5bb9\u7684\u5b9e\u73b0\u3002", "motivation": "\u5728\u590d\u6742\u7684\u591a\u697c\u5c42\u73af\u5883\u4e2d\uff0c\u51c6\u786e\u7684\u5782\u76f4\u5b9a\u4f4d\u548c\u697c\u5c42\u8bc6\u522b\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9\u6216LiDAR SLAM\u65b9\u6cd5\u5728\u5782\u76f4\u5b9a\u4f4d\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5dee\u5206\u6c14\u538b\u4f20\u611f\u6280\u672f\uff0c\u7ed3\u5408\u56fa\u5b9a\u57fa\u7ad9\u548c\u79fb\u52a8\u4f20\u611f\u5668\u7684\u5b9e\u65f6\u6c14\u538b\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u517c\u5bb9ROS\u7684\u8f6f\u4ef6\u5305\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c01\u95ed\u697c\u68af\u95f4\u548c\u7535\u68af\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u5782\u76f4\u7cbe\u5ea6\uff08RMSE: 0.29 m\uff09\u548c\u5b8c\u7f8e\u7684\u697c\u5c42\u8bc6\u522b\u7387\uff08100%\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5dee\u5206\u6c14\u538b\u4f20\u611f\u7684\u5782\u76f4\u5b9a\u4f4d\u6846\u67b6\u5728\u590d\u6742\u591a\u697c\u5c42\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u5782\u76f4\u7cbe\u5ea6\uff08RMSE: 0.29 m\uff09\u548c100%\u7684\u697c\u5c42\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e3a\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02295", "abs": "https://arxiv.org/abs/2601.02295", "authors": ["Chenyang Ma", "Guangyu Yang", "Kai Lu", "Shitong Xu", "Bill Byrne", "Niki Trigoni", "Andrew Markham"], "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding", "comment": "Project Page: https://dannymcy.github.io/cyclevla/", "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/", "AI": {"tldr": "CycleVLA\u662f\u4e00\u79cd\u4e3b\u52a8\u81ea\u6211\u7ea0\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u5931\u8d25\u548cMBR\u89e3\u7801\u7b56\u7565\u63d0\u5347VLA\u6a21\u578b\u7684\u6267\u884c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5931\u8d25\u68c0\u6d4b\u548c\u7ea0\u6b63\u65b9\u6cd5\u591a\u4e3a\u4e8b\u540e\u5904\u7406\uff0cCycleVLA\u65e8\u5728\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u548c\u7ea0\u6b63\u5931\u8d25\uff0c\u63d0\u5347VLA\u6a21\u578b\u7684\u6267\u884c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "CycleVLA\u7ed3\u5408\u4e86\u8fdb\u5ea6\u611f\u77e5\u7684VLA\u3001VLM-based\u5931\u8d25\u9884\u6d4b\u5668\u548cMBR\u89e3\u7801\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u4e3b\u52a8\u81ea\u6211\u7ea0\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCycleVLA\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\uff0cMBR\u89e3\u7801\u7b56\u7565\u5728\u96f6\u6837\u672c\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CycleVLA\u901a\u8fc7\u6574\u5408\u8fdb\u5ea6\u611f\u77e5\u7684VLA\u3001\u57fa\u4e8eVLM\u7684\u5931\u8d25\u9884\u6d4b\u5668\u548cMBR\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5931\u8d25\u9884\u6d4b\u548c\u4e3b\u52a8\u81ea\u6211\u7ea0\u6b63\u65b9\u9762\u3002"}}
{"id": "2601.01609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01609", "abs": "https://arxiv.org/abs/2601.01609", "authors": ["Albert Sadowski", "Jaros\u0142aw A. Chudziak"], "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration", "comment": null, "summary": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u63a8\u7406\u786e\u5b9a\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u89c4\u5219\u63a8\u7406\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u5c11\u6837\u672c\u63d0\u793a\u3002", "motivation": "\u5728\u9700\u8981\u53ef\u5ba1\u8ba1\u548c\u53ef\u8bc1\u660e\u51b3\u7b56\u7684\u9886\u57df\uff08\u5982\u4e34\u5e8a\u534f\u8bae\u3001\u8bc1\u636e\u89c4\u5219\u3001\u79d1\u5b66\u6807\u51c6\uff09\uff0c\u89c4\u5219\u63a8\u7406\u9700\u517c\u5177\u89e3\u91ca\u7075\u6d3b\u6027\u548c\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002LLMs\u63d0\u4f9b\u7075\u6d3b\u6027\u4f46\u65e0\u6cd5\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u7b26\u53f7\u7cfb\u7edf\u63d0\u4f9b\u4fdd\u8bc1\u4f46\u9700\u7ed3\u6784\u5316\u8f93\u5165\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u6a21\u5f0f\uff0c\u5c06LLMs\u7528\u4f5c\u672c\u4f53\u586b\u5145\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3aABox\u65ad\u8a00\uff0c\u540c\u65f6\u5229\u7528\u57fa\u4e8eSWRL\u7684\u63a8\u7406\u5668\u5e94\u7528\u89c4\u5219\u3002\u6846\u67b6\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u5b9e\u4f53\u8bc6\u522b\u3001\u65ad\u8a00\u63d0\u53d6\u548c\u7b26\u53f7\u9a8c\u8bc1\uff0c\u4efb\u52a1\u5b9a\u4e49\u57fa\u4e8eOWL 2\u672c\u4f53\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6cd5\u5f8b\u4f20\u95fb\u5224\u5b9a\u3001\u79d1\u5b66\u65b9\u6cd5\u4efb\u52a1\u5e94\u7528\u3001\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\uff09\u548c\u5341\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7ed3\u6784\u5316\u5206\u89e3\u5728\u603b\u4f53\u4e0a\u663e\u8457\u4f18\u4e8e\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4e14\u7b26\u53f7\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u989d\u5916\u6536\u76ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7075\u6d3b\u6027\u548c\u57fa\u4e8eSWRL\u7684\u63a8\u7406\u5668\u7684\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u89c4\u5219\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6cd5\u5f8b\u3001\u79d1\u5b66\u3001\u4e34\u5e8a\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.01056", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01056", "abs": "https://arxiv.org/abs/2601.01056", "authors": ["Ifeanyi Ezuma", "Ugochukwu Ugwu"], "title": "Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance", "comment": "10 pages, 8 figures. Code and datasets available upon request", "summary": "The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\\% and an average AUC of 96.8\\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\\% and accuracy of 99.84\\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5fae\u8c03\u7684InceptionResNet-v2\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u75c5\u7406\u5b66\u7684\u53d1\u5c55\uff0c\u81ea\u52a8\u5316\u7684\u56fe\u50cf\u5206\u6790\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u5fae\u8c03\u7684InceptionResNet-v2\u7f51\u7edc\u4f5c\u4e3a\u5206\u7c7b\u5668\u548c\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u5728LC25000\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u7684InceptionResNet-v2\u7f51\u7edc\u8fbe\u5230\u4e8696.01%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c96.8%\u7684\u5e73\u5747AUC\u3002\u4f7f\u7528\u6df1\u5ea6\u7279\u5f81\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5176\u4e2d\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fbe\u5230\u4e8699.99%\u7684AUC\u548c99.84%\u7684\u51c6\u786e\u7387\u3002\u5728\u566a\u58f0\u73af\u5883\u4e0b\uff0cGBM\u548cKNN\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684InceptionResNet-v2\u7f51\u7edc\u5728LC25000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u540e\uff0c\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.01718", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01718", "abs": "https://arxiv.org/abs/2601.01718", "authors": ["YuanLab. ai", ":", "Shawn Wu", "Sean Wang", "Louie Li", "Darcy Chen", "Allen Wang", "Jiangang Luo", "Xudong Zhao", "Joseph Shen", "Gawain Ma", "Jasper Jia", "Marcus Mao", "Claire Wang", "Hunter He", "Carol Wang", "Zera Zhang", "Jason Wang", "Chonly Shen", "Leo Zhang", "Logan Chen", "Qasim Meng", "James Gong", "Danied Zhao", "Penn Zheng", "Owen Zhu", "Tong Yu"], "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications", "comment": null, "summary": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.", "AI": {"tldr": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001MoE\u6a21\u578b\uff0c\u901a\u8fc7RAPO\u7b97\u6cd5\u89e3\u51b3\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4f01\u4e1a\u4efb\u52a1\u548c\u901a\u7528\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u9ad8\u6548\u8282\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4e2d\u5e38\u89c1\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5e76\u63d0\u5347\u4f01\u4e1a\u4efb\u52a1\u548c\u901a\u7528\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u63d0\u51faReflection-aware Adaptive Policy Optimization\uff08RAPO\uff09\u7b97\u6cd5\u6765\u8c03\u8282\u8fc7\u5ea6\u601d\u8003\u884c\u4e3a\u3002", "result": "\u5728\u4f01\u4e1a\u4efb\u52a1\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u590d\u6742\u8868\u683c\u7406\u89e3\u548c\u6458\u8981\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u9886\u57df\u5c55\u73b0\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u97001/4\u81f31/2\u7684\u5e73\u5747token\u5373\u53ef\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u4f01\u4e1a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u540c\u65f6\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5df2\u5b8c\u5168\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2601.01064", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01064", "abs": "https://arxiv.org/abs/2601.01064", "authors": ["Jianan Li", "Wangcai Zhao", "Tingfa Xu"], "title": "Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers", "comment": null, "summary": "Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.", "AI": {"tldr": "LSST\u662f\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u8d85\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\u548c\u521b\u65b0\u7684\u635f\u5931\u673a\u5236\uff0c\u9ad8\u6548\u5904\u7406\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\u3002", "motivation": "\u8d85\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ece\u538b\u7f29\u611f\u77e5\u6d4b\u91cf\u4e2d\u9ad8\u6548\u91cd\u5efa\u8d85\u5149\u8c31\u56fe\u50cf\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\uff0c\u7ed3\u5408Separate Spectral Transformer Blocks\uff08SSTB\uff09\u548cLightweight Spatial Convolution Blocks\uff08LSCB\uff09\uff0c\u5206\u522b\u5904\u7406\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\u3002SSTB\u4f7f\u7528Grouped Spectral Self-attention\u548cSpectrum Shuffle\u64cd\u4f5c\uff0cLSCB\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3002", "result": "LSST\u5728\u51cf\u5c11FLOPs\u548c\u53c2\u6570\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "LSST\u67b6\u6784\u901a\u8fc7\u5176\u8f7b\u91cf\u5316\u7684\u8bbe\u8ba1\u548c\u521b\u65b0\u7684\u635f\u5931\u673a\u5236\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d85\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u3002"}}
{"id": "2601.01210", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01210", "abs": "https://arxiv.org/abs/2601.01210", "authors": ["Kazuhiko Murasaki", "Shunsuke Konagai", "Masakatsu Aoki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission", "comment": null, "summary": "To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u901fLiDAR\u70b9\u4e91\u589e\u5bc6\u65b9\u6cd5\uff0c\u7ed3\u5408LiDAR\u4e0e\u5f69\u8272\u56fe\u50cf\uff0c\u901a\u8fc7CNN\u5b9e\u73b0\u5b9e\u65f6\u5bc6\u96c6\u6df1\u5ea6\u56fe\u751f\u6210\uff0c\u901f\u5ea6\u63d0\u534715\u500d\uff0c\u51e0\u4f55\u51c6\u786e\u65e0\u4f2a\u5f71\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u5448\u73b0\u7684\u4f4e\u5ef6\u8fdf\u7a7a\u95f4\u4f20\u8f93\u7cfb\u7edf\uff0c\u9700\u8981\u89e3\u51b3\u52a8\u60013D\u573a\u666f\u7684\u5bc6\u96c6\u6355\u83b7\u548c\u5b9e\u65f6\u5904\u7406\u95ee\u9898\u3002LiDAR\u4f20\u611f\u5668\u867d\u7136\u80fd\u5b9e\u65f6\u6355\u83b73D\u6570\u636e\uff0c\u4f46\u751f\u6210\u7684\u70b9\u4e91\u7a00\u758f\u3002", "method": "\u7ed3\u5408\u591a\u4e2aLiDAR\u8f93\u5165\u4e0e\u9ad8\u5206\u8fa8\u7387\u5f69\u8272\u56fe\u50cf\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u8054\u5408\u53cc\u8fb9\u6ee4\u6ce2\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u5168\u9ad8\u6e05\u5206\u8fa8\u7387\u5b9e\u65f6\uff0830 fps\uff09\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u901f\u5ea6\u6bd4\u6700\u8fd1\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5feb15\u500d\u4ee5\u4e0a\uff0c\u4e14\u751f\u6210\u7684\u5bc6\u96c6\u70b9\u4e91\u51e0\u4f55\u51c6\u786e\uff0c\u65e0\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6216\u91cd\u5f71\u4f2a\u5f71\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u901fLiDAR\u70b9\u4e91\u589e\u5bc6\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u7a7a\u95f4\u4f20\u8f93\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u60013D\u573a\u666f\u5bc6\u96c6\u6355\u83b7\u548c\u5b9e\u65f6\u5904\u7406\u7684\u96be\u9898\u3002"}}
{"id": "2601.01743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01743", "abs": "https://arxiv.org/abs/2601.01743", "authors": ["Bin Xu"], "title": "AI Agent Systems: Architectures, Applications, and Evaluation", "comment": null, "summary": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86AI\u4ee3\u7406\u67b6\u6784\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u6743\u8861\u3001\u8bc4\u4f30\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\uff08\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u4e0e\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u7684\u7cfb\u7edf\uff09\u6210\u4e3a\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u4e0e\u771f\u5b9e\u4e16\u754c\u8ba1\u7b97\u4e4b\u95f4\u7684\u5b9e\u7528\u63a5\u53e3\uff0c\u7406\u89e3\u5176\u67b6\u6784\u548c\u6311\u6218\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7efc\u5408\u73b0\u6709\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4ee3\u7406\u7ec4\u4ef6\u3001\u534f\u8c03\u6a21\u5f0f\u548c\u90e8\u7f72\u8bbe\u7f6e\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u6743\u8861\u548c\u8bc4\u4f30\u590d\u6742\u6027\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86AI\u4ee3\u7406\u67b6\u6784\u7684\u5173\u952e\u7ec4\u4ef6\u548c\u534f\u8c03\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u8bbe\u8ba1\u6743\u8861\u548c\u8bc4\u4f30\u6311\u6218\u3002", "conclusion": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86AI\u4ee3\u7406\u67b6\u6784\u7684\u5173\u952e\u8bbe\u8ba1\u6743\u8861\u548c\u8bc4\u4f30\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5982\u5de5\u5177\u52a8\u4f5c\u7684\u9a8c\u8bc1\u4e0e\u9632\u62a4\u3001\u53ef\u6269\u5c55\u7684\u5185\u5b58\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u4ee3\u7406\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u5728\u73b0\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u53ef\u91cd\u590d\u8bc4\u4f30\u3002"}}
{"id": "2601.01084", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01084", "abs": "https://arxiv.org/abs/2601.01084", "authors": ["Adari Rama Sukanya", "Puvvula Roopesh Naga Sri Sai", "Kota Moses", "Rimalapudi Sarvendranath"], "title": "A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields", "comment": "10-page dataset explanation paper", "summary": "We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5370\u5ea6\u6c34\u7a3b\u7530\u7684\u9ad8\u5206\u8fa8\u7387RGB\u548c\u591a\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8986\u76d6\u6240\u6709\u751f\u957f\u9636\u6bb5\uff0c\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u7814\u7a76\u3002", "motivation": "\u586b\u8865\u5370\u5ea6\u6c34\u7a3b\u7530\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\u5982\u9776\u5411\u55b7\u6d12\u3001\u75c5\u5bb3\u5206\u6790\u548c\u4ea7\u91cf\u4f30\u7b97\u3002", "method": "\u4f7f\u752820\u5146\u50cf\u7d20RGB\u76f8\u673a\u548c5\u5146\u50cf\u7d20\u56db\u6ce2\u6bb5\u591a\u5149\u8c31\u76f8\u673a\u91c7\u96c6\u56fe\u50cf\uff0c\u5f00\u53d1\u4e86\u6807\u51c6\u5316\u64cd\u4f5c\u6d41\u7a0b\uff08SOP\uff09\u548c\u68c0\u67e5\u8868\u4ee5\u786e\u4fdd\u6570\u636e\u91c7\u96c6\u7684\u53ef\u91cd\u590d\u6027\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b42,430\u5f20\u539f\u59cb\u56fe\u50cf\uff08415 GB\uff09\u7684\u6570\u636e\u96c6\uff0c\u5177\u67091\u5398\u7c73/\u50cf\u7d20\u7684\u5730\u9762\u91c7\u6837\u8ddd\u79bb\uff08GSD\uff09\u548c\u4e30\u5bcc\u7684\u5143\u6570\u636e\uff0c\u5982GPS\u5750\u6807\u3001\u98de\u884c\u9ad8\u5ea6\u548c\u73af\u5883\u6761\u4ef6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5370\u5ea6\u6c34\u7a3b\u7530\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u7387\u7684RGB\u548c\u591a\u5149\u8c31\u56fe\u50cf\uff0c\u8986\u76d6\u4e86\u4ece\u80b2\u82d7\u5230\u6536\u83b7\u7684\u6240\u6709\u751f\u957f\u9636\u6bb5\uff0c\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u7814\u7a76\u3002"}}
{"id": "2601.01085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01085", "abs": "https://arxiv.org/abs/2601.01085", "authors": ["Jiayi Xu", "Zhang Zhang", "Yuanrui Zhang", "Ruitao Chen", "Yixian Xu", "Tianyu He", "Di He"], "title": "Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models", "comment": null, "summary": "In this paper, we introduce \\emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \\emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.", "AI": {"tldr": "Luminark\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u89c6\u89c9\u751f\u6210\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7ea7\u4eae\u5ea6\u7edf\u8ba1\u5b9e\u73b0\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u89c6\u89c9\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u901a\u7528\u4e14\u80fd\u4fdd\u8bc1\u56fe\u50cf\u8d28\u91cf\u7684\u6c34\u5370\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5757\u7ea7\u4eae\u5ea6\u7edf\u8ba1\u5b9a\u4e49\u6c34\u5370\uff0c\u670d\u52a1\u63d0\u4f9b\u5546\u9884\u5b9a\u4e49\u4e8c\u8fdb\u5236\u6a21\u5f0f\u53ca\u5bf9\u5e94\u7684\u5757\u7ea7\u9608\u503c\u3002\u68c0\u6d4b\u65f6\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u5757\u7684\u4eae\u5ea6\u662f\u5426\u8d85\u8fc7\u9608\u503c\uff0c\u5e76\u9a8c\u8bc1\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6a21\u5f0f\u662f\u5426\u4e0e\u76ee\u6807\u4e00\u81f4\u3002\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u63a7\u5236\u8bef\u68c0\u7387\uff0c\u786e\u4fdd\u8ba4\u8bc1\u68c0\u6d4b\u3002", "result": "\u5728\u4e5d\u79cd\u6a21\u578b\uff08\u6269\u6563\u3001\u81ea\u56de\u5f52\u548c\u6df7\u5408\u6846\u67b6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLuminark\u5177\u6709\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u5f3a\u9c81\u68d2\u6027\u548c\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "Luminark\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u5177\u6709\u6982\u7387\u8ba4\u8bc1\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u65b0\u9896\u7684\u6c34\u5370\u5b9a\u4e49\u548c\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u5f3a\u9c81\u68d2\u6027\u548c\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2601.01528", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01528", "abs": "https://arxiv.org/abs/2601.01528", "authors": ["Yang Zhou", "Hao Shao", "Letian Wang", "Zhuofan Zong", "Hongsheng Li", "Steven L. Waslander"], "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving", "comment": "10 pages, 4 figures; Project Website: https://drivinggen-bench.github.io/", "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.", "AI": {"tldr": "DrivingGen\u662f\u9996\u4e2a\u7528\u4e8e\u751f\u6210\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u4e25\u683c\u7684\u57fa\u51c6\u6765\u8861\u91cf\u8fdb\u5c55\u548c\u6307\u5bfc\u4f18\u5148\u7ea7\u3002\u73b0\u6709\u8bc4\u4f30\u5ffd\u7565\u4e86\u5b89\u5168\u5173\u952e\u7684\u6210\u50cf\u56e0\u7d20\u3001\u8f68\u8ff9\u5408\u7406\u6027\u3001\u65f6\u95f4\u548c\u4ee3\u7406\u7ea7\u522b\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u81ea\u6211\u6761\u4ef6\u7684\u53ef\u63a7\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6765\u81ea\u9a7e\u9a76\u6570\u636e\u96c6\u548c\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891\u6e90\u7684\u591a\u6837\u5316\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u5957\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5171\u540c\u8bc4\u4f30\u89c6\u89c9\u771f\u5b9e\u6027\u3001\u8f68\u8ff9\u5408\u7406\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u3002", "result": "\u5bf914\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u660e\u663e\u7684\u6743\u8861\uff1a\u901a\u7528\u6a21\u578b\u770b\u8d77\u6765\u66f4\u597d\u4f46\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\uff0c\u800c\u9a7e\u9a76\u4e13\u7528\u6a21\u578b\u80fd\u771f\u5b9e\u6355\u6349\u8fd0\u52a8\u4f46\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u843d\u540e\u3002", "conclusion": "DrivingGen\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63a8\u52a8\u53ef\u9760\u3001\u53ef\u63a7\u4e14\u53ef\u90e8\u7f72\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u4eff\u771f\u3001\u89c4\u5212\u548c\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u3002"}}
{"id": "2601.01774", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01774", "abs": "https://arxiv.org/abs/2601.01774", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches", "comment": "14 pages", "summary": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.", "AI": {"tldr": "LLMs\u5728\u7b26\u53f7\u64cd\u4f5c\u548c\u77e5\u8bc6\u68c0\u7d22\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6570\u503c\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u66f4\u9002\u5408\u4f5c\u4e3a\u7ecf\u5178\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u76f4\u63a5\u6570\u503c\u9884\u6d4b\u6216\u7ed3\u5408\u7b26\u53f7\u64cd\u4f5c\u4e0e\u7ecf\u5178\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u6df7\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u5de5\u7a0b\u4e2d\u7684\u8d85\u8d8a\u65b9\u7a0b\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u516d\u79cd\u5148\u8fdb\u6a21\u578b\uff08GPT-5.1\u3001GPT-5.2\u3001Gemini-3-Flash\u3001Gemini-2.5-Lite\u3001Claude-Sonnet-4.5\u3001Claude-Opus-4.5\uff09\u5728100\u4e2a\u8de8\u4e03\u4e2a\u5de5\u7a0b\u9886\u57df\u7684\u95ee\u9898\u4e0a\uff0c\u6bd4\u8f83\u76f4\u63a5\u9884\u6d4b\u4e0e\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "result": "\u76f4\u63a5\u9884\u6d4b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a0.765\u81f31.262\uff0c\u800c\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\u7684\u8bef\u5dee\u4e3a0.225\u81f30.301\uff0c\u8bef\u5dee\u51cf\u5c1167.9%\u81f381.8%\u3002\u7535\u5b50\u5b66\u9886\u57df\u6539\u5584\u663e\u8457\uff0893.1%\uff09\uff0c\u6d41\u4f53\u529b\u5b66\u9886\u57df\u6539\u5584\u8f83\u5c0f\uff087.2%\uff09\u3002", "conclusion": "\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u64c5\u957f\u7b26\u53f7\u64cd\u4f5c\u548c\u9886\u57df\u77e5\u8bc6\u68c0\u7d22\uff0c\u4f46\u5728\u7cbe\u5ea6\u5173\u952e\u7684\u8fed\u4ee3\u7b97\u672f\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u7ecf\u5178\u6570\u503c\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u800c\u975e\u72ec\u7acb\u8ba1\u7b97\u5f15\u64ce\u3002"}}
{"id": "2601.01088", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01088", "abs": "https://arxiv.org/abs/2601.01088", "authors": ["Haq Nawaz Malik"], "title": "600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script", "comment": null, "summary": "This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.", "AI": {"tldr": "600K-KS-OCR\u6570\u636e\u96c6\u586b\u8865\u514b\u4ec0\u7c73\u5c14\u8bedOCR\u8d44\u6e90\u7a7a\u767d\uff0c\u5305\u542b60\u4e07\u5355\u8bcd\u7ea7\u56fe\u50cf\uff0c\u652f\u6301\u591a\u79cdOCR\u6a21\u578b\u8bad\u7ec3\uff0c\u91c7\u7528CC-BY-4.0\u8bb8\u53ef\u8bc1\u3002", "motivation": "\u89e3\u51b3\u514b\u4ec0\u7c73\u5c14\u8bed\u5728\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u9886\u57df\u7684\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684OCR\u7814\u7a76\u3002", "method": "\u751f\u6210\u65b9\u6cd5\u91c7\u7528\u4e86\u4e09\u79cd\u4f20\u7edf\u7684\u514b\u4ec0\u7c73\u5c14\u5b57\u4f53\uff0c\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u6a21\u62df\u771f\u5b9e\u6587\u6863\u9000\u5316\uff0c\u4ee5\u53ca\u591a\u6837\u5316\u7684\u80cc\u666f\u7eb9\u7406\u4ee5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u6570\u636e\u96c6\u4ee5\u5341\u4e2a\u5206\u533a\u5b58\u6863\u5f62\u5f0f\u5206\u53d1\uff0c\u603b\u8ba1\u7ea610.6 GB\uff0c\u91c7\u7528CC-BY-4.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4fbf\u4e8e\u7814\u7a76\u4f7f\u7528\u3002", "conclusion": "\u8be5\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86600K-KS-OCR\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5408\u6210\u8bed\u6599\u5e93\uff0c\u5305\u542b\u7ea6602,000\u4e2a\u5355\u8bcd\u7ea7\u5206\u5272\u56fe\u50cf\uff0c\u65e8\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u9488\u5bf9\u514b\u4ec0\u7c73\u5c14\u6587\u5b57\u7684\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u7cfb\u7edf\u3002\u6570\u636e\u96c6\u586b\u8865\u4e86\u514b\u4ec0\u7c73\u5c14\u8bed\uff08\u4e00\u79cd\u6fd2\u5371\u7684\u8fbe\u5c14\u5fb7\u8bed\uff0c\u4f7f\u7528\u4fee\u6539\u540e\u7684\u6ce2\u65af-\u963f\u62c9\u4f2f\u4e66\u5199\u7cfb\u7edf\uff0c\u7ea6\u4e03\u767e\u4e07\u4eba\u4f7f\u7528\uff09\u7684\u5173\u952e\u8d44\u6e90\u7f3a\u53e3\u3002"}}
{"id": "2601.01696", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01696", "abs": "https://arxiv.org/abs/2601.01696", "authors": ["Yian Liu", "Xiong Wang", "Ping Xu", "Lei Zhu", "Ming Yan", "Linyun Xue"], "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems", "comment": null, "summary": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.", "AI": {"tldr": "\u63d0\u51faCDO\u6a21\u5757\u4f18\u5316\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u8f66\u9053\u68c0\u6d4b\uff0c\u63d0\u5347\u7cbe\u5ea6\u4e14\u6613\u4e8e\u96c6\u6210\uff0c\u5b9e\u9a8c\u663e\u793a\u7cbe\u5ea6\u63d0\u53470.01%-1.5%\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u8f66\u9053\u68c0\u6d4b\u9762\u4e34\u89c6\u89c9\u4fe1\u53f7\u7a00\u758f\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u9488\u5bf9\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u73af\u5883\u7684\u901a\u7528\u4f18\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u534f\u65b9\u5dee\u5206\u5e03\u4f18\u5316\uff08CDO\uff09\u6a21\u5757\uff0c\u4e13\u95e8\u4e3a\u9ad8\u6548\u5b9e\u65f6\u5e94\u7528\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7d27\u5bc6\u5bf9\u9f50\u8f66\u9053\u7279\u5f81\u5206\u5e03\u4e0e\u771f\u5b9e\u6807\u7b7e\u6765\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\uff08CULane\u3001TuSimple\u548cLLAMAS\uff09\u4e0a\u6d4b\u8bd5\u4e86\u516d\u79cd\u6a21\u578b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7cbe\u5ea6\u63d0\u5347\u8303\u56f4\u57280.01%\u81f31.5%\u4e4b\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684CDO\u6a21\u5757\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6027\u80fd\u3001\u80fd\u6548\u548c\u64cd\u4f5c\u7075\u6d3b\u6027\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.01802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01802", "abs": "https://arxiv.org/abs/2601.01802", "authors": ["Qianjun Pan", "Junyi Wang", "Jie Zhou", "Yutao Yang", "Junsong Li", "Kaiyin Xu", "Yougen Zhou", "Yihan Li", "Jingyuan Zhao", "Qin Chen", "Ningning Zhou", "Kai Chen", "Liang He"], "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor", "comment": null, "summary": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.", "AI": {"tldr": "PsychEval\u662f\u4e00\u4e2a\u591a\u4f1a\u8bdd\u3001\u591a\u7597\u6cd5\u7684\u9ad8\u771f\u5b9e\u611fAI\u5fc3\u7406\u54a8\u8be2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86AI\u54a8\u8be2\u5e08\u7684\u771f\u5b9e\u6027\u3001\u591a\u7597\u6cd5\u8bad\u7ec3\u548c\u7cfb\u7edf\u6027\u8bc4\u4f30\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684AI\u5fc3\u7406\u8bc4\u4f30\u5de5\u5177\uff0c\u89e3\u51b3\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8bad\u7ec3\u9ad8\u771f\u5b9e\u611f\u7684AI\u54a8\u8be2\u5e08\u3001\u591a\u7597\u6cd5AI\u54a8\u8be2\u5e08\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7cfb\u7edf\u6027\u8bc4\u4f30AI\u54a8\u8be2\u5e08\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4f1a\u8bdd\u3001\u591a\u7597\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5PsychEval\uff0c\u5305\u542b6-10\u4e2a\u4f1a\u8bdd\u3001\u4e09\u79cd\u4e0d\u540c\u9636\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86677\u79cd\u5143\u6280\u80fd\u548c4577\u79cd\u539f\u5b50\u6280\u80fd\uff0c\u8986\u76d6\u4e94\u79cd\u6cbb\u7597\u6a21\u5f0f\u548c\u516d\u79cd\u6838\u5fc3\u5fc3\u7406\u4e3b\u9898\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cf\u548c\u4e34\u5e8a\u4fdd\u771f\u5ea6\uff0c\u6784\u5efa\u4e862000\u591a\u4e2a\u591a\u6837\u5316\u7684\u5ba2\u6237\u6863\u6848\u548c18\u4e2a\u6cbb\u7597\u7279\u5b9a\u53ca\u5171\u4eab\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "PsychEval\u4f5c\u4e3a\u4e00\u4e2a\u9ad8\u4fdd\u771f\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u80fd\u652f\u6301\u4e34\u5e8a\u8d23\u4efb\u611f\u548c\u9002\u5e94\u6027AI\u54a8\u8be2\u5e08\u7684\u81ea\u6211\u8fdb\u5316\u8bad\u7ec3\u3002"}}
{"id": "2601.01095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01095", "abs": "https://arxiv.org/abs/2601.01095", "authors": ["Hyeonjeong Ha", "Jinjin Ge", "Bo Feng", "Kaixin Ma", "Gargi Chakraborty"], "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame", "comment": "VideoLLM Fine-Grained Evaluation", "summary": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.", "AI": {"tldr": "NarrativeTrack\u662f\u9996\u4e2a\u8bc4\u4f30MLLMs\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u7406\u89e3\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u5176\u5728\u5b9e\u4f53\u8ddf\u8e2a\u548c\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u611f\u77e5\u4e0e\u65f6\u95f4\u63a8\u7406\u6574\u5408\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u7d22MLLMs\u5728\u89c6\u9891\u4e2d\u7406\u89e3\u65f6\u95f4\u5c55\u5f00\u53d9\u4e8b\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7Compositional Reasoning Progression (CRP)\u6846\u67b6\uff0c\u9010\u6b65\u589e\u52a0\u53d9\u4e8b\u590d\u6742\u6027\uff0c\u8bc4\u4f30MLLMs\u5728\u5b9e\u4f53\u5b58\u5728\u3001\u53d8\u5316\u548c\u6a21\u7cca\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u8f6c\u6362\u548c\u65f6\u95f4\u52a8\u6001\u4e2d\u65e0\u6cd5\u7a33\u5065\u8ddf\u8e2a\u5b9e\u4f53\uff0c\u5e38\u51fa\u73b0\u4e0a\u4e0b\u6587\u504f\u79fb\u4e0b\u7684\u8eab\u4efd\u5e7b\u89c9\u3002\u5f00\u6e90\u901a\u7528MLLMs\u611f\u77e5\u57fa\u7840\u5f3a\u4f46\u65f6\u95f4\u8fde\u8d2f\u6027\u5f31\uff0c\u89c6\u9891\u4e13\u7528MLLMs\u5219\u76f8\u53cd\u3002", "conclusion": "NarrativeTrack\u63ed\u793a\u4e86MLLMs\u5728\u53d9\u4e8b\u7406\u89e3\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff1a\u611f\u77e5\u57fa\u7840\u4e0e\u65f6\u95f4\u63a8\u7406\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53ea\u6709\u4e24\u8005\u7684\u6574\u5408\u624d\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u53d9\u4e8b\u7406\u89e3\u3002"}}
{"id": "2601.01989", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01989", "abs": "https://arxiv.org/abs/2601.01989", "authors": ["Aly R. Elkammar", "Karim M. Gamaleldin", "Catherine M. Elias"], "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis", "comment": null, "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728JAAD\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u8d85\u8d8aSOTA\u3002", "motivation": "\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u662f\u81ea\u52a8\u9a7e\u9a76\u4eceL3\u5411L4\u8fc7\u6e21\u7684\u5173\u952e\u6280\u672f\uff0c\u7406\u89e3\u884c\u4eba\u8fc7\u8857\u884c\u4e3a\u5bf9\u63d0\u5347\u9053\u8def\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e0d\u540c\u89c4\u6a21\u7684Transformer/Video Vision Transformer\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6a21\u6001\u8fdb\u884c\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u3002", "result": "\u5728JAAD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u5728\u51c6\u786e\u7387\u3001AUC\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u57fa\u4e8eTransformer/Video Vision Transformer\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6a21\u6001\uff0c\u5728\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728JAAD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002"}}
{"id": "2601.01816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01816", "abs": "https://arxiv.org/abs/2601.01816", "authors": ["Chris Duffey"], "title": "Admissibility Alignment", "comment": "24 pages, 2 figures, 2 tables.. Decision-theoretic alignment under uncertainty", "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Admissibility Alignment\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7MAP-AI\u7cfb\u7edf\u67b6\u6784\u5b9e\u73b0\uff0c\u5c06AI\u5bf9\u9f50\u89c6\u4e3a\u6982\u7387\u6027\u548c\u51b3\u7b56\u7406\u8bba\u5c5e\u6027\uff0c\u800c\u975e\u9759\u6001\u6216\u4e8c\u5143\u6761\u4ef6\u3002", "motivation": "\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u7ed3\u679c\u5206\u5e03\u7684\u53ef\u63a5\u53d7\u884c\u52a8\u548c\u51b3\u7b56\u9009\u62e9\u7684\u5c5e\u6027\uff0c\u901a\u8fc7\u5019\u9009\u653f\u7b56\u7684\u884c\u4e3a\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86MAP-AI\uff08Monte Carlo Alignment for Policy\uff09\u4f5c\u4e3a\u5b9e\u73b0Admissibility Alignment\u7684\u89c4\u8303\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7ed3\u679c\u5206\u5e03\u548c\u53ef\u63a5\u53d7\u6027\u63a7\u5236\u7684\u653f\u7b56\u9009\u62e9\u6765\u5f3a\u5236\u6267\u884c\u5bf9\u9f50\u3002", "result": "MAP-AI\u6846\u67b6\u901a\u8fc7\u8bc4\u4f30\u51b3\u7b56\u653f\u7b56\u5728\u591a\u4e2a\u53ef\u80fd\u672a\u6765\u4e2d\u7684\u8868\u73b0\uff0c\u660e\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u679c\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6267\u884c\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4f01\u4e1a\u53ca\u673a\u6784AI\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb\u548c\u5bf9\u9f50\u3002", "conclusion": "Admissibility Alignment\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u6cbb\u7406\u90a3\u4e9b\u5f71\u54cd\u7531\u653f\u7b56\u884c\u4e3a\u5728\u5206\u5e03\u548c\u5c3e\u90e8\u4e8b\u4ef6\u4e2d\u51b3\u5b9a\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2601.01099", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01099", "abs": "https://arxiv.org/abs/2601.01099", "authors": ["Mahmudul Hasan", "Mabsur Fatin Bin Hossain"], "title": "Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks", "comment": null, "summary": "This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.", "AI": {"tldr": "\u81ea\u5b9a\u4e49CNN\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u79cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6bd4\u8f83\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5c42CNN\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u8f7b\u91cf\u6a21\u578b\u9002\u7528\u4e8e\u7b80\u5355\u4efb\u52a1\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7f51\u7edc\u9009\u62e9\u6307\u5bfc\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540cCNN\u67b6\u6784\u5728\u591a\u79cd\u56fe\u50cf\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e8c\u5143\u5206\u7c7b\u3001\u7ec6\u7c92\u5ea6\u591a\u7c7b\u8bc6\u522b\u548c\u7269\u4f53\u68c0\u6d4b\u573a\u666f\uff09\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7f51\u7edc\u8bbe\u8ba1\u9009\u62e9\u7684\u4f9d\u636e\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u5e76\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u548c\u8fc1\u79fb\u5b66\u4e60CNN\u6a21\u578b\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002\u5206\u6790\u4e86\u7f51\u7edc\u6df1\u5ea6\u3001\u6b8b\u5dee\u8fde\u63a5\u548c\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u7b49\u67b6\u6784\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u5206\u7c7b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u66f4\u6df1\u5c42\u7684CNN\u67b6\u6784\u5728\u7ec6\u7c92\u5ea6\u591a\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u800c\u8f7b\u91cf\u7ea7\u9884\u8bad\u7ec3\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u5728\u7b80\u5355\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u4ecd\u7136\u975e\u5e38\u6709\u6548\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u5728\u7269\u4f53\u68c0\u6d4b\u573a\u666f\u4e2d\u4e5f\u5c55\u793a\u4e86\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u81ea\u5b9a\u4e49CNN\u67b6\u6784\u4e0e\u9884\u8bad\u7ec3\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u4e3a\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u548c\u8d44\u6e90\u9650\u5236\u9009\u62e9\u5408\u9002\u7684\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2601.01836", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01836", "abs": "https://arxiv.org/abs/2601.01836", "authors": ["Dasol Choi", "DongGeon Lee", "Brigitta Jesica Kartono", "Helena Berndt", "Taeyoun Kwon", "Joonwon Jang", "Haon Park", "Hwanjo Yu", "Minsuk Kahng"], "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs", "comment": null, "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.", "AI": {"tldr": "COMPASS\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30LLMs\u662f\u5426\u7b26\u5408\u7ec4\u7ec7\u653f\u7b56\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6267\u884c\u7981\u6b62\u653f\u7b56\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u5176\u5728\u4f01\u4e1aAI\u5b89\u5168\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u786e\u4fdd\u5176\u9075\u5b88\u7ec4\u7ec7\u7279\u5b9a\u653f\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4ec5\u5173\u6ce8\u666e\u904d\u5371\u5bb3\u3002", "method": "\u63d0\u51fa\u4e86COMPASS\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u662f\u5426\u7b26\u5408\u7ec4\u7ec7\u7684\u5141\u8bb8\u5217\u8868\u548c\u7981\u6b62\u5217\u8868\u653f\u7b56\u3002\u5728\u516b\u4e2a\u4e0d\u540c\u7684\u884c\u4e1a\u573a\u666f\u4e2d\u751f\u6210\u5e76\u9a8c\u8bc1\u4e865,920\u4e2a\u67e5\u8be2\uff0c\u6d4b\u8bd5\u5e38\u89c4\u5408\u89c4\u6027\u548c\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u3002", "result": "\u8bc4\u4f30\u4e03\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u5904\u7406\u5408\u6cd5\u8bf7\u6c42\u65f6\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u4f46\u5728\u6267\u884c\u7981\u6b62\u653f\u7b56\u65f6\u8868\u73b0\u6781\u5dee\uff0c\u4ec5\u62d2\u7edd13-40%\u7684\u5bf9\u6297\u6027\u8fdd\u89c4\u8bf7\u6c42\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u653f\u7b56\u5173\u952e\u90e8\u7f72\u6240\u9700\u7684\u9c81\u68d2\u6027\uff0cCOMPASS \u6846\u67b6\u4e3a\u7ec4\u7ec7AI\u5b89\u5168\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2601.01103", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01103", "abs": "https://arxiv.org/abs/2601.01103", "authors": ["Abhinav Attri", "Rajeev Ranjan Dwivedi", "Samiran Das", "Vinod Kumar Kurmi"], "title": "Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization", "comment": "Accepted at WACV 2026", "summary": "We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/", "AI": {"tldr": "HAQAGen \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u635f\u5931\u9879\u548c\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u7684 NIR-to-RGB \u8f6c\u6362\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u5206\u8fa8\u7387\u4e0d\u53d8\u7684 NIR-to-RGB \u7740\u8272\uff0c\u5e73\u8861\u8272\u5f69\u771f\u5b9e\u6027\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "method": "HAQAGen \u7ed3\u5408\u4e86\u5168\u5c40\u989c\u8272\u7edf\u8ba1\u5bf9\u9f50\u3001\u5c40\u90e8\u8272\u8c03-\u9971\u548c\u5ea6\u5148\u9a8c\u6ce8\u5165\u548c\u7eb9\u7406\u611f\u77e5\u76d1\u7763\uff0c\u91c7\u7528 Mamba \u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u63a8\u7406\u5f15\u64ce\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\uff0cHAQAGen \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u66f4\u9510\u5229\u7eb9\u7406\u548c\u81ea\u7136\u8272\u5f69\u7684\u56fe\u50cf\u3002", "conclusion": "HAQAGen \u88ab\u5b9a\u4f4d\u4e3a\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6210\u50cf\u573a\u666f\u4e2d\u7684 NIR-to-RGB \u8f6c\u6362\u3002"}}
{"id": "2601.01844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01844", "abs": "https://arxiv.org/abs/2601.01844", "authors": ["Udiptaman Das", "Krishnasai B. Atmakuri", "Duy Ho", "Chi Lee", "Yugyung Lee"], "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation", "comment": "13 pages, 5 tables, 4 figures", "summary": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u81ea\u7531\u6587\u672c\u6784\u5efa\u548c\u8bc4\u4f30\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\uff0c\u65e0\u9700\u9ec4\u91d1\u6807\u51c6\u6ce8\u91ca\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u53d9\u8ff0\u4e2d\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u9c81\u68d2\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u9650\u5236\u5728\u80bf\u7624\u5b66\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5305\u62ec\uff1a\uff081\uff09\u57fa\u4e8e\u63d0\u793a\u7684\u5b9e\u4f53\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u63d0\u53d6\uff1b\uff082\uff09\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff1b\uff083\uff09\u4e0e\u672c\u4f53\u5bf9\u9f50\u7684RDF/OWL\u6a21\u5f0f\u751f\u6210\uff1b\uff084\uff09\u591aLLM\u5171\u8bc6\u9a8c\u8bc1\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u548c\u8bed\u4e49\u7ec6\u5316\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u652f\u6301\u6301\u7eed\u7684\u7ec6\u5316\u548c\u81ea\u76d1\u7763\u8bc4\u4f30\uff0c\u5b9e\u73b0\u56fe\u8c31\u8d28\u91cf\u7684\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u4e24\u4e2a\u80bf\u7624\u5b66\u961f\u5217\uff08PDAC\u548cBRCA\uff09\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u7cbe\u786e\u5ea6\u3001\u76f8\u5173\u6027\u548c\u672c\u4f53\u5408\u89c4\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u6a21\u5f0f\u7ea6\u675f\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08KG-RAG\uff09\u7b56\u7565\uff0c\u76f4\u63a5\u4ece\u81ea\u7531\u6587\u672c\u6784\u5efa\u548c\u8bc4\u4f30\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u4f9d\u8d56\u9ec4\u91d1\u6807\u51c6\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u3001SPARQL\u517c\u5bb9\u4e14\u4e34\u5e8a\u57fa\u7840\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u7cbe\u786e\u5ea6\u3001\u76f8\u5173\u6027\u548c\u672c\u4f53\u5408\u89c4\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.01167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01167", "abs": "https://arxiv.org/abs/2601.01167", "authors": ["Tianheng Cheng", "Xinggang Wang", "Junchao Liao", "Wenyu Liu"], "title": "Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation", "comment": null, "summary": "Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.", "AI": {"tldr": "GAI\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u63d2\u503c\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u4e0e\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u4e0d\u5bf9\u9f50\u548c\u8ba1\u7b97\u8d1f\u62c5\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u4e0b\u7684\u6700\u4f18\u8bed\u4e49\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5750\u6807\u5f15\u5bfc\u7684\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u63d2\u503c\u65b9\u6cd5\uff08\u5982\u53cc\u7ebf\u6027\u63d2\u503c\uff09\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u7c97\u7cd9\uff0c\u5b58\u5728\u7279\u5f81\u4e0d\u5bf9\u9f50\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u8bed\u4e49\u4e30\u5bcc\u5316\u8ba1\u7b97\u8d1f\u62c5\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u63a8\u7406\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f15\u5bfc\u6ce8\u610f\u529b\u63d2\u503c\uff08GAI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u786e\u5b9a\u4e0d\u540c\u5206\u8fa8\u7387\u7279\u5f81\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\uff0c\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u63d2\u503c\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u7684\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u3002", "result": "\u57fa\u4e8eGAI\u7684\u8bed\u4e49\u5206\u5272\u7f51\u7edcGAIN\u5728Cityscapes\u4e0a\u8fbe\u523078.8 mIoU\uff0822.3 FPS\uff09\uff0c\u5728CamVid\u4e0a\u8fbe\u523080.6 mIoU\uff0864.5 FPS\uff09\uff0c\u5747\u4f7f\u7528NVIDIA 1080Ti GPU\uff0c\u6210\u4e3a\u4f4e\u5ef6\u8fdf\u8bed\u4e49\u5206\u5272\u7684\u6700\u65b0\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "GAI\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u63d2\u503c\u7ec6\u7c92\u5ea6\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u4e0e\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u4e0d\u5bf9\u9f50\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u4f4e\u5ef6\u8fdf\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u5206\u5272\u6548\u679c\u3002"}}
{"id": "2601.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01857", "abs": "https://arxiv.org/abs/2601.01857", "authors": ["Defei Xia", "Bingfeng Pi", "Shenbin Zhang", "Song Hua", "Yunfei Wei", "Lei Zuo"], "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios", "comment": null, "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u3001\u5de5\u5177\u7f16\u6392\u548c\u5206\u5c42\u5185\u5b58\u4e09\u5927\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5df2\u5728Jenius\u5e73\u53f0\u90e8\u7f72\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u6574\u4f53\u8bbe\u8ba1\u4e0a\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u7684\u7cfb\u7edf\u6027\u4f18\u5316\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u5b9e\u9645\u7ecf\u9a8c\u9a71\u52a8\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u63d0\u5347\u4ee3\u7406\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u54cd\u5e94\u751f\u6210\u65b9\u9762\u7684\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u5927\u5173\u952e\u521b\u65b0\uff1a(1) \u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u7b56\u7565\uff1b(2) \u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u6a21\u5757\uff1b(3) \u5206\u5c42\u5185\u5b58\u673a\u5236\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u4f7f\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u5347\u4e8620%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86token\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aJenius-Agent\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u548c\u5206\u5c42\u5185\u5b58\u673a\u5236\u4e09\u5927\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u4e86token\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002\u8be5\u6846\u67b6\u5df2\u5728Jenius\u5e73\u53f0\u90e8\u7f72\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01176", "abs": "https://arxiv.org/abs/2601.01176", "authors": ["Andr\u00e9s Bell-Navas", "Jes\u00fas Garicano-Mena", "Antonella Ausiello", "Soledad Le Clainche", "Mar\u00eda Villalba-Orero", "Enrique Lara-Pezzi"], "title": "CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops", "comment": "9 pages; 1 figure; letter", "summary": "Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.\n  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.\n  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.\n  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.", "AI": {"tldr": "\u5f00\u53d1\u4e86CardioMOD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u8d85\u58f0\u5fc3\u52a8\u56fe\u5faa\u73af\u5b9e\u73b0HFpEF\u7684\u591a\u7c7b\u8bca\u65ad\u548c\u8fde\u7eed\u53d1\u75c5\u9884\u6d4b\uff0c\u51c6\u786e\u738765%\uff0c\u9884\u6d4b\u8bef\u5dee21.72\u5468\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dAI\u6a21\u578b\u5728HFpEF\u65e9\u671f\u8bca\u65ad\u548c\u9884\u540e\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u5e76\u53d1\u75c7\u7684\u8868\u578b\u5206\u6790\u548c\u75be\u75c5\u8fdb\u5c55\u7684\u65f6\u95f4\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u5c0f\u9f20\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff0c\u901a\u8fc7\u9ad8\u9636\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08HODMD\uff09\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\uff0c\u7ed3\u5408Vision Transformers\u8fdb\u884c\u5206\u7c7b\u548c\u56de\u5f52\u5206\u6790\u3002", "result": "\u8bca\u65ad\u51c6\u786e\u7387\u4e3a65%\uff0c\u9884\u540e\u6a21\u5757\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a21.72\u5468\uff0cOB\u548cSAH\u7ec4\u7684\u9884\u6d4b\u6700\u4e3a\u51c6\u786e\u3002", "conclusion": "CardioMOD-Net\u6846\u67b6\u5c55\u793a\u4e86\u4ece\u5c0f\u6570\u636e\u96c6\u7684\u5355\u4e00\u8d85\u58f0\u5fc3\u52a8\u56fe\u5faa\u73af\u4e2d\u5b9e\u73b0\u591a\u7c7b\u8868\u578b\u5206\u6790\u548c\u8fde\u7eedHFpEF\u53d1\u75c5\u9884\u6d4b\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u524dHFpEF\u7814\u7a76\u63d0\u4f9b\u4e86\u8bca\u65ad\u548c\u9884\u540e\u5efa\u6a21\u7684\u57fa\u7840\u3002"}}
{"id": "2601.01875", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.01875", "abs": "https://arxiv.org/abs/2601.01875", "authors": ["Kewen Cao", "Jianxu Chen", "Yongbing Zhang", "Ye Zhang", "Hongxiao Wang"], "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence", "comment": null, "summary": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSQL\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5ba1\u8ba1\u7684\u7279\u5f81\u6d4b\u91cf\u548c\u63a8\u7406\uff0c\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u8bc1\u636e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u53ef\u8ffd\u6eaf\u6027\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2aSQL\u4e2d\u5fc3\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u7279\u5f81\u63a8\u7406\u4ee3\u7406\uff08\u6267\u884cSQL\u67e5\u8be2\u805a\u5408\u89c6\u89c9\u8bc1\u636e\uff09\u548c\u77e5\u8bc6\u6bd4\u8f83\u4ee3\u7406\uff08\u8bc4\u4f30\u53d1\u73b0\u4e0e\u75c5\u7406\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u75c5\u7406\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u6267\u884c\u7684SQL\u8ffd\u8e2a\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684SQL\u4e2d\u5fc3\u4ee3\u7406\u6846\u67b6\u901a\u8fc7\u53ef\u5ba1\u8ba1\u7684\u7279\u5f81\u6d4b\u91cf\u548c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u751f\u6210\u7684SQL\u8ffd\u8e2a\u5c06\u7ec6\u80de\u6d4b\u91cf\u4e0e\u8bca\u65ad\u7ed3\u8bba\u76f4\u63a5\u5173\u8054\u3002"}}
{"id": "2601.01181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01181", "abs": "https://arxiv.org/abs/2601.01181", "authors": ["Chenglizhao Chen", "Shaojiang Yuan", "Xiaoxue Lu", "Mengke Song", "Jia Song", "Zhenyu Wu", "Wenfeng Song", "Shuai Li"], "title": "GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation", "comment": null, "summary": "Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.", "AI": {"tldr": "\u63d0\u51faGenCAMO\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u89e3\u51b3\u4f2a\u88c5\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u7684\u4f2a\u88c5\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u6570\u636e\u4ee5\u8bad\u7ec3CDP\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86GenCAMO-DB\u6570\u636e\u96c6\u548cGenCAMO\u751f\u6210\u6846\u67b6\uff0c\u540e\u8005\u662f\u4e00\u4e2a\u73af\u5883\u611f\u77e5\u4e14\u65e0\u9700\u63a9\u7801\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u4f2a\u88c5\u56fe\u50cf\u5bc6\u96c6\u6807\u6ce8\u3002", "result": "\u5728\u591a\u6a21\u6001\u5b9e\u9a8c\u4e2d\uff0cGenCAMO\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "GenCAMO\u6846\u67b6\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4f2a\u88c5\u573a\u666f\u4e0b\u7684\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2601.01878", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01878", "abs": "https://arxiv.org/abs/2601.01878", "authors": ["Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Zhivar Sourati", "Jackson Trager", "Morteza Dehghani"], "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs", "comment": null, "summary": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u57fa\u51c6\u56e0\u7f3a\u4e4f\u660e\u786e\u7406\u8bba\u57fa\u7840\u800c\u4ea7\u751f\u6709\u6548\u6027\u5e7b\u89c9\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u8ffd\u8e2a\u5361\uff08TTC\uff09\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u589e\u5f3a\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u57fa\u51c6\u5f80\u5f80\u7f3a\u4e4f\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u88ab\u8bef\u89e3\u4e3a\u5e7f\u6cdb\u80fd\u529b\u7684\u8bc1\u636e\uff0c\u4ece\u800c\u4ea7\u751f\u7cfb\u7edf\u6027\u6709\u6548\u6027\u5e7b\u89c9\u3002", "method": "\u9996\u5148\u8bca\u65ad\u5e76\u5f62\u5f0f\u5316\u4e86\u7406\u8bba\u5dee\u8ddd\u95ee\u9898\uff0c\u968f\u540e\u8bbe\u8ba1\u4e86\u7406\u8bba\u8ffd\u8e2a\u5361\uff08TTC\uff09\uff0c\u7528\u4e8e\u660e\u786e\u8bb0\u5f55\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u3001\u76ee\u6807\u80fd\u529b\u3001\u64cd\u4f5c\u5316\u53ca\u5c40\u9650\u6027\u3002", "result": "\u7406\u8bba\u8ffd\u8e2a\u5361\uff08TTC\uff09\u7684\u5f15\u5165\u80fd\u591f\u589e\u5f3a\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u51c6\u6216\u8981\u6c42\u5bf9\u5355\u4e00\u7406\u8bba\u8fbe\u6210\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u7406\u8bba\u8ffd\u8e2a\u5361\uff08TTC\uff09\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6587\u6863\u5de5\u5177\uff0c\u65e8\u5728\u660e\u786e\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u3001\u76ee\u6807\u80fd\u529b\u7684\u7ec4\u6210\u90e8\u5206\u3001\u64cd\u4f5c\u5316\u53ca\u5176\u5c40\u9650\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002"}}
{"id": "2601.01192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01192", "abs": "https://arxiv.org/abs/2601.01192", "authors": ["Hao Lu", "Xuhui Zhu", "Wenjing Zhang", "Yanan Li", "Xiang Bai"], "title": "Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors", "comment": "Journal Extension of arXiv:2506.13067", "summary": "Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.", "AI": {"tldr": "OMAN++\u901a\u8fc7O2M\u5339\u914d\u548c\u4f4d\u79fb\u5148\u9a8c\u6ce8\u5165\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62e5\u6324\u573a\u666f\u4e0b\u7684\u89c6\u9891\u4e2a\u4f53\u8ba1\u6570\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VIC\u65b9\u6cd5\u5728\u62e5\u6324\u573a\u666f\uff08\u5982\u5730\u94c1\u901a\u52e4\uff09\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u653e\u677e\u6807\u51c6\u7684\u4e00\u5bf9\u4e00\u5339\u914d\uff08O2O\uff09\u4e3a\u4e00\u5bf9\u591a\u5339\u914d\uff08O2M\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4d\u79fb\u5148\u9a8c\u6ce8\u5165\u5668\uff0c\u7ed3\u5408\u9690\u5f0f\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u548cO2M\u5339\u914d\u5668\u3002", "result": "\u5728WuhanMetroCrowd\u6570\u636e\u96c6\u4e0a\u8bef\u5dee\u964d\u4f4e\u4e8638.12%\uff0c\u5e76\u5728SenseCrowd\u3001CroHD\u548cMovingDroneCrowd\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "OMAN++ \u5728\u62e5\u6324\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709VIC\u57fa\u7ebf\u3002"}}
{"id": "2601.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01910", "abs": "https://arxiv.org/abs/2601.01910", "authors": ["Minh Hieu Ha", "Khanh Ly Ta", "Hung Phan", "Tung Doan", "Tung Dao", "Dao Tran", "Huynh Thi Thanh Binh"], "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning", "comment": null, "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.", "AI": {"tldr": "MMP-A* \u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51e0\u4f55\u6709\u6548\u6027\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf A* \u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7eaf\u6587\u672c\u89c4\u5212\u5668\u5728\u7a7a\u95f4\u5b9a\u4f4d\u548c\u51e0\u4f55\u6709\u6548\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86 MMP-A* \u6846\u67b6\u3002", "method": "MMP-A* \u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u548c\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\uff0c\u52a8\u6001\u8c03\u8282\u4e0d\u786e\u5b9a\u8def\u5f84\u70b9\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u51e0\u4f55\u6709\u6548\u6027\u5e76\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMP-A* \u5728\u4e25\u91cd\u6742\u4e71\u548c\u62d3\u6251\u590d\u6742\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\u3002", "conclusion": "MMP-A* \u6846\u67b6\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u548c\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u5bfc\u822a\u7684\u6548\u7387\u548c\u51e0\u4f55\u6709\u6548\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01200", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01200", "abs": "https://arxiv.org/abs/2601.01200", "authors": ["Zhang Chen", "Shuai Wan", "Yuezhe Zhang", "Siyu Ren", "Fuzheng Yang", "Junhui Hou"], "title": "MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity", "comment": null, "summary": "The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.", "AI": {"tldr": "\u63d0\u51faMS-ISSM\u65b9\u6cd5\uff0c\u901a\u8fc7RBF\u548cResGrouped-MLP\u7f51\u7edc\u89e3\u51b3\u70b9\u4e91\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u4e0d\u89c4\u5219\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u70b9\u4e91\u7684\u975e\u7ed3\u6784\u5316\u548c\u4e0d\u89c4\u5219\u6027\u7ed9\u8d28\u91cf\u8bc4\u4f30\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u70b9\u5bf9\u70b9\u5339\u914d\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u9690\u5f0f\u7ed3\u6784\u76f8\u4f3c\u6027\u6d4b\u91cf\uff08MS-ISSM\uff09\u548cResGrouped-MLP\u8d28\u91cf\u8bc4\u4f30\u7f51\u7edc\uff0c\u901a\u8fc7RBF\u8868\u793a\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5206\u7ec4\u7f16\u7801\u7b56\u7565\u7ed3\u5408\u6b8b\u5dee\u5757\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS-ISSM\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MS-ISSM\u65b9\u6cd5\u5728\u70b9\u4e91\u8d28\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6307\u6807\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01939", "abs": "https://arxiv.org/abs/2601.01939", "authors": ["Victor Sanchez", "Chris Reinke", "Ahamed Mohamed", "Xavier Alameda-Pineda"], "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation", "comment": null, "summary": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.", "AI": {"tldr": "OpenSocInt\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u6a21\u62df\u5668\uff0c\u652f\u6301\u6a21\u5757\u5316\u8bad\u7ec3\u793e\u4ea4\u4ee3\u7406\uff0c\u5df2\u516c\u5f00\u53ef\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u7684\u7814\u7a76\u548c\u793e\u4ea4\u4ee3\u7406\u7684\u8bad\u7ec3\u3002", "method": "\u4ecb\u7ecd\u4e86OpenSocInt\u8f6f\u4ef6\u5305\u53ca\u5176\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u57fa\u4e8e\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u7684\u5b9e\u9a8c\u534f\u8bae\u5c55\u793a\u5176\u529f\u80fd\u3002", "result": "\u8f6f\u4ef6\u5305\u5df2\u6210\u529f\u5f00\u53d1\u5e76\u516c\u5f00\u53ef\u7528\uff0c\u652f\u6301\u591a\u79cd\u611f\u77e5\u7279\u5f81\u548c\u4ee3\u7406\u7684\u63a2\u7d22\u3002", "conclusion": "OpenSocInt\u662f\u4e00\u4e2a\u5f00\u6e90\u8f6f\u4ef6\u5305\uff0c\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u7684\u6a21\u62df\u5668\u548c\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u8bad\u7ec3\u793e\u4ea4\u4ee3\u7406\u3002\u8be5\u8f6f\u4ef6\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u652f\u6301\u63a2\u7d22\u4e0d\u540c\u611f\u77e5\u7279\u5f81\u7684\u4f7f\u7528\u3001\u7f16\u7801\u548c\u878d\u5408\uff0c\u4ee5\u53ca\u4e0d\u540c\u4ee3\u7406\u7684\u5e94\u7528\u3002"}}
{"id": "2601.01202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01202", "abs": "https://arxiv.org/abs/2601.01202", "authors": ["Jiazhu Dai", "Huihui Jiang"], "title": "RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models", "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.", "AI": {"tldr": "RefSR-Adv\u662f\u4e00\u79cd\u9488\u5bf9RefSR\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u53c2\u8003\u56fe\u50cf\u5bfc\u81f4\u8d85\u5206\u8fa8\u7387\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RefSR\u7684\u540e\u95e8\u653b\u51fb\uff0c\u800c\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86RefSR-Adv\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u6270\u52a8\u53c2\u8003\u56fe\u50cf\u6765\u964d\u4f4e\u8d85\u5206\u8fa8\u7387\u8f93\u51fa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u4e0e\u653b\u51fb\u6548\u679c\u5448\u6b63\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u7279\u5f81\u662f\u5173\u952e\u5b89\u5168\u7f3a\u9677\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86RefSR\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u65e8\u5728\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u5173\u6ce8RefSR\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002"}}
{"id": "2601.01976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01976", "abs": "https://arxiv.org/abs/2601.01976", "authors": ["Yasmine Souissi", "Fabrice Boissier", "Nida Meddouri"], "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes", "comment": null, "summary": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eFCA\u7684\u5206\u7c7b\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u7387\u3002", "motivation": "\u77e5\u8bc6\u53d1\u73b0\uff08KDD\uff09\u65e8\u5728\u4ece\u5927\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u9690\u85cf\u4e14\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u800c\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5b66\u4e60\u800c\u53d7\u5230\u8ba4\u53ef\u3002\u672c\u6587\u65e8\u5728\u7efc\u8ff0FCA\u5206\u7c7b\u5668\u7684\u6700\u65b0\u8fdb\u5c55\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u4ece\u540d\u4e49\u6570\u636e\u8ba1\u7b97\u95ed\u5305\u7b97\u5b50\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b9\u6cd5\u662f\u9ad8\u6548\u7684\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u57fa\u4e8e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u7684\u5206\u7c7b\u5668\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u6700\u76f8\u5173\u7684\u6982\u5ff5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u7387\u3002"}}
{"id": "2601.01204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01204", "abs": "https://arxiv.org/abs/2601.01204", "authors": ["Zunhai Su", "Weihao Ye", "Hansen Feng", "Keyu Fan", "Jing Zhang", "Dahai Yu", "Zhengwu Liu", "Ngai Wong"], "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression", "comment": null, "summary": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.", "AI": {"tldr": "XStreamVGGT\u901a\u8fc7\u526a\u679d\u548c\u91cf\u5316\u538b\u7f29KV\u7f13\u5b58\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u6d41\u5f0f3D\u5e94\u7528\u3002", "motivation": "StreamVGGT\u867d\u7136\u5229\u7528\u5e27\u7ea7\u56e0\u679c\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6d41\u5f0f\u91cd\u5efa\uff0c\u4f46\u968f\u7740\u8f93\u5165\u5e27\u7684\u7d2f\u79ef\uff0cKV\u7f13\u5b58\u7684\u65e0\u9650\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u4e0d\u65ad\u589e\u52a0\u3002", "method": "\u901a\u8fc7\u6709\u6548\u7684\u4ee4\u724c\u91cd\u8981\u6027\u8bc6\u522b\u526a\u679d\u591a\u89c6\u56fe\u8f93\u5165\u4ea7\u751f\u7684\u5197\u4f59KV\uff0c\u5e76\u5229\u7528KV\u5f20\u91cf\u7684\u72ec\u7279\u5206\u5e03\u8fdb\u884c\u91cf\u5316\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "XStreamVGGT\u5728\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e864.42\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e865.48\u500d\u3002", "conclusion": "XStreamVGGT\u901a\u8fc7\u8054\u5408\u526a\u679d\u548c\u91cf\u5316\u7cfb\u7edf\u6027\u5730\u538b\u7f29KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u6d41\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u6d41\u5f0f3D\u5e94\u7528\u3002"}}
{"id": "2601.01982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01982", "abs": "https://arxiv.org/abs/2601.01982", "authors": ["Noel Thomas"], "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)", "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "AI": {"tldr": "ChaosBench-Logic \u662f\u4e00\u4e2a\u8bc4\u4f30 LLM \u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u5728\u5355\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u548c\u5bf9\u8bdd\u4e2d\u4ecd\u6709\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u903b\u8f91\u548c\u7b26\u53f7\u63a8\u7406\u7684\u9886\u57df\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u56e0\u5176\u786e\u5b9a\u6027\u5e38\u88ab\u8bef\u89e3\u4e3a\u968f\u673a\u6027\u6216\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u7684\u6d4b\u8bd5\u57fa\u51c6\u3002", "method": "\u5f15\u5165\u4e86 ChaosBench-Logic \u57fa\u51c6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4e00\u9636\u903b\u8f91\uff08FOL\uff09\u672c\u4f53\u8bc4\u4f30 LLM \u5728 30 \u79cd\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u751f\u6210 621 \u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u591a\u79cd\u63a8\u7406\u7c7b\u522b\u3002", "result": "\u524d\u6cbf LLM\uff08\u5982 GPT-4\u3001Claude 3.5 Sonnet \u7b49\uff09\u5728\u5355\u9879\u51c6\u786e\u7387\u4e0a\u8fbe\u5230 91-94%\uff0c\u4f46\u5728\u7ec4\u5408\u9879\u4e0a\u5f97\u5206\u4e3a 0%\uff0c\u5bf9\u8bdd\u7ea7\u51c6\u786e\u7387\u4ece 53.1% \u5230 75.5% \u4e0d\u7b49\u3002", "conclusion": "ChaosBench-Logic \u4e3a\u8bca\u65ad LLM \u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5e76\u4e3a\u5f00\u53d1\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4ee5\u6539\u8fdb LLM \u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01993", "abs": "https://arxiv.org/abs/2601.01993", "authors": ["Dong Xue", "Jicheng Tu", "Ming Wang", "Xin Yan", "Fangzhou Liu", "Jie Hu"], "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support", "comment": "33 pages, 16 figures", "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.", "AI": {"tldr": "MindChat\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6MindCorpus\u548c\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u3002", "motivation": "\u89e3\u51b3\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u9886\u57df\u771f\u5b9e\u54a8\u8be2\u5bf9\u8bdd\u7a00\u7f3a\u4e14\u654f\u611f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6846\u67b6\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6MindCorpus\uff0c\u7ed3\u5408\u53cc\u95ed\u73af\u53cd\u9988\u8bbe\u8ba1\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff1b\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u9690\u79c1\u4f18\u5316\u6280\u672f\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "MindCorpus\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u679c\uff0cMindChat\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u66f4\u4f4e\u3002", "conclusion": "MindChat\u548cMindCorpus\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u9886\u57df\u5c55\u73b0\u4e86\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2601.01213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01213", "abs": "https://arxiv.org/abs/2601.01213", "authors": ["Riccardo Gelato", "Carlo Sgaravatti", "Jakob Grahn", "Giacomo Boracchi", "Filippo Maria Bianchi"], "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation", "comment": null, "summary": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.", "AI": {"tldr": "\u5229\u7528\u6539\u8fdb\u7684SAM\u6a21\u578b\u9002\u914dSentinel-1 SAR\u6570\u636e\uff0c\u901a\u8fc7\u591a\u9879\u6280\u672f\u4f18\u5316\u663e\u8457\u63d0\u5347\u96ea\u5d29\u6807\u6ce8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3SAR\u56fe\u50cf\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u52a0\u901f\u96ea\u5d29\u6620\u5c04\u7684\u6807\u6ce8\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u9002\u914d\u5668\u7f13\u89e3\u9886\u57df\u5dee\u5f02\uff0c\u591a\u7f16\u7801\u5668\u5904\u7406\u591a\u901a\u9053SAR\u8f93\u5165\uff0c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4ee5\u53ca\u9ad8\u6548\u8bad\u7ec3\u7b97\u6cd5\u4f18\u5316\u8ba1\u7b97\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SAR\u56fe\u50cf\u7684\u6807\u6ce8\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9002\u914d\u5668\u3001\u591a\u7f16\u7801\u5668\u3001\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u6210\u529f\u5c06SAM\u6a21\u578b\u9002\u914d\u4e8eSentinel-1 SAR\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96ea\u5d29\u6807\u6ce8\u6548\u7387\u3002"}}
{"id": "2601.02008", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02008", "abs": "https://arxiv.org/abs/2601.02008", "authors": ["Midhat Urooj", "Ayan Banerjee", "Sandeep Gupta"], "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging", "comment": "Accepted at AAAI Bridge Program 2026", "summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.", "AI": {"tldr": "XAIMeD\u662f\u4e00\u79cd\u7ed3\u5408\u4e34\u5e8a\u77e5\u8bc6\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u533b\u7597AI\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u7f55\u89c1\u7c7b\u522b\u654f\u611f\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u4e2d\u89e3\u91ca\u6027\u3001\u9886\u57df\u6cdb\u5316\u548c\u7f55\u89c1\u7c7b\u522b\u53ef\u9760\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u907f\u514d\u6df1\u5ea6\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5931\u6548\u548c\u5bf9\u7f55\u89c1\u4e34\u5e8a\u6761\u4ef6\u7684\u504f\u89c1\u3002", "method": "XAIMeD\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5c06\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7f16\u7801\u4e3a\u903b\u8f91\u8fde\u63a5\u8bcd\u548c\u539f\u5b50\u533b\u5b66\u547d\u9898\uff0c\u901a\u8fc7\u52a0\u6743\u7279\u5f81\u6ee1\u8db3\u5206\u6570\u91cf\u5316\u8bca\u65ad\u6548\u7528\uff0c\u5e76\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u5206\u652f\u4e0e\u795e\u7ecf\u9884\u6d4b\u3002\u901a\u8fc7\u7f6e\u4fe1\u52a0\u6743\u878d\u5408\u548c\u57fa\u4e8e\u71b5\u4e0d\u5e73\u8861\u589e\u76ca\uff08EIG\uff09\u53ca\u7f55\u89c1\u7c7b\u57fa\u5c3c\u7cfb\u6570\u7684\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\uff0c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u548c\u56db\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cXAIMeD\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u63d0\u9ad86%\uff0c\u7f55\u89c1\u7c7b\u522bF1\u5206\u6570\u63d0\u534710%\uff0c\u8fdc\u8d85\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "XAIMeD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u77e5\u8bc6\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597AI\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3001\u7f55\u89c1\u7c7b\u522b\u7684\u654f\u611f\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u4e34\u5e8a\u5bf9\u9f50\u7684\u89e3\u91ca\u3002"}}
{"id": "2601.01222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01222", "abs": "https://arxiv.org/abs/2601.01222", "authors": ["Mengfei Li", "Peng Li", "Zheng Zhang", "Jiahao Lu", "Chengfeng Zhao", "Wei Xue", "Qifeng Liu", "Sida Peng", "Wenxiao Zhang", "Wenhan Luo", "Yuan Liu", "Yike Guo"], "title": "UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass", "comment": null, "summary": "We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/", "AI": {"tldr": "UniSH\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u524d\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff08\u9c81\u68d2\u84b8\u998f\u548c\u4e24\u9636\u6bb5\u76d1\u7763\uff09\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u573a\u666f\u548c\u4eba\u4f53\u91cd\u5efa\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8be5\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u6807\u6ce8\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5bfc\u81f4\u4f9d\u8d56\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5f15\u5165\u4e86\u663e\u8457\u7684\u6a21\u62df\u5230\u771f\u5b9e\u7684\u57df\u5dee\u8ddd\uff0c\u8868\u73b0\u4e3a\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u4eba\u4f53\u51e0\u4f55\u4fdd\u771f\u5ea6\u4f4e\u4ee5\u53ca\u5bf9\u91ce\u5916\u89c6\u9891\u7684\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "UniSH\u91c7\u7528\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u524d\u9988\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u573a\u666f\u91cd\u5efa\u548cHMR\u7684\u5f3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u8fdb\u884c\u8bad\u7ec3\uff1a(1) \u4ece\u4e13\u5bb6\u6df1\u5ea6\u6a21\u578b\u4e2d\u63d0\u53d6\u9ad8\u9891\u7ec6\u8282\u7684\u9c81\u68d2\u84b8\u998f\u7b56\u7565\uff1b(2) \u4e24\u9636\u6bb5\u76d1\u7763\u65b9\u6848\uff0c\u5148\u5728\u5408\u6210\u6570\u636e\u4e0a\u5b66\u4e60\u7c97\u5b9a\u4f4d\uff0c\u7136\u540e\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u901a\u8fc7\u76f4\u63a5\u4f18\u5316SMPL\u7f51\u683c\u4e0e\u4eba\u4f53\u70b9\u4e91\u4e4b\u95f4\u7684\u51e0\u4f55\u5bf9\u5e94\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniSH\u5728\u4eba\u4f53\u4e2d\u5fc3\u573a\u666f\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5168\u5c40\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u4f18\u4e8e\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\u548c\u4ec5\u4f7f\u7528HMR\u7684\u65b9\u6cd5\u3002", "conclusion": "UniSH\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u573a\u666f\u51e0\u4f55\u548c\u4eba\u4f53\u91cd\u5efa\uff0c\u5e76\u5728\u4eba\u4f53\u4e2d\u5fc3\u573a\u666f\u91cd\u5efa\u548c\u5168\u5c40\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.02043", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02043", "abs": "https://arxiv.org/abs/2601.02043", "authors": ["Hendrik Kempt", "Alon Lavie"], "title": "Simulated Reasoning is Reasoning", "comment": "21 pages", "summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\u201c\u601d\u8003\u8fc7\u7a0b\u201d\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u5176\u7f3a\u4e4f\u5e38\u8bc6\u548c\u57fa\u7840\uff0c\u5bfc\u81f4\u63a8\u7406\u8106\u5f31\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u8fd9\u79cd\u73b0\u8c61\u7684\u54f2\u5b66\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u5b89\u5168\u6027\u548c\u89c4\u8303\u6027\u8003\u91cf\u3002", "motivation": "\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u6a21\u4eff\u201c\u601d\u8003\u8fc7\u7a0b\u201d\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u53ca\u8fd9\u79cd\u63a8\u7406\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u672c\u8d28\u533a\u522b\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u89e3\u91ca\u548c\u8bba\u8bc1\uff0c\u5206\u6790\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u53ca\u5176\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u7f3a\u4e4f\u5e38\u8bc6\u548c\u57fa\u7840\uff0c\u5bfc\u81f4\u5176\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u5b89\u5168\u6027\u548c\u89c4\u8303\u6027\u8003\u91cf\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6307\u51fa\u5176\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u6839\u672c\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u9632\u5fa1\u7684\u8003\u91cf\u3002"}}
{"id": "2601.01224", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01224", "abs": "https://arxiv.org/abs/2601.01224", "authors": ["Bac Nguyen", "Yuhta Takida", "Naoki Murata", "Chieh-Hsin Lai", "Toshimitsu Uesaka", "Stefano Ermon", "Yuki Mitsufuji"], "title": "Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment", "comment": null, "summary": "Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.", "AI": {"tldr": "CODA\u901a\u8fc7\u6ce8\u518c\u69fd\u548c\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\u6539\u8fdbSlot Attention\uff0c\u63d0\u5347\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "Slot Attention\uff08SA\uff09\u4e0e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\uff08OCL\uff09\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u69fd\u7ea0\u7f20\u548c\u5bf9\u8c61\u69fd\u4e0e\u56fe\u50cf\u5185\u5bb9\u5bf9\u9f50\u5f31\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5bf9\u6bd4\u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u5bf9\u9f50\uff08CODA\uff09\uff0c\u901a\u8fc7\uff08i\uff09\u4f7f\u7528\u6ce8\u518c\u69fd\u5438\u6536\u6b8b\u5dee\u6ce8\u610f\u529b\u51cf\u5c11\u5bf9\u8c61\u69fd\u4e4b\u95f4\u7684\u5e72\u6270\uff0c\uff08ii\uff09\u5e94\u7528\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\u663e\u5f0f\u589e\u5f3a\u69fd\u4e0e\u56fe\u50cf\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\uff08MOVi-C/E\uff09\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08VOC\uff0cCOCO\uff09\u4e0a\uff0cCODA\u5728\u5bf9\u8c61\u53d1\u73b0\uff08\u5982COCO\u4e0aFG-ARI\u63d0\u53476.1%\uff09\u3001\u5c5e\u6027\u9884\u6d4b\u548c\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "CODA\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u8fdb\u884c\u9c81\u68d2\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.02061", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02061", "abs": "https://arxiv.org/abs/2601.02061", "authors": ["Faizan Ahmed", "Aniket Dixit", "James Brusey"], "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management", "comment": "6 pages, accepted at NeurIPS workshop 2025", "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ad8\u9636\u52a8\u4f5c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6025\u52a8\u6700\u5c0f\u5316\u5b9e\u73b0\u5e73\u6ed1\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8bbe\u5907\u5207\u636260%\uff0c\u9002\u7528\u4e8e\u80fd\u6e90\u5173\u952e\u5e94\u7528\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5e38\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u3001\u9ad8\u9891\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u8fd9\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7531\u4e8e\u9ad8\u80fd\u8017\u548c\u673a\u68b0\u78e8\u635f\u800c\u53d7\u5230\u963b\u788d\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u9ad8\u9636\u5bfc\u6570\u60e9\u7f5a\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u52a8\u4f5c\u5e73\u6ed1\u6b63\u5219\u5316\uff0c\u4ece\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u7684\u7406\u8bba\u7406\u89e3\u5230\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7684\u5b9e\u9645\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u6025\u52a8\u6700\u5c0f\u5316\uff09\u59cb\u7ec8\u80fd\u5b9e\u73b0\u66f4\u4f18\u7684\u5e73\u6ed1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u3002\u5728HVAC\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u5e73\u6ed1\u7b56\u7565\u5c06\u8bbe\u5907\u5207\u6362\u51cf\u5c11\u4e8660%\uff0c\u5e26\u6765\u663e\u8457\u7684\u64cd\u4f5c\u6548\u76ca\u3002", "conclusion": "\u672c\u6587\u786e\u7acb\u4e86\u9ad8\u9636\u52a8\u4f5c\u6b63\u5219\u5316\u4f5c\u4e3a\u5728\u80fd\u6e90\u5173\u952e\u5e94\u7528\u4e2d\u8fde\u63a5RL\u4f18\u5316\u4e0e\u64cd\u4f5c\u7ea6\u675f\u7684\u6709\u6548\u6865\u6881\u3002"}}
{"id": "2601.01228", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01228", "abs": "https://arxiv.org/abs/2601.01228", "authors": ["Markus Haltmeier", "Lukas Neumann", "Nadja Gruber", "Johannes Schwab", "Gyeongha Hwang"], "title": "HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training", "comment": null, "summary": "Solving image reconstruction problems of the form \\(\\mathbf{A} \\mathbf{x} = \\mathbf{y}\\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \\((\\mathbf{x},\\mathbf{y})\\). In many practical settings, only measurements \\(\\mathbf{y}\\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.", "AI": {"tldr": "HyDRA\u662f\u4e00\u79cd\u4ec5\u9700\u6d4b\u91cf\u6570\u636e\u7684DEQ\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u5219\u5316\u548c\u65e9\u671f\u505c\u6b62\u51c6\u5219\uff0c\u5728\u7a00\u758f\u89c6\u56feCT\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5feb\u901f\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u4ec5\u6d4b\u91cf\u6570\u636e\u53ef\u7528\u65f6\u7684\u56fe\u50cf\u91cd\u5efa\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edfDEQ\u6a21\u578b\u9700\u8981\u76d1\u7763\u6570\u636e\u5bf9\u7684\u9650\u5236\u3002", "method": "HyDRA\u7ed3\u5408\u4e86\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u81ea\u9002\u5e94\u53bb\u566a\u6b63\u5219\u5316\u9879\uff0c\u5e76\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65e9\u671f\u505c\u6b62\u51c6\u5219\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u7a00\u758f\u89c6\u56feCT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHyDRA\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "HyDRA\u6846\u67b6\u5728\u4ec5\u4f7f\u7528\u6d4b\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7ed3\u5408\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u81ea\u9002\u5e94\u53bb\u566a\u6b63\u5219\u5316\u9879\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u65e9\u671f\u505c\u6b62\u51c6\u5219\uff0c\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u5feb\u901f\u63a8\u7406\u3002"}}
{"id": "2601.02071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02071", "abs": "https://arxiv.org/abs/2601.02071", "authors": ["Adeshola Okubena", "Yusuf Ali Mohammed", "Moe Elbadawi"], "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations", "comment": null, "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f18\u5316\u836f\u72693D\u6253\u5370\u5236\u5242\u5f00\u53d1\uff0c\u53d1\u73b0Llama2\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u9700\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bc4\u4f30\u6307\u6807\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u6982\u5ff5\uff0c\u4f7fAI\u7cfb\u7edf\u8d85\u8d8a\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u3001\u7c7b\u4eba\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u836f\u72693D\u6253\u5370\u4e2d\u5e7f\u6cdb\u7684\u5236\u5242\u6311\u6218\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u56db\u79cdLLM\u67b6\u6784\uff08\u57fa\u4e8e\u5305\u542b1400\u591a\u79cd\u5236\u5242\u7684FDM\u6570\u636e\u96c6\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5fae\u8c03\u548c\u751f\u6210\u53c2\u6570\u914d\u7f6e\uff0c\u4ee5\u63a8\u8350\u9002\u5408\u7684\u8f85\u6599\u5e76\u9884\u6d4b\u957f\u4e1d\u673a\u68b0\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLlama2\u6700\u9002\u5408\u63a8\u8350FDM\u5236\u5242\u7684\u8f85\u6599\uff1b\u6a21\u578b\u9009\u62e9\u548c\u53c2\u6570\u5316\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u8f83\u5c0f\u7684LLMs\u53ef\u80fd\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff1b\u6807\u51c6LLM\u6307\u6807\u4ec5\u8bc4\u4f30\u8bed\u8a00\u6027\u80fd\u800c\u975e\u5236\u5242\u53ef\u52a0\u5de5\u6027\u3002", "conclusion": "\u4e3a\u4e86\u63a8\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u836f\u7269\u5236\u5242\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u5fc5\u987b\u89e3\u51b3\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u914d\u7f6e\u4ee5\u53ca\u6570\u636e\u96c6\u89c4\u6a21\u7b49\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u5176\u4e0d\u4ec5\u5177\u5907\u8bed\u8a00\u80fd\u529b\uff0c\u8fd8\u80fd\u53ef\u9760\u5730\u652f\u6301\u5236\u5242\u5f00\u53d1\u3002"}}
{"id": "2601.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01240", "abs": "https://arxiv.org/abs/2601.01240", "authors": ["Ziqian Guan", "Xieyi Fu", "Yuting Wang", "Haowen Xiao", "Jiarui Zhu", "Yingying Zhu", "Yongtao Liu", "Lin Gu"], "title": "RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection", "comment": null, "summary": "Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.", "AI": {"tldr": "RFAssigner\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u6b63\u6837\u672c\u89e3\u51b3\u5bc6\u96c6\u68c0\u6d4b\u5668\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u5c3a\u5ea6\u5bf9\u8c61\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u68c0\u6d4b\u5668\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u5bf9\u5c0f\u5bf9\u8c61\u5206\u914d\u7684\u6b63\u6837\u672c\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5c3a\u5ea6\u4e0d\u5e73\u8861\u3002RFAssigner\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5206\u914d\u7b56\u7565\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RFAssigner\u9996\u5148\u5229\u7528\u57fa\u4e8e\u70b9\u7684\u5148\u9a8c\u5efa\u7acb\u521d\u59cb\u6b63\u6837\u672c\u96c6\uff0c\u7136\u540e\u901a\u8fc7\u9ad8\u65af\u611f\u53d7\u91ce\uff08GRF\uff09\u8ddd\u79bb\u5ea6\u91cf\u672a\u5206\u914d\u5019\u9009\u4f4d\u7f6e\u4e0e\u771f\u5b9e\u5bf9\u8c61\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u81ea\u9002\u5e94\u5730\u4ece\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u8865\u5145\u6b63\u6837\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u5bf9\u8c61\u5c3a\u5ea6\u5206\u5e03\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRFAssigner\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u5bf9\u8c61\u4e0a\uff0c\u4f7f\u7528FCOS-ResNet-50\u68c0\u6d4b\u5668\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "conclusion": "RFAssigner\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u9ad8\u65af\u611f\u53d7\u91ce\u8ddd\u79bb\u7684\u81ea\u9002\u5e94\u6b63\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u68c0\u6d4b\u5668\u5728\u591a\u5c3a\u5ea6\u5bf9\u8c61\u4e0a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6a21\u5757\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002"}}
{"id": "2601.02163", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02163", "abs": "https://arxiv.org/abs/2601.02163", "authors": ["Chuanrui Hu", "Xingze Gao", "Zuyi Zhou", "Dannong Xu", "Yi Bai", "Xintong Li", "Hui Zhang", "Tong Li", "Chong Zhang", "Lidong Bing", "Yafeng Deng"], "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning", "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS", "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.", "AI": {"tldr": "EverMemOS\u662f\u4e00\u79cd\u81ea\u7ec4\u7ec7\u5185\u5b58\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u63d0\u5347LLMs\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8fde\u8d2f\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u56e0\u5b58\u50a8\u5b64\u7acb\u8bb0\u5f55\u548c\u68c0\u7d22\u7247\u6bb5\u800c\u96be\u4ee5\u7ef4\u6301\u8fde\u8d2f\u884c\u4e3a\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165EverMemOS\u7cfb\u7edf\uff0c\u5305\u62ecEpisodic Trace Formation\u3001Semantic Consolidation\u548cReconstructive Recollection\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u5bf9\u8bdd\u6d41\u3001\u7ec4\u7ec7\u8bed\u4e49\u7ed3\u6784\u548c\u6307\u5bfc\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728PersonaMem v2\u548c\u5b9a\u6027\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u7528\u6237\u753b\u50cf\u548cForesight\u7b49\u80fd\u529b\u3002", "conclusion": "EverMemOS\u901a\u8fc7\u81ea\u7ec4\u7ec7\u5185\u5b58\u64cd\u4f5c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8ba1\u7b97\u5185\u5b58\u7684\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8fde\u8d2f\u6027\u8868\u73b0\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01260", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01260", "abs": "https://arxiv.org/abs/2601.01260", "authors": ["Hamad Khan", "Saddam Hussain Khan"], "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance", "comment": "28 Pages, Tables 12, Figure 09", "summary": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.", "AI": {"tldr": "MambaFormer\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u5b9a\u5236\u5316\u4e13\u5bb6\u6a21\u578b\uff0c\u9ad8\u6548\u89e3\u51b3\u533b\u7597QA\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u8ba1\u7b97\u6210\u672c\u4e0e\u7ebf\u6027\u65f6\u95f4\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684MambaFormer\u6df7\u5408MoE\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8def\u7531\u81f3\u5b9a\u5236\u5316Transformer\u4e13\u5bb6\uff08ET5\uff09\u6216\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e13\u5bb6\uff08EMamba\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u635f\u5931\u8054\u5408\u4f18\u5316\u8def\u7531\u51b3\u7b56\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "result": "MambaFormer\u5728DentalQA\u548cPubMedQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08BERTScore=0.9180\uff09\uff0c\u5ef6\u8fdf\u6781\u4f4e\uff080.077\u79d2\uff09\uff0c\u901f\u5ea6\u6bd4T5-Large\u5feb24.4\u500d\u3002", "conclusion": "MambaFormer\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\u548c\u5b9a\u5236\u5316\u4e13\u5bb6\u6a21\u578b\uff0c\u5728\u533b\u7597QA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u4f4e\u5ef6\u8fdf\u7684\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02170", "abs": "https://arxiv.org/abs/2601.02170", "authors": ["Haolang Lu", "Minghui Pan", "Ripeng Li", "Guoshun Nan", "Jialin Zhuang", "Zijie Zhao", "Zhongxiang Sun", "Kun Wang", "Yang Liu"], "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning", "comment": null, "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u68c0\u6d4b\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u524d\u7f00\u7ea7\u4fe1\u53f7\u8ddf\u8e2a\u5168\u5c40\u72b6\u6001\u6f14\u53d8\u3002", "motivation": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u5f80\u5f80\u5fae\u5999\u4e14\u8de8\u6b65\u9aa4\u4f20\u64ad\uff0c\u9700\u5c06\u5176\u89c6\u4e3a\u6f14\u53d8\u7684\u6f5c\u5728\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\u3002", "method": "\u5c06\u6b65\u9aa4\u7ea7\u5e7b\u89c9\u5224\u65ad\u89c6\u4e3a\u5c40\u90e8\u89c2\u5bdf\uff0c\u5e76\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u4ee5\u8ddf\u8e2a\u63a8\u7406\u72b6\u6001\u7684\u5168\u5c40\u6f14\u53d8\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5b9e\u65f6\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u3002"}}
{"id": "2601.01281", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01281", "abs": "https://arxiv.org/abs/2601.01281", "authors": ["Sifatullah Sheikh Urmi", "Kirtonia Nuzath Tabassum Arthi", "Md Al-Imran"], "title": "AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures", "comment": "6 pages, 6 figures, 3 tables. Conference paper", "summary": "The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.", "AI": {"tldr": "AI models, especially VFDNET with MobileNetV3, effectively detect deepfakes, addressing challenges in digital authenticity.", "motivation": "The rise of AI-generated deepfakes poses significant challenges to digital authenticity, necessitating robust detection methods.", "method": "Four AI-based models (three CNNs and one Vision Transformer) were evaluated using large face image datasets, with data preprocessing and augmentation techniques applied to enhance performance.", "result": "VFDNET paired with MobileNetV3 achieved superior accuracy, demonstrating efficient performance in deepfake detection.", "conclusion": "AI, particularly VFDNET with MobileNetV3, shows strong capabilities for reliable deepfake detection, highlighting its potential in maintaining digital authenticity."}}
{"id": "2601.02314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02314", "abs": "https://arxiv.org/abs/2601.02314", "authors": ["Sourena Khanzadeh"], "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "comment": null, "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($\u03c6$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($\u03c1$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u667a\u80fd\u4f53\u7684\u63a8\u7406\u8f68\u8ff9\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u4ec5\u4f5c\u4e3a\u2018\u63a8\u7406\u5267\u573a\u2019\u3002\u63d0\u51fa\u7684Project Ariadne\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u63ed\u793a\u4e86\u666e\u904d\u5b58\u5728\u7684\u2018\u56e0\u679c\u89e3\u8026\u2019\u73b0\u8c61\uff0c\u5efa\u8bae\u7528Ariadne\u8bc4\u5206\u63d0\u5347\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u627f\u62c5\u9ad8\u98ce\u9669\u7684\u81ea\u4e3b\u51b3\u7b56\u4efb\u52a1\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u6210\u4e3a\u5173\u952e\u7684\u5b89\u5168\u95ee\u9898\u3002\u5c3d\u7ba1\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u80fd\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u8fd9\u4e9b\u8f68\u8ff9\u662f\u5426\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u51b3\u7b56\u9a71\u52a8\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86Project Ariadne\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u548c\u53cd\u4e8b\u5b9e\u903b\u8f91\uff0c\u901a\u8fc7\u5bf9\u4e2d\u95f4\u63a8\u7406\u8282\u70b9\u8fdb\u884c\u786c\u5e72\u9884\uff08\u5982\u903b\u8f91\u53cd\u8f6c\u3001\u524d\u63d0\u5426\u5b9a\u7b49\uff09\uff0c\u6d4b\u91cf\u7ec8\u7aef\u7b54\u6848\u7684\u56e0\u679c\u654f\u611f\u6027\uff08\u03c6\uff09\uff0c\u4ece\u800c\u5ba1\u8ba1\u63a8\u7406\u7684\u56e0\u679c\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u666e\u904d\u5b58\u5728\u2018\u5fe0\u5b9e\u6027\u5dee\u8ddd\u2019\uff0c\u5e76\u68c0\u6d4b\u5230\u4e00\u79cd\u5e7f\u6cdb\u5b58\u5728\u7684\u6545\u969c\u6a21\u5f0f\u2018\u56e0\u679c\u89e3\u8026\u2019\uff08\u8fdd\u53cd\u5bc6\u5ea6\u03c1\u9ad8\u8fbe0.77\uff09\u3002\u667a\u80fd\u4f53\u5728\u5185\u90e8\u903b\u8f91\u77db\u76fe\u7684\u60c5\u51b5\u4e0b\u4ecd\u5f97\u51fa\u76f8\u540c\u7ed3\u8bba\uff0c\u8868\u660e\u5176\u63a8\u7406\u8f68\u8ff9\u4ec5\u4e3a\u2018\u63a8\u7406\u5267\u573a\u2019\uff0c\u51b3\u7b56\u5b9e\u9645\u7531\u6f5c\u5728\u53c2\u6570\u5148\u9a8c\u4e3b\u5bfc\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u81ea\u4e3b\u51b3\u7b56\u4e2d\u5b58\u5728\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u5176\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u53ef\u80fd\u53ea\u662f\u4e8b\u540e\u5408\u7406\u5316\u800c\u975e\u771f\u5b9e\u7684\u51b3\u7b56\u9a71\u52a8\u56e0\u7d20\u3002\u901a\u8fc7Project Ariadne\u6846\u67b6\uff0c\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u666e\u904d\u5b58\u5728\u2018\u56e0\u679c\u89e3\u8026\u2019\u73b0\u8c61\uff0c\u5373\u63a8\u7406\u8f68\u8ff9\u4e0e\u51b3\u7b56\u8131\u8282\u3002\u5efa\u8bae\u91c7\u7528Ariadne\u8bc4\u5206\u4f5c\u4e3a\u65b0\u57fa\u51c6\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002"}}
{"id": "2601.01285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01285", "abs": "https://arxiv.org/abs/2601.01285", "authors": ["Md. Sanaullah Chowdhury Lameya Sabrin"], "title": "S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss", "comment": null, "summary": "Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\\mathcal{O}(HW \\log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\\% Dice on polyp segmentation, 83.77\\% on surgical instruments (+17.85\\% over the prior art) and 80.90\\% on brain tumors, with consistent 3-18\\% improvements over specialized baselines while using 3.5--6$\\times$ fewer parameters than transformer-based methods.", "AI": {"tldr": "S2M-Net\u901a\u8fc7SSTM\u548cMASL\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e09\u96be\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u5728\u5c40\u90e8\u7cbe\u5ea6\u3001\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u73b0\u6709\u67b6\u6784\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "S2M-Net\u7ed3\u5408\u4e86\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff1a(i) Spectral-Selective Token Mixer (SSTM)\uff0c\u901a\u8fc7\u622a\u65ad\u76842D FFT\u548c\u53ef\u5b66\u4e60\u7684\u9891\u7387\u6ee4\u6ce2\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\uff1b(ii) Morphology-Aware Adaptive Segmentation Loss (MASL)\uff0c\u901a\u8fc7\u5206\u6790\u7ed3\u6784\u7279\u5f81\u81ea\u52a8\u8c03\u6574\u635f\u5931\u51fd\u6570\u3002", "result": "\u572816\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cS2M-Net\u8868\u73b0\u51fa\u8272\uff0c\u598296.12% Dice\u5728\u606f\u8089\u5206\u5272\u4e0a\uff0c83.77%\u5728\u5916\u79d1\u5668\u68b0\u4e0a\uff08\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad817.85%\uff09\uff0c80.90%\u5728\u8111\u80bf\u7624\u4e0a\uff0c\u4e14\u53c2\u6570\u6570\u91cf\u6bd4\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5c113.5-6\u500d\u3002", "conclusion": "S2M-Net\u901a\u8fc7\u521b\u65b0\u7684SSTM\u548cMASL\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e09\u96be\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u8ba1\u7b97\u6548\u7387\u3001\u5c40\u90e8\u7cbe\u5ea6\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.02346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R-7B, a compact reasoning-optimized model, achieves SOTA performance through efficient training and architecture, proving SLMs can rival larger models.", "motivation": "To prove that small language models (SLMs) can achieve competitive reasoning performance without increasing model size, emphasizing parameter efficiency.", "method": "Utilizes careful data curation, efficient supervised fine-tuning (SFT), RL scaling, and a hybrid-parallel architecture design for faster inference and token efficiency.", "result": "Falcon-H1R matches or outperforms SOTA reasoning models 2\u00d7 to 7\u00d7 larger across benchmarks, with advancements in inference speed, token efficiency, and accuracy.", "conclusion": "Falcon-H1R-7B demonstrates that compact models can achieve robust and scalable reasoning performance through targeted training and architectural optimizations, challenging the need for larger models."}}
{"id": "2601.01312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01312", "abs": "https://arxiv.org/abs/2601.01312", "authors": ["Kailash A. Hambarde", "Hugo Proen\u00e7a", "Md Rashidunnabi", "Pranita Samale", "Qiwei Yang", "Pingping Zhang", "Zijing Gong", "Yuhao Wang", "Xi Zhang", "Ruoshui Qu", "Qiaoyun He", "Yuhang Zhang", "Thi Ngoc Ha Nguyen", "Tien-Dung Mai", "Cheng-Jun Kang", "Yu-Fan Lin", "Jin-Hui Jiang", "Chih-Chung Hsu", "Tam\u00e1s Endrei", "Gy\u00f6rgy Cserey", "Ashwat Rajbhandari"], "title": "VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results", "comment": null, "summary": "Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .", "AI": {"tldr": "VReID-XFD\u662f\u4e00\u4e2a\u6781\u7aef\u8fdc\u8ddd\u79bb\u7a7a\u5bf9\u5730\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u968f\u8ddd\u79bb\u548c\u89c6\u89d2\u7684\u4e0b\u964d\u8d8b\u52bf\uff0c\u6700\u4f73\u65b9\u6cd5\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u91cd\u8bc6\u522b\u7cfb\u7edf\u5728\u6781\u7aef\u8fdc\u8ddd\u79bb\u4e0b\u56e0\u5206\u8fa8\u7387\u4e0b\u964d\u3001\u89c6\u89d2\u53d8\u5316\u3001\u8fd0\u52a8\u7ebf\u7d22\u4e0d\u7a33\u5b9a\u548c\u670d\u88c5\u53d8\u5316\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u65b0\u7684\u7814\u7a76\u57fa\u51c6\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f15\u5165\u4e86VReID-XFD\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u6781\u7aef\u8fdc\u8ddd\u79bb\uff08XFD\uff09\u7a7a\u4e2d\u5230\u5730\u9762\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\uff0c\u5305\u542b371\u4e2a\u8eab\u4efd\u300111,288\u4e2a\u8f68\u8ff9\u548c11.75\u767e\u4e07\u5e27\uff0c\u8986\u76d6\u591a\u79cd\u9ad8\u5ea6\u3001\u89c6\u89d2\u548c\u6c34\u5e73\u8ddd\u79bb\u3002", "result": "VReID-XFD-25\u6311\u6218\u5438\u5f15\u4e8610\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u6700\u4f73\u65b9\u6cd5SAS-PReID\u5728\u7a7a\u5bf9\u5730\u8bbe\u7f6e\u4e0b\u4ec5\u8fbe\u523043.93%\u7684mAP\u3002", "conclusion": "VReID-XFD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6781\u7aef\u8fdc\u8ddd\u79bb\u7a7a\u4e2d\u5230\u5730\u9762\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u89c6\u9891\u57fa\u51c6\u548c\u793e\u533a\u6311\u6218\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u968f\u9ad8\u5ea6\u548c\u8ddd\u79bb\u7684\u5355\u8c03\u4e0b\u964d\u4ee5\u53ca\u4fef\u89c6\u89c6\u89d2\u7684\u666e\u904d\u52a3\u52bf\u3002"}}
{"id": "2601.01322", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01322", "abs": "https://arxiv.org/abs/2601.01322", "authors": ["Hongjie Wang", "Niraj K. Jha"], "title": "LinMU: Multimodal Understanding Made Linear", "comment": "23 pages, 7 figures", "summary": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.", "AI": {"tldr": "LinMU\u662f\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684VLM\u8bbe\u8ba1\uff0c\u901a\u8fc7M-MATE\u5757\u548c\u4e09\u9636\u6bb5\u84b8\u998f\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u800c\u65e0\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u53ca\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "LinMU\u901a\u8fc7M-MATE\u5757\uff08\u7ed3\u5408Flex-MA\u5206\u652f\u548cLocal-Swin\u5206\u652f\uff09\u66ff\u6362\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u84b8\u998f\u6846\u67b6\uff08\u521d\u59cb\u5316\u3001\u8054\u5408\u5fae\u8c03\u3001LoRA\u9002\u914d\u5668\u5fae\u8c03\uff09\u8f6c\u6362\u9884\u8bad\u7ec3VLM\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLinMU\u6027\u80fd\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u5f53\uff0c\u4f46TTFT\u964d\u4f4e2.7\u500d\uff0c\u4ee4\u724c\u541e\u5410\u91cf\u63d0\u53479.0\u500d\u3002", "conclusion": "LinMU\u6846\u67b6\u8bc1\u660e\u4e86\u65e0\u9700\u4e8c\u6b21\u6ce8\u610f\u529b\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u7684\u957f\u4e0a\u4e0b\u6587VLM\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.01339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01339", "abs": "https://arxiv.org/abs/2601.01339", "authors": ["Weihang You", "Hanqi Jiang", "Yi Pan", "Junhao Chen", "Tianming Liu", "Fei Dou"], "title": "Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning", "comment": null, "summary": "Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.", "AI": {"tldr": "NeuroAlign\u901a\u8fc7\u6a21\u62df\u89c6\u89c9\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u7ed3\u5408NTCL\u548c\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\uff0c\u63d0\u5347\u4e86fMRI-\u89c6\u9891\u5bf9\u9f50\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u53cd\u6620\u89c6\u89c9\u5904\u7406\u7684\u5c42\u6b21\u6027\u548c\u65f6\u5e8f\u6027\uff0cNeuroAlign\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\uff1a\u5168\u5c40\u8bed\u4e49\u7406\u89e3\uff08NTCL\uff09\u548c\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u5339\u914d\uff08\u589e\u5f3a\u5411\u91cf\u91cf\u5316\uff09\uff0c\u7ed3\u5408DynaSyncMM-EMA\u5b9e\u73b0\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cNeuroAlign\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NeuroAlign\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86fMRI-\u89c6\u9891\u5bf9\u9f50\u7684\u7cbe\u5ea6\uff0c\u4e3a\u7406\u89e3\u89c6\u89c9\u8ba4\u77e5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.01352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01352", "abs": "https://arxiv.org/abs/2601.01352", "authors": ["Yixuan Lai", "He Wang", "Kun Zhou", "Tianjia Shao"], "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding", "comment": null, "summary": "Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u53c2\u8003\u89c6\u9891\u7684\u52a8\u6001\u8eab\u4efd\u6807\u8bb0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u8fd0\u52a8\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u57fa\u4e8e\u5355\u5f20\u56fe\u50cf\u751f\u6210\u89c6\u9891\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u65f6\u95f4\u7b7e\u540d\uff0c\u5bfc\u81f4\u59ff\u52bf\u9501\u5b9a\u3001\u4e0d\u81ea\u7136\u7684\u626d\u66f2\u548c\u2018\u5e73\u5747\u2019\u9762\u5b54\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8eab\u4efd\u4fdd\u6301\u4e0e\u8fd0\u52a8\u81ea\u7136\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u53c2\u8003\u6765\u6539\u8fdb\u89c6\u9891\u751f\u6210\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563-\u53d8\u6362\u5668\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d8\u4f53\uff0c\u8be5\u6a21\u578b\u5229\u7528\u77ed\u53c2\u8003\u89c6\u9891\u800c\u975e\u5355\u5f20\u8096\u50cf\u6765\u6355\u6349\u4e3b\u4f53\u7279\u5b9a\u7684\u52a8\u6001\u6a21\u5f0f\u3002\u901a\u8fc7Sinkhorn\u8def\u7531\u7f16\u7801\u5668\u4ece\u53c2\u8003\u89c6\u9891\u4e2d\u5b66\u4e60\u7d27\u51d1\u7684\u8eab\u4efd\u6807\u8bb0\uff0c\u8fd9\u4e9b\u6807\u8bb0\u80fd\u591f\u6355\u6349\u7279\u5f81\u52a8\u6001\uff0c\u540c\u65f6\u4e0e\u9884\u8bad\u7ec3\u4e3b\u5e72\u517c\u5bb9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u4e3b\u4f53\u548c\u63d0\u793a\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5927\u59ff\u52bf\u53d8\u5316\u548c\u4e30\u5bcc\u8868\u60c5\u4e0b\u7684\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u5fe0\u5b9e\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u77ed\u53c2\u8003\u89c6\u9891\u7684\u52a8\u6001\u8eab\u4efd\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u59ff\u52bf\u53d8\u5316\u548c\u4e30\u5bcc\u8868\u60c5\u4e0b\u8eab\u4efd\u4fdd\u6301\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u751f\u6210\u7684\u63d0\u793a\u5fe0\u5b9e\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002"}}
{"id": "2601.01356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01356", "abs": "https://arxiv.org/abs/2601.01356", "authors": ["Dang H. Pham", "Tu N. Nguyen", "Hoa N. Nguyen"], "title": "Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance", "comment": "in Vietnamese language", "summary": "Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\uff08\u76d1\u7763\u3001\u57df\u9002\u5e94\u3001\u65e0\u76d1\u7763\uff09\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u884c\u4eba\u91cd\u8bc6\u522b\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u5916\u89c2\u53d8\u5316\u3001\u57df\u504f\u79fb\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7b49\u6311\u6218\uff0c\u9700\u63d0\u5347\u6027\u80fd\u4ee5\u5e94\u5bf9\u5b9e\u9645\u76d1\u63a7\u9700\u6c42\u3002", "method": "1. SCM-ReID\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u6df7\u5408\u635f\u5931\u4f18\u5316\uff08\u5206\u7c7b\u3001\u4e2d\u5fc3\u3001\u4e09\u5143\u7ec4\u548c\u8d28\u5fc3-\u4e09\u5143\u7ec4\u635f\u5931\uff09\u30022. IQAGA\u548cDAPRH\u7ed3\u5408GAN\u56fe\u50cf\u589e\u5f3a\u3001\u57df\u4e0d\u53d8\u6620\u5c04\u548c\u4f2a\u6807\u7b7e\u7ec6\u5316\u30023. ViTC-UReID\u5229\u7528Vision Transformer\u7279\u5f81\u7f16\u7801\u548c\u76f8\u673a\u611f\u77e5\u4ee3\u7406\u5b66\u4e60\u3002", "result": "SCM-ReID\u5728Market-1501\u548cCUHK03\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff1bIQAGA\u548cDAPRH\u5728\u8de8\u57df\u573a\u666f\u4e2dmAP\u548cRank-1\u63d0\u5347\u9ad8\u8fbe12%\uff1bViTC-UReID\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u4e09\u79cd\u5148\u8fdb\u65b9\u6cd5\uff08SCM-ReID\u3001IQAGA\u548cDAPRH\u3001ViTC-UReID\uff09\uff0c\u5728\u76d1\u7763\u3001\u65e0\u76d1\u7763\u57df\u9002\u5e94\u548c\u5b8c\u5168\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u76d1\u63a7\u7cfb\u7edf\u7684\u7a33\u5065\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.01360", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01360", "abs": "https://arxiv.org/abs/2601.01360", "authors": ["Jiawei Fang", "Ruonan Zheng", "Xiaoxia Gao", "Shifan Jiang", "Anjun Chen", "Qi Ye", "Shihui Guo"], "title": "Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser", "comment": "11 pages, 4 figures", "summary": "Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.", "AI": {"tldr": "GID\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\uff0c\u7528\u4e8e\u5bbd\u677e\u670d\u88c5\u4e2dIMU\u4f20\u611f\u5668\u7684\u53bb\u566a\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5904\u7406\u548c\u4e13\u5bb6\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u52a8\u4f5c\u6355\u6349\u3002", "motivation": "\u89e3\u51b3\u5bbd\u677e\u670d\u88c5\u4e2dIMU\u4f20\u611f\u5668\u56e0\u4f4d\u79fb\u5f15\u5165\u7684\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u7a7f\u6234\u5f0f\u60ef\u6027\u52a8\u4f5c\u6355\u6349\u7684\u8212\u9002\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "GID\u91c7\u7528\u4e09\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff1a\u4f4d\u7f6e\u7279\u5b9a\u53bb\u566a\u3001\u81ea\u9002\u5e94\u8de8\u670d\u88c5\u878d\u5408\u548c\u901a\u7528\u59ff\u6001\u9884\u6d4b\uff0c\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u4e13\u5bb6\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u878d\u5408\u6a21\u5757\u3002", "result": "GID\u5728\u5355\u7528\u6237\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u53bb\u566a\uff0c\u5e76\u80fd\u6cdb\u5316\u81f3\u672a\u89c1\u8fc7\u7684\u7528\u6237\u3001\u52a8\u4f5c\u548c\u670d\u88c5\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709\u60ef\u6027\u52a8\u4f5c\u6355\u6349\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "GID\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684Transformer\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5bbd\u677e\u670d\u88c5\u4e2dIMU\u4f20\u611f\u5668\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u65f6\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7528\u6237\u3001\u52a8\u4f5c\u548c\u670d\u88c5\u7c7b\u578b\u3002"}}
{"id": "2601.01364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01364", "abs": "https://arxiv.org/abs/2601.01364", "authors": ["Mostofa Rafid Uddin", "Mahek Vora", "Qifeng Wu", "Muyuan Chen", "Min Xu"], "title": "Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography", "comment": null, "summary": "Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u566a\u58f0cryo-ET\u6570\u636e\u4e2d\u5206\u79bb\u5f62\u6001\u4e0e\u53d8\u6362\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u679c\u5e76\u53d1\u73b0\u65b0\u5f62\u6001\u3002", "motivation": "\u73b0\u6709\u7684\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u5728\u5206\u6790cryo-ET\u6570\u636e\u65f6\uff0c\u5e38\u9057\u6f0f\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u5f62\u6001\uff0c\u4e14\u9700\u8981\u5927\u91cf\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7684\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u591a\u9009\u62e9\u5b66\u4e60\u6a21\u5757\uff0c\u4ececryo-ET\u6570\u636e\u4e2d\u5206\u79bbSE(3)\u53d8\u6362\u4e0e\u5f62\u6001\u5185\u5bb9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9ecryo-ET\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4e86\u65b0\u7684\u5927\u5206\u5b50\u5f62\u6001\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u9ad8\u5ea6\u566a\u58f0\u7684cryo-ET\u6570\u636e\u4e2d\u5206\u79bbSE(3)\u53d8\u6362\u4e0e\u5f62\u6001\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f62\u6001\u5b66\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u5e76\u53d1\u73b0\u4e86\u6b64\u524d\u672a\u8bc6\u522b\u7684\u5927\u5206\u5b50\u5f62\u6001\u3002"}}
{"id": "2601.01386", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01386", "abs": "https://arxiv.org/abs/2601.01386", "authors": ["Xiaobao Wei", "Zhangjie Ye", "Yuxiang Gu", "Zunjie Zhu", "Yunfei Guo", "Yingying Shen", "Shan Zhao", "Ming Lu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Rongfeng Lu", "Hangjun Ye"], "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking", "comment": null, "summary": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u505c\u8f66\u573a\u666f3D\u91cd\u5efa\u57fa\u51c6ParkRecon3D\u548c\u6846\u67b6ParkGaussian\uff0c\u901a\u8fc7\u6574\u54083DGS\u6280\u672f\u548c\u505c\u8f66\u4f4d\u611f\u77e5\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce82D\u505c\u8f66\u4f4d\u611f\u77e5\uff0c\u800c3D\u91cd\u5efa\u5728\u590d\u6742\u505c\u8f66\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u51e0\u4f55\u6355\u6349\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5355\u7eaf\u63d0\u5347\u91cd\u5efa\u89c6\u89c9\u8d28\u91cf\u5e76\u4e0d\u80fd\u76f4\u63a5\u63d0\u5347\u81ea\u52a8\u505c\u8f66\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86ParkGaussian\u6846\u67b6\uff0c\u6574\u5408\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u505c\u8f66\u4f4d\u611f\u77e5\u7684\u91cd\u5efa\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728ParkRecon3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cParkGaussian\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u4e00\u81f4\u6027\u3002", "conclusion": "ParkGaussian\u6846\u67b6\u5728ParkRecon3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u611f\u77e5\u4e00\u81f4\u6027\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2601.01393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01393", "abs": "https://arxiv.org/abs/2601.01393", "authors": ["Shamik Shafkat Avro", "Nazira Jesmin Lina", "Shahanaz Sharmin"], "title": "Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets", "comment": "All authors contributed equally to this work", "summary": "This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684CustomCNN\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u5728\u591a\u9886\u57df\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u57ce\u5e02\u548c\u519c\u4e1a\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u67b6\u6784\u8bbe\u8ba1\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u591a\u9886\u57df\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u4e3a\u667a\u80fd\u57ce\u5e02\u548c\u519c\u4e1a\u6210\u50cf\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CustomCNN\uff09\uff0c\u7ed3\u5408\u4e86\u6b8b\u5dee\u8fde\u63a5\u3001Squeeze-and-Excitation\u6ce8\u610f\u529b\u673a\u5236\u3001\u6e10\u8fdb\u901a\u9053\u7f29\u653e\u548cKaiming\u521d\u59cb\u5316\uff0c\u4ee5\u63d0\u5347\u6570\u636e\u8868\u793a\u80fd\u529b\u548c\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cCustomCNN\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u6d41\u884cCNN\u67b6\u6784\u76f8\u5f53\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u57ce\u5e02\u548c\u519c\u4e1a\u6210\u50cf\u5e94\u7528\u4e2d\uff0c\u6df1\u601d\u719f\u8651\u7684\u67b6\u6784\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.01406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01406", "abs": "https://arxiv.org/abs/2601.01406", "authors": ["Habiba Kausar", "Saeed Anwar", "Omar Jamal Hammad", "Abdul Bais"], "title": "SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution", "comment": null, "summary": "Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.", "AI": {"tldr": "SwinIFS \u662f\u4e00\u79cd\u57fa\u4e8e\u5730\u6807\u5f15\u5bfc\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5148\u9a8c\u548c\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4fdd\u7559\u7684\u9762\u90e8\u56fe\u50cf\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u653e\u5927\u500d\u6570\u3002", "motivation": "\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u5728\u6062\u590d\u4e25\u91cd\u9000\u5316\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u65f6\u9762\u4e34\u4e22\u5931\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u548c\u8eab\u4efd\u7279\u5f81\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bc6\u96c6\u9ad8\u65af\u70ed\u56fe\u7684\u5173\u952e\u9762\u90e8\u6807\u5fd7\u548c\u7d27\u51d1\u7684 Swin Transformer \u4e3b\u5e72\uff0c\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u4fdd\u6301\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728 CelebA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSwinIFS \u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u3001\u66f4\u6e05\u6670\u7684\u590d\u539f\u6548\u679c\u548c\u66f4\u597d\u7684\u8eab\u4efd\u4fdd\u7559\uff0c\u5373\u4f7f\u5728 8 \u500d\u653e\u5927\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SwinIFS \u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9762\u90e8\u589e\u5f3a\u3001\u76d1\u63a7\u548c\u6570\u5b57\u4fee\u590d\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2601.01408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01408", "abs": "https://arxiv.org/abs/2601.01408", "authors": ["Gong Gao", "Zekai Wang", "Jian Zhao", "Ziqi Xie", "Xianhui Liu", "Weidong Zhao"], "title": "Mask-Guided Multi-Task Network for Face Attribute Recognition", "comment": "23 pages, 9 figures", "summary": "Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.", "AI": {"tldr": "MGMTN\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u6a21\u5b66\u4e60\u548c\u7ec4-\u5168\u5c40\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u5197\u4f59\u7279\u5f81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u591a\u4efb\u52a1\u5c5e\u6027\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u533a\u57df\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\uff0c\u5bfc\u81f4\u5197\u4f59\u7279\u5f81\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMGMTN\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86AML\uff08\u81ea\u9002\u5e94\u63a9\u6a21\u5b66\u4e60\uff09\u548cG2FF\uff08\u7ec4-\u5168\u5c40\u7279\u5f81\u878d\u5408\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5173\u952e\u70b9\u6807\u6ce8\u6a21\u578b\u548c\u5168\u5377\u79ef\u7f51\u7edc\uff0c\u7cbe\u786e\u5b9a\u4f4d\u5173\u952e\u9762\u90e8\u533a\u57df\u5e76\u751f\u6210\u7ec4\u63a9\u6a21\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMGMTN\u663e\u8457\u63d0\u5347\u4e86FAR\u6027\u80fd\u3002", "conclusion": "MGMTN\u901a\u8fc7\u7ed3\u5408AML\u548cG2FF\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u51cf\u5c11\u5197\u4f59\u7279\u5f81\u548c\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.01416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01416", "abs": "https://arxiv.org/abs/2601.01416", "authors": ["Yue Zhou", "Ran Ding", "Xue Yang", "Xue Jiang", "Xingzhao Liu"], "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval", "comment": "12 pages, 9 figures", "summary": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u7a7a\u4e2d\u4ee3\u7406AirSpatialBot\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u56fe\u50cf\u7406\u89e3\u9884\u8bad\u7ec3\u548c\u7a7a\u95f4\u7406\u89e3\u5fae\u8c03\uff0c\u5f00\u53d1\u4e86\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u7a7a\u4e2d\u4ee3\u7406AirSpatialBot\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u5177\u5907\u52a8\u6001\u4efb\u52a1\u89c4\u5212\u3001\u56fe\u50cf\u7406\u89e3\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b\u7684AirSpatialBot\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6AirSpatial\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u5177\u5907\u7ec6\u7c92\u5ea6\u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\u548c\u68c0\u7d22\u80fd\u529b\u7684\u7a7a\u4e2d\u4ee3\u7406AirSpatialBot\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01425", "abs": "https://arxiv.org/abs/2601.01425", "authors": ["Xu Guo", "Fulong Ye", "Xinghui Li", "Pengqi Tu", "Pengze Zhang", "Qichao Sun", "Songtao Zhao", "Xiangwang Hou", "Qian He"], "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "comment": "Project: https://guoxu1233.github.io/DreamID-V/", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "AI": {"tldr": "DreamID-V \u662f\u4e00\u4e2a\u57fa\u4e8e Diffusion Transformer \u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u7ba1\u9053\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4fdd\u7559\u7684\u540c\u65f6\u96be\u4ee5\u7ef4\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u7684\u4f18\u52bf\u65e0\u7f1d\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e Diffusion Transformer \u7684\u6846\u67b6 DreamID-V\uff0c\u91c7\u7528 Modality-Aware Conditioning \u6a21\u5757\u3001Synthetic-to-Real Curriculum \u673a\u5236\u548c Identity-Coherence Reinforcement Learning \u7b56\u7565\u3002", "result": "DreamID-V \u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "DreamID-V \u6846\u67b6\u5728\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u901a\u7528\u6027\uff0c\u53ef\u65e0\u7f1d\u9002\u5e94\u591a\u79cd\u4ea4\u6362\u76f8\u5173\u4efb\u52a1\u3002"}}
{"id": "2601.01431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01431", "abs": "https://arxiv.org/abs/2601.01431", "authors": ["Weiqi Yu", "Yiyang Yao", "Lin He", "Jianming Lv"], "title": "EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views", "comment": "PRCV 2025", "summary": "Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.", "AI": {"tldr": "EdgeNeRF\u5229\u7528\u8fb9\u7f18\u5f15\u5bfc\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u63d0\u5347\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\uff0c\u4fdd\u7559\u7ec6\u8282\u5e76\u51cf\u5c11\u4f2a\u5f71\uff0c\u53ef\u5373\u63d2\u5373\u7528\u3002", "motivation": "\u89e3\u51b3NeRF\u5728\u7a00\u758f\u8f93\u5165\u4e0b\u56e0\u51e0\u4f55\u4f2a\u5f71\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u5168\u5c40\u6df1\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\u4e22\u5931\u51e0\u4f55\u8fb9\u754c\u7ec6\u8282\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51faEdgeNeRF\u7b97\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u6cd5\u7ebf\u5728\u8fb9\u7f18\u5904\u7684\u7a81\u53d8\u7279\u6027\uff0c\u9996\u5148\u63d0\u53d6\u8f93\u5165\u56fe\u50cf\u7684\u8fb9\u7f18\uff0c\u7136\u540e\u5728\u975e\u8fb9\u7f18\u533a\u57df\u5e94\u7528\u6df1\u5ea6\u548c\u6cd5\u7ebf\u6b63\u5219\u5316\u7ea6\u675f\u3002", "result": "\u5728LLFF\u548cDTU\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEdgeNeRF\u5728\u4fdd\u7559\u9510\u5229\u51e0\u4f55\u8fb9\u754c\u548c\u6291\u5236\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6a21\u5757\u53ef\u5373\u63d2\u5373\u7528\u63d0\u5347\u5176\u4ed6\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "EdgeNeRF\u901a\u8fc7\u8fb9\u7f18\u5f15\u5bfc\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\uff0c\u4fdd\u7559\u4e86\u9ad8\u9891\u7ec6\u8282\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6\u65b9\u6cd5\u4e2d\u3002"}}
{"id": "2601.01439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01439", "abs": "https://arxiv.org/abs/2601.01439", "authors": ["Wenqi Ren", "Weijie Wang", "Meng Zheng", "Ziyan Wu", "Yang Tang", "Zhun Zhong", "Nicu Sebe"], "title": "In defense of the two-stage framework for open-set domain adaptive semantic segmentation", "comment": null, "summary": "Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.", "AI": {"tldr": "SATS\u901a\u8fc7\u5206\u79bb-\u9002\u5e94\u7b56\u7565\u548c\u786c\u672a\u77e5\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u96c6\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4e00\u9636\u6bb5\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u65f6\uff0c\u56e0\u6807\u6ce8\u4e0d\u5e73\u8861\u5bfc\u81f4\u5df2\u77e5\u7c7b\u522b\u8d1f\u8fc1\u79fb\u548c\u672a\u77e5\u7c7b\u522b\u6b20\u62df\u5408\u3002", "method": "\u63d0\u51faSATS\uff08\u5206\u79bb-\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff09\uff0c\u5305\u62ec\u5df2\u77e5/\u672a\u77e5\u7c7b\u522b\u5206\u79bb\u548c\u672a\u77e5\u611f\u77e5\u57df\u9002\u5e94\u4e24\u4e2a\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165\u786c\u672a\u77e5\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5728GTA5-to-Cityscapes\u548cSYNTHIA-to-Cityscapes\u57fa\u51c6\u4e0a\uff0cH-Score\u5206\u522b\u63d0\u53473.85%\u548c18.64%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SATS\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u548c\u9002\u5e94\u4e24\u4e2a\u6b65\u9aa4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u96c6\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2601.01454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01454", "abs": "https://arxiv.org/abs/2601.01454", "authors": ["Xiao Li", "Zilong Liu", "Yining Liu", "Zhuhong Li", "Na Dong", "Sitian Qin", "Xiaolin Hu"], "title": "PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.10918", "summary": "To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.", "AI": {"tldr": "PIN++\u662f\u4e00\u4e2a\u5305\u542b100K\u56fe\u50cf\u7684\u8be6\u7ec6\u90e8\u5206\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u90e8\u5206\u7684\u6a21\u578b\u6027\u80fd\u3002\u63d0\u51fa\u7684MPM\u6a21\u578b\u7ed3\u5408\u4f2a\u90e8\u5206\u6807\u7b7e\u548c\u539f\u59cb\u6ce8\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u8bc6\u522b\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u9ad8\u8d28\u91cf\u90e8\u5206\u6ce8\u91ca\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5f15\u5165\u4e86PartImageNet++\uff08PIN++\uff09\u6570\u636e\u96c6\uff0c\u63d0\u4f9bImageNet-1K\u6240\u6709\u7c7b\u522b\u7684\u8be6\u7ec6\u90e8\u5206\u6ce8\u91ca\u3002", "method": "\u5229\u7528PIN++\u8bad\u7ec3\u4e86\u4e00\u4e2a\u90e8\u5206\u5206\u5272\u7f51\u7edc\uff0c\u751f\u6210\u4f2a\u90e8\u5206\u6807\u7b7e\u7528\u4e8e\u672a\u6807\u6ce8\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86MPM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u4f20\u7edf\u8bc6\u522b\u67b6\u6784\u548c\u8f85\u52a9\u65c1\u8def\u5c42\uff0c\u540c\u65f6\u53d7\u5230\u4f2a\u90e8\u5206\u6807\u7b7e\u548c\u539f\u59cb\u90e8\u5206\u6ce8\u91ca\u7684\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u57fa\u4e8e\u90e8\u5206\u7684\u6a21\u578b\u5728\u5bf9\u8c61\u8bc6\u522b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u4e3a\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u90e8\u5206\u5206\u5272\u3001\u5bf9\u8c61\u5206\u5272\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff09\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\u3002", "conclusion": "PIN++\u6570\u636e\u96c6\u548cMPM\u6a21\u578b\u4e0d\u4ec5\u63d0\u5347\u4e86\u57fa\u4e8e\u90e8\u5206\u7684\u6a21\u578b\u5728\u5bf9\u8c61\u8bc6\u522b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u4e3a\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u90e8\u5206\u6ce8\u91ca\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01456", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01456", "abs": "https://arxiv.org/abs/2601.01456", "authors": ["Wentao Bian", "Fenglei Xu"], "title": "Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration", "comment": "10 pages, 4 figures, 3 tables", "summary": "In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in \"Fuse-then-Refine\" paradigms: the \"Plasticity-Stability Dilemma.\" In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.", "AI": {"tldr": "DA-FSS\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u548c\u51e0\u4f55\u8def\u5f84\uff0c\u89e3\u51b3\u4e86FS-PCS\u4e2d\u7684\u51b2\u7a81\u548cCLIP\u7684\u6df7\u6dc6\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5c11\u6837\u672c3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u2018\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\u2019\u548cCLIP\u7684\u7c7b\u95f4\u6df7\u6dc6\u5bfc\u81f4\u7684\u8bed\u4e49\u76f2\u533a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5e76\u884c\u4e13\u5bb6\u7ec6\u5316\u6a21\u5757\u548c\u5806\u53e0\u4ef2\u88c1\u6a21\u5757\uff08SAM\uff09\uff0c\u5206\u522b\u751f\u6210\u6a21\u6001\u76f8\u5173\u6027\u548c\u4ef2\u88c1\u5404\u6a21\u6001\u8def\u5f84\u3002\u51e0\u4f55\u4e13\u5bb6\u4fdd\u6301\u53ef\u5851\u6027\uff0c\u8bed\u4e49\u4e13\u5bb6\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u89e3\u8026\u5bf9\u9f50\u6a21\u5757\uff08DAM\uff09\u534f\u8c03\u77e5\u8bc6\u4f20\u9012\u3002", "result": "\u5728S3DIS\u548cScanNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDA-FSS\u4f18\u4e8eMM-FSS\uff0c\u4e14\u5728\u51e0\u4f55\u8fb9\u754c\u3001\u5b8c\u6574\u6027\u548c\u7eb9\u7406\u533a\u5206\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "DA-FSS\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u548c\u51e0\u4f55\u8def\u5f84\uff0c\u5e76\u76f8\u4e92\u8c03\u8282\u5176\u68af\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86FS-PCS\u4e2d\u7684\u2018\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\u2019\u548cCLIP\u7684\u7c7b\u95f4\u6df7\u6dc6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.01457", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01457", "abs": "https://arxiv.org/abs/2601.01457", "authors": ["Mingxing Zhan", "Li Zhang", "Beibei Wang", "Yingjie Wang", "Zenglin Shi"], "title": "Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation", "comment": null, "summary": "Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u8a00\u548c\u89c6\u89c9\u7279\u5f81\u7ed3\u5408\uff0c\u5728\u56fa\u5b9a\u9aa8\u5e72\u7f51\u7edc\u4e0b\u6062\u590d\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u7531\u4e8e\u5168\u5c40\u5c3a\u5ea6\u4e0d\u53ef\u8bc6\u522b\u548c\u57df\u8f6c\u79fb\u654f\u611f\u6027\u800c\u96be\u4ee5\u89e3\u51b3\uff0c\u800c\u76f8\u5bf9\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u5728\u8fc1\u79fb\u4e2d\u8868\u73b0\u826f\u597d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u7279\u5b9a\u7684\u4eff\u5c04\u53d8\u6362\u5728\u9006\u6df1\u5ea6\u4e2d\u6062\u590d\u5ea6\u91cf\u6df1\u5ea6\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6821\u51c6\u5934\u90e8\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5bf9\u6df1\u5ea6\u9aa8\u5e72\u7f51\u7edc\u548cCLIP\u6587\u672c\u7f16\u7801\u5668\u56fa\u5b9a\u3002\u5229\u7528\u8bed\u8a00\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8fb9\u754c\uff0c\u7ea6\u675f\u53ef\u884c\u6821\u51c6\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u9009\u62e9\u56fe\u50cf\u7279\u5b9a\u7684\u6821\u51c6\u3002", "result": "\u5728NYUv2\u548cKITTI\u4e0a\u63d0\u9ad8\u4e86\u57df\u5185\u51c6\u786e\u6027\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u5230SUN-RGBD\u548cDDAD\u65f6\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u8bed\u8a00\u7684\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u76f8\u5bf9\u6df1\u5ea6\u9aa8\u5e72\u7f51\u7edc\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u7684\u51c6\u786e\u6062\u590d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01460", "abs": "https://arxiv.org/abs/2601.01460", "authors": ["Mohd Usama", "Belal Ahmad", "Christer Gronlund", "Faleh Menawer R Althiyabi"], "title": "Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network", "comment": "15 pages, 9 figures, 4 tables", "summary": "Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d85\u58f0\u56fe\u50cf\u7684\u8de8\u8bbe\u5907\u6216\u8de8\u8bbe\u7f6e\u5206\u6790\uff0c\u901a\u8fc7\u56fe\u50cf\u7ffb\u8bd1\u8c03\u6574\u7eb9\u7406\u548c\u53bb\u9664\u566a\u58f0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6548\u679c\u663e\u8457\u3002", "motivation": "\u533b\u7597\u5f71\u50cf\u4e2d\uff0c\u4e0d\u540c\u8bbe\u5907\u6216\u53c2\u6570\u8bbe\u7f6e\u7684\u56fe\u50cf\u5b58\u5728\u7eb9\u7406\u548c\u566a\u58f0\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u8de8\u57df\u6027\u80fd\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u9488\u5bf9\u6bcf\u4e2a\u8bbe\u5907\u6216\u8bbe\u7f6e\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u5c06\u57df\u9002\u5e94\u4efb\u52a1\u8f6c\u5316\u4e3a\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8c03\u6574\u6e90\u57df\u56fe\u50cf\u7684\u7eb9\u7406\u6a21\u5f0f\u5e76\u53bb\u9664\u6df7\u54cd\u566a\u58f0\uff0c\u4ee5\u5339\u914d\u76ee\u6807\u57df\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u5185\u5bb9\u4e0d\u53d8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u7eb9\u7406\u6a21\u5f0f\u7684\u7ffb\u8bd1\u548c\u6df7\u54cd\u566a\u58f0\u7684\u53bb\u9664\uff0c\u57df\u9002\u5e94\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u65e0\u9002\u5e94\u60c5\u51b5\uff08\u5982\u76f4\u65b9\u56fe\u76f8\u5173\u6027\u548cBhattacharya\u8ddd\u79bb\u6307\u6807\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eGAN\u7684\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u8c03\u6574\u7eb9\u7406\u6a21\u5f0f\u548c\u53bb\u9664\u6df7\u54cd\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bbe\u5907\u6216\u8de8\u8bbe\u7f6e\u7684\u56fe\u50cf\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2601.01481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01481", "abs": "https://arxiv.org/abs/2601.01481", "authors": ["Mohammad Hassan Saghafi", "Seyed Majid Noorhosseini", "Seyed Abolfazl Seyed Javadein", "Hadi Khalili"], "title": "Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm", "comment": null, "summary": "In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ViBe\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d77\u5cb8\u89c6\u9891\u5e8f\u5217\u4e2d\u8239\u8236\u7684\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u7531\u4e8e\u6d77\u5cb8\u573a\u666f\u5177\u6709\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u8fd9\u4e9b\u6761\u4ef6\u7684\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ViBe\u65b9\u6cd5\u7528\u4e8e\u79fb\u52a8\u7269\u4f53\u68c0\u6d4b\uff0c\u80fd\u591f\u68c0\u6d4b\u8239\u8236\u548c\u5c3e\u6d41\uff0c\u5e76\u51cf\u5c11\u4e86\u4e22\u5931\u8239\u8236\u7684\u6982\u7387\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u8239\u8236\u7684\u51e0\u4f55\u7279\u6027\u548c\u4eae\u5ea6\u5931\u771f\u7b49\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c3e\u6d41\u6d88\u9664\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u548c\u65b9\u6cd5\u5728\u8239\u8236\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8239\u8236\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4e14\u7cbe\u786e\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01483", "abs": "https://arxiv.org/abs/2601.01483", "authors": ["Xinyu Qiu", "Heng Jia", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Yi Yang", "Linchao Zhu"], "title": "Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization", "comment": null, "summary": "Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.", "AI": {"tldr": "ADPO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u9a8c\u8bc1\u5956\u52b1\u548c\u89e3\u8026\u4f18\u5316\u673a\u5236\uff0c\u8054\u5408\u4f18\u5316\u7b54\u6848\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u6846\u67b6\u4ee5\u8054\u5408\u4f18\u5316\u751f\u6210\u548c\u9a8c\u8bc1\u3002", "method": "ADPO\u5f15\u5165\u4e86\u504f\u597d\u9a8c\u8bc1\u5956\u52b1\u548c\u89e3\u8026\u4f18\u5316\u673a\u5236\uff0c\u901a\u8fc7\u8ba1\u7b97\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u72ec\u7acb\u4f18\u52bf\uff0c\u5e94\u7528\u4ee4\u724c\u63a9\u7801\u9694\u79bb\u68af\u5ea6\uff0c\u5e76\u7ed3\u5408\u63a9\u7801GRPO\u76ee\u6807\u3002", "result": "ADPO\u5728\u9a8c\u8bc1AUC\u4e0a\u63d0\u534734.1%\uff0c\u63a8\u7406\u65f6\u95f4\u964d\u4f4e53.5%\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "ADPO\u901a\u8fc7\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u7b54\u6848\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2601.01485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01485", "abs": "https://arxiv.org/abs/2601.01485", "authors": ["Zobia Batool", "Diala Lteif", "Vijaya B. Kolachalama", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease", "comment": null, "summary": "Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.", "AI": {"tldr": "EM\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\u63d0\u5347AD\u8bca\u65ad\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\uff0c\u5e73\u5747\u5b8fF1\u63d0\u9ad82.4\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u7531\u4e8e\u626b\u63cf\u4eea\u3001\u534f\u8bae\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u5dee\u5f02\uff0c\u57fa\u4e8e\u7ed3\u6784\u78c1\u5171\u632f\u6210\u50cf\uff08sMRI\uff09\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65b0\u961f\u5217\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u8bca\u65ad\u9700\u8981\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Extended MixStyle (EM)\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u6765\u6a21\u62df\u591a\u6837\u5316\u7684\u5206\u5e03\u53d8\u5316\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5728\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u672a\u89c1\u8fc7\u7684\u961f\u5217\uff08\u603b\u8ba1n=3,126\uff09\u4e0a\u6d4b\u8bd5\uff0cEM\u7684\u5e73\u5747\u5b8fF1\u5206\u6570\u6bd4\u73b0\u6709\u6700\u4f73\u5355\u57df\u6cdb\u5316\uff08SDG\uff09\u57fa\u51c6\u63d0\u9ad8\u4e862.4\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Extended MixStyle (EM) \u6846\u67b6\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u6765\u6a21\u62df\u591a\u6837\u5316\u7684\u5206\u5e03\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bca\u65ad\u6a21\u578b\u5728\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5f02\u6784\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.01487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01487", "abs": "https://arxiv.org/abs/2601.01487", "authors": ["Ziyue Zhang", "Luxi Lin", "Xiaolin Hu", "Chao Chang", "HuaiXi Wang", "Yiyi Zhou", "Rongrong Ji"], "title": "DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion", "comment": null, "summary": "Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.", "AI": {"tldr": "DeepInv\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6269\u6563\u53cd\u6f14\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u8fed\u4ee3\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u56fe\u50cf\u5230\u566a\u58f0\u6620\u5c04\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u53cd\u6f14\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u53ef\u884c\u76d1\u7763\u4fe1\u53f7\u800c\u4f9d\u8d56\u8fd1\u4f3c\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6269\u6563\u53cd\u6f14\u65b9\u6cd5DeepInv\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u76ee\u6807\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u548c\u591a\u5c3a\u5ea6\u8bad\u7ec3\u8bad\u7ec3\u53c2\u6570\u5316\u53cd\u6f14\u6c42\u89e3\u5668\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0cDeepInv\u7684SSIM\u6bd4EasyInv\u9ad840.435%\uff0c\u901f\u5ea6\u6bd4ReNoise\u5feb9887.5%\u3002", "conclusion": "DeepInv\u901a\u8fc7\u81ea\u76d1\u7763\u76ee\u6807\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u6269\u6563\u53cd\u6f14\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2601.01507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01507", "abs": "https://arxiv.org/abs/2601.01507", "authors": ["Tao Li", "Qing Li", "Na Li", "Hui Xie"], "title": "DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation", "comment": null, "summary": "Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.\n  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.\n  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.", "AI": {"tldr": "DiffKD-DCIS\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347DCIS\u5347\u7ea7\u9884\u6d4b\uff0c\u5408\u6210\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u5b66\u751f\u7f51\u7edc\u6027\u80fd\u4f18\uff0c\u4e34\u5e8a\u6f5c\u529b\u5927\u3002", "motivation": "\u51c6\u786e\u9884\u6d4bDCIS\u5347\u7ea7\u81f3IDC\u5bf9\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56e0\u6570\u636e\u6709\u9650\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u8d85\u58f0\u56fe\u50cf\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff1b2\uff09\u6df1\u5ea6\u6559\u5e08\u7f51\u7edc\u4ece\u539f\u59cb\u548c\u5408\u6210\u6570\u636e\u4e2d\u63d0\u53d6\u9c81\u68d2\u7279\u5f81\uff1b3\uff09\u7d27\u51d1\u5b66\u751f\u7f51\u7edc\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b66\u4e60\uff0c\u5e73\u8861\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2d\u5fc31,435\u4f8b\u6570\u636e\u96c6\u4e2d\uff0c\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u826f\u597d\uff0c\u5b66\u751f\u7f51\u7edc\u53c2\u6570\u66f4\u5c11\u3001\u63a8\u7406\u66f4\u5feb\u3002\u5728\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u90e8\u5206\u7ec4\u5408\uff0c\u51c6\u786e\u6027\u4e0e\u8d44\u6df1\u653e\u5c04\u79d1\u533b\u5e08\u76f8\u5f53\uff0c\u4f18\u4e8e\u521d\u7ea7\u533b\u5e08\u3002", "conclusion": "DiffKD-DCIS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u5e08\u751f\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86DCIS\u5347\u7ea7\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u73b0\u4e86\u91cd\u8981\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2601.01512", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01512", "abs": "https://arxiv.org/abs/2601.01512", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI", "comment": "9 pages, 5 figures", "summary": "This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.", "AI": {"tldr": "GBU-Net \u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u4e13\u4e3a\u5de6\u5fc3\u5ba4 MRI \u5206\u5272\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cdice \u5206\u6570\u8fbe 97%\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u5206\u5272\u77ed\u8f74 cine MRI \u626b\u63cf\u4e2d\u5de6\u5fc3\u5ba4\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf CNN \u5206\u5272\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e group-batch-normalized U-Net \u6846\u67b6\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5305\u542b\u4e0b\u91c7\u6837\u8def\u5f84\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u548c\u4e0a\u91c7\u6837\u8def\u5f84\u7528\u4e8e\u7ec6\u8282\u6062\u590d\uff0c\u5e76\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "GBU-Net \u5728 SunnyBrook \u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86 97% \u7684 dice \u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de6\u5fc3\u5ba4\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "conclusion": "GBU-Net \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u7684\u5de6\u5fc3\u5ba4\u5206\u5272\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5916\u79d1\u673a\u5668\u4eba\u548c\u533b\u5b66\u5206\u6790\uff0c\u5176\u521b\u65b0\u8bbe\u8ba1\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.01513", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01513", "abs": "https://arxiv.org/abs/2601.01513", "authors": ["Gen Li", "Peiyu Liu"], "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation", "comment": null, "summary": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.", "AI": {"tldr": "VideoSpeculateRAG\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u548c\u5b9e\u4f53\u8fc7\u6ee4\u7b56\u7565\uff0c\u9ad8\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u7b54\u6848\u8d28\u91cf\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVideoSpeculateRAG\u6846\u67b6\uff0c\u5305\u542b\u63a8\u6d4b\u89e3\u7801\u7ba1\u9053\uff08\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u7b54\u6848\uff0c\u518d\u7531\u91cd\u91cf\u7ea7\u6a21\u578b\u9a8c\u8bc1\u548c\u4f18\u5316\uff09\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5b9e\u4f53\u8fc7\u6ee4\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793aVideoSpeculateRAG\u5728\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\u3002", "conclusion": "VideoSpeculateRAG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u548c\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.01526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01526", "abs": "https://arxiv.org/abs/2601.01526", "authors": ["Hongbing Li", "Linhui Xiao", "Zihan Zhao", "Qi Shen", "Yixiang Huang", "Bo Xiao", "Zhanyu Ma"], "title": "BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding", "comment": null, "summary": "Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.", "AI": {"tldr": "BARE\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u504f\u7f6e\u611f\u77e5\u548c\u63a8\u7406\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u65b0\u6a21\u5757\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8868\u793a\u548c\u8bed\u4e49\u63a8\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u8868\u793a\u8fc7\u5ea6\u7ea0\u7f20\u548c\u8bed\u4e49\u63a8\u7406\u4e0d\u8db3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u53c2\u8003\u7ebf\u7d22\u7684\u7406\u89e3\u3002", "method": "BARE\u91c7\u7528\u4e86\u4e00\u79cd\u673a\u5236\uff0c\u901a\u8fc7\u4e09\u4e2a\u65b0\u6a21\u5757\uff08\u8bed\u8a00\u663e\u8457\u6027\u8c03\u5236\u5668\u3001\u89c6\u89c9\u504f\u5dee\u6821\u6b63\u548c\u53c2\u8003\u5173\u7cfb\u589e\u5f3a\uff09\u6765\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5e76\u6784\u5efa\u53c2\u8003\u8bed\u4e49\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBARE\u4e0d\u4ec5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BARE\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u663e\u8457\u6027\u8c03\u5236\u5668\u3001\u89c6\u89c9\u504f\u5dee\u6821\u6b63\u548c\u53c2\u8003\u5173\u7cfb\u589e\u5f3a\u4e09\u4e2a\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8868\u793a\u8fc7\u5ea6\u7ea0\u7f20\u548c\u8bed\u4e49\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.01535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01535", "abs": "https://arxiv.org/abs/2601.01535", "authors": ["Zixuan Fu", "Lanqing Guo", "Chong Wang", "Binbin Song", "Ding Liu", "Bihan Wen"], "title": "Improving Flexible Image Tokenizers for Autoregressive Image Generation", "comment": null, "summary": "Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \\textbf{ReToK}, a flexible tokenizer with \\underline{Re}dundant \\underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \\textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \\textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \\href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}", "AI": {"tldr": "ReToK\u901a\u8fc7\u5197\u4f59\u4ee4\u724c\u586b\u5145\u548c\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\u4f18\u5316\u7075\u6d3b\u56fe\u50cf\u6807\u8bb0\u5668\uff0c\u63d0\u5347\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7075\u6d3b\u56fe\u50cf\u6807\u8bb0\u5668\u56e0\u5c3e\u90e8\u622a\u65ad\u5bfc\u81f4\u4fe1\u606f\u8fc7\u5ea6\u96c6\u4e2d\u5728\u65e9\u671f\u4ee4\u724c\uff0c\u4ece\u800c\u9650\u5236\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6548\u679c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faReToK\uff0c\u7ed3\u5408\u5197\u4f59\u4ee4\u724c\u586b\u5145\u548c\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\uff0c\u524d\u8005\u6fc0\u6d3b\u5c3e\u90e8\u4ee4\u724c\uff0c\u540e\u8005\u901a\u8fc7\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u65e9\u671f\u4ee4\u724c\u7684\u89e3\u7801\u7279\u5f81\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0cReToK\u7684\u751f\u6210\u6027\u80fd\u4f18\u4e8e\u7075\u6d3b\u548c\u56fa\u5b9a\u957f\u5ea6\u6807\u8bb0\u5668\u3002", "conclusion": "ReToK\u901a\u8fc7\u5197\u4f59\u4ee4\u724c\u586b\u5145\u548c\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7075\u6d3b\u56fe\u50cf\u6807\u8bb0\u5668\u7684\u6027\u80fd\uff0c\u5728ImageNet 256\u00d7256\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u56fa\u5b9a\u957f\u5ea6\u6807\u8bb0\u5668\u7684\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2601.01537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01537", "abs": "https://arxiv.org/abs/2601.01537", "authors": ["Gong Gao", "Zekai Wang", "Xianhui Liu", "Weidong Zhao"], "title": "FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition", "comment": "28 pages, 8figures", "summary": "To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.", "AI": {"tldr": "FAR-AMTN\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u6743\u91cd\u7b56\u7565\u4f18\u5316\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63d0\u5347\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u7684\u6cdb\u5316\u6027\u80fd\u5e76\u51cf\u5c11\u53c2\u6570\u3002", "motivation": "\u4f20\u7edf\u591a\u4efb\u52a1\u7f51\u7edc\u5728\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u4e2d\u5b58\u5728\u53c2\u6570\u7206\u70b8\u548c\u9ad8\u5c42\u6b21\u7279\u5f81\u4ea4\u4e92\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86FAR-AMTN\uff0c\u5305\u542bWeight-Shared Group-Specific Attention (WSGSA)\u6a21\u5757\u3001Cross-Group Feature Fusion (CGFF)\u6a21\u5757\u548cDynamic Weighting Strategy (DWS)\u3002", "result": "\u5728CelebA\u548cLFWA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFAR-AMTN\u5728\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "FAR-AMTN\u901a\u8fc7\u7ed3\u5408WSGSA\u6a21\u5757\u3001CGFF\u6a21\u5757\u548cDWS\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u7f51\u7edc\u5728\u9762\u90e8\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u3002"}}
{"id": "2601.01547", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01547", "abs": "https://arxiv.org/abs/2601.01547", "authors": ["Tianjun Gu", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding", "comment": null, "summary": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTSI\u8303\u5f0f\uff0c\u7ed3\u5408\u7269\u7406\u548c\u610f\u56fe\u63a8\u7406\uff0c\u5e76\u63a8\u51faEscherVerse\uff08\u57fa\u51c6\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\uff09\uff0c\u63a8\u52a8\u7a7a\u95f4\u667a\u80fd\u4ece\u88ab\u52a8\u63cf\u8ff0\u8f6c\u5411\u76ee\u7684\u9a71\u52a8\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5ffd\u89c6\u4e86\u7a7a\u95f4\u53d8\u5316\u80cc\u540e\u7684\u4eba\u7c7b\u610f\u56fe\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7TSI\u8303\u5f0f\u7edf\u4e00\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\uff0c\u5e76\u6784\u5efa\u4e86EscherVerse\uff0c\u5305\u542b\u5927\u89c4\u6a21\u5f00\u653e\u4e16\u754c\u57fa\u51c6\uff08Escher-Bench\uff09\u3001\u6570\u636e\u96c6\uff08Escher-35k\uff09\u548c\u6a21\u578b\uff08Escher\u7cfb\u5217\uff09\uff0c\u4ee5\u8bc4\u4f30\u4ee3\u7406\u5728\u52a8\u6001\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u4e2d\u5bf9\u7269\u4f53\u6301\u4e45\u6027\u3001\u72b6\u6001\u8f6c\u6362\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "EscherVerse\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u610f\u56fe\u9a71\u52a8\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u6311\u6218\u6a21\u578b\u5c06\u7269\u7406\u4e8b\u4ef6\u4e0e\u6f5c\u5728\u7684\u4eba\u7c7b\u76ee\u7684\u8054\u7cfb\u8d77\u6765\uff0c\u63a8\u52a8\u4e86\u7a7a\u95f4\u667a\u80fd\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Teleo-Spatial Intelligence (TSI)\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\uff0c\u5e76\u901a\u8fc7EscherVerse\uff08\u5305\u62ecEscher-Bench\u3001Escher-35k\u6570\u636e\u96c6\u548cEscher\u7cfb\u5217\u6a21\u578b\uff09\u63a8\u52a8TSI\u7814\u7a76\uff0c\u4e3a\u7a7a\u95f4\u667a\u80fd\u4ece\u88ab\u52a8\u573a\u666f\u63cf\u8ff0\u8f6c\u5411\u76ee\u7684\u9a71\u52a8\u7684\u6574\u4f53\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01593", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.01593", "abs": "https://arxiv.org/abs/2601.01593", "authors": ["Haonan Cai", "Yuxuan Luo", "Zhouhui Lian"], "title": "Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation", "comment": "25 pages", "summary": "Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.", "AI": {"tldr": "GAR-Font \u662f\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u611f\u77e5\u6807\u8bb0\u5668\u548c\u591a\u6a21\u6001\u98ce\u683c\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u7684\u8d28\u91cf\u548c\u98ce\u683c\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u4e2d\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u7684\u6311\u6218\uff0c\u7a81\u7834\u4f20\u7edf\u56fe\u50cf\u5230\u56fe\u50cf\u8303\u5f0f\u7684\u9650\u5236\uff0c\u5f15\u5165\u8bed\u8a00\u98ce\u683c\u9002\u914d\u5668\u3002", "method": "\u63d0\u51fa\u4e86 GAR-Font\uff0c\u4e00\u4e2a\u65b0\u578b\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5305\u542b\u5168\u5c40\u611f\u77e5\u6807\u8bb0\u5668\u3001\u591a\u6a21\u6001\u98ce\u683c\u7f16\u7801\u5668\u548c\u540e\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAR-Font \u5728\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5168\u5c40\u98ce\u683c\u5fe0\u5b9e\u5ea6\u548c\u6587\u672c\u98ce\u683c\u5f15\u5bfc\u4e0b\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5b57\u4f53\u3002", "conclusion": "GAR-Font \u901a\u8fc7\u5f15\u5165\u5168\u5c40\u611f\u77e5\u7684\u6807\u8bb0\u5668\u3001\u591a\u6a21\u6001\u98ce\u683c\u7f16\u7801\u5668\u548c\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4fdd\u6301\u5168\u5c40\u98ce\u683c\u5fe0\u5b9e\u5ea6\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.01608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01608", "abs": "https://arxiv.org/abs/2601.01608", "authors": ["Felix Krause", "Stefan Andreas Baumann", "Johannes Schusterbauer", "Olga Grebenkova", "Ming Gui", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "title": "Guiding Token-Sparse Diffusion Models", "comment": null, "summary": "Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.", "AI": {"tldr": "SG\u901a\u8fc7\u6807\u8bb0\u7ea7\u7a00\u758f\u6027\u4f18\u5316\u63a8\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u7531\u4e8e\u5bf9Classifier-free Guidance (CFG)\u7684\u54cd\u5e94\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002SG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Sparse Guidance (SG)\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u8bb0\u7ea7\u7a00\u758f\u6027\u800c\u975e\u6761\u4ef6\u6027\u4e22\u5f03\u6765\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u5728\u63a8\u7406\u9636\u6bb5\u66f4\u597d\u5730\u4fdd\u7559\u6761\u4ef6\u9884\u6d4b\u7684\u9ad8\u65b9\u5dee\u3002", "result": "SG\u5728ImageNet-256\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e861.58 FID\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1125%\uff0c\u5e76\u5728\u5339\u914d\u57fa\u7ebf\u8d28\u91cf\u65f6\u8282\u7701\u9ad8\u8fbe58%\u7684FLOPs\u3002\u6b64\u5916\uff0cSG\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u5347\u4e86\u6784\u56fe\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u541e\u5410\u91cf\u3002", "conclusion": "Sparse Guidance (SG) \u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u5229\u7528\u6807\u8bb0\u7ea7\u7a00\u758f\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u65b9\u5dee\u8f93\u51fa\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2601.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01613", "abs": "https://arxiv.org/abs/2601.01613", "authors": ["Kazi Ramisa Rifa", "Jie Zhang", "Abdullah Imran"], "title": "CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment", "comment": "18 pages, 9 figures, 5 tables", "summary": "Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.", "AI": {"tldr": "CAP-IQA\u6846\u67b6\u901a\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u878d\u5408\u53ca\u56e0\u679c\u53bb\u504f\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5f15\u5165\u7406\u60f3\u5316\u5b9a\u4e49\u5bfc\u81f4\u7684\u504f\u5dee\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA)\u6846\u67b6\uff0c\u7ed3\u5408CNN\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9886\u57df\u7279\u5b9a\u6587\u672c\u7f16\u7801\u5668\uff0c\u5229\u7528\u653e\u5c04\u5b66\u98ce\u683c\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408\u6280\u672f\u3002", "result": "\u57282023 LDCTIQA\u6311\u6218\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAP-IQA\u7684\u603b\u4f53\u76f8\u5173\u6027\u5f97\u5206\u6bd4\u699c\u9996\u56e2\u961f\u9ad84.24%\uff0c\u5e76\u572891,514\u5f20\u513f\u79d1CT\u56fe\u50cf\u7684\u5185\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAP-IQA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u7ea7\u5148\u9a8c\u548c\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u5e76\u5e94\u7528\u56e0\u679c\u53bb\u504f\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01639", "abs": "https://arxiv.org/abs/2601.01639", "authors": ["Gaurav Sekar"], "title": "An Empirical Study of Monocular Human Body Measurement Under Weak Calibration", "comment": "The paper consists of 8 pages, 2 figures (on pages 4 and 7), and 2 tables (both on page 6)", "summary": "Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e09\u79cd\u5f31\u6821\u51c6\u5355\u76ee\u7b56\u7565\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u6821\u51c6\u5047\u8bbe\u5bf9\u6d4b\u91cf\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u6d88\u8d39\u8bbe\u5907\u4e0a\u7684\u8f7b\u91cf\u7ea7\u4eba\u4f53\u6d4b\u91cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u53c2\u8003\u3002", "motivation": "\u7531\u4e8e\u5c3a\u5ea6\u6a21\u7cca\u6027\u3001\u89c6\u89d2\u654f\u611f\u6027\u548c\u7f3a\u4e4f\u660e\u786e\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u4ece\u5355\u76eeRGB\u56fe\u50cf\u4f30\u8ba1\u4eba\u4f53\u6d4b\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e09\u79cd\u5f31\u6821\u51c6\u5355\u76ee\u7b56\u7565\uff1a\u57fa\u4e8e\u5730\u6807\u7684\u51e0\u4f55\u65b9\u6cd5\u3001\u59ff\u6001\u9a71\u52a8\u7684\u56de\u5f52\u65b9\u6cd5\u548c\u5bf9\u8c61\u6821\u51c6\u7684\u8f6e\u5ed3\u65b9\u6cd5\uff0c\u4f7f\u7528\u6d88\u8d39\u7ea7\u76f8\u673a\u5728\u534a\u7ea6\u675f\u6761\u4ef6\u4e0b\u8fdb\u884c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u6821\u51c6\u5047\u8bbe\u5bf9\u6d4b\u91cf\u884c\u4e3a\u3001\u9c81\u68d2\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u5728\u4e0d\u540c\u4f53\u578b\u4e2d\u7684\u5f71\u54cd\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5355\u76ee\u4eba\u4f53\u6d4b\u91cf\u7cfb\u7edf\u5728\u6d88\u8d39\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5b9e\u8bc1\u8bbe\u8ba1\u53c2\u8003\uff0c\u63ed\u793a\u4e86\u7528\u6237\u5728\u6821\u51c6\u8fc7\u7a0b\u4e2d\u7684\u52aa\u529b\u4e0e\u6240\u5f97\u5468\u957f\u91cf\u7684\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2601.01687", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01687", "abs": "https://arxiv.org/abs/2601.01687", "authors": ["Abdur R. Fayjie", "Pankhi Kashyap", "Jutika Borah", "Patrick Vandewalle"], "title": "FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation", "comment": "20 pages, 6 figures, 7 tables", "summary": "Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.", "AI": {"tldr": "FALCON\u662f\u4e00\u79cd\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc72D\u5207\u7247\u5904\u7406\u5b9e\u73b0\u9ad8\u7cbe\u5ea63D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u89e3\u51b33D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u60a3\u8005\u7279\u5f02\u6027\u53d8\u5f02\u3001\u6570\u636e\u9690\u79c1\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFALCON\u6846\u67b6\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\uff08\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\uff09\u3001\u5bf9\u6297\u5fae\u8c03\u3001\u8fb9\u754c\u611f\u77e5\u5b66\u4e60\u548c\u4efb\u52a1\u611f\u77e5\u63a8\u7406\uff0c\u52a8\u6001\u9002\u5e94\u60a3\u8005\u7279\u5f02\u6027\u89e3\u5256\u53d8\u5f02\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFALCON\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684Hausdorff\u8ddd\u79bb\u5206\u6570\uff08\u8fb9\u754c\u7cbe\u5ea6\u6700\u4f18\uff09\uff0c\u4e14Dice\u76f8\u4f3c\u7cfb\u6570\u4e0e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "FALCON\u6846\u67b6\u901a\u8fc7\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u6570\u636e\u9700\u6c42\u3001\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u76f8\u5f53\u7684Dice\u76f8\u4f3c\u7cfb\u6570\u3002"}}
{"id": "2601.01660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01660", "abs": "https://arxiv.org/abs/2601.01660", "authors": ["Aymen Mir", "Riza Alp Guler", "Jian Wang", "Gerard Pons-Moll", "Bing Zhou"], "title": "Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows", "comment": "Our project page is available at https://miraymen.github.io/dgsm", "summary": "We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.", "AI": {"tldr": "DGSM\u548cSH\u91cd\u5149\u7167\u6280\u672f\u5b9e\u73b0\u4e863DGS\u865a\u62df\u5f62\u8c61\u4e0e\u573a\u666f\u4ea4\u4e92\u65f6\u7684\u8fde\u8d2f\u9634\u5f71\u548c\u5149\u7167\u6548\u679c\uff0c\u65e0\u9700\u7f51\u683c\u5316\u3002", "motivation": "\u89e3\u51b3\u52a8\u753b3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u865a\u62df\u5f62\u8c61\u4e0e3DGS\u573a\u666f\u6216\u52a8\u6001\u7269\u4f53\u4ea4\u4e92\u65f6\u7684\u5149\u7167\u548c\u9634\u5f71\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Deep Gaussian Shadow Maps (DGSM)\uff0c\u4e00\u79cd\u9488\u5bf93DGS\u8868\u793a\u7684\u4f53\u79ef\u9634\u5f71\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u7403\u5f62\u8c10\u6ce2(SH)\u57fa\u7684HDRI\u63a2\u9488\u8fdb\u884c\u5feb\u901f\u5149\u7167\u4f20\u8f93\u3002", "result": "\u5728AvatarX\u548cActorsHQ\u865a\u62df\u5f62\u8c61\u4e2d\u5c55\u793a\u4e86\u73af\u5883\u4e00\u81f4\u7684\u5149\u7167\u6548\u679c\uff0c\u5e76\u5728ScanNet++\u3001DL3DV\u548cSuperSplat\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u63d2\u5165\u7269\u4f53\u7684\u4ea4\u4e92\u3002", "conclusion": "DGSM\u548cSH\u91cd\u5149\u7167\u6280\u672f\u57283DGS\u8868\u793a\u4e2d\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7684\u9634\u5f71\u548c\u91cd\u5149\u7167\u6548\u679c\uff0c\u907f\u514d\u4e86\u7f51\u683c\u5316\u7684\u9700\u6c42\u3002"}}
{"id": "2601.01676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01676", "abs": "https://arxiv.org/abs/2601.01676", "authors": ["Jin Yao", "Radowan Mahmud Redoy", "Sebastian Elbaum", "Matthew B. Dwyer", "Zezhou Cheng"], "title": "LabelAny3D: Label Any Object 3D in the Wild", "comment": "NeurIPS 2025. Project page: https://uva-computer-vision-lab.github.io/LabelAny3D/", "summary": "Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \\emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.", "AI": {"tldr": "LabelAny3D\u901a\u8fc7\u5206\u6790-\u5408\u6210\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf3D\u6807\u6ce8\uff0c\u6784\u5efaCOCO3D\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u5355\u76ee3D\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u6a21\u578b\u5728\u91ce\u5916\u56fe\u50cf\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u91ce\u59163D\u6570\u636e\u96c6\u548c3D\u6807\u6ce8\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u6790-\u5408\u6210\u6846\u67b6LabelAny3D\uff0c\u4ece2D\u56fe\u50cf\u91cd\u5efa\u6574\u4f533D\u573a\u666f\u4ee5\u9ad8\u6548\u751f\u62103D\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86COCO3D\u65b0\u57fa\u51c6\u3002", "result": "LabelAny3D\u751f\u6210\u7684\u6807\u6ce8\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u6027\u80fd\uff0c\u8d28\u91cf\u4f18\u4e8e\u5148\u524d\u7684\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u3002", "conclusion": "LabelAny3D\u6846\u67b6\u901a\u8fc7\u5206\u6790-\u5408\u6210\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf3D\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u6807\u6ce8\u5728\u5f00\u653e\u4e16\u754c3D\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01677", "abs": "https://arxiv.org/abs/2601.01677", "authors": ["Zhengsen Xu", "Lanying Wang", "Sibo Cheng", "Xue Rui", "Kyle Gao", "Yimin Zhu", "Mabel Heffring", "Zack Dewis", "Saeid Taleghanidoozdoozan", "Megan Greenwood", "Motasem Alkayid", "Quinn Ledingham", "Hongjie He", "Jonathan Li", "Lincoln Linlin Xu"], "title": "Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada", "comment": null, "summary": "In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4fe1\u8d56\u7684\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u9a71\u52a8\u56e0\u7d20\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u52a0\u62ff\u5927\u897f\u90e8\u8868\u73b0\u4f18\u5f02\uff0cF1\u5f97\u52060.90\uff0cPR-AUC 0.98\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u52a0\u62ff\u5927\u897f\u90e8\u91ce\u706b\u6d3b\u52a8\u7684\u52a0\u5267\u5bfc\u81f4\u4e86\u91cd\u5927\u7684\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u5883\u635f\u5931\uff0c\u4f46\u91ce\u706b\u98ce\u9669\u7684\u51c6\u786e\u9884\u6d4b\u53d7\u5230\u70b9\u706b\u548c\u8513\u5ef6\u7684\u5185\u5728\u968f\u673a\u6027\u4ee5\u53ca\u591a\u79cd\u56e0\u7d20\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u957f\u5e8f\u5217\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u5efa\u6a21\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u6574\u5408\u4e86\u71c3\u6599\u6761\u4ef6\u3001\u6c14\u8c61\u3001\u6c14\u5019\u53d8\u7387\u3001\u5730\u5f62\u548c\u4eba\u7c7b\u6d3b\u52a8\u7b49\u591a\u79cd\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u57282023\u548c2024\u5e74\u521b\u7eaa\u5f55\u7684\u706b\u707e\u5b63\u8282\u4e2d\uff0c\u8be5\u6a21\u578b\u5728\u52a0\u62ff\u5927\u897f\u90e8\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\uff0cF1\u5f97\u5206\u4e3a0.90\uff0cPR-AUC\u4e3a0.98\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u4fe1\u8d56\u7684\u6570\u636e\u9a71\u52a8\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u5e8f\u5217\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u5efa\u6a21\u6574\u5408\u4e86\u5f02\u8d28\u6027\u9a71\u52a8\u56e0\u7d20\uff0c\u540c\u65f6\u660e\u786e\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u652f\u6301\u8fc7\u7a0b\u7ea7\u89e3\u91ca\u3002"}}
{"id": "2601.01680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01680", "abs": "https://arxiv.org/abs/2601.01680", "authors": ["Afzal Hossain", "Mst Rumana Sumi", "Stephanie Schuckers"], "title": "Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages", "comment": "Accepted and presented at IEEE IJCB 2025 conference; final published version forthcoming", "summary": "Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u5728\u5a74\u5e7c\u513f\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5e74\u9f84\u8d8a\u5c0f\u8bc6\u522b\u51c6\u786e\u7387\u8d8a\u4f4e\uff0c\u4f46DANN\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u5bf9\u667a\u80fd\u57ce\u5e02\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u5a74\u5e7c\u513f\u4eba\u8138\u8bc6\u522b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u5feb\u901f\u7684\u9762\u90e8\u5f62\u6001\u53d8\u5316\u3001\u9ad8\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u6570\u636e\u96c6\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u53d1\u80b2\u9636\u6bb5\u7684\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\uff08FaceNet\u3001ArcFace\u3001MagFace\u548cCosFace\uff09\u5728\u65b0\u5f00\u53d1\u7684\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5e94\u7528\u4e86\u9886\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u65b9\u6cd5\u6765\u51cf\u5c11\u5d4c\u5165\u6f02\u79fb\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c0\u81f36\u4e2a\u6708\u5a74\u513f\u7684\u8bc6\u522b\u51c6\u786e\u7387\u4ec5\u4e3a30.7%\uff080.1% FAR\uff09\uff0c\u800c2.5\u81f33\u5c81\u513f\u7ae5\u63d0\u5347\u81f364.7%\u3002DANN\u65b9\u6cd5\u5c06TAR\u63d0\u9ad8\u4e8612%\u4ee5\u4e0a\uff0c\u663e\u8457\u6539\u5584\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u667a\u80fd\u57ce\u5e02\u5e94\u7528\u4e2d\u6784\u5efa\u968f\u65f6\u95f4\u53ef\u9760\u7684\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u516c\u5171\u533b\u7597\u3001\u513f\u7ae5\u5b89\u5168\u548c\u6570\u5b57\u8eab\u4efd\u670d\u52a1\u7b49\u9886\u57df\u3002\u65e9\u671f\u5e74\u9f84\u7ec4\u7684\u6311\u6218\u51f8\u663e\u4e86\u672a\u6765\u7814\u7a76\u9690\u79c1\u4fdd\u62a4\u751f\u7269\u8ba4\u8bc1\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u89e3\u51b3\u65f6\u95f4\u53d8\u5f02\u6027\u95ee\u9898\u3002"}}
{"id": "2601.01781", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01781", "abs": "https://arxiv.org/abs/2601.01781", "authors": ["Lakshay Sharma", "Alex Marin"], "title": "Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery", "comment": "Accepted at CV4EO Workshop at WACV 2026", "summary": "Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \\href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.", "AI": {"tldr": "\u63d0\u51fa\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\u2014\u2014\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u5b50\u56fe\u50cf\u5728\u539f\u56fe\u50cf\u4e2d\u7684\u4f4d\u7f6e\u8bed\u4e49\u63a9\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u540c\u7b49\u6216\u66f4\u597d\u7684\u4e0b\u6e38\u5206\u5272\u6027\u80fd\uff08\u901a\u8fc7mIoU\u8861\u91cf\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\u65b9\u6cd5\u5728\u51cf\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.01798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01798", "abs": "https://arxiv.org/abs/2601.01798", "authors": ["Syed Abdul Hannan", "Hazim Bukhari", "Thomas Cantalapiedra", "Eman Ansar", "Massa Baali", "Rita Singh", "Bhiksha Raj"], "title": "VerLM: Explaining Face Verification Using Natural Language", "comment": null, "summary": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u7528\u4e8e\u4eba\u8138\u9a8c\u8bc1\uff0c\u4e0d\u4ec5\u80fd\u51c6\u786e\u5224\u65ad\u4e24\u5f20\u4eba\u8138\u56fe\u50cf\u662f\u5426\u5c5e\u4e8e\u540c\u4e00\u4eba\uff0c\u8fd8\u80fd\u901a\u8fc7\u7b80\u6d01\u548c\u8be6\u7ec6\u7684\u89e3\u91ca\u8bf4\u660e\u51b3\u7b56\u4f9d\u636e\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u8138\u9a8c\u8bc1\u7cfb\u7edf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u51c6\u786e\u9a8c\u8bc1\u53c8\u80fd\u89e3\u91ca\u51b3\u7b56\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u5e76\u9002\u914d\u4e00\u79cd\u6700\u521d\u4e3a\u97f3\u9891\u533a\u5206\u8bbe\u8ba1\u7684\u5148\u8fdb\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u89c6\u89c9\u8f93\u5165\uff0c\u7ed3\u5408\u4e86\u590d\u6742\u7684\u7279\u5f81\u63d0\u53d6\u6280\u672f\u548c\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684VLM\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4eba\u8138\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u4e24\u79cd\u89e3\u91ca\u98ce\u683c\u589e\u5f3a\u4e86\u51b3\u7b56\u7684\u900f\u660e\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u4eba\u8138\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.01689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01689", "abs": "https://arxiv.org/abs/2601.01689", "authors": ["Afzal Hossain", "Stephanie Schuckers"], "title": "Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data", "comment": null, "summary": "Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u5b9e\u5408\u6210\u6570\u636e\u589e\u5f3a\u80fd\u6709\u6548\u63d0\u5347\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u7684\u7eb5\u5411\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u8de8\u5ea6\u5185\u7684\u9519\u8bef\u7387\u3002", "motivation": "\u513f\u7ae5\u9762\u90e8\u5feb\u901f\u4e14\u975e\u7ebf\u6027\u7684\u751f\u957f\u5bfc\u81f4\u6a21\u677f\u6f02\u79fb\u548c\u968f\u65f6\u95f4\u589e\u52a0\u7684\u9a8c\u8bc1\u9519\u8bef\uff0c\u7814\u7a76\u5408\u6210\u4eba\u8138\u6570\u636e\u662f\u5426\u80fd\u4f5c\u4e3a\u7eb5\u5411\u7a33\u5b9a\u5668\u63d0\u5347\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u65f6\u95f4\u9c81\u68d2\u6027\u3002", "method": "\u5728YFA\u6570\u636e\u96c6\u4e0a\u91c7\u7528\u8eab\u4efd\u4e0d\u76f8\u4ea4\u534f\u8bae\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u8bbe\u7f6e\uff1a(i)\u672a\u7ecf\u6570\u636e\u96c6\u7279\u5b9a\u5fae\u8c03\u7684\u9884\u8bad\u7ec3MagFace\u5d4c\u5165\uff0c(ii)\u4ec5\u4f7f\u7528\u771f\u5b9e\u8bad\u7ec3\u4eba\u8138\u5fae\u8c03\u7684MagFace\uff0c(iii)\u7ed3\u5408\u771f\u5b9e\u548c\u5408\u6210\u751f\u6210\u8bad\u7ec3\u4eba\u8138\u5fae\u8c03\u7684MagFace\u3002\u5408\u6210\u6570\u636e\u4f7f\u7528StyleGAN2 ADA\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u540e\u751f\u6210\u8fc7\u6ee4\u6b65\u9aa4\u51cf\u5c11\u8eab\u4efd\u6cc4\u6f0f\u548c\u53bb\u9664\u4f2a\u5f71\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57286\u81f336\u4e2a\u6708\u7684\u6ce8\u518c\u9a8c\u8bc1\u95f4\u9694\u4e2d\uff0c\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u5fae\u8c03\u76f8\u8f83\u4e8e\u9884\u8bad\u7ec3\u57fa\u7ebf\u548c\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u5fae\u8c03\u663e\u8457\u964d\u4f4e\u4e86\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u7684\u9519\u8bef\u7387\uff0c\u4e3a\u63d0\u5347\u513f\u79d1\u4eba\u8138\u8bc6\u522b\u7684\u8eab\u4efd\u6301\u4e45\u6027\u63d0\u4f9b\u4e86\u98ce\u9669\u611f\u77e5\u7684\u8bc4\u4f30\u3002"}}
{"id": "2601.01807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01807", "abs": "https://arxiv.org/abs/2601.01807", "authors": ["Ubaidullah", "Muhammad Abid Hussain", "Mohsin Raza Jafri", "Rozi Khan", "Moid Sandhu", "Abd Ullah Khan", "Hyundong Shin"], "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification", "comment": null, "summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.", "AI": {"tldr": "LUMPNet \u662f\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b LSD\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LSD \u662f\u4e00\u79cd\u4f20\u67d3\u6027\u75c5\u6bd2\uff0c\u5bf9\u5168\u7403\u7ecf\u6d4e\u548c\u7cae\u98df\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u65e9\u671f\u7cbe\u786e\u8bc6\u522b\u5bf9\u9884\u9632\u7206\u53d1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408 YOLOv11\u3001\u57fa\u4e8e EfficientNet \u7684 CNN \u5206\u7c7b\u5668\u548c\u65b0\u578b\u81ea\u9002\u5e94\u6df7\u5408\u4f18\u5316\u5668\uff0c\u6784\u5efa\u4e86 LUMPNet \u6a21\u578b\u3002", "result": "LUMPNet \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230 99% \u548c 98%\uff0c\u4e14\u4f18\u4e8e AdamW \u4f18\u5316\u7684 EfficientNet-B0 \u6a21\u578b\u3002", "conclusion": "LUMPNet \u5728 LSD \u65e9\u671f\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe 99%\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe 98%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002"}}
{"id": "2601.01695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01695", "abs": "https://arxiv.org/abs/2601.01695", "authors": ["Ruiyu Mao", "Baoming Zhang", "Nicholas Ruozzi", "Yunhui Guo"], "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection", "comment": "10 pages, 7 figures. Submitted to CVPR 2026", "summary": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.", "AI": {"tldr": "LH3D\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u53ef\u5b66\u4e60\u6027\u9ad8\u7684\u8def\u8fb9\u573a\u666f\uff0c\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u53ef\u5b66\u4e60\u6027\u5bf93D\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8def\u8fb9\u611f\u77e5\u6570\u636e\u96c6\u901a\u5e38\u4f9d\u8d56\u8f66\u8f86\u4e0e\u8def\u8fb9\u7684\u534f\u540c\u6807\u6ce8\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u9700\u4ec5\u6807\u6ce8\u8def\u8fb9\u6570\u636e\uff0c\u5bfc\u81f4\u6807\u6ce8\u56f0\u96be\u4e14\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u56fa\u6709\u6a21\u7cca\u6837\u672c\u7684\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6LH3D\uff0c\u7528\u4e8e\u8def\u8fb9\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u9009\u62e9\u65e2\u4fe1\u606f\u4e30\u5bcc\u53c8\u53ef\u53ef\u9760\u6807\u6ce8\u7684\u573a\u666f\uff0c\u6291\u5236\u56fa\u6709\u6a21\u7cca\u6837\u672c\u3002", "result": "\u5728DAIR-V2X-I\u6570\u636e\u96c6\u4e0a\uff0cLH3D\u4ec5\u752825%\u7684\u6807\u6ce8\u9884\u7b97\u5c31\u8fbe\u5230\u4e86\u8f66\u8f86\u3001\u884c\u4eba\u548c\u9a91\u8f66\u800586.06%\u300167.32%\u548c78.67%\u7684\u5168\u6027\u80fd\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LH3D\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u53ef\u53ef\u9760\u6807\u6ce8\u7684\u573a\u666f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u53ef\u5b66\u4e60\u6027\u800c\u975e\u4e0d\u786e\u5b9a\u6027\u5bf9\u8def\u8fb93D\u611f\u77e5\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2601.01835", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01835", "abs": "https://arxiv.org/abs/2601.01835", "authors": ["Rashid Iqbal", "Saddam Hussain Khan"], "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images", "comment": "15 Pages, 7 Figures, 4 Tables", "summary": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.", "AI": {"tldr": "RSwinV2\u662f\u4e00\u79cd\u7ed3\u5408SwinTransformer\u548cIRB\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86Mpox\u75c5\u53d8\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347Mpox\u75c5\u53d8\u5206\u7c7b\u80fd\u529b\uff0c\u514b\u670d\u4f20\u7edfCNN\u548cSwinTransformer\u5728\u5c40\u90e8\u533a\u57df\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "RSwinV2\u65b9\u6cd5\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u5206\u5c42Transformer\u7ed3\u6784\u3001\u975e\u91cd\u53e0\u56fe\u50cf\u5757\u5904\u7406\u3001\u591a\u6ce8\u610f\u529b\u5934\u5d4c\u5165\u4ee5\u53ca\u5f15\u5165IRB\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u5e76\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5728Kaggle\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cRSwinV2\u5b9e\u73b0\u4e8696.21%\u7684\u51c6\u786e\u7387\u548c95.62%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u6807\u51c6CNN\u548cSwinTransformer\u6a21\u578b\u3002", "conclusion": "RSwinV2\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408SwinTransformer\u7684\u5168\u5c40\u94fe\u63a5\u80fd\u529b\u548cIRB\u7684\u5c40\u90e8\u6a21\u5f0f\u94fe\u63a5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Mpox\u75c5\u53d8\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u5177\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.01720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01720", "abs": "https://arxiv.org/abs/2601.01720", "authors": ["Xijie Huang", "Chengming Xu", "Donghao Luo", "Xiaobin Hu", "Peng Tang", "Xu Peng", "Jiangning Zhang", "Chengjie Wang", "Yanwei Fu"], "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing", "comment": null, "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.", "AI": {"tldr": "FFP-300K\u6570\u636e\u96c6\u548cAST-RoPE\u6846\u67b6\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u7684\u8fd0\u884c\u65f6\u4f9d\u8d56\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff08\u77ed\u3001\u4f4e\u5206\u8fa8\u7387\u3001\u7f3a\u4e4f\u591a\u6837\u6027\uff09\u800c\u4f9d\u8d56\u8fd0\u884c\u65f6\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFFP-300K\u6570\u636e\u96c6\u7684\u65b0\u6846\u67b6\uff0c\u91c7\u7528AST-RoPE\u52a8\u6001\u91cd\u6620\u5c04\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u7ed3\u5408\u81ea\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728EditVerseBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b66\u672f\u548c\u5546\u4e1a\u6a21\u578b\uff08PickScore\u63d0\u5347\u7ea60.2\uff0cVLM\u8bc4\u5206\u63d0\u5347\u7ea60.3\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165FFP-300K\u6570\u636e\u96c6\u548cAST-RoPE\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5bf9\u8fd0\u884c\u65f6\u6307\u5bfc\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2601.01874", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01874", "abs": "https://arxiv.org/abs/2601.01874", "authors": ["Shuhang Chen", "Yunqiu Xu", "Junjie Xie", "Aojun Lu", "Tao Feng", "Zeying Huang", "Ning Zhang", "Yi Sun", "Yi Yang", "Hangjie Yuan"], "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "comment": null, "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "AI": {"tldr": "CogFlow\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u4e2d\u89c6\u89c9\u7ebf\u7d22\u6574\u5408\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u7ebf\u7d22\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u7684\u5fe0\u5b9e\u6574\u5408\u548c\u5229\u7528\u95ee\u9898\uff0cCogFlow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CogFlow\u6846\u67b6\uff0c\u5305\u542b\u77e5\u8bc6\u5185\u5316\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u534f\u540c\u89c6\u89c9\u5956\u52b1\u3001\u77e5\u8bc6\u5185\u5316\u5956\u52b1\u6a21\u578b\u548c\u89c6\u89c9\u95e8\u63a7\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u5e38\u7528\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogFlow\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u8d21\u732e\u4e86\u5305\u542b12\u4e07\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6MathCog\u3002", "conclusion": "CogFlow\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u611f\u77e5\u2192\u5185\u5316\u2192\u63a8\u7406\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.01746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01746", "abs": "https://arxiv.org/abs/2601.01746", "authors": ["Lintong Wei", "Jian Lu", "Haozhe Cheng", "Jihua Zhu", "Kaibing Zhang"], "title": "Point-SRA: Self-Representation Alignment for 3D Representation Learning", "comment": "This is an AAAI 2026 accepted paper titled \"Point-SRA: Self-Representation Alignment for 3D Representation Learning\", spanning 13 pages in total. The submission includes 7 figures (fig1 to fig7) that visually support the technical analysis", "summary": "Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.", "AI": {"tldr": "Point-SRA\u901a\u8fc7\u52a8\u6001\u63a9\u7801\u548c\u6982\u7387\u5efa\u6a21\u4f18\u53163D\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e2a\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56fa\u5b9a\u63a9\u7801\u6bd4\u7387\u5ffd\u7565\u4e86\u591a\u7ea7\u8868\u5f81\u76f8\u5173\u6027\u548c\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u4e14\u4f9d\u8d56\u4e8e\u4e0e\u70b9\u4e91\u591a\u6837\u6027\u51b2\u7a81\u7684\u70b9\u7ea7\u91cd\u5efa\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoint-SRA\u76843D\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u63a9\u7801\u6bd4\u7387\u7684MAE\u3001MeanFlow Transformer\uff08MFT\uff09\u4ee5\u53ca\u53cc\u81ea\u8868\u5f81\u5bf9\u9f50\u673a\u5236\u3002", "result": "Point-SRA\u5728ScanObjectNN\u4e0a\u6bd4Point-MAE\u63d0\u5347\u4e865.37%\uff0c\u5728\u9885\u5185\u52a8\u8109\u7624\u5206\u5272\u4e2d\u8fbe\u5230\u4e8696.07%\u7684\u5e73\u5747IoU\uff08\u52a8\u8109\uff09\u548c86.87%\uff08\u52a8\u8109\u7624\uff09\uff0c\u57283D\u7269\u4f53\u68c0\u6d4b\u4e2dAP@50\u8fbe\u523047.3%\uff0c\u8d85\u8d8a\u4e86MaskPoint 5.12%\u3002", "conclusion": "Point-SRA\u901a\u8fc7\u81ea\u84b8\u998f\u548c\u6982\u7387\u5efa\u6a21\u5b9e\u73b0\u4e863D\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u8868\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.01908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01908", "abs": "https://arxiv.org/abs/2601.01908", "authors": ["Jingjing Wang", "Qianglin Liu", "Zhuo Xiao", "Xinning Yao", "Bo Liu", "Lu Li", "Lijuan Niu", "Fugen Zhou"], "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection", "comment": null, "summary": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.", "AI": {"tldr": "Nodule-DETR\u662f\u4e00\u79cd\u65b0\u578b\u7532\u72b6\u817a\u7ed3\u8282\u68c0\u6d4b\u53d8\u6362\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u63d0\u5347\u8d85\u58f0\u56fe\u50cf\u4e2d\u4f4e\u5bf9\u6bd4\u5ea6\u7ed3\u8282\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8d85\u58f0\u662f\u68c0\u6d4b\u7532\u72b6\u817a\u7ed3\u8282\u7684\u9996\u9009\u6210\u50cf\u65b9\u5f0f\uff0c\u4f46\u5176\u8bca\u65ad\u51c6\u786e\u6027\u5e38\u53d7\u4f4e\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u7ed3\u8282\u8fb9\u754c\u7684\u9650\u5236\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Nodule-DETR\u3002", "method": "Nodule-DETR\u662f\u4e00\u79cd\u65b0\u578b\u68c0\u6d4b\u53d8\u6362\u5668\uff08DETR\uff09\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u591a\u5149\u8c31\u9891\u57df\u901a\u9053\u6ce8\u610f\u529b\uff08MSFCA\uff09\u6a21\u5757\u3001\u5206\u5c42\u7279\u5f81\u878d\u5408\uff08HFF\uff09\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\uff08MSDA\uff09\u6a21\u5757\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7532\u72b6\u817a\u8d85\u58f0\u56fe\u50cf\u7684\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNodule-DETR\u5728mAP@0.5:0.95\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b0.149\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Nodule-DETR\u5c55\u793a\u4e86\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u7532\u72b6\u817a\u8bca\u65ad\u4e2d\u7684\u663e\u8457\u6f5c\u529b\uff0c\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u4f7f\u5176\u6210\u4e3a\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u5728\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.01749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01749", "abs": "https://arxiv.org/abs/2601.01749", "authors": ["Lei Zhu", "Lijian Lin", "Ye Zhu", "Jiahao Wu", "Xuehan Hou", "Yu Li", "Yunfei Liu", "Jie Chen"], "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement", "comment": "20 pages, 11i figures", "summary": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.", "AI": {"tldr": "MANGO\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7eaf\u56fe\u50cf\u7ea7\u76d1\u7763\u548c\u4ea4\u66ff\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u97f3\u9891\u9a71\u52a83D\u5934\u90e8\u751f\u6210\u65b9\u6cd5\u5728\u53cc\u5411\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8bf4\u8bdd\u8005\u573a\u666f\uff0c\u7f3a\u4e4f\u81ea\u7136\u7684\u53cc\u5411\u542c-\u8bf4\u4ea4\u4e92\uff0c\u4e14\u4f9d\u8d56\u4f2a3D\u6807\u7b7e\u5bfc\u81f4\u7cbe\u7ec6\u9762\u90e8\u52a8\u6001\u6355\u6349\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684Transformer\u548c\u53cc\u97f3\u9891\u4ea4\u4e92\u6a21\u5757\u5efa\u6a21\u591a\u8bf4\u8bdd\u8005\u97f3\u9891\u7684\u81ea\u71363D\u8fd0\u52a8\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5feb\u901f3D\u9ad8\u65af\u6e32\u67d3\u5668\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u63d0\u4f9b2D\u7ea7\u5149\u5ea6\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMANGO\u5728\u5efa\u6a21\u4e24\u4eba3D\u5bf9\u8bdd\u8fd0\u52a8\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u90e8\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "MANGO\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u7eaf\u56fe\u50cf\u7ea7\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a83D\u5934\u90e8\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\uff0c\u7279\u522b\u662f\u5728\u53cc\u5411\u5bf9\u8bdd\u573a\u666f\u4e2d\u3002"}}
{"id": "2601.01769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01769", "abs": "https://arxiv.org/abs/2601.01769", "authors": ["Hao Lu", "Ziniu Qian", "Yifu Li", "Yang Zhou", "Bingzheng Wei", "Yan Xu"], "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology", "comment": "The paper has been accepted by BIBM 2025", "summary": "In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6807\u51c6\u5316\u75c5\u7406\u4fe1\u606f\u63d0\u53d6\u6d41\u7a0b\u548cCTIS-QA\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316\u6d41\u7a0b\u548c\u6570\u636e\u96c6\uff0c\u63d0\u5347\u75c5\u7406\u62a5\u544a\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u901a\u8fc7CTIS-QA\u6a21\u578b\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e34\u5e8a\u75c5\u7406\u62a5\u544a\u6a21\u677f\uff08CPRT\uff09\u6765\u6807\u51c6\u5316\u63d0\u53d6\u75c5\u7406\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u4e86CTIS-Align\u548cCTIS-Bench\u6570\u636e\u96c6\u3002CTIS-QA\u6a21\u578b\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff0c\u5206\u522b\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u3002", "result": "CTIS-QA\u6a21\u578b\u5728WSI-VQA\u3001CTIS-Bench\u548c\u5e7b\u706f\u7247\u7ea7\u8bca\u65ad\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u8bca\u65ad\u6a21\u677f\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63d0\u53d6\u75c5\u7406\u4fe1\u606f\u5e76\u6784\u5efa\u4e86CTIS-QA\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002"}}
{"id": "2601.01784", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01784", "abs": "https://arxiv.org/abs/2601.01784", "authors": ["Boyang Zhao", "Xin Liao", "Jiaxin Chen", "Xiaoshuai Wu", "Yufeng Wu"], "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization", "comment": null, "summary": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.", "AI": {"tldr": "DDNet\u901a\u8fc7\u53cc\u6d41\u56fe\u5b66\u4e60\u548c\u89e3\u7f20\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd59%\u7684AP@0.95\u3002", "motivation": "AIGC\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u89c6\u9891\u7be1\u6539\u4ec5\u9700\u6539\u52a8\u5c0f\u7247\u6bb5\u5373\u53ef\u8bef\u5bfc\u89c2\u4f17\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u5c40\u9650\u4e8e\u5c40\u90e8\u89c6\u89d2\u800c\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u5f02\u5e38\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDDNet\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u8ddd\u79bb\u6d41\uff08\u6355\u6349\u5c40\u90e8\u4f2a\u5f71\uff09\u548c\u8bed\u4e49\u5185\u5bb9\u6d41\uff08\u6355\u6349\u957f\u7a0b\u8fde\u63a5\uff09\uff0c\u5e76\u5f15\u5165Trace Disentanglement and Adaptation\uff08TDA\uff09\u548cCross-Level Feature Embedding\uff08CLFE\uff09\u6280\u672f\u3002", "result": "\u5728ForgeryNet\u548cTVIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDNet\u7684AP@0.95\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea69%\uff0c\u8de8\u57df\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DDNet\u901a\u8fc7\u53cc\u6d41\u56fe\u5b66\u4e60\u548c\u89e3\u7f20\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4f2a\u9020\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728ForgeryNet\u548cTVIL\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7ea69%\u7684AP@0.95\u3002"}}
{"id": "2601.02016", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02016", "abs": "https://arxiv.org/abs/2601.02016", "authors": ["Matthias Bartolo", "Dylan Seychell", "Gabriel Hili", "Matthew Montebello", "Carl James Debono", "Saviour Formosa", "Konstantinos Makantasis"], "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach", "comment": "Code available on GitHub: https://github.com/mbar0075/lupi-for-object-detection", "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7279\u6743\u4fe1\u606f\u63d0\u5347\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u53ef\u7528\u4f46\u5728\u63a8\u7406\u65f6\u4e0d\u53ef\u7528\u7684\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u6027\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u5c06\u7279\u6743\u4fe1\u606f\uff08\u5982\u8fb9\u754c\u6846\u63a9\u7801\u3001\u663e\u8457\u6027\u56fe\u548c\u6df1\u5ea6\u7ebf\u7d22\uff09\u6ce8\u5165\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLUPI\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u590d\u6742\u5ea6\u6216\u6a21\u578b\u5927\u5c0f\u3002\u4e2d\u7b49\u548c\u5927\u5c3a\u5bf8\u7269\u4f53\u7684\u6027\u80fd\u63d0\u5347\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0cLUPI\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u63a8\u8fdb\u7269\u4f53\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u7b56\u7565\u3002"}}
{"id": "2601.02046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02046", "abs": "https://arxiv.org/abs/2601.02046", "authors": ["Shaocheng Shen", "Jianfeng Liang. Chunlei Cai", "Cong Geng", "Huiyu Duan", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "title": "Agentic Retoucher for Text-To-Image Generation", "comment": null, "summary": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.", "AI": {"tldr": "Agentic Retoucher \u901a\u8fc7\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u5faa\u73af\uff0c\u9ad8\u6548\u4fee\u6b63\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5c40\u90e8\u5931\u771f\uff0c\u663e\u8457\u63d0\u5347\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6210\u672c\u9ad8\u6216\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u66f4\u53ef\u9760\u4e14\u53ef\u63a7\u7684\u5c40\u90e8\u4fee\u6b63\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4ee3\u7406\uff1a(1) \u611f\u77e5\u4ee3\u7406\u5b66\u4e60\u4e0a\u4e0b\u6587\u663e\u8457\u6027\u4ee5\u5b9a\u4f4d\u5931\u771f\uff0c(2) \u63a8\u7406\u4ee3\u7406\u901a\u8fc7\u6e10\u8fdb\u504f\u597d\u5bf9\u9f50\u8fdb\u884c\u8bca\u65ad\uff0c(3) \u884c\u52a8\u4ee3\u7406\u57fa\u4e8e\u7528\u6237\u504f\u597d\u89c4\u5212\u5c40\u90e8\u4fee\u590d\u3002", "result": "\u5728 GenBlemish-27K \u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u3001\u5931\u771f\u5b9a\u4f4d\u548c\u7528\u6237\u504f\u597d\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Agentic Retoucher \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c42\u6b21\u5316\u51b3\u7b56\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u7684\u5faa\u73af\u5b9e\u73b0\u4e86\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5c40\u90e8\u5931\u771f\u7684\u9ad8\u6548\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2601.01804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01804", "abs": "https://arxiv.org/abs/2601.01804", "authors": ["Zhengjian Kang", "Qi Chen", "Rui Liu", "Kangtong Mo", "Xingyu Zhang", "Xiaoyu Deng", "Ye Zhang"], "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs", "comment": "7 pages, 4 figures", "summary": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.", "AI": {"tldr": "V-CORE\u901a\u8fc7\u663e\u5f0f\u65f6\u5e8f\u7ea6\u675f\u63d0\u5347\u89c6\u9891\u7406\u89e3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video-LLMs\uff09\u5728\u9700\u8981\u4e00\u81f4\u65f6\u5e8f\u548c\u56e0\u679c\u8fde\u8d2f\u6027\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u5e8f\u7ea6\u675f\u673a\u5236\u3002", "method": "V-CORE\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u805a\u5408\uff08LSA\uff09\u548c\u56e0\u679c\u611f\u77e5\u65f6\u5e8f\u6295\u5f71\u5668\uff08CATP\uff09\uff0c\u901a\u8fc7\u5757\u56e0\u679c\u6ce8\u610f\u529b\u548c\u7ec8\u7aef\u52a8\u6001\u6458\u8981\u4ee4\u724c\u786e\u4fdd\u65f6\u5e8f\u4fe1\u606f\u7684\u4e25\u683c\u6709\u5e8f\u805a\u5408\u3002", "result": "V-CORE\u5728NExT-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523061.2%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728MSVD-QA\u3001MSRVTT-QA\u548cTGIF-QA\u7b49\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5c24\u5176\u5728\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u5b50\u7c7b\u522b\u4e2d\u5206\u522b\u63d0\u53473.5%\u548c5.2%\u3002", "conclusion": "V-CORE\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u7684\u65f6\u95f4\u987a\u5e8f\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u663e\u5f0f\u65f6\u5e8f\u7ea6\u675f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.01818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01818", "abs": "https://arxiv.org/abs/2601.01818", "authors": ["Sungjune Park", "Hongda Mao", "Qingshuang Chen", "Yong Man Ro", "Yelin Kim"], "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning", "comment": "11 pages, 7 figures, 4 tables", "summary": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5668\u548c\u7279\u5b9a\u8bad\u7ec3\u76ee\u6807\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u52a8\u6001\u81ea\u6211\u4e2d\u5fc3\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u6a21\u7cca\u6027\uff0c\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u8868\u660e\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u8c03\u8282\u4eba\u7c7b\u6ce8\u610f\u529b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u5668\uff0c\u57fa\u4e8e\u8bed\u8a00\u573a\u666f\u63cf\u8ff0\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u9891\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u8bad\u7ec3\u76ee\u6807\uff1a\u4e00\u662f\u805a\u7126\u4e8e\u76ee\u6807\u5174\u8da3\u533a\u57df\uff0c\u4e8c\u662f\u6291\u5236\u4e0d\u592a\u53ef\u80fd\u5438\u5f15\u7b2c\u4e00\u4eba\u79f0\u6ce8\u610f\u529b\u7684\u65e0\u5173\u533a\u57df\u7684\u5e72\u6270\u3002", "result": "\u5728Ego4D\u548cAria Everyday Activities (AEA)\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u52a8\u6001\u81ea\u6211\u4e2d\u5fc3\u573a\u666f\u4e2d\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.02126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02126", "abs": "https://arxiv.org/abs/2601.02126", "authors": ["Xavier Bou", "Elliot Vincent", "Gabriele Facciolo", "Rafael Grompone von Gioi", "Jean-Michel Morel", "Thibaud Ehret"], "title": "Remote Sensing Change Detection via Weak Temporal Supervision", "comment": null, "summary": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.", "AI": {"tldr": "\u5229\u7528\u73b0\u6709\u5355\u65f6\u76f8\u6570\u636e\u96c6\u7684\u5f31\u65f6\u95f4\u76d1\u7763\u7b56\u7565\uff0c\u65e0\u9700\u65b0\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u53d8\u5316\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u96f6\u6837\u672c\u548c\u4f4e\u6570\u636e\u91cf\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u9065\u611f\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u907f\u514d\u9ad8\u6210\u672c\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u65f6\u95f4\u76d1\u7763\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u5c55\u5355\u65f6\u76f8\u6570\u636e\u96c6\u4e3a\u591a\u65f6\u76f8\u89c2\u6d4b\uff0c\u5e76\u5047\u8bbe\u771f\u5b9e\u53cc\u65f6\u76f8\u5bf9\u5927\u591a\u65e0\u53d8\u5316\uff0c\u540c\u65f6\u5229\u7528\u4e0d\u540c\u4f4d\u7f6e\u7684\u56fe\u50cf\u751f\u6210\u53d8\u5316\u6837\u672c\u3002\u91c7\u7528\u5bf9\u8c61\u611f\u77e5\u7684\u53d8\u5316\u56fe\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u5904\u7406\u5f31\u6807\u7b7e\u566a\u58f0\u3002", "result": "\u5728FLAIR\u548cIAILD\u822a\u7a7a\u6570\u636e\u96c6\u7684\u6269\u5c55\u7248\u672c\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u548c\u4f4e\u6570\u636e\u91cf\u4e0b\u7684\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u6cd5\u56fd\u5927\u533a\u57df\u5e94\u7528\u4e2d\u663e\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f31\u65f6\u95f4\u76d1\u7763\u7b56\u7565\u6709\u6548\u5229\u7528\u4e86\u73b0\u6709\u5355\u65f6\u76f8\u6570\u636e\u96c6\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u548c\u4f4e\u6570\u636e\u91cf\u4e0b\u7684\u9ad8\u6027\u80fd\u53d8\u5316\u68c0\u6d4b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.02147", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02147", "abs": "https://arxiv.org/abs/2601.02147", "authors": ["Sunny Gupta", "Shounak Das", "Amit Sethi"], "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models", "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World", "summary": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.", "AI": {"tldr": "BiPrompt\u901a\u8fc7\u53cc\u8fb9\u63d0\u793a\u4f18\u5316\u540c\u65f6\u51cf\u5c11\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e2d\u7684\u975e\u56e0\u679c\u7279\u5f81\u4f9d\u8d56\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u4ec5\u9488\u5bf9\u5355\u4e00\u6a21\u6001\uff08\u89c6\u89c9\u6216\u6587\u672c\uff09\u5bfc\u81f4\u7684\u90e8\u5206\u9c81\u68d2\u6027\u548c\u4e0d\u7a33\u5b9a\u9002\u5e94\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u5f15\u5bfc\u64e6\u9664\u4ee5\u6291\u5236\u80cc\u666f\u6fc0\u6d3b\uff0c\u5e76\u5728\u6587\u672c\u4fa7\u5f15\u5165\u5e73\u8861\u63d0\u793a\u5f52\u4e00\u5316\uff0c\u5bf9\u9f50\u7c7b\u5d4c\u5165\u81f3\u5404\u5411\u540c\u6027\u8bed\u4e49\u7a7a\u95f4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u504f\u7f6e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBiPrompt\u5728\u5e73\u5747\u548c\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\u3002", "conclusion": "BiPrompt\u6846\u67b6\u901a\u8fc7\u53cc\u8fb9\u63d0\u793a\u4f18\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e2d\u7684\u975e\u56e0\u679c\u7279\u5f81\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u9886\u57df\u76d1\u7763\u3002"}}
{"id": "2601.01847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01847", "abs": "https://arxiv.org/abs/2601.01847", "authors": ["Chuhang Ma", "Shuai Tan", "Ye Pan", "Jiaolong Yang", "Xin Tong"], "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting", "comment": "13 pages, 10 figures", "summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.", "AI": {"tldr": "ESGaussianFace \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u60c5\u611f\u5316\u548c\u98ce\u683c\u5316\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u70b9\u6e85\u5c04\u548c\u60c5\u611f\u97f3\u9891\u5f15\u5bfc\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u89c6\u9891\u751f\u6210\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u4e2d\u6027\u60c5\u611f\u89c6\u9891\u751f\u6210\uff0c\u60c5\u611f\u5316\u548c\u98ce\u683c\u5316\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u751f\u6210\u60c5\u611f\u5316\u548c\u98ce\u683c\u5316\u9762\u90e8\u52a8\u753b\u7684\u6846\u67b6\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u70b9\u6e85\u5c04\u6280\u672f\u91cd\u5efa3D\u573a\u666f\u5e76\u6e32\u67d3\u89c6\u9891\uff0c\u63d0\u51fa\u60c5\u611f\u97f3\u9891\u5f15\u5bfc\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e24\u4e2a3D\u9ad8\u65af\u70b9\u53d8\u5f62\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u60c5\u611f\u548c\u98ce\u683c\u7279\u5f81\u76843D\u9ad8\u65af\u70b9\u53d8\u5f62\u3002\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u9010\u6b65\u5b66\u4e60\u5634\u5507\u8fd0\u52a8\u3001\u60c5\u611f\u53d8\u5316\u548c\u98ce\u683c\u7279\u5f81\u3002", "result": "\u751f\u6210\u7684\u89c6\u9891\u5177\u6709\u9ad8\u6548\u7387\u3001\u9ad8\u8d28\u91cf\u548c3D\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u5634\u5507\u8fd0\u52a8\u51c6\u786e\u6027\u3001\u8868\u60c5\u53d8\u5316\u548c\u98ce\u683c\u7279\u5f81\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "ESGaussianFace \u6846\u67b6\u901a\u8fc7\u521b\u65b0\u76843D\u9ad8\u65af\u70b9\u53d8\u5f62\u9884\u6d4b\u5668\u548c\u60c5\u611f\u97f3\u9891\u5f15\u5bfc\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u60c5\u611f\u5316\u548c\u98ce\u683c\u5316\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u751f\u6210\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5634\u5507\u8fd0\u52a8\u51c6\u786e\u6027\u3001\u8868\u60c5\u53d8\u5316\u548c\u98ce\u683c\u7279\u5f81\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2601.01856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01856", "abs": "https://arxiv.org/abs/2601.01856", "authors": ["Joongwon Chae", "Lihui Luo", "Yang Liu", "Runming Wang", "Dongmei Yu", "Zeming Liang", "Xi Yuan", "Dayan Zhang", "Zhenglin Chen", "Peiwu Qin", "Ilmoon Chae"], "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection", "comment": null, "summary": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.\n  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.\n  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR", "AI": {"tldr": "GCR\u662f\u4e00\u79cd\u51e0\u4f55\u4e00\u81f4\u7684\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u8de8\u5934\u51b3\u7b56\u548c\u5934\u5185\u5f02\u5e38\u8bc4\u5206\uff0c\u89e3\u51b3\u4e86\u8de8\u5934\u8def\u7531\u4e2d\u7684\u5206\u6570\u53ef\u6bd4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6301\u7eed\u5f02\u5e38\u68c0\u6d4b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u7c7b\u522b\u8def\u7531\u65f6\uff0c\u7531\u4e8e\u5206\u6570\u5206\u5e03\u5728\u4e0d\u540c\u7c7b\u522b\u95f4\u5dee\u5f02\u8f83\u5927\uff0c\u5bfc\u81f4\u8def\u7531\u4e0d\u53ef\u9760\u3002GCR\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u7684\u8def\u7531\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GCR\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u7684\u51bb\u7ed3\u8865\u4e01\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6700\u5c0f\u5316\u7d2f\u79ef\u6700\u8fd1\u539f\u578b\u8ddd\u79bb\u6765\u8def\u7531\u6d4b\u8bd5\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u7684\u57fa\u4e8e\u539f\u578b\u7684\u8bc4\u5206\u89c4\u5219\u5728\u8def\u7531\u7684\u4e13\u5bb6\u5185\u8ba1\u7b97\u5f02\u5e38\u56fe\u3002", "result": "\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGCR\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u7a33\u5b9a\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u9057\u5fd8\u7684\u6301\u7eed\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "GCR\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u7684\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u7a33\u5b9a\u6027\u5e76\u7f13\u89e3\u4e86\u6301\u7eed\u6027\u80fd\u5d29\u6e83\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u9057\u5fd8\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2601.02204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02204", "abs": "https://arxiv.org/abs/2601.02204", "authors": ["Huichao Zhang", "Liao Qu", "Yiheng Liu", "Hang Chen", "Yangyang Song", "Yongsheng Dong", "Shikun Sun", "Xian Li", "Xu Wang", "Yi Jiang", "Hu Ye", "Bo Chen", "Yiming Gao", "Peng Liu", "Akide Liu", "Zhipeng Yang", "Qili Deng", "Linjie Xing", "Jiyang Liu", "Zhao Wang", "Yang Zhou", "Mingcong Liu", "Yi Zhang", "Qian He", "Xiwei Hu", "Zhongqi Qi", "Jie Shao", "Zhiye Fu", "Shuai Wang", "Fangmin Chen", "Xuezhi Chai", "Zhihua Wu", "Yitong Wang", "Zehuan Yuan", "Daniel K. Du", "Xinglong Wu"], "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "AI": {"tldr": "NextFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u548c\u56fe\u50cf\uff0c\u6311\u6218\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u7684\u4e0d\u540c\u7279\u6027\uff08\u6587\u672c\u4e25\u683c\u5e8f\u5217\u5316\uff0c\u56fe\u50cf\u5177\u6709\u5c42\u6b21\u6027\uff09\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u5904\u7406\u6846\u67b6\u4ee5\u6fc0\u6d3b\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7ed3\u5408\u6587\u672c\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u548c\u56fe\u50cf\u7684\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\uff0c\u901a\u8fc7\u7a33\u5065\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u524d\u7f00\u8c03\u4f18\u7b56\u7565\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "NextFlow\u57285\u79d2\u5185\u751f\u62101024x1024\u56fe\u50cf\uff0c\u901f\u5ea6\u8fdc\u8d85\u540c\u7c7bAR\u6a21\u578b\uff0c\u5e76\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u5ab2\u7f8e\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u3002", "conclusion": "NextFlow\u901a\u8fc7\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u67b6\u6784\u548c\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u6311\u6218\u4e86\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01865", "abs": "https://arxiv.org/abs/2601.01865", "authors": ["Wenlong Yang", "Canran Jin", "Weihang Yuan", "Chao Wang", "Lifeng Sun"], "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations", "comment": null, "summary": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.", "AI": {"tldr": "RRNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u5149\u6e90\u53c2\u6570\u4f30\u8ba1\u548c\u6df1\u5ea6\u611f\u77e5\u6e32\u67d3\u5b9e\u73b0\u9ad8\u6548\u5c40\u90e8\u91cd\u7167\u660e\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u4f1a\u8bae\u7b49\u5e94\u7528\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u65f6\u89c6\u9891\u589e\u5f3a\u5728\u76f4\u64ad\u5e94\u7528\u4e2d\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6709\u6548\u66dd\u5149\u63a7\u5236\uff08\u5c24\u5176\u662f\u5728\u4e0d\u5747\u5300\u5149\u7167\u4e0b\uff09\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "RRNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u53ef\u914d\u7f6e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u6700\u5c0f\u865a\u62df\u5149\u6e90\u96c6\u7684\u53c2\u6570\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6e32\u67d3\u6a21\u5757\u5b9e\u73b0\u5c40\u90e8\u91cd\u7167\u660e\uff0c\u65e0\u9700\u50cf\u7d20\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "RRNet\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6743\u8861\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5934\u652f\u6301\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u6027\u80fd\u3002", "conclusion": "RRNet\u901a\u8fc7\u5176\u53ef\u89e3\u91ca\u7684\u7167\u660e\u63a7\u5236\u548c\u9ad8\u6548\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u4f1a\u8bae\u3001AR\u8096\u50cf\u589e\u5f3a\u548c\u79fb\u52a8\u6444\u5f71\u7b49\u5b9e\u9645\u5e94\u7528\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4f4e\u5149\u589e\u5f3a\u3001\u5c40\u90e8\u7167\u660e\u8c03\u6574\u548c\u7729\u5149\u53bb\u9664\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.02206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02206", "abs": "https://arxiv.org/abs/2601.02206", "authors": ["Dachun Kai", "Zeyu Xiao", "Huyue Zhu", "Jiaxiao Wang", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras", "comment": "Accepted to AAAI 2026", "summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.", "AI": {"tldr": "RetinexEVSR\u662f\u4e00\u79cd\u65b0\u578b\u4e8b\u4ef6\u9a71\u52a8\u7684\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u4fe1\u53f7\u548cRetinex\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8d28\u91cf\u548c\u5904\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7531\u4e8e\u5bf9\u6bd4\u5ea6\u6709\u9650\u548c\u9ad8\u9891\u4fe1\u606f\u4e0d\u8db3\uff0c\u96be\u4ee5\u6062\u590d\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u5305\u62ec\u5149\u7167\u5f15\u5bfc\u7684\u4e8b\u4ef6\u589e\u5f3a\u6a21\u5757\u548c\u4e8b\u4ef6\u5f15\u5bfc\u7684\u53cd\u5c04\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u4ece\u566a\u58f0\u4e8b\u4ef6\u6570\u636e\u548c\u9000\u5316\u7684RGB\u5e27\u4e2d\u63d0\u53d6\u5e76\u6574\u5408\u6709\u7528\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728SDSD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u8fbe2.95 dB\uff0c\u540c\u65f6\u8fd0\u884c\u65f6\u51cf\u5c1165%\u3002", "conclusion": "RetinexEVSR\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u4fe1\u53f7\u548cRetinex\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u6210\u4e3a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.01870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01870", "abs": "https://arxiv.org/abs/2601.01870", "authors": ["Wenyu Shao", "Hongbo Liu", "Yunchuan Ma", "Ruili Wang"], "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.", "AI": {"tldr": "EGMT\u662f\u4e00\u79cd\u65b0\u578b\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f53\u7ea7\u6587\u672c\u63d0\u53d6\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u878d\u5408\u8d28\u91cf\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u53e5\u5b50\u7ea7\u6587\u672c\u4fe1\u606f\uff0c\u6613\u5f15\u5165\u8bed\u4e49\u566a\u58f0\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u7684\u6df1\u5c42\u8bed\u4e49\u4ef7\u503c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f53\u7ea7\u6587\u672c\u4fe1\u606f\u63d0\u53d6\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9e\u4f53\u7ea7\u6587\u672c\u4fe1\u606f\u63d0\u53d6\u3001\u5e76\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784\u6784\u5efa\u4ee5\u53ca\u5b9e\u4f53\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u5f00\u53d1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEGMT\u5728\u4fdd\u7559\u663e\u8457\u76ee\u6807\u3001\u7eb9\u7406\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u5b9e\u4f53\u6807\u6ce8\u7248\u672c\u3002", "conclusion": "EGMT\u65b9\u6cd5\u901a\u8fc7\u5b9e\u4f53\u5f15\u5bfc\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u5bc6\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2601.02242", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02242", "abs": "https://arxiv.org/abs/2601.02242", "authors": ["Grigorii Alekseenko", "Aleksandr Gordeev", "Irina Tolstykh", "Bulat Suleimanov", "Vladimir Dokholyan", "Georgii Fedorov", "Sergey Yakubson", "Aleksandra Tsybina", "Mikhail Chernyshov", "Maksim Kuprashevich"], "title": "VIBE: Visual Instruction Based Editor", "comment": null, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5c0f\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u6a21\u578b\uff0c\u9002\u5408\u4f4e\u6210\u672c\u548c\u5feb\u901f\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u8d28\u91cf\u4e0a\u6709\u9650\uff0c\u4e14\u4e3b\u6d41\u6269\u6563\u6a21\u578b\u53c2\u6570\u5e9e\u5927\uff086B-20B\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75282B\u53c2\u6570\u7684Qwen3-VL\u6a21\u578b\u6307\u5bfc\u7f16\u8f91\u8fc7\u7a0b\u548c1.6B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578bSana1.5\u8fdb\u884c\u56fe\u50cf\u751f\u6210\uff0c\u8bbe\u8ba1\u51b3\u7b56\u6db5\u76d6\u67b6\u6784\u3001\u6570\u636e\u5904\u7406\u3001\u8bad\u7ec3\u914d\u7f6e\u548c\u8bc4\u4f30\u3002", "result": "\u5728ImgEdit\u548cGEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u53c2\u6570\u66f4\u591a\u3001\u63a8\u7406\u6210\u672c\u66f4\u9ad8\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u7684\u7f16\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u6a21\u578b\u572824GB GPU\u5185\u5b58\u5185\u8fd0\u884c\uff0c\u751f\u62102K\u5206\u8fa8\u7387\u56fe\u50cf\u7ea64\u79d2\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u9ad8\u541e\u5410\u91cf\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684\u6a21\u578b\uff08Qwen3-VL\u548cSana1.5\uff09\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u548c\u4e25\u683c\u6e90\u4e00\u81f4\u6027\u3002"}}
{"id": "2601.01891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01891", "abs": "https://arxiv.org/abs/2601.01891", "authors": ["Niloufar Alipour Talemi", "Julia Boone", "Fatemeh Afghah"], "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems", "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop", "summary": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9065\u611f\u9886\u57df\u4ee3\u7406AI\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u548c\u67b6\u6784\u5206\u6790\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u5206\u6790\u8303\u5f0f\u6b63\u4ece\u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8f6c\u5411\u81ea\u4e3b\u4ee3\u7406AI\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5730\u7406\u7a7a\u95f4\u5de5\u4f5c\u6d41\u4e2d\u7f3a\u4e4f\u987a\u5e8f\u89c4\u5212\u548c\u4e3b\u52a8\u5de5\u5177\u534f\u8c03\u80fd\u529b\u3002", "method": "\u4ecb\u7ecd\u4e86\u533a\u5206\u5355\u667a\u80fd\u4f53\u534f\u4f5c\u8005\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u89c4\u5212\u673a\u5236\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u8bb0\u5fc6\u7ed3\u6784\u7b49\u67b6\u6784\u57fa\u7840\u3002", "result": "\u7efc\u8ff0\u4e86\u9065\u611f\u9886\u57df\u4e2d\u4ee3\u7406AI\u7684\u9996\u6b21\u5168\u9762\u56de\u987e\uff0c\u5e76\u8bc4\u4f30\u4e86\u4ece\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u5230\u8f68\u8ff9\u611f\u77e5\u63a8\u7406\u6b63\u786e\u6027\u7684\u65b0\u5174\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u6982\u8ff0\u4e86\u7a33\u5065\u3001\u81ea\u4e3b\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u53d1\u5c55\u7684\u6218\u7565\u8def\u7ebf\u56fe\u3002"}}
{"id": "2601.02246", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02246", "abs": "https://arxiv.org/abs/2601.02246", "authors": ["Annoor Sharara Akhand"], "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cdCNN\u8bad\u7ec3\u8303\u5f0f\uff0c\u8fc1\u79fb\u5b66\u4e60\u8868\u73b0\u6700\u4f73\uff0c\u5b9a\u5236CNN\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e0a\u5e73\u8861\u8f83\u597d\u3002", "motivation": "\u6bd4\u8f83\u4e0d\u540cCNN\u8bad\u7ec3\u8303\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "method": "\u5bf9\u4e09\u79cdCNN\u8bad\u7ec3\u8303\u5f0f\uff08\u5b9a\u5236CNN\u8bad\u7ec3\u3001\u9884\u8bad\u7ec3CNN\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3001\u8fc1\u79fb\u5b66\u4e60\uff09\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u5728\u51c6\u786e\u7387\u548c\u5b8fF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5b9a\u5236CNN\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5b9a\u5236CNN\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u9884\u7b97\u6709\u9650\u65f6\u63d0\u4f9b\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.01892", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01892", "abs": "https://arxiv.org/abs/2601.01892", "authors": ["Arjun Ramesh Kaushik", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Forget Less by Learning from Parents Through Hierarchical Relationships", "comment": "Accepted at AAAI-26", "summary": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.", "AI": {"tldr": "FLLP\u662f\u4e00\u79cd\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u901a\u8fc7\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u5c0f\u5316\u6982\u5ff5\u95f4\u7684\u5e72\u6270\uff0c\u5ffd\u7565\u4e86\u6982\u5ff5\u95f4\u53ef\u80fd\u7684\u79ef\u6781\u4e92\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u5e76\u652f\u6301\u65b0\u6982\u5ff5\u7684\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86FLLP\u6846\u67b6\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\uff08Lorentzian\u6d41\u5f62\uff09\u4e2d\u7684\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u5df2\u5b66\u4e60\u6982\u5ff5\u6307\u5bfc\u65b0\u6982\u5ff5\u7684\u9002\u5e94\uff0c\u5b9e\u73b0\u77e5\u8bc6\u7684\u6301\u7eed\u6574\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86FLLP\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "FLLP\u6846\u67b6\u901a\u8fc7\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5f15\u5165\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2601.02273", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02273", "abs": "https://arxiv.org/abs/2601.02273", "authors": ["Salim Khazem"], "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation", "comment": null, "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git", "AI": {"tldr": "TopoLoRA-SAM\u662f\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u7684\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdbSAM\u5728\u7279\u5b9a\u9886\u57df\u7684\u5206\u5272\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u53c2\u6570\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u5168\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u5728\u7279\u5b9a\u9886\u57df\u8bed\u4e49\u5206\u5272\uff08\u5c24\u5176\u662f\u8584\u7ed3\u6784\u548c\u566a\u58f0\u6a21\u6001\uff09\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5168\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\u548c\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86TopoLoRA-SAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u3001\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u5377\u79ef\u9002\u914d\u5668\u548c\u53ef\u5fae\u5206clDice\u7684\u62d3\u6251\u611f\u77e5\u76d1\u7763\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u3001\u606f\u8089\u5206\u5272\u548cSAR\u6d77\u9646\u5206\u5272\uff09\uff0cTopoLoRA-SAM\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u5e73\u5747Dice\u5206\u6570\uff0c\u4e14\u4ec5\u8bad\u7ec3\u4e865.2%\u7684\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "TopoLoRA-SAM\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u548c\u53c2\u6570\u9ad8\u6548\u7684\u9002\u5e94\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u8bad\u7ec3\u4e86\u5c11\u91cf\u53c2\u6570\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8584\u7ed3\u6784\u548c\u566a\u58f0\u6a21\u6001\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.01914", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01914", "abs": "https://arxiv.org/abs/2601.01914", "authors": ["Arjun Ramesh Kaushik", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion", "comment": "Accepted at WACV-26", "summary": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.", "AI": {"tldr": "HybridTAS\u901a\u8fc7\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u51e0\u4f55\uff0c\u5229\u7528\u52a8\u4f5c\u7684\u5c42\u6b21\u7ed3\u6784\u6539\u8fdb\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fed\u4ee3\u4f18\u5316\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4eba\u7c7b\u52a8\u4f5c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u663e\u5f0f\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHybridTAS\u6846\u67b6\uff0c\u5c06\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u51e0\u4f55\u7ed3\u5408\u5230\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u6811\u72b6\u5173\u7cfb\u4ece\u7c97\u5230\u7ec6\u6307\u5bfc\u52a8\u4f5c\u6807\u7b7e\u7684\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5728GTEA\u300150Salads\u548cBreakfast\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HybridTAS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u51e0\u4f55\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5229\u7528\u52a8\u4f5c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2601.01915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01915", "abs": "https://arxiv.org/abs/2601.01915", "authors": ["Yujie Hu", "Zecheng Tang", "Xu Jiang", "Weiqi Li", "Jian Zhang"], "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing", "comment": "a Conversational Assistant for Intelligent Image Editing", "summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.", "AI": {"tldr": "TalkPhoto \u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u901a\u7528\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4ea4\u4e92\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8c03\u7528\u73b0\u6709\u65b9\u6cd5\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u591a\u6307\u4ee4\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u9700\u8981\u6784\u5efa\u591a\u6307\u4ee4\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8017\u65f6\u8017\u529b\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u5f00\u6e90 LLM \u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\u6a21\u677f\uff0c\u5206\u5c42\u8c03\u7528\u73b0\u6709\u9ad8\u7ea7\u7f16\u8f91\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTalkPhoto \u5728\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7f16\u8f91\u8d28\u91cf\u548c\u66f4\u4f4e\u7684 token \u6d88\u8017\u3002", "conclusion": "TalkPhoto \u6846\u67b6\u901a\u8fc7\u514d\u8bad\u7ec3\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u5728\u591a\u4efb\u52a1\u7f16\u8f91\u4e2d\u7684\u4f18\u8d8a\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2601.01925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01925", "abs": "https://arxiv.org/abs/2601.01925", "authors": ["Lianjie Jia", "Yuhan Wu", "Binghao Ran", "Yifan Wang", "Lijun Wang", "Huchuan Lu"], "title": "AR-MOT: Autoregressive Multi-object Tracking", "comment": "12 pages, 5 figures", "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.", "AI": {"tldr": "AR-MOT\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u56de\u5f52\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u751f\u6210\u5b9e\u73b0\u7075\u6d3b\u8ddf\u8e2a\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709MOT\u65b9\u6cd5\u67b6\u6784\u50f5\u5316\u4e14\u4efb\u52a1\u7279\u5b9a\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u4efb\u52a1\u548c\u65b0\u8ddf\u8e2a\u573a\u666f\u3002", "method": "\u63d0\u51faAR-MOT\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u5c06MOT\u4efb\u52a1\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u5e76\u5f15\u5165Object Tokenizer\u3001RAA\u6a21\u5757\u548cTMF\u6a21\u5757\u3002", "result": "\u5728MOT17\u548cDanceTrack\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "AR-MOT\u901a\u8fc7\u81ea\u56de\u5f52\u8303\u5f0f\u5728LLM\u6846\u67b6\u4e2d\u5b9e\u73b0MOT\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u66f4\u901a\u7528\u548c\u7075\u6d3b\u7684MOT\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01926", "abs": "https://arxiv.org/abs/2601.01926", "authors": ["Zhifei Li", "Yiran Wang", "Chenyi Xiong", "Yujing Xia", "Xiaoju Hou", "Yue Zhao", "Miao Zhang", "Kui Xiao", "Bing Yang"], "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering", "comment": "Accepted to AAAI 2026", "summary": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.", "AI": {"tldr": "MacVQA\u662f\u4e00\u79cd\u65b0\u578b\u6301\u7eedVQA\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5185\u5b58\u548c\u566a\u58f0\u8fc7\u6ee4\u4f18\u5316\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u96be\u4ee5\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u3001\u9002\u5e94\u548c\u9c81\u68d2\u7279\u5f81\u8868\u793a\uff0cMacVQA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMacVQA\u7684\u65b0\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u548c\u95ee\u9898\u4fe1\u606f\u5e76\u8fc7\u6ee4\u566a\u58f0\uff0c\u91c7\u7528\u57fa\u4e8e\u539f\u578b\u7684\u8bb0\u5fc6\u5206\u914d\u4f18\u5316\u7279\u5f81\u8d28\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5728\u5341\u4e2a\u6301\u7eedVQA\u4efb\u52a1\u4e2d\uff0cMacVQA\u5e73\u5747\u51c6\u786e\u7387\u4e3a43.38%\uff0c\u5e73\u5747\u9057\u5fd8\u7387\u4e3a2.32%\uff1b\u5728\u65b0\u7ec4\u5408\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4e3a42.53%\uff0c\u5e73\u5747\u9057\u5fd8\u7387\u4e3a3.60%\u3002", "conclusion": "MacVQA\u901a\u8fc7\u81ea\u9002\u5e94\u5185\u5b58\u5206\u914d\u548c\u5168\u5c40\u566a\u58f0\u8fc7\u6ee4\uff0c\u5728\u6301\u7eed\u89c6\u89c9\u95ee\u7b54\u5b66\u4e60\u4e2d\u5e73\u8861\u4e86\u77e5\u8bc6\u83b7\u53d6\u3001\u4fdd\u7559\u548c\u7ec4\u5408\u6cdb\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.01950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01950", "abs": "https://arxiv.org/abs/2601.01950", "authors": ["Meng Wang", "Wenjing Dai", "Jiawan Zhang", "Xiaojie Guo"], "title": "Face Normal Estimation from Rags to Riches", "comment": null, "summary": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u9762\u90e8\u6cd5\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ec6\u5316\u7f51\u7edc\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u9700\u6c42\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u6cd5\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4ece\u7c97\u5230\u7ec6\u7684\u4f30\u8ba1\u5668\u6765\u7f13\u89e3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u9996\u5148\u4f7f\u7528\u5c0f\u6570\u636e\u96c6\u8bad\u7ec3\u4e00\u4e2a\u7b80\u6d01\u6a21\u578b\u751f\u6210\u7c97\u7cd9\u7684\u9762\u90e8\u6cd5\u7ebf\uff08\u79f0\u4e3a\u793a\u4f8b\uff09\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u4ee5\u4fee\u590d\u5c40\u90e8\u4f2a\u5f71\uff0c\u6700\u540e\u5b9a\u5236\u7ec6\u5316\u7f51\u7edc\u5c06\u8f93\u5165\u56fe\u50cf\u4e0e\u793a\u4f8b\u6620\u5c04\u5230\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u6cd5\u7ebf\u3002", "result": "\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6210\u672c\u548c\u4f30\u8ba1\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u9762\u90e8\u6cd5\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.01955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01955", "abs": "https://arxiv.org/abs/2601.01955", "authors": ["Zhexin Zhang", "Yifeng Zhu", "Yangyang Xu", "Long Chen", "Yong Du", "Shengfeng He", "Jun Yu"], "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization", "comment": null, "summary": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.", "AI": {"tldr": "MotionAdapter\u662f\u4e00\u4e2a\u5185\u5bb9\u611f\u77e5\u7684\u8fd0\u52a8\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u4e0e\u5916\u89c2\u5e76\u5b9a\u5236\u8fd0\u52a8\u573a\uff0c\u663e\u8457\u63d0\u5347\u4e86DiT-based T2V\u6a21\u578b\u7684\u8fd0\u52a8\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u89c6\u9891\u95f4\u590d\u6742\u8fd0\u52a8\u7684\u8fc1\u79fb\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "MotionAdapter\u9996\u5148\u901a\u8fc7\u5206\u67903D\u5168\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u7684\u8de8\u5e27\u6ce8\u610f\u529b\u6765\u63d0\u53d6\u8fd0\u52a8\u573a\uff0c\u7136\u540e\u5229\u7528DINO\u5f15\u5bfc\u7684\u8fd0\u52a8\u5b9a\u5236\u6a21\u5757\u6839\u636e\u5185\u5bb9\u5bf9\u5e94\u5173\u7cfb\u91cd\u65b0\u6392\u5217\u548c\u4f18\u5316\u8fd0\u52a8\u573a\uff0c\u6700\u7ec8\u6307\u5bfcDiT\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMotionAdapter\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u81ea\u7136\u652f\u6301\u590d\u6742\u8fd0\u52a8\u8fc1\u79fb\u548c\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "MotionAdapter\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u8fd0\u52a8\u4e0e\u5916\u89c2\uff0c\u5e76\u5f15\u5165DINO\u5f15\u5bfc\u7684\u8fd0\u52a8\u5b9a\u5236\u6a21\u5757\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728DiT-based T2V\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u8fd0\u52a8\u8fc1\u79fb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.01957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01957", "abs": "https://arxiv.org/abs/2601.01957", "authors": ["Tianbo Wang", "Yuqing Ma", "Kewei Liao", "Zhange Zhang", "Simin Li", "Jinyang Guo", "Xianglong Liu"], "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.", "AI": {"tldr": "AFTER\u65b9\u6cd5\u901a\u8fc7\u4e8b\u5b9e\u5f15\u5bfc\u7684\u89c6\u89c9-\u6587\u672c\u7f16\u8f91\uff0c\u6709\u6548\u51cf\u5c11LVLMs\u4e2d\u7684\u5bf9\u8c61\u5e7b\u89c9\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6548\u679c\u3002", "motivation": "LVLMs\u7531\u4e8e\u8bed\u8a00\u504f\u5dee\u6613\u4ea7\u751f\u5bf9\u8c61\u5e7b\u89c9\uff0c\u963b\u788d\u53ef\u4fe1AI\u5e94\u7528\u3002\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u5ffd\u89c6\u4e8b\u5b9e\u6587\u672c\u8bed\u4e49\u7684\u6307\u5bfc\uff0c\u96be\u4ee5\u663e\u5f0f\u7f13\u89e3\u8bed\u8a00\u504f\u5dee\u3002", "method": "AFTER\u65b9\u6cd5\u5305\u542bFAS\u548cQAO\u4e24\u4e2a\u6a21\u5757\uff0cFAS\u63d0\u4f9b\u4e8b\u5b9e\u548c\u901a\u7528\u6307\u5bfc\u4ee5\u7f16\u8f91\u6fc0\u6d3b\uff0cQAO\u5219\u5f15\u5165\u67e5\u8be2\u611f\u77e5\u504f\u79fb\u4f30\u8ba1\u5668\u4ee5\u589e\u5f3a\u7f16\u8f91\u7684\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u3002", "result": "AFTER\u5728\u591a\u4e2aLVLMs\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u7279\u522b\u662f\u5728AMBER\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8616.3%\u7684\u5e7b\u89c9\u51cf\u5c11\u3002", "conclusion": "AFTER\u65b9\u6cd5\u901a\u8fc7Factual-Augmented Activation Steering (FAS)\u548cQuery-Adaptive Offset Optimization (QAO)\u6709\u6548\u51cf\u5c11\u4e86LVLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002"}}
{"id": "2601.01963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01963", "abs": "https://arxiv.org/abs/2601.01963", "authors": ["Arjun Ramesh Kaushik", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Nalini Ratha", "Venu Govindaraju"], "title": "Forget Less by Learning Together through Concept Consolidation", "comment": "Accepted at WACV-26", "summary": "Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.", "AI": {"tldr": "FL2T\u6846\u67b6\u901a\u8fc7\u8de8\u6982\u5ff5\u5b66\u4e60\u6a21\u5757\u89e3\u51b3\u4e86CDMs\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u4fdd\u7559\u548c\u65b0\u6982\u5ff5\u6574\u5408\u6548\u679c\u3002", "motivation": "\u73b0\u6709CDMs\u5728\u6301\u7eed\u5b66\u4e60\u65b0\u6982\u5ff5\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e14\u591a\u6570\u7814\u7a76\u5ffd\u7565\u4e86\u6982\u5ff5\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u3002FL2T\u65e8\u5728\u5b9e\u73b0\u5e76\u53d1\u4e14\u987a\u5e8f\u65e0\u5173\u7684\u6982\u5ff5\u5b66\u4e60\uff0c\u540c\u65f6\u89e3\u51b3\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6FL2T\uff0c\u5305\u542b\u4e00\u4e2a\u96c6\u5408\u4e0d\u53d8\u6027\u7684\u8de8\u6982\u5ff5\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u4ee3\u7406\u6307\u5bfc\u8de8\u6982\u5ff5\u7684\u7279\u5f81\u9009\u62e9\uff0c\u4f18\u5316\u77e5\u8bc6\u4fdd\u7559\u548c\u8f6c\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFL2T\u663e\u8457\u63d0\u9ad8\u4e86\u6982\u5ff5\u4fdd\u7559\u7387\uff0c\u5e73\u5747CLIP\u56fe\u50cf\u5bf9\u9f50\u5206\u6570\u81f3\u5c11\u63d0\u5347\u4e862%\u3002", "conclusion": "FL2T\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8de8\u6982\u5ff5\u5b66\u4e60\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86CDMs\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u4fdd\u7559\u548c\u65b0\u6982\u5ff5\u6574\u5408\u7684\u80fd\u529b\u3002"}}
{"id": "2601.01984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01984", "abs": "https://arxiv.org/abs/2601.01984", "authors": ["Weijian Ma", "Shizhao Sun", "Tianyu Yu", "Ruiyu Wang", "Tat-Seng Chua", "Jiang Bian"], "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation", "comment": "Preprint. Under review", "summary": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u8c61\u84dd\u56fe\u548c\u4e09\u79cd\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u8981\u4e48\u8fc7\u4e8e\u5c40\u90e8\u5316\uff0c\u8981\u4e48\u5ffd\u89c6\u5bf9\u8c61\u95f4\u7684\u6574\u4f53\u7ec4\u7ec7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u84dd\u56fe\u5d4c\u5165\u63a8\u7406\u8f68\u8ff9\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\uff1b2. \u84dd\u56fe\u611f\u77e5\u5956\u52b1\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9f13\u52b1\u56e0\u679c\u5bf9\u9f50\uff1b3. \u6297\u6377\u5f84\u6570\u636e\u589e\u5f3a\u4ee5\u89c4\u907f\u8868\u9762\u7ebf\u7d22\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7a7a\u95f4\u84dd\u56fe\u6982\u5ff5\uff0c\u7ed3\u5408\u4e09\u79cd\u5173\u952e\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.01992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01992", "abs": "https://arxiv.org/abs/2601.01992", "authors": ["Chen Zhu", "Huiwen Zhang", "Yujie Li", "Mu He", "Xiaotian Qiao"], "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning", "comment": null, "summary": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8865\u4e01\u91cd\u8981\u6027\u611f\u77e5\uff08API\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u589e\u5f3a\u548c\u591a\u8d1f\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u96fe\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u96fe\u973e\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u96fe\u5bc6\u5ea6\u5206\u5e03\u7684\u5185\u5728\u590d\u6742\u6027\u3002", "method": "API\u6846\u67b6\u5305\u542b\u81ea\u52a8\u96fe\u751f\u6210\uff08AHG\uff09\u6a21\u5757\u548c\u5bc6\u5ea6\u611f\u77e5\u96fe\u53bb\u9664\uff08DHR\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u8d1f\u5bf9\u6bd4\u53bb\u96fe\uff08MNCD\uff09\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u96fe\u5bc6\u5ea6\u5206\u5e03\u7684\u9002\u5e94\u6027\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u89c6\u89c9\u8d28\u91cf\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u4e0d\u540c\u96fe\u5206\u5e03\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684API\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8865\u4e01\u91cd\u8981\u6027\u611f\u77e5\u548c\u6df7\u5408\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u96fe\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2601.01998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01998", "abs": "https://arxiv.org/abs/2601.01998", "authors": ["Chen Zhu", "Huiwen Zhang", "Mu He", "Yujie Li", "Xiaotian Qiao"], "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors", "comment": null, "summary": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u96fe\u973e\u548c\u4f4e\u5149\u5148\u9a8c\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f4\u96fe\u973e\u56fe\u50cf\u7684\u53ef\u89c1\u5ea6\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u5355\u4e00\u7c7b\u578b\u7684\u9000\u5316\uff08\u5982\u96fe\u973e\u6216\u4f4e\u5149\uff09\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u578b\u9000\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u53ef\u89c1\u5ea6\u63d0\u5347\u6709\u9650\u3002\u672c\u6587\u89c2\u5bdf\u5230\u4f4e\u5149\u548c\u96fe\u973e\u5148\u9a8c\u4e4b\u95f4\u7684\u5171\u4eab\u9886\u57df\u77e5\u8bc6\u53ef\u4ee5\u76f8\u4e92\u5f3a\u5316\u4ee5\u63d0\u5347\u53ef\u89c1\u5ea6\u3002", "method": "\u6a21\u578b\u5229\u7528\u56fe\u50cf\u3001\u5757\u548c\u50cf\u7d20\u7ea7\u522b\u7684\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u548c\u9891\u7387\u57df\u4e2d\u9010\u6b65\u6062\u590d\u5168\u5c40\u573a\u666f\u7ed3\u6784\u3001\u533a\u57df\u6a21\u5f0f\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u611f\u77e5\u8def\u7531\u5668\u81ea\u9002\u5e94\u5f15\u5bfc\u6bcf\u4e2a\u4e13\u5bb6\u7684\u8d21\u732e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591c\u95f4\u53bb\u96fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u767d\u5929\u53bb\u96fe\u548c\u4f4e\u5149\u589e\u5f3a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u578b\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u96fe\u973e\u548c\u4f4e\u5149\u5148\u9a8c\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f4\u96fe\u973e\u56fe\u50cf\u7684\u53ef\u89c1\u5ea6\uff0c\u5e76\u5728\u767d\u5929\u53bb\u96fe\u548c\u4f4e\u5149\u589e\u5f3a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.02018", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02018", "abs": "https://arxiv.org/abs/2601.02018", "authors": ["Guangqian Guo", "Aixi Ren", "Yong Guo", "Xuehui Yu", "Jiacheng Tian", "Wenli Li", "Yaoxing Wang", "Shan Gao"], "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement", "comment": "Diffusion-based latent space enhancement helps improve the robustness of SAM", "summary": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.", "AI": {"tldr": "GleSAM++\u901a\u8fc7\u751f\u6210\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u548c\u9000\u5316\u611f\u77e5\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SAMs\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9SAMs\u5728\u4e25\u91cd\u9000\u5316\u3001\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86GleSAM++\uff0c\u5229\u7528Generative Latent space Enhancement\u589e\u5f3a\u4f4e\u8d28\u91cf\u56fe\u50cf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165Feature Distribution Alignment (FDA)\u548cChannel Replication and Expansion (CRE)\u6280\u672f\u4ee5\u63d0\u5347\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e0e\u5206\u5272\u6846\u67b6\u7684\u517c\u5bb9\u6027\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86DAE\u673a\u5236\uff0c\u5c06\u4efb\u610f\u8d28\u91cf\u7279\u5f81\u7684\u91cd\u6784\u8fc7\u7a0b\u89e3\u8026\u4e3a\u9000\u5316\u7ea7\u522b\u9884\u6d4b\u548c\u9000\u5316\u611f\u77e5\u91cd\u6784\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGleSAM++\u5728\u590d\u6742\u9000\u5316\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u6e05\u6670\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u9000\u5316\u7c7b\u578b\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "GleSAM++\u901a\u8fc7\u5f15\u5165Degradation-aware Adaptive Enhancement (DAE)\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u9000\u5316\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u6e05\u6670\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u9000\u5316\u7c7b\u578b\u4e0a\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2601.02020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02020", "abs": "https://arxiv.org/abs/2601.02020", "authors": ["Shihan Peng", "Yuyang Xiong", "Hanyu Zhou", "Zhiwei Shi", "Haoyue Liu", "Gang Chen", "Luxin Yan", "Yi Chang"], "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.", "AI": {"tldr": "ADAE\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u589e\u5f3aDepth Anything\uff0c\u5728\u6076\u52a3\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u5728\u7406\u60f3\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6781\u7aef\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0cADAE\u65e8\u5728\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6765\u8865\u507f\u8fd9\u4e9b\u9000\u5316\u3002", "method": "ADAE\u6846\u67b6\u91c7\u7528\u71b5\u611f\u77e5\u7a7a\u95f4\u878d\u5408\u548c\u8fd0\u52a8\u5f15\u5bfc\u65f6\u95f4\u6821\u6b63\u4e24\u79cd\u7b56\u7565\uff0c\u5206\u522b\u5904\u7406\u5149\u7167\u5f15\u8d77\u7684\u9000\u5316\u548c\u8fd0\u52a8\u6a21\u7cca\u533a\u57df\u7684\u7279\u5f81\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ADAE\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u5176\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ADAE\u6846\u67b6\u6210\u529f\u589e\u5f3a\u4e86Depth Anything\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u901a\u8fc7\u71b5\u611f\u77e5\u7a7a\u95f4\u878d\u5408\u548c\u8fd0\u52a8\u5f15\u5bfc\u65f6\u95f4\u6821\u6b63\u7684\u4e92\u8865\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u7ee7\u627f\u4e0e\u6269\u5c55\u3002"}}
{"id": "2601.02029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02029", "abs": "https://arxiv.org/abs/2601.02029", "authors": ["Toshihiko Nishimura", "Hirofumi Abe", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding", "comment": "19", "summary": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.", "AI": {"tldr": "\u65e0\u9700\u6807\u6ce83D\u6570\u636e\u6216RGB\u56fe\u50cf\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b03D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\uff0c\u51c6\u786e\u5ea6\u5ab2\u7f8e\u76d1\u7763\u65b9\u6cd5\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e3D\u8bed\u4e49\u5206\u5272\u4e2d\u9700\u8981\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u6216\u914d\u5bf9RGB\u56fe\u50cf\u7684\u9650\u5236\uff0c\u540c\u65f6\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u5c063D\u70b9\u4e91\u6295\u5f71\u52302D\u56fe\u50cf\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5f15\u5bfc\u76842D\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u6295\u7968\u805a\u5408\u591a\u89c6\u89d2\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5206\u5272\u51c6\u786e\u5ea6\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u652f\u6301\u4efb\u610f\u6587\u672c\u67e5\u8be2\u7684\u5bf9\u8c61\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u865a\u62df\u76f8\u673a\u5c063D\u70b9\u4e91\u6295\u5f71\u52302D\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5f15\u5bfc\u76842D\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u65e0\u9700\u6807\u6ce8\u76843D\u8bad\u7ec3\u6570\u636e\u6216\u914d\u5bf9\u7684RGB\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u5ea6\uff0c\u5e76\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u3002"}}
{"id": "2601.02038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02038", "abs": "https://arxiv.org/abs/2601.02038", "authors": ["Yihan Zhu", "Mengying Ge"], "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off", "comment": null, "summary": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.", "AI": {"tldr": "AlignVTOFF\u901a\u8fc7\u5e76\u884cU-Net\u6846\u67b6\u548c\u6df7\u5408\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u8863\u4efb\u52a1\u4e2d\u7eb9\u7406\u8870\u51cf\u548c\u7ed3\u6784\u5931\u771f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u8f7b\u91cf\u7ea7\u6a21\u5757\u5bfc\u81f4\u7eb9\u7406\u8870\u51cf\u548c\u7ed3\u6784\u6a21\u5f0f\u4e22\u5931\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u9891\u7ec6\u8282\u548c\u590d\u6742\u51e0\u4f55\u53d8\u5f62\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faAlignVTOFF\u6846\u67b6\uff0c\u5305\u542bReference U-Net\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u51e0\u4f55\u4fdd\u771f\uff0c\u4ee5\u53caTSFA\u6a21\u5757\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u8bbe\u8ba1\u6ce8\u5165\u53c2\u8003\u670d\u88c5\u7279\u5f81\u3002", "result": "\u5728\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\uff0cAlignVTOFF\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7ed3\u679c\u5177\u6709\u66f4\u9ad8\u7684\u7ed3\u6784\u771f\u5b9e\u6027\u548c\u9ad8\u9891\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "conclusion": "AlignVTOFF\u901a\u8fc7\u7ed3\u5408Reference U-Net\u548cTSFA\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u8863\uff08VTOFF\uff09\u4efb\u52a1\u4e2d\u9ad8\u9891\u7ec6\u8282\u548c\u7ed3\u6784\u771f\u5b9e\u6027\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2601.02088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02088", "abs": "https://arxiv.org/abs/2601.02088", "authors": ["Jiahao Bao", "Huazhen Liu", "Yu Zhuang", "Leran Tao", "Xinyu Xu", "Yongtao Shi", "Mengjia Cheng", "Yiming Wang", "Congshuang Ku", "Ting Zeng", "Yilang Du", "Siyi Chen", "Shunyao Shen", "Suncheng Xiang", "Hongbo Yu"], "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction", "comment": "31 pages, 8 figures", "summary": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.", "AI": {"tldr": "PhysSFI-Net \u662f\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u9884\u6d4b\u6b63\u988c\u624b\u672f\u540e\u7684\u8f6f\u7ec4\u7ec7\u53d8\u5f62\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e34\u5e8a\u6f5c\u529b\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u7269\u529b\u5b66\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u7cbe\u786e\u9884\u6d4b\u8f6f\u7ec4\u7ec7\u53d8\u5f62\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u662f\u5fc5\u8981\u7684\u3002", "method": "PhysSFI-Net \u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5206\u5c42\u56fe\u6a21\u5757\uff08\u7ed3\u5408\u9885\u9762\u548c\u624b\u672f\u8ba1\u5212\u7f16\u7801\u5668\u53ca\u6ce8\u610f\u529b\u673a\u5236\uff09\u3001\u57fa\u4e8e LSTM \u7684\u5e8f\u5217\u9884\u6d4b\u5668\u548c\u751f\u7269\u529b\u5b66\u542f\u53d1\u7684\u6a21\u5757\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u9762\u90e8\u8868\u9762\u91cd\u5efa\u3002", "result": "PhysSFI-Net \u7684\u70b9\u4e91\u5f62\u72b6\u8bef\u5dee\u4e3a 1.070 +/- 0.088 mm\uff0c\u8868\u9762\u504f\u5dee\u8bef\u5dee\u4e3a 1.296 +/- 0.349 mm\uff0c\u6807\u5fd7\u5b9a\u4f4d\u8bef\u5dee\u4e3a 2.445 +/- 1.326 mm\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5 ACMT-Net\u3002", "conclusion": "PhysSFI-Net \u80fd\u591f\u4ee5\u66f4\u9ad8\u7684\u51c6\u786e\u5ea6\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u672f\u540e\u9762\u90e8\u5f62\u6001\u9884\u6d4b\uff0c\u663e\u793a\u51fa\u5728\u6b63\u988c\u624b\u672f\u89c4\u5212\u548c\u6a21\u62df\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.02091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02091", "abs": "https://arxiv.org/abs/2601.02091", "authors": ["Zhehuan Cao", "Fiseha Berhanu Tesema", "Ping Fu", "Jianfeng Ren", "Ahmed Nasr"], "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation", "comment": "13 pages, 10 figures. This manuscript is under review at IEEE Transactions on Geoscience and Remote Sensing", "summary": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u4ec5\u57fa\u4e8e\u5149\u5b66\u7684\u51b0\u789b\u5206\u5272\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578bMCD-Net\uff0c\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u6df1\u5c42\u9aa8\u5e72\u7f51\u7edc\u3002", "motivation": "\u51b0\u789b\u5206\u5272\u5bf9\u4e8e\u91cd\u5efa\u8fc7\u53bb\u51b0\u5ddd\u52a8\u6001\u548c\u8bc4\u4f30\u6c14\u5019\u9a71\u52a8\u7684\u666f\u89c2\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f31\u5149\u5b66\u5bf9\u6bd4\u548c\u9ad8\u5206\u8fa8\u7387DEM\u7684\u6709\u9650\u53ef\u7528\u6027\u963b\u788d\u4e86\u81ea\u52a8\u5316\u5236\u56fe\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86MCD-Net\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u7ebf\u6a21\u578b\uff0c\u96c6\u6210\u4e86MobileNetV2\u7f16\u7801\u5668\u3001CBAM\u6a21\u5757\u548cDeepLabV3+\u89e3\u7801\u5668\u3002", "result": "MCD-Net\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523062.3%\u7684mIoU\u548c72.8%\u7684Dice\u7cfb\u6570\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8660%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4ec5\u4f7f\u7528\u5149\u5b66\u56fe\u50cf\u5373\u53ef\u63d0\u4f9b\u53ef\u9760\u7684\u51b0\u789b\u4f53\u5206\u5272\uff0c\u4e3a\u9ad8\u6d77\u62d4\u51b0\u5ddd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u57fa\u7ebf\u3002"}}
{"id": "2601.02098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02098", "abs": "https://arxiv.org/abs/2601.02098", "authors": ["Jinlong Fan", "Shanshan Zhao", "Liang Zheng", "Jing Zhang", "Yuxiang Yang", "Mingming Gong"], "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting", "comment": null, "summary": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.", "AI": {"tldr": "InpaintHuman\u901a\u8fc7\u591a\u5c3a\u5ea6UV\u8868\u793a\u548c\u6269\u6563\u4fee\u590d\u6a21\u5757\uff0c\u4ece\u906e\u6321\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u5316\u8eab\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u96be\u4ee5\u751f\u6210\u5b8c\u6574\u76843D\u4eba\u4f53\u5316\u8eab\uff0c\u5bfc\u81f4\u51e0\u4f55\u635f\u574f\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002InpaintHuman\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u5c3a\u5ea6UV\u53c2\u6570\u5316\u8868\u793a\u548c\u5c42\u6b21\u5316\u7684\u7c97\u5230\u7ec6\u7279\u5f81\u63d2\u503c\uff0c\u7ed3\u5408\u8eab\u4efd\u4fdd\u6301\u6269\u6563\u4fee\u590d\u6a21\u5757\uff0c\u901a\u8fc7\u76f4\u63a5\u50cf\u7d20\u7ea7\u76d1\u7763\u786e\u4fdd\u8eab\u4efd\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\uff08PeopleSnapshot\u3001ZJU-MoCap\uff09\u548c\u771f\u5b9e\u573a\u666f\uff08OcMotion\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u59ff\u6001\u548c\u89c6\u89d2\u4e0b\u5747\u80fd\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "InpaintHuman\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6UV\u53c2\u6570\u5316\u8868\u793a\u548c\u8eab\u4efd\u4fdd\u6301\u6269\u6563\u4fee\u590d\u6a21\u5757\uff0c\u6210\u529f\u4ece\u906e\u6321\u7684\u5355\u76ee\u89c6\u9891\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5b8c\u6574\u4e14\u53ef\u52a8\u753b\u76843D\u4eba\u4f53\u5316\u8eab\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u5177\u6709\u7ade\u4e89\u4f18\u52bf\u3002"}}
{"id": "2601.02102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02102", "abs": "https://arxiv.org/abs/2601.02102", "authors": ["Jiaqi Yao", "Zhongmiao Yan", "Jingyi Xu", "Songpengcheng Xia", "Yan Xiang", "Ling Pei"], "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images", "comment": null, "summary": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u524d\u9988\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u51e0\u4f55\u6b63\u5219\u5316\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u76843D\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u5728\u7a00\u758f\u89c6\u89d2\u6216\u4f4e\u7eb9\u7406\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u867d\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u65f6\u6548\u7387\u4e14\u9700\u8981\u9010\u573a\u666f\u4f18\u5316\u3002\u663e\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u867d\u80fd\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\uff0c\u4f46\u591a\u6570\u524d\u9988\u53d8\u4f53\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u800c\u975e\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u6df1\u5ea6-\u6cd5\u7ebf\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u5c06\u6e32\u67d3\u6df1\u5ea6\u68af\u5ea6\u4e0e\u6cd5\u7ebf\u4fe1\u606f\u8026\u5408\uff0c\u76d1\u7763\u9ad8\u65af\u7684\u65cb\u8f6c\u3001\u5c3a\u5ea6\u548c\u4f4d\u7f6e\uff0c\u4ee5\u63d0\u9ad8\u70b9\u4e91\u548c\u8868\u9762\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u578b\u524d\u9988\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e3a\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02103", "abs": "https://arxiv.org/abs/2601.02103", "authors": ["Yating Wang", "Yuan Sun", "Xuan Wang", "Ran Yi", "Boyao Zhou", "Yipengjing Sun", "Hongyu Liu", "Yinuo Wang", "Lizhuang Ma"], "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "comment": null, "summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "AI": {"tldr": "HeadLighter \u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u6e10\u8fdb\u5f0f\u89e3\u7f20\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5934\u90e8\u751f\u6210\u6a21\u578b\u4e2d\u5916\u89c2\u4e0e\u7167\u660e\u7684\u7269\u7406\u5408\u7406\u5206\u89e3\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u548c\u663e\u5f0f\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5149\u7167\u4e0e\u5185\u5728\u5916\u89c2\u7684\u6df1\u5ea6\u7ea0\u7f20\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4f9d\u8d56\u5f3a\u5047\u8bbe\u8fdb\u884c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u5149\u7167\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u5efa\u6a21\u5149\u7167\u4e0d\u53d8\u7684\u5934\u90e8\u4f4d\u5c5e\u6027\u548c\u7269\u7406\u57fa\u7840\u7684\u6e32\u67d3\u7ec4\u4ef6\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u89e3\u7f20\u8bad\u7ec3\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u652f\u6301\u663e\u5f0f\u5149\u7167\u548c\u89c6\u89d2\u7f16\u8f91\u3002", "conclusion": "HeadLighter \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d1\u7763\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5934\u90e8\u751f\u6210\u6a21\u578b\u4e2d\u5916\u89c2\u4e0e\u7167\u660e\u7684\u7269\u7406\u5408\u7406\u5206\u89e3\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\u548c\u663e\u5f0f\u5149\u7167\u4e0e\u89c6\u89d2\u7f16\u8f91\u3002"}}
{"id": "2601.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02107", "abs": "https://arxiv.org/abs/2601.02107", "authors": ["Jiancheng Huang", "Mingfu Yan", "Songyan Chen", "Yi Huang", "Shifeng Chen"], "title": "MagicFight: Personalized Martial Arts Combat Video Generation", "comment": "Accepted by ACM MM 2024", "summary": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta", "AI": {"tldr": "MagicFight\u662f\u4e00\u4e2a\u9488\u5bf9\u53cc\u4eba\u6b66\u672f\u6218\u6597\u89c6\u9891\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u4eba\u751f\u6210\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7Unity\u751f\u6210\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u53cc\u4eba\u4e92\u52a8\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u5355\u4eba\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u53cc\u4eba\u6b66\u672f\u6218\u6597\u7684\u590d\u6742\u6027\u548c\u7ec6\u8282\uff0c\u5bfc\u81f4\u8eab\u4efd\u6df7\u6dc6\u3001\u80a2\u4f53\u5f02\u5e38\u548c\u52a8\u4f5c\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002", "method": "MagicFight\u901a\u8fc7\u6539\u8fdb\u548c\u8c03\u6574\u73b0\u6709\u6a21\u578b\u53ca\u7b56\u7565\uff0c\u5229\u7528Unity\u6e38\u620f\u7269\u7406\u5f15\u64ce\u751f\u6210\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u76843D\u89d2\u8272\u3001\u6b66\u672f\u52a8\u4f5c\u548c\u573a\u666f\u3002", "result": "MagicFight\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u53cc\u4eba\u6218\u6597\u89c6\u9891\uff0c\u4fdd\u6301\u4e2a\u4f53\u8eab\u4efd\u5e76\u786e\u4fdd\u52a8\u4f5c\u5e8f\u5217\u7684\u8fde\u8d2f\u6027\u3002", "conclusion": "MagicFight\u4e3a\u53cc\u4eba\u4e92\u52a8\u89c6\u9891\u751f\u6210\uff08\u5c24\u5176\u662f\u6b66\u672f\u6218\u6597\uff09\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.02112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02112", "abs": "https://arxiv.org/abs/2601.02112", "authors": ["Utkarsh Singh", "Absaar Ali", "Adarsh Roy"], "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model", "comment": "14 pages, 5 figures. Published in: Bramer M., Stahl F. (eds) Artificial Intelligence XLII. SGAI 2025. Lecture Notes in Computer Science, vol 16302. Springer, Cham", "summary": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u5207\u7247\u5904\u7406\u548c\u53cc\u5411LSTM\u9884\u6d4b\u8f66\u8f86Cd\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u7cbe\u5ea6\u6c14\u52a8\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u7684\u6c14\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982CFD\u548c\u98ce\u6d1e\u6d4b\u8bd5\uff09\u8d44\u6e90\u5bc6\u96c6\uff0c\u963b\u788d\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u5feb\u901f\u8fed\u4ee3\u3002\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u6216\u5bf9\u8be6\u7ec6\u51e0\u4f55\u8f93\u5165\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6a21\u578b\u5c063D\u8f66\u8f86\u70b9\u4e91\u6cbf\u6d41\u5411\u8f74\u5206\u89e3\u4e3a\u6709\u5e8f\u76842D\u6a2a\u622a\u9762\u5207\u7247\u5e8f\u5217\uff0c\u6bcf\u4e2a\u5207\u7247\u901a\u8fc7\u8f7b\u91cf\u7ea7PointNet2D\u6a21\u5757\u7f16\u7801\uff0c\u5207\u7247\u5d4c\u5165\u5e8f\u5217\u7531\u53cc\u5411LSTM\u5904\u7406\u4ee5\u6355\u6349\u7eb5\u5411\u51e0\u4f55\u6f14\u53d8\u3002", "result": "\u5728DrivAerNet++\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u51b3\u5b9a\u7cfb\u6570\uff08R^2 > 0.9528\uff09\u548c\u4f4e\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE \u2248 6.046 x 10^{-3}\uff09\uff0c\u63a8\u7406\u65f6\u95f4\u7ea6\u4e3a0.025\u79d2/\u6837\u672c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u5207\u7247\u5904\u7406\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b3D\u8f66\u8f86\u7684\u6c14\u52a8\u963b\u529b\u7cfb\u6570\uff08Cd\uff09\uff0c\u8be5\u65b9\u6cd5\u5728DrivAerNet++\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8bef\u5dee\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u4e3a\u6c7d\u8f66\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u6c14\u52a8\u53cd\u9988\u3002"}}
{"id": "2601.02139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02139", "abs": "https://arxiv.org/abs/2601.02139", "authors": ["Chenyang Lai", "Shuaiyu Chen", "Tianjin Huang", "Siyang Song", "Guangliang Cheng", "Chunbo Luo", "Zeyu Fu"], "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery", "comment": null, "summary": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.", "AI": {"tldr": "\u63d0\u51faOSCD\u4efb\u52a1\u548cTAHI\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u65f6\u5e8fSAR\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u63d0\u5347\u6cb9\u6c61\u76d1\u6d4b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8bef\u62a5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5355\u5f20SAR\u56fe\u50cf\u7684\u9759\u6001\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u771f\u5b9e\u6cb9\u6c61\u4e0e\u89c6\u89c9\u76f8\u4f3c\u7684\u6d77\u9762\u7279\u5f81\uff0c\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\u548c\u6709\u9650\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Oil Spill Change Detection (OSCD)\u4efb\u52a1\u548cTemporal-Aware Hybrid Inpainting (TAHI)\u6846\u67b6\uff0c\u5305\u62ec\u9ad8\u4fdd\u771f\u6df7\u5408\u4fee\u590d\u548c\u65f6\u5e8f\u771f\u5b9e\u6027\u589e\u5f3a\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "result": "OSCD\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u62a5\u5e76\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "OSCD\u7ed3\u5408TAHI\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\uff0c\u63d0\u9ad8\u4e86\u6cb9\u6c61\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u611f\u77e5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u6cb9\u6c61\u76d1\u6d4b\u4ef7\u503c\u3002"}}
{"id": "2601.02141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02141", "abs": "https://arxiv.org/abs/2601.02141", "authors": ["Romain Vo", "Juli\u00e1n Tachella"], "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems", "comment": null, "summary": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5927\u89c4\u6a21\u6210\u50cf\u95ee\u9898\u4e2d\u524d\u5411\u7b97\u5b50\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u5206\u5272\u548c\u7b97\u5b50\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u76843D\u91cd\u5efa\uff0c\u4e14\u4ec5\u9700\u5355\u4e2aGPU\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u6210\u50cf\u7b49\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\uff0c\u7531\u4e8e\u5168\u5c40\u524d\u5411\u7b97\u5b50\u5185\u5b58\u6d88\u8017\u8fc7\u5927\uff0c\u65e0\u6cd5\u5c06\u7b97\u5b50\u6574\u5408\u5230\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u91c7\u7528\u9886\u57df\u5206\u5272\u7b56\u7565\u548c\u6b63\u5e38\u7b97\u5b50\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6210\u50cf\u95ee\u9898\u4e2d\u524d\u5411\u7b97\u5b50\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u7684\u95ee\u9898\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u57283D X\u5c04\u7ebf\u9525\u675f\u65ad\u5c42\u626b\u63cf\u548c3D\u591a\u7ebf\u5708\u52a0\u901fMRI\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u5355\u4e2aGPU\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9886\u57df\u5206\u5272\u7b56\u7565\u548c\u6b63\u5e38\u7b97\u5b50\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u5927\u89c4\u6a21\u95ee\u9898\u7684\u524d\u5411\u7b97\u5b50\u6574\u5408\u5230\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u5b9e\u73b0\u4e86\u57283D X\u5c04\u7ebf\u9525\u675f\u65ad\u5c42\u626b\u63cf\u548c3D\u591a\u7ebf\u5708\u52a0\u901fMRI\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u5355\u4e2aGPU\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\u3002"}}
{"id": "2601.02177", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02177", "abs": "https://arxiv.org/abs/2601.02177", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson"], "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32", "comment": null, "summary": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $\u03c3$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5546\u54c1\u7ea7ESP32\u4f20\u611f\u5668\u4fe1\u53f7\u8d28\u91cf\u4e0d\u8db3\uff0c\u5bfc\u81f4\u591a\u4eba\u6b65\u6001\u8bc6\u522b\u51c6\u786e\u7387\u4f4e\uff08\u6700\u4f73\u65b9\u6cd5NMF\u4ec556%\uff09\uff0c\u6027\u80fd\u968f\u4eba\u6570\u589e\u52a0\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22\u591a\u4eba\u6b65\u6001\u8bc6\u522b\u7684\u6027\u80fd\u9650\u5236\u662f\u7b97\u6cd5\u95ee\u9898\u8fd8\u662f\u786c\u4ef6\u9650\u5236\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u4f9d\u8d56\u590d\u6742\u6602\u8d35\u8bbe\u5907\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\uff08FastICA\u3001SOBI\u3001PCA\u3001NMF\u3001\u5c0f\u6ce2\u53d8\u6362\u3001\u5f20\u91cf\u5206\u89e3\uff09\uff0c\u57281-10\u4eba\u573a\u666f\u4e0b\u4f7f\u7528ESP32\u4f20\u611f\u5668\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u8bca\u65ad\u6307\u6807\uff08\u4e3b\u4f53\u5185\u53d8\u5f02\u6027\u3001\u4e3b\u4f53\u95f4\u53ef\u533a\u5206\u6027\u3001\u6027\u80fd\u9000\u5316\u7387\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5728\u591a\u4eba\u5458\u573a\u666f\u4e0b\u51c6\u786e\u7387\u76f8\u4f3c\u4e14\u8f83\u4f4e\uff0845-56%\uff0c\u03c3=3.74%\uff09\uff0c\u6027\u80fd\u968f\u4eba\u6570\u589e\u52a0\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u786c\u4ef6\u4fe1\u53f7\u8d28\u91cf\u4e0d\u8db3\u3002", "conclusion": "\u5546\u54c1\u7ea7ESP32 WiFi\u4f20\u611f\u5668\u5728\u591a\u4eba\u5458\u6b65\u6001\u8bc6\u522b\u4e2d\u4fe1\u53f7\u8d28\u91cf\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u4eba\u5206\u79bb\u3002"}}
{"id": "2601.02189", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02189", "abs": "https://arxiv.org/abs/2601.02189", "authors": ["Cheng Ying Wu", "Yen Jui Chang"], "title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition", "comment": null, "summary": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.", "AI": {"tldr": "QuIC\u662f\u4e00\u79cd\u91cf\u5b50\u542f\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u6355\u6349\u4e8c\u9636\u7279\u5f81\u534f\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u6d45\u5c42\u7f51\u7edc\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u907f\u514d\u7279\u5f81\u7ef4\u5ea6\u7206\u70b8\u3002", "motivation": "\u89e3\u51b3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6df1\u5ea6\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u6d45\u5c42\u7f51\u7edc\u65e0\u6cd5\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faQuantum-inspired Interaction Classifier (QuIC)\uff0c\u901a\u8fc7\u91cf\u5b50\u529b\u5b66\u542f\u53d1\u7684\u53ef\u5b66\u4e60\u89c2\u6d4b\u7b97\u5b50\u6355\u6349\u4e8c\u9636\u7279\u5f81\u534f\u65b9\u5dee\uff0c\u652f\u6301\u7a33\u5b9a\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "QuIC\u663e\u8457\u63d0\u5347\u4e86\u6d45\u5c42\u7f51\u7edc\uff08\u5982VGG16\uff09\u7684\u6027\u80fd\uff0cTop-1\u51c6\u786e\u7387\u63d0\u5347\u8fd120%\uff0c\u5e76\u5728ResNet18\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\uff08SE-Block\uff09\u3002", "conclusion": "QuIC\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d45\u5c42\u7f51\u7edc\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7279\u5f81\u7ef4\u5ea6\u7206\u70b8\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u95ee\u9898\u3002"}}
{"id": "2601.02198", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02198", "abs": "https://arxiv.org/abs/2601.02198", "authors": ["Alexander M\u00f6llers", "Julius Hense", "Florian Schulz", "Timo Milbich", "Maximilian Alber", "Lukas Ruff"], "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models", "comment": null, "summary": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.", "AI": {"tldr": "\u7814\u7a76\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u653e\u5927\u500d\u6570\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u8fde\u7eed\u91c7\u6837\u548c\u4f18\u5316\u5206\u5e03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e2d\u95f4\u653e\u5927\u500d\u6570\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u7406\u89e3\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u653e\u5927\u500d\u6570\u4e0b\u7684\u6027\u80fd\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u653e\u5927\u500d\u6570\u91c7\u6837\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06\u653e\u5927\u500d\u6570\u91c7\u6837\u5efa\u6a21\u4e3a\u591a\u6e90\u57df\u9002\u5e94\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u91c7\u6837\u7b56\u7565\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u6743\u8861\u3002\u5f15\u5165\u4e86\u8fde\u7eed\u653e\u5927\u500d\u6570\u91c7\u6837\u548c\u4f18\u5316\u7684\u91c7\u6837\u5206\u5e03\u3002", "result": "\u8fde\u7eed\u91c7\u6837\u5728\u4e2d\u95f4\u653e\u5927\u500d\u6570\u4e0b\u663e\u8457\u4f18\u4e8e\u79bb\u6563\u91c7\u6837\uff0c\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe4\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u5316\u5206\u5e03\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u653e\u5927\u500d\u6570\u4e0b\u53ef\u9760\u6027\u80fd\u7684\u5b9e\u73b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.02203", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02203", "abs": "https://arxiv.org/abs/2601.02203", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson", "Quan Z. Sheng"], "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules", "comment": null, "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCSI-ResNet-A\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548cAdapter\u5fae\u8c03\u89e3\u51b3WiFi\u4eba\u7fa4\u8ba1\u6570\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3WiFi\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u5728\u8bbe\u5907\u65e0\u5173\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u9690\u79c1\u4fdd\u62a4\u7684\u7269\u8054\u7f51\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eCSI-ResNet-A\u67b6\u6784\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u4ee5\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7Adapter\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5728WiFlow\u6570\u636e\u96c6\u768410-shot\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u5b9e\u73b0\u4e860.44\u7684MAE\uff0c\u540c\u65f6\u5728WiAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.8%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u90e8\u7f72\u7684\u9c81\u68d2\u611f\u77e5\u7cfb\u7edf\u3002"}}
{"id": "2601.02211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02211", "abs": "https://arxiv.org/abs/2601.02211", "authors": ["Binglei Li", "Mengping Yang", "Zhiyu Tan", "Junping Zhang", "Hao Li"], "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion", "comment": "11 pages", "summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86MMDiT\u6a21\u578b\u7684\u6a21\u5757\u529f\u80fd\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5bf9\u9f50\u548c\u7f16\u8f91\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u5206\u6790MMDiT\u6a21\u578b\u7684\u7279\u5b9a\u7ec4\u4ef6\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u5c42\uff09\uff0c\u4f46\u5bf9\u4e0d\u540c\u6a21\u5757\u53ca\u5176\u4e0e\u6587\u672c\u6761\u4ef6\u4ea4\u4e92\u4f5c\u7528\u7684\u5168\u9762\u7406\u89e3\u4ecd\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5206\u6790\u6d41\u7a0b\uff0c\u901a\u8fc7\u79fb\u9664\u3001\u7981\u7528\u548c\u589e\u5f3a\u6587\u672c\u9690\u85cf\u72b6\u6001\u6765\u7814\u7a76\u6bcf\u4e2a\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51fa\u4e86\u8bad\u7ec3\u81ea\u7531\u7684\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u63a8\u7406\u52a0\u901f\u65b9\u9762\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86T2I-Combench++\u548cGenEval\u7684\u8bc4\u5206\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790MMDiT\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5bf9\u9f50\u3001\u7cbe\u786e\u7f16\u8f91\u548c\u63a8\u7406\u52a0\u901f\u7684\u6027\u80fd\uff0c\u4e3aMMDiT\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.02212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02212", "abs": "https://arxiv.org/abs/2601.02212", "authors": ["Jingjing Wang", "Zhuo Xiao", "Xinning Yao", "Bo Liu", "Lijuan Niu", "Xiangzhi Bai", "Fugen Zhou"], "title": "Prior-Guided DETR for Ultrasound Nodule Detection", "comment": null, "summary": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.", "AI": {"tldr": "\u5148\u9a8c\u5f15\u5bfcDETR\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u878d\u5165\u51e0\u4f55\u548c\u7ed3\u6784\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u8d85\u58f0\u7ed3\u8282\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u590d\u6742\u7ed3\u8282\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u8d85\u58f0\u7ed3\u8282\u68c0\u6d4b\u56e0\u5f62\u72b6\u4e0d\u89c4\u5219\u3001\u8fb9\u754c\u6a21\u7cca\u3001\u5c3a\u5ea6\u53d8\u5316\u5927\u53ca\u6591\u70b9\u566a\u58f0\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86SDFPR\u3001MSFFM\u548cDFI\u673a\u5236\uff0c\u9010\u6b65\u878d\u5165\u51e0\u4f55\u548c\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u67e5\u8be2\u7ec6\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u7532\u72b6\u817a\u8d85\u58f0\u6570\u636e\u96c6\uff08Thyroid I\u548cII\uff09\u53ca\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\uff08TN3K\u548cBUSI\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5148\u9a8c\u5f15\u5bfcDETR\u6846\u67b6\u5728\u8d85\u58f0\u7ed3\u8282\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5f62\u6001\u590d\u6742\u7ed3\u8282\u4e0a\u4f18\u4e8e\u73b0\u670918\u79cd\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2601.02228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02228", "abs": "https://arxiv.org/abs/2601.02228", "authors": ["Duoxun Tang", "Xueyi Zhang", "Chak Hin Wang", "Xi Xiao", "Dasen Dai", "Xinhang Jiang", "Wentao Shi", "Rui Li", "Qing Li"], "title": "FMVP: Masked Flow Matching for Adversarial Video Purification", "comment": null, "summary": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.", "AI": {"tldr": "FMVP\u901a\u8fc7\u63a9\u7801\u548cCFM\u7269\u7406\u7834\u574f\u5bf9\u6297\u7ed3\u6784\uff0c\u7ed3\u5408FGL\u5206\u79bb\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u51c0\u5316\u65b9\u6cd5\u91c7\u6837\u6548\u7387\u4f4e\u4e14\u8f68\u8ff9\u5f2f\u66f2\uff0c\u76f4\u63a5\u56de\u5f52\u5e72\u51c0\u89c6\u9891\u56e0\u6270\u52a8\u7ec6\u5fae\u800c\u96be\u4ee5\u6062\u590d\u5fe0\u5b9e\u5185\u5bb9\uff0c\u9700\u7269\u7406\u7834\u574f\u5bf9\u6297\u7ed3\u6784\u3002", "method": "FMVP\u91c7\u7528\u63a9\u7801\u7b56\u7565\u7834\u574f\u5bf9\u6297\u7ed3\u6784\uff0c\u5229\u7528\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u548c\u4fee\u590d\u76ee\u6807\u91cd\u5efa\u89c6\u9891\u52a8\u6001\uff0c\u5e76\u8bbe\u8ba1\u9891\u7387\u95e8\u63a7\u635f\u5931\uff08FGL\uff09\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u5bf9\u6297\u566a\u58f0\u3002\u653b\u51fb\u611f\u77e5\u548c\u901a\u7528\u8bad\u7ec3\u8303\u5f0f\u5206\u522b\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u5a01\u80c1\u3002", "result": "\u5728UCF-101\u548cHMDB-51\u4e0a\uff0cFMVP\u5bf9PGD\u548cCW\u653b\u51fb\u7684\u9c81\u68d2\u51c6\u786e\u7387\u5206\u522b\u8d85\u8fc787%\u548c89%\uff0c\u5bf9\u6297\u81ea\u9002\u5e94\u653b\u51fb\uff08DiffHammer\uff09\u8868\u73b0\u4f18\u5f02\uff0c\u96f6\u6837\u672c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe98%\uff08PGD\uff09\u548c79%\uff08CW\uff09\u3002", "conclusion": "FMVP\u901a\u8fc7\u7269\u7406\u7834\u574f\u5168\u5c40\u5bf9\u6297\u7ed3\u6784\u5e76\u91cd\u5efa\u5e72\u51c0\u89c6\u9891\u52a8\u6001\uff0c\u7ed3\u5408\u9891\u7387\u95e8\u63a7\u635f\u5931\u548c\u653b\u51fb\u611f\u77e5\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u5bf9\u6297\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.02249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02249", "abs": "https://arxiv.org/abs/2601.02249", "authors": ["Xiantai Xiang", "Guangyao Zhou", "Zixiao Wen", "Wenshuai Li", "Ben Niu", "Feng Wang", "Lijia Huang", "Qiantong Wang", "Yuhan Liu", "Zongxu Pan", "Yuxin Hu"], "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection", "comment": null, "summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.", "AI": {"tldr": "SLGNet\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u548c\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\uff0c\u9ad8\u6548\u63d0\u5347\u591a\u6a21\u6001\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u73af\u5883\u52a8\u6001\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5728\u9ad8\u5bf9\u6bd4\u5ea6\u6216\u591c\u95f4\u73af\u5883\u7b49\u590d\u6742\u573a\u666f\u4e0b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86SLGNet\u6846\u67b6\uff0c\u5305\u62ec\u7ed3\u6784\u611f\u77e5\u9002\u914d\u5668\uff08Structure-Aware Adapter\uff09\u548c\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\u6a21\u5757\uff08Language-Guided Modulation\uff09\uff0c\u5229\u7528\u51bb\u7ed3\u7684ViT\u57fa\u7840\u6a21\u578b\u52a8\u6001\u6ce8\u5165\u5c42\u6b21\u7ed3\u6784\u8868\u793a\u548c\u8bed\u8a00\u5f15\u5bfc\u7684\u73af\u5883\u611f\u77e5\u3002", "result": "\u5728LLVIP\u3001FLIR\u3001KAIST\u548cDroneVehicle\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0cLLVIP\u4e0amAP\u8fbe66.1\uff0c\u540c\u65f6\u51cf\u5c11\u7ea687%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "SLGNet\u901a\u8fc7\u7ed3\u5408\u5c42\u6b21\u7ed3\u6784\u5148\u9a8c\u548c\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.02256", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02256", "abs": "https://arxiv.org/abs/2601.02256", "authors": ["Shikun Sun", "Liao Qu", "Huichao Zhang", "Yiheng Liu", "Yangyang Song", "Xian Li", "Xu Wang", "Yi Jiang", "Daniel K. Du", "Xinglong Wu", "Jia Jia"], "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6846\u67b6\u7ba1\u7406VAR\u6a21\u578b\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\uff0c\u6574\u5408\u4e09\u79cd\u7ec4\u4ef6\u63d0\u5347\u6837\u672c\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3VAR\u6a21\u578b\u5728\u5f02\u6784\u8f93\u5165\u7ed3\u6784\u4e2d\u4ea7\u751f\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5bf9\u9f50\u4e0d\u4f73\u3002", "method": "\u6574\u5408\u4e86\u4e09\u79cd\u534f\u540c\u7ec4\u4ef6\uff1a\u7a33\u5b9a\u4e2d\u95f4\u5956\u52b1\u3001\u52a8\u6001\u65f6\u95f4\u6b65\u91cd\u52a0\u6743\u65b9\u6848\u548c\u57fa\u4e8e\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u7684\u63a9\u7801\u4f20\u64ad\u7b97\u6cd5\u3002", "result": "\u5728\u6837\u672c\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u9f50\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7840GRPO\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u901a\u8fc7\u7ba1\u7406\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86VAR\u6a21\u578b\u7684\u6837\u672c\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u6709\u6548\u7684\u4f18\u5316\u3002"}}
{"id": "2601.02267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02267", "abs": "https://arxiv.org/abs/2601.02267", "authors": ["Renke Wang", "Zhenyu Zhang", "Ying Tai", "Jian Yang"], "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies", "comment": "Page: https://wrk226.github.io/DiffProxy.html, Code: https://github.com/wrk226/DiffProxy", "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html", "AI": {"tldr": "DiffProxy\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5148\u9a8c\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u4eba\u4f53\u4ee3\u7406\u751f\u6210\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u6807\u6ce8\u4e0d\u5b8c\u7f8e\u5bfc\u81f4\u8bad\u7ec3\u504f\u5dee\uff0c\u4ee5\u53ca\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u9886\u57df\u5dee\u8ddd\u7684\u95ee\u9898\u3002", "method": "DiffProxy\u6846\u67b6\u5305\u62ec\u591a\u6761\u4ef6\u673a\u5236\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u4eba\u4f53\u4ee3\u7406\u3001\u624b\u90e8\u7ec6\u5316\u6a21\u5757\u589e\u5f3a\u5c40\u90e8\u7ec6\u8282\uff0c\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u63d0\u5347\u4f18\u5316\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u906e\u6321\u548c\u90e8\u5206\u89c6\u89d2\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffProxy\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u548c\u5408\u6210\u6570\u636e\u7684\u7cbe\u786e\u76d1\u7763\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5c24\u5176\u5728\u906e\u6321\u548c\u90e8\u5206\u89c6\u89d2\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.02281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02281", "abs": "https://arxiv.org/abs/2601.02281", "authors": ["Shuai Yuan", "Yantai Yang", "Xiaotian Yang", "Xupeng Zhang", "Zhonghao Zhao", "Lingming Zhang", "Zhipeng Zhang"], "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "comment": null, "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "AI": {"tldr": "InfiniteVGGT\u901a\u8fc7\u6eda\u52a8\u5185\u5b58\u548c\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u4e863D\u51e0\u4f55\u7406\u89e3\u7684\u957f\u671f\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63a8\u51faLong3D\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u6a21\u578b\u65e0\u6cd5\u5b9e\u65f6\u5904\u7406\uff0c\u800c\u6d41\u5f0f\u67b6\u6784\u5728\u957f\u671f\u7a33\u5b9a\u6027\u6216\u65e0\u9650\u8f93\u5165\u652f\u6301\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u53ef\u6269\u5c55\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86InfiniteVGGT\uff0c\u4e00\u79cd\u56e0\u679c\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u91c7\u7528\u6709\u754c\u4f46\u81ea\u9002\u5e94\u7684KV\u7f13\u5b58\u5b9e\u73b0\u6eda\u52a8\u5185\u5b58\uff0c\u5e76\u7ed3\u5408\u8bad\u7ec3\u65e0\u5173\u7684\u6ce8\u610f\u529b\u65e0\u5173\u526a\u679d\u7b56\u7565\u3002", "result": "InfiniteVGGT\u5728\u65e0\u9650\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6d41\u5f0f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Long3D\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u957f\u671f\u7a33\u5b9a\u6027\u3002", "conclusion": "InfiniteVGGT \u901a\u8fc7\u521b\u65b0\u7684\u6eda\u52a8\u5185\u5b58\u673a\u5236\u548c\u8bad\u7ec3\u65e0\u5173\u7684\u526a\u679d\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u671f3D\u51e0\u4f55\u7406\u89e3\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Long3D\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u65e0\u9650\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.02289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02289", "abs": "https://arxiv.org/abs/2601.02289", "authors": ["Tom Burgert", "Leonard Hackel", "Paolo Rota", "Beg\u00fcm Demir"], "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery", "comment": "accepted for publication at IEEE/CVF Winter Conference on Applications of Computer Vision", "summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.", "AI": {"tldr": "GeoRank\u901a\u8fc7\u4f18\u5316\u7403\u9762\u8ddd\u79bb\u5d4c\u5165\u5730\u7406\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u4e2d\u9762\u4e34\u5730\u7406\u548c\u65f6\u95f4\u53d8\u5316\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51faGeoRank\u65b9\u6cd5\uff0c\u4f18\u5316\u7403\u9762\u8ddd\u79bb\u4ee5\u5d4c\u5165\u5730\u7406\u5173\u7cfb\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u9002\u5e94\u6027\u3002", "result": "GeoRank\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u6574\u5408\u5730\u7406\u5143\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u6301\u7eed\u6539\u8fdb\u591a\u79cd\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff08\u5982BYOL\u3001DINO\uff09\u3002", "conclusion": "GeoRank\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u7403\u9762\u8ddd\u79bb\u5c06\u5730\u7406\u5173\u7cfb\u5d4c\u5165\u5b66\u4e60\u7279\u5f81\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u7684\u5904\u7406\u6548\u679c\u3002"}}
{"id": "2601.02299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02299", "abs": "https://arxiv.org/abs/2601.02299", "authors": ["Sara In\u00e1cio", "Hugo Proen\u00e7a", "Jo\u00e3o C. Neves"], "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting", "comment": "9 pages", "summary": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.", "AI": {"tldr": "\u63d0\u51fa\u4e86SortWaste\u6570\u636e\u96c6\u548cClutterScore\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5e9f\u7269\u5206\u62e3\u7684\u89c6\u89c9\u590d\u6742\u6027\uff0c\u5e76\u5728\u5851\u6599\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f9759.7%\u7684mAP\uff0c\u4f46\u9ad8\u6742\u4e71\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u5e9f\u7269\u7ba1\u7406\u4e2d\u7684\u81ea\u52a8\u5206\u62e3\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u89c6\u89c9\u590d\u6742\u6027\u800c\u53d1\u5c55\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86SortWaste\u6570\u636e\u96c6\u548cClutterScore\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u573a\u666f\u7684\u89c6\u89c9\u590d\u6742\u6027\u3002", "result": "\u5728\u5851\u6599\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523059.7%\u7684mAP\uff0c\u4f46\u5728\u9ad8\u6742\u4e71\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u5851\u6599\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8659.7%\u7684mAP\uff0c\u4f46\u5728\u9ad8\u5ea6\u6742\u4e71\u7684\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u591a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2601.02309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02309", "abs": "https://arxiv.org/abs/2601.02309", "authors": ["Xiaopeng Guo", "Yinzhe Xu", "Huajian Huang", "Sai-Kit Yeung"], "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "comment": "12 pages. Received by RA-L", "summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "AI": {"tldr": "360DVO\u662f\u9996\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u5411\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7DAS-Feat\u548cODBA\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u6216\u5149\u5ea6\u76ee\u6807\u7684\u65b9\u6cd5\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5982\u5267\u70c8\u8fd0\u52a8\u548c\u5149\u7167\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u5931\u771f\u611f\u77e5\u7403\u5f62\u7279\u5f81\u63d0\u53d6\u5668\uff08DAS-Feat\uff09\u548c\u5168\u5411\u53ef\u5fae\u5206\u675f\u8c03\u6574\uff08ODBA\uff09\u6a21\u5757\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cOVO\u57fa\u51c6\u548c\u516c\u5f00\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c360DVO\u5728\u9c81\u68d2\u6027\u4e0a\u63d0\u534750%\uff0c\u51c6\u786e\u6027\u4e0a\u63d0\u534737.5%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u3002", "conclusion": "360DVO\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u5168\u5411\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08OVO\uff09\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.02315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02315", "abs": "https://arxiv.org/abs/2601.02315", "authors": ["Saurabh Kaushik", "Lalit Maurya", "Beth Tellman"], "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping", "comment": "Accepted at CV4EO Workshop @ WACV 2026", "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}", "AI": {"tldr": "Prithvi-CAFE\u901a\u8fc7\u7ed3\u5408GFM\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548cCNN\u6b8b\u5dee\u5206\u652f\uff0c\u5728\u6d2a\u6c34\u6620\u5c04\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9GFMs\u5728\u6d2a\u6c34\u6620\u5c04\u4efb\u52a1\u4e2d\u96be\u4ee5\u6355\u6349\u5173\u952e\u5c40\u90e8\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Prithvi-CAFE\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "Prithvi-CAFE\u901a\u8fc7\u5c06Prithvi GFM\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e0e\u5e76\u884cCNN\u6b8b\u5dee\u5206\u652f\u7ed3\u5408\uff0c\u5e76\u589e\u5f3a\u5377\u79ef\u6ce8\u610f\u529b\u6a21\u5757\uff08CAM\uff09\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u6548\u7684\u5fae\u8c03\u548c\u591a\u5c3a\u5ea6\u3001\u591a\u5c42\u6b21\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728Sen1Flood11\u548cFloodPlanet\u6570\u636e\u96c6\u4e0a\uff0cPrithvi-CAFE\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfU-Net\u548c\u5176\u4ed6GFMs\u3002", "conclusion": "Prithvi-CAFE\u5c55\u793a\u4e86\u5728\u9700\u8981\u591a\u901a\u9053\u548c\u591a\u6a21\u6001\u6570\u636e\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\u4e14\u5c40\u90e8\u7ec6\u8282\u81f3\u5173\u91cd\u8981\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2601.02318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02318", "abs": "https://arxiv.org/abs/2601.02318", "authors": ["Roja Sahoo", "Anoop Namboodiri"], "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching", "comment": "15 pages, 8 figures, 5 tables. Submitted to ICPR 2026", "summary": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).", "AI": {"tldr": "F2P\u6846\u67b6\u901a\u8fc7\u878d\u5408\u95ea\u5149\u4e0e\u975e\u95ea\u5149\u6307\u7eb9\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u65e0\u63a5\u89e6\u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u810a\u7ebf\u6e05\u6670\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u63a5\u89e6\u5f0f\u6307\u7eb9\u8bc6\u522b\u5b58\u5728\u536b\u751f\u548c\u4fbf\u5229\u6027\u95ee\u9898\uff0c\u800c\u65e0\u63a5\u89e6\u56fe\u50cf\u56e0\u5149\u7167\u53d8\u5316\u548c\u566a\u58f0\u5bfc\u81f4\u810a\u7ebf\u6e05\u6670\u5ea6\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faF2P\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efaFNF\u6570\u636e\u5e93\u3001\u624b\u52a8\u95ea\u5149-\u975e\u95ea\u5149\u51cf\u6cd5\u3001\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u3001U-Net\u589e\u5f3a\u6a21\u5757\u548c\u6df1\u5ea6\u5d4c\u5165\u6a21\u578b\u3002", "result": "F2P\u5728\u8bc6\u522b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff08AUC=0.999\uff0cEER=1.12%\uff09\uff0c\u4f18\u4e8e\u5355\u6355\u83b7\u57fa\u7ebf\u65b9\u6cd5\uff08Verifinger, DeepPrint\uff09\u3002", "conclusion": "Fusion2Print (F2P) \u6846\u67b6\u901a\u8fc7\u878d\u5408\u95ea\u5149\u4e0e\u975e\u95ea\u5149\u63a5\u89e6\u5f0f\u6307\u7eb9\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u7eb9\u8bc6\u522b\u7684\u6e05\u6670\u5ea6\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4e0e\u63a5\u89e6\u5f0f\u6307\u7eb9\u517c\u5bb9\u7684\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\u3002"}}
{"id": "2601.02329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02329", "abs": "https://arxiv.org/abs/2601.02329", "authors": ["Laurent Caraffa"], "title": "BEDS: Bayesian Emergent Dissipative Structures", "comment": "19 pages", "summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, \u03c0, \u03c6) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking G\u00f6del's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.", "AI": {"tldr": "BEDS\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u70ed\u529b\u5b66\u548c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u63d0\u51fa\u5b66\u4e60\u662f\u71b5\u8f93\u51fa\u9a71\u52a8\u7684\u7ed3\u6784\u5f62\u6210\u8fc7\u7a0b\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u3002", "motivation": "\u7edf\u4e00\u7269\u7406\u3001\u751f\u7269\u548c\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u5b66\u4e60\u6982\u5ff5\uff0c\u63a2\u7d22\u53ef\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u7684\u672c\u8d28\u3002", "method": "\u57fa\u4e8ePrigogine\u7684\u8017\u6563\u7ed3\u6784\u7406\u8bba\uff0c\u5efa\u7acb\u4e86\u70ed\u529b\u5b66\u8fc7\u7a0b\u4e0e\u8d1d\u53f6\u65af\u66f4\u65b0\u4e4b\u95f4\u7684\u5f62\u5f0f\u540c\u6784\uff0c\u63a8\u5bfc\u4e86\u6570\u5b66\u5e38\u6570\uff08e, \u03c0, \u03c6\uff09\u4f5c\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u56fa\u5b9a\u70b9\uff0c\u5e76\u63d0\u51fa\u731c\u60f3\u5c06\u54e5\u5fb7\u5c14\u4e0d\u5b8c\u5907\u5b9a\u7406\u4e0e\u70ed\u529b\u5b66\u7ea6\u675f\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u63d0\u51fa\u4e86BEDS\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5728\u5bf9\u7b49\u7f51\u7edc\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u5206\u5e03\u5f0f\u5171\u8bc6\u7cfb\u7edf\u9ad8\u516d\u4e2a\u6570\u91cf\u7ea7\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "BEDS\u6846\u67b6\u5c06\u975e\u5e73\u8861\u70ed\u529b\u5b66\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u3001\u4fe1\u606f\u51e0\u4f55\u548c\u673a\u5668\u5b66\u4e60\u7edf\u4e00\u8d77\u6765\uff0c\u63d0\u51fa\u5b66\u4e60\u672c\u8d28\u4e0a\u662f\u901a\u8fc7\u71b5\u8f93\u51fa\u5c06\u901a\u91cf\u8f6c\u5316\u4e3a\u7ed3\u6784\u7684\u8fc7\u7a0b\u3002\u8be5\u7406\u8bba\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5bf9\u5b66\u4e60\u548c\u8ba1\u7b97\u672c\u8d28\u7684\u7406\u8bba\u89c1\u89e3\uff0c\u8fd8\u4e3a\u53ef\u6301\u7eed\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002"}}
{"id": "2601.02339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02339", "abs": "https://arxiv.org/abs/2601.02339", "authors": ["Jingming He", "Chongyi Li", "Shiqi Wang", "Sam Kwong"], "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding", "comment": "Accepted by ICCV 2025", "summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8bed\u4e49\u548c\u6e32\u67d3\u76843D\u9ad8\u65af\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u63cf\u8ff0\u7b26\u548c\u81ea\u9002\u5e94\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8bed\u4e49\u548c\u6e32\u67d3\u5206\u652f\u5206\u5f00\u5904\u7406\uff0c\u4ec5\u4f9d\u8d562D\u76d1\u7763\u800c\u5ffd\u75653D\u9ad8\u65af\u51e0\u4f55\uff0c\u4e14\u81ea\u9002\u5e94\u7b56\u7565\u4ec5\u4f9d\u8d56\u6e32\u67d3\u68af\u5ea6\uff0c\u5bfc\u81f4\u5728\u7ec6\u5fae\u6216\u65e0\u7eb9\u7406\u533a\u57df\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86\u5404\u5411\u5f02\u60273D\u9ad8\u65af\u5207\u6bd4\u96ea\u592b\u63cf\u8ff0\u7b26\u6765\u6355\u6349\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u8bed\u4e49\u548c\u5f62\u72b6\u4fe1\u53f7\u81ea\u9002\u5e94\u8c03\u6574\u9ad8\u65af\u5206\u914d\u548c\u7403\u8c10\u51fd\u6570\uff0c\u8fd8\u91c7\u7528\u4e86\u8de8\u573a\u666f\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e32\u67d3\u5e27\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u6e32\u67d3\u5206\u652f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u9ad8\u65af\u5efa\u6a21\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e32\u67d3\u5e27\u7387\u3002"}}
{"id": "2601.02353", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02353", "abs": "https://arxiv.org/abs/2601.02353", "authors": ["Shahnawaz Alam", "Mohammed Mudassir Uddin", "Mohammed Kaif Pasha"], "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices", "comment": null, "summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.", "AI": {"tldr": "\u8bba\u6587\u7ed3\u5408\u6a21\u578b\u526a\u679d\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u63d0\u51faDACIS\u65b9\u6cd5\u548cPMP\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u4f7f\u5b9e\u65f6\u7530\u95f4\u8bca\u65ad\u5728\u4f4e\u6210\u672c\u8bbe\u5907\u4e0a\u53ef\u884c\u3002", "motivation": "\u504f\u8fdc\u5730\u533a\u519c\u6c11\u7f3a\u4e4f\u5b9e\u9a8c\u5ba4\u6216\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u56f0\u96be\u4e14\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Disease-Aware Channel Importance Scoring (DACIS)\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4e09\u9636\u6bb5\u7684Prune-then-Meta-Learn-then-Prune (PMP)\u6d41\u7a0b\uff0c\u901a\u8fc7\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u4e2d\u5bf9\u533a\u5206\u690d\u7269\u75c5\u5bb3\u6700\u91cd\u8981\u7684\u90e8\u5206\uff0c\u5e76\u8fdb\u884c\u526a\u679d\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "\u5728PlantVillage\u548cPlantDoc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e8678%\uff0c\u540c\u65f6\u4fdd\u6301\u4e8692.3%\u7684\u539f\u59cb\u51c6\u786e\u7387\uff0c\u538b\u7f29\u540e\u7684\u6a21\u578b\u5728Raspberry Pi 4\u4e0a\u4ee5\u6bcf\u79d27\u5e27\u7684\u901f\u5ea6\u8fd0\u884c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684DACIS\u65b9\u6cd5\u548cPMP\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6a21\u578b\u526a\u679d\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u5e76\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4e3a\u5c0f\u519c\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u7530\u95f4\u8bca\u65ad\u65b9\u6848\u3002"}}
{"id": "2601.02356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02356", "abs": "https://arxiv.org/abs/2601.02356", "authors": ["Jing Tan", "Zhaoyang Zhang", "Yantao Shen", "Jiarui Cai", "Shuo Yang", "Jiajun Wu", "Wei Xia", "Zhuowen Tu", "Stefano Soatto"], "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes", "comment": "Project page: https://sparkstj.github.io/talk2move", "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.", "AI": {"tldr": "Talk2Move \u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7 GRPO \u548c\u7a7a\u95f4\u5956\u52b1\u6a21\u578b\u5b9e\u73b0\u6587\u672c\u6307\u4ee4\u4e0b\u7684\u76ee\u6807\u7a7a\u95f4\u53d8\u6362\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u4e14\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u76ee\u6807\u7a7a\u95f4\u53d8\u6362\u662f\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u7ea7\u51e0\u4f55\u53d8\u6362\uff08\u5982\u5e73\u79fb\u3001\u65cb\u8f6c\u6216\u7f29\u653e\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Talk2Move \u91c7\u7528 Group Relative Policy Optimization (GRPO) \u548c\u7a7a\u95f4\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6837\u5316 rollout \u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u53d8\u4f53\u63a2\u7d22\u51e0\u4f55\u52a8\u4f5c\uff0c\u65e0\u9700\u6602\u8d35\u7684\u914d\u5bf9\u6570\u636e\u3002", "result": "\u5728\u7cbe\u9009\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTalk2Move \u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u4e00\u81f4\u4e14\u8bed\u4e49\u5fe0\u5b9e\u7684\u76ee\u6807\u53d8\u6362\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Talk2Move \u901a\u8fc7 GRPO \u548c\u7a7a\u95f4\u5956\u52b1\u6a21\u578b\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u4e00\u81f4\u4e14\u8bed\u4e49\u5fe0\u5b9e\u7684\u76ee\u6807\u53d8\u6362\uff0c\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u573a\u666f\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u65b9\u6cd5\u3002"}}
{"id": "2601.02358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02358", "abs": "https://arxiv.org/abs/2601.02358", "authors": ["Junyi Chen", "Tong He", "Zhoujie Fu", "Pengfei Wan", "Kun Gai", "Weicai Ye"], "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "comment": "Project page: https://sotamak1r.github.io/VINO-web/", "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "AI": {"tldr": "VINO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u751f\u6210\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u6269\u6563\u4e3b\u5e72\u548c\u4ea4\u9519\u6761\u4ef6\u4ee4\u724c\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u521b\u4f5c\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5355\u4e00\u6846\u67b6\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u907f\u514d\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u6216\u72ec\u7acb\u6a21\u5757\uff0c\u63d0\u5347\u89c6\u89c9\u521b\u4f5c\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "VINO\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MMDiT\uff09\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u9010\u6b65\u6269\u5c55\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u591a\u53c2\u8003\u57fa\u7840\u3001\u957f\u6307\u4ee4\u8ddf\u968f\u53ca\u8de8\u9759\u6001\u548c\u52a8\u6001\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u8eab\u4efd\u4fdd\u6301\u3002", "result": "VINO\u5728\u591a\u6837\u5316\u7684\u751f\u6210\u548c\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u51c6\u786e\u7684\u6307\u4ee4\u8ddf\u968f\u3001\u6539\u8fdb\u7684\u53c2\u8003\u548c\u5c5e\u6027\u4fdd\u6301\u80fd\u529b\uff0c\u4ee5\u53ca\u66f4\u53ef\u63a7\u7684\u591a\u8eab\u4efd\u7f16\u8f91\u3002", "conclusion": "VINO\u5c55\u793a\u4e86\u5b9e\u73b0\u53ef\u6269\u5c55\u7edf\u4e00\u89c6\u89c9\u751f\u6210\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u5e76\u7a81\u51fa\u4e86\u4ea4\u9519\u4e0a\u4e0b\u6587\u8ba1\u7b97\u4f5c\u4e3a\u901a\u7528\u89c6\u89c9\u521b\u4f5c\u57fa\u7840\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.02359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02359", "abs": "https://arxiv.org/abs/2601.02359", "authors": ["Kaede Shiohara", "Toshihiko Yamasaki", "Vladislav Golyanik"], "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors", "comment": "17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/", "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.", "AI": {"tldr": "ExposeAnyone\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891\u751f\u6210\u8868\u60c5\u5e8f\u5217\u5e76\u8ba1\u7b97\u8eab\u4efd\u8ddd\u79bb\uff0c\u6709\u6548\u68c0\u6d4b\u672a\u77e5\u6df1\u5ea6\u4f2a\u9020\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5bf9\u5e72\u6270\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\uff0c\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u8bad\u7ec3\u5bfc\u81f4\u8fc7\u62df\u5408\u3002\u81ea\u76d1\u7763\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u96be\u4ee5\u4ec5\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5230\u5224\u522b\u6027\u8868\u793a\u3002", "method": "\u63d0\u51faExposeAnyone\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b8c\u5168\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891\u751f\u6210\u8868\u60c5\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u53c2\u8003\u96c6\u5bf9\u7279\u5b9a\u5bf9\u8c61\u8fdb\u884c\u4e2a\u6027\u5316\u5efa\u6a21\uff0c\u901a\u8fc7\u6269\u6563\u91cd\u5efa\u8bef\u5dee\u8ba1\u7b97\u8eab\u4efd\u8ddd\u79bb\u3002", "result": "1) \u5728DF-TIMIT\u3001DFDCP\u3001KoDF\u548cIDForge\u6570\u636e\u96c6\u4e0a\u5e73\u5747AUC\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u9ad84.22\u4e2a\u767e\u5206\u70b9\uff1b2) \u80fd\u68c0\u6d4bSora2\u751f\u6210\u7684\u89c6\u9891\uff1b3) \u5bf9\u6a21\u7cca\u548c\u538b\u7f29\u7b49\u5e72\u6270\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "ExposeAnyone \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b8c\u5168\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891\u751f\u6210\u8868\u60c5\u5e8f\u5217\uff0c\u5e76\u5728\u4e2a\u6027\u5316\u5230\u7279\u5b9a\u5bf9\u8c61\u540e\uff0c\u901a\u8fc7\u6269\u6563\u91cd\u5efa\u8bef\u5dee\u8ba1\u7b97\u8eab\u4efd\u8ddd\u79bb\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u6df1\u5ea6\u4f2a\u9020\u7684\u6709\u6548\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u68c0\u6d4bSora2\u751f\u6210\u7684\u89c6\u9891\uff0c\u5c55\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9ad8\u9c81\u68d2\u6027\u3002"}}
