<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.RO](#cs.RO) [Total: 29]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.NI](#cs.NI) [Total: 18]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data](https://arxiv.org/abs/2601.02392)
*Mo Chen*

Main category: cs.CV

TL;DR: Dense-MAE通过自监督预训练提升CTA钙化伪影去除效果，减少对标注数据的依赖，优化狭窄诊断。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉钙化在CTA中产生伪影，影响管腔狭窄诊断。现有DCNN方法依赖大量标注数据，但医学领域数据稀缺。

Method: 提出Dense-MAE框架，采用随机掩码3D血管腔体块并训练Dense-Unet重建缺失几何结构的预训练策略。

Result: 实验表明，基于MAE的权重初始化显著提升了钙化去除网络的重建精度和狭窄估计效果，尤其在少样本情况下。

Conclusion: Dense-MAE作为一种自监督学习框架，通过预训练策略显著提高了CTA中钙化斑块去除的准确性，尤其在少样本场景下表现优异。

Abstract: Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.

</details>


### [2] [MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion](https://arxiv.org/abs/2601.02414)
*Jichao Zhu,Jun Yu*

Main category: cs.CV

TL;DR: MIAR通过模态交互和对齐表示，解决了多模态情感识别中的模态差异和贡献不均问题，实验显示其性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态融合时未能充分处理模态间的分布差异及其对任务的不同贡献，且缺乏对多样化文本模型特征的鲁棒泛化能力。

Method: 提出了一种名为Modality Interaction and Alignment Representation (MIAR)的新方法，通过特征交互生成特征令牌来表示模态的全局表示，并利用对比学习和归一化策略对齐不同模态。

Result: 在CMU-MOSI和CMU-MOSEI数据集上的实验结果表明，MIAR优于当前最先进的多模态情感识别方法。

Conclusion: MIAR网络通过模态交互和对齐表示，显著提升了多模态情感识别的性能，并在CMU-MOSI和CMU-MOSEI数据集上超越了现有方法。

Abstract: Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.

</details>


### [3] [Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion](https://arxiv.org/abs/2601.02415)
*Wangyuan Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种对称互促（SMP）跨模态特征融合方法，通过多通道特征提取和注意力机制，有效提升了多模态情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析在人类-计算机交互和情感计算领域具有重要意义，但现有研究在特征提取和融合方面存在不足，尤其是模态间特征差异的忽视。本文旨在解决这些问题。

Method: 本文首先提取多通道特征以获得更全面的特征信息，并在视觉和听觉模态中采用双通道特征以增强模态内特征表示。其次，提出了一种对称互促（SMP）跨模态特征融合方法，结合对称跨模态注意力机制和自注意力机制，促进模态间有用信息的交换。

Result: 在两个基准数据集上的实验证明了所提方法的有效性和优越性。

Conclusion: 实验结果表明，所提出的对称互促（SMP）跨模态特征融合方法在情感分析任务中表现出色，验证了其在特征信息融合和模态交互方面的有效性。

Abstract: Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.

</details>


### [4] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: CoCoT框架通过动态多区域定位和关系感知推理，显著提升跨模态推理性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有跨模态推理方法对单一粗粒度图像区域的过度依赖及推理步骤间语义碎片化的问题。

Method: 提出了CoCoT框架，包含动态多区域定位和关系感知推理两大创新，构建了CoCoT-70K数据集。

Result: 在LLaVA-1.5和Qwen2-VL上分别实现了15.4%和4.0%的平均准确率提升。

Conclusion: CoCoT框架通过动态多区域定位和关系感知推理，显著提升了跨模态推理性能，并在多个基准测试中取得了显著的准确率提升。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [5] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen是一个基于40,000小时游戏视频训练的视觉-动作基础模型，在多种游戏类型中表现出色，并在未见游戏中实现了显著的任务成功率提升。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个通用的游戏智能体，能够在多种游戏类型中表现出色，并推动通用具身智能体的研究。

Method: 通过自动从公开的游戏视频中提取玩家动作构建了一个互联网规模的视频-动作数据集，并训练了一个统一的大规模行为克隆视觉-动作模型。

Result: NitroGen在3D动作游戏的战斗遭遇、2D平台游戏的高精度控制以及程序生成世界的探索中表现出色。

Conclusion: NitroGen展示了在多样化游戏领域中强大的能力，并在未见过的游戏中实现了高达52%的相对任务成功率提升。

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [6] [TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers](https://arxiv.org/abs/2601.02437)
*Zhibo Wang,Zuoyuan Zhang,Xiaoyi Pang,Qile Zhang,Xuanyi Hao,Shuguo Zhuo,Peng Sun*

Main category: cs.CV

TL;DR: TAP-ViTs 是一种无需原始本地数据的任务自适应剪枝框架，通过GMM和双粒度评估实现高效定制化剪枝。


<details>
  <summary>Details</summary>
Motivation: 解决现有剪枝方法无法在隐私保护的移动计算环境中实现任务定制化剪枝的问题。

Method: 通过基于高斯混合模型（GMM）的度量数据集构建机制推断设备级任务特性，并开发了双粒度重要性评估剪枝策略。

Result: 在多种ViT架构和数据集上的实验表明，TAP-ViTs在相同压缩比下始终优于最先进的剪枝方法。

Conclusion: TAP-ViTs 提出了一种新颖的任务自适应剪枝框架，能够在隐私保护的前提下为不同设备生成定制化的剪枝模型，显著优于现有方法。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.

</details>


### [7] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 文本推理在BIQA中的作用有限，自洽范式能有效缩小图像与文本预测差距，为优化提供方向。


<details>
  <summary>Details</summary>
Motivation: 探讨文本信息在盲图像质量评估（BIQA）中的作用及其对分数相关图像内容的表征能力。

Method: 通过比较三种学习图像-文本-分数关系的范式（Chain-of-Thought、Self-Consistency、Autoencoder）来分析文本信息对质量预测的贡献。

Result: 实验显示，现有模型在仅使用文本信息时预测性能显著下降；自洽范式显著缩小了图像与文本预测的差距（PLCC/SRCC差异降至0.02/0.03），而自动编码器范式效果较弱。

Conclusion: 研究发现，自洽范式（Self-Consistency）在缩小图像与文本预测差距方面表现最佳，为BIQA及高级视觉任务中的文本推理优化提供了方向。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [8] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: MLLM在医学图像分类任务中表现有限，视觉编码器优化和数据集质量比规模更重要，LLM更适合辅助角色而非主要分类器。


<details>
  <summary>Details</summary>
Motivation: 评估MLLM在膝关节骨关节炎（OA）放射学分类中的表现，该领域在现有医学MLLM基准中代表性不足，但全球影响数亿人。

Method: 通过系统消融研究，调整视觉编码器、连接器和大型语言模型（LLM）的组件及训练策略，评估各组件对诊断准确性的贡献。

Result: 仅训练的视觉编码器在分类准确性上优于完整MLLM流程，微调LLM未显著提升性能。小规模平衡数据集（500张图像）的LoRA微调效果优于大规模不平衡数据集（5,778张图像）。

Conclusion: 多模态大型语言模型（MLLM）在医学图像诊断分类任务中表现不佳，更适合作为解释和报告生成工具。建议优先优化视觉编码器并精心策划数据集。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [9] [A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction](https://arxiv.org/abs/2601.02445)
*Parashjyoti Borah,Sanghamitra Sarkar,Ranjan Phukan*

Main category: cs.CV

TL;DR: 该论文通过深度学习框架，将季风预测视为计算机视觉任务，成功生成了高分辨率的网格化降雨预测，适用于区域资源管理。


<details>
  <summary>Details</summary>
Motivation: 传统的长期预报方法缺乏空间细节，无法满足区域资源管理的需求。为了解决这一问题，作者提出了一种新颖的深度学习框架，将网格化季风预测重新定义为时空计算机视觉任务。

Method: 利用85年的ERA5再分析数据和IMD降雨数据，将多变量前季风期大气和海洋场视为多通道图像序列，通过基于卷积神经网络（CNN）的架构，学习从五个月前季风期（1月至5月）到后续季风季节高分辨率网格化降雨模式的复杂映射。

Result: 该框架成功地为季风季节的四个月份（6月至9月）以及整个季节平均值生成了不同的预测，证明了其在季节内和季节性展望中的实用性。

Conclusion: 该论文提出的深度学习框架成功地为印度夏季风（ISM）提供了高分辨率的网格化降雨预测，不仅覆盖了整个季风季节，还能逐月预测，显著提升了区域资源管理的实用性。

Abstract: The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.

</details>


### [10] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

TL;DR: 论文利用隐式神经表示（INRs）开发了两种3D分析框架，解决了视网膜OCT各向异性数据的处理问题，支持跨协议分析和容积评估。


<details>
  <summary>Details</summary>
Motivation: 常规临床OCT成像因大切片间距导致高度各向异性的图像，传统2D方法无法保证相邻B-scan结果的一致性，且受限于训练数据分辨率。INRs的连续表示特性为各向异性数据分析提供了新思路。

Method: 1) 通过结合en-face模态的额外信息进行B-scan间插值，以保留B-scan间的相关结构。2) 创建一个分辨率无关的视网膜图谱，支持对数据无严格要求的通用分析。两种方法均利用可泛化的INRs，通过基于群体的训练改善视网膜形状表示。

Result: 提出的框架能够处理大B-scan间距的OCT图像，实现了视网膜结构和病理的容积评估，为临床提供了更灵活的分析工具。

Conclusion: 该论文提出了两种基于隐式神经表示（INRs）的框架，用于视网膜OCT体积的密集3D分析，解决了传统2D方法在处理各向异性数据时的局限性。这些框架不仅提高了视网膜形状表示的准确性，还为不同成像协议的数据分析提供了灵活性。

Abstract: Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [11] [PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding](https://arxiv.org/abs/2601.02457)
*Souhail Hadgi,Bingchen Gong,Ramana Sundararaman,Emery Pierson,Lei Li,Peter Wonka,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 提出一种直接从点云生成语言对齐补丁级特征的3D编码器，无需多视角渲染，显著提升3D部件分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型在全局任务（检索、分类）上表现优异，但在局部部件级推理上表现不佳，且现有方法依赖昂贵的多视角渲染和大型语言模型提示工程。

Method: 采用两阶段预训练方法：首先从视觉编码器（如DINOv2）蒸馏密集2D特征到3D补丁，然后通过多正对比目标将这些补丁嵌入与部件级文本嵌入对齐。

Result: 该方法在多个3D部件分割基准测试中表现优异，且推理速度快。

Conclusion: 提出的3D编码器在无需多视角渲染的情况下，实现了零样本3D部件分割，显著优于以往基于渲染和前馈的方法。

Abstract: Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/

</details>


### [12] [CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking](https://arxiv.org/abs/2601.02521)
*Amirreza Parvahan,Mohammad Hoseyni,Javad Khoramdel,Amirhossein Nikoofard*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级框架，将3D CT数据视为视频流，结合YOLO和ByteTrack算法，显著提升了颅内出血检测的精度，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上3D CNN的高内存和计算需求问题，实现高效的3D医学影像分析。

Method: 将3D CT数据视为视频流，采用多代YOLO Nano架构作为切片级骨干网络，结合ByteTrack算法确保解剖一致性，并引入混合推理策略和时空一致性过滤器以减少初始化延迟。

Result: 在独立测试数据上，检测精度从0.703提升至0.779，同时保持高灵敏度。

Conclusion: 该方法通过将3D CT数据重新构建为视频流，结合轻量级YOLO架构和ByteTrack算法，显著提升了颅内出血检测的精度和效率，为资源受限环境下的实时医疗分析提供了可行方案。

Abstract: Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.

</details>


### [13] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: MovieRecapsQA是一个开放式多模态VideoQA基准测试，通过电影回顾视频生成QA对，评估显示视觉问题最具挑战性且模型依赖文本输入。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA基准测试难以捕捉多模态推理且多为非开放式，无法评估自由形式答案。MovieRecapsQA旨在填补这一空白。

Method: 利用电影回顾视频生成QA对，并提供显式文本上下文用于评估。基准测试包含多种视频长度和问题分类。

Result: 评估了七种最先进的多模态大语言模型，发现视觉问题最具挑战性，模型倾向于依赖文本输入，且所有模型在提取视频信息方面表现不佳。

Conclusion: MovieRecapsQA是一个新颖的开端多模态VideoQA基准测试，通过电影回顾视频生成约8.2K个QA对，并提供了验证答案所需的‘事实’。该基准测试支持多种视频长度和问题分类，用于细粒度分析。评估显示，视觉问题最具挑战性，模型倾向于依赖文本输入，且所有模型在从视频内容中提取准确信息方面仍有困难。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [14] [Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network](https://arxiv.org/abs/2601.02566)
*Junbin Zhang,Hamid Reza Tohidypour,Yixiao Wang,Panos Nasiopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种结合Vision Mamba和G-GNN的深度学习网络，用于定位浅伪造和深伪造图像中的篡改区域，实验证明其准确率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对浅伪造和深伪造图像篡改定位的研究较少，本文探索了深度学习网络在此任务中的可行性。

Method: 利用Vision Mamba网络提取特征图以清晰描述篡改与未篡改区域的边界，并通过提出的G-GNN模块进一步强化这种区分。

Result: 评估结果表明，所提方法在推理准确率上优于其他先进方法。

Conclusion: 本文提出了一种结合Vision Mamba网络和G-GNN模块的方法，能够有效定位浅伪造和深伪造图像中的篡改区域，并在准确率上优于现有技术。

Abstract: Image manipulation localization is a critical research task, given that forged images may have a significant societal impact of various aspects. Such image manipulations can be produced using traditional image editing tools (known as "shallowfakes") or advanced artificial intelligence techniques ("deepfakes"). While numerous studies have focused on image manipulation localization on either shallowfake images or deepfake videos, few approaches address both cases. In this paper, we explore the feasibility of using a deep learning network to localize manipulations in both shallow- and deep-fake images, and proposed a solution for such purpose. To precisely differentiate between authentic and manipulated pixels, we leverage the Vision Mamba network to extract feature maps that clearly describe the boundaries between tampered and untouched regions. To further enhance this separation, we propose a novel Guided Graph Neural Network (G-GNN) module that amplifies the distinction between manipulated and authentic pixels. Our evaluation results show that our proposed method achieved higher inference accuracy compared to other state-of-the-art methods.

</details>


### [15] [DreamLoop: Controllable Cinemagraph Generation from a Single Photograph](https://arxiv.org/abs/2601.02646)
*Aniruddha Mahapatra,Long Mai,Cusuh Ham,Feng Liu*

Main category: cs.CV

TL;DR: DreamLoop是一种无需专门训练数据的视频合成框架，通过时间桥接和运动条件化从单张照片生成可控cinemagraphs，效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有技术局限于简单、低频运动或特定领域（如水和烟雾），且缺乏专门数据生成无缝、可控循环的cinemagraphs。DreamLoop旨在解决这些问题，提供更灵活的生成方式。

Method: 通过训练通用视频扩散模型实现两个目标：时间桥接和运动条件化，从而灵活生成cinemagraphs。在推理阶段，利用输入图像作为首尾帧条件实现无缝循环，并通过静态轨迹保持背景静止。

Result: DreamLoop能够生成与用户意图一致的高质量cinemagraphs，优于现有方法。

Conclusion: DreamLoop是一种创新的视频合成框架，能够从单张照片生成高质量、复杂的cinemagraphs，无需专门的cinemagraph训练数据，且在用户控制下表现出色。

Abstract: Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.

</details>


### [16] [GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images](https://arxiv.org/abs/2601.02709)
*Shuman He,Xiehua Li,Xioaju Yang,Yang Xiong,Keqin Li*

Main category: cs.CV

TL;DR: GRRE是一种基于G通道移除重建误差的新方法，能有效检测AI生成图像，尤其在跨模型泛化和抗干扰性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速进步使得区分合成图像与真实图像变得困难，现有检测方法在面对新型或未见过的生成模型时准确性下降，缺乏强泛化能力。

Method: 提出了一种基于G通道移除重建误差（GRRE）的检测方法，利用真实图像与AI生成图像在重建误差上的差异进行检测。

Result: GRRE在多个生成模型（包括训练时未见过的模型）上均表现出高检测准确性，且对抗干扰和后处理操作具有强鲁棒性。

Conclusion: GRRE方法通过基于通道移除重建的新检测范式，展示了在生成AI时代保护图像真实性的潜力，尤其是在跨模型泛化和抗干扰性方面表现优异。

Abstract: The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.

</details>


### [17] [CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos](https://arxiv.org/abs/2601.02716)
*Taeyeon Kim,Youngju Na,Jumin Lee,Minhyuk Sung,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: CAMO是一个无需模板或3D监督的类别无关框架，通过3D高斯溅射和语义对应实现高效2D视频到3D运动迁移。


<details>
  <summary>Details</summary>
Motivation: 解决2D视频到3D资产运动迁移中的姿态歧义性和多样形状挑战，避免依赖类别特定模板或显式3D监督。

Method: 提出CAMO框架，结合形态参数化的3D高斯溅射模型和密集语义对应关系，通过优化联合调整形状与姿态。

Result: 实验结果显示，CAMO在运动准确性、效率和视觉一致性上优于现有方法，适用于多样对象类别和日常视频场景。

Conclusion: CAMO框架通过形态参数化的3D高斯溅射模型和密集语义对应关系，有效解决了形状与姿态的歧义性问题，实现了多样类别的高效、准确运动迁移。

Abstract: Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.

</details>


### [18] [Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing](https://arxiv.org/abs/2601.02721)
*Guoquan Zheng,Jie Hao,Huiyu Duan,Yongming Han,Liang Yuan,Dong Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该论文提出了一种鲁棒的3D网格显著性GT获取框架，通过VCS和HCD算法解决了现有方法的局限性，提供了与人类感知一致的高保真3D注意力获取方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D网格显著性GT获取方法与2D图像方法一致，忽略了3D几何拓扑与2D图像阵列的差异。当前的VR眼动追踪流程依赖单射线采样和欧几里得平滑，导致纹理注意力和信号泄漏。

Method: 提出了一种鲁棒的框架，包括视图锥采样（VCS）策略和混合流形-欧几里得约束扩散（HCD）算法。VCS通过高斯分布的射线束模拟人类中央凹感受野，提高复杂拓扑的采样鲁棒性；HCD融合流形测地约束和欧几里得尺度，确保拓扑一致的显著性传播。

Result: 该框架通过VCS和HCD算法，有效缓解了“拓扑短路”和混叠问题，提供了高保真的3D注意力获取范式，为3D网格显著性研究提供了更准确和鲁棒的基线。

Conclusion: 该框架通过引入VCS策略和HCD算法，解决了现有3D网格显著性GT获取方法的局限性，提供了一个更准确、鲁棒的3D注意力获取范式，与人类自然感知一致。

Abstract: Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating "topological short-circuits" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.

</details>


### [19] [Foreground-Aware Dataset Distillation via Dynamic Patch Selection](https://arxiv.org/abs/2601.02727)
*Longzhen Li,Guang Li,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种前景感知的数据集蒸馏方法，通过动态补丁选择和双路径机制提升性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的方法存在高计算开销、内存限制和生成不真实噪声图像的问题，而非优化方法虽缓解了部分问题，但刚性补丁选择策略可能丢弃关键信息。

Method: 利用Grounded SAM2识别前景对象并计算每张图像的前景占用率，从而推导出类别补丁决策阈值。基于这些阈值设计动态补丁选择策略，针对每张图像选择最有信息的补丁或直接调整图像大小。

Result: 在多个基准测试中，该方法在蒸馏性能上一致优于现有方法，生成的蒸馏数据集更具信息量和代表性，且在不同架构和图像组成中表现更鲁棒。

Conclusion: 提出的方法通过动态补丁选择策略和双路径机制，显著提升了数据集蒸馏的性能，生成了更具信息量和代表性的蒸馏数据集，并在不同架构和图像组成中表现出更强的鲁棒性。

Abstract: In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.

</details>


### [20] [HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps](https://arxiv.org/abs/2601.02730)
*Xuchang Zhong,Xu Cao,Jinke Feng,Hao Fang*

Main category: cs.CV

TL;DR: 提出一种同形变换引导的姿态估计网络，通过BEV语义对齐和同形学习，显著提升图像到地图定位的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于回归的方法往往忽略固有几何先验，导致训练效率低下和定位精度有限。

Method: 构建满足同形约束的输入对，将地面视图特征投影到BEV域，并与地图特征进行语义对齐。利用同形关系引导特征融合，并将姿态输出限制在有效可行区域内。

Result: 在nuScenes数据集上的大量实验表明，该方法显著优于现有的最先进视觉定位方法。

Conclusion: 该论文提出了一种新颖的同形变换引导的姿态估计网络，通过将地面视图特征投影到BEV域并与地图特征进行语义对齐，显著提高了训练效率和定位精度。该方法在nuScenes数据集上表现优异，超越了现有最先进的视觉定位方法。

Abstract: Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.

</details>


### [21] [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/abs/2601.02737)
*Zanting Ye,Xiaolong Niu,Xuanbin Wu,Xu Han,Shengyuan Liu,Jing Hao,Zhihao Peng,Hao Sun,Jieqin Lv,Fanghu Wang,Yanchao Huang,Hubing Wu,Yixuan Yuan,Habib Zaidi,Arman Rahmim,Yefeng Zheng,Lijun Lu*

Main category: cs.CV

TL;DR: 研究发现MLLMs在功能成像中存在感知差距，提出AVA策略显著改善诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在功能成像中的能力，发现当前视觉编码器无法独立于形态学先验解码功能示踪剂生物分布。

Method: 提出Atomic Visual Alignment (AVA)微调策略，强制模型在高级诊断推理前掌握低层功能感知。

Result: AVA显著缩小感知差距，提升诊断准确性。

Conclusion: AVA方法有效解决了功能感知差距，将CoT从幻觉来源转变为稳健推理工具，诊断准确率提升高达14.83%。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.

</details>


### [22] [D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2601.02747)
*Zixiao Wen,Zhen Yang,Xianjie Bao,Lei Zhang,Xiantai Xiang,Wenshuai Li,Yuhan Liu*

Main category: cs.CV

TL;DR: D$^3$R-DETR通过双域密度细化提升微小物体检测精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于微小物体的像素信息极其有限且物体密度变化显著，主流基于Transformer的检测器存在收敛慢和查询-物体匹配不准确的问题。

Method: 提出了一种基于DETR的检测器D$^3$R-DETR，通过融合空间和频域信息来细化低层特征图，并利用其丰富细节预测更准确的物体密度图。

Result: 在AI-TOD-v2数据集上的大量实验表明，D$^3$R-DETR在微小物体检测上优于现有最先进的检测器。

Conclusion: D$^3$R-DETR通过双域密度细化方法显著提升了微小物体检测的准确性，并在AI-TOD-v2数据集上超越了现有最先进的检测器。

Abstract: Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.

</details>


### [23] [Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups](https://arxiv.org/abs/2601.02759)
*Hyungtae Lim,Minkyun Seo,Luca Carlone,Jaesik Park*

Main category: cs.CV

TL;DR: BUFFER-X 是一种无需训练的点云配准框架，通过自适应参数、分布感知采样和坐标归一化实现零样本泛化，BUFFER-X-Lite 进一步优化效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在零样本泛化中的三个关键限制：固定参数不适用于多尺度、学习的关键点检测器跨域性能差、绝对坐标导致尺度不匹配。

Method: 通过几何引导的自适应超参数估计、分布感知的最远点采样和块级坐标归一化，结合分层多尺度匹配，实现了鲁棒的配准。BUFFER-X-Lite 通过早期退出策略和快速位姿求解器提高了效率。

Result: 在包含 12 个数据集的综合基准测试中，方法表现出色，BUFFER-X-Lite 计算时间减少 43% 且保持精度。

Conclusion: BUFFER-X 和 BUFFER-X-Lite 在不需手动调整或测试领域先验知识的情况下，实现了零样本泛化，适用于多样化的点云配准场景。

Abstract: Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.

</details>


### [24] [AnyDepth: Depth Estimation Made Easy](https://arxiv.org/abs/2601.02760)
*Zeyu Ren,Zeyu Zhang,Wukai Li,Qingxiang Liu,Hao Tang*

Main category: cs.CV

TL;DR: 提出轻量级零样本单目深度估计框架，结合DINOv3编码器和SDT解码器，减少参数并提升精度，强调数据质量与模型设计的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有单目深度估计方法对大规模数据集和复杂解码器的依赖，提高效率和泛化能力。

Method: 采用DINOv3作为视觉编码器获取高质量密集特征，并设计了Simple Depth Transformer（SDT）作为紧凑的基于Transformer的解码器，通过单路径特征融合和上采样过程减少计算开销。此外，提出基于质量的过滤策略以筛选有害样本。

Result: 在五个基准测试中，该框架在精度上超越了DPT，同时减少了约85%-89%的参数数量。

Conclusion: 本研究通过提出轻量级和数据中心的框架，强调了平衡模型设计与数据质量对实现高效且泛化的零样本深度估计的重要性。

Abstract: Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.

</details>


### [25] [ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration](https://arxiv.org/abs/2601.02763)
*Xu Zhang,Huan Zhang,Guoli Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: ClearAIR是一种新型All-in-One图像恢复框架，结合人类视觉感知和多模态评估，通过分层策略解决过平滑和伪影问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AiOIR方法过度依赖退化特定表示，导致过平滑和伪影问题。为解决这一问题，研究团队提出ClearAIR，受人类视觉感知启发，旨在更准确地处理复杂、复合的退化问题。

Method: ClearAIR采用分层、由粗到细的恢复策略：1）使用基于MLLM的图像质量评估（IQA）模型进行全局评估；2）通过区域感知和任务识别管道，结合语义交叉注意力和退化感知模块实现局部恢复；3）利用自监督的内部线索重用机制恢复细节。

Result: 实验结果表明，ClearAIR在多种合成和真实数据集上均取得了优于现有方法的性能。

Conclusion: ClearAIR框架通过结合人类视觉感知（HVP）和多模态大语言模型（MLLM）的评估方法，成功解决了现有All-in-One图像恢复（AiOIR）方法中的过平滑和伪影问题，并在多样化的合成和真实数据集上表现出卓越性能。

Abstract: All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.

</details>


### [26] [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/abs/2601.02771)
*Boyu Chang,Qi Wang,Xi Guo,Zhixiong Nan,Yazhou Yao,Tianfei Zhou*

Main category: cs.CV

TL;DR: AbductiveMLLM通过语言与图像双模态协同，显著提升MLLMs的视觉溯因推理能力，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在溯因推理能力上不及人类，受人类认知中语言与图像溯因交互的启发，旨在通过模拟双模态行为提升MLLMs的推理能力。

Method: 提出AbductiveMLLM，包含REASONER（在语言域中探索并修剪假设）和IMAGINER（通过文本到图像扩散模型模拟视觉场景），两者以端到端方式联合训练。

Result: 在标准VAR基准测试中，AbductiveMLLM表现优于传统解决方案和先进MLLMs，达到最先进性能。

Conclusion: AbductiveMLLM通过结合REASONER和IMAGINER的双模态行为，显著提升了多模态大语言模型（MLLMs）在视觉溯因推理（VAR）任务中的表现，实现了最先进的性能。

Abstract: Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.

</details>


### [27] [EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework](https://arxiv.org/abs/2601.02783)
*Junjue Wang,Yanfei Zhong,Zihang Chen,Zhuo Zheng,Ailong Ma,Liangpei Zhang*

Main category: cs.CV

TL;DR: 论文提出了EarthVLNet框架和EarthVLSet数据集，通过渐进式方法结合语义分割和关系推理，提升了地球视觉的综合场景理解能力，并在多个任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 地球视觉在对象识别方面取得了里程碑式的进展，但在对象关系推理方面缺乏探索，限制了综合场景理解的能力。为了解决这一问题，论文提出了一个渐进式框架。

Method: 提出了一个渐进式的地球视觉-语言理解和生成框架（EarthVLNet），包括多任务数据集（EarthVLSet）和语义引导网络。EarthVLNet通过分阶段实现语义分割、关系推理和综合理解，其中第一阶段生成对象语义以指导视觉问答（VQA）。

Result: 在三个基准测试中（语义分割、多项选择和开放式VQA），EarthVLNet表现出优越性，并指出了三个未来方向：分割特征持续增强VQA性能、多项选择任务对视觉编码器更敏感、开放式任务需要更高级的视觉编码器和语言解码器。

Conclusion: 该论文提出的EarthVLNet框架和EarthVLSet数据集为地理空间对象识别和关系推理提供了新的基准，推动了地球视觉在综合场景理解方面的进展。

Abstract: Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.

</details>


### [28] [DreamStyle: A Unified Framework for Video Stylization](https://arxiv.org/abs/2601.02785)
*Mengtian Li,Jinshu Chen,Songtao Zhao,Wanquan Feng,Pengqi Tu,Qian He*

Main category: cs.CV

TL;DR: DreamStyle是一个支持多种风格条件的统一视频风格化框架，通过高质量数据采集和LoRA训练，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法局限于单一风格条件，且缺乏高质量数据集，导致风格不一致和时间闪烁问题。

Method: 基于普通图像到视频（I2V）模型，采用低秩适应（LoRA）和特定令牌上矩阵训练，以减少不同条件令牌之间的混淆。

Result: DreamStyle在三种视频风格化任务中表现优异，定性定量评估均优于竞争对手。

Conclusion: DreamStyle在风格一致性和视频质量上优于现有方法，能够胜任文本引导、风格图像引导和首帧引导三种视频风格化任务。

Abstract: Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.

</details>


### [29] [Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD](https://arxiv.org/abs/2601.02792)
*Petteri Teikari,Neliana Fuenmayor*

Main category: cs.CV

TL;DR: Textile IR 是一种双向中间表示，通过七层验证阶梯集成CAD、仿真和生命周期评估，解决时尚设计中的复合不确定性问题，使设计师能同时权衡可持续性、可制造性和美学。


<details>
  <summary>Details</summary>
Motivation: 现有工具存在孤岛问题：模式软件仅保证可缝合输出但不理解悬垂性，物理仿真预测行为但无法自动修复模式。Textile IR 旨在通过语义粘合实现集成，解决复合不确定性（如材料测试误差、仿真近似和LCA数据库差距）。

Method: Textile IR 采用七层验证阶梯，从廉价语法检查到昂贵的物理验证，支持双向反馈（如仿真失败建议修改模式）。通过场景图表示，将时尚工程形式化为三个领域的约束满足问题。

Result: Textile IR 实现了双向反馈（如实时更新可持续性估计）和不确定性传播，并提出了六项研究重点。框架为时尚中小企业降低了专业工程需求。

Conclusion: Textile IR 提出了一种双向中间表示，通过七层验证阶梯连接制造验证的CAD、基于物理的仿真和生命周期评估，解决了时尚设计中的复合不确定性问题。该框架使工程约束可感知、可操作且即时反馈，帮助设计师同时权衡可持续性、可制造性和美学。

Abstract: We introduce Textile IR, a bidirectional intermediate representation that connects manufacturing-valid CAD, physics-based simulation, and lifecycle assessment for fashion design. Unlike existing siloed tools where pattern software guarantees sewable outputs but understands nothing about drape, and physics simulation predicts behaviour but cannot automatically fix patterns, Textile IR provides the semantic glue for integration through a seven-layer Verification Ladder -- from cheap syntactic checks (pattern closure, seam compatibility) to expensive physics validation (drape simulation, stress analysis). The architecture enables bidirectional feedback: simulation failures suggest pattern modifications; material substitutions update sustainability estimates in real time; uncertainty propagates across the pipeline with explicit confidence bounds. We formalise fashion engineering as constraint satisfaction over three domains and demonstrate how Textile IR's scene-graph representation enables AI systems to manipulate garments as structured programs rather than pixel arrays. The framework addresses the compound uncertainty problem: when measurement errors in material testing, simulation approximations, and LCA database gaps combine, sustainability claims become unreliable without explicit uncertainty tracking. We propose six research priorities and discuss deployment considerations for fashion SMEs where integrated workflows reduce specialised engineering requirements. Key contribution: a formal representation that makes engineering constraints perceptible, manipulable, and immediately consequential -- enabling designers to navigate sustainability, manufacturability, and aesthetic tradeoffs simultaneously rather than discovering conflicts after costly physical prototyping.

</details>


### [30] [StableDPT: Temporal Stable Monocular Video Depth Estimation](https://arxiv.org/abs/2601.02793)
*Ivan Sobko,Hayko Riemenschneider,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: StableDPT 通过集成时间模块和跨注意力机制，显著提升视频深度估计的时域稳定性，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 单图像深度估计模型应用于视频序列时，存在时域不稳定性和闪烁伪影问题，需要一种能够保持高效计算的同时提升时域稳定性的方法。

Method: StableDPT 基于现成的 Vision Transformer (ViT) 编码器和 Dense Prediction Transformer (DPT) 头，通过引入时间层和高效的跨注意力机制，整合关键帧信息以捕捉全局上下文和帧间关系。

Result: 在多个基准数据集上的评估表明，StableDPT 在时域一致性、性能表现和计算效率（2倍加速）方面均优于现有方法。

Conclusion: StableDPT 提出了一种新颖的方法，通过集成时间模块，显著提升了视频序列中深度估计的时域稳定性，同时保持了高效的计算性能。

Abstract: Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.

</details>


### [31] [Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining](https://arxiv.org/abs/2601.02806)
*Mingzhou Jiang,Jiaying Zhou,Nan Zeng,Mickael Li,Qijie Tang,Chao He,Huazhu Fu,Honghui He*

Main category: cs.CV

TL;DR: 本文提出了一种拓扑感知框架，通过TACM和TCPM机制解决H&E到IHC虚拟染色中的空间错位和局部变形问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: IHC染色过程复杂、耗时且昂贵，限制了其临床应用。虚拟染色提供了一种经济高效的替代方案，但相邻切片作为真实数据常导致弱配对数据，阻碍有效监督学习。

Method: 提出了一种拓扑感知一致性匹配（TACM）机制和拓扑约束病理匹配（TCPM）机制，通过图对比学习和拓扑扰动来学习鲁棒的匹配模式，并基于节点重要性对齐病理阳性区域。

Result: 在两个基准数据集上的四个染色任务中，该方法优于现有技术，实现了更高的生成质量和临床相关性。

Conclusion: 本文提出的拓扑感知框架在H&E到IHC虚拟染色任务中表现优异，生成质量高且临床相关性更强。

Abstract: Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.

</details>


### [32] [SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models](https://arxiv.org/abs/2601.02825)
*Ruiyang Zhang,Dongzhan Zhou,Zhedong Zheng*

Main category: cs.CV

TL;DR: SketchThinker-R1 通过草图式推理优化大型多模态模型的推理效率，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 受人类高效草图式推理启发，旨在减少长推理过程带来的计算开销。

Method: 方法包括三个阶段：草图模式冷启动阶段、SketchJudge 奖励模型训练阶段和草图思维强化学习阶段。

Result: 在四个基准测试中，推理令牌成本降低了64%以上，且不影响最终答案准确性。

Conclusion: SketchThinker-R1 通过引入草图式推理显著降低了大型多模态模型的计算开销，同时保持了答案准确性。

Abstract: Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.

</details>


### [33] [DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection](https://arxiv.org/abs/2601.02831)
*Yuetong Li,Qing Zhang,Yilin Zhao,Gongyang Li,Zeming Liu*

Main category: cs.CV

TL;DR: DGA-Net利用深度提示和跨模态图增强，显著提升伪装物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有伪装物体检测方法主要依赖稀疏提示（如点或框），未能充分利用深度信息。本文旨在通过密集深度提示和跨模态融合，提升检测精度。

Method: 提出DGA-Net框架，结合深度提示范式和跨模态图增强（CGE）模块，以及锚点引导细化（AGR）模块，通过全局锚点和非局部路径优化特征层次信息传递。

Result: 定量和定性实验表明，DGA-Net在伪装物体检测任务中优于现有最先进方法。

Conclusion: DGA-Net通过深度提示和跨模态图增强模块，显著提升了伪装物体检测的性能，超越了现有最先进方法。

Abstract: To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.

</details>


### [34] [Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection](https://arxiv.org/abs/2601.02837)
*Yuteng Liu,Duanni Meng,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: SEF-DETR通过频率引导和动态嵌入增强优化查询初始化，显著提升红外小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的检测器在红外小目标检测中因自注意力机制导致目标相关嵌入被背景特征淹没，性能下降。

Method: SEF-DETR包含三个核心模块：频率引导的补丁筛选（FPS）、动态嵌入增强（DEE）和可靠性一致性感知融合（RCF），分别用于抑制背景特征、增强目标感知表示和优化查询初始化。

Result: 在三个公开红外小目标数据集上的实验表明，SEF-DETR优于现有方法，实现了更高的检测性能。

Conclusion: SEF-DETR框架通过频率引导的补丁筛选、动态嵌入增强和可靠性一致性感知融合，显著提升了红外小目标检测的性能，为复杂背景下的低信噪比目标检测提供了高效解决方案。

Abstract: Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.

</details>


### [35] [Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion](https://arxiv.org/abs/2601.02881)
*Jakob Lønborg Christensen,Morten Rieger Hannemose,Anders Bjorholm Dahl,Vedrana Andersen Dahl*

Main category: cs.CV

TL;DR: 提出扩散式通用图像分割框架，通过位置感知调色板、2D格雷码排序和tanh激活优化性能，虽未超越掩码架构，但缩小差距并引入新能力。


<details>
  <summary>Details</summary>
Motivation: 旨在实现不依赖掩码框架的通用图像分割，通过扩散模型以整体方式预测完整分割。

Method: 提出了基于扩散的通用图像分割框架，采用位置感知调色板和2D格雷码排序优化性能，并引入tanh激活函数处理离散数据。通过优化扩散参数，sigmoid损失加权表现最佳，最终采用x-prediction。

Result: 模型缩小了与领先掩码架构的性能差距，展示了独特能力如模糊建模，且所有模型均从头训练。

Conclusion: 当前模型虽未超越领先的基于掩码的架构，但缩小了性能差距，并引入了如原则性模糊建模等独特能力。结合大规模预训练或可提示条件，可能实现更具竞争力的模型。

Abstract: This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.

</details>


### [36] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: TA-Prompting通过时间锚点和事件连贯采样策略提升VideoLLMs的时间感知能力，在密集视频描述任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在未修剪视频中难以精确识别事件边界，导致生成的描述缺乏时间基础。

Method: 提出TA-Prompting方法，利用时间锚点精确定位事件并提示VideoLLMs进行时间感知的视频事件理解；引入事件连贯采样策略选择具有时间连贯性和跨模态相似性的描述。

Result: 在基准数据集上的实验表明，TA-Prompting在密集视频描述、时刻检索和TemporalQA任务中表现优于现有VideoLLMs。

Conclusion: TA-Prompting方法通过引入时间锚点和事件连贯采样策略，显著提升了VideoLLMs在密集视频描述和时间理解任务中的性能，优于现有最先进方法。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [37] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: Zoom-IQA是一种基于视觉语言模型的图像质量评估方法，通过两阶段训练（监督微调+强化学习）提升推理可靠性，实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的IQA方法因视觉与文本线索整合能力有限，导致推理不可靠。Zoom-IQA旨在模拟关键认知行为（不确定性感知、区域推理和迭代优化）以解决这一问题。

Method: 提出Zoom-IQA模型，采用两阶段训练流程：1) 在GR-IQA数据集上进行监督微调（SFT），使模型能够基于关键区域进行评估；2) 通过强化学习（RL）进行动态策略探索，辅以KL-Coverage正则化器和渐进重采样策略。

Result: 实验表明，Zoom-IQA在鲁棒性、可解释性和泛化性方面表现优异。

Conclusion: Zoom-IQA在图像质量评估中展现出更高的鲁棒性、可解释性和泛化能力，并在下游任务（如图像修复）中验证了其有效性。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


### [38] [DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification](https://arxiv.org/abs/2601.02924)
*Aihua Zheng,Ya Gao,Shihao Li,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: DCG-ReID通过动态权重和特定融合策略，解决了多模态车辆ReID中的模态质量分布不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了平衡和不平衡质量分布数据的不同需求，难以解耦类内一致性和模态间异质性的冲突。

Method: 提出DCG-ReID方法，包括动态置信度解耦权重机制（DCDW）和两种融合策略（CFM和GFM），分别处理平衡和不平衡质量分布的数据。

Result: 在三个多模态ReID基准测试（WMVeID863、MSVR310、RGBNT100）上验证了方法的有效性。

Conclusion: DCG-ReID通过动态置信度解耦权重机制和两种场景特定融合策略，有效解决了多模态车辆ReID中的模态质量分布不平衡问题，提升了识别性能。

Abstract: Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.

</details>


### [39] [PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding](https://arxiv.org/abs/2601.02927)
*Iñaki Erregue,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: PrismVAU 是一种轻量级实时视频异常理解系统，通过两阶段方法和自动提示工程优化，无需复杂训练或外部模块，实现了高效且可解释的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有的 VAU 方法通常依赖微调的多模态大语言模型（MLLMs）或外部模块，这些方法引入了昂贵的注释、复杂的训练流程和高推理开销。PrismVAU 旨在解决这些问题，提供一种轻量级且高效的解决方案。

Method: PrismVAU 采用两阶段方法：1) 通过文本锚点的相似性计算帧级异常分数的粗粒度评分模块；2) 基于 MLLM 的细化模块，通过系统和用户提示对异常进行上下文描述。文本锚点和提示通过弱监督的自动提示工程（APE）框架优化。

Result: 在标准 VAD 基准测试中，PrismVAU 展示了竞争性的检测性能和可解释的异常描述，且无需依赖指令调优、帧级注释或外部模块。

Conclusion: PrismVAU 是一种轻量级且高效的系统，能够在实时视频异常理解（VAU）中提供竞争性的检测性能和可解释的异常描述，无需依赖指令调优、帧级注释或外部模块。

Abstract: Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

</details>


### [40] [HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection](https://arxiv.org/abs/2601.02928)
*Md. Asif Hossain,G M Mota-Tahrin Tayef,Nabil Subhan*

Main category: cs.CV

TL;DR: HybridSolarNet结合EfficientNet-B0和CBAM，通过焦点损失和余弦退火优化，在太阳能板故障检测中实现高准确率和实时性能。


<details>
  <summary>Details</summary>
Motivation: 传统太阳能板检测方法繁琐且易出错，现有深度学习模型要么过大不适合边缘设备，要么因学习技术不足导致准确率偏差。

Method: 提出了HybridSolarNet模型，集成EfficientNet-B0与CBAM，采用焦点损失和余弦退火技术，并在Kaggle数据集上进行了严格的拆分增强协议。

Result: 在5折分层交叉验证中，平均准确率达92.37% +/- 0.41，F1分数0.9226 +/- 0.39，模型仅需16.3 MB存储，推理速度54.9 FPS。

Conclusion: HybridSolarNet结合了EfficientNet-B0和CBAM，通过焦点损失和余弦退火优化，实现了高效的太阳能板故障检测，适用于实时无人机应用。

Abstract: Manual inspections for solar panel systems are a tedious, costly, and error-prone task, making it desirable for Unmanned Aerial Vehicle (UAV) based monitoring. Though deep learning models have excellent fault detection capabilities, almost all methods either are too large and heavy for edge computing devices or involve biased estimation of accuracy due to ineffective learning techniques. We propose a new solar panel fault detection model called HybridSolarNet. It integrates EfficientNet-B0 with Convolutional Block Attention Module (CBAM). We implemented it on the Kaggle Solar Panel Images competition dataset with a tight split-before-augmentation protocol. It avoids leakage in accuracy estimation. We introduced focal loss and cosine annealing. Ablation analysis validates that accuracy boosts due to added benefits from CBAM (+1.53%) and that there are benefits from recognition of classes with imbalanced samples via focal loss. Overall average accuracy on 5-fold stratified cross-validation experiments on the given competition dataset topped 92.37% +/- 0.41 and an F1-score of 0.9226 +/- 0.39 compared to baselines like VGG19, requiring merely 16.3 MB storage, i.e., 32 times less. Its inference speed measured at 54.9 FPS with GPU support makes it a successful candidate for real-time UAV implementation. Moreover, visualization obtained from Grad-CAM illustrates that HybridSolarNet focuses on actual locations instead of irrelevant ones.

</details>


### [41] [VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on](https://arxiv.org/abs/2601.02945)
*Xinyi Wei,Sijing Wu,Zitong Xu,Yunhao Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 构建首个专为VTON设计的质量评估数据集VTONQA，包含8,132张图像和24,396个MOSs，用于评估VTON模型和IQA指标，揭示现有方法不足。


<details>
  <summary>Details</summary>
Motivation: 现有VTON模型常出现服装变形和身体不一致等问题，需要可靠的VTON生成图像质量评估方法。

Method: 构建了VTONQA数据集，包含8,132张由11种代表性VTON模型生成的图像，并收集了24,396个平均意见分数（MOSs），覆盖三个评估维度（服装合身性、身体兼容性和整体质量）。

Result: 基于VTONQA，对VTON模型和多种图像质量评估（IQA）指标进行了基准测试，揭示了现有方法的局限性并突显了所提出数据集的价值。

Conclusion: VTONQA数据集及其基准测试为虚拟试穿（VTON）图像的质量评估提供了可靠基础，有助于质量评估方法和VTON模型的进一步发展。

Abstract: With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.

</details>


### [42] [LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing](https://arxiv.org/abs/2601.02987)
*Wingwa Fu,Takayuki Okatani*

Main category: cs.CV

TL;DR: LAMS-Edit通过潜在和注意力混合调度器，结合P2P框架，有效平衡内容保留与编辑应用，支持精确编辑和风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散模型的文本到图像编辑在内容保留与编辑应用之间的平衡问题，以及真实图像编辑的挑战。

Method: 利用反转过程中的中间状态（潜在表示和注意力图），通过加权插值和调度器控制，结合Prompt-to-Prompt（P2P）框架。

Result: 实验表明，LAMS-Edit在内容保留和编辑应用之间取得了有效平衡，并支持区域掩码精确编辑和LoRA风格迁移。

Conclusion: LAMS-Edit通过结合潜在表示和注意力图的加权插值，有效平衡了内容保留与编辑应用，并支持精确编辑和风格迁移。

Abstract: Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.

</details>


### [43] [ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation](https://arxiv.org/abs/2601.02988)
*Rianne Weber,Niels Rocholl,Max de Grauw,Mathias Prokop,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: ULS+是ULS模型的增强版，通过整合新数据集和优化输入尺寸，显著提升了病灶分割的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 原始ULS模型在CT扫描中基于点击点周围的兴趣体积（VOIs）进行全身病灶分割，新公开的数据集可进一步提升模型性能。

Method: ULS+整合了新增的公共数据集，并采用更小的输入图像尺寸，以提高准确性和推理速度。

Result: 在所有比较中，ULS+显著优于ULS，并在ULS23挑战赛测试阶段排行榜上排名第一。

Conclusion: ULS+通过数据驱动的更新和临床验证，为稳健且临床相关的病灶分割模型奠定了基础。

Abstract: In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference.
  We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.

</details>


### [44] [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/abs/2601.02991)
*Chengcheng Feng,Haojie Yin,Yucheng Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 针对CVQA任务，提出结合模块化CoT和强化学习的推理框架，显著提升小型MLLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准CoT在CVQA中因状态纠缠、虚假转换和探索效率低导致的性能下降问题，特别是小型模型在资源受限环境中的表现。

Method: 结合模块化CoT生成、GRPO强化微调和结构化奖励的漫画推理框架。

Result: 在五个挑战性基准测试中，3B模型超越现有方法，插件实验平均提升12.1%。

Conclusion: 提出的漫画推理框架显著提升了小型MLLM在CVQA任务中的性能，并在更广泛的幽默和抽象视觉推理任务中展现出优越性。

Abstract: Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.

</details>


### [45] [Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection](https://arxiv.org/abs/2601.03001)
*Li Wang,Boqi Li,Hang Chen,Xingjian Wu,Yichen Wang,Jiewen Tan,Xinyu Zhang,Huaping Liu*

Main category: cs.CV

TL;DR: RiSe框架通过风险意图选择性检测，优化车辆-基础设施协同感知的通信效率，显著降低带宽需求并保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决车辆-基础设施协同感知中的通信带宽与特征冗余之间的权衡问题，特别是现有方法在非关键背景区域传输冗余特征的效率低下。

Method: 提出了Risk-intent Selective detection (RiSe)框架，包括Potential Field-Trajectory Correlation Model (PTCM)和Intention-Driven Area Prediction Module (IDAPM)，实现语义选择性融合。

Result: 在DeepAccident数据集上的实验表明，RiSe将通信量降至全特征共享的0.71%，同时保持最先进的检测精度。

Conclusion: RiSe框架通过语义选择性融合方案，有效降低了通信带宽需求，同时保持了先进的检测精度，在带宽效率和感知性能之间建立了竞争优势。

Abstract: Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.

</details>


### [46] [ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios](https://arxiv.org/abs/2601.03011)
*Yihan Wei,Shenghai Yuan,Tianchen Deng,Boyang Lou,Enwen Hu*

Main category: cs.CV

TL;DR: ReCCur是一个低计算框架，通过多代理递归流程将噪声网络图像转化为可审计标签，适用于资源受限的corner-case场景处理。


<details>
  <summary>Details</summary>
Motivation: 解决罕见或极端场景（corner cases）因网络数据噪声大、标签脆弱及边缘部署限制而难以大规模整理的问题。

Method: ReCCur框架包括大规模数据获取与过滤、专家混合知识蒸馏以及区域证据VLM对抗性标签生成三个步骤，结合多种编码器和一致性验证机制。

Result: 在现实corner-case场景（如洪水车辆检测）中，ReCCur在消费级GPU上运行，持续提升纯度和可分离性，且需要最少人工监督。

Conclusion: ReCCur框架在资源有限的情况下，通过多代理递归流程，有效地将噪声网络图像转化为可审计的细粒度标签，为下游训练和评估提供了实用基础。

Abstract: Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.

</details>


### [47] [SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection](https://arxiv.org/abs/2601.03024)
*Kim Jun-Seong,Tae-Hyun Oh,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: SA-ResGS通过自增强点云和残差学习，提升主动场景重建中的视图选择效率和不确定性量化准确性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏和宽基线视图导致的监督不足问题，提升不确定性估计的可靠性和监督效果。

Method: 提出了一种自增强残差3D高斯泼溅框架，结合三角测量生成自增强点云，并引入残差学习策略以增强高斯泼溅的监督效果。

Result: 实验表明，SA-ResGS在重建质量和视图选择鲁棒性上优于现有基线方法。

Conclusion: SA-ResGS通过自增强点云和残差学习策略，显著提升了场景重建的质量和视图选择的鲁棒性，同时改善了不确定性量化的准确性。

Abstract: We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.

</details>


### [48] [Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries](https://arxiv.org/abs/2601.03030)
*Ali Kashefi*

Main category: cs.CV

TL;DR: 提出了两种基于PointNet的生成模型（Flow Matching PointNet和Diffusion PointNet），用于不规则几何形状的流体流动预测，相比传统方法更准确且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理不规则几何形状时的高频噪声问题，并简化网络架构。

Method: 结合PointNet与流匹配和扩散模型，直接在点云表示的计算域上操作，避免了均匀网格的局限性。

Result: 在圆柱绕流问题中，提出的框架在速度和压力场预测以及升力和阻力计算上表现更优。

Conclusion: Flow Matching PointNet和Diffusion PointNet在预测流体流动变量方面比传统方法更准确，且对不完整几何形状具有更强的鲁棒性。

Abstract: We present two novel generative geometric deep learning frameworks, termed Flow Matching PointNet and Diffusion PointNet, for predicting fluid flow variables on irregular geometries by incorporating PointNet into flow matching and diffusion models, respectively. In these frameworks, a reverse generative process reconstructs physical fields from standard Gaussian noise conditioned on unseen geometries. The proposed approaches operate directly on point-cloud representations of computational domains (e.g., grid vertices of finite-volume meshes) and therefore avoid the limitations of pixelation used to project geometries onto uniform lattices. In contrast to graph neural network-based diffusion models, Flow Matching PointNet and Diffusion PointNet do not exhibit high-frequency noise artifacts in the predicted fields. Moreover, unlike such approaches, which require auxiliary intermediate networks to condition geometry, the proposed frameworks rely solely on PointNet, resulting in a simple and unified architecture. The performance of the proposed frameworks is evaluated on steady incompressible flow past a cylinder, using a geometric dataset constructed by varying the cylinder's cross-sectional shape and orientation across samples. The results demonstrate that Flow Matching PointNet and Diffusion PointNet achieve more accurate predictions of velocity and pressure fields, as well as lift and drag forces, and exhibit greater robustness to incomplete geometries compared to a vanilla PointNet with the same number of trainable parameters.

</details>


### [49] [Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion](https://arxiv.org/abs/2601.03046)
*Han Zhang,Yanwei Wang,Fang Li,Hongjun Wang*

Main category: cs.CV

TL;DR: DFRCP是一种动态模糊鲁棒卷积金字塔，作为YOLOv11的插件，通过自适应注入模糊特征和高效CUDA并行处理，显著提升模糊条件下的检测准确性，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在抑制运动模糊时丢失判别性结构或增加延迟的问题，提升在资源受限设备上的模糊鲁棒检测性能。

Method: DFRCP通过结合大尺度和中尺度特征，并引入动态鲁棒开关单元自适应注入模糊特征，增强全局感知。模糊特征通过旋转和非线性插值多尺度特征合成，并通过透明卷积合并。

Result: 在模糊测试集上，DFRCP使YOLOv11的准确率比基线提高了约10.4%，且仅增加适度的训练时间开销。

Conclusion: DFRCP作为YOLOv11的插件升级，显著提升了在模糊条件下的检测准确性，同时保持了较低的延迟，适合边缘设备部署。

Abstract: Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.

</details>


### [50] [On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning](https://arxiv.org/abs/2601.03048)
*Siyi Lyu,Quan Liu,Feng Yan*

Main category: cs.CV

TL;DR: ViT在空间推理任务中的失败源于架构固有复杂性，无法高效处理非可解群的结构。


<details>
  <summary>Details</summary>
Motivation: 探索ViT在空间推理任务中系统性失败的原因，超越数据规模的解释。

Method: 将空间理解形式化为学习群同态，并分析ViT在非可解群（如SO(3)）上的计算复杂度。

Result: 证明恒定深度ViT在多项式精度下严格受限于TC0，无法高效处理NC1-完全问题。

Conclusion: Vision Transformers (ViTs) 在空间推理任务中存在固有复杂性限制，无法高效捕捉非可解空间结构。

Abstract: Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.

</details>


### [51] [IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation](https://arxiv.org/abs/2601.03054)
*Yankai Jiang,Qiaoru Li,Binlu Xu,Haoran Sun,Chao Ding,Junting Dong,Yuxiang Cai,Xuhong Zhang,Jianwei Yin*

Main category: cs.CV

TL;DR: IBISAgent是一种新型代理MLLM，通过多步决策和迭代优化提升医学图像分割性能，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM在像素级理解上存在两大挑战：一是隐含分割标记和联合调优导致灾难性遗忘和泛化能力受限；二是单次推理缺乏迭代优化能力。

Method: 提出了一种名为IBISAgent的新型代理MLLM，将分割任务重新定义为视觉中心的多步决策过程，支持掩码迭代优化。采用两阶段训练框架，包括冷启动监督微调和代理强化学习。

Result: 实验表明，IBISAgent在复杂医学参考和推理分割任务中表现优于现有SOTA方法。

Conclusion: IBISAgent通过多步视觉推理和迭代优化，显著提升了医学图像分割的性能，并在实验中超越了现有方法。

Abstract: Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.

</details>


### [52] [Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding](https://arxiv.org/abs/2601.03056)
*Zhen Wang,Jiaojiao Zhao,Qilong Wang,Yongfeng Dong,Wenlong Yu*

Main category: cs.CV

TL;DR: CFSG通过结构化解耦概念和特征空间，动态调整组件比例，显著提升细粒度域泛化性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 细粒度域泛化任务中，模型对细微线索过于敏感，抑制关键特征导致性能下降。受人类分类机制启发，需有效结合共同和特定属性。

Method: 提出Concept-Feature Structuralized Generalization (CFSG)，将概念和特征空间解耦为共同、特定和混杂三个结构化组件，并引入自适应机制动态调整比例。

Result: 在三个单源基准数据集上，CFSG平均性能提升9.87%，优于现有方法3.08%。可解释性分析验证了多粒度结构化知识的有效整合。

Conclusion: CFSG模型通过显式解耦概念和特征空间，并引入自适应机制动态调整组件比例，显著提升了细粒度域泛化任务的性能，实验验证了其有效性。

Abstract: Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.

</details>


### [53] [Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA](https://arxiv.org/abs/2601.03073)
*Tong Wu,Thanet Markchom*

Main category: cs.CV

TL;DR: 提出多代理LLM框架解决卡通VQA挑战，通过视觉、语言和批评代理协作，在Pororo和Simpsons数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 标准大型语言模型（LLMs）在自然图像上训练，无法充分应对卡通图像中的视觉抽象和叙事驱动上下文挑战。

Method: 采用多代理LLM框架，包含视觉代理、语言代理和批评代理，协作进行结构化推理。

Result: 在Pororo和Simpsons两个卡通VQA数据集上系统评估，实验结果表明各代理对最终预测的贡献，深入理解了卡通VQA和多模态推理中基于LLM的多代理行为。

Conclusion: 该论文提出了一个多代理LLM框架，专门针对卡通图像的视觉问答任务，通过视觉代理、语言代理和批评代理的协作，整合视觉线索和叙事上下文，有效解决了卡通图像中的视觉抽象和叙事驱动上下文挑战。

Abstract: Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.

</details>


### [54] [LesionTABE: Equitable AI for Skin Lesion Detection](https://arxiv.org/abs/2601.03090)
*Rocio Mexia Diaz,Yasmin Greenway,Petru Manescu*

Main category: cs.CV

TL;DR: LesionTABE是一个公平性框架，通过对抗性去偏和基础模型嵌入，显著提升了AI在皮肤病诊断中的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: AI在皮肤病学中的临床应用因肤色偏见而受限，尤其是在深色皮肤上诊断性能下降。

Method: LesionTABE框架结合了对抗性去偏技术和皮肤病学特定的基础模型嵌入。

Result: LesionTABE在多个数据集上评估，公平性指标比ResNet-152基线提高了25%以上，同时诊断准确性也有所提升。

Conclusion: LesionTABE通过结合对抗性去偏和皮肤病学特定的基础模型嵌入，显著提高了公平性指标，同时提升了整体诊断准确性，为临床AI的公平应用迈出了重要一步。

Abstract: Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.

</details>


### [55] [Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs](https://arxiv.org/abs/2601.03100)
*Chenchen Lin,Sanbao Su,Rachel Luo,Yuxiao Chen,Yan Wang,Marco Pavone,Fei Miao*

Main category: cs.CV

TL;DR: TGIF通过动态融合视觉编码器的多层次特征，显著减少MLLMs的幻觉现象，并在多个任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs通常仅利用视觉编码器的单一深层特征，忽视了其丰富的层次结构，导致模型依赖语言先验而非图像证据，产生视觉未基础的幻觉。现有方法多集中于文本侧，未充分利用视觉层的多层次特征。

Method: TGIF是一种轻量级模块，将视觉编码器的各层视为深度方向的“专家”，并根据提示动态预测视觉特征的融合方式。该方法遵循直接外部融合原则，无需更新视觉编码器，且计算开销极小。

Result: 集成TGIF的LLaVA-1.5-7B在幻觉、OCR和VQA基准测试中表现一致提升，同时在ScienceQA、GQA和MMBench上保持或提升了性能。

Conclusion: TGIF（文本引导的层间融合）通过动态融合视觉编码器的多层次特征，显著增强了多模态大语言模型（MLLMs）的视觉基础能力，减少了幻觉现象，并在多个基准测试中表现优异。

Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.

</details>


### [56] [LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition](https://arxiv.org/abs/2601.03124)
*B. M. Shahria Alam,Md. Nasim Ahmed*

Main category: cs.CV

TL;DR: 研究通过Xception模型和对抗训练实现了96.23%准确率的葡萄叶病害分类，并部署了带热图可视化的Web应用。


<details>
  <summary>Details</summary>
Motivation: 葡萄叶病害会降低作物产量和品质，及时检测对提升农业生产力至关重要。

Method: 使用InceptionV3和Xception两种预训练模型，数据集经过严格预处理后按70%训练、20%验证、10%测试划分。对抗训练用于增强鲁棒性，Grad-CAM用于确认病害区域。

Result: Xception模型表现优异，准确率达96.23%，优于InceptionV3。

Conclusion: 研究成功开发了一个基于Xception模型的葡萄叶病害分类系统，准确率达96.23%，并通过对抗训练和Grad-CAM增强了模型的鲁棒性和可解释性。最终部署了一个带有热图可视化功能的Web应用。

Abstract: Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.

</details>


### [57] [Unified Thinker: A General Reasoning Modular Core for Image Generation](https://arxiv.org/abs/2601.03127)
*Sashuai Zhou,Qiang Zhou,Jijin Hu,Hanqing Yang,Yue Cao,Junpeng Ma,Yinchao Ma,Jun Song,Tiezheng Ge,Cheng Yu,Bo Zheng,Zhou Zhao*

Main category: cs.CV

TL;DR: Unified Thinker通过解耦思考与生成模块，结合两阶段训练，显著提升图像生成的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在逻辑密集型指令遵循上存在不足，闭源系统表现优异，开源模型需提升可执行推理能力。

Method: 提出Unified Thinker架构，采用两阶段训练范式：先构建结构化规划接口，再通过强化学习将策略与像素级反馈结合。

Result: Unified Thinker在图像推理和生成质量上取得显著提升。

Conclusion: Unified Thinker架构通过解耦思考与生成模块，显著提升了图像生成中的逻辑推理能力，并在文本到图像生成和图像编辑任务中表现出色。

Abstract: Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.

</details>


### [58] [LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images](https://arxiv.org/abs/2601.03163)
*Matěj Pekár,Vít Musil,Rudolf Nenutil,Petr Holub,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LSP-DETR 是一种端到端的细胞核分割方法，通过轻量级 Transformer 和星凸多边形表示，显著提升了效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在计算病理学中处理大尺寸图像时效率低、依赖后处理的问题。

Method: 使用轻量级 Transformer 和线性复杂度处理大尺寸图像，通过星凸多边形表示细胞核，并采用新颖的径向距离损失函数。

Result: 在 PanNuke 和 MoNuSeg 数据集上表现出优异的泛化能力和效率，速度比现有最快方法快五倍以上。

Conclusion: LSP-DETR 提供了一种高效、精确的细胞核实例分割方法，显著提升了计算病理学中的处理速度和泛化能力。

Abstract: Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.

</details>


### [59] [DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation](https://arxiv.org/abs/2601.03178)
*Jiajun jiao,Haowei Zhu,Puyuan Yang,Jianghui Wang,Ji Liu,Ziqiong Liu,Dong Li,Yuejian Fang,Junhai Yong,Bin Wang,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出DiffBench和DiffAgent框架，通过LLM驱动自动生成和评估扩散模型加速代码，DiffAgent在闭环工作流程中优化策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的多步推理过程导致计算开销大，阻碍实际部署，需要加速技术但如何结合多种加速方法仍是挑战。

Method: 提出了DiffBench基准测试和DiffAgent代理，DiffAgent采用闭环工作流程，结合规划、调试和代码生成组件，并通过遗传算法从执行环境中提取性能反馈。

Result: DiffBench实现了三阶段自动化评估流程，DiffAgent能生成任意扩散模型的最优加速策略和代码。

Conclusion: DiffAgent显著优于现有LLM，在生成有效的扩散加速策略方面表现出色，DiffBench为生成的代码提供了全面评估。

Abstract: Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.

</details>


### [60] [AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2601.03191)
*Anees Ur Rehman Hashmi,Numan Saeed,Christoph Lippert*

Main category: cs.CV

TL;DR: AnatomiX是一种专为解剖学基础胸部X射线解释设计的多任务多模态大语言模型，显著提升了性能和解剖学理解。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态医学大语言模型在空间推理和解剖学理解上的不足，尤其是解剖对应关系的建立问题。

Method: 采用两阶段方法：首先识别解剖结构并提取特征，然后利用大型语言模型执行多样化的下游任务。

Result: 在多个基准测试中，AnatomiX展现出卓越的解剖学推理能力，性能提升显著。

Conclusion: AnatomiX显著提升了胸部X射线解释中的解剖学推理能力，并在多个任务上实现了超过25%的性能提升。

Abstract: Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix

</details>


### [61] [UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision](https://arxiv.org/abs/2601.03193)
*Ruiyan Han,Zhen Fang,XinYu Sun,Yuchen Ma,Ziheng Wang,Yu Zeng,Zehui Chen,Lin Chen,Wenxuan Huang,Wei-Jie Xu,Yi Cao,Feng Zhao*

Main category: cs.CV

TL;DR: UniCorn通过自监督角色划分框架显著提升多模态模型的生成质量，实验证明其在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决统一多模态模型在跨模态理解与生成之间的差距，即传导性失语现象。

Method: 提出了UniCorn框架，将单一UMM划分为Proposer、Solver和Judge三个角色，通过自生成高质量交互和认知模式重构，将潜在理解转化为显式生成信号。

Result: 在六个通用图像生成基准测试中全面显著超越基础模型，尤其在TIIF、DPG、CompBench和UniCycle上达到SOTA性能，WISE和OneIG上分别提升+5.0和+6.5。

Conclusion: UniCorn框架通过自监督学习和角色划分显著提升了统一多模态模型的生成能力，同时保持了强大的理解能力。

Abstract: While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.

</details>


### [62] [LTX-2: Efficient Joint Audio-Visual Foundation Model](https://arxiv.org/abs/2601.03233)
*Yoav HaCohen,Benny Brazowski,Nisan Chiprut,Yaki Bitterman,Andrew Kvochko,Avishai Berkowitz,Daniel Shalem,Daphna Lifschitz,Dudu Moshe,Eitan Porat,Eitan Richardson,Guy Shiran,Itay Chachy,Jonathan Chetboun,Michael Finkelson,Michael Kupchick,Nir Zabari,Nitzan Guetta,Noa Kotler,Ofir Bibi,Ori Gordon,Poriya Panet,Roi Benita,Shahar Armon,Victor Kulikov,Yaron Inger,Yonatan Shiftan,Zeev Melumian,Zeev Farbman*

Main category: cs.CV

TL;DR: LTX-2是一个开源基础模型，通过非对称双流Transformer统一生成高质量视听内容，显著优于现有文本到视频模型，且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型缺乏音频提供的语义、情感和氛围线索，LTX-2旨在统一生成高质量的视听内容。

Method: 采用非对称双流Transformer架构，包含14B参数视频流和5B参数音频流，通过双向视听交叉注意力层、时间位置嵌入和跨模态AdaLN实现共享时间步条件。

Result: LTX-2在开源系统中实现了最先进的视听质量和提示遵循，同时以较低的计算成本和推理时间达到与专有模型相当的效果。

Conclusion: LTX-2作为开源基础模型，在生成高质量、时间同步的视听内容方面表现出色，计算成本和推理时间显著低于专有模型，且所有模型权重和代码均已公开。

Abstract: Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.

</details>


### [63] [A Versatile Multimodal Agent for Multimedia Content Generation](https://arxiv.org/abs/2601.03250)
*Daoan Zhang,Wenlin Yao,Xiaoyang Wang,Yebowen Hu,Jiebo Luo,Dong Yu*

Main category: cs.CV

TL;DR: 提出MultiMedia-Agent，通过代理系统解决AIGC多模态集成问题，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC模型在多模态集成方面存在局限，无法有效完成端到端的复杂任务。基于代理系统的兴起为解决这一问题提供了可能。

Method: 采用基于代理的系统，包括数据生成管道、内容创作工具库和偏好对齐评估指标。引入技能获取理论建模训练数据筛选和代理训练，设计了两阶段关联策略（自关联和模型偏好关联）进行计划优化，并通过三阶段方法（基础/成功计划微调和偏好优化）训练代理。

Result: 实验结果表明，MultiMedia-Agent能有效生成更优质的多媒体内容。

Conclusion: 论文提出的MultiMedia-Agent在多媒体内容生成方面表现出色，相比现有模型能产生更优质的多模态输出。

Abstract: With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.

</details>


### [64] [InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields](https://arxiv.org/abs/2601.03252)
*Hao Yu,Haotong Lin,Jiawei Wang,Jiaxin Li,Yida Wang,Xueyang Zhang,Yue Wang,Xiaowei Zhou,Ruizhen Hu,Sida Peng*

Main category: cs.CV

TL;DR: InfiniDepth利用神经隐式场实现任意分辨率的深度估计，在合成和真实数据集上表现优异，尤其擅长细节恢复，并提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法局限于离散图像网格，限制了输出分辨率的可扩展性和几何细节恢复。

Method: 使用局部隐式解码器，通过查询连续2D坐标的深度，实现任意分辨率和精细深度估计。

Result: 在合成和真实世界基准测试中，InfiniDepth在相对和度量深度估计任务上均达到最先进性能，尤其在细节丰富区域表现优异。同时，它在大视角变化下的新视角合成任务中也能产生高质量结果，减少空洞和伪影。

Conclusion: InfiniDepth通过神经隐式场表示深度，实现了任意分辨率和精细几何细节的深度估计，在合成和真实世界基准测试中均表现出色，尤其在细节丰富区域。此外，它还能在大视角变化下提升新视角合成的质量。

Abstract: Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.

</details>


### [65] [Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training](https://arxiv.org/abs/2601.03256)
*Hexiao Lu,Xiaokun Sun,Zeyu Cai,Hao Guo,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: Muses是一种无训练的3D生物生成方法，通过骨骼驱动设计、体素组装和图像引导纹理生成，实现高质量、风格一致的3D资产。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖部分感知优化、手动组装或2D图像生成，常因复杂的部分级操作和域外生成限制导致不现实或不连贯的3D资产。Muses旨在通过骨骼基础解决这些问题。

Method: Muses采用无训练方法，基于3D骨骼结构，通过图约束推理构建骨架，指导体素组装和图像引导的外观建模，实现结构感知的设计、组合与生成。

Result: 实验证明Muses在视觉保真度、文本描述对齐及灵活3D编辑方面达到最先进水平。

Conclusion: Muses通过创新的骨骼驱动方法，实现了高质量、风格一致的3D生物生成，并在视觉保真度和文本描述对齐方面表现出色，展现了灵活的3D编辑潜力。

Abstract: We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 该论文提出了一种改进的XRL框架，通过生成透明规则和自动评估，提升了文本解释的质量和可信度，适用于工业场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有XRL技术中文本解释正确性不足和评估局限的问题，提升解释的可信度和实用性。

Method: 提出了一种新颖的XRL框架，结合专家知识和自动谓词生成器，利用LLM和聚类技术生成文本解释，并将其转化为透明规则进行质量评估。

Result: 在三个开源环境和电信用例中验证了框架的可重复性和工业适用性，生成的透明规则在特定任务上表现良好。

Conclusion: 该框架通过生成透明规则并改进其质量，解决了现有方法的局限性，并在特定任务上实现了满意性能，为XRL领域提供了系统性、定量化的评估方法。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [67] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: SimpleMem通过语义压缩和高效检索，显著提升LLM代理的记忆管理效率，平衡性能与成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长期交互中因冗余或高token成本导致效率低下的问题。

Method: 提出了基于语义无损压缩的三阶段流程：1) 语义结构化压缩；2) 递归记忆整合；3) 自适应查询感知检索。

Result: 在基准数据集上，方法在准确性、检索效率和推理成本上均优于基线，F1平均提升26.4%，推理token消耗减少高达30倍。

Conclusion: SimpleMem框架通过语义无损压缩和高效的三阶段流程，显著提升了长期交互中的记忆管理效率，实现了性能与效率的优越平衡。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [68] [Orchestral AI: A Framework for Agent Orchestration](https://arxiv.org/abs/2601.02577)
*Alexander Roman,Jacob Roman*

Main category: cs.AI

TL;DR: Orchestral是一个统一LLM代理开发的Python框架，解决了跨提供商的API碎片化问题，支持高级功能且保持简洁和模块化。


<details>
  <summary>Details</summary>
Motivation: LLM代理框架的快速扩散导致开发者面临供应商锁定或复杂多包生态系统的选择问题，跨提供商集成工具调用存在API碎片化、消息格式不兼容等挑战。

Method: Orchestral是一个轻量级的Python框架，定义了消息、工具和LLM使用的通用表示，支持自动工具模式生成和同步执行模型。

Result: Orchestral实现了跨提供商的无缝操作，消除了手动格式转换，支持高级代理功能如工具调用、上下文压缩等，同时保持架构的模块化和可扩展性。

Conclusion: Orchestral框架通过提供统一的类型安全接口，解决了LLM代理框架中的碎片化问题，支持跨提供商的无缝操作，同时保持了科学计算和生产部署所需的简洁性。

Abstract: The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.

</details>


### [69] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 本文研究了设备端AI模型在实际部署中的关键问题，通过构建LiveChatBench基准并实验验证，证明了所提方法在特定任务上能达到与商业模型相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管设备端AI模型效率高，但关于其实际部署方面的研究较少，例如设备的CPU利用率和热条件。本文旨在填补这一研究空白，探讨设备端模型在现实服务中的部署问题。

Method: 通过大量实验，本文研究了部署设备端模型时必须解决的两个关键问题：设备端模型的选择与每个模型的资源消耗，以及设备端模型在领域适应方面的能力和潜力。为此，研究聚焦于实时聊天消息翻译任务，并手动构建了LiveChatBench基准，包含1000个韩英平行句对。

Result: 在五种移动设备上的实验表明，所提出的方法在特定任务上达到了与商业模型如GPT-5.1相当的性能。

Conclusion: 本文的结论是，尽管在高度受限的部署环境和模型选择方面需要仔细考虑，但所提出的方法在特定任务上仍能达到与GPT-5.1等商业模型相媲美的性能。研究结果为设备端AI社区提供了有意义的见解。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [70] [AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents](https://arxiv.org/abs/2601.02643)
*Mehmet Kurmaz*

Main category: cs.AI

TL;DR: 本研究提出偏好感知的查询修复方法，通过LLM推断约束重要性，实验表明局部加权偏好对齐最佳，全局加权约束松弛最优。引入AWARE-US基准测试评估代理表现。


<details>
  <summary>Details</summary>
Motivation: 工具调用对话代理在查询结构化数据库时经常面临两个相互关联的问题：欠规范和不可行性。现有方法通常简单地返回“无结果”或使用临时规则放松约束，这可能违背用户意图。本研究旨在通过偏好感知的查询修复来解决这些问题。

Method: 本研究提出了三种基于LLM的方法来推断对话中的约束重要性：局部加权、全局一次性加权和成对排序。实验评估了这些方法在偏好对齐和正确约束松弛上的表现。

Result: 实验结果显示，局部加权方法在偏好对齐上表现最佳，全局加权方法在正确约束松弛上表现最好。AWARE-US基准测试的引入为评估代理在解决不可行性时的表现提供了标准。

Conclusion: 本研究提出了一种偏好感知的查询修复方法，通过三种LLM-based方法推断对话中的约束重要性，实验表明局部加权方法在偏好对齐上表现最佳，全局加权方法在正确约束松弛上表现最好。同时，AWARE-US基准测试的引入为需要根据人物角色偏好解决查询不可行性的代理提供了评估标准。

Abstract: Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with "no results" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.

</details>


### [71] [Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks](https://arxiv.org/abs/2601.02666)
*Hadi Partovi Aria,Zhe Xu*

Main category: cs.AI

TL;DR: GTL-CIRL框架结合因果图时序逻辑与强化学习，提升样本效率和可解释性，适用于复杂网络系统。


<details>
  <summary>Details</summary>
Motivation: 传统的黑盒强化学习方法忽视了局部变化在网络结构中的传播，导致样本效率低且缺乏可解释性。

Method: 该方法通过鲁棒性塑造奖励、收集反例并使用高斯过程驱动的贝叶斯优化来精炼参数化的因果模板，有效捕捉系统动态的时空相关性。

Result: 在基因和电力网络的案例研究中，GTL-CIRL相比标准强化学习基线表现出更快的学习速度和更清晰、可验证的行为。

Conclusion: GTL-CIRL框架通过结合因果图时序逻辑和强化学习，显著提升了决策任务中的样本效率和可解释性，适用于基因和电力网络等复杂系统。

Abstract: Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.

</details>


### [72] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: HAPO框架通过动态归因和语义单元优化，解决了提示漂移问题，提升了提示工程的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法存在提示漂移和可解释性下降的问题，需要一种更高效且可扩展的优化方法。

Method: 提出了分层归因提示优化（HAPO）框架，包括动态归因机制、语义单元优化和多模态友好进展。

Result: HAPO在单/多图像QA和复杂任务分析中表现优异，优于其他自动化提示优化方法。

Conclusion: HAPO框架通过动态归因机制、语义单元优化和多模态友好进展，显著提升了提示优化的效率和可扩展性，为可扩展的提示工程建立了新模式。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [73] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: MultiSessionCollab基准评估智能体如何学习用户偏好以提升多会话协作质量，记忆机制显著改善任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着对话智能体与用户协作经验的积累，适应用户偏好对建立长期关系和提升协作质量至关重要。

Method: 提出了MultiSessionCollab基准，开发了配备记忆的长期协作智能体，并利用用户模拟器行为生成学习信号。

Result: 实验表明，记忆机制显著提升了长期协作效果，用户研究也证实了记忆对实际用户体验的改善。

Conclusion: 配备记忆的智能体在长期协作中表现更佳，提高了任务成功率、交互效率，并减少了用户努力。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [74] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: Time-Scaling optimizes reasoning over time in cognitive agents, enhancing deep problem-solving without increasing static model parameters, inspired by human sequential reasoning.


<details>
  <summary>Details</summary>
Motivation: The convergence of AI paradigms into cognitive agents with closed-loop capabilities highlights the need for systematic extension and optimization of reasoning over time, akin to human sequential reasoning under cognitive constraints.

Method: Architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control.

Result: Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories, though methods like CoT and ToT have limitations in search completeness and efficiency.

Conclusion: Time-Scaling represents a critical frontier for enhancing deep reasoning and problem-solving in cognitive agents, positioning explicit temporal reasoning management as foundational.

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [75] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 论文探讨了LLM向自主代理的转变，提出了一个集成框架并识别了关键研究优先级，强调需在技术稳健性和伦理保障方面同步进展。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM从被动文本生成器向自主、目标驱动系统的转变，揭示这一转变在人工智能领域的根本性意义。

Method: 通过分析LLM架构的演进，提出一个集成框架，描述感知、记忆、规划和工具执行等核心组件如何将LLM与自主行为连接起来。

Result: 提出了三个贡献：(1) LLM能力如何通过推理-行动-反思循环扩展至代理性；(2) 连接LLM与自主行为的核心组件框架；(3) 对应用及安全、对齐、可靠性和可持续性等挑战的批判性评估。

Conclusion: 论文强调，负责任地推进LLM向自主代理的转变需要同时提升技术稳健性、可解释性和伦理保障，以在实现潜力的同时减少错位和意外后果的风险。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [76] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT结合LLM与视觉模型，通过分层结构减少幻觉，在多样化查询中表现优异（90.71%匹配率），适用于遥感决策。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法缺乏处理多样化现实查询的灵活性和全面分析的智能性。

Method: 采用分层结构减少幻觉问题，结合LLM与视觉基础模型构建ChangeGPT框架，并在包含140个问题的精选数据集上进行评估。

Result: ChangeGPT（尤其是GPT-4-turbo后端）在工具选择能力（精确率/召回率）和整体查询准确率（匹配率）上表现优异，达到90.71%的匹配率，特别擅长处理需要多步推理和强大工具选择能力的变化相关查询。

Conclusion: ChangeGPT通过集成大型语言模型（LLM）与视觉基础模型，提供了一个智能、适应性强且支持多类型变化分析的解决方案，显著提升了遥感应用中的决策能力。

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [77] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: HAL框架通过可解释的奖励信号对齐语言模型与对话人类相似性，提升模型的人类似表现。


<details>
  <summary>Details</summary>
Motivation: 对话人类相似性在人类-AI交互中至关重要，但难以定义、测量和优化，现有改进多依赖规模或广泛监督训练。

Method: HAL从对比对话数据中提取显式对话特征，将其组合为紧凑的标量分数，并作为透明奖励信号用于对齐。

Result: 人类评估显示，HAL对齐的模型在对话中更频繁被视为人类似。

Conclusion: HAL框架通过可解释的数据驱动奖励，成功将语言模型与对话人类相似性对齐，且不影响模型整体性能。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [78] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 该论文提出了一种结合因果推理和知识图谱的AI系统，用于医学系统综述，显著提高了准确性并消除了幻觉，展示了在高风险医疗中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 动机是解决当前AI在系统综述任务中的幻觉问题，尤其是在影响患者护理的错误不可接受的情况下，提供一种更可靠的方法。

Method: 方法是通过整合显式因果推理与双层知识图谱，构建了一个因果图增强的检索生成系统，强制实施证据优先协议，确保每个因果主张都能追溯到检索到的文献，并自动生成有向无环图以可视化干预-结果路径。

Result: 结果显示，在234篇痴呆症运动摘要的评估中，CausalAgent实现了95%的准确率、100%的检索成功率和零幻觉，而基线AI的准确率为34%且幻觉率为10%。

Conclusion: 该论文的结论是，其提出的因果图增强检索生成系统在医学系统综述任务中表现出高准确性和零幻觉，展示了可信赖医疗AI和因果推理在高风险医疗中的潜力。

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [79] [Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs](https://arxiv.org/abs/2601.02818)
*Muzhen Zhang,Yujie Cheng,Zhanxiang Lei*

Main category: cs.AI

TL;DR: 首次在储层空间预测中引入量子增强的LSTM模型（QLSTMA），通过量子电路提升渗透率预测精度，实验显示8量子比特模型性能最优。


<details>
  <summary>Details</summary>
Motivation: 渗透率的广泛范围和高变异性使现有方法难以提供可靠预测，因此需要开发更先进的模型。

Method: 提出了两种量子化结构：QLSTMA-SG和QLSTMA-IG，通过变分量子电路（VQCs）结合量子纠缠和叠加原理，增强对复杂地质参数（如渗透率）的预测能力。

Result: 8量子比特的QLSTMA-IG模型显著优于传统LSTMA，MAE降低19%，RMSE降低20%，尤其在复杂测井数据区域表现突出。

Conclusion: 本研究为量子-经典混合神经网络在储层预测中的应用奠定了基础，展示了增加量子比特数能进一步提升模型精度，尽管目前仍依赖经典模拟。

Abstract: Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.

</details>


### [80] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 提出神经符号深度强化学习方法，利用逻辑规则加速学习并提升泛化能力，实验验证其在复杂任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习算法需要大量训练数据且泛化能力有限，尤其在面对超出小规模训练场景的挑战时表现不佳。

Method: 通过将部分策略表示为逻辑规则，并在训练过程中进行在线推理，采用两种机制（探索时偏置动作分布和利用时重新缩放Q值）来引导学习。

Result: 在网格世界环境的挑战性变体中，该方法在完全可观测和部分可观测设置下均优于现有奖励机器基线。

Conclusion: 该论文提出了一种结合神经符号知识的深度强化学习方法，显著提高了样本效率和泛化能力，尤其在稀疏奖励环境和长规划视野任务中表现优异。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [81] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: M3MAD-Bench是一个标准化基准，用于评估多智能体辩论方法，覆盖多领域、多模态和多维度指标，提供系统性性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有MAD研究存在两个基本限制：评估在碎片化和不一致的设置下进行，阻碍了公平比较，且主要局限于依赖纯文本输入的单模态场景。

Method: 引入了M3MAD-Bench，一个统一且可扩展的基准，用于评估跨多领域任务、多模态输入和多维度指标的MAD方法。

Result: 在九种基础模型上评估MAD方法，涵盖了不同架构、规模和模态能力，提供了关于MAD在纯文本和多模态场景下有效性、鲁棒性和效率的系统性见解。

Conclusion: M3MAD-Bench提供了一个可靠的基础，用于未来在标准化MAD评估方面的研究。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [82] [SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection](https://arxiv.org/abs/2601.02871)
*Zhiyong Cao,Dunqiang Liu,Qi Dai,Haojun Xu,Huaiyan Xu,Huan He,Yafei Liu,Siyuan Liu,XiaoLin Lin,Ke Ma,Ruqian Shi,Sijia Yao,Hao Wang,Sicheng Zhou*

Main category: cs.AI

TL;DR: SimRPD是一个三阶段框架，通过用户模拟器生成数据、多维评估筛选数据，并训练对话代理，显著提升了招聘场景中的对话代理性能。


<details>
  <summary>Details</summary>
Motivation: 解决目标导向领域特定训练数据稀缺的问题，以提升招聘场景中任务导向主动对话代理的性能。

Method: 提出了一个三阶段框架SimRPD：1) 开发高保真用户模拟器合成大规模对话数据；2) 引入基于意图链(CoI)的多维评估框架筛选高质量数据；3) 在筛选后的数据集上训练招聘主动对话代理。

Result: 实验证明SimRPD在真实招聘场景中优于现有模拟器数据选择策略。

Conclusion: SimRPD框架在真实招聘场景中表现出色，超越了现有基于模拟器的数据选择策略，展示了其在工业部署中的实用价值以及在其他业务导向对话场景中的潜在适用性。

Abstract: Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.

</details>


### [83] [ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal是一个结合结构化探索、批判性改进和跨问题记忆的框架，在多步推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多步推理中的挑战，尤其是在复杂领域如数学和创意写作中，现有方法缺乏对替代解决方案路径的结构化探索和跨问题的持续学习。

Method: ReTreVal整合了Tree-of-Thoughts探索、自我改进、基于LLM的批判评分和反思记忆，构建了一个基于问题复杂度的自适应深度推理树，每个节点通过LLM生成的反馈进行迭代自我批判和改进。

Result: 在500个数学问题和创意写作任务中，ReTreVal使用Qwen 2.5 7B作为底层LLM，表现优于ReAct、Reflexion和Self-Refine。

Conclusion: ReTreVal框架通过结合结构化探索、自我批判驱动的改进和跨问题记忆，在数学和创意写作任务中持续优于现有方法，特别适用于需要探索性推理、严格验证和知识迁移的任务。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [84] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 研究发现LLMs在逻辑推理中存在‘逻辑相变’，提出‘神经符号课程调谐’框架，显著提升高复杂度下的推理表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在符号逻辑推理中的能力，尤其是在高复杂度下的表现，为解决数学推理和法律判决等高风险领域的可靠决策提供支持。

Method: 研究通过系统性分析逻辑复杂性增加下的推理表现，发现了‘逻辑相变’现象，并设计了‘神经符号课程调谐’框架，通过自适应对齐自然语言与逻辑符号来优化训练动态。

Result: 在五个基准测试中，该方法平均提升了朴素提示和思维链提示的准确性（分别为+1.26和+3.95），并增强了未见逻辑组合的泛化能力。

Conclusion: 该研究揭示了LLMs在逻辑推理中的‘逻辑相变’现象，并提出了一种名为‘神经符号课程调谐’的框架，有效缓解了高复杂度下的逻辑推理崩溃问题，显著提升了推理准确性和泛化能力。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [85] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: BoT-R 通过联合处理相关查询，利用跨实例信号提升推理性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理系统独立处理查询，忽略了共享推理模式和一致性约束等有价值的跨实例信号。

Method: 提出了 Batch-of-Thought (BoT) 方法，通过多代理反射架构（BoT-R）进行联合评估，利用跨实例信号进行错误检测和高质量推理模板识别。

Result: 在三个模型家族和六个基准测试中，BoT-R 显著提高了准确性和置信度校准，同时将推理成本降低了高达 61%。

Conclusion: Batch-of-Thought (BoT) 方法通过联合处理相关查询，实现了跨实例学习，显著提升了大型语言模型推理系统的准确性、置信度校准，并降低了推理成本。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [86] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出RationaleTS方法，通过理据引导的上下文学习提升时间序列推理，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型因缺乏连接时间观察与下游结果的理据先验，导致依赖表面模式匹配而非原则性推理，因此提出基于理据的上下文学习方法。

Method: 首先诱导标签条件化的理据（从可观察证据到潜在结果的推理路径），然后设计混合检索机制（平衡时间模式和语义上下文）以检索相关理据先验，用于新样本的上下文推理。

Result: 在三个领域的时间序列推理任务中，RationaleTS表现出显著的有效性和效率。

Conclusion: RationaleTS方法通过引入基于理据的上下文学习，显著提升了多模态大语言模型在时间序列推理任务中的表现，并通过实验验证了其有效性和效率。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [87] [Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks](https://arxiv.org/abs/2601.03062)
*Qusai Khaled,Pasquale De Marinis,Moez Louati,David Ferras,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的模糊图神经网络（FGENConv），用于水分配网络的泄漏检测和定位，平衡了精确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在捕捉传感器数据的时空依赖性方面表现出色，但其黑盒性质及缺乏针对水网络的基于图的可解释模型限制了实际应用。

Method: 通过整合互信息识别关键网络区域，并利用模糊逻辑提供基于规则的清晰解释，开发了一种模糊增强的广义图卷积网络（FGENConv）。

Result: FGENConv在泄漏检测和定位方面分别达到了0.889和0.814的Graph F1分数，略低于GENConv的0.938和0.858，但提供了空间局部化和基于模糊规则的解释。

Conclusion: 提出的模糊图神经网络（FGENConv）在精确性和可解释性之间取得了平衡，为水力工程师验证预测泄漏位置、节省人力资源和优化维护策略提供了可能。

Abstract: Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.

</details>


### [88] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本文介绍了一个用于数字孪生的保证框架，结合TEA方法，旨在为航空业中的数字孪生技术提供监管和评估支持。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在航空业具有巨大潜力，但缺乏明确的监管框架。本文旨在填补这一空白，提供一个可操作的框架来评估和证明数字孪生的准确性和功能性。

Method: 通过结合数字孪生开发和人工智能/机器学习（AI/ML）在空管管理（ATM）中的新兴指导，提出了一个保证框架，并利用TEA方法开发保证案例。

Result: 提出了一个保证框架，该框架不仅帮助研究人员评估数字孪生的优势和局限性，还为未来的监管需求提供了基础。

Conclusion: 本文提出了一个用于数字孪生技术的保证框架，该框架结合了可信和道德保证（TEA）方法，旨在为数字孪生的准确性和功能性提供证据，并支持与利益相关者和监管机构的讨论。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [89] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 本文提出了一种简单有效的自动提示工程系统，首次应用于隐晦列名扩展（CNE）任务，并在英语和德语数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 隐晦列名扩展（CNE）对表格数据搜索、访问和理解至关重要，但现有研究极少。本文旨在填补这一空白，并首次将自动提示工程应用于非英语语言。

Method: 该系统无需调整或明确任务线索，在数据库表的隐晦列名扩展（CNE）任务中进行了评估，涵盖英语和德语数据集。

Result: 评估结果显示，该系统在英语和德语数据集上均有效，是首个应用于CNE任务和非英语语言的自动提示工程工作。

Conclusion: 本文介绍了一种自动提示工程的系统，设计简单且应用广泛，无需调整或明确任务线索，效果与现有方法相当。

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


### [90] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent框架通过外部化状态解决长视野任务中的上下文增长问题，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在长视野任务中因上下文无限增长和错误累积而崩溃的问题。

Method: InfiAgent框架通过将代理的推理上下文严格限制在固定范围内，并通过工作区状态快照和最近动作的固定窗口重建上下文。

Result: 在DeepResearch和80篇文献综述任务中，InfiAgent与大型专有系统竞争，并在长视野覆盖上显著优于上下文中心基线。

Conclusion: InfiAgent通过将持久状态外部化为文件中心的状态抽象，为稳定长视野任务代理提供了实用基础。

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [91] [MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents](https://arxiv.org/abs/2601.03236)
*Dongming Jiang,Yi Li,Guanpeng Li,Bingzhe Li*

Main category: cs.AI

TL;DR: MAGMA提出多图代理记忆架构，通过策略引导的遍历优化长上下文推理，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似度的单一内存存储方法在解释性和查询意图与检索证据的对齐上存在局限，导致推理准确性不足。

Method: MAGMA采用多图架构（语义、时间、因果和实体图），将检索过程定义为策略引导的遍历，实现了查询自适应的选择和结构化上下文构建。

Result: 在LoCoMo和LongMemEval数据集上的实验表明，MAGMA在长视野推理任务中表现优于现有代理记忆系统。

Conclusion: MAGMA通过多图代理记忆架构显著提升了长上下文推理任务的准确性，并在实验中优于现有技术。

Abstract: Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [92] [ProSoftArena: Benchmarking Hierarchical Capabilities of Multimodal Agents in Professional Software Environments](https://arxiv.org/abs/2601.02399)
*Jiaxin Ai,Yukang Feng,Fanrui Zhang,Jianwen Sun,Zizhen Li,Chuanhao Li,Yifan Chang,Wenxiao Wu,Ruoxi Wang,Mingliang Zhai,Kaipeng Zhang*

Main category: cs.SE

TL;DR: ProSoftArena 是一个针对专业软件环境的多模态代理评估平台，实验表明当前代理在复杂任务中表现不佳，为未来改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要局限于浏览器和基本桌面应用，无法满足专业软件工作流程的需求，因此需要专门针对专业软件环境的评估平台。

Method: 构建了一个包含436个任务的基准测试，涵盖6个学科和13个专业应用，并建立了可执行的真实计算机环境和基于执行的评估框架，结合人类参与评估。

Result: 实验显示，即使表现最佳的代理在L2任务上的成功率仅为24.4%，且在L3多软件工作流中完全失败。

Conclusion: ProSoftArena 填补了专业软件环境中多模态代理评估的空白，为构建更强大的代理提供了宝贵的设计原则和见解。

Abstract: Multimodal agents are making rapid progress on general computer-use tasks, yet existing benchmarks remain largely confined to browsers and basic desktop applications, falling short in professional software workflows that dominate real-world scientific and industrial practice. To close this gap, we introduce ProSoftArena, a benchmark and platform specifically for evaluating multimodal agents in professional software environments. We establish the first capability hierarchy tailored to agent use of professional software and construct a benchmark of 436 realistic work and research tasks spanning 6 disciplines and 13 core professional applications. To ensure reliable and reproducible assessment, we build an executable real-computer environment with an execution-based evaluation framework and uniquely incorporate a human-in-the-loop evaluation paradigm. Extensive experiments show that even the best-performing agent attains only a 24.4\% success rate on L2 tasks and completely fails on L3 multi-software workflow. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents in professional software settings. This project is available at: https://prosoftarena.github.io.

</details>


### [93] [The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming](https://arxiv.org/abs/2601.02410)
*Aizierjiang Aiersilan*

Main category: cs.SE

TL;DR: 本文提出VCP框架，通过三个指标评估Vibe Coding的教学效果，帮助教育者权衡其利弊。


<details>
  <summary>Details</summary>
Motivation: 探讨Vibe Coding是否是一种更好的软件工程学习方法，并分析学生使用AI的不同方式（加速vs认知卸载）对学习成果的影响。

Method: 通过三个定量指标（Cold Start Refactor、Hallucination Trap Detection、Explainability Gap）进行系统评估，并进行对照比较。

Result: VCP框架为教育者提供了定量依据，以区分Vibe Coding在哪些情境下能促进真正掌握，哪些情境下可能导致技术债务和表面能力。

Conclusion: 本文提出了Vibe-Check Protocol (VCP)框架，旨在通过定量指标评估Vibe Coding在软件工程教育中的效果，帮助教育者确定其最佳教学边界。

Abstract: The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.

</details>


### [94] [Talks that Builds: Exploring Communication factors for the Success of Emerging Professional in Product Teams](https://arxiv.org/abs/2601.02421)
*Nyan Lin Zaw*

Main category: cs.SE

TL;DR: 本研究探索了年轻专业人士（18-27岁）团队成功的新因素，如好奇心、地理位置接近等，填补了现有文献的空白。


<details>
  <summary>Details</summary>
Motivation: 现有组织沟通研究多关注经验丰富的专业人士（27岁以上，5年以上经验），而对年轻新兴专业人士团队的研究较少。本研究旨在探索这一群体团队成功的独特因素。

Method: 研究通过考察年轻专业人士（18-27岁）组成的产品团队，分析了影响其成功的因素，并与传统因素进行了对比。

Result: 研究发现，传统因素部分适用，但新因素如好奇心、地理位置接近、文档记录和资源获取对团队成功有显著影响。

Conclusion: 本研究填补了文献中关于新兴专业人士（18-27岁）团队成功因素的空白，识别了好奇心、地理位置接近、文档记录和资源获取等新因素对团队生产力和项目成果的影响。

Abstract: This paper recognizes that most organizational communication study focuses on established professionals aged above 27 with more than five years of experience. In contrast, this study examines product teams with younger emerging professionals aged 18-27 and explores which factors influence their success. While some established factors still apply, others become less relevant, and new ones such as curiosity, locational proximity, documentation, access to resources were identified in the study. Overall, this study fills a gap in the literature on how these newer factors shape team productivity and project outcomes based on the success rate of the product the team developed.

</details>


### [95] [WebCoderBench: Benchmarking Web Application Generation with Comprehensive and Interpretable Evaluation Metrics](https://arxiv.org/abs/2601.02430)
*Chenxu Liu,Yingjie Fu,Wei Yang,Ying Zhang,Tao Xie*

Main category: cs.SE

TL;DR: WebCoderBench是首个针对LLM生成Web应用的基准，结合真实用户需求和自动化评估，展示不同模型的性能差异，助力模型优化。


<details>
  <summary>Details</summary>
Motivation: 构建一个适用于LLM生成Web应用的基准面临挑战，如需要真实用户需求、无需依赖真实实现或测试用例的可泛化评估指标，以及可解释的评估结果。

Method: 引入了WebCoderBench基准，包含1,572个真实用户需求，提供24个细粒度评估指标，结合基于规则和LLM作为评判的范式进行自动化评估。

Result: 实验覆盖12个代表性LLM和2个基于LLM的代理，结果显示没有模型在所有指标上占优，为开发者提供了针对性优化的机会。

Conclusion: WebCoderBench 是一个真实世界收集、可泛化且可解释的基准，为LLM生成的Web应用提供了全面的评估框架，展示了不同LLM在不同指标上的表现差异，为模型优化提供了方向。

Abstract: Web applications (web apps) have become a key arena for large language models (LLMs) to demonstrate their code generation capabilities and commercial potential. However, building a benchmark for LLM-generated web apps remains challenging due to the need for real-world user requirements, generalizable evaluation metrics without relying on ground-truth implementations or test cases, and interpretable evaluation results. To address these challenges, we introduce WebCoderBench, the first real-world-collected, generalizable, and interpretable benchmark for web app generation. WebCoderBench comprises 1,572 real user requirements, covering diverse modalities and expression styles that reflect realistic user intentions. WebCoderBench provides 24 fine-grained evaluation metrics across 9 perspectives, combining rule-based and LLM-as-a-judge paradigm for fully automated, objective, and general evaluation. Moreover, WebCoderBench adopts human-preference-aligned weights over metrics to yield interpretable overall scores. Experiments across 12 representative LLMs and 2 LLM-based agents show that there exists no dominant model across all evaluation metrics, offering an opportunity for LLM developers to optimize their models in a targeted manner for a more powerful version.

</details>


### [96] [Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection](https://arxiv.org/abs/2601.02438)
*Yun Bian,Yi Chen,HaiQuan Wang,ShiHao Li,Zhe Cui*

Main category: cs.SE

TL;DR: TaCCS-DFA通过Fisher信息和自适应门控机制优化多模态融合，显著提升漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法假设添加模态必然带来额外信息，但实际上序列和图表示可能存在冗余，且图模态质量波动可能稀释主导模态的判别信号。

Method: 提出TaCCS-DFA框架，利用Fisher信息估计低秩主Fisher子空间，限制跨模态注意力于任务敏感方向，并通过自适应门控机制动态调整图模态的贡献。

Result: 在BigVul、Devign和ReVeal数据集上，TaCCS-DFA表现出色，以CodeT5为骨干时，F1分数达87.80%，比基线Vul-LMGNNs提升6.3个百分点。

Conclusion: TaCCS-DFA框架通过引入Fisher信息作为几何度量，实现了任务导向的互补融合，显著提升了软件漏洞检测的性能。

Abstract: Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.

</details>


### [97] [The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance](https://arxiv.org/abs/2601.02454)
*Saba Naqvi,Mohammad Baqar,Nawaz Ali Mohammad*

Main category: cs.SE

TL;DR: 本文提出了一种多智能体测试框架，通过闭环反馈机制显著提升测试质量和覆盖率，减少无效测试和人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前的基于AI的测试生成器由于缺乏执行感知反馈，仍然存在静态、单次输出的问题，经常产生无效、冗余或不可执行的测试。

Method: 引入了一个代理多模型测试框架，这是一个闭环、自我纠正的系统，其中测试生成代理、执行与分析代理以及审查与优化代理协作生成、执行、分析和优化测试，直到收敛。

Result: 在基于微服务的应用程序上的实证评估显示，与单模型基线相比，无效测试减少了60%，覆盖率提高了30%，并显著减少了人工工作量。

Conclusion: 多智能体、反馈驱动的循环可以将软件测试演变为一个自主、持续学习的质量保证生态系统，用于自修复、高可靠性的代码库。

Abstract: Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.

</details>


### [98] [Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support](https://arxiv.org/abs/2601.02504)
*Elizaveta Artser,Daniil Karol,Anna Potriasaeva,Aleksei Rostovskii,Katsiaryna Dzialets,Ekaterina Koshchenko,Xiaotian Su,April Yi Wang,Anastasiia Birillo*

Main category: cs.SE

TL;DR: An AI-powered IDE debugging assistant, using RAG and LLMs, improves debugging efficiency and education, validated by multi-level evaluations.


<details>
  <summary>Details</summary>
Motivation: Debugging is a critical but often neglected skill in programming education and software development, necessitating innovative tools to bridge this gap.

Method: The assistant integrates RAG with LLMs, program slicing, and custom heuristics to provide real-time debugging support, optimizing efficiency and accuracy.

Result: The tool's effectiveness is validated through technical analysis, UX studies, and classroom tests, showing its utility in teaching debugging.

Conclusion: The AI-powered debugging assistant demonstrates significant potential in enhancing debugging education and practice, supported by positive evaluations across technical, user experience, and classroom settings.

Abstract: Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.

</details>


### [99] [Green LLM Techniques in Action: How Effective Are Existing Techniques for Improving the Energy Efficiency of LLM-Based Applications in Industry?](https://arxiv.org/abs/2601.02512)
*Pelin Rabia Kuran,Rumbidzai Chitakunye,Vincenzo Stoico,Ilja Heitlager,Justus Bogner*

Main category: cs.SE

TL;DR: 研究分析了四种优化技术对LLM工业应用能源效率的影响，发现小型与大型模型协作是唯一能在不显著损害性能的情况下显著减少能源消耗的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速采用引发了对其高能耗的担忧，尤其是在工业规模部署时。目前缺乏关于这些技术在LLM工业应用中有效性的实证证据。

Method: 分析了Schuberg Philis公司的聊天机器人应用，选择了四种技术（小型与大型模型协作、提示优化、量化和批处理）并应用了八种变体，通过实验比较了它们对能源消耗、准确性和响应时间的影响。

Result: 提示优化和2位量化等技术显著减少了能源使用（高达90%），但对准确性产生了不可接受的负面影响。唯一在不显著损害其他性能的情况下实现显著能源节约的技术是通过Nvidia的Prompt Task and Complexity Classifier（NPCC）的小型与大型模型协作。

Conclusion: 研究表明，减少基于LLM应用的能源消耗在实践中并不困难，但提高能源效率（即在不损害其他性能的情况下减少能源使用）仍然具有挑战性。

Abstract: The rapid adoption of large language models (LLMs) has raised concerns about their substantial energy consumption, especially when deployed at industry scale. While several techniques have been proposed to address this, limited empirical evidence exists regarding the effectiveness of applying them to LLM-based industry applications. To fill this gap, we analyzed a chatbot application in an industrial context at Schuberg Philis, a Dutch IT services company. We then selected four techniques, namely Small and Large Model Collaboration, Prompt Optimization, Quantization, and Batching, applied them to the application in eight variations, and then conducted experiments to study their impact on energy consumption, accuracy, and response time compared to the unoptimized baseline.
  Our results show that several techniques, such as Prompt Optimization and 2-bit Quantization, managed to reduce energy use significantly, sometimes by up to 90%. However, these techniques especially impacted accuracy negatively, to a degree that is not acceptable in practice. The only technique that achieved significant and strong energy reductions without harming the other qualities substantially was Small and Large Model Collaboration via Nvidia's Prompt Task and Complexity Classifier (NPCC) with prompt complexity thresholds. This highlights that reducing the energy consumption of LLM-based applications is not difficult in practice. However, improving their energy efficiency, i.e., reducing energy use without harming other qualities, remains challenging. Our study provides practical insights to move towards this goal.

</details>


### [100] [On the Effectiveness of Proposed Techniques to Reduce Energy Consumption in RAG Systems: A Controlled Experiment](https://arxiv.org/abs/2601.02522)
*Zhinuan,Guo,Chushu Gao,Justus Bogner*

Main category: cs.SE

TL;DR: 研究发现RAG系统中特定节能技术（如调整检索阈值和减小嵌入大小）可大幅降低能耗（60%）且不损失准确性，为可持续ML应用提供实证指导。


<details>
  <summary>Details</summary>
Motivation: 机器学习（如RAG系统）的能源需求增长引发了对环境可持续性的担忧，但现有绿色技术在RAG系统中的实证研究不足。

Method: 通过在生产级RAG系统上进行的控制实验，评估了五种节能技术对能耗、延迟和准确性的影响。

Result: 实验显示，某些技术（如提高相似性检索阈值和减小嵌入大小）可显著降低能耗（高达60%），且不影响准确性；而其他技术（如索引策略）可能导致准确性下降高达30%。

Conclusion: 本研究首次全面实证了RAG系统的节能设计技术，为开发者和研究人员构建可持续的RAG应用提供了指导。

Abstract: The rising energy demands of machine learning (ML), e.g., implemented in popular variants like retrieval-augmented generation (RAG) systems, have raised significant concerns about their environmental sustainability. While previous research has proposed green tactics for ML-enabled systems, their empirical evaluation within RAG systems remains largely unexplored. This study presents a controlled experiment investigating five practical techniques aimed at reducing energy consumption in RAG systems. Using a production-like RAG system developed at our collaboration partner, the Software Improvement Group, we evaluated the impact of these techniques on energy consumption, latency, and accuracy.
  Through a total of 9 configurations spanning over 200 hours of trials using the CRAG dataset, we reveal that techniques such as increasing similarity retrieval thresholds, reducing embedding sizes, applying vector indexing, and using a BM25S reranker can significantly reduce energy usage, up to 60% in some cases. However, several techniques also led to unacceptable accuracy decreases, e.g., by up to 30% for the indexing strategies. Notably, finding an optimal retrieval threshold and reducing embedding size substantially reduced energy consumption and latency with no loss in accuracy, making these two techniques truly energy-efficient. We present the first comprehensive, empirical study on energy-efficient design techniques for RAG systems, providing guidance for developers and researchers aiming to build sustainable RAG applications.

</details>


### [101] [PerspectiveCoach: Exploring LLMs for Developer Reflection](https://arxiv.org/abs/2601.02559)
*Lauren Olson,Emitzá Guzmán,Florian Kunneman*

Main category: cs.SE

TL;DR: PerspectiveCoach是一种LLM驱动的对话工具，通过结构化视角采择练习帮助开发者反思软件设计对边缘化群体的影响，研究证实其有效但人际互动效果较弱。


<details>
  <summary>Details</summary>
Motivation: 针对软件开发中缺乏结构化工具帮助开发者批判性思考边缘化用户体验的问题，设计并评估一种新型对话工具。

Method: 通过一项包含18名前端开发者的对照研究，结合质性分析和文本相似性分析，评估PerspectiveCoach在支持伦理推理和用户视角采择方面的效果。

Result: 研究表明，PerspectiveCoach显著提升了开发者的自我意识、视角广度和伦理表达细腻度，但在人际互动中的基线表现低于人-人对话。

Conclusion: PerspectiveCoach作为一种基于大型语言模型的对话工具，有效支持开发者在软件设计中进行批判性自我反思，提升对边缘化用户群体的关注，但在人际视角采择方面仍有改进空间。

Abstract: Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.

</details>


### [102] [Compressed code: the hidden effects of quantization and distillation on programming tokens](https://arxiv.org/abs/2601.02563)
*Viacheslav Siniaev,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: cs.SE

TL;DR: 研究分析了LLMs中编程语言标记的编码方式，提出了冷启动概率分析方法，并评估了优化技术对代码生成的影响，为理论理解和实践应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在压缩模型中的标记级机制，尤其是编程语言的编码方式。

Method: 通过系统分析编程语言标记表示，引入冷启动概率分析方法，并全面评估不同模型优化技术对标记级表示和代码生成质量的影响。

Result: 实验通过全面的概率分布分析和评估指标，揭示了标记级行为的关键见解，并提供了在优化约束下保持代码生成质量的实证指南。

Conclusion: 研究发现为LLM代码生成的理论理解和生产环境中优化模型的实践应用提供了重要见解。

Abstract: Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.

</details>


### [103] [State of the Quantum Software Engineering Ecosystem](https://arxiv.org/abs/2601.02601)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 论文利用GPT-5模型分析了量子软件工程生态系统的现状，重点关注了学术界和产业界的活跃参与者及其成就。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别在QSE领域表现活跃且取得显著成果的机构和企业，这些成果通过同行评审的出版物或风险资本市场融资体现。

Method: 研究采用了基于大型语言模型（LLMs）的人工智能技术，特别是OpenAI的GPT-5模型，通过ChatGPT工具进行分析。

Result: 研究识别了在量子软件工程领域表现突出的机构和企业，并分析了它们的成就和活动。

Conclusion: 该论文总结了量子软件工程（QSE）生态系统的现状，突出了学术界和产业界的成就与活动，并特别关注了成功的创业项目。

Abstract: We study the current state of the Quantum Software Engineering (QSE) ecosystem, focusing on the achievements, activities, and engagements from academia and industry, with a special focus on successful entrepreneurial endeavors in this arena. Our research methodology is a novel one, featuring the state-of-the-art in Artificial Intelligence (AI), namely Large Language Models (LLMs), especially Generative Pretrained Transformers (GPT). We use one of such models, namely the OpenAI GPT-5 model, through the ChatGPT tool. The goal is to identify institutions and companies that are highly active and have achieved distinguished results in QSE, evidenced by peer-reviewed publications or raised capital in the venture capital market.

</details>


### [104] [TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs](https://arxiv.org/abs/2601.02632)
*Alireza Ezaz,Ghazal Khodabandeh,Majid Babaei,Naser Ezzati-Jivan*

Main category: cs.SE

TL;DR: TAAF结合KG和LLM，自动化分析大规模执行轨迹，提升准确率31.2%，减少手动分析需求。


<details>
  <summary>Details</summary>
Motivation: 现有工具依赖预定义分析，自定义洞察需要编写领域特定脚本，耗时且易错。TAAF旨在通过自动化分析减少手动检查和深度系统专业知识的需求。

Method: TAAF采用时间索引知识图谱（KG）构建轨迹事件关系，并利用大型语言模型（LLM）解析查询特定子图，以回答自然语言问题。

Result: 在TraceQA-100基准测试中，TAAF在三个LLM和多种时间设置下，答案准确率最高提升31.2%。

Conclusion: TAAF通过结合时间索引、知识图谱和大型语言模型，显著提升了执行轨迹分析的准确性和效率，特别是在多跳和因果推理任务中，为下一代轨迹分析工具奠定了基础。

Abstract: Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.

</details>


### [105] [Enterprise Identity Integration for AI-Assisted Developer Services: Architecture, Implementation, and Case Study](https://arxiv.org/abs/2601.02698)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 文章提出了一种将OAuth 2.0和OIDC集成到MCP开发环境中的架构，解决了企业SSO集成问题，并通过原型验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 企业需要确保AI辅助开发工具在现有身份、访问控制和治理要求下运行，而MCP规范仅提供了最小的授权模型，且缺乏集成企业SSO的指导。

Method: 文章描述了一种将OAuth 2.0和OpenID Connect（OIDC）集成到支持MCP的开发环境中的实用架构，包括IDE扩展如何获取和呈现令牌，MCP服务器如何通过身份提供商验证令牌，以及如何利用范围和声明实施最小权限访问。

Result: 通过使用Visual Studio Code、基于Python的MCP服务器和符合OIDC标准的IdP的原型实现，验证了该方法的可行性。案例研究评估了认证延迟、令牌验证开销、操作考虑和AI特定风险。

Conclusion: 该文章提出了一种可部署的模式，适用于采用AI辅助开发工具的组织，同时保持身份保证和可审计性。

Abstract: AI-assisted developer services are increasingly embedded in modern IDEs, yet enterprises must ensure these tools operate within existing identity, access control, and governance requirements. The Model Context Protocol (MCP) enables AI assistants to retrieve structured internal context, but its specification provides only a minimal authorization model and lacks guidance on integrating enterprise SSO. This article presents a practical architecture that incorporates OAuth 2.0 and OpenID Connect (OIDC) into MCP-enabled developer environments. It describes how IDE extensions obtain and present tokens, how MCP servers validate them through an identity provider, and how scopes and claims can enforce least-privilege access. A prototype implementation using Visual Studio Code, a Python-based MCP server, and an OIDC-compliant IdP demonstrates feasibility. A case study evaluates authentication latency, token-validation overhead, operational considerations, and AI-specific risks. The approach provides a deployable pattern for organizations adopting AI-assisted developer tools while maintaining identity assurance and auditability.

</details>


### [106] [Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices](https://arxiv.org/abs/2601.02732)
*Lingzhe Zhang,Tong Jia,Yunpeng Zhai,Leyi Pan,Chiming Duan,Minghua He,Mengxi Jia,Ying Li*

Main category: cs.SE

TL;DR: AMER-RCL：基于递归推理和代理记忆的微服务根因定位框架，提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法存在推理浅层和跨警报冗余问题，受SRE专家分析启发，需设计更高效的根因定位方法。

Method: 提出AMER-RCL框架，结合递归推理引擎和多代理机制，通过递归分析警报和跨警报记忆重用优化推理过程。

Result: 实验表明AMER-RCL在定位准确性和推理效率上均优于现有技术。

Conclusion: AMER-RCL框架通过递归推理和代理记忆机制显著提升了微服务系统中根因定位的准确性和效率，优于现有方法。

Abstract: As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.

</details>


### [107] [Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism](https://arxiv.org/abs/2601.02736)
*Lingzhe Zhang,Tong Jia,Yunpeng Zhai,Leyi Pan,Chiming Duan,Minghua He,Pei Xiao,Ying Li*

Main category: cs.SE

TL;DR: SpecRCA是一个基于假设-验证范式的微服务根因分析框架，解决了现有LLM方法的局限性，实验显示其高效且准确。


<details>
  <summary>Details</summary>
Motivation: 微服务系统的复杂性和动态运行时交互导致异常频发，现有基于LLM的根因分析方法存在探索多样性不足和依赖大规模模型导致的推理速度慢问题。

Method: SpecRCA采用假设-验证范式，包括假设生成模块快速生成候选根因，并行验证模块高效验证这些假设。

Result: 在AIOps 2022数据集上的初步实验表明，SpecRCA在准确性和效率上优于现有方法。

Conclusion: SpecRCA框架通过假设-验证范式，显著提升了微服务系统中根因分析的准确性和效率，为复杂环境下的可扩展和可解释RCA提供了实用解决方案。

Abstract: Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.

</details>


### [108] [CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation](https://arxiv.org/abs/2601.02868)
*Peiding Wang,Li Zhang,Fang Liu,Chongyang Tao,Yinghao Zhu*

Main category: cs.SE

TL;DR: CodeMEM是一个AST引导的动态内存管理系统，解决了仓库级代码生成中的上下文遗忘问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的记忆管理方法在仓库级代码生成中存在局限性，如上下文遗忘和错误重入。

Method: 提出了CodeMEM系统，包含Code Context Memory和Code Session Memory两个组件，分别通过AST引导的LLM操作动态维护仓库上下文，以及通过基于AST的分析构建代码中心的交互历史表示。

Result: 在CodeIF-Bench和CoderEval基准测试中，CodeMEM在指令遵循和交互轮次减少方面表现优异，分别提升了12.2%和11.5%，并减少了2-3轮交互。

Conclusion: CodeMEM通过AST引导的动态内存管理系统显著提升了仓库级迭代代码生成的性能，在指令遵循和减少交互轮次方面取得了最先进的结果。

Abstract: Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.

</details>


### [109] [Few-shot learning for security bug report identification](https://arxiv.org/abs/2601.02971)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: Proposes a few-shot learning technique (SetFit) for identifying security bug reports with limited labeled data, achieving superior performance (AUC 0.865) over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional ML techniques for classifying security bug reports rely on large labeled datasets, which are often scarce, leading to poor model performance and limited real-world applicability.

Method: The study employs SetFit, a state-of-the-art few-shot learning framework combining sentence transformers with contrastive learning and parameter-efficient fine-tuning, trained on a small labeled dataset of bug reports.

Result: The approach achieves an AUC of 0.865, outperforming traditional ML techniques across all evaluated datasets.

Conclusion: SetFit-based few-shot learning offers a promising alternative to traditional ML techniques for identifying security bug reports, especially in scenarios with scarce labeled data.

Abstract: Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.

</details>


### [110] [A Dataset of Low-Rated Applications from the Amazon Appstore for User Feedback Analysis](https://arxiv.org/abs/2601.03009)
*Nek Dil Khan,Javed Ali Khan,Darvesh Khan,Jianqiang Li,Mumrez Khan,Shah Fahad Khan*

Main category: cs.SE

TL;DR: 研究通过收集和标注低评分应用的用户评论，创建了一个公开数据集，旨在帮助理解常见问题并改进软件质量。


<details>
  <summary>Details</summary>
Motivation: 低评分应用的研究潜力尚未被充分挖掘，尽管它们可能揭示有价值的见解，以改善用户体验和软件质量。

Method: 研究引入了从亚马逊软件应用商店（ASA）收集的64个低评分应用的79,821条用户评论数据集，并手动标注了6000条评论，将其分类为六个不同的问题类别。

Result: 数据集捕获了用户最常见的问题，并为开发基于机器学习的用户反馈自动分类方法提供了资源。

Conclusion: 该数据集为研究人员和开发者提供了一个重要工具，用于理解低评分应用中的常见问题，并为软件改进提供数据支持。

Abstract: In todays digital landscape, end-user feedback plays a crucial role in the evolution of software applications, particularly in addressing issues that hinder user experience. While much research has focused on high-rated applications, low-rated applications often remain unexplored, despite their potential to reveal valuable insights. This study introduces a novel dataset curated from 64 low-rated applications sourced from the Amazon Software Appstore (ASA), containing 79,821 user reviews. The dataset is designed to capture the most frequent issues identified by users, which are critical for improving software quality. To further enhance the dataset utility, a subset of 6000 reviews was manually annotated to classify them into six district issue categories: user interface (UI) and user experience (UX), functionality and features, compatibility and device specificity, performance and stability, customer support and responsiveness, and security and privacy issues. This annotated dataset is a valuable resource for developing machine learning-based approaches aiming to automate the classification of user feedback into various issue types. Making both the annotated and raw datasets publicly available provides researchers and developers with a crucial tool to understand common issues in low-rated apps and inform software improvements. The comprehensive analysis and availability of this dataset lay the groundwork for data-derived solutions to improve software quality based on user feedback. Additionally, the dataset can provide opportunities for software vendors and researchers to explore various software evolution-related activities, including frequently missing features, sarcasm, and associated emotions, which will help better understand the reasons for comparatively low app ratings.

</details>


### [111] [NavAI: A Generalizable LLM Framework for Navigation Tasks in Virtual Reality Environments](https://arxiv.org/abs/2601.03251)
*Xue Qin,Matthew DiGiovanni*

Main category: cs.SE

TL;DR: NavAI是一个基于LLM的VR导航框架，在目标导向任务中表现出色（89%成功率），但也暴露了完全依赖LLMs的局限性，尤其是在动态目标评估方面。


<details>
  <summary>Details</summary>
Motivation: 现有的导航技术主要针对360度图像数据集和3D模拟器中的路径优化，无法直接应用于沉浸式VR环境，因此需要一种新的解决方案。

Method: 提出了NavAI，一个基于大型语言模型（LLM）的可推广导航框架，支持跨多样VR应用的基本动作和复杂目标导向任务。

Result: NavAI在三个不同的VR环境中通过目标导向和探索性任务进行评估，结果显示其在目标导向任务中达到了89%的成功率。

Conclusion: 论文讨论了NavAI在VR导航中的成功应用，同时也指出了完全依赖LLMs的局限性，特别是在需要动态目标评估的场景中，并提出了未来研究的方向。

Abstract: Navigation is one of the fundamental tasks for automated exploration in Virtual Reality (VR). Existing technologies primarily focus on path optimization in 360-degree image datasets and 3D simulators, which cannot be directly applied to immersive VR environments. To address this gap, we present NavAI, a generalizable large language model (LLM)-based navigation framework that supports both basic actions and complex goal-directed tasks across diverse VR applications. We evaluate NavAI in three distinct VR environments through goal-oriented and exploratory tasks. Results show that it achieves high accuracy, with an 89% success rate in goal-oriented tasks. Our analysis also highlights current limitations of relying entirely on LLMs, particularly in scenarios that require dynamic goal assessment. Finally, we discuss the limitations observed during the experiments and offer insights for future research directions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [112] [Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges](https://arxiv.org/abs/2601.02377)
*Xinyu Huang,Shyam Karthick V B,Taozhao Chen,Mitch Bryson,Thomas Chaffey,Huaming Chen,Kim-Kwang Raymond Choo,Ian R. Manchester*

Main category: cs.RO

TL;DR: 论文系统调查了LLM控制机器人的安全威胁与防御策略，强调情境感知安全的必要性，并提供了发展路线图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）与机器人技术的整合带来了安全漏洞，尤其是‘体现差距’导致的物理威胁，现有解决方案不足以应对。

Method: 通过系统调查，总结了新兴威胁格局及相应的防御策略，讨论了攻击向量的综合分类，并分析了一系列防御机制。

Result: 论文综述了攻击向量和防御机制，并评估了相关数据集和基准测试，突出了情境感知安全的重要性。

Conclusion: 该论文强调了情境感知安全解决方案的紧迫性，并为开发安全、可靠的大型语言模型（LLM）控制的机器人提供了基础路线图。

Abstract: The integration of Large Language Models (LLMs) into robotics has revolutionized their ability to interpret complex human commands and execute sophisticated tasks. However, such paradigm shift introduces critical security vulnerabilities stemming from the ''embodiment gap'', a discord between the LLM's abstract reasoning and the physical, context-dependent nature of robotics. While security for text-based LLMs is an active area of research, existing solutions are often insufficient to address the unique threats for the embodied robotic agents, where malicious outputs manifest not merely as harmful text but as dangerous physical actions. In this work, we present a systematic survey, summarizing the emerging threat landscape and corresponding defense strategies for LLM-controlled robotics. Specifically, we discuss a comprehensive taxonomy of attack vectors, covering topics such as jailbreaking, backdoor attacks, and multi-modal prompt injection. In response, we analyze and categorize a range of defense mechanisms, from formal safety specifications and runtime enforcement to multi-LLM oversight and prompt hardening. Furthermore, we review key datasets and benchmarks used to evaluate the robustness of these embodied systems. By synthesizing current research, this work highlights the urgent need for context-aware security solutions and provides a foundational roadmap for the development of safe, secure, and reliable LLM-controlled robotics.

</details>


### [113] [Modeling the Mental World for Embodied AI: A Comprehensive Review](https://arxiv.org/abs/2601.02378)
*Biyuan Liu,Daigang Xu,Lei Jiang,Wenjun Guo,Ping Chen*

Main category: cs.RO

TL;DR: 综述通过整合100多项研究，首次构建了心理世界模型（MWM）的完整框架，区分了MWM与物理世界模型（PWM）的差异，并系统分析了ToM推理范式与评估方法，推动具身智能体融入社会。


<details>
  <summary>Details</summary>
Motivation: 传统物理世界模型（PWM）无法满足社会智能建模的需求，而心理世界模型（MWM）作为人类内部心理状态的结构化表示，成为实现自然人机协作和动态社会适应的关键认知基础。当前MWM研究面临概念框架碎片化、推理机制脱节等瓶颈。

Method: 系统综合了100多项权威研究，首次构建了完整的MWM理论框架，区分了MWM与PWM的本质差异，并通过两种心理元素表征范式系统定义了MWM的关键组成部分。

Result: 构建了MWM的完整理论框架，系统定义了其关键组成部分，全面分析了两种核心ToM推理范式及19种ToM方法，并综合了26个ToM评估基准。

Conclusion: 本综述旨在推动具身智能体融入人类社会，并促进人机协作交互的深入发展。

Abstract: As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice.
  To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.

</details>


### [114] [Movement Primitives in Robotics: A Comprehensive Survey](https://arxiv.org/abs/2601.02379)
*Nolan B. Gutierrez,William J. Beksi*

Main category: cs.RO

TL;DR: 本文系统综述了运动基元在机器人领域的框架、应用及挑战，旨在为从业者提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 生物系统的连续运动启发研究者识别运动基元，以生成机器人自主系统的运动命令。

Method: 按时间顺序提供了运动基元方法及应用的百科全书式概述，包括基于弹簧阻尼系统、概率耦合演示、神经网络等多种技术。

Result: 综述了运动基元框架及其在机器人领域的成功应用，并分析了实际应用中的挑战。

Conclusion: 本文综述了运动基元在机器人领域的应用，系统回顾了主要框架的优缺点，并探讨了实际应用中的开放性问题与挑战。

Abstract: Biological systems exhibit a continuous stream of movements, consisting of sequential segments, that allow them to perform complex tasks in a creative and versatile fashion. This observation has led researchers towards identifying elementary building blocks of motion known as movement primitives, which are well-suited for generating motor commands in autonomous systems, such as robots. In this survey, we provide an encyclopedic overview of movement primitive approaches and applications in chronological order. Concretely, we present movement primitive frameworks as a way of representing robotic control trajectories acquired through human demonstrations. Within the area of robotics, movement primitives can encode basic motions at the trajectory level, such as how a robot would grasp a cup or the sequence of motions necessary to toss a ball. Furthermore, movement primitives have been developed with the desirable analytical properties of a spring-damper system, probabilistic coupling of multiple demonstrations, using neural networks in high-dimensional systems, and more, to address difficult challenges in robotics. Although movement primitives have widespread application to a variety of fields, the goal of this survey is to inform practitioners on the use of these frameworks in the context of robotics. Specifically, we aim to (i) present a systematic review of major movement primitive frameworks and examine their strengths and weaknesses; (ii) highlight applications that have successfully made use of movement primitives; and (iii) examine open questions and discuss practical challenges when applying movement primitives in robotics.

</details>


### [115] [InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation](https://arxiv.org/abs/2601.02456)
*Junhao Cai,Zetao Cai,Jiafei Cao,Yilun Chen,Zeyu He,Lei Jiang,Hang Li,Hengjie Li,Yang Li,Yufei Liu,Yanan Lu,Qi Lv,Haoxiang Ma,Jiangmiao Pang,Yu Qiao,Zherui Qiu,Yanqing Shen,Xu Shi,Yang Tian,Bolun Wang,Hanqing Wang,Jiaheng Wang,Tai Wang,Xueyuan Wei,Chao Wu,Yiman Xie,Boyang Xing,Yuqiang Yang,Yuyin Yang,Qiaojun Yu,Feng Yuan,Jia Zeng,Jingjing Zhang,Shenghan Zhang,Shi Zhang,Zhuoma Zhaxi,Bowen Zhou,Yuanzhen Zhou,Yunsong Zhou,Hongrui Zhu,Yangkun Zhu,Yuchen Zhu*

Main category: cs.RO

TL;DR: InternVLA-A1通过混合专家架构结合语义理解与动态预测，在机器人任务中表现优异，显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型缺乏物理世界动态推理能力，而世界模型方法又缺乏语义基础且对预测错误脆弱。因此，需要一种能结合语义理解和动态预测能力的模型。

Method: 采用统一的Mixture-of-Transformers架构，协调场景理解、视觉预见生成和动作执行三个专家模块，通过统一的掩码自注意力机制实现无缝交互。基于InternVL3和Qwen3-VL，构建了2B和3B参数规模的模型，并在混合合成-真实数据集上进行预训练。

Result: 在12个真实世界机器人任务和仿真基准测试中，InternVLA-A1显著优于pi0和GR00T N1.5等领先模型，日常任务提升14.5%，动态场景（如传送带分拣）提升40%-73.3%。

Conclusion: InternVLA-A1通过统一的Mixture-of-Transformers架构，成功将语义理解与动态预测能力结合，显著提升了在真实世界机器人任务和仿真基准中的表现。

Abstract: Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.

</details>


### [116] [Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints](https://arxiv.org/abs/2601.02505)
*Jiazhen Liu,Glen Neville,Jinwoo Park,Sonia Chernova,Harish Ravichandar*

Main category: cs.RO

TL;DR: STEAM问题通过E-ITAGS算法解决，优化任务分配、调度和路径规划，同时学习特质-效能映射，实验显示其效能优于基线且满足约束。


<details>
  <summary>Details</summary>
Motivation: 复杂多机器人任务需要异构团队联合优化任务分配、调度和路径规划，以提高团队在严格约束下的性能。STEAM问题通过建模机器人能力和效能映射来超越典型的二元成功-失败模型。

Method: STEAM问题通过E-ITAGS算法解决，该算法通过交织任务分配、调度和路径规划，同时优化任务性能并尊重时间预算。算法还包括一个可实现性感知的主动学习模块，用于高效学习特质-效能映射。

Result: E-ITAGS生成的分配比基线具有更高的效能，同时满足资源和时空约束。主动学习方法样本高效，并在数据和计算效率之间建立了原则性的权衡。

Conclusion: E-ITAGS算法在应急响应领域的详细数值模拟和实验中显示出比基线更高的效能分配，同时尊重资源和时空约束。主动学习方法样本高效，并在数据和计算效率之间建立了原则性的权衡。

Abstract: Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency.

</details>


### [117] [Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects](https://arxiv.org/abs/2601.02645)
*Samarth Kalluraya,Yiannis Kantaros*

Main category: cs.RO

TL;DR: BRiDGE是一种3D环境中的导航规划器，通过移动物体解决目标区域不可达问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有导航规划器假设目标区域可达，但在3D环境中常因物理断开而失败，需通过移动物体创建新的可通行连接。

Method: 开发了BRiDGE，一种基于采样的规划器，逐步构建机器人和物体配置的树状结构，计算可行的移动、放置和顺序计划。

Result: 实验验证了BRiDGE的有效性，包括数值和硬件实验。

Conclusion: BRiDGE是一种概率完备的规划器，通过非均匀采样策略加速规划，有效解决了3D环境中目标区域不可达的问题。

Abstract: Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness.

</details>


### [118] [Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search](https://arxiv.org/abs/2601.02649)
*Jiangyi Fang,Bowen Zhou,Haotian Wang,Xin Zhu,Leye Wang*

Main category: cs.RO

TL;DR: 论文提出结合前瞻信息的MCTS框架，显著提升在线3D-BP在分布变化下的性能。


<details>
  <summary>Details</summary>
Motivation: 现代物流系统中，DRL方法难以适应短期分布变化，论文认为前瞻信息是解决这一问题的关键。

Method: 论文将在线3D-BP问题建模为MPC问题，并采用MCTS框架进行求解，结合动态探索先验和辅助奖励设计。

Result: 实验表明，该方法在分布变化下性能提升超过10%，在线部署平均提升4%，最佳情况下超过8%。

Conclusion: 该论文提出的框架在真实数据集上表现优异，尤其在分布变化情况下实现了超过10%的性能提升，证明了其有效性。

Abstract: Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\% gains under distributional shifts, 4\% average improvement in online deployment, and up to more than 8\% in the best case--demonstrating the effectiveness of our framework.

</details>


### [119] [Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter](https://arxiv.org/abs/2601.02686)
*Haixin Jin,Nikhil Uday Shinde,Soofiyan Atar,Hongzhan Yu,Dylan Hirsch,Sicun Gao,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: DCBF是一种学习的安全框架，适用于密集环境中的机器人操作，可扩展且可迁移。


<details>
  <summary>Details</summary>
Motivation: 传统安全框架将接触视为不安全，限制了机器人在密集日常环境中的功能。随着对象数量增加，基于模型的安全操作方法变得计算不可行，而学习方法通常将安全性与特定任务绑定，难以迁移。

Method: 通过离线学习一个可组合的、以对象为中心的函数，隐式捕捉物理交互中的安全约束，避免了多对象动力学显式建模的计算复杂性。

Result: 在密集杂乱环境的模拟实验中验证了DCBF能够实现无碰撞导航和安全、接触丰富的交互。

Conclusion: Dense Contact Barrier Functions (DCBF) 提供了一种可扩展且可转移的安全框架，适用于密集环境中的机器人操作，无需重新训练即可应用于新任务。

Abstract: Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings.

</details>


### [120] [Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization](https://arxiv.org/abs/2601.02704)
*Kento Kawaharazuka,Keita Yoneda,Takahiro Hattori,Shintaro Inoue,Kei Okada*

Main category: cs.RO

TL;DR: 通过多目标优化分析现有机械臂结构，为未来设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种6-DOF和7-DOF机械臂，但其关节配置和连杆长度比例多为经验确定，缺乏统一标准。为探讨机械臂的最优结构，本研究进行了多目标优化。

Method: 采用多目标优化方法，从末端执行器可达性和关节扭矩两个角度进行优化。

Result: 分析了现有机械臂结构在优化采样结果中的位置，并提供了设计建议。

Conclusion: 本文通过多目标优化分析了现有机械臂结构在末端执行器可达性和关节扭矩方面的表现，为未来机械臂设计提供了见解。

Abstract: Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.

</details>


### [121] [Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM](https://arxiv.org/abs/2601.02723)
*Wenzheng Zhang,Kazuki Adachi,Yoshitaka Hara,Sousuke Nakamura*

Main category: cs.RO

TL;DR: 提出了一种改进DPV-SLAM闭环性能的方法，通过AnyLoc替代BoVW并引入自适应阈值机制，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的BoVW方法依赖手工特征，在不同视角和光照条件下表现不佳，因此需要更鲁棒的闭环检测方法。

Method: 通过集成基于学习的视觉位置识别技术AnyLoc替代传统的BoVW闭环检测方法，并引入自适应机制动态调整相似性阈值。

Result: 在室内外数据集上的实验表明，该方法在闭环准确性和鲁棒性上显著优于原始DPV-SLAM。

Conclusion: 所提出的方法为现代SLAM系统提供了一种实用且可扩展的解决方案，显著提升了闭环检测的准确性和鲁棒性。

Abstract: Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems.

</details>


### [122] [Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning](https://arxiv.org/abs/2601.02738)
*Kexin Guo,Zihan Yang,Yuhang Liu,Jindou Jia,Xiang Yu*

Main category: cs.RO

TL;DR: 本文提出自监督残差学习与轨迹优化框架，通过混合模型处理未知动态效应，生成可精确跟踪的激进轨迹，四旋翼飞行实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的物理模型对复杂机器人系统的精确建模存在局限，导致在控制器合成时存在残差物理，难以准确跟踪激进轨迹。

Method: 首先学习闭环模型中的未知动态效应，将其视为标称动力学的残差，形成混合模型。利用轨迹级数据和解析梯度进行学习，实现任意积分步长的准确长时预测。随后开发轨迹优化器，计算最小化残差物理的最优参考轨迹。

Result: 提出的混合动力学方法使优化器能够输出可精确跟踪的激进运动，尤其在四旋翼飞行器的敏捷飞行中验证了其有效性。

Conclusion: 通过结合自监督残差学习和轨迹优化框架，本文提出的方法能够有效处理复杂机器人系统中的未知动态效应，生成可精确跟踪的激进轨迹，尤其是在四旋翼飞行器的敏捷飞行中表现优异。

Abstract: Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked.

</details>


### [123] [Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation](https://arxiv.org/abs/2601.02762)
*Zihan Yang,Jindou Jia,Meng Wang,Yuhang Liu,Kexin Guo,Xiang Yu*

Main category: cs.RO

TL;DR: 提出了一种基于元学习和反馈校准的通用干扰估计框架，有效解决了非结构性干扰的估计问题，并在实验中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现代机器人应用中，由于未知时变干扰的存在，精确控制一直是一个开放性问题。现有的基于元学习的方法需要共享环境结构的表示，缺乏对现实非结构性干扰的灵活性。

Method: 结合元学习和反馈校准的在线适应方法，从有限时间窗口的过去观测中提取特征，学习无需预定义结构假设的统一表示。

Result: 理论分析表明，在线学习误差和干扰估计误差可以同时收敛。实验证明，该框架能有效估计多种快速变化的干扰。

Conclusion: 该框架通过统一的元表示和反馈校准的在线适应，有效估计了快速变化的非结构性干扰，并在四旋翼飞行实验中验证了其有效性。

Abstract: Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.

</details>


### [124] [Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs](https://arxiv.org/abs/2601.02766)
*Md. Anowar Hossain,Mohd. Ehsanul Hoque*

Main category: cs.RO

TL;DR: 该论文提出了一种多模式电动轮椅控制系统，整合了四种控制接口和实时健康监测，实验显示高精度和低延迟，并符合安全标准。


<details>
  <summary>Details</summary>
Motivation: 电动轮椅是ALS、中风后偏瘫和痴呆相关行动障碍患者的重要辅助工具。现有系统在控制模式切换和实时健康监测方面存在不足，因此需要开发一种更灵活、更安全的解决方案。

Method: 研究整合了操纵杆、语音、手势和眼电（EOG）四种控制模式，并结合了心率变异性、血氧饱和度（SpO2）和皮肤温度的连续监测框架。通过两点校准生物传感器，确保了数据的准确性。实验评估包括20名行动不便的参与者执行500次室内导航命令。

Result: 实验结果显示，操纵杆控制的识别准确率为99%，语音控制为97±2%，手势控制为95±3%，平均闭环延迟为20±0.5毫秒。生命体征监测的误差极低（心率≤2 bpm，皮肤温度≤0.5°C，SpO2≤1%）。护理人员可通过Android应用接收实时警报。

Conclusion: 该研究提出了一种创新的多模式电动轮椅控制系统，结合了四种互补的控制接口和实时生命体征监测，显著提升了患者的独立性和护理人员的实时监督能力。该系统不仅满足了ISO 7176-31和IEC 80601-2-78安全标准，还为未来的自适应机器学习改进奠定了基础。

Abstract: Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability.
  Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements.

</details>


### [125] [M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination](https://arxiv.org/abs/2601.02777)
*Jingcheng Cao,Chaoran Xiong,Jianmin Song,Shang Yan,Jiachen Liu,Ling Pei*

Main category: cs.RO

TL;DR: M-SEVIQ是一个多波段立体事件视觉和惯性四足数据集，用于解决现有事件相机数据集的局限性，支持敏捷机器人感知和多模态视觉研究。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机在低光条件和快速运动下易产生模糊图像，而事件相机具有低延迟、高时间分辨率和高动态范围的优势，但现有数据集在立体配置和多波段感知领域存在局限。

Method: 通过配备立体事件相机、帧相机、惯性测量单元（IMU）和关节编码器的Unitree Go2机器人收集数据，包含30多个真实世界序列，涵盖不同速度、光照波长和照明条件。

Result: 提供了全面的校准数据（包括内参、外参和时间对齐），以支持精确的传感器融合和基准测试。

Conclusion: M-SEVIQ数据集为敏捷机器人感知、传感器融合、语义分割和多模态视觉研究提供了支持，特别是在具有挑战性的环境中。

Abstract: Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.

</details>


### [126] [Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation](https://arxiv.org/abs/2601.02778)
*Haoyu Dong,Zhengmao He,Yang Li,Zhibin Li,Xinyu Yi,Zhe Zhao*

Main category: cs.RO

TL;DR: 提出了一种结合触觉和扭矩反馈的模拟到真实强化学习框架，首次实现完全在模拟中训练并零样本转移到真实硬件的多指灵巧手可控抓取。


<details>
  <summary>Details</summary>
Motivation: 解决多指灵巧手在真实硬件上直接部署控制策略的困难，包括接触密集的物理和不完美的驱动问题。

Method: 提出了一种实用的模拟到真实强化学习框架，利用密集触觉反馈和关节扭矩传感来调节物理交互。具体包括：(i) 快速触觉模拟，(ii) 电流到扭矩校准，(iii) 驱动动力学建模。

Result: 策略展示了两种关键技能：(1) 基于命令的可控抓取力跟踪，(2) 手中物体的重新定向，均无需在机器人上进行微调即可稳健执行。

Conclusion: 通过结合触觉和扭矩反馈，并有效建模传感/驱动系统，该系统为实现可靠的灵巧操作提供了实用解决方案。这是首次展示完全在模拟中训练并零样本转移到真实硬件的多指灵巧手的可控抓取。

Abstract: Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.

</details>


### [127] [Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data](https://arxiv.org/abs/2601.02798)
*Sicong Gao,Chen Qian,Laurence Xian,Liao Wu,Maurice Pagnucco,Yang Song*

Main category: cs.RO

TL;DR: 提出一种基于视觉深度强化学习的内窥镜机器人自主导航方法，显著减少与肠壁接触，提升导航精度和患者舒适度。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜机器人在狭窄管状环境中自主导航时避免与内壁接触的长期挑战，减少患者不适。

Method: 采用基于单目深度估计的视觉深度强化学习框架，构建了真实的肠道模拟环境，并通过生成大量合成图像优化深度感知模型。

Result: 相比原始Depth Anything模型，深度准确率提升了39.2%，导航J指数降低了0.67，证明了方法的鲁棒性和有效性。

Conclusion: 提出的基于视觉深度强化学习的框架显著提升了内窥镜机器人在狭窄管状环境中的自主导航能力，减少了与肠壁的接触，提高了患者的舒适度。

Abstract: Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $δ_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach.

</details>


### [128] [Soft Responsive Materials Enhance Humanoid Safety](https://arxiv.org/abs/2601.02857)
*Chunzheng Wang,Yiyuan Zhang,Annan Tang,Ziqiu Zeng,Haoran Chen,Quan Gao,Zixuan Zhuang,Boyu Li,Zhilin Xiong,Aoqian Zhang,Ce Hao,Siyuan Luo,Tongyang Zhao,Cecilia Laschi,Fan Shi*

Main category: cs.RO

TL;DR: 本研究提出了一种软硬协同设计框架，利用非牛顿流体材料提升人形机器人安全性，通过模拟和主动策略学习，显著减少跌落冲击，增强机器人鲁棒性和环境安全性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在人类中心环境中的应用受到其易跌倒和刚性金属塑料结构对人和环境构成风险的限制。

Method: 采用软硬协同设计框架，利用非牛顿流体基软响应材料增强人形机器人安全性，并通过物理模拟指导保护器的放置和厚度，以及学习主动跌落策略。

Result: 应用于42公斤全尺寸人形机器人时，保护器显著降低了峰值冲击力，允许机器人多次跌落而无硬件损坏，包括从3米高处跌落和长楼梯翻滚。

Conclusion: 通过结合响应性材料、结构协同设计和基于学习的控制，本研究推动了交互安全、适合工业应用的人形机器人发展。

Abstract: Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots.

</details>


### [129] [Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion](https://arxiv.org/abs/2601.02873)
*Arthur Haffemayer,Alexandre Chapin,Armand Jordana,Krzysztof Wojciechowski,Florent Lamiraux,Nicolas Mansard,Vladimir Petrik*

Main category: cs.RO

TL;DR: 扩散模型+MPC混合方法在多障碍物环境中高效生成可行运动轨迹，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统优化控制器在多障碍物环境中难以快速生成可行解，而扩散模型缺乏对场景结构的高效条件化方法。

Method: 采用扩散变换器（diffusion transformer）结合物体中心槽注意力机制（object-centric slot attention）生成初始轨迹，并通过最优控制问题（MPC）进行细化，确保刚体动力学和碰撞约束。

Result: 在基准任务中，该方法比基于采样的规划器或单独组件具有更高的成功率和更低的延迟。真实机器人实验验证了其可靠性和安全性。

Conclusion: 结合扩散模型和模型预测控制（MPC）的方法在严格时间限制下实现了可靠且高效的运动生成，显著提高了成功率和降低了延迟。

Abstract: Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.

</details>


### [130] [LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments](https://arxiv.org/abs/2601.02905)
*Sara Micol Ferraina,Michele Brienza,Francesco Argenziano,Emanuele Musumeci,Vincenzo Suriani,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: LOST-3DSG 是一种轻量级开放词汇 3D 场景图方法，通过语义实体跟踪提升动态物体追踪效率，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有动态物体追踪方法因依赖重型基础模型而效率低下，需要一种更轻量、高效的解决方案。

Method: 采用基于 word2vec 和句子嵌入的语义方法进行实体跟踪，避免了高维视觉嵌入的存储需求。

Result: 在真实 3D 环境中使用 TIAGo 机器人进行的实验表明，LOST-3DSG 在动态物体追踪中表现优异。

Conclusion: LOST-3DSG 是一种轻量级的开放词汇 3D 场景图方法，通过语义实体跟踪和避免存储密集的 CLIP 视觉特征，显著提升了动态物体追踪的效率和性能。

Abstract: Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.

</details>


### [131] [Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters](https://arxiv.org/abs/2601.02948)
*Matti Vahs,Jaeyoun Choi,Niklas Schmid,Jana Tumova,Chuchu Fan*

Main category: cs.RO

TL;DR: PRMPPI框架通过在线学习和安全约束优化，实现动态环境下机器人安全高效操作。


<details>
  <summary>Details</summary>
Motivation: 动态环境中机器人需在物理参数不确定或变化时保持安全，现有方法在参数学习和安全保证方面存在不足。

Method: PRMPPI结合了Stein变分梯度下降进行参数粒子信念维护，使用Conformal Prediction评估安全约束，并并行优化名义性能驱动和安全备份轨迹。

Result: 实验表明，PRMPPI在成功率、跟踪误差和参数估计准确性上均优于基线方法。

Conclusion: PRMPPI控制框架通过整合在线参数学习和概率安全约束，成功实现了在动态环境中机器人安全操作的目标，仿真和硬件实验验证了其优于基线方法的性能。

Abstract: Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines.

</details>


### [132] [Learning to Act Robustly with View-Invariant Latent Actions](https://arxiv.org/abs/2601.02994)
*Youngjoon Jeong,Junha Chun,Taesup Kim*

Main category: cs.RO

TL;DR: VILA通过潜在动作建模学习视角不变表示，提升机器人策略在视角变化下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉基础的机器人策略在视角变化时表现不佳，现有方法依赖场景级多视角观测，但忽略了物理动力学对鲁棒泛化的重要性。

Method: 提出了View-Invariant Latent Action (VILA)，通过建模捕捉轨迹间转移模式的潜在动作，学习基于物理动力学的视角不变表示。

Result: 实验表明，基于VILA的策略在模拟和现实环境中均能有效泛化到未见视角，并良好迁移到新任务。

Conclusion: VILA作为一种强大的预训练框架，通过基于真实动作序列的动作引导目标，有效提升了策略在未见视角下的泛化能力，并增强了下游学习性能。

Abstract: Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.

</details>


### [133] [A Bi-directional Adaptive Framework for Agile UAV Landing](https://arxiv.org/abs/2601.03037)
*Chunhui Zhao,Xirui Kao,Yilin Lu,Yang Lyu*

Main category: cs.RO

TL;DR: 该论文提出了一种双向合作着陆框架，通过移动平台主动倾斜和四旋翼无人机优化轨迹，显著提高了动态场景中的着陆效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统着陆方法在高度动态场景中效率低下，主要因为其将平台视为被动目标，导致复杂的顺序机动。

Method: 通过将问题从单智能体跟踪挑战转化为耦合系统优化，设计了双向合作着陆框架，其中移动平台主动倾斜表面以优化着陆姿态，同时四旋翼无人机规划时间最优且动态可行的轨迹。

Result: 在动态场景中验证了框架的有效性，显著提升了自主四旋翼无人机在复杂和时间紧迫任务中的恢复能力。

Conclusion: 该论文提出的双向合作着陆框架通过将移动平台视为主动参与者，显著提高了四旋翼无人机在动态场景中的着陆效率、精度和鲁棒性。

Abstract: Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions.

</details>


### [134] [Validating Generalist Robots with Situation Calculus and STL Falsification](https://arxiv.org/abs/2601.03038)
*Changwen Li,Rongjie Yan,Chih-Hong Cheng,Jian Zhang*

Main category: cs.RO

TL;DR: 提出一个结合抽象推理与具体证伪的两层验证框架，用于验证通用机器人自主性，实验证明其能有效发现控制器中的失败案例。


<details>
  <summary>Details</summary>
Motivation: 通用机器人能够执行多样化操作，但验证其正确性具有挑战性，因为每个任务的操作上下文和正确性规范超出了传统验证方法的假设范围。

Method: 框架包括抽象层和具体层。抽象层使用情境演算建模世界并推导最弱前置条件，结合约束感知的组合测试生成多样化的语义有效世界-任务配置；具体层则将这些配置实例化，进行基于仿真的STL监控证伪。

Result: 在桌面操作任务上的实验表明，该框架能有效发现NVIDIA GR00T控制器中的失败案例。

Conclusion: 该论文提出了一个两层验证框架，结合抽象推理与具体系统证伪，有效揭示了通用机器人自主性中的失败案例，展示了其在验证通用机器人自主性方面的潜力。

Abstract: Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy.

</details>


### [135] [PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms](https://arxiv.org/abs/2601.03040)
*Arup Kumar Sahoo,Itzik Klein*

Main category: cs.RO

TL;DR: PiDR是一种物理信息惯性航位推算框架，通过整合物理原理提升纯惯性导航的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决纯惯性导航中传统深度学习模型的黑盒性质、有限监督数据下的学习效果差以及物理原则保持不足的问题。

Method: 提出了PiDR框架，通过物理信息残差组件将惯性导航原理显式整合到网络训练过程中。

Result: 在移动机器人和自主水下车辆的真实数据集上评估，定位精度提升超过29%。

Conclusion: PiDR提供了一个轻量级且有效的架构，能够在资源受限的平台上部署，实现恶劣场景下的实时纯惯性导航。

Abstract: A fundamental requirement for full autonomy is the ability to sustain accurate navigation in the absence of external data, such as GNSS signals or visual information. In these challenging environments, the platform must rely exclusively on inertial sensors, leading to pure inertial navigation. However, the inherent noise and other error terms of the inertial sensors in such real-world scenarios will cause the navigation solution to drift over time. Although conventional deep-learning models have emerged as a possible approach to inertial navigation, they are inherently black-box in nature. Furthermore, they struggle to learn effectively with limited supervised sensor data and often fail to preserve physical principles. To address these limitations, we propose PiDR, a physics-informed inertial dead-reckoning framework for autonomous platforms in situations of pure inertial navigation. PiDR offers transparency by explicitly integrating inertial navigation principles into the network training process through the physics-informed residual component. PiDR plays a crucial role in mitigating abrupt trajectory deviations even under limited or sparse supervision. We evaluated PiDR on real-world datasets collected by a mobile robot and an autonomous underwater vehicle. We obtained more than 29% positioning improvement in both datasets, demonstrating the ability of PiDR to generalize different platforms operating in various environments and dynamics. Thus, PiDR offers a robust, lightweight, yet effective architecture and can be deployed on resource-constrained platforms, enabling real-time pure inertial navigation in adverse scenarios.

</details>


### [136] [SOP: A Scalable Online Post-Training System for Vision-Language-Action Models](https://arxiv.org/abs/2601.03044)
*Mingjie Pan,Siyuan Feng,Qinglin Zhang,Xinchen Li,Jianheng Song,Chendi Qu,Yi Wang,Chuankang Li,Ziyu Xiong,Zhi Chen,Yi Liu,Jianlan Luo*

Main category: cs.RO

TL;DR: SOP系统通过在线、分布式、多任务后训练，提升通用VLA模型在现实任务中的性能，支持快速策略更新与规模化学习。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法多为离线、单机器人或任务特定，限制了有效的策略适应和可扩展的实时交互学习。

Method: 提出了一个可扩展的在线后训练（SOP）系统，支持在线、分布式、多任务的后训练，采用闭环架构，结合执行与学习，并通过机器人舰队持续流式传输经验与人工干预信号。

Result: SOP显著提升了大型预训练VLA模型的性能，同时保持跨任务的共享策略，后训练在几小时内即可见效，且性能随机器人数量近线性增长。

Conclusion: 紧密耦合在线学习与大规模机器人部署是实现高效、可靠且可扩展的通用机器人策略后训练的关键。

Abstract: Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.

</details>


### [137] [A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints](https://arxiv.org/abs/2601.03055)
*Shiying Dong,Zhipeng Shen,Rudolf Reiter,Hailong Huang,Bingzhao Gao,Hong Chen,Wen-Hua Chen*

Main category: cs.RO

TL;DR: 提出时间缩放直接多重射击和凸松弛方法，有效解决非凸最优控制问题，提升求解效率和优化性。


<details>
  <summary>Details</summary>
Motivation: 由于自主代理在时空约束下的最优控制问题（OCP）通常是非凸的，传统方法（如预定义路径点时间或非凸轨迹优化）往往导致次优解，因此需要更高效的求解方法。

Method: 采用时间缩放直接多重射击方案分割预测时域，并结合基于半定规划的凸松弛方法，利用提升公式的稀疏模式。

Result: 仿真和实际实验（如四旋翼飞行器路径点飞行任务）证明了该方法在求解最优性和计算效率上的显著提升。

Conclusion: 该论文提出了一种时间缩放直接多重射击方案和快速半定规划凸松弛方法，显著改善了非线性程序求解的数值特性，并通过仿真和实际实验验证了方法的优化性和计算效率。

Abstract: Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.

</details>


### [138] [HEXAR: a Hierarchical Explainability Architecture for Robots](https://arxiv.org/abs/2601.03070)
*Tamlin Love,Ferran Gebellí,Pradip Pramanick,Antonio Andriella,Guillem Alenyà,Anais Garrell,Raquel Ros,Silvia Rossi*

Main category: cs.RO

TL;DR: HEXAR是一种分层可解释性框架，通过模块化解释器和选择器为复杂机器人系统提供定制化解释，实验证明其在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统日益复杂，现有可解释性方法要么过于模块化难以从高层行为角度查询，要么过于单一无法利用机器人架构的模块化特性，因此需要一种新的分层框架来弥补这一缺陷。

Method: HEXAR采用分层架构，结合多种解释技术（如基于LLM的推理、因果模型、特征重要性等），通过专门的组件解释器和解释器选择器，为特定机器人模块生成定制化解释。

Result: 在TIAGo机器人上进行的180种场景-查询变体实验中，HEXAR在根因识别、错误信息排除和运行时效率方面显著优于端到端和聚合基线方法。

Conclusion: HEXAR框架在机器人系统中显著提升了可解释性，尤其在根因识别、错误信息排除和运行时效率方面优于现有基线方法，为透明自主系统提供了有前景的方向。

Abstract: As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems.

</details>


### [139] [Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees](https://arxiv.org/abs/2601.03097)
*Omayra Yago Nieto,Alexandre Anahory Simoes,Juan I. Giribet,Leonardo Colombo*

Main category: cs.RO

TL;DR: 该论文提出了一种基于双四元数和高斯过程的学习型轨迹跟踪控制器，能够在未知干扰下实现鲁棒的姿态控制，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对自主机器人平台在运动过程中遇到的未知干扰、传感器诱导的干扰、未建模的驱动耦合和环境不确定性等问题，提出一种不依赖显式参数模型的学习型控制器。

Method: 控制器在双四元数框架下设计，工作在速度级别，利用高斯过程（GP）回归在线学习和补偿未知的状态依赖干扰和建模不完美，同时保持刚体运动的代数结构和耦合特性。

Result: 仿真结果表明，在存在局部化干扰（如磁力计扰动引起的相关旋转和平移效应）的情况下，控制器能够实现准确且平滑的轨迹跟踪。

Conclusion: 该论文提出的基于学习的轨迹跟踪控制器通过结合几何建模和概率学习，实现了自主机器人系统在存在未知干扰和建模不完美情况下的鲁棒、数据高效的姿态控制。

Abstract: We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.
  The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.
  Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems.

</details>


### [140] [A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting](https://arxiv.org/abs/2601.03200)
*Ziyang Sun,Lingfan Bao,Tianhu Peng,Jingcheng Sun,Chengxu Zhou*

Main category: cs.RO

TL;DR: A practical framework using 3DGS for fast, photorealistic digital twin construction, enhancing sim-to-real transfer with semantic and geometric consistency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry in existing approaches.

Method: The framework employs 3D Gaussian Splatting (3DGS) for photorealistic reconstruction, enhanced with visibility-aware semantic fusion for accurate 3D labeling, and a filter-based geometry conversion method for collision-ready models.

Result: The system constructs high-quality digital twins within minutes from sparse RGB inputs, supporting robust manipulation in real-world trials with a Franka Emika Panda robot.

Conclusion: 3DGS-based digital twins, enhanced with semantic and geometric consistency, provide a fast, reliable, and scalable solution for sim-to-real transfer in unstructured environments.

Abstract: Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [141] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: eess.IV

TL;DR: 提出EGxFSL和xGAL框架，结合可解释性和主动学习，显著提升医学图像分析性能，验证了跨模态适用性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标记数据稀缺和模型可解释性不足的问题，限制了临床AI的部署。

Method: EGxFSL通过Grad-CAM-based Dice损失结合放射科医生定义的感兴趣区域进行空间监督，xGAL则通过预测不确定性和注意力错位迭代选择样本。

Result: 在BraTS、VinDr-CXR和SIIM-COVID-19数据集上，模型准确率分别达到92%、76%和62%，xGAL在680个样本下达到76%准确率，显著优于随机采样。

Conclusion: EGxFSL和xGAL框架在医学图像分析中有效解决了数据稀缺和模型可解释性问题，显著提升了模型性能并增强了临床部署的可行性。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [142] [A $O^*((2 + ε)^k)$ Time Algorithm for Cograph Deletion Using Unavoidable Subgraphs in Large Prime Graphs](https://arxiv.org/abs/2601.02532)
*Manuel Lafond,Francis Sarrazin*

Main category: cs.DS

TL;DR: 本文提出了一种基于模分解的新方法，显著提高了Cograph Deletion问题的算法效率，时间复杂度降至O^*((2 + ε)^k)，并探讨了其在其他图修改问题中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的参数化算法策略难以进一步改进，因此需要探索新的方法来解决Cograph Deletion问题，以提升计算效率并扩展应用范围。

Method: 通过模分解将问题分解为独立解决的模块和商图，核心问题简化为在素数图上求解。利用Chudnovsky等人的素数图结构特征，设计了一种递归分支树算法。

Result: 新算法的时间复杂度为O^*((2 + ε)^k)，优于现有最佳算法O^*(2.303^k)，并首次应用了素数图特征。

Conclusion: 本文提出了一种基于模分解的新方法来解决Cograph Deletion问题，显著提高了算法的时间复杂度至O^*((2 + ε)^k)，并探讨了该方法在其他图修改问题中的潜在应用。

Abstract: We study the parameterized complexity of the Cograph Deletion problem, which asks whether one can delete at most $k$ edges from a graph to make it $P_4$-free. This is a well-known graph modification problem with applications in computation biology and social network analysis.
  All current parameterized algorithms use a similar strategy, which is to find a $P_4$ and explore the local structure around it to perform an efficient recursive branching.
  The best known algorithm achieves running time $O^*(2.303^k)$ and requires an automated search of the branching cases due to their complexity.
  Since it appears difficult to further improve the current strategy, we devise a new approach using modular decompositions. We solve each module and the quotient graph independently, with the latter being the core problem. This reduces the problem to solving on a prime graph, in which all modules are trivial. We then use a characterization of Chudnovsky et al. stating that any large enough prime graph has one of seven structures as an induced subgraph. These all have many $P_4$s, with the quantity growing linearly with the graph size, and we show that these allow a recursive branch tree algorithm to achieve running time $O^*((2 + ε)^k)$ for any $ε> 0$.
  This appears to be the first algorithmic application of the prime graph characterization and it could be applicable to other modification problems. Towards this goal, we provide the exact set of graph classes $\H$ for which the $\H$-free editing problem can make use of our reduction to a prime graph, opening the door to improvements for other modification problems.

</details>


### [143] [A Practical 73/50 Approximation for Contiguous Monotone Moldable Job Scheduling](https://arxiv.org/abs/2601.02836)
*Klaus Jansen,Felix Ohnesorge*

Main category: cs.DS

TL;DR: 本文提出了一种高效算法，解决了单调可塑作业调度问题，近似比为≈(1.4593 + ε)，时间复杂度为O(nm log(1/ε))，实际性能优于理论预测。


<details>
  <summary>Details</summary>
Motivation: 针对单调可塑作业调度问题，现有算法在时间复杂度和近似比方面存在不足，需要一种更高效且实用的解决方案。

Method: 提出了一种新的算法，结合了近似比和时间复杂度的优化，适用于单调可塑作业调度问题。

Result: 新算法在近似比和时间复杂度上优于现有最佳算法，实际性能表现优异。

Conclusion: 本文提出了一种新的实用高效算法，其近似比为≈(1.4593 + ε)，时间复杂度为O(nm log(1/ε))，适用于问题的连续变体。实验结果表明，实际性能显著优于理论最坏情况近似比。

Abstract: In moldable job scheduling, we are provided $m$ identical machines and $n$ jobs that can be executed on a variable number of machines. The execution time of each job depends on the number of machines assigned to execute that job. For the specific problem of monotone moldable job scheduling, jobs are assumed to have a processing time that is non-increasing in the number of machines.
  The previous best-known algorithms are: (1) a polynomial-time approximation scheme with time complexity $Ω(n^{g(1/\varepsilon)})$, where $g(\cdot)$ is a super-exponential function [Jansen and Thöle '08; Jansen and Land '18], (2) a fully polynomial approximation scheme for the case of $m \geq 8\frac{n}{\varepsilon}$ [Jansen and Land '18], and (3) a $\frac{3}{2}$ approximation with time complexity $O(nm\log(mn))$ [Wu, Zhang, and Chen '23].
  We present a new practically efficient algorithm with an approximation ratio of $\approx (1.4593 + \varepsilon)$ and a time complexity of $O(nm \log \frac{1}{\varepsilon})$. Our result also applies to the contiguous variant of the problem. In addition to our theoretical results, we implement the presented algorithm and show that the practical performance is significantly better than the theoretical worst-case approximation ratio.

</details>


### [144] [Hardness of Regular Expression Matching with Extensions](https://arxiv.org/abs/2601.03020)
*Taisei Nogami,Tachio Terauchi*

Main category: cs.DS

TL;DR: 本文分析扩展正则表达式匹配的计算复杂度，证明某些扩展操作无法在多项式时间内解决，解释了长期未能改进时间复杂度的原因。


<details>
  <summary>Details</summary>
Motivation: 探讨正则表达式扩展操作（如反向引用、交集和补集）对匹配问题计算复杂度的影响，解释为何长期以来无法改进ERE匹配的时间复杂度。

Method: 通过理论分析，结合Orthogonal Vectors Conjecture和k-Clique Hypothesis，研究了扩展正则表达式匹配的时间复杂度。

Result: 证明扩展正则表达式匹配问题无法在O(n^{2-ε} poly(m))或O(n^{ω-ε} poly(m))时间内解决，并指出已有算法的优化空间有限。

Conclusion: 本文证明了正则表达式匹配问题在扩展了反向引用、交集和补集操作后，无法在特定时间复杂度内解决，并揭示了这些扩展对计算复杂度的影响。

Abstract: The regular expression matching problem asks whether a given regular expression of length $m$ matches a given string of length $n$. As is well known, the problem can be solved in $O(nm)$ time using Thompson's algorithm. Moreover, recent studies have shown that the matching problem for regular expressions extended with a practical extension called lookaround can be solved in the same time complexity. In this work, we consider three well-known extensions to regular expressions called backreference, intersection and complement, and we show that, unlike in the case of lookaround, the matching problem for regular expressions extended with any of the three (for backreference, even when restricted to one capturing group) cannot be solved in $O(n^{2-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon > 0$ under the Orthogonal Vectors Conjecture. Moreover, we study the matching problem for regular expressions extended with complement in more detail, which is also known as extended regular expression (ERE) matching. We show that there is no ERE matching algorithm that runs in $O(n^{ω-\varepsilon} \mathrm{poly}(m))$ time ($2 \le ω< 2.3716$ is the exponent of square matrix multiplication) for any constant $\varepsilon > 0$ under the $k$-Clique Hypothesis, and there is no combinatorial ERE matching algorithm that runs in $O(n^{3-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon > 0$ under the Combinatorial $k$-Clique Hypothesis. This shows that the $O(n^3 m)$-time algorithm introduced by Hopcroft and Ullman in 1979 and recently improved by Bille et al. to run in $O(n^ωm)$ time using fast matrix multiplication was already optimal in a sense, and sheds light on why the theoretical computer science community has struggled to improve the time complexity of ERE matching with respect to $n$ and $m$ for more than 45 years.

</details>


### [145] [Density Matters: A Complexity Dichotomy of Deleting Edges to Bound Subgraph Density](https://arxiv.org/abs/2601.03129)
*Matthias Bentert,Tom-Lukas Breitkopf,Vincent Froese,Anton Herrmann,André Nichterlein*

Main category: cs.DS

TL;DR: 本文对$τ$-BDED问题进行了完整分类，明确了多项式时间可解和NP难的条件，并提出了基于树宽和随机算法的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究$τ$-BDED问题的计算复杂性，以填补现有文献中对不同$τ$值下问题可解性的空白。

Method: 通过将问题转化为新的受限网络流问题，利用最大s-t流或一般因子方法进行求解。

Result: 1. 若$2τ$为半整数或$τ<2/3$，则$τ$-BDED可在多项式时间内求解；否则为NP难问题。2. 针对树宽和整数$τ$，提出了固定参数可解性和随机快速算法。

Conclusion: 本文提供了关于$τ$-BDED问题的完整分类，明确了多项式时间可解和NP难的条件，并展示了基于树宽和随机算法的解决方案。

Abstract: We study $τ$-Bounded-Density Edge Deletion ($τ$-BDED), where given an undirected graph $G$, the task is to remove as few edges as possible to obtain a graph $G'$ where no subgraph of $G'$ has density more than $τ$. The density of a (sub)graph is the number of edges divided by the number of vertices. This problem was recently introduced and shown to be NP-hard for $τ\in \{2/3, 3/4, 1 + 1/25\}$, but polynomial-time solvable for $τ\in \{0,1/2,1\}$ [Bazgan et al., JCSS 2025]. We provide a complete dichotomy with respect to the target density $τ$:
  1. If $2τ\in \mathbb{N}$ (half-integral target density) or $τ< 2/3$, then $τ$-BDED is polynomial-time solvable.
  2. Otherwise, $τ$-BDED is NP-hard.
  We complement the NP-hardness with fixed-parameter tractability with respect to the treewidth of $G$. Moreover, for integral target density $τ\in \mathbb{N}$, we show $τ$-BDED to be solvable in randomized $O(m^{1 + o(1)})$ time. Our algorithmic results are based on a reduction to a new general flow problem on restricted networks that, depending on $τ$, can be solved via Maximum s-t-Flow or General Factors. We believe this connection between these variants of flow and matching to be of independent interest.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [146] [Fair Distribution of Digital Payments: Balancing Transaction Flows for Regulatory Compliance](https://arxiv.org/abs/2601.02369)
*Ashlesha Hota,Shashwat Kumar,Daman Deep Singh,Abolfazl Asudeh,Palash Dey,Abhijnan Chakraborty*

Main category: cs.NI

TL;DR: The paper proposes DTAS, a heuristic to fairly redistribute UPI transactions under a 30% cap, proving its efficiency and practicality through experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to prevent a duopoly in India's digital payment ecosystem by enforcing a 30% transaction volume cap on UPI apps, which requires redistributing transactions without causing user inconvenience.

Method: The paper formalizes the problem as the Minimum Edge Activation Flow (MEAF) problem on a bipartite network and proposes the DTAS heuristic, which exploits flow structure and capacity reuse.

Result: Experiments show that DTAS achieves near-optimal solutions quickly, making it a viable method for enforcing transaction caps.

Conclusion: The paper concludes that the proposed Decoupled Two-Stage Allocation Strategy (DTAS) effectively addresses the computational challenge of redistributing UPI transactions to enforce the 30% cap, providing a practical and efficient solution.

Abstract: The concentration of digital payment transactions in just two UPI apps like PhonePe and Google Pay has raised concerns of duopoly in India s digital financial ecosystem. To address this, the National Payments Corporation of India (NPCI) has mandated that no single UPI app should exceed 30 percent of total transaction volume. Enforcing this cap, however, poses a significant computational challenge: how to redistribute user transactions across apps without causing widespread user inconvenience while maintaining capacity limits? In this paper, we formalize this problem as the Minimum Edge Activation Flow (MEAF) problem on a bipartite network of users and apps, where activating an edge corresponds to a new app installation. The objective is to ensure a feasible flow respecting app capacities while minimizing additional activations. We further prove that Minimum Edge Activation Flow is NP-Complete. To address the computational challenge, we propose scalable heuristics, named Decoupled Two-Stage Allocation Strategy (DTAS), that exploit flow structure and capacity reuse. Experiments on large semi-synthetic transaction network data show that DTAS finds solutions close to the optimal ILP within seconds, offering a fast and practical way to enforce transaction caps fairly and efficiently.

</details>


### [147] [A Deep-SIC Channel Estimator Scheme in NOMA Network](https://arxiv.org/abs/2601.02373)
*Sumita Majhi,Kaushal Shelke,Pinaki Mitra*

Main category: cs.NI

TL;DR: Deep-SIC 是一种基于 Transformer 和 PDD 的预测模型，显著提升 5G 网络切换可靠性，减少失败率和乒乓效应。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于瞬时信道测量的切换触发机制因信息过时或不准确导致的失败和乒乓效应问题。

Method: 提出 Deep-SIC 模型，利用 Transformer 和 PDD（来自 NOMA 的 SIC 副产品）进行信道质量预测和切换决策优化。

Result: 模型学习速度比现有算法快 68%，切换失败率降低 40%，乒乓效应显著减少，低 SNR 下 NRMSE 降低 20%。

Conclusion: Deep-SIC 通过 Transformer 模型和 PDD 反馈信号显著提升了 5G 及下一代移动自组织网络中的切换可靠性，减少了切换失败率和乒乓效应，同时具备快速学习和稳定性。

Abstract: In 5G and next-generation mobile ad-hoc networks, reliable handover is a key requirement, which guarantees continuity in connectivity, especially for mobile users and in high-density scenarios. However, conventional handover triggers based on instantaneous channel measurements are prone to failures and the ping-pong effect due to outdated or inaccurate channel state information. To address this, we introduce Deep-SIC, a knowledge-based channel prediction model that employs a Transformer-based approach to predict channel quality and optimise handover decisions. Deep-SIC is a unique model that utilises Partially Decoded Data (PDD), a byproduct of successive interference cancellation (SIC) in NOMA, as a feedback signal to improve its predictions continually. This special purpose enables learners to learn quickly and stabilise their learning. Our model learns 68\% faster than existing state-of-the-art algorithms, such as Graph-NOMA, while offering verifiable guarantees of stability and resilience to user mobility (Theorem~2). When simulated at the system level, it can be shown that our strategy can substantially enhance network performance: the handover failure rate can be reduced by up to 40\%, and the ping-pong effect can be mitigated, especially at vehicular speeds (e.g., 60 km/h). Moreover, Deep-SIC has a 20\% smaller normalised root mean square error (NRMSE) in low-SNR situations than state-of-the-art algorithms with linear computational complexity, $O(K)$. This work has introduced a new paradigm for robust and predictive mobility management in dynamic wireless networks.

</details>


### [148] [Optimal Oblivious Load-Balancing for Sparse Traffic in Large-Scale Satellite Networks](https://arxiv.org/abs/2601.02537)
*Rudrapatna Vallabh Ramakanth,Eytan Modiano*

Main category: cs.NI

TL;DR: 本文研究了环面网络中的无感知负载均衡问题，证明了稀疏流量下Valiant方案的非最优性，并提出了一个达到理论下限的最优方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于大规模低地球轨道（LEO）卫星网络中的负载均衡问题，该网络可建模为环面结构，且流量稀疏且集中在热点区域。

Method: 通过线性规划建模问题，并分析不同流量密度下的理论下限。

Result: 结果显示，当1<k≤N²/2时，无感知路由方案的最坏情况负载下限约为√(2k)/4；当N²/2≤k≤N²时，下限为N/4。

Conclusion: 本文证明了在稀疏流量下，Valiant负载均衡方案并非最优，并构建了一个达到理论下限的最优无感知负载均衡方案。此外，还发现了非无感知路由与无感知路由在最坏情况负载之间存在√2倍的差距。

Abstract: Oblivious load-balancing in networks involves routing traffic from sources to destinations using predetermined routes independent of the traffic, so that the maximum load on any link in the network is minimized. We investigate oblivious load-balancing schemes for a $N\times N$ torus network under sparse traffic where there are at most $k$ active source-destination pairs. We are motivated by the problem of load-balancing in large-scale LEO satellite networks, which can be modelled as a torus, where the traffic is known to be sparse and localized to certain hotspot areas. We formulate the problem as a linear program and show that no oblivious routing scheme can achieve a worst-case load lower than approximately $\frac{\sqrt{2k}}{4}$ when $1<k \leq N^2/2$ and $\frac{N}{4}$ when $N^2/2\leq k\leq N^2$. Moreover, we demonstrate that the celebrated Valiant Load Balancing scheme is suboptimal under sparse traffic and construct an optimal oblivious load-balancing scheme that achieves the lower bound. Further, we discover a $\sqrt{2}$ multiplicative gap between the worst-case load of a non-oblivious routing and the worst-case load of any oblivious routing. The results can also be extended to general $N\times M$ tori with unequal link capacities along the vertical and horizontal directions.

</details>


### [149] [A Secure Edge Gateway Architecture for Wi-Fi-Enabled IoT](https://arxiv.org/abs/2601.02376)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.NI

TL;DR: 提出了一种安全边缘网关架构，有效提升Wi-Fi物联网安全性，实验结果显示攻击事件显著减少且性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 旨在加强本地网络保护，同时不改变现有基础设施。

Method: 提出了一种安全边缘网关架构，作为Wi-Fi接入点与核心网络之间的中间控制点，监控流量、隔离不可信设备并防止常见的无线攻击。设计侧重于自适应流量过滤和轻量级策略执行。

Result: 在真实办公室环境中部署的原型网关，成功减少了87%的欺骗事件，将去认证后的恢复时间提高了42%，同时网络延迟仅增加3.1%，吞吐量降低不到4%。

Conclusion: 在边缘层实施安全功能可以显著提升Wi-Fi物联网环境的韧性，且不会引入明显的开销或需要专用硬件。

Abstract: This paper presents a Secure Edge Gateway Architecture for Wi-Fi-Enabled IoT designed to strengthen local network protection without altering existing infrastructure. The proposed gateway acts as an intermediate control point between Wi-Fi access points and the core network, monitoring traffic, isolating untrusted devices, and preventing common wireless attacks such as spoofing, deauthentication, and unauthorized access. The design focuses on adaptive traffic filtering and lightweight policy enforcement instead of complex analytical models, making it suitable for medium-sized network environments. The prototype gateway was deployed in a real office with around 70 total devices, including 28 IoT units such as sensors, cameras, and smart controllers. Over ten days of continuous operation, the system reduced successful spoofing incidents by 87% and improved recovery time after deauthentication by 42%, while increasing network latency by only 3.1% and reducing throughput by less than 4% compared to a baseline WPA3 configuration. These results confirm that implementing security functions at the edge layer can significantly improve the resilience of Wi-Fi-enabled IoT environments without introducing noticeable overhead or requiring specialized hardware.

</details>


### [150] [How to Discover Knowledge for FutureG: Contextual RAG and LLM Prompting for O-RAN](https://arxiv.org/abs/2601.02382)
*Nathan Conger,Nathan Scollar,Kemal Davaslioglu,Yalin E. Sagduyu,Sastry Kompella*

Main category: cs.NI

TL;DR: 提出了一个基于Contextual RAG的问答框架，用于5G/6G网络中的ORAN环境，显著提升问答准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决O-RAN快速变化的规范和接口带来的挑战，减少人工处理复杂文档的负担和错误。

Method: 采用Contextual RAG策略，通过候选答案引导文档检索和分块特定上下文，提升LLM性能。评估使用了ORANBenchmark-13K数据集，并比较了三种LLM和两种提示策略。

Result: Contextual RAG在准确率上优于标准RAG和基础提示，同时保持运行时间和CO2排放的竞争力。

Conclusion: Contextual RAG框架在5G/6G网络中展示了显著的性能提升，特别是在ORAN环境中，能够在不需微调LLM的情况下，提供更准确且可持续的问答解决方案。

Abstract: We present a retrieval-augmented question answering framework for 5G/6G networks, where the Open Radio Access Network (O-RAN) has become central to disaggregated, virtualized, and AI-driven wireless systems. While O-RAN enables multi-vendor interoperability and cloud-native deployments, its fast-changing specifications and interfaces pose major challenges for researchers and practitioners. Manual navigation of these complex documents is labor-intensive and error-prone, slowing system design, integration, and deployment. To address this challenge, we adopt Contextual Retrieval-Augmented Generation (Contextual RAG), a strategy in which candidate answer choices guide document retrieval and chunk-specific context to improve large language model (LLM) performance. This improvement over traditional RAG achieves more targeted and context-aware retrieval, which improves the relevance of documents passed to the LLM, particularly when the query alone lacks sufficient context for accurate grounding. Our framework is designed for dynamic domains where data evolves rapidly and models must be continuously updated or redeployed, all without requiring LLM fine-tuning. We evaluate this framework using the ORANBenchmark-13K dataset, and compare three LLMs, namely, Llama3.2, Qwen2.5-7B, and Qwen3.0-4B, across both Direct Question Answering (Direct Q&A) and Chain-of-Thought (CoT) prompting strategies. We show that Contextual RAG consistently improves accuracy over standard RAG and base prompting, while maintaining competitive runtime and CO2 emissions. These results highlight the potential of Contextual RAG to serve as a scalable and effective solution for domain-specific Q&A in ORAN and broader 5G/6G environments, enabling more accurate interpretation of evolving standards while preserving efficiency and sustainability.

</details>


### [151] [Base Station Deployment under EMF constrain by Deep Reinforcement learning](https://arxiv.org/abs/2601.02385)
*Mohammed Mallik,Guillaume Villemaud*

Main category: cs.NI

TL;DR: 论文提出结合cGAN和DQN的框架，显著优化5G/6G基站部署效率，满足动态场景需求。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络快速扩展和6G技术出现，需要高效仿真工具来评估覆盖范围和射频电磁场暴露等关键性能指标，并优化基站部署。

Method: 基于条件生成对抗网络（cGAN）预测位置特定的接收信号强度（RSS）和电磁场（EMF）暴露，并结合深度Q网络（DQN）进行基站部署优化。

Result: 相比传统射线追踪仿真，cGAN将推理和部署时间从几小时缩短到秒级；GAN-DQN框架在覆盖和暴露约束下实现了有效的基站部署策略。

Conclusion: 论文提出的cGAN-DQN框架显著减少了基站部署时间，从几小时缩短到几秒，适用于动态场景下的实时设计和调整，以满足预定义的网络性能目标。

Abstract: As 5G networks rapidly expand and 6G technologies emerge, characterized by dense deployments, millimeter-wave communications, and dynamic beamforming, the need for scalable simulation tools becomes increasingly critical. These tools must support efficient evaluation of key performance metrics such as coverage and radio-frequency electromagnetic field (RF-EMF) exposure, inform network design decisions, and ensure compliance with safety regulations. Moreover, base station (BS) placement is a crucial task in the network design, where satisfying coverage requirements is essential. To address these, based on our previous work, we first propose a conditional generative adversarial network (cGAN) that predicts location specific received signal strength (RSS), and EMF exposure simultaneously from the network topology, as images. As a network designing application, we propose a Deep Q Network (DQN) framework, using the trained cGAN, for optimal base station (BS) deployment in the network. Compared to conventional ray tracing simulations, the proposed cGAN reduces inference and deployment time from several hours to seconds. Unlike a standalone cGAN, which provides static performance maps, the proposed GAN-DQN framework enables sequential decision making under coverage and exposure constraints, learning effective deployment strategies that directly solve the BS placement problem. Thus making it well suited for real time design and adaptation in dynamic scenarios in order to satisfy pre defined network specific heterogeneous performance goals.

</details>


### [152] [Regional Resource Management for Service Provisioning in LEO Satellite Networks: A Topology Feature-Based DRL Approach](https://arxiv.org/abs/2601.02387)
*Chenxi Bao,Di Zhou,Min Sheng,Yan Shi,Jiandong Li,Zhili Sun*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度强化学习的自适应资源管理算法，显著提升了卫星网络在不同规模下的服务性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星网络的固有拓扑动态性和不确定的网络规模要求端到端服务供应的资源链必须高效重新规划，因此实现高度自适应的资源管理在实际部署应用中具有重要意义。

Method: 设计了区域资源管理（RRM）模式，并基于深度强化学习框架开发了一种基于拓扑特征的动态自适应资源管理算法。

Result: 数值结果表明，所提算法在收敛性能和收敛速度上表现最佳，服务性能显著提升，增益分别超过对比算法2.7%、11.9%和10.2%。

Conclusion: 该论文提出的基于拓扑特征的动态自适应资源管理算法在RRM模式下显著提升了不同网络规模下的服务性能，收敛性能和收敛速度均优于对比算法。

Abstract: Satellite networks with wide coverage are considered natural extensions to terrestrial networks for their long-distance end-to-end (E2E) service provisioning. However, the inherent topology dynamics of low earth orbit satellite networks and the uncertain network scales bring an inevitable requirement that resource chains for E2E service provisioning must be efficiently re-planned. Therefore, achieving highly adaptive resource management is of great significance in practical deployment applications. This paper first designs a regional resource management (RRM) mode and further formulates the RRM problem that can provide a unified decision space independent of the network scale. Subsequently, leveraging the RRM mode and deep reinforcement learning framework, we develop a topology feature-based dynamic and adaptive resource management algorithm to combat the varying network scales. The proposed algorithm successfully takes into account the fixed output dimension of the neural network and the changing resource chains for E2E service provisioning. The matched design of the service orientation information and phased reward function effectively improves the service performance of the algorithm under the RRM mode. The numerical results demonstrate that the proposed algorithm with the best convergence performance and fastest convergence rate significantly improves service performance for varying network scales, with gains over compared algorithms of more than 2.7%, 11.9%, and 10.2%, respectively.

</details>


### [153] [Generative AI for Networking](https://arxiv.org/abs/2601.02389)
*Faisal Zaman,Ouns Bouachir,Moayad Aloqaily,Ismaeel Al Ridhawi*

Main category: cs.NI

TL;DR: GenAI和LLMs通过自然语言处理和自注意力机制，显著提升了网络管理的自主性和效率，实现了自适应性网络的愿景。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI和LLMs如何通过自然语言理解和生成能力，优化网络管理，实现自主和自优化的通信系统。

Method: 利用自注意力机制的transformer模型进行长期流量预测。

Result: GenAI和LLMs能够高效处理客户查询、预测网络拥堵、自动化故障排除，并优化内容交付，从而提升电信服务的整体性能和用户体验。

Conclusion: GenAI和LLMs在提升网络性能和实现自适应性网络方面发挥了关键作用，展示了它们在电信网络中的变革性力量。

Abstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are revolutionizing network management systems, paving the way towards fully autonomous and self-optimizing communication systems. These models enable networks to address complex decision-making tasks across both short-term operational scenarios and long-term strategic planning. Through natural language understanding, LLMs can analyze customer inquiries, predict network congestion patterns, and automate troubleshooting processes, leading to more efficient customer support and network maintenance. GenAI can optimize content delivery by generating personalized recommendations, improving user engagement, and dynamically adjusting network resources based on real-time demands, ultimately enhancing overall performance and user experience in telecommunication services. In this paper, we discuss the pivotal role of GenAI in advancing network performance and achieving the ultimate objective of self-adaptive networks. Moreover, we present a use case that leverages the self-attention mechanism of transformers to perform long-term traffic prediction. Harnessing these cutting-edge technologies demonstrates the transformative power of LLM and GenAI in revolutionizing telecommunication networks, elevating resilience and adaptability to unprecedented levels.

</details>


### [154] [SLASh: Simulation of LISLs Aboard LEO Satellite Shells](https://arxiv.org/abs/2601.02396)
*Davy Romine,Andrew Kingery,Guanqun Song,Ting Zhu*

Main category: cs.NI

TL;DR: SLASh是一个可定制的LEO卫星网络模拟工具，用于优化网络设计和资源利用。


<details>
  <summary>Details</summary>
Motivation: 当前低地球轨道（LEO）卫星网络缺乏开源模拟模型，导致资源浪费和效率低下，SLASh旨在解决这一问题。

Method: 提出SLASh模拟工具，允许用户设计具有特定特性的模拟网络，并生成抽象的遥测数据以模拟网络中的数据传输。

Result: SLASh能够模拟真实世界条件下的网络性能，支持用户比较不同框架下的网络能力。

Conclusion: SLASh作为一种高度可定制的卫星网络模拟工具，填补了开源模拟模型的空白，能够帮助用户设计和测试特定特性的网络，从而优化资源利用并提升全球连接效率。

Abstract: Recent advances in satellite technology have introduced a new frontier of wireless networking by establishing Low Earth Orbit (LEO) Satellite networks that work to connect difficult to reach areas and improve global connectivity. These novel advancements lack robust open-source simulation models that can highlight potential bottlenecks or potential wasted resources, wasting terrestrial users and the companies that provide these networks time and money. To that end, we propose SLASh, a highly-customizable satellite network simulation which allows users to design a simulated network with specific characteristics, and constructs them analog to real-world conditions. Additionally, SLASh can generate abstract telemetry that can be simulated moving throughout the network, allowing users to compare network capabilities across a variety of frameworks.

</details>


### [155] [AI-Native Integrated Sensing and Communications for Self-Organizing Wireless Networks: Architectures, Learning Paradigms, and System-Level Design](https://arxiv.org/abs/2601.02398)
*S. Zhang,M. Feizarefi,A. F. Mirzaei*

Main category: cs.NI

TL;DR: 论文综述了AI原生ISAC自组织无线网络，提出了统一分类法并探讨了未来挑战，旨在推动6G及更高版本系统的部署。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络的规模、异构性和动态性增加，需要自组织网络智能来管理资源、拓扑和服务，而AI是实现这一愿景的关键。

Method: 论文采用系统级综述方法，开发了一个统一的分类法，涵盖ISAC信号模型、网络状态抽象、学习驱动的自组织机制及跨层架构。

Result: 论文综述了AI在ISAC中的应用，包括新兴学习范式（如深度强化学习、图学习等），并讨论了实际考虑因素（如权衡、可扩展性等）。

Conclusion: 该论文提出了AI原生ISAC自组织无线网络的综合框架，并探讨了未来研究方向，以实现可部署、可信且可扩展的6G及更高版本系统。

Abstract: Integrated Sensing and Communications (ISAC) is emerging as a foundational paradigm for next-generation wireless networks, enabling communication infrastructures to simultaneously support data transmission and environment sensing. By tightly coupling radio sensing with communication functions, ISAC unlocks new capabilities for situational awareness, localization, tracking, and network adaptation. At the same time, the increasing scale, heterogeneity, and dynamics of future wireless systems demand self-organizing network intelligence capable of autonomously managing resources, topology, and services. Artificial intelligence (AI), particularly learning-driven and data-centric methods, has become a key enabler for realizing this vision. This survey provides a comprehensive and system-level review of AI-native ISAC-enabled self-organizing wireless networks. We develop a unified taxonomy that spans: (i) ISAC signal models and sensing modalities, (ii) network state abstraction and perception from sensing-aware radio data, (iii) learning-driven self-organization mechanisms for resource allocation, topology control, and mobility management, and (iv) cross-layer architectures integrating sensing, communication, and network intelligence. We further examine emerging learning paradigms, including deep reinforcement learning, graph-based learning, multi-agent coordination, and federated intelligence that enable autonomous adaptation under uncertainty, mobility, and partial observability. Practical considerations such as sensing-communication trade-offs, scalability, latency, reliability, and security are discussed alongside representative evaluation methodologies and performance metrics. Finally, we identify key open challenges and future research directions toward deployable, trustworthy, and scalable AI-native ISAC systems for 6G and beyond.

</details>


### [156] [Auction-Driven Spectrum Allocation With AutoEncoder-Based Compression in Rural Wireless Networks: A Novel Framework for Reliable Telemedicine](https://arxiv.org/abs/2601.02402)
*Nadjemat El Houda Issaad,Ismail Lotfi,Mohamed Senouci,Zekri Lougmiri*

Main category: cs.NI

TL;DR: 论文提出混合框架（AE压缩+拍卖频谱分配），优化农村无线网络，提升远程医疗效率。


<details>
  <summary>Details</summary>
Motivation: 农村医疗面临通信效率低和频谱资源利用不足的问题，亟需解决方案以提升远程医疗服务质量。

Method: 论文提出了一种新颖的混合框架，结合了自动编码器（AE）的数据压缩技术和拍卖理论的动态频谱分配机制。

Result: 大量仿真验证表明，该框架能有效提高频谱利用率、传输效率和整体连接性。

Conclusion: 该论文提出的混合框架通过结合自动编码器数据压缩和拍卖理论频谱分配，显著提升了农村无线网络的通信效率和频谱利用率，为农村远程医疗基础设施提供了实用解决方案。

Abstract: Rural healthcare faces numerous challenges, including limited access to specialized medical services and diagnostic equipment, which delays patient care. Enhancing the ability to transmit medical images and data from rural areas to urban hospitals via wireless networks is critical. However, bandwidth limitations, unreliable networks, and concerns over data security and privacy hinder efficient transmission. Additionally, the high data volume of medical content and the limited battery life of IoT devices pose further challenges. To address these challenges, data compression techniques such as Autoencoders (AEs) offer promising solutions by significantly reducing the communication overhead without sacrificing essential image quality or details. Additionally, spectrum allocation mechanisms in rural areas are often inefficient, leading to poor resource utilization. Auction theory presents a dynamic and adaptive approach to optimize spectrum allocation. This paper proposes a novel hybrid framework that integrates AE-based data compression with auction-based spectrum allocation, addressing both communication efficiency and spectrum utilization in rural wireless networks. Extensive simulations validate the framework's ability to improve spectrum utilization, transmission efficiency, and overall connectivity, offering a practical solution for enhancing rural telemedicine infrastructure.

</details>


### [157] [Which Deep Learner? A Systematic Evaluation of Advanced Deep Forecasting Models Accuracy and Efficiency for Network Traffic Prediction](https://arxiv.org/abs/2601.02694)
*Eilaf MA Babai,Aalaa MA Babai,Koji Okamura*

Main category: cs.NI

TL;DR: 本研究评估了多种时间序列预测模型在真实网络流量数据上的表现，发现了平衡准确性和效率的架构，并提出了超越传统RNN的新方向。


<details>
  <summary>Details</summary>
Motivation: 网络流量预测对于现代网络管理的自动化至关重要，但由于网络环境和流量时间尺度的多样性，需要确定有效的部署选择和建模方向。

Method: 本研究系统地识别并评估了十二种先进的TSF模型，包括基于transformer和传统深度学习方法，以及三种统计基线模型，在四个真实流量数据集上进行了多时间尺度和预测范围的测试。

Result: 结果突出了性能区域、效率阈值以及平衡准确性和效率的有前景架构，展示了其对流量挑战的鲁棒性。

Conclusion: 研究表明，某些架构在准确性和效率之间取得了平衡，对网络流量挑战表现出鲁棒性，并提出了超越传统RNN的新方向。

Abstract: Network traffic prediction is essential for automating modern network management. It is a difficult time series forecasting (TSF) problem that has been addressed by Deep Learning (DL) models due to their ability to capture complex patterns. Advances in forecasting, from sophisticated transformer architectures to simple linear models, have improved performance across diverse prediction tasks. However, given the variability of network traffic across network environments and traffic series timescales, it is essential to identify effective deployment choices and modeling directions for network traffic prediction. This study systematically identify and evaluates twelve advanced TSF models -- including transformer-based and traditional DL approaches, each with unique advantages for network traffic prediction -- against three statistical baselines on four real traffic datasets, across multiple time scales and horizons, assessing performance, robustness to anomalies, data gaps, external factors, data efficiency, and resource efficiency in terms of time, memory, and energy. Results highlight performance regimes, efficiency thresholds, and promising architectures that balance accuracy and efficiency, demonstrating robustness to traffic challenges and suggesting new directions beyond traditional RNNs.

</details>


### [158] [Probabilistic Time Slot Leasing in TDMA-Based IoT Networks for Enhanced Channel Utilization](https://arxiv.org/abs/2601.02930)
*Hicham Lakhlef,Mohamed Ali Zormati,Khaled Abid,Toufik Ahmed*

Main category: cs.NI

TL;DR: 本文提出了一种分布式TDMA调度协议，通过动态重新分配未使用时间槽和轻量级概率机制，显著提升了物联网网络的性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模资源受限的无线网络（如物联网）中，高效的通信调度仍是一个关键挑战，现有解决方案在动态环境中表现不佳。

Method: 提出了一种新颖的完全分布式TDMA调度协议，通过智能地重新分配暂时不活跃节点的未使用时间槽给高需求节点，并采用轻量级概率机制管理未使用槽的临时租赁。

Result: 模拟实验表明，该协议显著提升了吞吐量、延迟和可靠性。

Conclusion: 本文提出的分布式TDMA调度协议在资源受限的无线网络中显著提升了吞吐量、延迟和可靠性，展示了其在下一代物联网网络中作为自适应和节能调度方案的潜力。

Abstract: In large-scale resource-constrained wireless networks, such as those prevalent in the Internet of Things (IoT), efficient communication scheduling remains a critical challenge. Among the various approaches, Time Division Multiple Access (TDMA) protocols have been widely adopted for their structured and collision-free communication capabilities. Nevertheless, despite extensive research in this area, current solutions often exhibit suboptimal performance, particularly in dynamic environments where node activity levels fluctuate over time.
  This paper introduces a novel fully distributed TDMA-based scheduling protocol that intelligently maximizes the utilization of communication resources. The proposed approach adaptively reallocates underutilized time slots, originally assigned to temporarily inactive nodes, to those experiencing higher communication demands. This dynamic reallocation not only improves channel utilization but also reduces idle periods, thereby enhancing overall network efficiency. To further enhance performance, we incorporate a lightweight probabilistic mechanism that governs the temporal leasing of unused slots. This mechanism balances the trade-off between slot availability and transmission reliability, minimizing packet loss while preserving fairness and stability within the network.
  Simulations across a range of network scenarios demonstrate that our protocol significantly improves throughput, latency, and reliability in resource-constrained environments. These results highlight the protocol's potential as a robust and scalable solution for adaptive and energy-efficient scheduling in next-generation IoT networks.

</details>


### [159] [Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System](https://arxiv.org/abs/2601.03171)
*Silvano Cortesi,Lukas Schulthess,Davide Plozza,Christian Vogt,Michele Magno*

Main category: cs.NI

TL;DR: Eco-WakeLoc achieves energy-neutral, high-accuracy indoor localization by combining wake-up radios, solar harvesting, and adaptive scheduling, reducing energy use while maintaining responsiveness.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between efficiency and responsiveness in indoor localization systems, especially for mobile robots in GPS-denied environments, where traditional RTLS either require continuous power or lack responsiveness.

Method: The system combines ultra-low power wake-up radios (WuRs) with solar energy harvesting, activating anchor nodes only on demand. It employs cooperative localization with active tags initiating ranging exchanges and passive tags reusing messages for TDOA positioning. An AIMD-based energy-aware scheduler adapts localization rates based on harvested energy.

Result: The system achieves centimeter-level positioning accuracy with measured energy consumption of 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment shows an average accuracy of 43cm in dynamic indoor environments, with year-long simulations confirming sustained energy-neutral operation.

Conclusion: Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling.

Abstract: Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling.

</details>


### [160] [Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey](https://arxiv.org/abs/2601.03181)
*Han Zhang,Mohammad Farzanullah,Mohammad Ghassemi,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 本文探讨了基础模型（FMs）在无线网络中的应用，特别是多模态FM在预测和控制任务中的潜力，并讨论了开发无线专用FM的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）被认为是重塑人工智能未来的突破性技术，将其整合到无线网络中可以开发出能够处理多样化网络管理请求和复杂无线任务的多功能AI代理。

Method: 讨论了FM在多模态上下文信息理解中的应用，并分别解释了FM在预测和控制任务中的使用方法。介绍了从数据集和方法论两个角度开发无线专用FM的进展。

Result: 探讨了FM在无线网络管理中的预测和控制任务中的应用，并提出了开发无线专用FM的方法。

Conclusion: 本文总结了FM增强无线网络面临的挑战和未来发展方向。

Abstract: Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of FMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless network management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.

</details>


### [161] [TaNG: Modeling Packet Classification with TSS-assisted Neural Networks on GPUs](https://arxiv.org/abs/2601.03187)
*Zhengyu Liao,Shiyou Qian*

Main category: cs.NI

TL;DR: TaNG通过神经网络和半结构化设计解决规则重叠问题，结合CPU-GPU框架显著提升吞吐量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现有学习型方法在重叠规则和GPU集成上的不足。

Method: TaNG采用单一神经网络结合半结构化设计和CPU-GPU混合流框架。

Result: 与NuevoMatch和NeuTree相比，TaNG吞吐量提升12.19倍和9.37倍，性能稳定性提升98.84倍和156.98倍。

Conclusion: TaNG在512k规则集上实现了比现有学习方法的显著吞吐量和性能稳定性提升。

Abstract: Packet classification is a core function in software-defined networks, and learning-based methods have recently shown significant throughput gains on large-scale rulesets. However, existing learning-based approaches struggle with overlapping rules, leading to incomplete model coverage or excessive rule replication. Their limited GPU integration also hampers performance with large-scale rulesets. To address these issues, we propose TaNG, which utilizes a single neural network trained on multi-dimensional features to ensure complete coverage without duplicating rules. TaNG employs a semi-structured design that combines the neural network model with a tuple space, reducing model complexity. Furthermore, we develop a mechanism based on the semi-structure for rule updates. Finally, we implement a CPU-GPU hybrid streaming framework tailored for learning-based methods, further enhancing throughput. On our GPU-based classification framework with 512k rulesets, TaNG achieves 12.19x and 9.37x higher throughput and 98.84x and 156.98x higher performance stability compared to two state-of-the-art learning methods NuevoMatch and NeuTree, respectively.

</details>


### [162] [oneTwin: Online Digital Network Twin via Neural Radio Radiance Field](https://arxiv.org/abs/2601.03216)
*Yuru Zhang,Ming Zhao,Qiang Liu,Nakjung Choi*

Main category: cs.NI

TL;DR: oneTwin是首个在线数字双胞胎系统，结合增强模拟器和NRRF，显著提升数字网络双胞胎的实时性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于模拟器和神经网络的方案）在保真度、同步性和可操作性方面无法有效实现数字网络双胞胎，因此需要一种新的在线数字双胞胎系统来预测物理层指标。

Method: oneTwin系统由两个主要组件构成：增强模拟器和NRRF。增强模拟器通过材料调优算法优化建筑材料，NRRF则通过神经学习算法持续更新DNN。系统采用Sionna RT作为模拟器，并开发新的DNN作为NRRF。

Result: 实验结果表明，oneTwin在实时更新（0.98秒）方面表现优异，并在分布内和分布外测试数据集上分别减少了36.39%和57.50%的双胞胎与真实差距。

Conclusion: oneTwin系统通过结合增强模拟器和神经无线电辐射场（NRRF），显著减少了数字网络双胞胎与真实网络之间的差距，并在实时更新和性能提升方面优于现有解决方案。

Abstract: Digital network twin is a promising technology that replicates real-world networks in real-time and assists with the design, operation, and management of next-generation networks. However, existing approaches (e.g., simulator-based and neural-based) cannot effectively realize the digital network twin, in terms of fidelity, synchronicity, and tractability. In this paper, we propose oneTwin, the first online digital twin system, for the prediction of physical layer metrics. We architect the oneTwin system with two primary components: an enhanced simulator and a neural radio radiance field (NRRF). On the one hand, we achieve the enhanced simulator by designing a material tuning algorithm that incrementally optimizes the building materials to minimize the twin-to-real gap. On the other hand, we achieve the NRRF by designing a neural learning algorithm that continually updates its DNNs based on both online and simulated data from the enhanced simulator. We implement oneTwin system using Sionna RT as the simulator and developing new DNNs as the NRRF, under a public cellular network. Extensive experimental results show that, compared to state-of-the-art solutions, oneTwin achieves real-time updating (0.98s), with 36.39% and 57.50% reductions of twin-to-real gap under in-distribution and out-of-distribution test datasets, respectively.

</details>


### [163] [inRAN: Interpretable Online Bayesian Learning for Network Automation in Open Radio Access Networks](https://arxiv.org/abs/2601.03219)
*Ming Zhao,Yuru Zhang,Qiang Liu,Ahan Kak,Nakjung Choi*

Main category: cs.NI

TL;DR: inRAN是一种可解释的在线贝叶斯学习框架，用于Open RAN的网络自动化，通过可解释模型和安全优化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖深度神经网络的“黑盒”策略，缺乏可解释性和透明度，阻碍了实际网络部署。

Method: inRAN框架包含三个关键组件：1) 通过集成Kolmogorov-Arnold Networks (KANs)构建可解释替代模型；2) 结合遗传搜索和信任区域下降法的安全优化求解器；3) 通过持续模型学习和自适应阈值偏移的在线动态跟踪器。

Result: 实验结果表明，inRAN在不可预测的时间演化网络动态下，以92.67%的保证率满足基于机会的约束，且资源使用与现有方法相当。

Conclusion: inRAN框架通过集成可解释的替代模型和安全优化求解器，显著提升了Open RAN中网络自动化的性能，尤其在不可预测的动态环境下表现优异。

Abstract: Emerging AI/ML techniques have been showing great potential in automating network control in open radio access networks (Open RAN). However, existing approaches heavily rely on blackbox policies parameterized by deep neural networks, which inherently lack interpretability, explainability, and transparency, and create substantial obstacles in practical network deployment. In this paper, we propose inRAN, a novel interpretable online Bayesian learning framework for network automation in Open RAN. The core idea is to integrate interpretable surrogate models and safe optimization solvers to continually optimize control actions, while adapting to non-stationary dynamics in real-world networks. We achieve the inRAN framework with three key components: 1) an interpretable surrogate model via ensembling Kolmogorov-Arnold Networks (KANs); 2) safe optimization solvers via integrating genetic search and trust-region descent method; 3) an online dynamics tracker via continual model learning and adaptive threshold offset. We implement inRAN in an end-to-end O-RAN-compliant network testbed, and conduct extensive over-the-air experiments with the focused use case of network slicing. The results show that, inRAN substantially outperforms state-of-the-art works, by guaranteeing the chance-based constraint with a 92.67% assurance ratio with comparative resource usage throughout the online network control, under unforeseeable time-evolving network dynamics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [164] [Proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024)](https://arxiv.org/abs/2601.02898)
*Wim Vanderbauwhede,Lauritz Thamsen,José Cano*

Main category: cs.DC

TL;DR: LOCO 2024研讨会聚焦低碳计算技术，旨在减少IT碳排放。


<details>
  <summary>Details</summary>
Motivation: 探讨低碳计算技术以减少信息技术的碳排放。

Method: 通过专家演讲、小组讨论和论文展示等形式进行交流。

Result: 汇集了全球专家对低碳计算的多方面见解和实践经验。

Conclusion: 本次研讨会总结了低碳计算领域的最新进展，并提出了未来研究方向。

Abstract: This is the proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024).

</details>


### [165] [Software-Defined Agentic Serving](https://arxiv.org/abs/2601.03197)
*Saurabh Agarwal,Marco Laju,Jayanth Srinivasa,Myungjin Lee,Aditya Akella*

Main category: cs.DC

TL;DR: 论文提出SDN启发的动态代理服务框架，优化多代理LLM管道的运行时通信效率。


<details>
  <summary>Details</summary>
Motivation: 随着多代理LLM管道的复杂性增加，现有服务范式无法适应动态服务条件，需要可编程且系统感知的服务系统。

Method: 提出了一种SDN（软件定义网络）启发的代理服务框架，动态控制通信属性。

Result: 该架构实现了高效、响应迅速的代理系统，并支持高级意图驱动的代理服务。

Conclusion: 该论文提出了一种新的SDN启发式代理服务框架，能够根据运行时状态动态调整通信属性，从而实现高效、响应迅速的代理系统，并为高级意图驱动的代理服务铺平了道路。

Abstract: As multi-agent LLM pipelines grow in complexity, existing serving paradigms fail to adapt to the dynamic serving conditions. We argue that agentic serving systems should be programmable and system-aware, unlike existing serving which statically encode the parameters. In this work, we propose a new SDN-inspired agentic serving framework that helps control the key attributes of communication based on runtime state. This architecture enables serving-efficient, responsive agent systems and paves the way for high-level intent-driven agentic serving.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [166] [Stroke Patches: Customizable Artistic Image Styling Using Regression](https://arxiv.org/abs/2601.03114)
*Ian Jaffray,John Bronskill*

Main category: cs.GR

TL;DR: 提出一种基于回归和笔画补丁的新方法，通过U-Net模型实现图像艺术风格的可控渲染。


<details>
  <summary>Details</summary>
Motivation: 现有神经风格迁移或基于扩散的方法缺乏对笔画组成和细节水平的显式控制，因此提出了一种新的回归方法。

Method: 使用基于U-Net的回归模型，结合程序生成的笔画补丁集，控制笔画的形状、大小、方向、密度、颜色和噪声水平。

Result: 训练后的模型能够以多种独特、富有表现力且可定制的风格渲染任何输入图像。

Conclusion: 该方法通过可扩展的笔画补丁集和U-Net回归模型，实现了对图像艺术风格的显式控制和多样化渲染。

Abstract: We present a novel, regression-based method for artistically styling images. Unlike recent neural style transfer or diffusion-based approaches, our method allows for explicit control over the stroke composition and level of detail in the rendered image through the use of an extensible set of stroke patches. The stroke patch sets are procedurally generated by small programs that control the shape, size, orientation, density, color, and noise level of the strokes in the individual patches. Once trained on a set of stroke patches, a U-Net based regression model can render any input image in a variety of distinct, evocative and customizable styles.

</details>
