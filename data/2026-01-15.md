<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DS](#cs.DS) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR](https://arxiv.org/abs/2601.08834)
*Yufeng Zhong,Lei Chen,Zhixiong Zeng,Xuanle Zhao,Deyang Jiang,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: FD-RL通过格式解耦强化学习优化OCR模型，显著提升格式敏感文档的识别性能，并在基准测试中创下新高。


<details>
  <summary>Details</summary>
Motivation: 发现高级OCR模型在格式化文本（如公式、表格）上熵显著高于纯文本，表明其在格式敏感文档上存在高输出不确定性。

Method: 提出格式解耦强化学习（FD-RL），利用高熵模式进行针对性优化，包括基于熵的数据过滤策略和针对不同格式类型的解耦奖励。

Result: FD-RL在OmniDocBench上平均得分90.41，创下了端到端模型的新纪录。

Conclusion: FD-RL通过格式解耦强化学习显著提升了OCR模型在格式敏感文档上的性能，验证了数据、训练、过滤和奖励策略的有效性。

Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.

</details>


### [2] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

Main category: cs.CV

TL;DR: 本研究分析了视觉语言和生成模型在图像旋转和分布偏移下的偏见传播和鲁棒性下降问题，提出了旋转鲁棒缓解策略，实验证明其有效提升鲁棒性并减少偏见。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）和生成图像模型在输入变换下的鲁棒性和公平性不足的问题。

Method: 提出了结合数据增强、表示对齐和模型级正则化的旋转鲁棒缓解策略。

Result: 实验结果表明，所提出的方法显著提高了鲁棒性，同时在不牺牲整体性能的情况下减少了偏见放大。

Conclusion: 本研究揭示了当前多模态系统的关键局限性，并提供了实用的缓解技术，以构建更可靠和公平的AI模型。

Abstract: Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.

</details>


### [3] [R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images](https://arxiv.org/abs/2601.08867)
*Qingyu Liu,Zhongjie Ba,Jianmin Guo,Qiu Wang,Zhibo Wang,Jie Shi,Kui Ren*

Main category: cs.CV

TL;DR: R$^2$BD通过统一重建模型和单步残差计算，高效检测多种生成模型的假图像，速度和精度均显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的方法效率低下且局限于扩散模型，无法泛化到其他生成范式如GANs。

Method: 提出R$^2$BD框架，包含G-LDM（模拟VAEs、GANs和扩散模型生成行为）和单步残差偏差计算模块。

Result: 在10个公开数据集的基准测试中，R$^2$BD比现有方法快22倍以上，且检测精度更高。

Conclusion: R$^2$BD框架通过G-LDM统一重建模型和残差偏差计算模块，显著提升了检测效率和泛化能力，在跨数据集评估中平均优于现有方法13.87%。

Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.
  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.
  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.

</details>


### [4] [Residual Cross-Modal Fusion Networks for Audio-Visual Navigation](https://arxiv.org/abs/2601.08868)
*Yi Wang,Yinfeng Yu,Bin Ren*

Main category: cs.CV

TL;DR: CRFN通过双向残差交互改进音频-视觉导航中的多模态融合，实验显示其在跨域泛化和模态依赖性分析上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决音频-视觉具身导航任务中多模态融合时异质特征交互的挑战，避免单模态主导或信息退化。

Method: 提出了一种跨模态残差融合网络（CRFN），通过双向残差交互实现互补建模和细粒度对齐，同时保持各自表征的独立性。

Result: 在Replica和Matterport3D数据集上，CRFN显著优于现有融合基线方法，并展现出更强的跨域泛化能力。

Conclusion: CRFN显著优于现有的融合基线方法，并在跨域泛化上表现更强。实验还揭示了智能体在不同数据集上表现出差异化的模态依赖性，为理解具身智能体的跨模态协作机制提供了新视角。

Abstract: Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.

</details>


### [5] [ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection](https://arxiv.org/abs/2601.08873)
*Hema Hariharan Samson*

Main category: cs.CV

TL;DR: ForensicFormer通过多尺度分层框架和跨注意力变换器，显著提升伪造检测性能，适用于未知操作技术场景。


<details>
  <summary>Details</summary>
Motivation: 传统取证方法在AI生成图像和高级编辑工具面前失效，亟需一种能适应未知操作技术的通用检测方法。

Method: 采用分层多尺度框架，结合低级别伪影检测、中级别边界分析及高级别语义推理，通过跨注意力变换器统一处理。

Result: 在七个不同测试集上平均准确率达86.8%，JPEG压缩下保持83%准确率（基线为66%），像素级伪造定位F1得分为0.76。

Conclusion: ForensicFormer通过结合多尺度分析和跨注意力变换器，显著提升了跨域伪造检测的准确性和鲁棒性，为实际应用提供了有效解决方案。

Abstract: The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.

</details>


### [6] [Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement](https://arxiv.org/abs/2601.08875)
*Jiahao Qin,Yiwen Wang*

Main category: cs.CV

TL;DR: SAR-Net通过场景-外观解耦实现跨域图像配准，实验显示其性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统配准方法在域偏移时因亮度一致性假设失效而表现不佳，需一种新方法解决跨域配准问题。

Method: 提出SAR-Net框架，通过场景一致性损失和域对齐损失实现跨域配准，理论证明了其有效性。

Result: 在双向扫描显微镜数据上，SAR-Net达到0.885 SSIM和0.979 NCC，性能提升3.1倍，且保持77 fps的实时性能。

Conclusion: SAR-Net通过场景-外观解耦的方法有效解决了跨域图像配准问题，理论分析和实验验证均表明其在保持实时性能的同时显著提升了配准精度。

Abstract: Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent space (Proposition 2). Empirically, we validate SAR-Net on bidirectional scanning microscopy, where coupled domain shift and geometric distortion create a challenging real-world testbed. Our method achieves 0.885 SSIM and 0.979 NCC, representing 3.1x improvement over the strongest baseline, while maintaining real-time performance (77 fps). Ablation studies confirm that both scene consistency and domain alignment losses are necessary: removing either degrades performance by 90% SSIM or causes 223x increase in latent alignment error, respectively. Code and data are available at https://github.com/D-ST-Sword/SAR-NET.

</details>


### [7] [The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models](https://arxiv.org/abs/2601.08876)
*Shuai Chen,Hao Chen,Yuanchen Bei,Tianyang Zhao,Zhibo Zhou,Feiran Huang*

Main category: cs.CV

TL;DR: 本文提出‘语义生命周期’框架，统一描述基础模型驱动的具身AI中语义知识的演变，分析了获取、表示和存储三个阶段的进展，并总结挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能体面临日益复杂的环境和开放任务，对更通用和鲁棒的语义处理能力的需求变得迫切。

Method: 提出了‘语义生命周期’作为统一框架，分析并比较了在获取、表示和存储三个关键阶段的最近进展。

Result: 通过基础模型的跨领域泛化能力和丰富语义先验，重塑了具身AI研究的格局。

Conclusion: 本文总结了现有挑战，并概述了未来研究的有前景方向。

Abstract: Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.

</details>


### [8] [TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts](https://arxiv.org/abs/2601.08881)
*Yu Xu,Hongbin Yan,Juan Cao,Yiji Cheng,Tiankai Hang,Runze He,Zijin Yin,Shiyi Zhang,Yuxin Zhang,Jintao Li,Chunyu Wang,Qinglin Lu,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 提出一种新框架，通过语义意图注入MoE路由，有效缓解任务干扰，提升图像生成与编辑性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集扩散变换器架构中共享参数空间在冲突目标（如局部编辑与主题驱动生成）间的妥协问题。

Method: 引入分层任务语义标注方案创建结构化任务描述符，并设计预测对齐正则化以对齐路由决策与高层语义。

Result: 模型在保真度和质量上优于密集基线，专家自然发展出清晰且语义相关的专业化。

Conclusion: 提出的框架通过语义意图注入MoE路由，有效缓解了任务干扰，提升了生成和编辑任务的质量和保真度。

Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.

</details>


### [9] [Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization](https://arxiv.org/abs/2601.08882)
*Thomas Snyder,H. Lexie Yang,Stefan Schnake,Steffen Schotthöfer*

Main category: cs.CV

TL;DR: DLRT框架通过流形约束优化压缩视觉Transformer模型，在边缘设备上实现高性能地理空间模型，参数减少显著且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备部署地理空间基础模型时，大参数量和压缩导致的精度损失问题。

Method: 利用流形约束优化框架DLRT，在迁移学习过程中压缩基于视觉Transformer的地理空间基础模型。

Result: 在多样化的地理空间基准测试中，该方法显著减少了参数数量且精度损失最小，优于现成的低秩方法如LoRA。

Conclusion: 通过DLRT框架压缩大型视觉Transformer模型，在保持任务特定精度的同时实现了显著参数减少，为资源受限的边缘设备提供了高性能的解决方案。

Abstract: Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. However, their large parameter counts and the accuracy loss often induced by compression limit practical adoption. In this work, we leverage manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning. By enforcing structured low-dimensional parameterizations aligned with downstream objectives, this approach achieves strong compression while preserving task-specific accuracy. We show that the method outperforms of-the-shelf low-rank methods as LoRA. Experiments on diverse geospatial benchmarks confirm substantial parameter reduction with minimal accuracy loss, enabling high-performing, on-device geospatial models.

</details>


### [10] [Adaptive few-shot learning for robust part quality classification in two-photon lithography](https://arxiv.org/abs/2601.08885)
*Sixian Jia,Ruo-Syuan Mei,Chenhui Shao*

Main category: cs.CV

TL;DR: 该论文提出了一种自适应计算机视觉框架，通过新颖性检测、增量学习和领域适应方法，在动态制造环境中高效维护质量模型，实验结果显示高准确率和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉模型在动态制造环境中表现不佳，无法检测新缺陷类别、从少量数据高效更新或适应新零件几何形状，因此需要一种自适应框架来解决这些问题。

Method: 框架基于相同的尺度鲁棒骨干模型，整合了三种关键方法：基于线性判别分析（LDA）的统计假设检验框架用于新颖性检测，两阶段基于排练的少样本增量学习策略，以及少样本领域对抗神经网络（DANN）用于少样本领域适应。

Result: 在双光子光刻（TPL）数据集上评估，假设检验方法以99-100%的准确率识别新类别批次；增量学习方法仅用K=20样本将新类别整合到92%准确率；领域适应模型仅用K=5样本在目标领域达到96.19%准确率。

Conclusion: 该论文提出的自适应计算机视觉框架在动态制造环境中表现出色，能够高效检测新缺陷类别、从少量数据更新模型并适应新零件几何形状，为质量模型的整个生命周期维护提供了稳健且数据高效的解决方案。

Abstract: Two-photon lithography (TPL) is an advanced additive manufacturing (AM) technique for fabricating high-precision micro-structures. While computer vision (CV) is proofed for automated quality control, existing models are often static, rendering them ineffective in dynamic manufacturing environments. These models typically cannot detect new, unseen defect classes, be efficiently updated from scarce data, or adapt to new part geometries. To address this gap, this paper presents an adaptive CV framework for the entire life-cycle of quality model maintenance. The proposed framework is built upon a same, scale-robust backbone model and integrates three key methodologies: (1) a statistical hypothesis testing framework based on Linear Discriminant Analysis (LDA) for novelty detection, (2) a two-stage, rehearsal-based strategy for few-shot incremental learning, and (3) a few-shot Domain-Adversarial Neural Network (DANN) for few-shot domain adaptation. The framework was evaluated on a TPL dataset featuring hemisphere as source domain and cube as target domain structures, with each domain categorized into good, minor damaged, and damaged quality classes. The hypothesis testing method successfully identified new class batches with 99-100% accuracy. The incremental learning method integrated a new class to 92% accuracy using only K=20 samples. The domain adaptation model bridged the severe domain gap, achieving 96.19% accuracy on the target domain using only K=5 shots. These results demonstrate a robust and data-efficient solution for deploying and maintaining CV models in evolving production scenarios.

</details>


### [11] [Variance-Penalized MC-Dropout as a Learned Smoothing Prior for Brain Tumour Segmentation](https://arxiv.org/abs/2601.08956)
*Satyaki Roy Chowdhury,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: UAMSA-UNet通过多尺度注意力和平滑先验，显著提升脑肿瘤分割精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和U-Net方法在肿瘤浸润区域产生噪声边界，需要一种更精确的分割方法。

Method: 引入UAMSA-UNet，一种基于蒙特卡洛Dropout的不确定性感知多尺度注意力贝叶斯U-Net，通过数据驱动的平滑先验和多尺度特征融合，捕捉细节和全局上下文。

Result: 在BraTS2023上，Dice相似系数提升3.3%，平均IoU提升2.7%；在BraTS2024上，Dice提升4.5%，IoU提升4.0%，同时FLOPs减少42.5%。

Conclusion: UAMSA-UNet通过结合多尺度注意力机制和学习到的平滑先验，不仅提升了分割质量，还提高了计算效率，为未来与基于transformer的模块集成提供了灵活的基础。

Abstract: Brain tumor segmentation is essential for diagnosis and treatment planning, yet many CNN and U-Net based approaches produce noisy boundaries in regions of tumor infiltration. We introduce UAMSA-UNet, an Uncertainty-Aware Multi-Scale Attention-based Bayesian U-Net that in- stead leverages Monte Carlo Dropout to learn a data-driven smoothing prior over its predictions, while fusing multi-scale features and attention maps to capture both fine details and global context. Our smoothing-regularized loss augments binary cross-entropy with a variance penalty across stochas- tic forward passes, discouraging spurious fluctuations and yielding spatially coherent masks. On BraTS2023, UAMSA- UNet improves Dice Similarity Coefficient by up to 3.3% and mean IoU by up to 2.7% over U-Net; on BraTS2024, it delivers up to 4.5% Dice and 4.0% IoU gains over the best baseline. Remarkably, it also reduces FLOPs by 42.5% rel- ative to U-Net++ while maintaining higher accuracy. These results demonstrate that, by combining multi-scale attention with a learned smoothing prior, UAMSA-UNet achieves both better segmentation quality and computational efficiency, and provides a flexible foundation for future integration with transformer-based modules for further enhanced segmenta- tion results.

</details>


### [12] [Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring](https://arxiv.org/abs/2601.08977)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: Thermo-LIO是一种新型多传感器系统，通过融合热成像和LiDAR提升结构健康监测的精确性和覆盖范围，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统二维热成像技术在评估复杂几何形状、难以接近区域及次表面缺陷方面存在局限性，需要一种更有效的监测方法。

Method: 研究开发了一种多模态融合方法，结合热成像和LiDAR，实现精确校准和同步，并与LiDAR-惯性里程计（LIO）集成，以实现大规模结构的全覆盖监测。

Result: 实验验证显示，Thermo-LIO能比传统方法更准确地检测热异常和结构缺陷，提升诊断精度并实现实时处理。

Conclusion: Thermo-LIO系统通过融合热成像和高分辨率LiDAR，显著提升了结构健康监测（SHM）的精确性和覆盖范围，为大型土木基础设施的先进监测方法提供了重要支持。

Abstract: Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects. This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR. To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings. Second, it integrates this fusion approach with LiDAR-Inertial Odometry (LIO), enabling full coverage of large-scale structures and allowing for detailed monitoring of temperature variations and defect detection across inspection cycles. Experimental validations, including case studies on a bridge and a hall building, demonstrate that Thermo-LIO can detect detailed thermal anomalies and structural defects more accurately than traditional methods. The system enhances diagnostic precision, enables real-time processing, and expands inspection coverage, highlighting the crucial role of multimodal sensor integration in advancing SHM methodologies for large-scale civil infrastructure.

</details>


### [13] [Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams](https://arxiv.org/abs/2601.09107)
*Lachlan Holden,Feras Dayoub,Alberto Candela,David Harvey,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种利用跨视图神经网络和合成数据在行星机器人任务中实现高精度定位的新方法，解决了真实数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 行星机器人任务的自主性需要高精度定位，但真实空间数据稀缺，因此需要开发一种能在有限视野的单目RGB图像输入下实现定位的方法。

Method: 使用跨视图定位的双编码器深度神经网络，结合语义分割和视觉基础模型，利用大量合成数据弥补真实图像数据的不足，并通过粒子滤波进行状态估计。

Result: 该方法在简单和复杂轨迹上均能实现基于地面视图图像序列的准确位置估计。

Conclusion: 通过结合跨视图定位的双编码器深度神经网络、语义分割和粒子滤波，本文提出了一种在行星机器人任务中实现高精度定位的新方法。

Abstract: Accurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use ground-aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.

</details>


### [14] [SAM-pose2seg: Pose-Guided Human Instance Segmentation in Crowds](https://arxiv.org/abs/2601.08982)
*Constantin Kolomiiets,Miroslav Purkrabek,Jiri Matas*

Main category: cs.CV

TL;DR: 通过姿势引导微调SAM模型，提升其在遮挡情况下的分割性能，同时保持泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在遮挡情况下（关键点部分或完全不可见）的分割性能下降问题。

Method: 采用PoseMaskRefine微调策略，将高可见性的姿势关键点整合到SAM原有的迭代校正过程中，并在推理时仅选择三个最高可见性的关键点以简化提示。

Result: 实验表明，该方法在多个数据集上提高了分割的鲁棒性和准确性，甚至可以从单个关键点预测出准确的掩码。

Conclusion: 通过姿势引导的微调，SAM模型在保持原有泛化能力的同时，实现了对遮挡情况更鲁棒和准确的人体分割。

Abstract: Segment Anything (SAM) provides an unprecedented foundation for human segmentation, but may struggle under occlusion, where keypoints may be partially or fully invisible. We adapt SAM 2.1 for pose-guided segmentation with minimal encoder modifications, retaining its strong generalization. Using a fine-tuning strategy called PoseMaskRefine, we incorporate pose keypoints with high visibility into the iterative correction process originally employed by SAM, yielding improved robustness and accuracy across multiple datasets. During inference, we simplify prompting by selecting only the three keypoints with the highest visibility. This strategy reduces sensitivity to common errors, such as missing body parts or misclassified clothing, and allows accurate mask prediction from as few as a single keypoint. Our results demonstrate that pose-guided fine-tuning of SAM enables effective, occlusion-aware human segmentation while preserving the generalization capabilities of the original model. The code and pretrained models will be available at https://mirapurkrabek.github.io/BBox-MaskPose.

</details>


### [15] [Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets](https://arxiv.org/abs/2601.09605)
*Jeremiah Coholich,Justin Wit,Robert Azarcon,Zsolt Kira*

Main category: cs.CV

TL;DR: MANGO是一种创新的图像翻译方法，通过特定损失函数和设计，有效提升机器人策略在多样视角下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中视觉策略对相机视角变化的脆弱性，以及真实世界数据稀缺和视角多样性不足的问题。

Method: 提出MANGO方法，结合了分割条件InfoNCE损失、高度正则化的判别器设计和改进的PatchNCE损失。

Result: MANGO在测试中优于其他图像翻译方法，使用其增强数据训练的模仿学习策略在未见视角下的成功率高达60%。

Conclusion: MANGO通过其创新的图像翻译方法有效解决了机器人操作中的视觉sim2real挑战，显著提升了策略在未见视角下的成功率。

Abstract: Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\% on views that the non-augmented policy fails completely on.

</details>


### [16] [Instance camera focus prediction for crystal agglomeration classification](https://arxiv.org/abs/2601.09004)
*Xiaoyu Ji,Chenhao Zhang,Tyler James Downard,Zoltan Nagy,Ali Shakouri,Fengqing Zhu*

Main category: cs.CV

TL;DR: 通过焦点预测网络和实例分割模型结合的方法，提高了晶体聚集分类和分割的准确性，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 由于二维成像的固有局限性，从显微图像分析晶体聚集具有挑战性，特别是重叠晶体可能在不同深度层上看似连接。

Method: 首先通过实例相机焦点预测网络量化相机焦点，预测2类焦点级别，然后结合实例分割模型和预测的焦点级别进行聚集分类。

Result: 提出的方法在晶体聚集分类和分割精度上优于基线模型。

Conclusion: 提出的方法在晶体聚集分类和分割精度上优于基线模型，特别是在高氯酸铵晶体和糖晶体数据集上表现优异。

Abstract: Agglomeration refers to the process of crystal clustering due to interparticle forces. Crystal agglomeration analysis from microscopic images is challenging due to the inherent limitations of two-dimensional imaging. Overlapping crystals may appear connected even when located at different depth layers. Because optical microscopes have a shallow depth of field, crystals that are in-focus and out-of-focus in the same image typically reside on different depth layers and do not constitute true agglomeration. To address this, we first quantified camera focus with an instance camera focus prediction network to predict 2 class focus level that aligns better with visual observations than traditional image processing focus measures. Then an instance segmentation model is combined with the predicted focus level for agglomeration classification. Our proposed method has a higher agglomeration classification and segmentation accuracy than the baseline models on ammonium perchlorate crystal and sugar crystal dataset.

</details>


### [17] [Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning](https://arxiv.org/abs/2601.09708)
*Chi-Pin Huang,Yunze Man,Zhiding Yu,Min-Hung Chen,Jan Kautz,Yu-Chiang Frank Wang,Fu-En Yang*

Main category: cs.CV

TL;DR: Fast-ThinkAct 是一种高效的推理框架，通过潜在推理链减少延迟，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）任务推理方法因冗长的推理轨迹导致高推理延迟，影响了实际应用的效率。

Method: Fast-ThinkAct 通过从教师模型中蒸馏学习潜在推理链（CoT），并采用偏好引导的目标来对齐操作轨迹，从而实现了语言和视觉规划能力的迁移。

Result: 实验表明，Fast-ThinkAct 在推理延迟上比现有技术降低了89.3%，同时在多种任务中保持了高性能。

Conclusion: Fast-ThinkAct 通过可表达的潜在推理实现了紧凑且高效的规划，显著降低了推理延迟，同时在长时规划、少样本适应和失败恢复方面保持了高性能。

Abstract: Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.

</details>


### [18] [Changes in Visual Attention Patterns for Detection Tasks due to Dependencies on Signal and Background Spatial Frequencies](https://arxiv.org/abs/2601.09008)
*Amar Kavuri,Howard C. Gifford,Mini Das*

Main category: cs.CV

TL;DR: 研究图像和信号特性对视觉注意机制的影响，发现检测性能受后期感知阶段限制，信号可检测性与目标形态和背景复杂性相关。


<details>
  <summary>Details</summary>
Motivation: 研究图像和信号特性在数字图像信号检测任务中对视觉注意机制的影响，应用于涉及复杂异质背景的信号或模式识别的数字成像领域。

Method: 使用模拟乳腺断层图像作为平台，通过数字乳腺模型（Bakic和XCAT）生成不同乳腺密度和结构的数字乳腺断层合成（DBT）图像。在投影过程中随机插入两种具有不同空间频率特性的病变，生成异常案例。六名人类观察者参与了定位和检测3毫米球形病变和6毫米针状病变的观察研究，并收集了眼动数据以估计注视指标。

Result: 发现视觉注意机制因背景和信号空间频率依赖性的不同而有所差异，针状病变的注视持续时间增加。

Conclusion: 检测性能在复杂视觉环境中受到后期感知阶段的强烈限制，决策失败是错误的主要原因。信号可检测性同时受目标形态和背景复杂性的影响，揭示了局部信号特征与全局解剖噪声之间的关键相互作用。

Abstract: We aim to investigate the impact of image and signal properties on visual attention mechanisms during a signal detection task in digital images. The application of insight yielded from this work spans many areas of digital imaging where signal or pattern recognition is involved in complex heterogenous background. We used simulated tomographic breast images as the platform to investigate this question. While radiologists are highly effective at analyzing medical images to detect and diagnose diseases, misdiagnosis still occurs. We selected digital breast tomosynthesis (DBT) images as a sample medical images with different breast densities and structures using digital breast phantoms (Bakic and XCAT). Two types of lesions (with distinct spatial frequency properties) were randomly inserted in the phantoms during projections to generate abnormal cases. Six human observers participated in observer study designed for a locating and detection of an 3-mm sphere lesion and 6-mm spicule lesion in reconstructed in-plane DBT slices. We collected eye-gaze data to estimate gaze metrics and to examine differences in visual attention mechanisms. We found that detection performance in complex visual environments is strongly constrained by later perceptual stages, with decision failures accounting for the largest proportion of errors. Signal detectability is jointly influenced by both target morphology and background complexity, revealing a critical interaction between local signal features and global anatomical noise. Increased fixation duration on spiculated lesions suggests that visual attention is differentially engaged depending on background and signal spatial frequency dependencies.

</details>


### [19] [Depth-Wise Representation Development Under Blockwise Self-Supervised Learning for Video Vision Transformers](https://arxiv.org/abs/2601.09040)
*Jonas Römer,Timo Dickscheid*

Main category: cs.CV

TL;DR: 研究探讨了块式自监督学习在掩码视频建模中的应用，发现块式训练能产生接近端到端基线的表示，并揭示了早期结构暴露和后期饱和的特点。


<details>
  <summary>Details</summary>
Motivation: 受块式自监督学习（BWSSL）最新进展的启发，研究是否可以在不使用端到端反向传播的情况下训练掩码视频变换器。

Method: 通过将编码器分割为块，每个块使用局部掩码重建损失进行优化，应用块式学习到掩码自动编码视频视觉变换器。

Result: 在不同模型大小和分割粒度下，训练收敛并产生接近匹配端到端基线的表示，通过线性探测和检索代理进行评估。

Conclusion: 研究发现，块式训练在早期阶段暴露了更高层次的结构，而后期块则趋于饱和并在更几何保持的机制中运行。此外，块式训练还可能导致令牌级别的变化，这些变化与更强的早期混合一致，但可能被汇总指标忽略。这些发现指出了后期块饱和和界面形成为剩余差距的贡献者。

Abstract: End-to-end backpropagation couples all layers through a global error signal, enabling coordinated learning but requiring long-range credit assignment. Motivated by recent progress in blockwise self-supervised learning (BWSSL), we ask whether masked video transformers can be trained without end-to-end backpropagation. Applying BWSSL to masked video modeling remains relatively underexplored and must handle spatiotemporal context and long-range temporal structure. More broadly, analyses that compare BWSSL and end-to-end training in terms of learning dynamics and depth-wise representation development remain sparse. We apply blockwise learning to a masked autoencoding video vision transformer by partitioning the encoder into blocks, each of which is optimized with a local masked reconstruction loss. Across model sizes and partition granularities, training converges and yields representations close to matched end-to-end baselines under linear-probe and retrieval proxies. In order to compare intermediate representations, we analyze depth-wise decodability, inter-block similarity, and patch-level diagnostics. Blockwise training exposes higher-level structure earlier, while later blocks saturate and operate in a more geometry-preserving regime. It can also induce token-level shifts consistent with stronger early mixing that pooled metrics can miss. These findings point to late-block saturation and interface formation as contributors to the remaining gap.

</details>


### [20] [Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking](https://arxiv.org/abs/2601.09078)
*Junze Shi,Yang Yu,Jian Shi,Haibo Luo*

Main category: cs.CV

TL;DR: STDTrack通过密集采样和多帧信息融合，将时空依赖集成到轻量级跟踪器中，显著提升性能，在多个基准测试中达到SOTA，同时保持实时效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级跟踪器在训练中普遍采用稀疏采样（每序列仅一个模板和一个搜索图像），未能充分利用视频中的时空信息，导致性能受限。

Method: 提出STDTrack框架，采用密集视频采样最大化时空信息利用，引入时空传播令牌指导每帧特征提取，设计MFIFM模块融合历史上下文信息，并开发多尺度预测头以适应不同尺寸目标。

Result: 在六个基准测试中取得最先进结果，尤其在GOT-10k上性能媲美某些非实时高性能跟踪器（如MixFormer），同时在GPU和CPU上分别达到192 FPS和41 FPS的实时效率。

Conclusion: STDTrack通过密集视频采样和多帧信息融合模块（MFIFM）成功将可靠的时空依赖集成到轻量级跟踪器中，显著提升了性能，并在多个基准测试中达到最先进水平。

Abstract: Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).

</details>


### [21] [Small but Mighty: Dynamic Wavelet Expert-Guided Fine-Tuning of Large-Scale Models for Optical Remote Sensing Object Segmentation](https://arxiv.org/abs/2601.09108)
*Yanguang Sun,Chao Wang,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: WEFT是一种动态小波专家引导的微调方法，通过减少可训练参数，高效适应大规模基础模型于光学遥感图像分割任务，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于中等规模预训练模型，难以充分利用大规模基础模型的潜力，且全参数微调导致训练困难。

Method: 通过引入任务特定的小波专家提取器和专家引导的条件适配器，动态调节小波专家的输出并注入可训练特征，实现高效微调。

Result: WEFT在三个光学遥感图像数据集上优于21种最先进方法，并在伪装、自然和医疗场景中取得最佳结果。

Conclusion: WEFT提出了一种新颖的动态小波专家引导微调范式，显著提升了光学遥感图像分割任务的性能，并在多种场景中实现了最优结果。

Abstract: Accurately localizing and segmenting relevant objects from optical remote sensing images (ORSIs) is critical for advancing remote sensing applications. Existing methods are typically built upon moderate-scale pre-trained models and employ diverse optimization strategies to achieve promising performance under full-parameter fine-tuning. In fact, deeper and larger-scale foundation models can provide stronger support for performance improvement. However, due to their massive number of parameters, directly adopting full-parameter fine-tuning leads to pronounced training difficulties, such as excessive GPU memory consumption and high computational costs, which result in extremely limited exploration of large-scale models in existing works. In this paper, we propose a novel dynamic wavelet expert-guided fine-tuning paradigm with fewer trainable parameters, dubbed WEFT, which efficiently adapts large-scale foundation models to ORSIs segmentation tasks by leveraging the guidance of wavelet experts. Specifically, we introduce a task-specific wavelet expert extractor to model wavelet experts from different perspectives and dynamically regulate their outputs, thereby generating trainable features enriched with task-specific information for subsequent fine-tuning. Furthermore, we construct an expert-guided conditional adapter that first enhances the fine-grained perception of frozen features for specific tasks by injecting trainable features, and then iteratively updates the information of both types of feature, allowing for efficient fine-tuning. Extensive experiments show that our WEFT not only outperforms 21 state-of-the-art (SOTA) methods on three ORSIs datasets, but also achieves optimal results in camouflage, natural, and medical scenarios. The source code is available at: https://github.com/CSYSI/WEFT.

</details>


### [22] [SAM-Aug: Leveraging SAM Priors for Few-Shot Parcel Segmentation in Satellite Time Series](https://arxiv.org/abs/2601.09110)
*Kai Hu,Yaozu Feng,Vladimir Lysenko,Ya Guo Member,Huayi Wu*

Main category: cs.CV

TL;DR: SAM-Aug利用SAM的几何感知能力提升少样本遥感图像分割性能，无需额外标注，在PASTIS-R基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决标记数据稀缺或获取成本高的情况下，少样本时间序列遥感图像语义分割性能下降的问题。

Method: 构建无云合成图像，并利用SAM在无监督方式下生成几何感知掩码先验，通过RegionSmoothLoss损失函数将这些先验整合到训练中，以增强预测一致性。

Result: 在PASTIS-R基准测试中，SAM-Aug在5%标记数据设置下，平均测试mIoU达到36.21%，优于现有基线方法2.33个百分点（相对提升6.89%）。最佳情况下（seed=42），测试mIoU达40.28%，相对提升11.2%。

Conclusion: SAM-Aug通过利用Segment Anything Model（SAM）的几何感知分割能力，显著提升了少样本时间序列遥感图像的分割性能，为土地覆盖监测提供了一种无需人工标注或模型微调的可扩展解决方案。

Abstract: Few-shot semantic segmentation of time-series remote sensing images remains a critical challenge, particularly in regions where labeled data is scarce or costly to obtain. While state-of-the-art models perform well under full supervision, their performance degrades significantly under limited labeling, limiting their real-world applicability. In this work, we propose SAM-Aug, a new annotation-efficient framework that leverages the geometry-aware segmentation capability of the Segment Anything Model (SAM) to improve few-shot land cover mapping. Our approach constructs cloud-free composite images from temporal sequences and applies SAM in a fully unsupervised manner to generate geometry-aware mask priors. These priors are then integrated into training through a proposed loss function called RegionSmoothLoss, which enforces prediction consistency within each SAM-derived region across temporal frames, effectively regularizing the model to respect semantically coherent structures. Extensive experiments on the PASTIS-R benchmark under a 5 percent labeled setting demonstrate the effectiveness and robustness of SAM-Aug. Averaged over three random seeds (42, 2025, 4090), our method achieves a mean test mIoU of 36.21 percent, outperforming the state-of-the-art baseline by +2.33 percentage points, a relative improvement of 6.89 percent. Notably, on the most favorable split (seed=42), SAM-Aug reaches a test mIoU of 40.28 percent, representing an 11.2 percent relative gain with no additional labeled data. The consistent improvement across all seeds confirms the generalization power of leveraging foundation model priors under annotation scarcity. Our results highlight that vision models like SAM can serve as useful regularizers in few-shot remote sensing learning, offering a scalable and plug-and-play solution for land cover monitoring without requiring manual annotations or model fine-tuning.

</details>


### [23] [Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning](https://arxiv.org/abs/2601.09111)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 提出slow4fast-VLN框架，结合快速和慢速推理模块，提升视觉语言导航在开放世界中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统VLN方法在封闭集假设下训练和测试，难以应对现实世界中的多样未见环境和指令，因此需要提升导航的泛化能力。

Method: 提出slow4fast-VLN框架，包含快速推理模块（实时输出动作）和慢速推理模块（分析记忆并提取经验），通过动态交互优化导航策略。

Result: 框架通过快速与慢速推理的交互，实现了对未见场景的动态适应和高效导航。

Conclusion: 通过提出的slow4fast-VLN框架，实现了在开放世界中面对未见环境和指令时的动态适应与高效导航，显著提升了传统VLN方法的泛化能力。

Abstract: Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.

</details>


### [24] [LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models](https://arxiv.org/abs/2601.09116)
*Haoyan Gong,Hongbin Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于Qwen3-VL的端到端结构感知多模态推理框架，通过字符感知多模态推理模块（CMRM）和LoRA微调策略，解决了车牌识别中的退化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的车牌识别（LPR）面临运动模糊、低分辨率和复杂照明等严重退化问题。传统的“恢复-再识别”两阶段范式存在像素级优化目标与字符识别语义目标不一致的问题，导致伪影干扰和错误累积。视觉语言模型（VLMs）虽然具有强大的通用能力，但缺乏对车牌字符序列（如固定长度、特定顺序）的显式结构建模。

Method: 论文提出了一种端到端的结构感知多模态推理框架，核心创新是字符感知多模态推理模块（CMRM），该模块通过可学习的字符槽查询和交叉注意力机制，从视觉特征中检索字符位置的细粒度证据，并通过残差调制将这些字符感知表示注入视觉令牌中。

Result: 在合成和现实世界严重退化数据集上的大量实验表明，该方法显著优于现有的恢复-识别组合和通用VLMs。

Conclusion: 该论文提出的基于Qwen3-VL的结构感知多模态推理框架，通过字符感知多模态推理模块（CMRM）和LoRA参数高效微调策略，显著提升了低质量文本识别任务的性能，验证了在大模型中引入结构化推理的优越性。

Abstract: Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.

</details>


### [25] [LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data](https://arxiv.org/abs/2601.09118)
*Jackie Alex,Guoqiang Huan*

Main category: cs.CV

TL;DR: LPCANet是一种高效、轻量级的铁路缺陷检测网络，结合RGB-D数据和多模态融合，显著提升性能并验证了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于视觉的铁路缺陷检测方法的高计算复杂度、参数过多和准确性不足的问题。

Method: 提出了一种轻量级金字塔交叉注意力网络（LPCANet），结合MobileNetv2、轻量级金字塔模块（LPM）、交叉注意力机制（CAM）和空间特征提取器（SFE）进行多模态融合和结构分析。

Result: 在三个无监督RGB-D铁路数据集上，LPCANet以仅9.90百万参数、2.50 G FLOPs和162.60 fps的推理速度达到最先进性能，显著优于18种现有方法。

Conclusion: LPCANet在铁路缺陷检测中表现出色，具有高效率和准确性，为工业缺陷检测提供了实用价值。未来工作将集中于进一步模型压缩以实现实时部署。

Abstract: This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\% in $S_α$, +0.86\% in IOU, and +1.77\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.

</details>


### [26] [Beyond Seen Bounds: Class-Centric Polarization for Single-Domain Generalized Deep Metric Learning](https://arxiv.org/abs/2601.09121)
*Xin Yuan,Meiqi Wan,Wei Liu,Xin Xu,Zheng Wang*

Main category: cs.CV

TL;DR: CenterPolar通过离心扩展和向心约束动态调整领域分布，提升了SDG-DML在未见类别和领域的泛化能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: SDG-DML面临测试时的类别和领域双重偏移问题，现有方法依赖代理扩展生成样本，但无法模拟实际中的广泛领域偏移。因此，需要一种能动态扩展和约束领域分布的方法。

Method: CenterPolar包含两个协作的类中心极化阶段：类中心离心扩展（C^3E）和类中心向心约束（C^4）。C^3E通过离心扩展将源数据远离类中心以泛化到更多未见领域；C^4则通过向心约束将所有样本拉向类中心并增强类间分离。

Result: 在多个数据集（CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, Office-Home）上的实验表明，CenterPolar优于现有方法。

Conclusion: CenterPolar框架通过动态扩展和约束领域分布，显著提升了SDG-DML任务在未见类别和领域上的泛化能力，实验验证了其优于现有方法的性能。

Abstract: Single-domain generalized deep metric learning (SDG-DML) faces the dual challenge of both category and domain shifts during testing, limiting real-world applications. Therefore, aiming to learn better generalization ability on both unseen categories and domains is a realistic goal for the SDG-DML task. To deliver the aspiration, existing SDG-DML methods employ the domain expansion-equalization strategy to expand the source data and generate out-of-distribution samples. However, these methods rely on proxy-based expansion, which tends to generate samples clustered near class proxies, failing to simulate the broad and distant domain shifts encountered in practice. To alleviate the problem, we propose CenterPolar, a novel SDG-DML framework that dynamically expands and constrains domain distributions to learn a generalizable DML model for wider target domain distributions. Specifically, \textbf{CenterPolar} contains two collaborative class-centric polarization phases: (1) Class-Centric Centrifugal Expansion ($C^3E$) and (2) Class-Centric Centripetal Constraint ($C^4$). In the first phase, $C^3E$ drives the source domain distribution by shifting the source data away from class centroids using centrifugal expansion to generalize to more unseen domains. In the second phase, to consolidate domain-invariant class information for the generalization ability to unseen categories, $C^4$ pulls all seen and unseen samples toward their class centroids while enforcing inter-class separation via centripetal constraint. Extensive experimental results on widely used CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, and Office-Home datasets demonstrate the superiority and effectiveness of our CenterPolar over existing state-of-the-art methods. The code will be released after acceptance.

</details>


### [27] [SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL](https://arxiv.org/abs/2601.09136)
*Lijun Liu,Linwei Chen,Zhishou Zhang,Meng Tian,Hengfu Cui,Ruiyang Li,Zhaocheng Liu,Qiang Ju,Qianxi Li,Hong-Yu Zhou*

Main category: cs.CV

TL;DR: SkinFlow通过优化视觉信息传输效率，在有限参数下实现皮肤病诊断的高精度，性能超越通用大模型。


<details>
  <summary>Details</summary>
Motivation: 通用大型视觉语言模型（LVLM）在皮肤病学中因‘分散注意力’而表现不佳，需探索非参数扩展的精准医疗路径。

Method: 采用Virtual-Width Dynamic Vision Encoder（DVE）和两阶段强化学习策略，分别对齐显性医学描述和重建隐性诊断纹理。

Result: 在Fitzpatrick17k基准测试中，7B模型Top-1准确率提升12.06%，Top-6准确率提升28.57%，超越通用大模型。

Conclusion: 优化几何容量和信息流在诊断推理上优于原始参数扩展，SkinFlow框架在有限参数下实现了更高的诊断精度。

Abstract: General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.

</details>


### [28] [SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection](https://arxiv.org/abs/2601.09147)
*Chenhao Fu,Han Fang,Xiuzheng Zheng,Wenbo Wei,Yonghua Li,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: SSVP通过多尺度视觉编码融合和动态提示生成，显著提升零样本异常检测性能，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本异常检测方法受限于单一视觉骨干网络，难以平衡全局语义泛化与细粒度结构判别能力。

Method: 提出了协同语义-视觉提示（SSVP）方法，包括层次化语义-视觉协同机制（HSVS）、视觉条件提示生成器（VCPG）和视觉-文本异常映射器（VTAM），以融合多尺度视觉编码并动态生成提示。

Result: 在七个工业基准测试中验证了方法的鲁棒性，SSVP在MVTec-AD上实现了93.0%的Image-AUROC和92.2%的Pixel-AUROC，显著优于现有零样本方法。

Conclusion: SSVP方法通过融合多尺度视觉编码和动态提示生成，显著提升了零样本异常检测的性能，在多个工业基准测试中达到了最先进水平。

Abstract: Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.

</details>


### [29] [From Snow to Rain: Evaluating Robustness, Calibration, and Complexity of Model-Based Robust Training](https://arxiv.org/abs/2601.09153)
*Josué Martínez-Martínez,Olivia Brown,Giselle Zeno,Pooya Khorrami,Rajmonda Caceres*

Main category: cs.CV

TL;DR: 基于模型的训练方法在自然损坏鲁棒性上优于传统基线，尤其是对抗训练变体，但计算成本较高；数据增强变体在性能相近下更高效。


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全敏感领域的可靠性面临自然损坏鲁棒性的挑战，需探索更有效的训练方法。

Method: 研究采用基于模型的训练方法，利用学习的干扰变异模型生成真实损坏，并结合随机覆盖与对抗性优化的混合策略。

Result: 在CURE-TSR数据集上，基于模型的方法在准确性、校准和训练复杂度上均优于基线方法（Vanilla、对抗训练和AugMix）。

Conclusion: 模型基于对抗训练的方法在所有类型的自然损坏中展现出最强的鲁棒性，尽管计算成本较高；而基于模型的数据增强方法在保持相当鲁棒性的同时显著降低了计算复杂度。

Abstract: Robustness to natural corruptions remains a critical challenge for reliable deep learning, particularly in safety-sensitive domains. We study a family of model-based training approaches that leverage a learned nuisance variation model to generate realistic corruptions, as well as new hybrid strategies that combine random coverage with adversarial refinement in nuisance space. Using the Challenging Unreal and Real Environments for Traffic Sign Recognition dataset (CURE-TSR), with Snow and Rain corruptions, we evaluate accuracy, calibration, and training complexity across corruption severities. Our results show that model-based methods consistently outperform baselines Vanilla, Adversarial Training, and AugMix baselines, with model-based adversarial training providing the strongest robustness under across all corruptions but at the expense of higher computation and model-based data augmentation achieving comparable robustness with $T$ less computational complexity without incurring a statistically significant drop in performance. These findings highlight the importance of learned nuisance models for capturing natural variability, and suggest a promising path toward more resilient and calibrated models under challenging conditions.

</details>


### [30] [Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies](https://arxiv.org/abs/2601.09169)
*Jamie Magrill,Leah Gornstein,Sandra Seekins,Barry Magrill*

Main category: cs.CV

TL;DR: 评估五种生成式AI图像平台在建筑图像生成中的准确性，发现总体准确率有限，支持对合成内容标记和谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能文本到图像系统越来越多地用于生成建筑图像，但其在历史规则严格领域再现准确图像的能力仍不明确。

Method: 评估了五种广泛使用的生成式人工智能图像平台（Adobe Firefly、DALL-E 3、Google Imagen 3、Microsoft Image Generator和Midjourney），使用30个涵盖风格、类型和规范化元素的建筑提示。每个提示-生成器组合产生四张图像（共600张图像）。两位建筑历史学家根据预定义标准独立评分，并通过共识解决分歧。

Result: 常见提示的图像输出准确率是罕见提示的2.7倍（p < 0.05）。各平台总体准确率有限（最高52%，最低32%，平均42%）。全正确（4/4）结果在各平台间相似，而全错误（0/4）结果差异显著，Imagen 3失败最少，Microsoft Image Generator失败最多。定性分析发现重复模式，如过度装饰、中世纪风格与后期复兴风格的混淆，以及描述性提示的误表示。

Conclusion: 研究支持对生成式人工智能合成内容进行可见标记、制定未来训练数据集的来源标准，并谨慎在教育中使用生成式人工智能建筑图像。

Abstract: Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.

</details>


### [31] [N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight Mobile Detection of Rice Leaf Diseases](https://arxiv.org/abs/2601.09170)
*Dung Ta Nguyen Duc,Thanh Bui Dang,Hoang Le Minh,Tung Nguyen Viet,Huong Nguyen Thanh,Dong Trinh Cong*

Main category: cs.CV

TL;DR: N EIoU YOLOv9 通过新型损失函数提升农业病害检测的准确性和效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 针对农业病害图像中常见的小目标和低对比度目标，增强弱回归信号并减少梯度干扰。

Method: 提出了一种基于非单调梯度聚焦和几何解耦原理的 N EIoU 损失函数，并将其集成到轻量级 YOLOv9t 架构中。

Result: 在自采集的稻田叶片数据集上，平均精度达到 90.3%，比基线提高了 4.3%，并在更严格的评估标准下提升了定位准确性。

Conclusion: N EIoU YOLOv9 有效平衡了准确性、优化稳定性和计算效率，适用于基于边缘的农业监测系统。

Abstract: In this work, we propose N EIoU YOLOv9, a lightweight detection framework based on a signal aware bounding box regression loss derived from non monotonic gradient focusing and geometric decoupling principles, referred to as N EIoU (Non monotonic Efficient Intersection over Union). The proposed loss reshapes localization gradients by combining non monotonic focusing with decoupled width and height optimization, thereby enhancing weak regression signals for hard samples with low overlap while reducing gradient interference. This design is particularly effective for small and low contrast targets commonly observed in agricultural disease imagery. The proposed N EIoU loss is integrated into a lightweight YOLOv9t architecture and evaluated on a self collected field dataset comprising 5908 rice leaf images across four disease categories and healthy leaves. Experimental results demonstrate consistent performance gains over the standard CIoU loss, achieving a mean Average Precision of 90.3 percent, corresponding to a 4.3 percent improvement over the baseline, with improved localization accuracy under stricter evaluation criteria. For practical validation, the optimized model is deployed on an Android device using TensorFlow Lite with Float16 quantization, achieving an average inference time of 156 milliseconds per frame while maintaining accuracy. These results confirm that the proposed approach effectively balances accuracy, optimization stability, and computational efficiency for edge based agricultural monitoring systems.

</details>


### [32] [From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows](https://arxiv.org/abs/2601.09191)
*Qizhen Lan,Aaron Choi,Jun Ma,Bo Wang,Zhaogming Zhao,Xiaoqian Jiang,Yu-Chun Hsu*

Main category: cs.CV

TL;DR: 通过知识蒸馏，将高性能医疗图像分割模型转化为紧凑、高效的学生模型，适用于本地临床部署，保持高准确率并显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割模型在临床工作流程中的部署常受限于本地基础设施的计算资源，且云推理可能受治理和安全政策限制，高容量模型的计算需求阻碍了其在医院环境中的实际部署和长期维护。

Method: 提出了一种部署导向的框架，利用知识蒸馏将高性能分割模型转化为一系列紧凑的学生模型，同时保持与现有临床系统的架构兼容性。

Result: 在参数大幅减少（94%）的情况下，蒸馏后的学生模型保留了教师模型几乎所有的分割准确率（98.7%），并在CPU推理延迟上实现了高达67%的降低，且无需额外部署开销。

Conclusion: 知识蒸馏提供了一种实用且可靠的方法，将研究级的分割模型转化为可维护、可部署的组件，适用于现实世界医疗系统中的本地临床工作流程。

Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.

</details>


### [33] [Point Tracking as a Temporal Cue for Robust Myocardial Segmentation in Echocardiography Videos](https://arxiv.org/abs/2601.09207)
*Bahar Khodabakhshian,Nima Hashemi,Armin Saadat,Zahra Gholami,In-Chang Hwang,Samira Sojoudi,Christina Luong,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: Point-Seg通过点跟踪增强时间一致性，在超声心动图视频中实现稳定心肌分割，优于传统方法，尤其在低质量数据中表现突出。


<details>
  <summary>Details</summary>
Motivation: 超声心动图视频中的心肌分割由于低对比度、噪声和解剖变异性而具有挑战性，传统深度学习方法要么忽略时间信息，要么依赖基于记忆的特征传播，导致误差累积。

Method: 提出了Point-Seg，一种基于Transformer的分割框架，通过点跟踪模块跟踪关键解剖标志点，利用运动感知信号引导分割，减少漂移，并引入时间平滑损失增强帧间一致性。

Result: 在公共和私有数据集上的实验表明，Point-Seg在高质量数据上与最先进模型精度相当，在低质量数据上表现更优，且提供了像素级心肌运动信息，对下游任务至关重要。

Conclusion: Point-Seg展示了点跟踪可以作为视频分割的有效时间线索，为超声心动图视频中的心肌分割提供了可靠且可推广的方法。

Abstract: Purpose: Myocardium segmentation in echocardiography videos is a challenging task due to low contrast, noise, and anatomical variability. Traditional deep learning models either process frames independently, ignoring temporal information, or rely on memory-based feature propagation, which accumulates error over time. Methods: We propose Point-Seg, a transformer-based segmentation framework that integrates point tracking as a temporal cue to ensure stable and consistent segmentation of myocardium across frames. Our method leverages a point-tracking module trained on a synthetic echocardiography dataset to track key anatomical landmarks across video sequences. These tracked trajectories provide an explicit motion-aware signal that guides segmentation, reducing drift and eliminating the need for memory-based feature accumulation. Additionally, we incorporate a temporal smoothing loss to further enhance temporal consistency across frames. Results: We evaluate our approach on both public and private echocardiography datasets. Experimental results demonstrate that Point-Seg has statistically similar accuracy in terms of Dice to state-of-the-art segmentation models in high quality echo data, while it achieves better segmentation accuracy in lower quality echo with improved temporal stability. Furthermore, Point-Seg has the key advantage of pixel-level myocardium motion information as opposed to other segmentation methods. Such information is essential in the computation of other downstream tasks such as myocardial strain measurement and regional wall motion abnormality detection. Conclusion: Point-Seg demonstrates that point tracking can serve as an effective temporal cue for consistent video segmentation, offering a reliable and generalizable approach for myocardium segmentation in echocardiography videos. The code is available at https://github.com/DeepRCL/PointSeg.

</details>


### [34] [Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy](https://arxiv.org/abs/2601.09209)
*Qiang Hu,Qimei Wang,Yingjie Guo,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: PaGKD是一种无配对组级知识蒸馏框架，通过全局和局部模块实现跨模态学习，显著提升WLI模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖配对的NBI-WLI图像的局限性，利用未配对数据实现跨模态知识迁移。

Method: PaGKD包含两个互补模块：组级原型蒸馏（GKD-Pro）和组级密集蒸馏（GKD-Den），分别通过共享病变感知查询和激活派生关系图实现全局语义一致性和局部结构对齐。

Result: 在四个临床数据集上，PaGKD相对AUC提升了3.3%、1.1%、2.8%和3.2%，显著优于现有方法。

Conclusion: PaGKD框架通过无配对数据的组级知识蒸馏，显著提升了WLI-only模型的性能，为跨模态学习开辟了新方向。

Abstract: White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.

</details>


### [35] [Affostruction: 3D Affordance Grounding with Generative Reconstruction](https://arxiv.org/abs/2601.09211)
*Chunghyun Park,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: Affostruction通过生成完整几何和智能视角选择，显著提升功能定位和3D重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在可见表面预测功能区域，无法处理未观察区域，限制了功能定位的准确性。

Method: 提出Affostruction框架，包括生成式多视角重建、基于流的功能定位和功能驱动的主动视角选择。

Result: 在功能定位上实现了19.1 aIoU（40.4%提升），3D重建上实现了32.67 IoU（67.7%提升）。

Conclusion: Affostruction通过生成完整的几何形状并在包括未观察区域的全形状上定位功能，显著提升了功能定位和3D重建的性能。

Abstract: This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4\% improvement) and 32.67 IoU for 3D reconstruction (67.7\% improvement), enabling accurate affordance prediction on complete shapes.

</details>


### [36] [Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation](https://arxiv.org/abs/2601.09212)
*Xingyao Li,Fengzhuo Zhang,Cunxiao Du,Hui Ji*

Main category: cs.CV

TL;DR: COOL-SD通过理论支持的退火松弛推测解码，显著提升自回归图像生成的速度或质量。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型由于序列性和图像标记的模糊性导致推理速度慢，现有松弛推测解码方法缺乏理论支持。

Method: 提出了COOL-SD，一种基于退火松弛的推测解码方法，通过分析目标模型与松弛推测解码之间的总变差距离和扰动分析，设计出最优的重采样分布和退火策略。

Result: 实验证明COOL-SD在速度-质量权衡上优于先前方法。

Conclusion: COOL-SD通过理论分析和实验验证，在保持图像生成质量的同时显著提高了速度，或在相同延迟下实现了更好的质量，优于现有方法。

Abstract: Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.

</details>


### [37] [SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion](https://arxiv.org/abs/2601.09213)
*Jialu Li,Taiyan Zhou*

Main category: cs.CV

TL;DR: SpikeVAEDiff结合VDVAE和Versatile Diffusion，从神经脉冲数据重建高质量图像，VISI区域是关键。


<details>
  <summary>Details</summary>
Motivation: 解决从神经活动中重建自然视觉场景的挑战，提升基于脉冲数据的图像重建质量和分辨率。

Method: 采用两阶段框架：第一阶段使用VDVAE将神经脉冲信号映射到潜在表示生成低分辨率初步重建；第二阶段通过回归模型将脉冲信号映射到CLIP-Vision和CLIP-Text特征，利用Versatile Diffusion进行图像到图像的细化生成。

Result: 在Allen Visual Coding-Neuropixels数据集上验证，VISI区域表现出最显著的激活并对重建质量起关键作用，脉冲数据在时空分辨率上优于fMRI。

Conclusion: SpikeVAEDiff框架通过结合VDVAE和Versatile Diffusion模型，成功地从神经脉冲数据中重建出高分辨率且语义丰富的图像，VISI区域在重建质量中起关键作用。

Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.

</details>


### [38] [Disentangle Object and Non-object Infrared Features via Language Guidance](https://arxiv.org/abs/2601.09228)
*Fan Liu,Ting Wu,Chuanyi Zhang,Liang Yao,Xing Ma,Yuhui Zheng*

Main category: cs.CV

TL;DR: 提出视觉-语言表示学习范式，通过文本监督和特征解耦提升红外目标检测性能，在两个数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 红外图像由于低对比度和边缘信息弱，难以提取区分性强的目标特征，导致检测性能受限。

Method: 论文提出了语义特征对齐（SFA）模块和目标特征解耦（OFD）模块，通过文本特征的引导来增强目标特征的区分性并减少噪声。

Result: 在M³FD和FLIR两个基准数据集上分别达到了83.7%和86.1%的mAP，性能显著提升。

Conclusion: 该论文提出了一种新颖的视觉-语言表示学习范式，通过文本监督和特征对齐与解耦，显著提升了红外目标检测的性能。实验结果表明，该方法在两个基准数据集上达到了最先进的性能。

Abstract: Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.

</details>


### [39] [SPOT-Face: Forensic Face Identification using Attention Guided Optimal Transport](https://arxiv.org/abs/2601.09229)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: SPOT-Face是一种基于超像素图的跨域法医面部识别框架，通过GNN和注意力机制提升颅骨/素描与面部的匹配效果，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决法医调查中因缺乏常见DNA识别手段（如头发、软组织）而面临的挑战，以及现有深度学习方法在跨域结构对应建模上的不足。

Method: 构建基于超像素的图结构，并利用不同的图神经网络（GNNs）提取图嵌入，通过注意力引导的最优传输机制建立跨域对应关系。

Result: 在IIT_Mandi_S2F和CUFS数据集上的实验表明，SPOT-Face在识别指标（如Recall、mAP）上显著优于现有基于图的基线方法。

Conclusion: SPOT-Face框架在法医调查中显著提升了颅骨和素描与面部的匹配效果，证明了其在跨域识别任务中的高效性。

Abstract: Person identification in forensic investigations becomes very challenging when common identification means for DNA (i.e., hair strands, soft tissue) are not available. Current methods utilize deep learning methods for face recognition. However, these methods lack effective mechanisms to model cross-domain structural correspondence between two different forensic modalities. In this paper, we introduce a SPOT-Face, a superpixel graph-based framework designed for cross-domain forensic face identification of victims using their skeleton and sketch images. Our unified framework involves constructing a superpixel-based graph from an image and then using different graph neural networks(GNNs) backbones to extract the embeddings of these graphs, while cross-domain correspondence is established through attention-guided optimal transport mechanism. We have evaluated our proposed framework on two publicly available dataset: IIT\_Mandi\_S2F (S2F) and CUFS. Extensive experiments were conducted to evaluate our proposed framework. The experimental results show significant improvement in identification metrics ( i.e., Recall, mAP) over existing graph-based baselines. Furthermore, our framework demonstrates to be highly effective for matching skulls and sketches to faces in forensic investigations.

</details>


### [40] [CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation](https://arxiv.org/abs/2601.09230)
*Haodi Yao,Fenghua He,Ning Hao,Yao Su*

Main category: cs.CV

TL;DR: CLIDD是一种高效且高精度的局部特征匹配方法，适用于实时空间智能任务，显著减少模型大小并提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决空间智能任务（如机器人导航和增强现实）中需要高判别力和计算效率的局部特征表示问题。

Method: 提出了跨层独立可变形描述（CLIDD），通过从独立特征层次直接采样，利用可学习偏移捕获跨尺度的细粒度结构细节，同时避免统一密集表示的计算负担。

Result: CLIDD在匹配精度和计算效率上均表现优异，超紧凑变体仅使用0.004M参数即可匹配SuperPoint的精度，高性能配置超越所有当前最先进方法，并在边缘设备上超过200 FPS。

Conclusion: CLIDD方法通过结合高判别力和计算效率，为实时空间智能任务提供了强大且可扩展的解决方案。

Abstract: Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.

</details>


### [41] [Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method](https://arxiv.org/abs/2601.09238)
*Jackie Alex,Justin Petter*

Main category: cs.CV

TL;DR: 提出一种结合知识嵌入和超网络引导条件控制的Stable Diffusion框架，用于从有限数据中生成逼真且可控的缺陷图像，显著提升下游检测性能。


<details>
  <summary>Details</summary>
Motivation: 变电站仪表在电力网格监控中至关重要，但其裂纹和其他物理缺陷的检测因标注样本稀缺而受限。

Method: 1. 通过DreamBooth风格的知识嵌入微调Stable Diffusion骨干网络，弥补自然图像预训练模型与工业设备之间的领域差距。2. 引入几何裂纹建模模块，参数化缺陷属性生成空间约束控制图。3. 设计轻量级超网络动态调制扩散模型的去噪过程。

Result: 在真实变电站仪表数据集上的实验表明，该方法显著优于现有增强和生成基线，FID降低32.7%，多样性指标提高，下游缺陷检测器的mAP提升15.3%。

Conclusion: 该框架为工业检测系统提供了一种实用、高质量的数据合成解决方案，适用于缺陷样本稀缺的情况。

Abstract: Substation meters play a critical role in monitoring and ensuring the stable operation of power grids, yet their detection of cracks and other physical defects is often hampered by a severe scarcity of annotated samples. To address this few-shot generation challenge, we propose a novel framework that integrates Knowledge Embedding and Hypernetwork-Guided Conditional Control into a Stable Diffusion pipeline, enabling realistic and controllable synthesis of defect images from limited data.
  First, we bridge the substantial domain gap between natural-image pre-trained models and industrial equipment by fine-tuning a Stable Diffusion backbone using DreamBooth-style knowledge embedding. This process encodes the unique structural and textural priors of substation meters, ensuring generated images retain authentic meter characteristics.
  Second, we introduce a geometric crack modeling module that parameterizes defect attributes--such as location, length, curvature, and branching pattern--to produce spatially constrained control maps. These maps provide precise, pixel-level guidance during generation.
  Third, we design a lightweight hypernetwork that dynamically modulates the denoising process of the diffusion model in response to the control maps and high-level defect descriptors, achieving a flexible balance between generation fidelity and controllability.
  Extensive experiments on a real-world substation meter dataset demonstrate that our method substantially outperforms existing augmentation and generation baselines. It reduces Frechet Inception Distance (FID) by 32.7%, increases diversity metrics, and--most importantly--boosts the mAP of a downstream defect detector by 15.3% when trained on augmented data. The framework offers a practical, high-quality data synthesis solution for industrial inspection systems where defect samples are rare.

</details>


### [42] [DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos](https://arxiv.org/abs/2601.09240)
*Jiajun Chen,Jing Xiao,Shaohan Cao,Yuming Zhu,Liang Liao,Jun Pan,Mi Wang*

Main category: cs.CV

TL;DR: DeTracker针对非稳定卫星视频提出GLMD和TDFP模块，显著提升多目标跟踪性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 非稳定卫星视频中的平台抖动和小目标弱外观特征导致多目标跟踪性能下降，需针对性解决方案。

Method: 提出了DeTracker框架，包含Global--Local Motion Decoupling (GLMD)模块和Temporal Dependency Feature Pyramid (TDFP)模块，分别用于运动解耦和时序特征融合。

Result: 在SDM-Car-SU模拟数据集和真实卫星视频数据上分别达到61.1%和47.3%的MOTA，显著优于现有方法。

Conclusion: DeTracker通过GLMD和TDFP模块显著提升了非稳定卫星视频中的多目标跟踪性能，并在模拟和真实数据集上验证了其优越性。

Abstract: Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.

</details>


### [43] [A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation](https://arxiv.org/abs/2601.09243)
*Sheng-Chi Hsu,Ting-Yu Yen,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: A$^2$TG通过自适应各向异性纹理提升高斯泼溅的效率和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中固定方形纹理导致的内存效率低下和场景适应性受限的问题。

Method: 采用梯度引导的自适应规则，联合确定纹理分辨率和长宽比，实现非均匀、细节感知的纹理分配。

Result: 在多个基准数据集上，A$^2$TG在保持渲染质量的同时，显著降低了内存需求。

Conclusion: A$^2$TG通过引入各向异性纹理和自适应规则，显著提升了纹理效率和渲染质量，同时降低了内存消耗。

Abstract: Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering. While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability. In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture. Our method employs a gradient-guided adaptive rule to jointly determine texture resolution and aspect ratio, enabling non-uniform, detail-aware allocation that aligns with the anisotropic nature of Gaussian splats. This design significantly improves texture efficiency, reducing memory consumption while enhancing image quality. Experiments on multiple benchmark datasets demonstrate that A TG consistently outperforms fixed-texture Gaussian Splatting methods, achieving comparable rendering fidelity with substantially lower memory requirements.

</details>


### [44] [Integrating Diverse Assignment Strategies into DETRs](https://arxiv.org/abs/2601.09247)
*Yiwei Zhang,Jin Gao,Hanshi Wang,Fudong Ge,Guan Luo,Weiming Hu,Zhipeng Zhang*

Main category: cs.CV

TL;DR: LoRA-DETR通过多个LoRA分支集成多样化的一对多分配策略，提升了DETR检测器的性能，同时保持模型简洁，无需额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有DETR风格检测器的一对一匹配策略收敛缓慢，而现有的一对多分配方法复杂且缺乏统一设计。研究发现性能提升源于分配策略的多样性而非监督信号的数量。

Method: 提出LoRA-DETR框架，通过多个低秩适应（LoRA）分支在训练时集成不同的“一对多”分配规则，这些分支在推理时被丢弃，不增加额外计算成本。

Result: 在不同基线上进行的大量实验验证了LoRA-DETR的有效性，展示了多样化的“一对多”监督可以集成以实现最先进的结果。

Conclusion: LoRA-DETR提出了一种灵活且轻量级的框架，通过集成多样化的分配策略，显著提升了DETR风格检测器的性能，同时保持了模型的简洁性。

Abstract: Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of ``one-to-many'' supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse ``one-to-many'' supervision can be integrated to achieve state-of-the-art results without compromising model elegance.

</details>


### [45] [Hybrid guided variational autoencoder for visual place recognition](https://arxiv.org/abs/2601.09248)
*Ni Wang,Zihan You,Emre Neftci,Thorben Schoepe*

Main category: cs.CV

TL;DR: 提出一种基于事件视觉传感器和引导VAE的紧凑VPR模型，具有高鲁棒性和泛化能力，适用于移动机器人导航。


<details>
  <summary>Details</summary>
Motivation: 现有VPR模型在内存占用和鲁棒性之间存在权衡，难以在移动设备上高效部署。本研究旨在克服这些限制。

Method: 结合事件视觉传感器和基于脉冲神经网络的编码器，开发了一种新型引导变分自编码器（VAE），用于特征解耦和地点识别。

Result: 模型在16个不同地点的室内VPR数据集中表现出与现有方法相当的分类性能，并在不同光照条件下保持鲁棒性，同时展示了对新场景的高泛化能力。

Conclusion: 该研究提出了一种紧凑且鲁棒的基于事件视觉传感器的引导变分自编码器（VAE）模型，用于视觉地点识别（VPR），显著提升了移动机器人在已知和未知室内环境中的导航能力。

Abstract: Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.

</details>


### [46] [PhyRPR: Training-Free Physics-Constrained Video Generation](https://arxiv.org/abs/2601.09255)
*Yibo Zhao,Hengjia Li,Xiaofei He,Boxi Wu*

Main category: cs.CV

TL;DR: 提出三阶段流程PhyRPR，解耦物理理解与视觉合成，提升视频生成的物理合理性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散视频生成模型虽能合成视觉合理的视频，但难以满足物理约束，主要原因是高维物理理解与低维视觉合成的纠缠。

Method: 采用训练免费的三阶段流程：PhyReason（物理状态推理与关键帧合成）、PhyPlan（确定性合成可控粗运动支架）、PhyRefine（通过潜在融合策略将支架注入扩散采样以细化外观）。

Result: 实验表明，该方法在物理约束下显著提升了物理合理性和运动可控性。

Conclusion: 通过提出的三阶段流程PhyRPR，成功将物理理解与视觉合成解耦，显著提升了视频生成的物理合理性和运动可控性。

Abstract: Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.

</details>


### [47] [Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery](https://arxiv.org/abs/2601.09262)
*Maria Sdraka,Dimitrios Michail,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: BAM-MRCD是一种新型深度学习模型，结合多源卫星影像，实现了高时空分辨率的野火燃烧区域快速检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前卫星系统的空间分辨率和时间重访频率之间存在权衡，限制了深度学习模型在野火发生后快速绘制燃烧区域的操作性应用。

Method: 提出了一种名为BAM-MRCD的新型深度学习模型，利用多分辨率、多源卫星影像（MODIS和Sentinel-2）进行燃烧区域的快速检测。

Result: BAM-MRCD模型能够高精度检测小规模野火，性能优于类似的变更检测模型和基线方法。

Conclusion: BAM-MRCD模型通过结合多分辨率、多源卫星影像（MODIS和Sentinel-2），成功实现了高时空分辨率的燃烧区域快速制图，显著提升了小规模野火的检测精度。

Abstract: Delineating wildfire affected areas using satellite imagery remains challenging due to irregular and spatially heterogeneous spectral changes across the electromagnetic spectrum. While recent deep learning approaches achieve high accuracy when high-resolution multispectral data are available, their applicability in operational settings, where a quick delineation of the burn scar shortly after a wildfire incident is required, is limited by the trade-off between spatial resolution and temporal revisit frequency of current satellite systems. To address this limitation, we propose a novel deep learning model, namely BAM-MRCD, which employs multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for the timely production of detailed burnt area maps with high spatial and temporal resolution. Our model manages to detect even small scale wildfires with high accuracy, surpassing similar change detection models as well as solid baselines. All data and code are available in the GitHub repository: https://github.com/Orion-AI-Lab/BAM-MRCD.

</details>


### [48] [BrainSegNet: A Novel Framework for Whole-Brain MRI Parcellation Enhanced by Large Models](https://arxiv.org/abs/2601.09263)
*Yucheng Li,Xiaofan Wang,Junyi Wang,Yijie Li,Xi Zhu,Mubai Du,Dian Sheng,Wei Zhang,Fan Zhang*

Main category: cs.CV

TL;DR: BrainSegNet结合U-Net和SAM，优化全脑分割至95个区域，精度和鲁棒性显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统模板配准方法效率低，现有大模型（如SAM）未针对脑部高精度分割优化。

Method: 提出BrainSegNet框架，结合U-Net跳连和SAM的Transformer模块，引入多尺度注意力解码器和边界细化模块。

Result: 在HCP数据集上，BrainSegNet在多标签分割任务中表现优于现有方法。

Conclusion: BrainSegNet通过整合U-Net跳连和SAM的Transformer模块，显著提升了全脑分割的精度和鲁棒性，优于现有方法。

Abstract: Whole-brain parcellation from MRI is a critical yet challenging task due to the complexity of subdividing the brain into numerous small, irregular shaped regions. Traditionally, template-registration methods were used, but recent advances have shifted to deep learning for faster workflows. While large models like the Segment Anything Model (SAM) offer transferable feature representations, they are not tailored for the high precision required in brain parcellation. To address this, we propose BrainSegNet, a novel framework that adapts SAM for accurate whole-brain parcellation into 95 regions. We enhance SAM by integrating U-Net skip connections and specialized modules into its encoder and decoder, enabling fine-grained anatomical precision. Key components include a hybrid encoder combining U-Net skip connections with SAM's transformer blocks, a multi-scale attention decoder with pyramid pooling for varying-sized structures, and a boundary refinement module to sharpen edges. Experimental results on the Human Connectome Project (HCP) dataset demonstrate that BrainSegNet outperforms several state-of-the-art methods, achieving higher accuracy and robustness in complex, multi-label parcellation.

</details>


### [49] [GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials](https://arxiv.org/abs/2601.09265)
*Bei Huang,Yixin Chen,Ruijie Lu,Gang Zeng,Hongbin Zha,Yuru Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: GaussianFluent 是一个统一框架，结合生成模型和 CD-MPM 方法，实现了脆性断裂的高保真实时模拟与渲染。


<details>
  <summary>Details</summary>
Motivation: 解决现有 3DGS 方法在脆性断裂模拟中的两大障碍：缺乏连贯纹理的体素内部表示和断裂感知模拟方法的缺失。

Method: 1. 通过生成模型引导内部高斯密度化合成逼真内部结构；2. 集成优化的连续损伤物质点法（CD-MPM）实现高速脆性断裂模拟。

Result: 实验证明 GaussianFluent 能处理复杂场景（如混合材料物体和多阶段断裂传播），实现以往方法无法达到的逼真实时渲染效果。

Conclusion: GaussianFluent 通过结合生成模型和优化的 CD-MPM 方法，实现了高保真、实时的动态物体状态模拟与渲染，为 VR 和机器人等下游应用提供了潜力。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.

</details>


### [50] [Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain](https://arxiv.org/abs/2601.09298)
*Lianying Chao,Haoran Cai,Xubin Li,Kai Zhang,Sijie Wu,Rui Xu*

Main category: cs.CV

TL;DR: 提出多阶段训练策略的DICModel，在ICT领域图像描述任务中表现优异，性能超越更大参数模型。


<details>
  <summary>Details</summary>
Motivation: 解决ICT领域中传统方法无法解析图像模态知识以及多模态LLM缺乏足够领域知识的问题。

Method: 采用多阶段渐进训练策略，包括使用Mermaid工具和LLMs合成约7K图像-文本对进行第一阶段监督微调，ICT领域专家手动标注约2K图像-文本对进行第二阶段监督微调，以及专家和LLMs联合合成约1.5K视觉问答数据进行基于指令的监督微调。

Result: 实验结果表明，仅7B参数的DICModel性能优于其他32B参数的SOTA模型，BLEU指标分别提升约56.8%和20.8%，在ICT专家构建的客观问题上准确率超过Qwen2.5-VL 32B模型1%。

Conclusion: 本研究提出的DICModel在ICT领域能高效准确地从图像中提取逻辑文本，有望推动ICT领域多模态模型的发展。

Abstract: In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.

</details>


### [51] [Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification](https://arxiv.org/abs/2601.09416)
*Yaxi Chen,Zi Ye,Shaheer U. Saeed,Oliver Yu,Simin Ni,Jie Huang,Yipeng Hu*

Main category: cs.CV

TL;DR: 该论文通过结合放射组学特征和分层损失函数，提升了骨肉瘤肿瘤区域分类的准确性和可解释性，在公开数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 骨肉瘤（OS）的病理评估对预后和治疗规划至关重要，但传统手动评估方法劳动强度大、主观性强且存在观察者间差异。尽管数字病理学在自动化坏死量化方面取得进展，但深度学习模型在患者层面的泛化能力显著下降。

Method: 论文提出了两种新方法：1）在模型训练中加入放射组学特征作为多模态输入；2）优化两个具有分层类别的二分类任务（肿瘤与非肿瘤、存活与非存活肿瘤），而非传统的三分类任务。

Result: 实验证明，放射组学特征的加入和分层损失函数的优化显著提升了分类性能，并在TCIA OS Tumor Assessment数据集上实现了新的最佳性能。

Conclusion: 该论文提出了一种结合放射组学特征和分层损失函数的新方法，显著提高了骨肉瘤（OS）肿瘤区域分类的准确性，并在公开数据集上实现了新的最先进性能。

Abstract: Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.

</details>


### [52] [Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2601.09316)
*Xinming Fang,Chaoyan Huang,Juncheng Li,Jun Wang,Jun Shi,Guixu Zhang*

Main category: cs.CV

TL;DR: 提出频率误差引导的MRI重建框架，通过条件扩散模型和深度展开网络联合优化欠采样与重建，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多对比MRI重建虽利用全采样参考扫描的互补信息，但现有方法存在参考融合策略浅显、互补信息利用不足以及固定欠采样模式三大局限。

Method: 采用条件扩散模型学习频率误差先验（FEP），并将其整合到一个统一的框架中，联合优化欠采样模式和重建网络。模型驱动的深度展开框架同时利用频率域和图像域信息，并结合空间对齐模块和参考特征分解策略。

Result: 在多种成像模态、加速率（4-30倍）和采样方案下，该方法在定量指标和视觉质量上均优于现有技术。

Conclusion: 该论文提出的频率误差引导重建框架在多种成像模态、加速率和采样方案下均表现出优于现有方法的性能，且在定量指标和视觉质量上均有显著提升。

Abstract: Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.

</details>


### [53] [Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?](https://arxiv.org/abs/2601.09433)
*David Reid,Ognjen Arandjelovic*

Main category: cs.CV

TL;DR: ViT 首次用于古钱币语义识别，表现优于 CNN，验证了多模态自动学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化分析古钱币有助于研究者从大量钱币中提取更多历史信息，并帮助收藏者理解其买卖对象。现有研究主要依赖 CNN，本文探索 ViT 在此任务上的表现。

Method: 本文首次将 Vision Transformer (ViT) 深度学习架构应用于古钱币语义元素识别任务，采用多模态数据（图像和非结构化文本）进行全自动学习，并与 CNN 模型进行对比。

Result: ViT 模型在准确性上优于新训练的 CNN 模型。

Conclusion: Vision Transformer (ViT) 模型在古钱币语义元素识别任务上表现优于卷积神经网络 (CNN)，展示了其在多模态数据（图像和非结构化文本）自动学习中的潜力。

Abstract: Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.

</details>


### [54] [Beyond the final layer: Attentive multilayer fusion for vision transformers](https://arxiv.org/abs/2601.09322)
*Laure Ciernik,Marco Morik,Lukas Thede,Luca Eyring,Shinichi Nakajima,Zeynep Akata,Lukas Muttenthaler*

Main category: cs.CV

TL;DR: 本文提出了一种注意力探测机制，动态融合Vision Transformer各层表示，显著提升下游任务适应性能，并验证了中间层信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基础模型的兴起，如何高效地将其适应于下游任务是一个核心挑战。线性探测虽然计算高效，但通常仅限于最后一层的表示，而任务相关信息实际上分布在网络层次结构中。

Method: 采用了一种注意力探测机制，动态融合Vision Transformer所有层的表示，学习识别目标任务最相关的层，并将低层次结构线索与高层次语义抽象相结合。

Result: 在20个不同数据集和多个预训练基础模型上，该方法相比标准线性探测取得了显著且一致的性能提升。注意力热图进一步显示，与预训练领域不同的任务最能从中层表示中受益。

Conclusion: 研究强调了中间层信息的重要性，并提出了一种基于注意力机制的任务感知方法，以在基于探测的适应中释放其潜力。

Abstract: With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.

</details>


### [55] [See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval](https://arxiv.org/abs/2601.09350)
*Mingyu Jeon,Sungjin Han,Jinkwon Hwang,Minchol Kwon,Jonghee Kim,Junyeong Kim*

Main category: cs.CV

TL;DR: SMORE框架通过语义编码、重要性调制和帧压缩技术，高效处理视频任务，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视频时刻检索方法因稀疏帧采样可能导致信息丢失，尤其是在长视频中，SMORE旨在解决这一问题。

Method: SMORE框架采用查询引导的语义编码、查询感知的重要性调制和自适应帧压缩技术，以减少冗余并保留关键内容。

Result: SMORE在QVHighlights、Charades-STA和ActivityNet-Captions基准测试中实现了最先进的性能。

Conclusion: SMORE框架通过查询引导的语义编码、重要性调制和自适应帧压缩，在保持高信息分辨率的同时提升了内存效率，并在多个基准测试中实现了最先进的性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.

</details>


### [56] [Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling](https://arxiv.org/abs/2601.09566)
*Shuyang Xiang,Hao Guan*

Main category: cs.CV

TL;DR: 研究发现，低分辨率视觉输入可作为中文语言建模的有效替代方案，性能接近传统方法，并在低资源下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉结构（包含语义和语音信息）是否能作为字符级建模的替代方案，以提升预测效果。

Method: 使用灰度图像（分辨率低至8×8像素）作为单个字符的输入，替代传统的基于索引的标记方法。

Result: 低分辨率视觉输入达到39.2%的准确率，与基于索引的基线（39.1%）相当；在低资源设置下，视觉输入表现出显著的“热启动”效应（0.4%训练时准确率超过12%，而基于索引的模型低于6%）。

Conclusion: 研究表明，即使是最低分辨率的视觉输入也能为中文语言建模提供稳健且高效的信号，为传统的基于索引的字符表示方法提供了补充视角。

Abstract: Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \times 8$ pixels. Remarkably, these inputs achieve 39.2\% accuracy, comparable to the index-based baseline of 39.1\%. Such low-resource settings also exhibit a pronounced \emph{hot-start} effect: by 0.4\% of total training, accuracy reaches above 12\%, while index-based models lag at below 6\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.

</details>


### [57] [Spectral Complex Autoencoder Pruning: A Fidelity-Guided Criterion for Extreme Structured Channel Compression](https://arxiv.org/abs/2601.09352)
*Wei Liu,Xing Deng,Haijian Shao,Yingtao Jiang*

Main category: cs.CV

TL;DR: SCAP uses spectral reconstruction fidelity to prune channels, achieving high compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To measure functional redundancy at the level of individual output channels for efficient network pruning.

Method: Constructs a complex interaction field by pairing input activations with output-channel activations, transforms it to the frequency domain, and uses a low-capacity autoencoder to reconstruct normalized spectra to measure channel redundancy.

Result: Achieves 90.11% FLOP reduction and 96.30% parameter reduction with only a 1.67% drop in Top-1 accuracy on VGG16/CIFAR-10.

Conclusion: Spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression, as demonstrated by the high FLOP and parameter reduction with minimal accuracy drop.

Abstract: We propose Spectral Complex Autoencoder Pruning (SCAP), a reconstruction-based criterion that measures functional redundancy at the level of individual output channels. For each convolutional layer, we construct a complex interaction field by pairing the full multi-channel input activation as the real part with a single output-channel activation (spatially aligned and broadcast across input channels) as the imaginary part. We transform this complex field to the frequency domain and train a low-capacity autoencoder to reconstruct normalized spectra. Channels whose spectra are reconstructed with high fidelity are interpreted as lying close to a low-dimensional manifold captured by the autoencoder and are therefore more compressible; conversely, channels with low fidelity are retained as they encode information that cannot be compactly represented by the learned manifold. This yields an importance score (optionally fused with the filter L1 norm) that supports simple threshold-based pruning and produces a structurally consistent pruned network. On VGG16 trained on CIFAR-10, at a fixed threshold of 0.6, we obtain 90.11% FLOP reduction and 96.30% parameter reduction with an absolute Top-1 accuracy drop of 1.67% from a 93.44% baseline after fine-tuning, demonstrating that spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression.

</details>


### [58] [Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process](https://arxiv.org/abs/2601.09410)
*Sangjun Han,Youngmi Hur*

Main category: cs.CV

TL;DR: 论文提出两种方法（细节损失和重复上下采样）以增强超分辨率图像的高频细节，实验证明其在CNN和注意力模型中均有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于超分辨率任务中增强细节至关重要，需要强调贡献高频信息的像素，因此论文旨在提出有效方法以增强超分辨率图像中的高频细节。

Method: 论文提出了两种方法：1）基于拉普拉斯金字塔的细节损失，通过分别生成和控制超分辨率图像与细节图像来引导模型；2）重复上下采样过程，通过从多个低分辨率特征中提取多样化信息来增强细节损失的效果。

Result: 实验表明，基于CNN的模型结合所提方法取得了最先进的结果，超越了当前所有基于CNN甚至部分基于注意力的模型。此外，将细节损失应用于现有基于注意力的模型也显示出改进。

Conclusion: 该论文提出的两种方法（基于拉普拉斯金字塔的细节损失和重复上下采样过程）在不同模型结构中均能有效提升超分辨率图像的质量，展示了其广泛适用性和优越性。

Abstract: With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.

</details>


### [59] [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/abs/2601.09613)
*Yonglin Tian,Qiyao Zhang,Wei Xu,Yutong Wang,Yihao Wu,Xinyi Li,Xingyuan Dai,Hui Zhang,Zhiyong Cui,Baoqing Guo,Zujun Yu,Yisheng Lv*

Main category: cs.CV

TL;DR: 论文提出CogRail基准和联合微调框架，显著提升视觉语言模型在铁路入侵感知任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常局限于固定视觉范围内的对象分类，并基于规则启发式确定入侵状态，忽视了潜在入侵风险目标。认知入侵感知需要理解目标的空间上下文和时间动态，这对传统视觉模型构成挑战。

Method: 通过整合三个核心任务（位置感知、运动预测和威胁分析）的联合微调框架，对视觉语言模型（VLMs）进行微调，以适应认知入侵感知的需求。

Result: 实验表明，当前大规模多模态模型在认知入侵感知任务中难以应对复杂的时空推理需求，而提出的联合微调框架显著提升了性能。

Conclusion: 论文提出的联合微调框架显著提升了模型在认知入侵感知任务中的性能，突出了结构化多任务学习在提高准确性和可解释性方面的优势。

Abstract: Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.

</details>


### [60] [Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs](https://arxiv.org/abs/2601.09430)
*Rui Zhu,Xin Shen,Shuchen Wu,Chenxi Miao,Xin Yu,Yang Li,Weikang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CV

TL;DR: 论文提出Video-MSR基准测试和MSR-9K数据集，显著提升多跳空间推理能力，揭示了现有模型的局限性并提供了改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单步感知到判断任务，而复杂视觉空间逻辑链场景的研究不足，因此需要开发专门的多跳空间推理评估工具。

Method: 研究引入了Video-MSR基准测试，包含四个任务（受限定位、链式参考检索、路径规划和反事实物理推导），并通过模型生成与人工验证构建了3,052个视频实例和4,993个问答对。

Result: 对20个先进MLLM的评估显示，模型在表面感知上表现良好，但在MSR任务中性能显著下降。通过MSR-9K数据集微调Qwen-VL，Video-MSR上的性能提升了7.82%。

Conclusion: 论文强调了多跳空间推理（MSR）在动态视频场景中的重要性，并通过Video-MSR基准测试和MSR-9K数据集显著提升了模型的MSR能力。

Abstract: Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.

</details>


### [61] [PrivLEX: Detecting legal concepts in images through Vision-Language Models](https://arxiv.org/abs/2601.09449)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

TL;DR: PrivLEX是一种基于法律概念的可解释图像隐私分类器，利用VLM实现零样本概念检测，无需训练标签。


<details>
  <summary>Details</summary>
Motivation: 开发一种与法律概念对齐的可解释隐私分类器，以识别图像中的个人数据概念。

Method: PrivLEX利用视觉语言模型（VLM）的识别能力，通过零样本VLM概念检测和标签自由的概念瓶颈模型实现可解释分类。

Result: PrivLEX能够识别图像中的个人数据概念，并分析了人类标注者对这类概念的敏感性。

Conclusion: PrivLEX是一种新颖的图像隐私分类器，其决策基于法律定义的个人数据概念，无需在训练中使用明确的概念标签即可提供可解释的分类。

Abstract: We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.

</details>


### [62] [MAD: Motion Appearance Decoupling for efficient Driving World Models](https://arxiv.org/abs/2601.09452)
*Ahmad Rahimi,Valentin Gerard,Eloi Zablocki,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出高效两阶段框架，将通用视频扩散模型转化为可控驾驶世界模型，先学习结构化运动，再合成真实视频，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型虽能生成逼真视频，但作为自动驾驶的世界模型时，缺乏结构化运动和物理一致性。直接适应这些模型需要大量领域数据和昂贵微调。

Method: 采用两阶段框架：首先将通用视频扩散模型适应于预测简化的骨架化运动视频，专注于物理和社会合理性；然后复用同一骨干网络，以这些运动序列为条件合成真实RGB视频。

Result: MAD-LTX模型在计算效率上显著优于现有方法（仅需6%的计算资源），并支持文本、自我和对象控制。

Conclusion: 提出的两阶段解耦方法（先学习结构化运动，再合成真实RGB视频）显著提高了效率，MAD-LTX模型在性能和可控性上均优于现有开源模型。

Abstract: Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively "dressing" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/

</details>


### [63] [Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity](https://arxiv.org/abs/2601.09497)
*Ritabrata Chakraborty,Hrishit Mitra,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 论文研究了跨数据集目标检测在设定特异性下的表现，发现相同设定类型内转移稳定，跨类型转移显著下降，开放标签评估带来有限增益。


<details>
  <summary>Details</summary>
Motivation: 研究跨数据集目标检测（CD-OD）在设定特异性下的表现，揭示其内在结构及转移稳定性。

Method: 通过将基准数据集分为设定无关数据集和设定特定数据集，评估标准检测器家族在所有训练-测试对上的表现，并使用CLIP相似度进行开放标签对齐。

Result: 研究发现，相同设定类型内的转移相对稳定，而跨设定类型的转移显著下降且常不对称；开放标签评估带来一致但有界的增益。

Conclusion: 论文提供了在设定特异性下跨数据集目标检测（CD-OD）的原则性特征描述，并为在分布偏移下评估检测器提供了实用指导。

Abstract: Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.

</details>


### [64] [V-DPM: 4D Video Reconstruction with Dynamic Point Maps](https://arxiv.org/abs/2601.09499)
*Edgar Sucar,Eldar Insafutdinov,Zihang Lai,Andrea Vedaldi*

Main category: cs.CV

TL;DR: V-DPM扩展动态点地图至视频输入，利用VGGT基础实现高效动态3D/4D重建。


<details>
  <summary>Details</summary>
Motivation: 现有DPMs仅适用于图像对且需后处理优化，限制了其在视频中的应用。

Method: 在VGGT基础上实现V-DPM，利用少量合成数据适应动态场景预测。

Result: V-DPM不仅能恢复动态深度，还能捕捉场景中每一点的完整3D运动。

Conclusion: V-DPM通过将动态点地图（DPMs）扩展到视频输入，实现了动态场景的3D和4D重建，并在性能上达到最新水平。

Abstract: Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.

</details>


### [65] [Video Joint-Embedding Predictive Architectures for Facial Expression Recognition](https://arxiv.org/abs/2601.09524)
*Lennart Eing,Cristina Luna-Jiménez,Silvan Mertes,Elisabeth André*

Main category: cs.CV

TL;DR: V-JEPA通过嵌入式预训练提升面部表情识别性能，在RAVDESS和CREMA-D上表现优异，并具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的视频理解预训练方法依赖像素级重建，可能捕捉无关信息（如背景颜色）。V-JEPA通过嵌入式预测避免这一问题，旨在提升FER的效率和准确性。

Method: 使用V-JEPA架构，通过预测被遮蔽区域的嵌入来训练编码器，避免了传统像素级重建方法的局限性。随后在RAVDESS和CREMA-D数据集上训练浅层分类器。

Result: 在RAVDESS上达到最先进性能，在CREMA-D上超越所有其他基于视觉的方法（+1.48 WAR），并展示了强大的跨数据集泛化能力。

Conclusion: 该论文展示了V-JEPA在面部表情识别（FER）中的创新应用，通过纯嵌入式的预训练方法显著提升了性能，并展示了强大的跨数据集泛化能力。

Abstract: This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.

</details>


### [66] [GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection](https://arxiv.org/abs/2601.09528)
*Alfio Spoto,Rosario Leonardi,Francesco Ragusa,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: 论文提出GlovEgo-HOI数据集和GlovEgo-Net模型，通过合成数据与扩散过程解决工业EHOI分析的数据稀缺问题，并开源了数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 工业安全领域的Egocentric Human-Object Interaction（EHOI）分析面临标注数据稀缺的挑战，限制了鲁棒模型的开发。

Method: 论文提出了一种数据生成框架，结合合成数据和扩散过程，为真实世界图像添加逼真的个人防护装备（PPE）。同时，提出了GlovEgo-Net模型，该模型集成了Glove-Head和Keypoint-Head模块，利用手部姿态信息增强交互检测。

Result: 实验证明了所提数据生成框架和GlovEgo-Net模型的有效性。

Conclusion: 该论文提出了GlovEgo-HOI数据集和GlovEgo-Net模型，通过合成数据与扩散过程的结合，有效解决了工业安全领域中EHOI分析的数据稀缺问题。

Abstract: Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.

</details>


### [67] [Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server](https://arxiv.org/abs/2601.09531)
*Yue Yao,Ruining Yang,Tom Gedeon*

Main category: cs.CV

TL;DR: 通过分层数据服务器和BMM算法优化训练集结构，提升无监督域适应任务的模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决目标域数据无法实时标注时，如何从大规模数据服务器构建具有竞争力的训练集的问题，避免因训练集缺乏目标模式而导致模型性能下降。

Method: 引入分层数据服务器和二分模式匹配算法（BMM），通过一对一匹配源和目标模式，构建优化的训练集。

Result: BMM算法构建的训练集在目标域上的域间隙更小，模型准确性更高，且与现有UDA方法结合可进一步提升性能。

Conclusion: 通过优化数据服务器的结构和引入二分模式匹配算法（BMM），本研究在无监督域适应（UDA）任务中实现了更高的模型准确性，且与现有模型为中心的UDA方法正交。

Abstract: We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.

</details>


### [68] [Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model](https://arxiv.org/abs/2601.09572)
*Tianli Tao,Ziyang Wang,Delong Yang,Han Zhang,Le Zhang*

Main category: cs.CV

TL;DR: DF-DiffCom是一种基于KAN增强扩散模型的方法，通过变形场提升纵向脑图像补全的保真度和灵活性，支持多模态扩展。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度生成模型在纵向脑MRI图像补全中保真度不足和应用场景受限的问题。

Method: 采用KAN增强的扩散模型，结合变形场进行纵向脑图像补全。

Result: 在OASIS-3数据集上，DF-DiffCom的PSNR提高了5.6%，SSIM提高了0.12，且支持多种MRI模态。

Conclusion: DF-DiffCom通过结合KAN增强的扩散模型和变形场，显著提高了纵向脑图像生成的保真度和灵活性，支持多种MRI模态的扩展应用。

Abstract: Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.

</details>


### [69] [OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2601.09575)
*Sheng-Yu Huang,Jaesung Choe,Yu-Chiang Frank Wang,Cheng Sun*

Main category: cs.CV

TL;DR: OpenVoxel 是一种无需训练的算法，利用 MLLMs 对稀疏体素进行分组和标注，显著提升了开放词汇 3D 场景理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决开放词汇 3D 场景理解任务中的分组和标注问题，同时避免传统方法中需要训练和嵌入的复杂性。

Method: OpenVoxel 基于稀疏体素栅格化（SVR）模型，利用多模态大型语言模型（MLLMs）进行文本到文本搜索，无需训练或引入 CLIP/BERT 文本编码器的嵌入。

Result: 实验表明，OpenVoxel 在复杂指代表达分割（RES）任务中优于现有方法。

Conclusion: OpenVoxel 是一种无需训练的算法，通过利用强大的视觉语言模型和多模态大型语言模型，成功构建了信息丰富的场景地图，并在复杂指代表达分割任务中表现出色。

Abstract: We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.

</details>


### [70] [Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents](https://arxiv.org/abs/2601.09586)
*Said Yasin,Torsten Zesch*

Main category: cs.CV

TL;DR: 研究探讨了手写文档视觉反馈的挑战，比较了模块化和端到端系统，发现两者均未达标，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 手写是一项在教育中至关重要的技能，但提供手写文档的视觉反馈是一个重要却未被充分研究的领域。

Method: 通过实证比较模块化和端到端系统，分析从手写输入图像到正确放置信息性错误反馈的挑战。

Result: 研究发现，当前两种方法均无法达到可接受的总体质量。

Conclusion: 尽管模块化和端到端系统目前在提供手写文档的视觉反馈方面尚未达到可接受的整体质量，但研究指出了主要挑战并提出了未来研究的议程。

Abstract: Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.

</details>


### [71] [Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric](https://arxiv.org/abs/2601.09601)
*Emmanuele Barberi,Felice Sfravara,Filippo Cucinotta*

Main category: cs.CV

TL;DR: IDEM是一种基于微分熵的点云配准新方法，优于传统度量，适用于复杂场景。


<details>
  <summary>Details</summary>
Motivation: 传统点云配准方法（如ICP）依赖于欧氏距离，存在非交换性问题，且在点云存在密度差异、噪声、孔洞或部分重叠时效果不佳。需要一种更鲁棒的度量方法以应对这些挑战。

Method: 提出了一种基于微分熵的新度量方法，称为迭代微分熵最小化（IDEM），用于精细的刚性成对3D点云配准。该方法不依赖于固定点云的选择，并在变换过程中显示出清晰的最小值对应最佳对齐。

Result: 通过多个案例研究，IDEM在复杂情况下（如密度差异、噪声、孔洞和部分重叠）表现优于RMSE、Chamfer距离和Hausdorff距离，能够实现更优的对齐效果。

Conclusion: 作者提出的基于微分熵的度量方法IDEM在点云配准中表现出色，尤其在处理密度差异、噪声、孔洞和部分重叠等复杂情况时优于传统方法如RMSE、Chamfer距离和Hausdorff距离。

Abstract: Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.

</details>


### [72] [GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.09606)
*Manning Gao,Leheng Zhang,Shiqin Han,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.CV

TL;DR: GRCF通过动态边际排序和MAE校准，解决了多模态情感分析中的排序和校准问题，在回归和分类任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法对标签噪声敏感且忽视样本间相对顺序，导致预测不稳定和相关性对齐差。GRCF旨在解决这些问题，同时适应性地关注难排序样本和动态调整语义距离。

Method: 提出了两阶段组级排序与校准框架（GRCF），第一阶段使用GRPO启发的动态边际排序损失构建细粒度排序结构，第二阶段通过MAE驱动的目标校准预测幅度。

Result: GRCF在核心回归基准测试中达到最先进性能，并在多模态幽默检测和讽刺检测等分类任务中展现出强泛化能力。

Conclusion: GRCF框架通过两阶段方法（动态边际排序损失和MAE驱动的目标）解决了现有多模态情感分析中的问题，实现了细粒度排序结构和预测校准，并在回归和分类任务中均表现出色。

Abstract: Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.

</details>


### [73] [Identifying Models Behind Text-to-Image Leaderboards](https://arxiv.org/abs/2601.09647)
*Ali Naseh,Yuefeng Peng,Anshuman Suri,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.CV

TL;DR: 研究发现T2I模型生成的图像在嵌入空间中具有独特特征，可被高精度去匿名化，暴露排行榜安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 探讨文本到图像（T2I）模型在匿名化排行榜中的安全性问题，发现匿名性容易被破坏。

Method: 通过分析22个模型和280个提示生成的15万张图像，采用基于质心的方法进行去匿名化。

Result: 研究发现每个T2I模型的生成图像在嵌入空间中形成独特聚类，能够实现高精度的去匿名化。

Conclusion: 该研究揭示了基于投票的T2I模型排行榜存在的安全漏洞，并呼吁更强的匿名化防御措施。

Abstract: Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.

</details>


### [74] [AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking](https://arxiv.org/abs/2601.09652)
*Emanuel da Costa Silva,Tatiana Taís Schein,José David García Ramos,Eduardo Lawson da Silva,Stephanie Loi Brião,Felipe Gomes de Oliveira,Paulo Lilles Jorge Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat+ 是一个专为自动化视觉任务设计的水下特征增强管道，显著提升了目标检测、分类和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 水下视频分析因低光照、颜色失真和浑浊等因素而极具挑战性，这些因素会直接影响机器人应用中感知模块的性能。

Method: AquaFeat+ 是一个端到端训练的插件式管道，包含颜色校正、分层特征增强和自适应残差输出模块，直接由最终应用的损失函数指导。

Result: 在 FishTrack23 数据集上的训练和评估显示，AquaFeat+ 在目标检测、分类和跟踪指标上取得了显著改进。

Conclusion: AquaFeat+ 被证明能有效提升水下机器人应用中感知任务的性能，特别是在目标检测、分类和跟踪指标上。

Abstract: Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications.

</details>


### [75] [Image2Garment: Simulation-ready Garment Generation from a Single Image](https://arxiv.org/abs/2601.09658)
*Selim Emir Can,Jan Ackermann,Kiyohiro Nakayama,Ruofan Liu,Tong Wu,Yang Zheng,Hugo Bertiche,Menglei Chai,Thabo Beeler,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种从单张图像估计模拟就绪服装的前馈框架，通过视觉语言模型和轻量级预测器实现高精度材料属性和物理参数估计。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏图像到物理的数据集以及问题的病态性质，从单张图像中估计物理上准确的、可用于模拟的服装具有挑战性。

Method: 首先微调视觉语言模型以从真实图像推断材料组成和织物属性，然后训练一个轻量级预测器，将这些属性映射到相应的物理织物参数。

Result: 实验表明，该方法在材料组成估计和织物属性预测方面具有更高的准确性，并且通过物理参数估计器，实现了比现有图像到服装方法更高保真度的模拟。

Conclusion: 该论文提出了一种无需迭代优化的前馈框架，能够从单张图像中估计出可用于模拟的服装，并在材料组成估计和织物属性预测方面表现出更高的准确性。

Abstract: Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.

</details>


### [76] [LiteEmbed: Adapting CLIP to Rare Classes](https://arxiv.org/abs/2601.09661)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: LiteEmbed是一种轻量级框架，通过优化CLIP的文本嵌入实现少样本个性化适配，显著提升罕见类别的识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视觉语言模型（如CLIP）在预训练中罕见类别（如新兴实体或文化特定类别）上的零样本识别性能不足的问题。

Method: LiteEmbed通过基于PCA的分解方法，在CLIP的词汇空间内优化文本嵌入，结合粗粒度对齐和细粒度分离的双目标策略，既保持了全局语义一致性，又增强了视觉相似类别间的区分度。

Result: 实验表明，LiteEmbed在分类、检索、分割和检测任务中均显著优于现有方法，有效提升了模型在罕见或未见类别上的表现。

Conclusion: LiteEmbed作为一种轻量级框架，成功实现了CLIP模型的少样本个性化适配，无需重新训练编码器即可添加新类别，显著提升了在罕见或未见类别上的性能。

Abstract: Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.

</details>


### [77] [Self-Supervised Animal Identification for Long Videos](https://arxiv.org/abs/2601.09663)
*Xuyang Fang,Sion Hannuna,Edwin Simpson,Neill Campbell*

Main category: cs.CV

TL;DR: 提出一种高效自监督方法，无需手动标注，在消费级硬件上实现高精度动物识别，准确率>97%，内存消耗低。


<details>
  <summary>Details</summary>
Motivation: 解决传统动物识别方法需要大量手动标注和计算资源的问题，特别是在长时视频中，现有自监督方法因内存限制和时间误差传播而不适用。

Method: 通过将动物识别重新定义为全局聚类任务，采用预训练骨干网络、自举机制和匈牙利算法进行伪标签分配，并结合二元交叉熵损失函数，实现了高效的特征学习。

Result: 在3D-POP鸽子和8头小牛喂食视频等真实数据集上，该方法达到了超过97%的准确率，且每批次GPU内存消耗低于1GB，优于标准对比方法。

Conclusion: 本研究提出了一种高效的自监督方法，能够在消费级硬件上实现高精度的动物个体识别，解决了传统方法需要大量手动标注和计算资源的问题。

Abstract: Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.

</details>


### [78] [SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings](https://arxiv.org/abs/2601.09665)
*Yuchen Wu,Jiahe Li,Xiaohan Yu,Lina Yu,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: SCE-SLAM通过场景坐标嵌入和全局约束，显著减少单目SLAM的尺度漂移，提升轨迹精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 单目视觉SLAM在长序列中因缺乏全局约束而累积尺度漂移，现有方法无法有效解决这一问题。

Method: SCE-SLAM包含两个关键模块：几何引导的聚合和场景坐标束调整。前者通过几何调制注意力传播历史观测的尺度信息，后者通过解码场景坐标嵌入的显式3D坐标约束锚定当前估计到参考尺度。

Result: 在KITTI、Waymo和vKITTI数据集上，SCE-SLAM将绝对轨迹误差降低了8.36米（KITTI），同时保持36 FPS的实时性能，并实现大规模场景的尺度一致性。

Conclusion: SCE-SLAM通过场景坐标嵌入和全局约束，有效解决了单目视觉SLAM中的尺度漂移问题，显著提升了轨迹精度和尺度一致性。

Abstract: Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.

</details>


### [79] [STEP3-VL-10B Technical Report](https://arxiv.org/abs/2601.09668)
*Ailin Huang,Chengyuan Yao,Chunrui Han,Fanqi Wan,Hangyu Guo,Haoran Lv,Hongyu Zhou,Jia Wang,Jian Zhou,Jianjian Sun,Jingcheng Hu,Kangheng Lin,Liang Zhao,Mitt Huang,Song Yuan,Wenwen Qu,Xiangfeng Wang,Yanlin Lai,Yingxiu Zhao,Yinmin Zhang,Yukang Shi,Yuyang Chen,Zejia Weng,Ziyang Meng,Ang Li,Aobo Kong,Bo Dong,Changyi Wan,David Wang,Di Qi,Dingming Li,En Yu,Guopeng Li,Haiquan Yin,Han Zhou,Hanshan Zhang,Haolong Yan,Hebin Zhou,Hongbo Peng,Jiaran Zhang,Jiashu Lv,Jiayi Fu,Jie Cheng,Jie Zhou,Jisheng Yin,Jingjing Xie,Jingwei Wu,Jun Zhang,Junfeng Liu,Kaijun Tan,Kaiwen Yan,Liangyu Chen,Lina Chen,Mingliang Li,Qian Zhao,Quan Sun,Shaoliang Pang,Shengjie Fan,Shijie Shang,Siyuan Zhang,Tianhao You,Wei Ji,Wuxun Xie,Xiaobo Yang,Xiaojie Hou,Xiaoran Jiao,Xiaoxiao Ren,Xiangwen Kong,Xin Huang,Xin Wu,Xing Chen,Xinran Wang,Xuelin Zhang,Yana Wei,Yang Li,Yanming Xu,Yeqing Shen,Yuang Peng,Yue Peng,Yu Zhou,Yusheng Li,Yuxiang Yang,Yuyang Zhang,Zhe Xie,Zhewei Huang,Zhenyi Lu,Zhimin Fan,Zihui Cheng,Daxin Jiang,Qi Han,Xiangyu Zhang,Yibo Zhu,Zheng Ge*

Main category: cs.CV

TL;DR: STEP3-VL-10B 是一个10B规模的轻量级多模态模型，通过创新预训练和后训练策略，性能媲美更大模型，开源提供高效基准。


<details>
  <summary>Details</summary>
Motivation: 重新定义紧凑效率与前沿多模态智能之间的权衡，提供轻量级但高性能的开源模型。

Method: 采用统一的全解冻预训练策略，结合语言对齐的感知编码器和Qwen3-8B解码器，并通过超过1k次强化学习迭代的后训练流程，实现了视觉-语言的协同。

Result: STEP3-VL-10B 在多个基准测试中表现优异，如MMBench 92.2%、MMMU 80.11%，复杂推理任务中AIME2025 94.43%、MathVision 75.95%。

Conclusion: STEP3-VL-10B 是一个轻量级开源基础模型，通过统一预训练和强化学习后训练策略，在紧凑的10B规模下实现了与更大模型相媲美甚至超越的性能，为社区提供了高效且可复现的基准。

Abstract: We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.

</details>


### [80] [Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering](https://arxiv.org/abs/2601.09697)
*Jieying Chen,Jeffrey Hu,Joan Lasenby,Ayush Tewari*

Main category: cs.CV

TL;DR: SRENDER通过稀疏关键帧和3D渲染实现高效视频生成，比基线快40倍，适用于实时交互应用。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散模型的视频生成方法计算效率低下的问题，满足实时交互应用（如具身AI和VR/AR）的需求。

Method: 采用扩散模型生成稀疏关键帧，通过3D重建和渲染合成完整视频，并引入模型预测最优关键帧数量以自适应分配计算资源。

Result: SRENDER在生成20秒视频时比基于扩散的基线方法快40倍以上，且保持了高视觉保真度和时间稳定性。

Conclusion: SRENDER方法通过稀疏关键帧生成和3D重建渲染，显著提升了视频生成的效率，同时保持了高视觉保真度和时间稳定性，为高效可控的视频合成提供了实用路径。

Abstract: Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.

</details>


### [81] [COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation](https://arxiv.org/abs/2601.09698)
*Tony Danjun Wang,Tolga Birdal,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: COMPOSE 是一种新框架，通过超图分割解决多视角姿态对应匹配问题，显著提升了3D姿态估计的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的优化方法通常依赖于两阶段流程，即先检测每个视角的2D关键点，然后关联这些检测结果以三角化3D姿态。这些方法仅通过成对关联来建模对应问题，将全局一致性（即循环一致性）视为软约束，导致在虚假关联传播错误时变得脆弱。

Method: COMPOSE 提出了一种新颖的框架，将多视角姿态对应匹配问题建模为超图分割问题，并引入了一种高效的几何剪枝策略以减少搜索空间。

Result: COMPOSE 在平均精度上比之前的优化方法提高了23%，比自监督端到端学习方法提高了11%。

Conclusion: COMPOSE 通过将多视角姿态对应匹配问题建模为超图分割问题，显著提升了3D姿态估计的精度，为这一广泛研究的问题提供了有前景的解决方案。

Abstract: 3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.

</details>


### [82] [SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3](https://arxiv.org/abs/2601.09699)
*Ruiqi Shen,Chang Liu,Henghui Ding*

Main category: cs.CV

TL;DR: SAM3-DMS通过细粒度内存选择优化了多目标视频分割，提升了跟踪稳定性。


<details>
  <summary>Details</summary>
Motivation: 原始SAM3在复杂多目标场景中的群体级集体内存选择表现不佳，因其采用同步决策且依赖平均性能，忽略了单个对象的可靠性。

Method: 提出了一种无需训练的分离策略（SAM3-DMS），对单个对象进行细粒度内存选择。

Result: 实验表明，SAM3-DMS在身份保持和跟踪稳定性方面表现优异，尤其在目标密度增加时优势更为明显。

Conclusion: SAM3-DMS通过细粒度的内存选择策略，显著提升了复杂多目标场景下的身份保持和跟踪稳定性，为野外多目标视频分割奠定了坚实基础。

Abstract: Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [83] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 利用机器学习动态优化GEMM线程数，在特定硬件上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代多核共享内存系统的复杂性使得确定最小化多线程GEMM运行时间的线程数具有挑战性，传统方法难以适应不同架构和数据规模的需求。

Method: 通过构建ADSALA软件库，利用机器学习模型实时选择最优线程数，基于训练数据动态调整GEMM任务的线程配置。

Result: 在两种不同的HPC节点架构（Intel Cascade Lake和AMD Zen 3）上测试显示，对于内存使用在100 MB以内的GEMM任务，实现了25%至40%的速度提升。

Conclusion: 论文提出了一种基于机器学习的动态线程数选择方法，显著提升了GEMM在多核共享内存系统中的运行效率，相比传统BLAS实现实现了25%至40%的加速。

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [84] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: 提出PDCC协议，通过用户随机数交易排序和周期性共识实现支付系统动态重配置，性能高效。


<details>
  <summary>Details</summary>
Motivation: 现代支付系统需要高性能且动态可配置的解决方案，传统全局交易排序方法存在性能瓶颈。

Method: 设计了基于拜占庭一致性广播的交易驱动动态重配置协议PDCC，避免全局交易排序以提高性能。

Result: PDCC实现了平滑的动态重配置过程，且未对原有系统性能造成影响。

Conclusion: PDCC（Payment Dynamic Config Change）协议通过结合用户随机数交易排序和周期性系统共识机制，实现了现代支付系统中的动态平滑重配置，且不影响原有系统性能。

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [85] [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)
*Yifei Xie,Btissam Er-Rahmadi,Xiao Chen,Tiejun Ma,Jane Hillston*

Main category: cs.DC

TL;DR: 提出VCO模型优化并行BFT的视图变更，通过混合整数规划和改进算法提升性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决并行BFT中视图变更过程的性能瓶颈问题，避免因被动视图变更机制选择不可用或慢速节点作为领导者而导致的性能下降。

Method: 提出基于混合整数规划的VCO模型，优化领导者选择和跟随者重新分配，采用分解方法和改进的Benders切割求解模型，并提出迭代备份领导者选择算法。

Result: 在Microsoft Azure云环境中实验证明，VCO驱动的并行BFT在正常运行和故障条件下均优于现有配置方法。

Conclusion: VCO模型在网络规模增大时仍然有效，适合高性能并行BFT系统。

Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.

</details>


### [86] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: LatencyPrism是一种零侵入的延迟分析系统，能实时监控LLM推理延迟并区分正常波动与异常，F1分数达0.98。


<details>
  <summary>Details</summary>
Motivation: LLM推理延迟直接影响用户体验和运营成本，现有分析方法受限于侵入式设计或硬件绑定实现，难以适应异构推理环境的实时生产分析需求。

Method: LatencyPrism通过低开销的实时批处理级别监控，结合毫秒级触发的警报机制，区分工作负载驱动的延迟变化与潜在问题指示的异常。

Result: LatencyPrism已在数千个XPU上部署超过六个月，其异常检测的F1分数达到0.98，并展示了强大的根本原因分析能力。

Conclusion: LatencyPrism是一种零侵入、多平台的延迟雕刻系统，能够在无需代码修改或服务重启的情况下，分解推理延迟、主动警报异常并保证SLO合规性。

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


### [87] [High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data](https://arxiv.org/abs/2601.09334)
*Valerio Besozzi,Matteo Della Bartola,Patrizio Dazzi,Marco Danelutto*

Main category: cs.DC

TL;DR: 本文综述了无服务器计算在云计算和高性能计算中的应用，提出了分类法并分析了研究趋势，为下一代无服务器解决方案的开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算和云计算的融合，无服务器计算作为一种高效处理动态、并行和分布式工作负载的执行模型，展现出巨大潜力。

Method: 对2018年至2025年初发表的122篇研究文章进行了系统性文献综述，提出了包含八个主要研究方向和使用案例领域的分类法，并分析了出版趋势和作者合作网络。

Result: 提出了一个包含八个主要研究方向和九个使用案例领域的分类法，分析了出版趋势和作者合作网络，展示了这一新兴研究领域的兴趣和互联性。

Conclusion: 本文旨在为研究者和实践者提供有价值的参考，指导下一代无服务器解决方案的开发，以支持并行计算密集型应用。

Abstract: The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [88] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE是一个结合LLMs的硬件设计单元测试生成与调试框架，显著提升了测试覆盖率和调试效率。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的单元测试生成和调试过程需要深入的设计功能理解和创造性，且传统方法耗时费力。

Method: LAUDE集成了提示工程和设计执行信息，利用闭源和开源LLMs生成单元测试并调试硬件设计代码。

Result: 在VerilogEval数据集上，LAUDE生成的单元测试在组合和时序设计中分别检测到100%和93%的bug，并成功调试了93%和84%的设计。

Conclusion: LAUDE框架通过结合LLMs的CoT推理能力和设计源代码的语义理解，显著提高了硬件设计的单元测试生成和调试效率。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [89] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 本文探讨了生成式AI对软件工程教育的影响，提出了LLM集成教育的理论框架，并强调需要实证研究验证其长期效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT和GitHub Copilot）的集成正在重塑软件工程实践，但现有教育课程仍以手动代码生成为核心，导致评估有效性和学习成果的担忧。

Method: 采用概念性研究方法，提出理论框架和教学设计模型。

Result: 提出了一个框架，说明问题分析、设计、实现和测试如何从构建转向批判、验证和人类-AI协作。

Conclusion: 本文提出了一种理论框架来分析生成式AI如何改变核心软件工程能力，并介绍了LLM集成教育的教学设计模型。文章强调了传统抄袭机制的不适应性，提倡转向过程透明模型。最后，文章指出了进行纵向实证研究以评估这些干预措施及其对学习的长期影响的必要性。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [90] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 通过GEPA框架优化提示，显著提升了小型LLMs生成OpenACC pragma的编译成功率和性能，使其在GPU卸载指令生成中达到或超过更大、更昂贵模型的效果。


<details>
  <summary>Details</summary>
Motivation: OpenACC降低了GPU卸载的门槛，但编写高性能的pragma仍然复杂，需要深厚的内存层次、数据移动和并行化策略领域知识。LLMs为自动化并行代码生成提供了有前景的解决方案，但简单的提示常导致语法错误的指令、无法编译的代码或性能未能超过CPU基线。

Method: 利用GEPA（GEnetic-PAreto）框架，通过反思反馈循环迭代优化提示，包括指令的交叉和变异，以及基于专家策划的金标准和预测指令之间的条款和条款参数级别不匹配的结构化反馈。

Result: 在PolyBench套件上的评估显示，使用优化提示生成的OpenACC pragma的编译成功率显著提高，特别是对于“nano”级模型。GPT-4.1 Nano的编译成功率从66.7%提升至93.3%，GPT-5 Nano从86.7%提升至100%。此外，优化提示还使实现功能性GPU加速的程序数量增加了21%。

Conclusion: 提示优化有效释放了较小、较便宜的LLMs在编写稳定且有效的GPU卸载指令方面的潜力，为HPC工作流中基于指令的自动化并行化建立了经济高效的途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [91] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 本文提出了一个量化多LLM生态系统可靠性的框架，并通过案例研究验证了其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗、金融和法律等敏感领域的部署日益增多，但其集成引发了关于信任、责任和可靠性的紧迫问题。

Method: 通过分析系统行为、评估多个LLM的不确定性并实施动态监控流程，提出了一个框架来量化和提高模型在受监管约束下的可靠性。

Result: 案例研究（金融合规和医疗诊断）展示了自适应信任度量在现实场景中的适用性。

Conclusion: 自适应信任度量是受监管行业中安全且可扩展AI采用的基础推动者。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [92] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput是一个Python库，通过自动生成GUI简化算法参数配置，支持跨环境使用，提升可访问性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 计算算法的参数配置通常需要编程技能，且不同环境的接口差异和会话间参数设置的不可持续性导致重复输入、迭代探索缓慢，以及可重复性降低。

Method: EZInput采用声明式规范系统，开发者只需定义输入需求和验证约束，库负责环境检测、界面渲染、参数验证及会话持久化，支持Jupyter笔记本、Google Colab和终端环境。

Result: EZInput实现了“一次编写，随处运行”的架构，支持多种科学计算输入类型，内置验证确保数据完整性，并通过轻量级YAML文件实现参数持久化，减少冗余输入并增强可重复性。

Conclusion: EZInput是一个跨运行时环境的Python库，通过自动生成图形用户界面，使非编程专家的终端用户也能轻松使用计算工具，显著提升了算法的可访问性和可重复性。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [93] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: 该论文提出了首个应用中心化且白盒的AI-Native基准测试套件AI-NativeBench，揭示了传统评估范式无法捕捉的系统级工程现实，并开源了相关资源。


<details>
  <summary>Details</summary>
Motivation: 从Cloud-Native向AI-Native架构的转变使传统的黑盒评估范式不足，现有基准测试仅衡量原始模型能力而忽略了系统级执行动态。

Method: 通过引入基于模型上下文协议（MCP）和代理间（A2A）标准的AI-NativeBench基准测试套件，将代理跨度作为分布式跟踪中的一等公民，实现了对工程特性的细粒度分析。

Result: 在21个系统变体上应用该基准测试，揭示了传统指标无法捕捉的关键工程现实：参数悖论、普遍推理主导和昂贵失败模式。

Conclusion: 该研究为从衡量模型能力转向工程化可靠的AI-Native系统提供了首个系统性证据，并开源了基准测试和数据集以促进可复现性和进一步研究。

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [94] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 论文提出开发阶段隔离策略，通过工具Foreman检测构建系统威胁，如XZ Utils攻击，未来计划自动检查隔离以防范流水线中毒。


<details>
  <summary>Details</summary>
Motivation: C项目的构建系统代码对软件供应链安全至关重要，但现有技术仅关注程序代码，忽略了构建系统本身的安全。构建系统中毒可能绕过漏洞检测工具。

Method: 提出了开发阶段隔离策略，将构建自动化的信息和行为权限建模为程序代码，并开发了原型工具Foreman。

Result: Foreman成功检测并警告了XZ Utils攻击中涉及的测试文件。

Conclusion: 作者提出了开发阶段隔离的新策略，并开发了工具Foreman来检测构建系统中的潜在威胁，如XZ Utils攻击中的测试文件。未来计划通过自动检查开发阶段隔离来防范流水线中毒。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [95] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLM生成的测试用例不稳定性较高，主因是依赖未保证的顺序（63%）。闭源系统中不稳定性传递更明显，需定制化上下文以减少问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的测试用例中不稳定性问题的普遍性和根本原因，为开发者提供实际指导。

Method: 通过放大测试套件（使用GPT-4o和Mistral-Large-Instruct-2407两种LLM），评估生成测试用例的不稳定性，并手动检查根本原因。

Result: 生成的测试用例中不稳定性比例略高于现有测试，最常见的原因是依赖未保证的顺序（占63%）。闭源系统（如SAP HANA）中不稳定性传递更普遍。

Conclusion: 研究表明，LLM生成的测试用例中存在较高的不稳定性（flakiness），尤其是在闭源系统中更为明显。开发者在使用LLM生成测试时，应提供定制化的上下文以减少不稳定性传递。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [96] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner是一个针对自动驾驶系统Plan模型的安全测试框架，通过结构分析和引导式模糊测试生成测试场景，检测出520种危险行为，覆盖率高且效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶系统（ADS）的Plan模型，解决生成结构上有意义的测试场景和检测危险规划行为的核心挑战。

Method: SafePlanner通过结构分析Plan模型的实现（场景转换逻辑和分层控制流），提取可行的场景转换，并结合NPC行为生成测试场景。采用引导式模糊测试探索Plan模型的行为空间。

Result: 生成了20635个测试用例，检测出520种危险行为，覆盖率达到83.63%的函数覆盖率和63.22%的决策覆盖率，在错误发现和效率上优于基线方法。

Conclusion: SafePlanner在Baidu Apollo上成功检测出520种危险行为，并通过手动分析归类为15个根本原因。其中4个问题通过补丁修复后未再出现，且未观察到明显副作用。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [97] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar 是一个代理协调框架，用于深度学习库更新中的缺陷和影响分析，显著提高了缺陷识别和影响分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习库（如 Transformers 和 Megatron）的缺陷可能对下游用户程序产生复杂影响，但现有方法难以准确分析这些影响。

Method: DepRadar 通过协调四个专业代理（PR Miner、Code Diff Analyzer、Orchestrator Agent 和 Impact Analyzer）分三步进行缺陷分析：提取缺陷语义、合成统一缺陷模式并检查下游程序是否触发缺陷。

Result: DepRadar 在 157 个 PR 和 70 个提交上实现了 90% 的缺陷识别精确度，并在 122 个客户端程序中以 90% 的召回率和 80% 的精确度识别出受影响案例。

Conclusion: DepRadar 是一个高效的框架，用于深度学习库更新中的细粒度缺陷和影响分析，其在缺陷识别和影响分析方面表现出色，显著优于其他基线方法。

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [98] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 为解决能源研究缺乏元数据模式的问题，本文开发并测试了一个新模式，结果显示其在形式化与互操作性间取得了平衡，且信息呈现方式对用户至关重要。


<details>
  <summary>Details</summary>
Motivation: 许多领域（包括能源研究）缺乏既定的元数据模式，这影响了研究软件的可发现性和可重用性，也不符合FAIR4RS原则。

Method: 基于需求分析开发了能源研究软件的元数据模式，并通过用户测试进行评估。

Result: 结果表明，该模式在满足能源研究者特定需求的同时，平衡了形式化与互操作性的需求；用户测试显示，信息的良好呈现对元数据创建至关重要。

Conclusion: 本文提供了设计能源研究软件元数据模式的挑战与机遇的见解，强调了模式在平衡形式化与互操作性方面的有效性。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [99] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 研究分析了nf-core管道的问题和拉取请求，识别了13个挑战，发现标签和代码片段有助于快速解决，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core社区广泛采用，但用户在开发和维护这些管道时面临的挑战知之甚少。

Method: 使用BERTopic建模分析了25,173个问题和拉取请求，识别了13个关键挑战，并统计了解决动态和影响因素。

Result: 识别了13个关键挑战，89.38%的问题和拉取请求最终关闭，一半在三天内解决。标签和代码片段显著提高解决可能性。

Conclusion: 本研究为nf-core管道的协作开发和维护提供了可行的见解，强调了提升其可用性、可持续性和可重复性的机会。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [100] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro通过自动化提取系统调用和生成输入数据，有效复现系统级并发错误。


<details>
  <summary>Details</summary>
Motivation: 复现系统级并发错误需要输入数据和精确的系统调用交错顺序，但现有工具无法满足这一需求，且错误报告缺乏详细信息。

Method: SysPro通过信息检索、正则表达式匹配和类别分区方法自动提取系统调用名称并生成输入数据，利用动态源代码插装复现错误。

Result: 实证研究表明，SysPro在真实世界基准测试中表现优异。

Conclusion: SysPro是一种有效且高效的方法，能够从错误报告中定位和复现系统级并发错误。

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [101] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 新版LLM在测试生成中表现优于现有技术，优先针对程序类生成测试可降低成本。


<details>
  <summary>Details</summary>
Motivation: 探讨现有基于LLM的测试生成技术是否因使用较弱的基线（旧版LLM和简单提示）而高估了其性能优势，新版LLM可能使这些技术失去优势。

Method: 复制了四种最先进的基于LLM的测试生成工具（HITS、SymPrompt、TestSpark、CoverUp），并将其与单纯的LLM方法进行比较，实验覆盖393个类和3,657个方法。

Result: 单纯的LLM方法在所有测试有效性指标上均优于现有方法：行覆盖率（提高17.72%）、分支覆盖率（提高19.80%）和变异分数（提高20.92%），且成本相当。优先针对程序类生成测试可减少约20%的LLM请求。

Conclusion: 研究结论表明，单纯的LLM方法在测试生成中能超越现有最先进方法，且在成本相当的情况下实现更高的覆盖率。建议优先针对程序类进行测试生成以提高效率。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [102] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder通过语法简化、数据合成和微调策略，提升代码生成效率，在HumanEval上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型(LLMs)在代码生成中存在效率问题，如每次生成需完整推理和内存消耗高，而现有研究多关注推理阶段优化，生成阶段研究不足。

Method: 提出了ShortCoder框架，包括：1) 10种Python语法级简化规则；2) 结合规则重写和LLM引导的混合数据合成管道；3) 注入简洁意识的微调策略。

Result: ShortCoder在HumanEval上实现了18.1%-37.8%的生成效率提升，同时保持代码生成的性能。

Conclusion: ShortCoder框架通过语法级简化规则、混合数据合成管道和微调策略，显著提升了代码生成效率，同时在HumanEval基准测试中表现优于现有方法。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [103] [Streamlined Pathway (SP) Approach: An Efficient Load Balancer to Enhance Quality of Service](https://arxiv.org/abs/2601.08887)
*Aymen Hasan Alawadi*

Main category: cs.NI

TL;DR: SP模型是一种高效的DCN流调度方案，利用SDN和少量统计信息，显著提升QoS，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在DCN中实现最优QoS同时最小化资源消耗是一个重大挑战，需要高效的负载均衡机制。

Method: 提出了一种名为SP模型的流调度解决方案，该方案利用SDN范式，仅需收集DCN数据平面的少量统计信息（如端口吞吐量和大象流信息），并结合传统的ECMP机制。

Result: 实验和理论分析表明，SP模型在QoS提升方面表现优异，特别是在二分带宽、DCN链路利用率、丢包率和数据包传输延迟等方面优于现有技术。

Conclusion: SP模型通过利用SDN范式，仅需从DCN数据平面收集少量统计信息，便能显著提升QoS，在多个性能指标上优于现有技术如Sieve、Hedera和ECMP。

Abstract: Efficient load-balancing mechanisms are critical for maximizing performance and increasing the quality of service (QoS) of data center networks (DCNs). Obtaining the optimal QoS while minimizing resource consumption remains a significant challenge. This paper proposes the streamlined pathway (SP) model, which is a flow scheduling solution that requires minimal statistical knowledge of the DCN data plane. The SP model utilizes the software-defined networks (SDN) paradigm with less information gathered from the DCN data plane, besides the traditional hash-based flow scheduling mechanism, the Equal Cost Multi-Path (ECMP). In SDN, the proposed methodology harnesses a minimal yet powerful set of statistical data extracted from the DCN data plane, including port throughput and elephant flow information on the aggregate switches of the DCN fat-tree topology. Several experiments, in addition to theoretical analysis, have been conducted to demonstrate the efficiency of the proposed SP model in terms of QoS enhancement. These results confirm that SP outperforms leading techniques such as Sieve, Hedera, and ECMP, concerning bisection bandwidth, DCN link utilization, packet loss, and packet delivery latency.

</details>


### [104] [UAV-enabled Computing Power Networks: Design and Performance Analysis under Energy Constraints](https://arxiv.org/abs/2601.09493)
*Yiqin Deng,Zhengru Fang,Senkang Hu,Yanan Ma,Xiaoyu Guo,Haixia Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: 论文提出了一种无人机驱动的计算能力网络（UAV-CPN），通过优化无人机高度和发射功率，显著提升了计算能力，特别是在双能源约束下。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决多接入边缘计算中存在的通信瓶颈和‘孤岛效应’，通过无人机作为动态中继，将计算任务从请求区域外包到扩展的服务区域，从而提升计算能力。

Method: 论文提出了一种创新框架，通过利用无处不在的计算能力分布和自适应无人机定位来提升计算节点可访问性，建立了无人机驱动的计算能力网络（UAV-CPN）。此外，引入了任务完成概率来量化计算能力性能，并在混合能源架构下通过联合优化无人机高度和发射功率来提升性能。

Result: 广泛的评估显示，UAV-CPN在性能上有显著提升，尤其是在双能源约束下平衡通信和计算能力方面表现出色。

Conclusion: 该论文的结论强调了UAV-CPN在显著提升计算能力方面的潜力，特别是在双能源约束下平衡通信和计算能力的重要性。

Abstract: This paper presents an innovative framework that boosts computing power by utilizing ubiquitous computing power distribution and enabling higher computing node accessibility via adaptive UAV positioning, establishing a UAV-enabled Computing Power Network (UAV-CPN). In a UAV-CPN, a UAV functions as a dynamic relay, outsourcing computing tasks from the request zone to an expanded service zone with diverse computing nodes, including vehicle onboard units, edge servers, and dedicated powerful nodes. This approach has the potential to alleviate communication bottlenecks and overcome the "island effect" observed in multi-access edge computing. A significant challenge is to quantify computing power performance under complex dynamics of communication and computing. To address this challenge, we introduce task completion probability to capture the capability of UAV-CPNs for task computing. We further enhance UAV-CPN performance under a hybrid energy architecture by jointly optimizing UAV altitude and transmit power, where fuel cells and batteries collectively power both UAV propulsion and communication systems. Extensive evaluations show significant performance gains, highlighting the importance of balancing communication and computing capabilities, especially under dual-energy constraints. These findings underscore the potential of UAV-CPNs to significantly boost computing power.

</details>


### [105] [FairShare: Auditable Geographic Fairness for Multi-Operator LEO Spectrum Sharing](https://arxiv.org/abs/2601.09641)
*Seyed Bagher Hashemi Natanzi,Hossein Mohammadi,Vuk Marojevic,Bo Tang*

Main category: cs.NI

TL;DR: 传统动态频谱共享方法加剧城乡数字鸿沟，FairShare框架通过配额机制实现地理公平，提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统动态频谱共享方法可能无意中加剧农村数字鸿沟，本研究旨在验证这一假设并提出解决方案。

Method: 通过大规模、符合3GPP标准的非地面网络（NTN）模拟，系统评估了标准分配策略，并设计了一个轻量级的基于配额的FairShare框架。

Result: 研究发现SNR优先调度导致城乡接入差距达1.65倍，而FairShare框架将差距降至0.72倍，同时减少调度运行时3.3%。

Conclusion: 本研究提出FairShare框架，成功解决了动态频谱共享中的地理公平问题，不仅逆转了城乡接入差距，还提升了调度效率，为下一代卫星网络的公平频谱治理提供了实用方案。

Abstract: Dynamic spectrum sharing (DSS) among multi-operator low Earth orbit (LEO) mega-constellations is essential for coexistence, yet prevailing policies focus almost exclusively on interference mitigation, leaving geographic equity largely unaddressed. This work investigates whether conventional DSS approaches inadvertently exacerbate the rural digital divide. Through large-scale, 3GPP-compliant non-terrestrial network (NTN) simulations with geographically distributed users, we systematically evaluate standard allocation policies. The results uncover a stark and persistent structural bias: SNR-priority scheduling induces a 1.65x urban-rural access disparity, privileging users with favorable satellite geometry. Counter-intuitively, increasing system bandwidth amplifies rather than alleviates this gap, with disparity rising from 1.0x to 1.65x as resources expand. To remedy this, we propose FairShare, a lightweight, quota-based framework that enforces geographic fairness. FairShare not only reverses the bias, achieving an affirmative disparity ratio of Delta_geo = 0.72x, but also reduces scheduler runtime by 3.3%. This demonstrates that algorithmic fairness can be achieved without trading off efficiency or complexity. Our work provides regulators with both a diagnostic metric for auditing fairness and a practical, enforceable mechanism for equitable spectrum governance in next-generation satellite networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 研究通过构建ConvoLearn数据集和微调Mistral 7B，显著提升了LLM在教学对话中的表现，为建构主义AI导师的开发提供了框架。


<details>
  <summary>Details</summary>
Motivation: 在教育应用中，大型语言模型（LLM）存在一些基本教学限制，如倾向于直接提供解决方案而非支持对话式学习。

Method: 通过控制人类教师与模拟学生之间的互动，构建了一个半合成的1250个导师-学生对话数据集（每个对话20轮），并利用QLoRA进行训练。

Result: 人类评估显示，经过微调的Mistral 7B（M = 4.10, SD = 1.03）在整体表现上显著优于其基础版本（M = 2.59, SD = 1.11）和Claude Sonnet 4.5（M = 2.87, SD = 1.29）。

Conclusion: 本研究提出了一个潜在框架，指导未来建构主义AI导师的开发和评估。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [107] [ART: Action-based Reasoning Task Benchmarking for Medical AI Agents](https://arxiv.org/abs/2601.08988)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji*

Main category: cs.AI

TL;DR: ART是一个基于行动的临床推理基准测试，通过真实EHR数据生成任务，评估AI代理在检索、聚合和阈值推理中的表现，发现后两者存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估涉及阈值评估、时间聚合和条件逻辑的基于行动的任务方面表现不足，需要更可靠的临床决策支持AI代理。

Method: 研究团队开发了一个四阶段流程（场景识别、任务生成、质量审计和评估），从真实EHR数据中挖掘具有挑战性的任务，针对已知的推理弱点。

Result: 在600个任务上评估GPT-4o-mini和Claude 3.5 Sonnet显示，经过提示优化后检索近乎完美，但在聚合（28-64%）和阈值推理（32-38%）方面存在显著差距。

Conclusion: ART基准测试通过揭示基于行动的EHR推理中的失败模式，推动了更可靠的临床AI代理的发展，这对于在高需求护理环境中支持工作能力至关重要。

Abstract: Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings

</details>


### [108] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 研究评估了LLM代理在真实电商环境中的任务完成能力，揭示了能力层次及当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在交互环境中的多步任务完成能力，以推动其在现实世界中的部署。

Method: 在Surge提供的真实电子商务RL环境中，对150项工作场所任务进行了实证研究，分析了基于大语言模型（LLM）的代理能力层次。

Result: 性能最佳的模型在约40%的任务中失败，失败模式沿能力层次（工具使用、规划与目标形成、适应性、基础性、常识推理）呈现可预测分布。

Conclusion: 当前前沿模型虽能展示连贯的多步行为，但在真实工作场景中实现人类水平的任务完成仍存在显著能力差距。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [109] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自然语言处理的语义调度范式，通过LLM简化集群工作负载分配，实验证明其在高精度解析和调度质量上优于传统方法，但需异步处理以解决延迟问题。


<details>
  <summary>Details</summary>
Motivation: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing to bridge this gap.

Method: The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed.

Result: Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations.

Conclusion: This work confirms the viability of semantic soft affinity for simplifying workload orchestration, though it highlights limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness.

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [110] [Human-AI Co-design for Clinical Prediction Models](https://arxiv.org/abs/2601.09072)
*Jean Feng,Avni Kothari,Patrick Vossler,Andrew Bishara,Lucas Zier,Newton Addo,Aaron Kornblith,Yan Shuo Tan,Chandan Singh*

Main category: cs.AI

TL;DR: HACHI框架通过AI代理与临床专家协作，高效开发可解释的临床预测模型，实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统临床预测模型开发过程耗时且资源密集，难以整合非结构化临床笔记中的大量概念。

Method: HACHI采用迭代的人机协作框架，AI代理快速探索临床笔记中的候选概念，临床专家提供反馈优化模型学习过程。

Result: HACHI在急性肾损伤和创伤性脑损伤预测任务中表现优于现有方法，发现了新临床相关概念并提升模型泛化能力。

Conclusion: HACHI框架通过结合AI代理和临床专家的反馈，显著提升了临床预测模型（CPMs）的开发效率和效果，并在实际预测任务中表现优异。

Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.

</details>


### [111] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与执行，高效解决多约束规划问题，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM方法在多约束规划中的局限性，如推理不一致、错误累积、高成本及灵活性不足。

Method: 引入SCOPE框架，分离查询特定推理与通用代码执行，生成一致、确定且可重用的求解器函数。

Result: SCOPE在TravelPlanner任务中达到93.1%的成功率，比最佳基线（CoT）提升61.6%，同时降低推理成本1.4倍，时间减少约4.67倍。

Conclusion: SCOPE框架通过分离推理与执行，实现了在多约束规划问题中的高效、低成本解决方案，显著提升了性能并降低了推理成本和时间。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [112] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: DScheLLM利用微调的大型语言模型和双系统推理架构，有效解决了动态生产调度中的扰动问题，展示了在智能调度优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动的适应性和泛化能力有限，需要一种更灵活、通用的解决方案。

Method: 构建了一个基于大型语言模型的统一框架来处理动态事件，使用运筹学求解器生成的精确调度数据训练快速和慢速推理模式，并采用LoRA技术对Huawei OpenPangu Embedded-7B模型进行微调。

Result: 在标准作业车间调度基准测试中，快速思维模式能高效生成高质量调度，慢速思维模式能生成与求解器兼容且格式良好的决策输入。

Conclusion: DScheLLM 展示了大型语言模型在动态生产调度中的潜力，通过双系统（快速-慢速）推理架构有效处理不同规模的扰动，为智能和自适应调度优化提供了新方向。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [113] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 本文提出AviationLMM，一个多模态基础模型，旨在整合民用航空的异构数据流，提升情境感知和决策支持能力，并探讨了实现这一愿景的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 民用航空是全球交通和商业的基石，但现有的AI解决方案局限于孤立任务或单一模态，难以整合异构数据，限制了情境感知和实时决策支持能力。

Method: 论文首先分析了现有AI解决方案的不足，然后描述了AviationLMM的模型架构，该架构能够处理多模态输入（如语音、雷达、传感器等），并进行跨模态对齐与融合，生成灵活的输出。

Result: 论文提出了AviationLMM的设计理念，并指出了实现这一愿景所需解决的关键研究问题，包括数据获取、对齐与融合、预训练、推理等。

Conclusion: 本文提出了AviationLMM的愿景，旨在通过统一民用航空的异构数据流，推动集成、可信且保护隐私的航空AI生态系统的发展。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [114] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 综述了LLM和MLLM中的记忆机制，将其分为隐式、显式和代理三类，并讨论了多模态应用与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和MLLM从静态预测模型发展为支持持续学习和个性化推理的交互系统，记忆机制成为其架构和功能演进的核心主题。本文旨在系统梳理相关研究，为未来方向提供参考。

Method: 本文采用文献综述的方法，将现有研究组织为一个分类体系，涵盖隐式记忆（模型内部参数中的知识）、显式记忆（外部存储与检索组件）和代理记忆（自主代理中的持久结构）。

Result: 提出了记忆的三大分类框架（隐式、显式、代理），并总结了各范式下的关键技术（如参数解释、动态知识存储、多代理协作）。同时，探讨了多模态场景中的记忆整合及开放挑战。

Conclusion: 记忆机制在现代大型语言模型（LLM）和多模态LLM中扮演着核心角色，其发展推动了模型从静态预测向交互式系统的转变。本文通过系统综述，总结了记忆在LLM和MLLM中的分类（隐式、显式和代理记忆）及其应用，并指出了未来研究的关键挑战（如容量、对齐、事实一致性和跨系统互操作性）。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [115] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: PRA是一种AI代理设计，通过模拟个体用户的隐私关注形成过程，结合隐私与认知理论，预测隐私关注并捕捉跨领域推理模式。


<details>
  <summary>Details</summary>
Motivation: 超越群体层面的情感分析，模拟个体用户如何基于个人评论历史和上下文线索形成隐私关注。

Method: PRA整合隐私与认知理论，模拟用户特定的隐私推理过程，包括重构用户的‘隐私心智’、动态激活相关隐私记忆，并生成反映用户可能反应的合成评论。

Result: 实验表明，PRA在真实世界的Hacker News讨论中，隐私关注预测优于基线代理，并能捕捉跨领域（如AI、电商、医疗）的可转移推理模式。

Conclusion: PRA通过模拟用户隐私关注的形成过程，展示了在隐私预测和跨领域推理模式捕捉方面的优越性。

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [116] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文批判现有LLM自动生成评审的方法，提出以LLM辅助人类评审员的教育与协助模式，旨在解决评审员缺口问题并提升评审质量。


<details>
  <summary>Details</summary>
Motivation: AI研究的快速发展加剧了评审员缺口，威胁到同行评审的可持续性，并导致低质量评审的恶性循环。

Method: 提出了两个互补的系统：(i) LLM辅助的导师系统，用于培养评审员的长期能力；(ii) LLM辅助的反馈系统，帮助评审员提升评审质量。

Result: 通过定义高质量同行评审的核心原则，并基于这些原则设计系统，本文为提升评审质量提供了新思路。

Conclusion: 本文提出了一种以人为中心的LLM辅助评审模式，旨在通过教育和协助人类评审员来提升评审质量，从而构建更可持续的学术生态系统。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [117] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM代理的元自适应推理框架，通过前瞻策略和工具使用优势值估计，解决了局部短视和轨迹不稳定的问题，实现了高效的多工具推理。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理推理中的局部短视生成和轨迹不稳定性问题，平衡全局有效性和计算效率。

Method: 提出MAXS框架，结合前瞻策略、工具使用优势值估计、步一致性方差和步间趋势斜率联合选择推理步骤，并引入轨迹收敛机制控制计算成本。

Result: 在三个基础模型和五个数据集上的实证研究表明，MAXS在性能和推理效率上均优于现有方法。

Conclusion: MAXS框架在性能和推理效率上均优于现有方法，通过前瞻策略和工具使用优势值的结合，实现了全局有效性与计算效率的平衡。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [118] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow通过连续概率流量化推理步骤贡献，提升推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理范式缺乏对步骤信息增益的量化机制，导致推理效率低下和优化困难。

Method: 提出了CoT-Flow框架，包括基于流的贪婪解码策略和无验证器的密集奖励强化学习方法。

Result: 在多个挑战性基准测试中，CoT-Flow在推理效率和性能之间取得了优越的平衡。

Conclusion: CoT-Flow框架通过将推理步骤重新定义为连续概率流，有效量化了每一步对最终答案的贡献，显著提升了推理效率和性能。

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [119] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出LLM多智能体框架，通过协调政策减少疫情感染和死亡，验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 人类驱动的疫情应对往往分散且被动，政策制定孤立且滞后，难以实现主动干预和全球疫情缓解。

Method: 提出了一个基于大型语言模型（LLM）的多智能体政策制定框架，每个行政区域分配一个LLM智能体作为AI政策助手，通过模拟疫情演变和跨区域通信，联合探索干预情景并制定协调政策。

Result: 使用美国2020年4月至12月的州级COVID-19数据验证，框架在单个州层面分别减少累计感染和死亡达63.7%和40.1%，跨州汇总减少39.0%和27.0%。

Conclusion: LLM多智能体系统能够通过协调决策实现更有效的疫情控制，减少感染和死亡人数。

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [120] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER 是一种自适应激活空间干预框架，通过动态组合推理向量库提升 LLM 推理效率，显著优于静态方法和 CoT 推理。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法采用静态、手动干预，无法适应复杂推理的动态特性，因此需要一种更灵活、高效的干预框架。

Method: RISER 构建了一个可重用的推理向量库，并利用轻量级路由器动态组合这些向量。路由器通过强化学习在任务级奖励下优化，以涌现和组合方式激活潜在的认知原语。

Result: 在七个多样化基准测试中，RISER 的平均零样本准确率比基础模型提高了 3.4-6.5%，并且在 2-3 倍更高的标记效率下超越了思维链（CoT）式推理，同时保持了稳健的准确率提升。

Conclusion: RISER 提出了一种自适应引导大型语言模型（LLM）推理的框架，通过动态组合可重用的推理向量库，实现了更高效和可控的推理。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [121] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 论文提出$A^3$-Bench，通过双尺度记忆驱动激活评估科学推理，填补了现有基准的不足，并验证了记忆机制对推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注最终答案或逐步推理的连贯性，忽视了记忆驱动机制在人类推理中的作用。为了填补这一空白，论文提出了$A^3$-Bench，旨在通过双尺度记忆驱动激活评估科学推理。

Method: 论文首先通过SAPM流程标注了2,198个科学推理问题，随后提出了基于锚点与吸引子的双尺度记忆评估框架，并引入AAUI指标来衡量记忆激活率。最后通过多种基础模型和范式的实验验证了$A^3$-Bench的有效性。

Result: 实验验证了$A^3$-Bench的有效性，并分析了记忆激活如何影响推理性能，为记忆驱动的科学推理提供了新的见解。

Conclusion: 通过$A^3$-Bench的提出与实验验证，论文展示了记忆驱动机制对科学推理的重要性，并提供了衡量记忆激活率的新方法AAUI，为未来研究提供了新的评估框架和洞见。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [122] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu,Chao Feng,Lang Mei,Chong Chen*

Main category: cs.AI

TL;DR: M$^3$Searcher是一种模块化多模态信息搜索代理，通过多目标奖励优化，解决了多模态信息搜索的挑战，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅限于文本模态，扩展到多模态设置面临专业化与泛化的权衡问题及训练数据稀缺的挑战。

Method: 提出了M$^3$Searcher，一种模块化的多模态信息搜索代理，明确将信息获取与答案推导分离，并通过检索导向的多目标奖励进行优化。

Result: M$^3$Searcher在实验中优于现有方法，展示了强大的迁移适应性和有效推理能力。

Conclusion: M$^3$Searcher在复杂多模态任务中表现出色，具有强大的迁移适应能力和有效推理能力。

Abstract: Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.

</details>


### [123] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR是一种无需参数的推理时遗忘框架，通过多步骤动态抑制敏感内容，解决LRMs隐私泄露问题，并在评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）生成复杂思维链（CoT）轨迹时可能嵌入敏感信息，现有LLM遗忘方法无法有效清除中间步骤的敏感内容，导致隐私泄露和安全风险。

Method: 提出Sensitive Trajectory Regulation (STaR)框架，包括语义感知检测、安全提示前缀注入、轨迹感知抑制和令牌级自适应过滤。

Result: 在R-TOFU基准测试中，STaR实现了全面且稳定的遗忘效果，效用损失最小。

Conclusion: STaR框架通过语义感知检测、全局安全约束注入、轨迹感知抑制和令牌级自适应过滤，实现了在推理过程中全面的隐私保护，并在R-TOFU基准测试中展示了最小的效用损失。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [124] [Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures](https://arxiv.org/abs/2601.09293)
*Sofiene Lassoued,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出一种结合Petri网和强化学习的新框架，用于动态作业车间调度，在随机作业到达和机器故障条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决动态作业车间调度问题中的随机作业到达和意外机器故障带来的挑战，以更好地反映真实制造场景。

Method: 采用基于模型的范式，使用着色时间Petri网表示调度环境，并结合Maskable Proximal Policy Optimization进行动态决策，同时限制代理在每个决策点采取可行动作。

Result: 在动态JSSP基准测试中，该方法在最小化制造周期方面 consistently 优于传统的启发式和基于规则的方法。

Conclusion: 该论文提出了一个结合可解释的Petri网模型与自适应强化学习策略的框架，用于解决动态和不确定制造环境中的实时调度问题，表现出更强的弹性、可扩展性和可解释性。

Abstract: We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.

</details>


### [125] [Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving](https://arxiv.org/abs/2601.09353)
*Ioannis Peridis,Dimitrios Troullinos,Georgios Chalkiadakis,Pantelis Giankoulidis,Ioannis Papamichail,Markos Papageorgiou*

Main category: cs.AI

TL;DR: 研究结合MCTS和预训练NN，优化无车道交通中的自动驾驶规划，提升安全性和效率，并分析计算资源与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 无车道交通环境能更好地利用道路横向容量，但为自动驾驶带来了独特且更具挑战性的场景，需探索高效规划方法。

Method: 采用蒙特卡洛树搜索（MCTS）规划方法，结合预训练神经网络（NN）引导选择阶段，以增强树搜索过程。

Result: 实验评估显示，NN引导的MCTS变体在安全（碰撞率）和效率（速度）方面表现更优，并观察到车辆因尾部快速车辆存在而产生的‘推挤行为’。

Conclusion: 研究通过MCTS与预训练神经网络结合的方法，在无车道交通环境中提升了自动驾驶的安全性和效率，同时探讨了计算资源与解决方案质量之间的权衡。

Abstract: Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.

</details>


### [126] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 本文提出了一种主动任务导向代理的新交互范式，通过意图条件监控和事件触发跟进能力，结合合成数据和ChronosBench基准，验证了模型在动态环境中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型代理主要处于被动响应模式，无法长期维持用户意图或动态适应环境变化。

Method: 通过高质量数据合成流程构建动态环境中的复杂多轮对话数据，并提出了新的基准测试ChronosBench。

Result: 微调模型在包含用户意图变化的复杂任务中表现优异，任务完成率达85.19%，优于其他测试模型。

Conclusion: 本文提出的主动任务导向代理模型在复杂任务中实现了85.19%的任务完成率，验证了数据驱动策略的有效性。

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [127] [EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines](https://arxiv.org/abs/2601.09465)
*Shuo Zhang,Chaofa Yuan,Ryan Guo,Xiaomin Yu,Rui Xu,Zhangquan Chen,Zinuo Li,Zhi Yang,Shuhao Guan,Zhenheng Tang,Sen Hu,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: cs.AI

TL;DR: EvoFSM 是一种结构化自进化框架，通过有限状态机（FSM）实现可控的适应性，显著提升多跳问答和决策任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的代理依赖固定工作流，难以适应开放性问题，而自由形式的自进化常导致不稳定、幻觉和指令漂移。

Method: EvoFSM 通过解耦优化空间为宏观的 Flow（状态转移逻辑）和微观的 Skill（特定状态行为），在明确的行为边界下进行针对性改进。

Result: EvoFSM 在五个多跳问答基准测试中表现优异，尤其在 DeepSearch 基准上达到 58.0% 准确率。

Conclusion: EvoFSM 通过结构化的自进化框架，在保持控制的同时实现了适应性，显著提升了多跳问答和交互决策任务的性能。

Abstract: While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.

</details>


### [128] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 研究提出T2Q评估范式，揭示任务成功与环境理解的差距，并指出探索和状态表示是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式主要依赖任务成功的轨迹指标，未能评估代理是否具备对环境的可迁移理解能力。

Method: 提出了Task-to-Quiz（T2Q）评估范式，并在T2QBench中实例化，包含30个环境和1,967个基于难度的QA对。

Result: 实验表明任务成功常不能代表环境理解，且当前记忆机制无法有效帮助代理获取环境模型。

Conclusion: 研究指出了当前LLM代理在环境理解方面的不足，并提出了Task-to-Quiz（T2Q）评估范式，为开发更具泛化能力的自主代理提供了基础。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [129] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: 提出统一生成式多模态推理框架Omni-R1和Omni-R1-Zero，后者无需多模态标注，表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用单一任务特定推理模式，限制了多模态任务的泛化能力。

Method: 提出了统一生成式多模态推理范式，通过生成中间图像实现多样化推理技能的统一，并具体化为Omni-R1（两阶段SFT+RL框架）和Omni-R1-Zero（无需多模态标注）。

Result: Omni-R1实现了广泛多模态任务的统一生成式推理，Omni-R1-Zero在平均表现上匹配甚至超越Omni-R1。

Conclusion: Omni-R1和Omni-R1-Zero展示了生成式多模态推理的统一框架，后者甚至在某些情况下超越了前者，为多模态推理提供了有前景的方向。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [130] [LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach](https://arxiv.org/abs/2601.09635)
*Kuo Liang,Yuhang Lu,Jianming Mao,Shuyi Sun,Chunwei Yang,Congcong Zeng,Xiao Jin,Hanzhang Qin,Ruihao Zhu,Chung-Piaw Teo*

Main category: cs.AI

TL;DR: LEAN-LLM-OPT：基于LLM代理的轻量级框架，自动化构建大规模优化模型，性能优异且实用性强。


<details>
  <summary>Details</summary>
Motivation: 解决大规模优化模型构建耗时耗力的问题，利用LLM的文本处理能力简化建模流程。

Method: LEAN-LLM-OPT采用轻量级代理工作流，由上游LLM代理动态构建建模流程，下游代理执行具体生成任务，结合辅助工具处理数据操作。

Result: 在GPT-4.1和gpt-oss-20B实例化下，LEAN-LLM-OPT在大规模优化任务中表现优异，并创建了首个综合基准Large-Scale-OR和Air-NRM。

Conclusion: LEAN-LLM-OPT框架通过LLM代理团队协作，实现了大规模优化问题的自动化建模，并在实际应用（如新加坡航空的收益管理）中展示了领先性能。

Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.

</details>


### [131] [PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records](https://arxiv.org/abs/2601.09636)
*Yibo Lyu,Gongwei Chen,Rui Shao,Weili Guan,Liqiang Nie*

Main category: cs.AI

TL;DR: 提出了PersonalAlign任务和AndroidIntent基准，用于评估代理在模糊指令和主动建议中的表现，并开发了HIM-Agent，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，用户意图往往是隐含且复杂的，需要代理利用长期用户记录来解决模糊指令并预测潜在习惯。

Method: 提出了Hierarchical Intent Memory Agent (HIM-Agent)，通过持续更新的个人记忆和分层组织用户偏好与习惯来实现个性化。

Result: HIM-Agent在AndroidIntent基准测试中，执行和主动性能分别提升了15.7%和7.3%。

Conclusion: HIM-Agent通过持续更新的个人记忆和分层组织用户偏好与习惯，显著提升了GUI代理的执行和主动性能。

Abstract: While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

</details>


### [132] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL 是一种多智能体测试时强化学习框架，通过结构化文本经验注入多智能体推理，提升决策准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际应用中表现出色，但多智能体强化学习（MARL）训练资源密集且不稳定，队友的协同适应导致非平稳性，奖励稀疏且方差高。

Method: MATTRL 通过形成多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识决策。同时研究了信用分配机制，构建轮级经验池并重新注入对话。

Result: 在医学、数学和教育等领域的挑战性基准测试中，MATTRL 比多智能体基线平均准确率提高 3.67%，比单智能体基线提高 8.67%。

Conclusion: MATTRL 提供了一种稳定、有效且高效的方法，无需调整即可实现分布偏移鲁棒的多智能体推理。

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


### [133] [Automating Supply Chain Disruption Monitoring via an Agentic AI Approach](https://arxiv.org/abs/2601.09680)
*Sara AlMahri,Liming Xu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 提出了一种AI框架，通过七个代理实时监测和分析供应链中断，显著提升响应速度和成本效率，为供应链韧性提供新方案。


<details>
  <summary>Details</summary>
Motivation: 现代供应链日益面临地缘政治事件、需求冲击、贸易限制和自然灾害等中断的威胁，但大多数公司缺乏对一级供应商之外的可见性，导致上游漏洞直到影响下游时才被发现。

Method: 引入了一个最小监督的代理AI框架，由七个由大型语言模型和确定性工具驱动的专门代理组成，共同检测来自非结构化新闻的中断信号，将其映射到多层供应商网络，基于网络结构评估暴露风险，并推荐缓解措施。

Result: 在30个合成场景中评估了该框架，覆盖三家汽车制造商和五类中断，系统在核心任务中实现了高准确度（F1分数介于0.962至0.991之间），端到端分析平均耗时3.83分钟，每次中断成本为0.0836美元。

Conclusion: 本研究为构建能够管理深层供应链网络中断的韧性、主动和自主供应链奠定了基础。

Abstract: Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [134] [Fairness risk and its privacy-enabled solution in AI-driven robotic applications](https://arxiv.org/abs/2601.08953)
*Le Liu,Bangguo Yu,Nynke Vellinga,Ming Cao*

Main category: cs.RO

TL;DR: 论文提出了一种结合公平性与隐私性的统一框架，通过机器人导航任务验证了隐私预算可共同用于实现公平性目标，为AI伦理使用提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在自主机器和算法中的广泛应用，公平性问题日益凸显。尽管关于公平性的直觉普遍存在，但缺乏一个既能捕捉用户效用又能考虑数据随机性的精确且可实现的定义。

Method: 论文提出了一种基于用户效用和数据随机性的实用感知公平性度量，并分析了公平性与用户数据隐私的联合关系，推导出隐私预算如何影响公平性度量的条件。

Result: 研究通过机器人导航任务测试了提出的框架，发现隐私预算可以共同用于满足公平性目标，这在法律要求下的大多数机器人系统中具有重要意义。

Conclusion: 该论文提出了一种统一框架，将公平性与隐私性结合考虑，并通过机器人导航任务验证了其有效性。研究表明，隐私预算可以共同用于实现公平性目标，为AI的伦理使用和增强对自主机器人的信任迈出了重要一步。

Abstract: Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.

</details>


### [135] [Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation](https://arxiv.org/abs/2601.09031)
*Xuetao Li,Wenke Huang,Mang Ye,Jifeng Xuan,Bo Du,Sheng Liu,Miao Li*

Main category: cs.RO

TL;DR: RGMP-S是一种新型人形机器人操作方法，通过几何先验和尖峰特征提升场景理解和数据效率，实验证明其泛化能力和性能优势。


<details>
  <summary>Details</summary>
Motivation: 人形机器人操作需要高层次的语义推理和低层次的动作生成，但现有方法在精确场景理解和数据高效学习方面存在挑战。

Method: 提出了一种结合几何先验的多模态策略（RGMP-S），包括长时程几何先验技能选择器和递归自适应尖峰网络，用于高效的运动合成和场景理解。

Result: 在Maniskill仿真基准和三种异构真实机器人系统上的实验表明，RGMP-S优于现有基线方法，并验证了其在多样化泛化场景中的有效性。

Conclusion: RGMP-S方法通过结合几何先验和尖峰特征，在未见环境中实现了鲁棒的泛化能力，并在稀疏演示场景下提升了数据效率，实验验证了其优于现有方法的性能。

Abstract: Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.

</details>


### [136] [Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object](https://arxiv.org/abs/2601.09104)
*Ko Yamamoto,Kyosuke Ishibashi,Hiroki Ishikawa,Osamu Azami*

Main category: cs.RO

TL;DR: 本文提出一种液压驱动软体抓手设计方法，通过数学模型和有限元分析实现大型重物抓取，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有气动软体抓手因压力限制无法满足大型重物的抓取需求，液压驱动具有更高压力潜力。

Method: 基于数学模型确定设计参数，结合有限元分析选择合适材料，开发了液压驱动的软体抓手。

Result: 实验成功抓取20公斤物体，并实现了弯曲角度的闭环控制。

Conclusion: 液压驱动的软体机器人抓手成功实现了对大型重物（约10-20公斤）的抓取，并通过闭环控制实现了弯曲角度的精确控制。

Abstract: This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.

</details>


### [137] [CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space](https://arxiv.org/abs/2601.09163)
*Tong Wu,Shoujie Li,Junhao Gong,Changqing Guo,Xingting Li,Shilong Mu,Wenbo Ding*

Main category: cs.RO

TL;DR: CEI框架通过功能相似性和梯度优化，实现跨机器人形态的数据和策略迁移，平均迁移率82.4%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人基础模型因数据集偏差而过度适应特定视角、机械臂和平行夹爪的问题，实现跨形态的演示迁移。

Method: 提出Cross-Embodiment Interface (CEI)框架，利用功能相似性（通过Directional Chamfer Distance量化）和梯度优化对齐轨迹，合成未见机器人形态的观测和动作。

Result: 在仿真中成功将数据和策略从Franka Panda迁移到16种不同形态，并在真实任务中实现UR5+AG95与UR5+Xhand的双向迁移，平均迁移率82.4%。

Conclusion: CEI框架通过功能相似性度量和梯度优化，成功实现了跨机器人形态的数据和策略迁移，平均迁移率达82.4%，并展示了空间泛化和多模态运动生成能力。

Abstract: Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \CEI introduces the concept of \textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \CEI transfers data and policies from a Franka Panda robot to \textbf{16} different embodiments across \textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\%. Finally, we demonstrate that \CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/

</details>


### [138] [Vision-Conditioned Variational Bayesian Last Layer Dynamics Models](https://arxiv.org/abs/2601.09178)
*Paul Brunzema,Thomas Lew,Ray Zhang,Takeru Shirasawa,John Subosits,Marcus Greiff*

Main category: cs.RO

TL;DR: 提出视觉条件变分贝叶斯动力学模型，通过预判环境变化提升车辆竞速控制性能，实验验证其优于传统反应式方法。


<details>
  <summary>Details</summary>
Motivation: 自主框架下实现主动适应（如驾驶员预判路面摩擦）仍具挑战性，尤其在快速变化条件下。传统建模方法难以捕捉系统行为的突变，而自适应方法通常是反应式的，可能因适应过晚而无法保证安全。

Method: 模型首先学习名义车辆动力学，然后通过潜在特征的逐特征仿射变换进行微调，实现上下文感知的动力学预测，最终集成到最优控制器中。

Result: 在Lexus LC500车辆涉水竞速实验中，视觉条件模型成功完成所有12圈测试，而所有无视觉上下文的基线模型均失控，证明了方法的有效性。

Conclusion: 论文提出了一种基于视觉条件的变分贝叶斯最后一层动力学模型，通过视觉上下文预测环境变化，成功应用于车辆竞速控制，验证了在高性能应用中主动动力学适应的重要性。

Abstract: Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.

</details>


### [139] [Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces](https://arxiv.org/abs/2601.09231)
*Shuoye Li,Zhiyuan Song,Yulin Li,Zhihai Bi,Jun Ma*

Main category: cs.RO

TL;DR: 论文提出了一种基于多项式超曲面的非线性避障方法，解决了传统凸近似方法的保守性问题，适用于复杂和非凸环境。


<details>
  <summary>Details</summary>
Motivation: 传统基于凸近似的避障方法在复杂和非凸环境中过于保守，限制了机器人的灵活性和效率。论文旨在消除这一限制，提出更通用的非线性分离方法。

Method: 通过推广经典的分离超平面定理，论文证明了多项式超曲面可以分离任意两个不相交的有界闭集，并基于此构建了一个非线性规划（NLP）问题，联合优化机器人轨迹和分离多项式的系数。

Result: 仿真和实际实验表明，该方法在非凸环境中能够实现平滑、无碰撞且敏捷的机动，优于基于凸近似的基线方法。

Conclusion: 该论文提出了一种基于多项式超曲面的非线性分离方法，有效解决了传统线性分离器在非凸环境中的局限性，实现了在复杂和狭窄环境中的高效避障。

Abstract: An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.

</details>


### [140] [Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report](https://arxiv.org/abs/2601.09318)
*Ro'i Lang,Elon Rimon*

Main category: cs.RO

TL;DR: 本文提出了一种用于3D空间中运动规划的多项式导航函数，能有效处理球形和圆柱形障碍物，避免局部最小值，并通过理论分析和模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D工作空间中存在球形和圆柱形障碍物时的运动规划问题，尤其是在障碍物交叉的情况下避免局部最小值。

Method: 采用平滑多项式隐函数对球形和圆柱形障碍物进行建模，并通过梯度和Hessian分析确保导航函数在目标点具有唯一的非退化最小值。

Result: 理论分析和数值模拟表明，所提出的多项式导航函数在复杂3D环境中有效，能够确保路径规划的唯一性和无局部最小值。

Conclusion: 本文通过多项式导航函数在3D工作空间中实现了有效的运动规划，即使在存在交叉障碍物的情况下也能避免局部最小值，并通过数值模拟验证了理论结果的正确性。

Abstract: This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.

</details>


### [141] [ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving](https://arxiv.org/abs/2601.09377)
*Xuemei Yao,Xiao Yang,Jianbin Sun,Liuwei Xie,Xuebin Shao,Xiyu Fang,Hang Su,Kewei Yang*

Main category: cs.RO

TL;DR: ReflexDiffusion通过反射调整优化扩散轨迹规划器，显著提升高横向加速度场景的安全性，驾驶分数提升14.1%。


<details>
  <summary>Details</summary>
Motivation: 解决现有轨迹规划器在数据不平衡导致的高风险场景（如急转弯）中系统失效的问题，提升自动驾驶车辆在物理极限附近操作时的轨迹预测安全性和可靠性。

Method: 引入了一种梯度调整机制，在迭代去噪过程中计算条件与非条件噪声预测之间的梯度，以放大关键条件信号（如道路曲率和横向车辆动力学）。

Result: 在nuPlan Test14-hard基准测试中，ReflexDiffusion在高横向加速度场景下的驾驶分数比现有最优方法提高了14.1%。

Conclusion: ReflexDiffusion框架通过反射调整增强了基于扩散的轨迹规划器，显著提高了高横向加速度场景下的安全性，为自动驾驶车辆在挑战性驾驶条件下提供了实用的安全解决方案。

Abstract: Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.

</details>


### [142] [Data Scaling for Navigation in Unknown Environments](https://arxiv.org/abs/2601.09444)
*Lauri Suomela,Naoki Takahata,Sasanka Kuruppu Arachchige,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 大规模多样化数据训练可实现零样本导航，数据多样性比数量更重要，回归模型在噪声数据中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习导航策略在训练未见环境中的泛化问题。

Method: 使用来自35个国家161个地点的4,565小时众包数据集，训练点目标导航策略，并在四个国家的125公里自主驾驶中评估闭环控制性能。

Result: 数据多样性显著提升导航性能，而数据数量的增加效果有限。回归模型在噪声众包数据中表现优于生成和序列架构。

Conclusion: 大规模训练数据和数据多样性是实现零样本导航的关键，尤其是在未知环境中，其性能接近特定环境演示训练的模型。数据多样性比数据数量更为重要。

Abstract: Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.
  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.

</details>


### [143] [CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion](https://arxiv.org/abs/2601.09512)
*Ralf Römer,Yi Zhang,Angela P. Schoellig*

Main category: cs.RO

TL;DR: CLARE 是一种无需示例的持续学习框架，通过模块化适配器和动态路由机制，使机器人能持续适应新任务并保留旧知识，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法在机器人领域通常需要存储历史数据、难以处理长任务序列或依赖任务标识符，限制了其在实际长期操作中的应用。

Method: CLARE 在选定的前馈层中引入轻量级模块化适配器，并通过层间特征相似性指导模型扩展。部署时，基于自动编码器的路由机制动态激活最相关的适配器。

Result: 在 LIBERO 基准测试中，CLARE 在新任务上表现优异且未发生灾难性遗忘，显著优于基于示例的方法。

Conclusion: CLARE 是一种高效、无需示例的持续学习框架，通过轻量级模块化适配器和自动扩展机制，成功解决了机器人长期操作中的任务适应和知识保留问题。

Abstract: To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.

</details>


### [144] [Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations](https://arxiv.org/abs/2601.09518)
*Wei-Jin Huang,Yue-Yi Zhang,Yi-Lin Wei,Zhi-Wei Xia,Juantao Tan,Yuan-Ming Li,Zhilin Zhao,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: PAIR和D-STAR共同解决了人形机器人交互中的数据生成与策略学习问题，通过物理感知重定向和分层策略实现了高效的交互学习。


<details>
  <summary>Details</summary>
Motivation: 人形机器人与人类物理交互的研究因缺乏高质量的HHoI数据而受限，而利用丰富的HHI数据则面临标准重定向方法无法保持必要接触的挑战。

Method: PAIR是一种以接触为中心的、两阶段的物理感知交互重定向方法，用于生成物理一致的HHoI数据；D-STAR是一种分层策略，通过解耦时空动作推理（何时行动与何处行动）来提升交互理解。

Result: PAIR成功生成了高质量的HHoI数据，D-STAR策略在模仿学习基础上实现了超越单纯轨迹模仿的交互理解，并通过实验验证了其显著优于基线方法的性能。

Conclusion: PAIR和D-STAR的结合为解决人形机器人与人类物理交互问题提供了一个完整的解决方案，显著提升了性能并实现了从HHI数据中学习复杂全身交互的有效流程。

Abstract: Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.

</details>


### [145] [Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping](https://arxiv.org/abs/2601.09578)
*Jiajun Sun,Yangyi Ou,Haoyuan Zheng,Chao yang,Yue Ma*

Main category: cs.RO

TL;DR: 本文提出了一种将热信息语义增强到3D点云地图的新方法，通过融合可见光和红外图像，并利用热通道分割高温目标，生成兼具几何精度和语义理解的地图，适用于灾害评估和工业维护。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，自主机器人导航和环境感知对SLAM技术提出了更高要求，需要不仅具备几何精度，还能理解环境语义，尤其是在灾害评估和工业维护等应用中。

Method: 首先进行可见光和红外图像的像素级融合，然后将实时LiDAR点云投影到融合图像流上，接着在热通道中分割热源特征以识别高温目标，最后将温度信息作为语义层应用于最终的3D地图。

Result: 生成的地图不仅具有精确的几何结构，还包含对环境的语义理解，特别是能够识别高温目标，为特定应用提供了重要价值。

Conclusion: 本文提出的方法通过融合可见光和红外图像，并将温度信息作为语义层应用于3D点云地图，不仅提升了地图的几何精度，还增强了环境语义理解，特别适用于灾害快速评估和工业预防性维护等特定应用。

Abstract: In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [146] [TIDI-GS: Floater Suppression in 3D Gaussian Splatting for Enhanced Indoor Scene Fidelity](https://arxiv.org/abs/2601.09291)
*Sooyeun Yang,Cheyul Im,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.GR

TL;DR: TIDI-GS是一个轻量级插件框架，通过浮游物修剪算法和深度损失函数，有效消除3D高斯溅射中的浮游物，提升几何精度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射技术（3DGS）在生成高质量实时3D场景时会产生浮游物（floaters），这些几何不准确性影响了模型的实用性。

Method: TIDI-GS是一种轻量级插件框架，包含基于多视角一致性、空间关系和训练中学习的重要性评分的浮游物修剪算法（TIDI），以及保留高频细节的机制和基于单目深度的损失函数。

Result: 实验证明，TIDI-GS显著提升了重建的感知质量和几何完整性。

Conclusion: TIDI-GS成功提升了3D高斯溅射技术的几何完整性和感知质量，使其成为高保真应用的可靠数字资产。

Abstract: 3D Gaussian Splatting (3DGS) is a technique to create high-quality, real-time 3D scenes from images. This method often produces visual artifacts known as floaters--nearly transparent, disconnected elements that drift in space away from the actual surface. This geometric inaccuracy undermines the reliability of these models for practical applications, which is critical. To address this issue, we introduce TIDI-GS, a new training framework designed to eliminate these floaters. A key benefit of our approach is that it functions as a lightweight plugin for the standard 3DGS pipeline, requiring no major architectural changes and adding minimal overhead to the training process. The core of our method is a floater pruning algorithm--TIDI--that identifies and removes floaters based on several criteria: their consistency across multiple viewpoints, their spatial relationship to other elements, and an importance score learned during training. The framework includes a mechanism to preserve fine details, ensuring that important high-frequency elements are not mistakenly removed. This targeted cleanup is supported by a monocular depth-based loss function that helps improve the overall geometric structure of the scene. Our experiments demonstrate that TIDI-GS improves both the perceptual quality and geometric integrity of reconstructions, transforming them into robust digital assets, suitable for high-fidelity applications.

</details>


### [147] [Variable Basis Mapping for Real-Time Volumetric Visualization](https://arxiv.org/abs/2601.09417)
*Qibiao Li,Yuxuan Wang,Youcheng Cai,Huangsheng Du,Ligang Liu*

Main category: cs.GR

TL;DR: VBM框架通过小波域分析将体积数据转换为3D高斯喷射表示，实现了实时体积可视化，显著提升了渲染效率和效果。


<details>
  <summary>Details</summary>
Motivation: 大规模体积数据的实时可视化具有挑战性，传统的直接体积渲染和基于体素的方法计算成本过高。

Method: 提出了Variable Basis Mapping（VBM）框架，通过小波域分析将体积场转换为3D高斯喷射表示。具体包括预计算紧凑的小波到高斯转换库、解析高斯构造以及轻量级的图像空间微调阶段。

Result: 在多样化数据集上的实验表明，VBM显著加速了收敛并提升了渲染质量。

Conclusion: VBM框架通过将体积场转换为3D高斯喷射表示，显著加速了收敛并提升了渲染质量，实现了实时体积可视化。

Abstract: Real-time visualization of large-scale volumetric data remains challenging, as direct volume rendering and voxel-based methods suffer from prohibitively high computational cost. We propose Variable Basis Mapping (VBM), a framework that transforms volumetric fields into 3D Gaussian Splatting (3DGS) representations through wavelet-domain analysis. First, we precompute a compact Wavelet-to-Gaussian Transition Bank that provides optimal Gaussian surrogates for canonical wavelet atoms across multiple scales. Second, we perform analytical Gaussian construction that maps discrete wavelet coefficients directly to 3DGS parameters using a closed-form, mathematically principled rule. Finally, a lightweight image-space fine-tuning stage further refines the representation to improve rendering fidelity. Experiments on diverse datasets demonstrate that VBM significantly accelerates convergence and enhances rendering quality, enabling real-time volumetric visualization.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [148] [An Almost-Optimal Upper Bound on the Push Number of the Torus Puzzle](https://arxiv.org/abs/2601.08989)
*Matteo Caporrella,Stefano Leucci*

Main category: cs.DS

TL;DR: 本文优化了Torus Puzzle的push number上界，通过新算法将差距从线性缩小到对数级别。


<details>
  <summary>Details</summary>
Motivation: 研究Torus Puzzle的push number和drag number，尤其是push number的上界与下界之间的差距较大，需要进一步优化。

Method: 通过设计一种算法，使用O(mn·log max{m,n})单位旋转来解决Torus Puzzle。

Result: 提出的算法在更严格的模型下，将push number的上界从O(mn·max{m,n})优化到O(mn·log max{m,n})。

Conclusion: 本文提出了一种在更严格模型下解决Torus Puzzle的算法，将已知上界和下界之间的差距从Θ(max{m,n})缩小到Θ(log max{m,n})。

Abstract: We study the Torus Puzzle, a solitaire game in which the elements of an input $m \times n$ matrix need to be rearranged into a target configuration via a sequence of unit rotations (i.e., circular shifts) of rows and/or columns. Amano et al.\ proposed a more permissive variant of the above puzzle, where each row and column rotation can shift the involved elements by any amount of positions. The number of rotations needed to solve the puzzle in the original and in the permissive variants of the puzzle are respectively known as the \emph{push number} and the \emph{drag number}, where the latter is always smaller than or equal to the former and admits an existential lower bound of $Ω(mn)$. While this lower bound is matched by an $O(mn)$ upper bound, the push number is not so well understood. Indeed, to the best of our knowledge, only an $O(mn \cdot \max\{ m, n \})$ upper bound is currently known. In this paper, we provide an algorithm that solves the Torus Puzzle using $O(mn \cdot \log \max \{m, n\})$ unit rotations in a model that is more restricted than that of the original puzzle. This implies a corresponding upper bound on the push number and reduces the gap between the known upper and lower bounds from $Θ(\max\{m,n\})$ to $Θ(\log \max\{m, n\})$.

</details>


### [149] [A Grouped Sorting Queue Supporting Dynamic Updates for Timer Management in High-Speed Network Interface Cards](https://arxiv.org/abs/2601.09081)
*Zekun Wang,Binghao Yue,Weitao Pan,Jianyi Shi,Yue Hao*

Main category: cs.DS

TL;DR: 该论文提出了一种支持更新和分组排序的硬件优先级队列，显著提升了定时器管理的性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有定时器管理方案存在软件负载重、精度低、缺乏硬件更新支持和溢出等问题，限制了网络接口卡（NICs）在高吞吐量任务中的性能。

Method: 采用一维（1D）脉动阵列和移位寄存器的混合架构，实现了支持更新操作和分组排序机制的硬件优先级队列。

Result: 在28nm工艺下，4K深度、16位定时器队列实现了超过500 MHz（175 Mpps，12 ns精度）的性能，FPGA上超过300 MHz（116 Mpps），相比现有设计分别减少了31%的LUTs和25%的FFs使用量。

Conclusion: 该论文提出的硬件优先级队列设计，通过更新和分组排序操作，有效解决了现有定时器管理方案的软件负载重、精度低、缺乏硬件更新支持和溢出问题，显著提升了性能和资源利用率。

Abstract: With the hardware offloading of network functions, network interface cards (NICs) undertake massive stateful, high-precision, and high-throughput tasks, where timers serve as a critical enabling component. However, existing timer management schemes suffer from heavy software load, low precision, lack of hardware update support, and overflow. This paper proposes two novel operations for priority queues--update and group sorting--to enable hardware timer management. To the best of our knowledge, this work presents the first hardware priority queue to support an update operation through the composition and propagation of basic operations to modify the priorities of elements within the queue. The group sorting mechanism ensures correct timing behavior post-overflow by establishing a group boundary priority to alter the sorting process and element insertion positions. Implemented with a hybrid architecture of a one-dimension (1D) systolic array and shift registers, our design is validated through packet-level simulations for flow table timeout management. Results demonstrate that a 4K-depth, 16-bit timer queue achieves over 500 MHz (175 Mpps, 12 ns precision) in a 28nm process and over 300 MHz (116 Mpps) on an FPGA. Critically, it reduces LUTs and FFs usage by 31% and 25%, respectively, compared to existing designs.

</details>


### [150] [Dynamic Hierarchical $j$-Tree Decomposition and Its Applications](https://arxiv.org/abs/2601.09139)
*Gramoz Goranci,Monika Henzinger,Peter Kiss,Ali Momeni,Gernot Zöcklein*

Main category: cs.DS

TL;DR: 提出动态层次化$j$-树分解框架，实现多对数近似和次线性更新时间，适用于多种基于切割的优化问题。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在容量限制的无向图中，边插入和删除情况下基于切割的优化问题的近似算法设计。

Method: 通过动态切割稀疏化算法处理顶点分割，并结合新的结构洞察来维护森林。

Result: 实现了多对数近似因子，并在$O(n^ε)$摊销更新时间内支持边更新，为多个基于切割的优化问题提供了首个在次线性时间内实现多对数近似的完全动态算法。

Conclusion: 该论文提出了一种新的算法框架，用于动态维护层次化$j$-树分解，从而在完全动态设置中为基于切割的优化问题提供了新的权衡方案。

Abstract: We develop a new algorithmic framework for designing approximation algorithms for cut-based optimization problems on capacitated undirected graphs that undergo edge insertions and deletions. Specifically, our framework dynamically maintains a variant of the hierarchical $j$-tree decomposition of [Madry FOCS'10], achieving a poly-logarithmic approximation factor to the graph's cut structure and supporting edge updates in $O(n^ε)$ amortized update time, for any arbitrarily small constant $ε\in (0,1)$.
  Consequently, we obtain new trade-offs between approximation and update/query time for fundamental cut-based optimization problems in the fully dynamic setting, including all-pairs minimum cuts, sparsest cut, multi-way cut, and multi-cut. For the last three problems, these trade-offs give the first fully-dynamic algorithms achieving poly-logarithmic approximation in sub-linear time per operation.
  The main technical ingredient behind our dynamic hierarchy is a dynamic cut-sparsifier algorithm that can handle vertex splits with low recourse. This is achieved by white-boxing the dynamic cut sparsifier construction of [Abraham et al. FOCS'16], based on forest packing, together with new structural insights about the maintenance of these forests under vertex splits. Given the versatility of cut sparsification in both the static and dynamic graph algorithms literature, we believe this construction may be of independent interest.

</details>


### [151] [Computational Complexity of Swish](https://arxiv.org/abs/2601.09289)
*Takashi Horiyama,Takehiro Ito,Jun Kawahara,Shin-ichi Minato,Akira Suzuki,Ryuhei Uehara,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文证明Swish游戏在每张卡两个符号时为NP完全问题，填补了之前的研究空白，并提供了无变换时的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决Swish游戏中每张卡两个符号情况的计算复杂性，填补现有研究的空白。

Method: 通过限制卡片的变换（水平或垂直翻转、180度旋转）证明NP难解性，并扩展到允许所有三种变换的原始设置。同时，展示了在无变换情况下的多项式时间算法。

Result: 证明Swish在每张卡两个符号且允许变换时为NP完全问题，而在无变换时存在多项式时间算法。

Conclusion: 本文建立了Swish游戏在不同符号数量和允许变换下的完整计算复杂性特征，特别是解决了每张卡两个符号的开放性问题，证明其为NP完全问题。

Abstract: Swish is a card game in which players are given cards having symbols (hoops and balls), and find a valid superposition of cards, called a "swish." Dailly, Lafourcade, and Marcadet (FUN 2024) studied a generalized version of Swish and showed that the problem is solvable in polynomial time with one symbol per card, while it is NP-complete with three or more symbols per card. In this paper, we resolve the previously open case of two symbols per card, which corresponds to the original game. We show that Swish is NP-complete for this case. Specifically, we prove the NP-hardness when the allowed transformations of cards are restricted to a single (horizontal or vertical) flip or 180-degree rotation, and extend the results to the original setting allowing all three transformations. In contrast, when neither transformation is allowed, we present a polynomial-time algorithm. Combining known and our results, we establish a complete characterization of the computational complexity of Swish with respect to both the number of symbols per card and the allowed transformations.

</details>


### [152] [Engineering Compressed Matrix Multiplication with the Fast Walsh-Hadamard Transform](https://arxiv.org/abs/2601.09477)
*Joel Andersson,Matti Karppa*

Main category: cs.DS

TL;DR: The paper presents a FWHT-based implementation of Pagh's compressed matrix multiplication algorithm, showing it outperforms DGEMM in sparse or magnitude-skewed matrices, with up to 40x speedup.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a practical and high-performance solution for matrix multiplication, especially when the product matrix is sparse or dominated by a small subset of large elements.

Method: The method involves implementing Pagh's compressed matrix multiplication algorithm, leveraging fast polynomial multiplication via the FFT, and replacing the FFT with the Fast Walsh-Hadamard Transform (FWHT) for sketched multiplication.

Result: Empirical results show that the FWHT variant is up to 4 times faster than the FFT-based version and achieves a speedup of up to 40 over DGEMM from Intel MKL under favorable conditions.

Conclusion: The paper concludes that the FWHT-based implementation of Pagh's compressed matrix multiplication algorithm is practical, outperforming state-of-the-art DGEMM implementations under specific conditions, and is released as free software with NumPy-compatible Python bindings.

Abstract: We present an implementation of Pagh's compressed matrix multiplication algorithm, a randomized algorithm that constructs sketches of matrices to compute an unbiased estimate of their product. By leveraging fast polynomial multiplication via the FFT, the algorithm achieves high performance when the product matrix is sparse or contains only a small number of entries with magnitudes significantly larger than the rest. We show empirically that the algorithm is practical and can outperform state-of-the-art DGEMM implementations when the product matrix has few nonzero entries or is otherwise dominated by a small subset of elements with large magnitude. As a minor theoretical contribution, we replace the FFT with the Fast Walsh-Hadamard Transform (FWHT) in sketched multiplication, preserving all correctness and variance guarantees of the original algorithm.
  Experiments with our carefully engineered multithreaded CPU implementation for dense double-precision matrices on 64-core CPU nodes across a range of synthetic benchmarks, exhibiting variable sparsity patterns, show that the FWHT variant is up to 4 times faster than the FFT-based version. Under favorable sparsity and magnitude patterns in the product matrix, our FWHT-based implementation achieves a speedup of up to 40 over DGEMM from Intel MKL, with low probability of error in the estimates. Our implementation is released as free software and comes with NumPy-compatible Python bindings.

</details>


### [153] [How many users have been here for a long time? Efficient solutions for counting long aggregated visits](https://arxiv.org/abs/2601.09489)
*Peyman Afshani,Rezaul Chowdhury,Inge Li Gørtz,Mayank Goswami,Francesco Silvestri,Mariafiore Tognon*

Main category: cs.DS

TL;DR: 本文解决了Counting Long Aggregated Visits问题，提出了精确和近似数据结构，适用于大规模移动数据集分析，并在几何设置中优化了性能。


<details>
  <summary>Details</summary>
Motivation: 该问题源于大规模移动数据集分析中的查询需求，旨在统计在查询区域内聚合访问时间超过阈值$k$的不同用户数量。

Method: 本文描述了基于空间-时间权衡的精确数据结构，以及基于采样和草图技术的近似解决方案。在几何设置中，区域为$\mathbb{R}^d$中的点，查询为超矩形，提出了性能更优的精确数据结构。

Result: 提出了多种数据结构和算法，包括精确和近似解决方案，并在几何设置中实现了性能提升。

Conclusion: 本文提出了针对Counting Long Aggregated Visits问题的多种精确和近似数据结构，并探讨了条件和无条件下限。通过空间-时间权衡的精确数据结构及基于采样和草图技术的近似解决方案，为大规模移动数据集分析提供了高效支持。

Abstract: This paper addresses the Counting Long Aggregated Visits problem, which is defined as follows. We are given $n$ users and $m$ regions, where each user spends some time visiting some regions. For a parameter $k$ and a query consisting of a subset of $r$ regions, the task is to count the number of distinct users whose aggregate time spent visiting the query regions is at least $k$. This problem is motivated by queries arising in the analysis of large-scale mobility datasets. We present several exact and approximate data structures for supporting counting long aggregated visits, as well as conditional and unconditional lower bounds. First, we describe an exact data structure that exhibits a space-time tradeoff, as well as efficient approximate solutions based on sampling and sketching techniques. We then study the problem in geometric settings where regions are points in $\mathbb{R}^d$ and queries are hyperrectangles, and derive exact data structures that achieve improved performance in these structured spaces.

</details>


### [154] [Permutation Matching Under Parikh Budgets: Linear-Time Detection, Packing, and Disjoint Selection](https://arxiv.org/abs/2601.09577)
*MD Nazmul Alam Shanto,Md. Tanzeem Rahat,Md. Manzurul Hasan*

Main category: cs.DS

TL;DR: 论文提出了一种高效的滑动窗口框架，用于解决排列模式匹配及其优化变体问题，所有算法均在线性时间内完成。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决排列模式匹配中的优化和打包变体问题，超越传统的检测任务，满足实际应用需求。

Method: 论文采用滑动窗口框架维护模式P与当前文本窗口的Parikh向量差，实现了排列匹配的高效算法。对于MFSP问题，使用双指针可行性维护算法；对于非重叠匹配选择，采用贪心最早结束策略。

Result: 论文证明了排列匹配、MFSP和非重叠匹配选择问题均可在线性时间内解决，并提供了紧致的算法界限。

Conclusion: 该论文提出了一个统一的滑动窗口框架，用于解决排列（混乱/阿贝尔）模式匹配问题，并在O(n + σ)时间和O(σ)空间内实现。此外，论文还引入了组合优化变体MFSP，证明其同样可在O(n + σ)时间内解决，并通过贪心策略解决了非重叠匹配选择问题。

Abstract: We study permutation (jumbled/Abelian) pattern matching over a general alphabet $Σ$. Given a pattern P of length m and a text T of length n, the classical task is to decide whether T contains a length-m substring whose Parikh vector equals that of P . While this existence problem admits a linear-time sliding-window solution, many practical applications require optimization and packing variants beyond mere detection. We present a unified sliding-window framework based on maintaining the Parikh-vector difference between P and the current window of T , enabling permutation matching in O(n + σ) time and O(σ) space, where σ = |Σ|. Building on this foundation, we introduce a combinatorial-optimization variant that we call Maximum Feasible Substring under Pattern Supply (MFSP): find the longest substring S of T whose symbol counts are component-wise bounded by those of P . We show that MFSP can also be solved in O(n + σ) time via a two-pointer feasibility maintenance algorithm, providing an exact packing interpretation of P as a resource budget. Finally, we address non-overlapping occurrence selection by modeling each permutation match as an equal-length interval and proving that a greedy earliest-finishing strategy yields a maximum-cardinality set of disjoint matches, computable in linear time once all matches are enumerated. Our results provide concise, provably correct algorithms with tight bounds, and connect frequency-based string matching to packing-style optimization primitives.

</details>
