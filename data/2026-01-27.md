<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AI](#cs.AI) [Total: 69]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.RO](#cs.RO) [Total: 32]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 提出一种无需训练的推理时颜色控制方法，结合多种技术实现精确颜色目标，适用于Stable Diffusion修复流程。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散系统中颜色控制的失败问题，特别是在设计导向的工作流程中需要满足用户指定的颜色目标。

Method: 采用ROI-based inpainting进行空间选择，background-latent re-imposition防止ROI外颜色漂移，latent nudging通过梯度引导结合CIE Lab和线性RGB的复合损失函数。

Result: 该方法不仅能满足平均颜色约束，还能通过分布感知目标避免局部感知显著失败，实现了精确的颜色控制。

Conclusion: 该方法通过结合ROI-based inpainting、background-latent re-imposition和latent nudging等技术，实现了无需额外训练的精确颜色控制，适用于标准Stable Diffusion修复流程。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [2] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS是一种移动场景建模范式，通过三个协同设计的操作符在移动设备上实现高效、高保真的3D高斯泼溅训练，解决了资源受限问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅（3DGS）方法在移动设备上受限于分钟级训练预算和硬件可用峰值内存，无法高效建模。PocketGS旨在在紧密耦合的约束下实现设备上的3DGS训练，同时保持高感知保真度。

Method: PocketGS通过三个操作符实现：G构建几何忠实的点云先验；I注入局部表面统计以种子各向异性高斯，减少早期条件差距；T通过缓存中间体和索引映射梯度散射展开alpha合成，实现稳定的移动反向传播。

Result: 实验表明，PocketGS能够超越主流工作站3DGS基线，提供高质量重建，实现完全在设备上的实用捕捉到渲染工作流程。

Conclusion: PocketGS通过三个协同设计的操作符（G、I、T）解决了标准3DGS在移动设备上的训练效率、内存紧凑性和建模保真度之间的矛盾，实现了高质量的场景重建，并支持完全在设备上的从捕捉到渲染的实用工作流程。

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [3] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出一种新方法，将B-Reps转化为k-cell粒子集，通过多模态流匹配框架实现拓扑与几何的联合生成，显著提升CAD模型的质量和编辑能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理B-Reps的异质性和层级结构时，无法充分利用几何关系（如邻接和共享），限制了上下文感知和错误恢复能力。

Method: 采用多模态流匹配框架，将B-Reps分解为k-cell粒子集，并通过共享潜在空间实现几何耦合。

Result: 实验表明，该方法在无条件生成和条件任务（如单视图或点云重建）中均能生成高保真CAD模型，且在局部修复和非流形结构合成中表现优异。

Conclusion: 该方法通过将B-Reps重新表述为组合的k-cell粒子集，成功解决了传统方法在生成建模中的局限性，显著提升了CAD模型的有效性和可编辑性。

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [4] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 研究科学图像合成的生成范式、评估和下游应用，提出逻辑驱动的ImgCoder框架和SciGenBench评估标准，验证高保真合成图像对多模态推理的提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像（T2I）模型在生成科学图像时视觉逻辑分歧的问题，提升科学图像合成的严谨性和下游推理价值。

Method: 提出了ImgCoder框架，采用逻辑驱动的“理解-规划-编码”工作流程以提高结构精确性，并引入SciGenBench评估生成图像的信息效用和逻辑有效性。

Result: 评估揭示了基于像素的模型的系统性失败模式，并强调了表达能力与精确性之间的权衡。微调LMMs在合成科学图像上显示出与文本领域类似的扩展趋势。

Conclusion: 高保真科学图像合成是解锁大规模多模态推理能力的可行路径，通过微调大型多模态模型（LMMs）在严格验证的合成科学图像上，可以获得持续的推理提升。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [5] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于隐式表示的缝制图案建模方法，通过潜在空间编码和流匹配模型，实现了复杂图案的生成与图像估计，提升了数字时尚设计的效率。


<details>
  <summary>Details</summary>
Motivation: 由于面板几何形状和接缝排列的广泛变异性，准确建模缝制图案仍然具有挑战性。本研究旨在解决这一问题，为时尚设计、制作和物理模拟等应用提供更高效的解决方案。

Method: 采用隐式表示方法，将每个面板表示为定义其边界的符号距离场和标识接缝端点的无符号距离场，并将这些场编码到连续潜在空间中，实现可微分网格化。使用潜在流匹配模型学习面板组合的分布，并通过缝合预测模块从提取的边缘段恢复接缝关系。

Result: 该方法能够准确建模和生成具有复杂结构的缝制图案，并在从图像估计缝制图案方面表现出优于现有方法的精度。此外，它还支持图案完成和重新适配等应用。

Conclusion: 该研究提出了一种基于隐式表示的缝制图案建模方法，通过连续潜在空间和潜在流匹配模型，实现了复杂结构缝制图案的准确建模与生成，为数字时尚设计提供了实用工具。

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [6] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 论文提出一种双增强框架，结合空间流形扩展和语义对象注入，通过INR和Sim2Real技术提升医学图像分割的数据效率，实验证明其显著增强模型性能。


<details>
  <summary>Details</summary>
Motivation: 针对复杂病理（如脑膜瘤）的精确分割需求，模型需充分利用有限高质量标注中的潜在信息，以最大化现有数据集的价值。

Method: 利用隐式神经表示（INR）建模连续速度场，并通过线性混合变形场在变形空间内插值，高效生成解剖学上合理的变体。此外，引入Sim2Real病变注入模块，将病变纹理移植到健康解剖背景中，构建高保真模拟域。

Result: 在混合数据集上的综合实验表明，该框架显著提升了包括nnU-Net和U-Mamba在内的先进模型的数据效率和鲁棒性。

Conclusion: 该论文提出的双增强框架通过空间流形扩展和语义对象注入的协同整合，显著提升了医学图像分割的数据效率和模型鲁棒性，为有限标注预算下的高性能医学图像分析提供了有效策略。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [7] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 论文提出MV-S2V任务，通过多视角参考图像生成视频，引入合成数据生成流程和TS-RoPE技术，实现了3D主题一致性和高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有的S2V方法仅限于单视角主题参考，未能充分利用视频主题控制的潜力。因此，作者提出并解决了更具挑战性的多视角S2V（MV-S2V）任务，以强制实现3D级别的主题一致性。

Method: 开发了一个合成数据生成流程来生成高度定制的合成数据，并辅以小规模真实世界捕获的数据集来增强MV-S2V的训练。此外，引入了Temporally Shifted RoPE（TS-RoPE）来区分参考条件中的跨主题和跨视角。

Result: 该框架在多视角参考图像下实现了卓越的3D主题一致性和高质量的视觉输出。

Conclusion: 该论文提出的MV-S2V框架在多视角参考图像下实现了卓越的3D主题一致性，并生成了高质量的视觉输出，为主题驱动视频生成开辟了新方向。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [8] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 提出一种自动化方法，通过图像分析分类红细胞变形，优于现有技术，适合临床诊断。


<details>
  <summary>Details</summary>
Motivation: 传统显微镜观察红细胞变形耗时且主观误差率高，需要自动化方法来提高效率和准确性。

Method: 使用Chan-Vese主动轮廓模型分割图像中的对象，并通过基本形状分析描述符（CSF和ESF）对红细胞进行分类。

Result: 实验结果显示，该方法在F-measure值（正常细胞0.97， elongated细胞0.95）和整体多类性能指标上优于现有方法。

Conclusion: 该方法在镰状细胞贫血的诊断中表现优于现有技术，适合临床治疗和诊断支持。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [9] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: PPISP通过物理基础的ISP校正模块解决多视角3D重建中的光度不一致问题，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多视角3D重建方法对由相机光学特性和图像信号处理（ISP）变化引起的光度不一致高度敏感，现有缓解策略缺乏物理基础且对新视角泛化能力差。

Method: 提出了物理上合理的ISP（PPISP）校正模块，通过基于物理和可解释的变换来解耦相机固有和捕获依赖的效应。PPISP控制器在输入视图上训练，预测新视角的ISP参数。

Result: PPISP在标准基准测试中达到了最先进的性能，支持直观控制并在可用时整合元数据。

Conclusion: PPISP模块通过物理基础和可解释的变换，有效解决了多视角3D重建中的光度不一致问题，并在标准基准测试中达到了最先进的性能。

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [10] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 研究通过新基准AMVICC评估MLLMs和IGMs的视觉推理局限性，发现失败模式有共享和特定性，IGMs在细粒度视觉属性控制上表现不佳，为未来跨模态对齐研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习快速发展，视觉语言模型（VLMs）仍无法理解或生成基本的视觉概念（如物体方向、数量或空间关系），这凸显了基础视觉推理的不足。

Method: 通过将MMVP基准问题转化为显式和隐式提示，创建了AMVICC基准，用于分析不同模态的失败模式。测试了11个MLLMs和3个IGMs在九类视觉推理任务中的表现。

Result: 结果显示失败模式在模型和模态之间常常共享，但某些失败是模型或模态特定的，可能归因于多种因素。IGMs在响应提示时难以操控特定视觉组件，尤其是在显式提示中。

Conclusion: 该研究为未来跨模态对齐研究奠定了基础，提供了一个框架来探究生成和解释失败是否源于共享的局限性，以指导未来统一视觉语言建模的改进。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [11] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: GimmBO利用两阶段贝叶斯优化改进扩散模型适配器合并的交互探索，显著提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于手动滑块调整的适配器合并方法扩展性差且权重选择困难，无法高效探索庞大的设计空间。

Method: 提出GimmBO框架，利用偏好贝叶斯优化（PBO）进行交互式探索，并设计两阶段BO后端以应对高维空间的挑战。

Result: 实验表明GimmBO在收敛性、成功率上优于基线方法，并通过扩展展示了框架的灵活性。

Conclusion: GimmBO通过引入两阶段贝叶斯优化后端，显著提高了在高维空间中采样效率和收敛性，优于传统贝叶斯优化和线性搜索基线，并通过模拟和用户研究验证了其有效性。

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [12] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 研究提出混合视觉管道，结合Xception特征和简单ML分类器，实现高效C&D碎片分类，准确率高达99.5%，适用于现场部署。


<details>
  <summary>Details</summary>
Motivation: 建筑业产生大量碎片，有效分类对可持续废物管理和资源回收至关重要。传统方法效率低，需自动化解决方案。

Method: 研究采用预训练的Xception网络提取深度特征，并系统评估了多种机器学习分类器（如SVM、kNN、Bagged Trees、LDA和Logistic回归）。数据集包含1,800张平衡的高质量图像，涵盖陶瓷/瓷砖、混凝土、垃圾/废物和木材四类材料。

Result: 混合管道（Xception特征与简单分类器如线性SVM、kNN和Bagged Trees结合）实现了最高99.5%的准确率和宏F1分数，性能优于复杂深度学习模型。

Conclusion: 该研究提出了一种混合视觉管道，结合深度特征提取和传统机器学习分类器，用于自动化建筑和拆除（C&D）碎片分类。该方法在准确性和效率上优于复杂的端到端深度学习模型，为现场部署的碎片识别提供了实用解决方案，并展望了未来与机器人和现场自动化系统的集成。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [13] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: MANGO是一个全球性的大规模红树林数据集，解决了现有数据集的局限性，并通过基准测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有红树林检测数据集存在覆盖范围有限、数据不公开或缺乏单日图像-掩膜对等问题，制约了深度学习在红树林监测中的应用。

Method: 通过检索2020年红树林区域的Sentinel-2影像，采用目标检测驱动的方法选择最佳单日观测数据，并与红树林年度掩膜对齐，构建了包含42,703个标记图像-掩膜对的数据集。

Result: MANGO数据集覆盖124个国家，提供了大规模、全球性的红树林图像-掩膜对，并通过基准测试展示了其在多种语义分割架构下的适用性。

Conclusion: MANGO数据集为全球红树林监测提供了一个大规模、公开可用的资源，并通过基准测试验证了其在多种语义分割架构下的有效性，为全球红树林保护提供了可靠的技术支持。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [14] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 提出了一种结合布局分析和OCR的流程，用于转录保留特殊符号的历史文献，并在多种文本类型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 历史文献中的特殊符号和字符具有独特意义，需要保留其原始风格和重要性，因此需要一种能够处理这些特征的转录方法。

Method: 扩展现有的文本行识别方法，结合布局分析模型和OCR模型，形成完整的转录流程。

Result: 在多个数据集上验证了该流程的有效性，能够处理手写、印刷及多语言文本。

Conclusion: 该论文提出的流程通过结合布局分析模型和OCR模型，有效解决了历史文献转录中的特殊符号和字符保留问题，并在多种数据集上验证了其高效性。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [15] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 该研究探索了结合Leap Motion和RGB摄像头的多模态方法在阿拉伯手语识别中的潜力，系统架构采用并行子网络处理不同模态数据，最终在自定义数据集上达到78%的准确率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯手语（ArSL）是聋人和听力障碍社区的重要沟通方式，但现有识别系统因依赖单一传感器（如Leap Motion或RGB摄像头）而面临复杂手部方向跟踪不足和3D手部动作识别不精确等挑战。

Method: 系统架构包括两个并行子网络：一个用于Leap Motion数据的自定义密集神经网络，包含dropout和L2正则化；另一个是基于微调VGG16模型的图像子网络，通过数据增强技术优化。两种模态的特征表示在融合模型中拼接，并通过全连接层传递，最终通过SoftMax激活进行分类。

Result: 系统在包含18个ArSL单词的自定义数据集上评估，正确识别了13个单词，总体准确率为78%。

Conclusion: 该研究初步验证了多模态融合方法在阿拉伯手语识别中的可行性，并指出了进一步优化和数据集扩展的方向。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [16] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: DMST通过解耦成员矩阵和子空间矩阵，优化MCR2目标，显著提升视觉模型的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有设计中，MCR2的成员矩阵与子空间矩阵U的紧密耦合导致错误令牌投影下的冗余编码，需解耦二者功能关系以提高效率。

Method: 提出直接学习成员矩阵，并从全空间S中推导稀疏子空间，通过梯度展开优化MCR2目标，得到可解释的稀疏线性注意力算子DMSA。

Result: 在ToST中用DMSA替换注意力模块（称为DMST），不仅编码缩减率更快，且在ImageNet-1K上的top-1准确率提升1.08%-1.45%。

Conclusion: DMST通过解耦MCR2中的成员矩阵和子空间矩阵，显著提高了视觉任务的效率和可解释性，并在ImageNet-1K数据集上表现出更高的准确率。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [17] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的语义分割方法，从噪声严重的TEM图像中提取3D原子信息，实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传输电子显微镜（TEM）图像中因严重噪声而难以提取3D原子级信息的问题。

Method: 将深度估计问题转化为语义分割问题，并利用受合成噪声污染的模拟数据训练深度卷积神经网络生成像素级深度分割图。

Result: 在模拟图像和真实TEM数据中成功估算了CeO2纳米颗粒原子柱的深度，实验结果显示了方法的准确性和鲁棒性。

Conclusion: 该方法通过深度卷积神经网络成功地从噪声严重的TEM图像中提取了3D原子级信息，证明了其准确性、校准性和对噪声的鲁棒性。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [18] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: Noisomics通过对比预训练模型CoP将噪声解码为信息源，仅需少量样本即可超越传统方法，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代传感器将物理信号与复杂算法伪影纠缠在一起，传统方法难以分离这些因素，且需要大量监督数据。

Method: 利用流形假设和合成噪声基因组，CoP采用对比学习分离语义信号与随机扰动，打破了传统深度学习的缩放规律。

Result: CoP仅需100个训练样本即可超越传统方法在10万个样本上的表现，数据依赖性和计算需求降低了三个数量级。在12个不同领域的数据集上，零样本泛化能力显著，估计误差减少63.8%，决定系数提高85.1%。

Conclusion: Noisomics框架通过对比预训练基础模型（CoP）将噪声从干扰转变为信息源，显著降低了数据依赖性和计算需求，并在多个领域展示了强大的零样本泛化能力。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [19] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: VLM引导的框架通过语义先验加速并稳定了射频材料参数估计，显著提升了收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 在6G系统中，精确的射频材料参数对电磁数字孪生至关重要，但基于梯度的逆向射线追踪（RT）对初始化敏感且在有限测量下成本高昂。

Method: 提出了一种视觉语言模型（VLM）引导的框架，通过解析场景图像推断材料类别，并通过ITU-R材料表映射到定量先验，提供有信息的电导率初始化和优化的发射器/接收器布局。

Result: 实验表明，与均匀或随机初始化和随机布局基线相比，该方法实现了2-4倍的收敛速度提升和10-100倍的最终参数误差降低，仅需少量接收器即可达到低于0.1%的平均相对误差。

Conclusion: 语义先验从视觉语言模型（VLM）中有效指导基于物理的优化，实现了快速且可靠的射频材料参数估计。

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [20] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: SiMiC是一种基于深度学习的硅微结构表征方法，通过注意力机制CNN从SEM图像中自动提取特征，提高了分析效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统SEM分析需要大量人工操作，限制了通量和可重复性，因此需要一种更高效、一致的方法来表征硅微结构。

Method: 利用基于注意力机制的卷积神经网络（CNN）从SEM图像中高效提取形态特征，如大小、形状和顶端曲率。

Result: SiMiC在保持可解释性的同时实现了高精度，显著减少了人工干预并提高了测量一致性。

Conclusion: 该研究提出的SiMiC框架为数据驱动的微结构分析奠定了基础，直接关联场发射性能，为优化冷阴极和SEM电子源的设计提供了新途径。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [21] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: 该论文介绍了ISAS 2025的异常行为识别挑战赛，旨在通过姿态数据自动化识别发育障碍个体的异常行为。40个团队采用多种方法参与，结果凸显了在低维数据中建模罕见行为的困难。


<details>
  <summary>Details</summary>
Motivation: 挑战赛旨在解决在发育障碍个体设施中自动化识别异常行为的迫切需求，使用非侵入式的姿态估计数据。

Method: 参赛团队采用了从经典机器学习到深度学习架构的多种方法，基于从模拟场景视频中提取的骨架关键点来区分正常和异常活动。评估采用了Leave-One-Subject-Out (LOSO)策略以确保主体无关的泛化能力。

Result: 40个团队参与了挑战赛，提交的作品主要使用宏平均F1分数来评估，以应对类别不平衡问题。结果显示了在低维数据中建模罕见行为的挑战。

Conclusion: 该挑战赛强调了在嘈杂、低维数据中建模罕见、突发行为的困难，并突出了在行为建模中捕捉时间和上下文细微差别的重要性。这些见解可能有助于未来在医疗保健和行为监测领域开发具有社会责任感的AI应用。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [22] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: SP-VLM是一种新型单像素视觉-语言模型，通过低维数据保护隐私，同时实现行为监测，适用于隐私敏感环境。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感环境中（如洗手间、更衣室），传统监控因隐私法规和伦理问题受限，需要一种既能保护隐私又能监测异常行为的解决方案。

Method: 通过单像素模态捕获人类动态，并结合视觉-语言集成推断复杂行为模式。

Result: SP-VLM在低采样率下抑制身份可识别性，同时仍能提取有意义的行为语义，实现异常检测、人数统计和活动理解。

Conclusion: SP-VLM框架为隐私敏感环境提供了一种人权对齐的安全监控路径，既能支持及时干预，又避免了侵入式监控的常态化。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [23] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: 研究开发了一种稳健的人体活动识别系统，用于髋部骨折康复中的连续体育活动监测，通过合成数据辅助显著提高了姿势转换检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 髋部骨折康复期间的体育活动对减轻老年患者长期功能衰退至关重要，但在临床实践中很少量化。现有的商用可穿戴活动追踪系统通常针对中年人开发，因此在步态较慢且多变的老年人中表现不可靠。

Method: 24名80岁以上的健康老年人被纳入研究，在模拟自由生活条件下进行75分钟的日常活动（行走、站立、坐、躺下和姿势转换），同时佩戴位于下背部和前大腿的两个加速度计。模型稳健性通过留一受试者交叉验证进行评估。

Result: 通过合成数据辅助的特征干预模型（FIM）实现了可靠的活动识别，平均F1分数分别为行走0.896、站立0.927、坐0.997、躺下0.937和姿势转换0.816。与未使用合成数据的对照模型相比，FIM显著提高了姿势转换检测。

Conclusion: 初步结果表明，在老年人中实现稳健的活动识别是可行的。需要在髋部骨折患者群体中进一步验证，以评估所提出的监测系统的临床实用性。

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [24] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 论文提出Ego4OOD基准和轻量网络，有效应对自中心视频动作识别中的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决自中心视频动作识别中由于领域偏移带来的挑战，如类内时空变异性大、长尾特征分布及动作与环境强相关性。

Method: 提出了一种基于聚类的协变量偏移度量方法，并采用一对多的二元训练目标，将多类动作识别分解为独立的二元分类任务。

Result: 轻量级的两层全连接网络在Argo1M和Ego4OOD上表现与最先进方法相当，且参数更少、无需额外模态。

Conclusion: 论文通过引入Ego4OOD基准和一种轻量级的全连接网络，展示了在自中心视频动作识别中，控制协变量偏移和定量领域特征对研究分布外泛化的重要性。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [25] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 提出了一种基于计算机视觉的自动化子弹孔检测和迭代跟踪系统，显著提高了步枪瞄准的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的步枪瞄准过程依赖人工检查，存在效率低下和人为错误的风险，因此需要一种自动化的计算机视觉系统来提高准确性和效率。

Method: 结合YOLOv8进行小物体检测和IoU分析，提出了一种新颖的数据增强技术，通过移除而非添加对象来模拟真实射击序列，并引入了基于ORB的预处理流程以标准化目标方向。

Result: 系统在子弹孔检测上达到97.0%的平均精度，射击迭代分配准确率为88.8%。

Conclusion: 该系统在子弹孔检测和射击迭代分配方面表现出色，不仅提高了步枪瞄准的效率和准确性，还具有在需要时间区分视觉相似对象的其他领域中广泛应用的潜力。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [26] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新分类法，通过状态构建和动态建模填补视频生成模型与世界模型理论的差距，并呼吁转向功能基准测试以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 填补当代“无状态”视频架构与经典以状态为中心的世界模型理论之间的差距。

Method: 提出了一种以状态构建和动态建模为核心的新分类法，将状态构建分为隐式范式（上下文管理）和显式范式（潜在压缩），并通过知识整合和架构重构分析动态建模。

Result: 倡导从视觉保真度转向功能基准测试，测试物理持久性和因果推理。

Conclusion: 通过解决数据驱动的记忆和压缩保真度以及潜在因素解耦与推理先验整合等挑战，该领域可以从生成视觉上合理的视频发展为构建稳健、通用的世界模拟器。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [27] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: 提出基于最优传输的两级图像分割方法，通过超像素聚类和Wasserstein距离合并，提升不均匀图像的分割效果与效率。


<details>
  <summary>Details</summary>
Motivation: 针对传统基于平均颜色距离的超像素合并策略在强不均匀性图像分割中的局限性，提出一种基于分布最优传输距离的统一数学框架。

Method: 该方法采用两级聚类：首先通过线性最小二乘分配问题将像素分组为超像素，可视为离散最优传输问题的特例；随后使用平方2-Wasserstein距离贪婪合并超像素为对象级分割。

Result: 数值实验表明，该方法在挑战性图像上实现了更高的分割准确性，同时保持了高计算效率。

Conclusion: 该论文提出的基于最优传输（OT）距离的两级聚类方法在保持高计算效率的同时，显著提升了具有强不均匀性图像的分割准确性。

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [28] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: GlassesGB结合3D高斯混合形状和2D生成技术，支持3D头部虚拟形象的定制化眼镜生成，解决了VR中个性化眼镜设计的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试戴系统大多仅支持预定义的眼镜模板，缺乏细粒度的用户驱动定制能力，而GlassesGAN仅限于2D图像生成。

Method: 结合3D高斯混合形状和2D生成技术，提出了GlassesGB框架，支持3D头部虚拟形象的定制化眼镜生成。

Result: GlassesGB有效解决了2D生成定制与3D头部虚拟形象渲染之间的挑战，实现了VR应用中的个性化眼镜设计。

Conclusion: GlassesGB框架成功地将2D生成定制与3D头部虚拟形象渲染相结合，为VR应用中的个性化眼镜设计提供了解决方案。

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [29] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GRASP是一种参数高效的微调策略，通过空间结构化软提示和问题引导的稀疏融合，有效提升MLLMs在遥感视觉问答任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于遥感图像的大尺度变化、稀疏目标分布和复杂区域语义特征，现有的微调方法容易过拟合背景噪声或忽略目标细节，限制了MLLMs在遥感任务中的有效性。

Method: GRASP通过问题引导的稀疏融合机制，动态地将任务特定上下文聚合成紧凑的全局提示，使模型能够专注于相关区域并过滤背景噪声。

Result: GRASP在多个RSVQA基准测试中取得了与现有方法竞争的性能，同时保持了高参数效率。

Conclusion: GRASP方法在保持高参数效率的同时，在多个RSVQA基准测试中表现出色，优于现有的微调和基于提示的方法。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [30] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 提出了一种基于生成式AI的自动LoD草图提取框架，解决了传统手动建模的低效问题，实验验证了其在几何一致性和渐进简化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统LoD建模过程依赖手动操作，耗时耗力且易产生几何不一致。生成式AI的快速发展为从草图输入生成多级建筑模型提供了新可能，但缺乏高质量配对LoD训练数据限制了其应用。

Method: 结合计算机视觉技术和生成式AI方法，建立了一个从详细表示到体积抽象的渐进提取流程。

Result: 实验结果表明，该方法在LoD级别间保持了强几何一致性，SSIM值分别为0.7319（LoD3到LoD2）和0.7532（LoD2到LoD1），归一化Hausdorff距离分别为图像对角线的25.1%和61.0%。

Conclusion: 该框架通过生成式AI模型自动提取多级细节（LoD）草图，有效保持了几何一致性，为AI驱动的多级建筑生成和分层建模提供了可靠的数据和技术支持。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [31] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 本研究填补了医学影像AI性能不确定性量化中置信区间方法行为的空白，通过大规模分析揭示了关键影响因素，为未来指南制定奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI的性能不确定性量化对于可靠验证和临床转化至关重要，但社区对置信区间方法的多样性和行为了解不足。

Method: 通过大规模实证分析，涵盖24个分割和分类任务，每个任务组使用19个训练模型，广泛采用多种性能指标、聚合策略和置信区间方法。

Result: 研究揭示了五个主要发现，包括样本量需求、性能指标选择、聚合策略、机器学习问题类型以及不同置信区间方法的可靠性和精确性差异。

Conclusion: 本研究为医学影像AI性能不确定性报告的指南制定提供了关键组成部分，揭示了不同置信区间方法在不同应用场景下的可靠性和精确性差异。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [32] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark是一种隐蔽且无害的黑盒医学分割模型所有权验证方法，通过调节模型不确定性和QR码水印实现高效验证，实验证明其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 医学数据标注成本高且受限于专家资源，隐私和伦理问题进一步加剧了挑战。现有的模型保护技术主要关注分类和生成任务，而医学图像分析中关键的分割模型保护研究不足。

Method: 提出了一种新颖的StealthMark方法，通过微妙调节模型不确定性而不改变最终分割输出，结合模型无关的解释方法（如LIME）提取特征归因，并在特定触发条件下显示可验证的水印。水印设计为QR码以增强所有权声明的可识别性。

Result: 在四个医学影像数据集和五个主流分割模型上的实验表明，StealthMark在保持原始模型分割性能的同时，所有权验证效果显著（如SAM模型上ASR超过95%，Dice和AUC分数下降小于1%）。

Conclusion: StealthMark是一种有效、隐蔽且无害的方法，用于在黑盒条件下验证医学分割模型的所有权，显著优于基于后门的水印方法，并展示了实际部署的强大潜力。

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [33] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: iFSQ通过替换激活函数优化FSQ量化，揭示离散与连续表示的最佳平衡点为4比特/维度，并比较了AR与扩散模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 解决FSQ量化中因等间隔量化导致的激活崩溃问题，统一离散和连续表示的建模与基准测试。

Method: 提出iFSQ，通过替换原始FSQ的激活函数为分布匹配映射，确保均匀先验，从而优化量化效果。

Result: iFSQ在数学上保证了最优的bin利用率和重建精度，揭示了离散与连续表示的最佳平衡点，并展示了AR和扩散模型的性能差异。

Conclusion: iFSQ通过简单的激活函数替换解决了FSQ的量化问题，同时揭示了离散和连续表示之间的最佳平衡点约为每维度4比特，并展示了AR和扩散模型在不同阶段的性能特点。

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [34] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: UniRG框架通过强化学习优化医学影像报告生成，显著提升性能并在多机构泛化中表现优异，尤其在CXR报告生成中创下新SOTA。


<details>
  <summary>Details</summary>
Motivation: 前沿模型在自然语言文本理解和推理方面表现出色，但在多模态理解和推理（如生物医学等高价值垂直领域）仍存在显著能力差距。医学影像报告生成是一个典型例子。

Method: 通过强化学习作为统一机制，直接优化针对终端应用设计的评估指标，UniRG显著提升了监督微调的性能，并在不同机构和临床实践中实现了持久的泛化能力。

Result: 在公开可用的胸部X光（CXR）数据上训练的UniRG-CXR，在CXR报告生成中进行了全面评估，并在ReXrank基准测试中创下了新的SOTA成绩。

Conclusion: UniRG-CXR在权威的ReXrank基准测试中表现优异，显著超越了现有技术，展现了其在医学影像报告生成领域的强大潜力。

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [35] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: 提出一种结合全局和局部频率正则化的新方法，用于稀疏视图下的3D重建，并发布了一个多光谱温室数据集和开源基准测试包。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D高斯泼溅（3DGS）模型在稀疏视图条件下几何不稳定和细节丢失的关键限制。

Method: 整合全局和局部频率正则化的新方法，用于稳定几何结构并保留稀疏视图条件下的细节。

Result: 实验表明，该方法在自建的多光谱数据集及标准基准测试中均优于现有基线。

Conclusion: 提出的方法在稀疏视图条件下实现了更清晰、更稳定且光谱一致的3D重建，优于现有基线方法。数据集和代码已公开。

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [36] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: DUET数据集和运动学识别框架填补了隐私保护社交互动测量的方法学空白，通过多模态数据和迁移学习实现了跨环境泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中缺乏一致且隐私保护的社交互动表示和测量方法的问题，以支持社会资本理论预测的重要互动形式评估。

Method: 引入DUET数据集和基于运动学的识别框架，利用四种传感模态和三种建筑环境上下文，通过迁移学习架构直接从隐私保护的骨骼运动中推断交流功能。

Result: DUET捕捉了12种二元互动，涵盖所有五种运动功能，并通过基准测试量化了交流功能识别的难度，识别框架展示了功能的结构化聚类和表示质量与分类性能的强关联。

Conclusion: DUET数据集及其识别框架为隐私保护的社交互动测量提供了新方法，填补了领域方法学空白，并展示了其在跨环境和对象泛化能力上的潜力。

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [37] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: 论文提出了一种改进的滑动窗口粗粒度化方法，用于分析脑MRI数据，发现结构复杂度随年龄下降，尤其在粗尺度上表现显著，证明了该方法在预测生物年龄中的实用性。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是将结构复杂度分析扩展到三维信号，特别是脑磁共振成像（MRI），以捕捉体积数据的多尺度组织。

Method: 论文采用了滑动窗口粗粒度化方案，以提供更平滑的估计并在大尺度上提高鲁棒性，解决了传统基于块的粗粒度化方法在粗分辨率下因采样有限而不稳定的问题。

Result: 研究发现，结构复杂度随着年龄增长而系统性下降，且这种效应在更粗的尺度上最为明显。

Conclusion: 该论文的结论是，结构复杂度分析作为一种可靠的多尺度信号处理工具，适用于3D成像数据分析，并能有效预测基于脑MRI的生物年龄。

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [38] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 该论文提出了一种语义V2X框架，通过传输语义嵌入而非原始视频数据，显著降低通信开销并提升碰撞预测性能。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要实时碰撞预测，但传统方法因带宽和延迟限制不切实际，因此研究旨在通过语义通信解决这一问题。

Method: 提出了一个语义V2X框架，利用V-JEPA生成未来帧的时空语义嵌入，并通过轻量级注意力探测器和分类器解码预测碰撞。

Result: 实验结果显示，该框架在碰撞预测上F1分数提高了10%，同时传输需求比原始视频减少了四个数量级。

Conclusion: 该研究验证了语义V2X通信在智能交通系统中实现实时碰撞预测的潜力，显著降低了通信开销并保持了预测准确性。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [39] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: 提出一种半监督领域适应框架，通过扩散模型生成目标感知的合成图像，显著提升计算病理学中的领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的深度学习模型由于领域偏移难以在不同队列和机构间泛化，现有方法无法充分利用目标域的无标签数据或依赖可能扭曲组织结构的图像到图像转换。

Method: 提出了一种半监督领域适应（SSDA）框架，利用潜在扩散模型生成保留形态且目标感知的合成图像。

Result: 提出的方法在目标队列的保留测试集上将加权F1分数从0.611提高到0.706，宏F1分数从0.641提高到0.716。

Conclusion: 目标感知的基于扩散的合成数据增强为计算病理学中的领域泛化提供了一种有前景且有效的方法。

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [40] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4通过多教师蒸馏技术，提升了性能并新增功能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 通过统一的学生模型保留并提升多个教师模型的独特能力，同时优化计算效率和分辨率支持。

Method: 采用多教师蒸馏技术，结合SigLIP2、DINOv3和SAM3教师模型，训练了-SO400M和-H两种变体。

Result: C-RADIOv4在核心指标上有所改进，新增了SAM3模仿能力，支持多分辨率，并提供了高效的ViTDet选项。

Conclusion: C-RADIOv4模型家族通过多教师蒸馏技术，显著提升了关键下游任务的性能，同时保持了计算复杂度不变，并新增了SAM3模仿能力、多分辨率支持和ViTDet选项。

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [41] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: 开源桥梁损伤检测系统结合SAM3和隐私保护技术，高效完成损伤检测并保护区域信息。


<details>
  <summary>Details</summary>
Motivation: 日本法律规定每五年需对基础设施进行视觉检测，但现场拍摄的损伤图像常包含区域信息，可能引发公众焦虑。因此需在准确提取损伤特征的同时保护区域隐私。

Method: 采用Segment Anything Model (SAM) 3进行钢筋腐蚀检测，利用DBSCAN自动补全遗漏区域，通过高斯模糊处理保护施工标志区域，并应用四种预处理方法提升OCR精度。

Result: 系统通过GPU优化实现每张图像1.7秒的处理速度，技术栈包括SAM3、PyTorch、OpenCV等，实现了高效且隐私保护的桥梁检测。

Conclusion: 本文提出了一种具有区域隐私保护功能的开源桥梁损伤检测系统，有效平衡了损伤特征提取与区域信息保护的需求，并通过技术优化实现了高效的桥梁检测。

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [42] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: PEAfowl是一种增强感知的多视图VLA策略，通过改进空间推理和指令接地，显著提升了双手机器人在杂乱场景中的操作性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉-语言-动作模型在多视图特征融合和语言指令接地方面的不足，以提升双手机器人在杂乱场景中的操作稳定性。

Method: PEAfowl通过预测每个token的深度分布、执行可微3D提升和聚合局部跨视图邻居来形成几何基础、跨视图一致的表示，并采用Perceiver风格的文本感知读取来替代全局条件化。

Result: 在RoboTwin 2.0上，PEAfowl比最强基线提高了23.0个百分点的成功率。

Conclusion: PEAfowl在RoboTwin 2.0上显著提升了成功率，并通过真实机器人实验验证了其可靠的模拟到现实迁移能力。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [43] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: FineVAU提出新基准和FVScore指标，改进视频异常理解评估，揭示LVLM在细粒度和空间理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VAU评估方法无法充分捕捉LVLM响应的丰富性和视觉基础性，且过于依赖语言质量而非事实相关性。

Method: 通过结构化、全自动的流程构建FineW3数据集，并开发FVScore这一与人类感知对齐的评估指标。

Result: FVScore在人类评估中显示出优于现有方法的对齐性，LVLM在细粒度和空间理解上表现不足。

Conclusion: FineVAU提出了一个新的基准和评估指标FVScore，旨在更全面地理解视频中的异常事件，包括事件、参与实体和位置。实验揭示了LVLM在空间和细粒度时间理解上的局限性。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [44] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LingBot-Depth通过掩码深度建模和自动数据整理优化深度图，性能超越顶级RGB-D相机，并提供了RGB与深度模态的对齐表示。


<details>
  <summary>Details</summary>
Motivation: 深度传感器的不准确性可以被视为反映潜在几何模糊性的“掩码”信号。

Method: 提出了LingBot-Depth，一个通过掩码深度建模利用视觉上下文优化深度图的深度补全模型，并包含一个自动数据整理流程以实现可扩展训练。

Result: 模型在多个下游任务中表现优异，代码、检查点和3M RGB-深度对（包括2M真实数据和1M模拟数据）已向空间感知社区发布。

Conclusion: LingBot-Depth模型在深度精度和像素覆盖方面优于顶级RGB-D相机，并提供了RGB和深度模态之间的对齐潜在表示。

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [45] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: MinkUNeXt-VINE是一种轻量级深度学习方法，通过预处理和多损失策略，在葡萄园环境中实现了高效的地点识别，尤其适用于低成本LiDAR输入。


<details>
  <summary>Details</summary>
Motivation: 农业环境缺乏结构化特征和显著地标，使得移动机器人的地点识别任务具有挑战性。

Method: 提出了一种轻量级的深度学习方法MinkUNeXt-VINE，结合了预处理和Matryoshka Representation Learning多损失方法。

Result: 在多种评估案例和两个长期葡萄园数据集上，该方法展示了高效权衡输出和鲁棒性能。

Conclusion: MinkUNeXt-VINE 方法在葡萄园环境中表现出色，尤其在处理低成本、稀疏LiDAR输入和低维输出时，展现了高效性和鲁棒性。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [46] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: Cross360通过跨注意力架构整合局部与全局信息，显著提升360°深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理球形图像的全局连续性和局部一致性时存在困难，尤其是局部补丁特征缺乏全局感知，全局表示无法解决补丁边界特征提取的差异。

Method: 提出了Cross360架构，包括Cross Projection Feature Alignment模块和Progressive Feature Aggregation with Attention模块，利用切线投影和等距柱状投影特征进行信息整合。

Result: Cross360在多数基准数据集上显著优于现有方法，尤其是在完整360°图像可用的情况下。

Conclusion: Cross360通过创新的跨注意力架构，成功整合了局部和全局信息，显著提升了360°深度估计的准确性和全局一致性。

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [47] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: Fluxamba是一种轻量级拓扑感知架构，通过SFB实现各向异性信息流，显著提升地质线性特征分割性能，计算成本极低，适用于实时部署。


<details>
  <summary>Details</summary>
Motivation: 精确分割地质线性特征需要捕捉复杂各向异性拓扑中的长程依赖关系。现有State Space Models (SSMs)依赖刚性、轴对齐扫描轨迹，导致与曲线目标的拓扑不匹配，产生上下文碎片化和特征侵蚀。

Method: Fluxamba是一种轻量级架构，引入了拓扑感知的特征校正框架，核心设计是Structural Flux Block (SFB)，通过Anisotropic Structural Gate (ASG)和Prior-Modulated Flow (PMF)实现各向异性信息流。此外，还包含Hierarchical Spatial Regulator (HSR)和High-Fidelity Focus Unit (HFFU)以优化多尺度语义对齐和信噪比。

Result: Fluxamba在LROC-Lineament数据集上达到89.22%的F1分数和89.87%的mIoU，实时推理速度超过24 FPS，仅需3.4M参数和6.3G FLOPs，计算成本比重量级基线降低两个数量级。

Conclusion: Fluxamba在多种地质基准测试中建立了新的最先进水平，特别是在LROC-Lineament数据集上表现优异，实现了89.22%的F1分数和89.87%的mIoU。其轻量级架构显著降低了计算成本，并在分割保真度和部署可行性之间建立了新的Pareto前沿。

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [48] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: DMEF是一种动态元集成框架，通过自适应加权轻量级CNN模型，在边缘设备上高效实现高精度植物病害检测。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备（如物联网传感器、智能手机和嵌入式系统）因计算资源和能源预算有限而难以部署深度学习模型的问题。

Method: 提出了一种动态元集成框架（DMEF），通过自适应加权机制动态结合三个轻量级卷积神经网络（MobileNetV2、NASNetMobile和InceptionV3）的预测，优化精度提升（DeltaAcc）与计算效率（模型大小）之间的权衡。

Result: 在土豆和玉米病害的基准数据集上，分别实现了99.53%和96.61%的分类准确率，优于独立模型和静态集成方法，推理延迟小于75毫秒，模型参数少于100万。

Conclusion: DMEF通过动态加权机制在资源受限的边缘设备上实现了高精度的植物病害检测，为农业监测提供了可扩展的解决方案。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [49] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: ClinNet通过建模结构差异、稳定特征表示和联合估计分级与不确定性，显著提升KOA分级的准确性和可信度，并支持临床安全部署。


<details>
  <summary>Details</summary>
Motivation: 由于KOA分级在X光图像中存在细微的级间差异、标注不确定性及疾病进展的序数特性，传统深度学习方法将其视为确定性多分类问题，忽略了退化的连续性和专家标注的不确定性。

Method: ClinNet框架包含三个核心组件：(1) 双边不对称编码器（BAE）建模内外侧结构差异；(2) 诊断记忆库维护类别原型以稳定特征表示；(3) 基于NIG分布的序数回归头联合估计连续KL分级和认知不确定性。

Result: 实验表明，ClinNet的Quadratic Weighted Kappa达到0.892，准确率为0.768，显著优于现有基线（p < 0.001），且其不确定性估计能有效标记分布外样本和潜在误诊。

Conclusion: ClinNet通过结合双边不对称编码器、诊断记忆库和基于NIG分布的序数回归头，成功提升了KOA分级的准确性和可信度，其不确定性估计还能有效识别异常样本和潜在误诊，为临床安全部署铺平了道路。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [50] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3是一个基于扩散Transformer的多模态视频生成模型，支持三种生成范式，通过优化数据处理和训练策略，在多项指标上达到或接近最先进水平。


<details>
  <summary>Details</summary>
Motivation: 视频生成是构建世界模型的基础，多模态上下文推理是能力的核心测试。

Method: 基于扩散Transformer的统一多模态上下文学习框架，支持三种生成范式：参考图像到视频合成、视频到视频扩展和音频引导视频生成。采用跨帧配对、图像编辑和语义重写的数据处理流程，结合多分辨率联合优化的训练策略。

Result: SkyReels-V3在视觉质量、指令遵循和特定方面指标上表现优异，接近领先闭源系统。

Conclusion: SkyReels-V3在视觉质量、指令遵循和特定方面指标上达到或接近最先进水平，接近领先的闭源系统。

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [51] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: SymbolSight框架通过优化符号设计减少视网膜假体用户的字母混淆，模拟显示混淆度降低22倍。


<details>
  <summary>Details</summary>
Motivation: 视网膜假体的低空间分辨率和时间持久性导致阅读困难，特别是在连续字母呈现时，一个符号的残影会干扰下一个符号的感知。研究旨在通过优化符号设计而非依赖硬件改进来解决这一问题。

Method: 研究采用了模拟假体视觉（SPV）和神经代理观察者来估计符号间的混淆度，并利用特定语言的双字母统计数据进行符号到字母的映射优化。

Result: 在阿拉伯语、保加利亚语和英语的模拟中，优化的异质符号集将预测混淆度中位数降低了22倍，相对于原生字母集。

Conclusion: 该研究表明，通过优化视觉符号的选择和映射，可以显著减少视网膜假体用户在连续字母呈现中的混淆，为未来的心理物理和临床评估提供了高效的设计空间筛选方法。

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [52] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: GPM模块通过注入几何先验提升U-Net在息肉分割中的性能，尤其在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积神经网络、Transformer和Mamba的U-Net变体在息肉分割中仍难以捕捉几何和结构线索，尤其在低对比度或复杂结肠镜场景中。

Method: 通过微调视觉几何基础Transformer（VGGT）在模拟的ColonDepth数据集上估计息肉图像的深度图，然后利用GPM模块将这些几何先验编码到编码器的特征图中，并通过空间和通道注意力机制进行细化。

Result: 在五个公共息肉分割数据集上的广泛实验表明，GPM模块在三个强基线模型上均取得了显著提升。

Conclusion: 提出的几何先验引导模块（GPM）通过将显式几何先验注入U-Net架构，显著提升了息肉分割的性能，尤其在低对比度或复杂场景中表现优异。

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [53] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: AGE-Net通过多技术整合和不确定性处理，显著提升了膝关节KL分级的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 膝关节KL分级因细微结构变化、长距离解剖依赖性和分级边界模糊性而具有挑战性。

Method: 提出AGE-Net框架，整合了Spectral--Spatial Fusion (SSF)、Anatomical Graph Reasoning (AGR)和Differential Refinement (DFR)，并采用Normal-Inverse-Gamma (NIG)证据回归头和成对序数排名约束。

Result: 在膝关节KL数据集上，AGE-Net的QWK为0.9017 +/- 0.0045，MSE为0.2349 +/- 0.0028，优于基线CNN模型。

Conclusion: AGE-Net在膝关节KL分级任务中表现出色，通过结合SSF、AGR和DFR技术，显著提升了性能，并在不确定性质量和可解释性方面展现了优势。

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [54] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 构建了Real-Texts数据集并提出TEXTS-Diff模型，显著提升了真实场景文本图像的超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中文本图像数据稀缺，导致文本区域恢复效果不佳，且孤立文本样本限制了背景重建质量。

Method: 提出了TEXTS-Aware Diffusion Model (TEXTS-Diff)，通过结合抽象概念和具体文本区域来提升文本细节的理解和生成质量。

Result: 实验表明，该方法在多个评估指标上达到最优性能，显著提升了文本区域的恢复准确性和背景的视觉质量。

Conclusion: 本文提出的TEXTS-Diff方法在文本图像超分辨率任务中表现出色，能够有效恢复文本区域的清晰度和背景的视觉质量，并在复杂场景中展现出卓越的泛化能力和文本恢复准确性。

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [55] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: STARS通过非对称对齐和像素级语义采样，解决了多模态数据缺失问题，提升了语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感技术中模态数据缺失是常见挑战，现有方法存在特征崩溃和泛化过度问题。

Method: STARS采用双向翻译和停止梯度的非对称对齐机制，结合像素级语义采样对齐策略（PSA）。

Result: STARS显著提升了不完全多模态输入下的语义分割性能，尤其在少数类别识别上表现优异。

Conclusion: STARS框架通过非对称对齐机制和像素级语义采样对齐策略，有效解决了多模态数据缺失问题，提升了语义分割性能。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [56] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: 提出YUV色彩空间的新型轻量级低光图像增强方法，通过双流注意力模块提升性能，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视通道特异性退化模式和跨通道交互，限制了轻量级低光图像增强的性能。

Method: 采用YUV色彩空间分析，设计双流全局-局部注意力模块（Y通道）、Y引导的局部感知频率注意力模块（UV通道）和引导交互模块进行特征融合。

Result: 在多个基准测试中达到最先进水平，视觉质量显著提升且参数数量大幅减少。

Conclusion: 该论文提出了一种基于YUV色彩空间的新型轻量级低光图像增强范式，通过双流全局-局部注意力模块和Y引导的局部感知频率注意力模块，显著提升了视觉质量并减少了参数数量。

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [57] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: NeRF-MIR通过PERE和PIRE机制优化NeRF的掩码图像恢复，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 针对NeRF在掩码图像恢复中的不足，尤其是随机光线发射难以学习复杂纹理的问题，提出NeRF-MIR以提升恢复效果。

Method: 提出PERE策略优化光线发射，PIRE机制进行自训练恢复，以及动态加权损失函数自动调整掩码区域权重。

Result: NeRF-MIR在真实数据和构建数据集上表现优于现有方法，验证了其有效性。

Conclusion: NeRF-MIR通过引入PERE策略、PIRE机制和动态加权损失函数，显著提升了基于NeRF的掩码图像恢复性能，并在真实数据和构建数据集上验证了其优越性。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [58] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: HyDeMiC是一种基于CNN的高光谱矿物分类器，在噪声环境下表现优异，适用于真实世界应用。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在高维HSI数据的环境噪声、传感器限制和计算复杂性方面表现不佳，因此需要一种更稳健的矿物分类方法。

Method: 本研究开发了HyDeMiC，一种基于卷积神经网络（CNN）的矿物分类器，使用来自USGS库的115种矿物的实验室测量高光谱数据进行训练。训练数据集通过将参考矿物光谱与HSI传感器响应函数卷积生成。

Result: HyDeMiC在清洁和低噪声数据集上实现了近乎完美的分类准确度（MCC = 1.00），并在中等噪声条件下保持了强劲性能。

Conclusion: HyDeMiC在存在中等噪声的情况下表现出强大的分类性能，突显了其在真实世界高光谱成像应用中的潜力。

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [59] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD是一种不确定性引导的轮廓感知位移框架，通过超像素和动态损失提升半监督医学图像分割的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督分割位移策略仅操作于矩形区域，忽视了解剖结构，导致边界失真和语义不一致。

Method: 提出UCAD框架，利用超像素生成与解剖边界对齐的区域，并通过不确定性引导的选择机制选择性地置换挑战性区域，同时引入动态不确定性加权一致性损失来稳定训练。

Result: UCAD在有限标注条件下，显著优于现有半监督分割方法，实现了更高的分割准确性。

Conclusion: UCAD框架通过结合超像素生成解剖学一致区域和不确定性引导的选择机制，显著提升了半监督医学图像分割的准确性和语义一致性。

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [60] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: PPIA是一种无需模型访问的黑盒物理提示注入攻击，通过视觉提示显著影响LVLM，攻击成功率高达98%。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法通常需要访问输入通道或依赖用户查询知识，这些假设在实际部署中很少成立，因此需要一种更通用的攻击方法。

Method: 结合离线选择高识别度和语义有效的视觉提示，以及基于时空注意力的环境感知策略放置，确保注入提示既可见又对模型行为有影响。

Result: 在10种先进的LVLM上评估，PPIA在视觉问答、规划和导航等任务中攻击成功率高达98%，且在不同物理条件下表现出强鲁棒性。

Conclusion: PPIA是一种高效的黑盒物理提示注入攻击方法，能够在无需访问模型或其输入的情况下，通过视觉观察显著影响LVLM的行为。

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [61] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出基于扩散模型的鲁棒水印框架，通过优化反转噪声和迭代去噪过程，结合自注意力约束，显著提升水印鲁棒性和图像质量，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的水印系统在图像质量影响较小的情况下隐藏水印，但在传输过程中遇到图像损坏时缺乏鲁棒性，影响其实际应用价值。因此，需要一种既能保持图像质量又能增强水印鲁棒性的方法。

Method: 该方法首先通过空文本优化过程将干净图像转换为反转噪声，并在潜在空间优化后，通过扩散模型的迭代去噪过程生成高质量水印图像。自注意力约束和伪掩码策略被引入以防止反转噪声优化扭曲图像原始语义。

Result: 实验结果显示，该方法在12种不同图像变换下平均优于稳定签名方法10%，证明了其在多种图像损坏情况下的优异性能。

Conclusion: 本文提出了一种基于扩散模型的高质量鲁棒水印框架，通过空文本优化过程和潜在空间优化，结合自注意力约束和伪掩码策略，显著提升了水印的鲁棒性和图像质量。实验结果表明，该方法在多种图像损坏情况下表现优异，尤其在COCO数据集上平均优于稳定签名方法10%。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [62] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 论文提出了一种改进的时空多视图表示学习框架，用于事件相机动作识别，显著提升了准确率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有SMVRL方法在事件相机动作识别中存在翻译变异的空间分箱表示和早期简单拼接融合架构的局限性，论文旨在解决这些问题并提升识别性能。

Method: 论文重新审视了时空多视图表示学习（SMVRL）的关键设计阶段，提出了：（i）通过翻译不变性密集转换稀疏事件的时空多视图表示，（ii）双分支动态融合架构，建模不同视图间运动特征的样本互补性，（iii）生物启发式时间扭曲增强，模拟真实世界人类动作的速度变化。

Result: 在HARDVS、DailyDVS-200和THU-EACT-50-CHL三个数据集上，Top-1准确率分别提升了7.0%、10.7%和10.2%，同时参数减少30.1%，计算量降低35.7%。

Conclusion: 该论文提出了一种新颖且强大的事件相机动作识别（EAR）框架，通过翻译不变性密集转换稀疏事件、双分支动态融合架构和生物启发式时间扭曲增强，显著提升了识别准确率，同时减少了参数和计算量。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [63] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: ReLE是一个可扩展系统，用于诊断能力各向异性（模型在不同领域性能的非均匀性），通过混合评分和动态调度减少评估成本，揭示模型高度专业化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在中文理解方面进展迅速，但准确评估其能力仍受基准饱和和高计算成本的挑战。静态排行榜虽提供快照排名，但常掩盖能力间的结构性权衡。

Method: 提出了两个方法贡献：(1) 符号接地混合评分机制，消除推理任务中基于嵌入的假阳性；(2) 基于Neyman分配和噪声校正的动态方差感知调度器，相比全通评估减少70%的计算成本，同时保持排名相关性ρ=0.96。

Result: 评估了304个模型（189个商业，115个开源），揭示了聚合排名对权重方案高度敏感：模型在ReLE中的排名稳定性振幅（RSA）为11.4，而传统基准中约为5.0，证实现代模型高度专业化而非普遍优越。

Conclusion: ReLE系统被定位为一种高频诊断工具，用于监测不断演变的模型格局，而非全面静态基准的替代品。

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [64] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: HAAF通过跨层级对齐和双分支推理，解决了视觉语言模型在病理学中的粒度不匹配问题，显著提升细粒度异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 精准病理学依赖在特定感兴趣区域（ROIs）内检测细粒度形态异常，而现有视觉语言（V-L）模型因通用表示无法解析此类细微缺陷，面临粒度不匹配问题。当前适应方法常将模态视为独立流，未能将语义提示与ROI特定视觉上下文结合。

Method: 提出了分层适应和对齐框架（HAAF），其核心是跨层级缩放对齐机制（CLSA），通过视觉特征先注入文本提示生成内容自适应描述符，再空间引导视觉编码器聚焦异常。此外，采用双分支推理策略结合语义评分与几何原型以确保少样本设置下的稳定性。

Result: 在四个基准测试中，HAAF显著优于现有方法，并在低资源场景中有效扩展。

Conclusion: HAAF通过其核心的跨层级缩放对齐机制（CLSA）和双分支推理策略，在低资源场景下显著优于现有方法，并能有效结合领域特定骨干网络（如CONCH）进行扩展。

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [65] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 提出了一种基于邻域签名的SFDA方法，通过优化目标域样本的预测相似性，有效减少噪声邻居影响，在VisDA等数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于邻域一致性的SFDA方法容易因误导性邻域信息而产生错误，因此需要一种更有效的方法来提升适应性。

Method: 通过优化目标域样本预测的相似性和不相似性，利用邻域签名概念来学习更具信息性的聚类并减少噪声邻居的影响。

Result: 在VisDA数据集上表现优于现有方法，并在其他基准数据集上取得了有竞争力的结果。

Conclusion: 论文提出的基于邻域签名的方法在SFDA任务中表现出色，特别是在VisDA数据集上超越了现有方法，同时在其他基准数据集上也取得了有竞争力的结果。

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [66] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于Firebase的云物联网系统，用于同步环境监控和设备控制，具有高可靠性和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的激增为远程监控和控制应用创造了前所未有的机会。传统监控系统在实时数据可访问性、远程可控性和云集成方面存在局限性。

Method: 该系统利用ESP32微控制器与DHT22温湿度传感器和HC-SR04超声波距离传感器接口，同时通过基于云的界面远程控制两个LED指示灯。实时传感器数据被传输到Firebase，提供一个可从多个设备同时访问的同步平台。

Result: 实验结果表明，数据传输成功率为99.2%，实时控制延迟低于1.5秒，并支持持久数据存储以供历史分析。

Conclusion: 该系统提供了一个可扩展的框架，适用于从智能家居自动化到工业监控的各种物联网应用，总实现成本为32.50美元。Firebase的集成提供了强大的云功能，无需复杂的服务器基础设施，使资源有限的开发者和研究人员也能访问高级物联网应用。

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [67] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: CoT-Seg通过思维链推理和自我校正提升分割性能，适用于复杂查询和域外图像。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法在复杂查询和域外图像上表现不佳，受人类逐步思考启发，提出需逐步推理、自我评估和优化的系统。

Method: CoT-Seg利用预训练MLLMs（如GPT-4o）的推理能力分解查询，提取细粒度语义，并通过自我校正阶段迭代优化分割结果。

Result: CoT-Seg在ReasonSeg-Hard数据集上表现优异，展示了其在处理复杂分割任务中的有效性。

Conclusion: CoT-Seg框架通过结合思维链推理与自我校正，显著提升了复杂场景下的分割可靠性，为视觉语言驱动的分割提供了新范式。

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [68] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 论文提出了一种改进XCA血管分割和标记的方法，通过联合超分辨率和增强，以及神经网络模型，显著提高了分割精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: XCA是评估冠状动脉疾病的临床金标准，但定量分析受限于常规数据中血管分割的困难。低对比度、运动、透视缩短、重叠和导管干扰会降低分割质量并导致跨中心域偏移。

Method: 从670个电影序列中选择最佳帧，应用联合超分辨率和增强。比较了经典Meijering、Frangi和Sato血管滤波器，以及U-Net、FPN和Swin Transformer等神经网络基线。第二阶段分配血管身份（LAD、LCX、RCA）。

Result: SVR每张图像调优提高了所有经典滤波器的Dice分数（如Frangi：0.759 vs. 0.741）。FPN在冠状动脉专用监督下达到0.914+/-0.007 Dice，合并冠状动脉+导管标签进一步提高到0.931+/-0.006。在DCA1外部测试中，Dice下降至0.798（冠状动脉专用）和0.814（合并），但轻量级域内微调恢复至0.881+/-0.014和0.882+/-0.015。血管类型标记的准确率分别为RCA 98.5%（Dice 0.844）、LAD 95.4%（0.786）和LCX 96.2%（0.794）。

Conclusion: 通过学习每张图像的调参方法可以增强传统管道，而高分辨率FPN模型和合并标签监督通过适度适应提高了稳定性和外部迁移能力。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [69] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit通过跨尺度门控融合、层融合-分离块和课程训练，有效解决了单图像反射分离中的传输-反射混淆问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非线性混合下存在传输-反射混淆问题，特别是在深层解码器层中，由于隐式融合机制和多尺度协调不足。

Method: 提出了ReflexSplit，一个双流框架，包含三个关键创新：跨尺度门控融合（CrGF）、层融合-分离块（LFSB）和课程训练。

Result: 在合成和真实世界基准测试中实现了最先进的性能。

Conclusion: ReflexSplit在合成和真实世界基准测试中表现出色，具有卓越的感知质量和鲁棒的泛化能力。

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [70] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: PhaSR通过双级先验对齐（PAN和GSRA）实现多样化光照下的阴影去除，性能优越且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 多样化光照条件下分离光照与固有反射率具有挑战性，尤其当物理先验未正确对齐时。

Method: 首先采用物理对齐归一化（PAN）进行闭式光照校正，包括Gray-world归一化、对数域Retinex分解和动态范围重组；其次通过几何-语义矫正注意力（GSRA）实现跨模态对齐，结合深度几何和DINO-v2语义嵌入。

Result: 实验表明PhaSR在阴影去除中表现优异，复杂度低，并能泛化到多源环境光照。

Conclusion: PhaSR通过双级先验对齐（PAN和GSRA）有效解决了多样化光照条件下的阴影去除问题，表现出色且复杂度低，并能泛化到传统方法在多源光照下失败的环境。

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [71] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: BMDS-Net 通过鲁棒主干和贝叶斯微调，解决了 Transformer 模型在缺失模态和置信度校准上的问题，提升了临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的 Transformer 模型（如 Swin UNETR）在临床应用中存在对缺失模态敏感和缺乏置信度校准的问题，BMDS-Net 旨在解决这些问题，提升临床鲁棒性和可信度。

Method: BMDS-Net 结合了 Zero-Init Multimodal Contextual Fusion (MMCF) 模块和 Residual-Gated Deep Decoder Supervision (DDS) 机制，构建了一个鲁棒的确定性主干网络，并通过贝叶斯微调策略实现了概率性预测。

Result: 在 BraTS 2021 数据集上的实验表明，BMDS-Net 不仅保持了竞争力，还在缺失模态情况下表现出卓越的稳定性。

Conclusion: BMDS-Net 通过集成 Zero-Init MMCF 模块和 Residual-Gated DDS 机制，构建了一个鲁棒的确定性主干网络，并通过内存高效的贝叶斯微调策略，实现了体素级不确定性映射，显著提升了在缺失模态情况下的稳定性和临床可信度。

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [72] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: FMIR是一种基于基础模型的医学图像配准框架，通过特征编码器和通用注册头，在单一数据集上训练，实现了高性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像配准中取得了前所未有的速度，但其临床应用受限于泛化能力不足，尤其是在医学数据集通常较小的情况下。

Method: FMIR结合了基础模型特征编码器用于提取解剖结构，以及通用注册头，并通过通道正则化策略在单一数据集上进行训练。

Result: FMIR在域内性能达到最先进水平，同时在域外图像上保持稳健的配准效果。

Conclusion: FMIR框架通过结合基础模型特征编码器和通用注册头，以及通道正则化策略，在有限资源下实现了可推广的医学影像基础模型构建。

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [73] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: 通过生成合成图像结合文本比较，提升零样本准确率预测质量，帮助用户评估VLM适用性。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户难以评估所选视觉语言模型（VLM）在其特定问题上是否有效的问题。

Method: 基于文本比较和生成合成图像的方法，用于评估和优化零样本准确率预测。

Result: 在标准CLIP基准数据集上的实验表明，基于图像的方法能帮助用户在没有标注样本的情况下预测VLM的有效性。

Conclusion: 结合生成图像的方法显著提高了零样本准确率预测的质量，并为用户提供了评估过程中使用的图像类型反馈。

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [74] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 提出对象纹理强度（OTI）作为模型无关且视觉可解释的图像攻击性度量方法，解决了现有方法的局限性，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图像攻击性度量方法依赖模型代理且缺乏视觉可解释性，限制了其在实际任务中的应用。

Method: 通过分析决策边界以及对抗性扰动的中高频特性，提出对象纹理强度（OTI）作为图像攻击性的度量标准。

Result: 综合实验表明，OTI不仅有效且计算高效，还为对抗性机器学习社区提供了对攻击性的视觉理解。

Conclusion: 本文提出了一种新颖的、模型无关且视觉可解释的图像攻击性度量方法——对象纹理强度（OTI），有效解决了现有方法的局限性，并在理论和实验上验证了其有效性和计算效率。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [75] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: 利用显著性地图驱动的预处理技术结合传统压缩方法，实现卫星图像的可变速率压缩，优化存储和带宽成本。


<details>
  <summary>Details</summary>
Motivation: 卫星图像的压缩是一个重要研究领域，因为每天收集的大量图像增加了存储和带宽成本。许多下游任务仅对图像中的小区域感兴趣，这些区域一旦确定，可用于优化图像编码。

Method: 使用可变大小的平滑核映射到不同的量化显著性水平，以处理图像像素，从而优化下游压缩和编码方案。

Result: 通过显著性地图驱动的预处理技术与传统压缩方法结合，实现了在单个大型卫星图像中的可变速率压缩。

Conclusion: 本研究展示了如何通过显著性地图驱动的预处理技术与传统有损压缩编码标准结合，实现单个大型卫星图像中的可变速率压缩。

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [76] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: STA是一种通过重写输入提示干扰代理推理的攻击方法，能在不修改模型或工具的情况下增加计算开销，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型（LLMs）在外部工具辅助下进行代理推理时的潜在漏洞，特别是在工具调用过程中可能遭受的恶意操纵。

Method: STA 被设计为一个迭代的多智能体协作框架，具有明确的重写策略控制，能够从原始提示生成语义保真度高的良性重写提示。

Result: 在6个模型、12个工具、4个代理框架和13个数据集的广泛实验中，STA的有效性得到了验证。

Conclusion: Sponge Tool Attack (STA) 是一种有效的攻击方法，能够在保持任务语义和用户意图的前提下，通过重写输入提示来干扰代理推理过程，导致计算开销大幅增加。

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [77] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: Stylizing ViT 通过共享权重注意力块设计，实现了医学图像的高质量风格化增强，显著提升领域泛化性能，并在测试时增强中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析中因数据异质性和稀缺性导致的领域泛化问题，传统数据增强方法在显著领域偏移下效果有限，而现有风格化增强方法在风格多样性或生成图像质量上存在不足。

Method: 提出了一种新颖的 Vision Transformer 编码器 Stylizing ViT，利用权重共享的注意力块同时进行自注意力和跨注意力操作，以实现解剖结构保持和风格迁移。

Result: 在组织病理学和皮肤病学的三个图像分类任务中，Stylizing ViT 相比现有技术提升了高达 13% 的准确率，且生成的图像无伪影；测试时增强进一步带来 17% 的性能提升。

Conclusion: Stylizing ViT 通过共享权重的注意力块设计，在保持解剖结构一致性的同时实现了多样化的风格迁移，显著提升了医学图像分析的领域泛化能力，并在推理阶段通过测试时增强进一步提高了性能。

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [78] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP 通过双路径解码器直接从冻结的 CLIP 视觉编码器中解锁和解释潜在几何知识，无需文本编码器，显著提升了几何结构感知能力。


<details>
  <summary>Details</summary>
Motivation: CLIP 在语义理解方面取得了巨大成功，但本质上难以感知几何结构。现有方法试图通过文本提示查询 CLIP 来弥补这一差距，但这一过程通常间接且低效。

Method: SPACE-CLIP 采用双路径解码器架构，包括语义路径和结构路径。语义路径解释高级特征，动态使用特征级线性调制（FiLM）进行全局上下文条件化；结构路径从早期层提取细粒度空间细节。这两种互补流进行层次融合，实现语义上下文和精确几何的稳健合成。

Result: 在 KITTI 基准测试上的大量实验表明，SPACE-CLIP 显著优于之前的基于 CLIP 的方法。消融研究验证了双路径的协同融合对这一成功至关重要。

Conclusion: SPACE-CLIP 提供了一种高效且架构优雅的方案，用于重新利用大规模视觉模型，不仅是一个独立的深度估计器，还是一个可轻松集成的空间感知模块，适用于下一代具身AI系统。

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [79] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: PG 是一种无需训练的框架，通过两阶段布局提示嫁接，解决了多食物图像生成中的对象纠缠问题，实现了可控的食物分离。


<details>
  <summary>Details</summary>
Motivation: 现实世界的餐食图像通常包含多种食物，而现有的文本到图像扩散模型在多食物图像生成中存在对象纠缠问题，导致相邻食物融合。

Method: PG 采用两阶段流程：首先通过布局提示建立不同区域，待布局稳定后将目标提示嫁接。该方法允许用户通过编辑布局来控制食物的分离或混合。

Result: 在两个食物数据集上的实验表明，PG 显著提高了目标对象的出现率，并提供了可控分离的定性证据。

Conclusion: Prompt Grafting (PG) 框架通过结合显式空间提示和隐式布局指导，显著提高了多食物图像生成的准确性，并提供了可控的食物分离能力。

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [80] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: Uni-RS是首个针对遥感设计的统一多模态模型，通过显式空间布局规划和监督，解决了理解和生成间的空间不对称问题，显著提升了生成的空间忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有的统一遥感多模态模型在图像识别和描述中能准确定位对象，但在文本到图像生成中常无法忠实执行相同的空间关系，而空间关系是遥感中的核心语义信息。

Method: 1. 引入显式的空间布局规划（Spatial-Layout Planning），将文本指令转化为空间布局计划；2. 实施空间感知查询监督（Spatial-Aware Query Supervision），使可学习查询偏向于指令中明确指定的空间关系；3. 开发图像-标题空间布局变化（Image-Caption Spatial Layout Variation），让模型接触系统性的几何一致空间变换。

Result: 在多个基准测试中，Uni-RS显著提升了文本到图像生成的空间忠实性。

Conclusion: Uni-RS显著提升了遥感文本到图像生成中的空间忠实性，同时在多模态理解任务（如图像描述、视觉定位和VQA任务）中保持了强大的性能。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [81] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: StyleDecoupler 通过互信息最小化分离风格特征，无需微调即可在多模态模型中实现高效风格检索。


<details>
  <summary>Details</summary>
Motivation: 艺术风格的表示因其与语义内容的深度纠缠而具有挑战性。

Method: 利用单模态表示作为内容参考，通过互信息最小化从多模态嵌入中分离风格特征。

Result: 在 WeART 和 WikiART 数据集上实现了最先进的风格检索性能，并支持风格关系映射和生成模型评估等应用。

Conclusion: StyleDecoupler 是一种无需微调的即插即用模块，能够从多模态嵌入中分离纯风格特征，并在风格检索等任务中表现出色。

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [82] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: 提出自动化深度学习框架，高效量化红细胞群体动态变化，解决细胞重叠和标注稀缺问题，提升实验通量和治疗效果评估。


<details>
  <summary>Details</summary>
Motivation: 准确识别镰状细胞在不同生物物理条件下的形态变化，尤其是在密集和重叠细胞群体中。

Method: 采用自动化深度学习框架，整合AI辅助标注、分割、分类和实例计数，使用nnU-Net分割模型和分水岭算法解决细胞重叠问题。

Result: 框架在少量标注数据下实现高分割性能，显著提升实验通量，捕捉药物依赖性镰状行为，并揭示细胞形态演变的独特力学特征。

Conclusion: 该AI驱动框架建立了一个可扩展且可重复的计算平台，用于研究细胞生物力学和评估微生理系统中的治疗效果。

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [83] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 结合3D高斯点渲染和稀疏体素栅格化的优势，提出体素初始化及深度几何监督方法，提升几何准确性和表面完整性，同时保持快速收敛。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点渲染和稀疏体素栅格化各有优缺点，前者收敛快但表面保真度有限，后者几何清晰但初始化慢且未充分利用场景结构。结合两者优势以提升重建效果。

Method: 提出了一种体素初始化方法，将体素放置在合理位置并具有适当的细节级别，为每场景优化提供了强起点；并提出了改进的深度几何监督，将多视图线索转化为直接的每射线深度正则化。

Result: 在标准基准测试中，该方法在几何准确性、细结构恢复和表面完整性方面优于现有方法，同时保持了快速收敛。

Conclusion: 通过结合3D高斯点渲染和稀疏体素栅格化的优势，并提出体素初始化方法及深度几何监督，该方法在几何准确性、细结构恢复和表面完整性方面优于现有方法，同时保持了快速收敛。

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [84] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文系统评估了INR在ASSR中的表现，发现复杂方法改进有限，训练配置关键，新损失函数有效，缩放定律适用。


<details>
  <summary>Details</summary>
Motivation: 填补对现有方法效果和训练配方影响的系统性研究空白，为ASSR领域提供基准和未来方向。

Method: 通过比较现有技术在不同设置下的表现，并贡献统一框架和代码库以促进可重复比较。

Result: 发现复杂INR方法改进有限，训练配置影响显著，新损失函数提升纹理保真度，缩放定律适用。

Conclusion: 近期更复杂的INR方法仅比早期方法有边际改进；模型性能与训练配置强相关；提出的损失函数增强了纹理保真度；缩放定律适用于基于INR的ASSR。

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [85] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 论文提出了一种端到端代理框架，通过ScripterAgent和DirectorAgent从对话生成电影视频，解决了语义鸿沟问题，并在评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决视频生成模型在从高级概念（如对话）生成长篇连贯叙事时的‘语义鸿沟’问题。

Method: 论文提出了一种新颖的端到端代理框架，包括ScripterAgent和DirectorAgent，用于从对话生成电影视频。ScripterAgent将粗略对话翻译为细粒度可执行电影脚本，DirectorAgent则使用跨场景连续生成策略协调视频模型。

Result: 综合评估显示，该框架显著提高了脚本忠实度和时间保真度，并揭示了当前模型在视觉表现与脚本忠实度之间的权衡。

Conclusion: 该论文的结论是，当前最先进的视频生成模型在视觉表现和脚本忠实度之间存在关键权衡，为自动化电影制作的未来提供了有价值的见解。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [86] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: FaNeRV通过频率感知的神经表示和多分辨率监督，有效解决了INR视频压缩中的频谱偏差问题，提升了重建质量和率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于INR的视频压缩框架存在频谱偏差，倾向于低频成分，导致重建过度平滑和率失真性能不佳。

Method: 提出FaNeRV，一种频率感知的神经表示方法，包括多分辨率监督策略、动态高频注入机制和频率分解网络模块。

Result: FaNeRV在标准基准测试中显著优于现有INR方法，并与传统编解码器竞争。

Conclusion: FaNeRV通过显式解耦视频的低频和高频成分，结合多分辨率监督策略和动态高频注入机制，显著提升了视频重建的质量和率失真性能，超越了现有的INR方法，并与传统编解码器竞争。

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [87] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: TeNeRV通过IFF和GAM机制整合时间依赖，显著提升视频压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于INR的方法将时间维度作为独立输入，难以捕捉复杂时间依赖关系。

Method: 提出了一种层次化时间神经表示（TeNeRV），包含帧间特征融合（IFF）模块和GoP自适应调制（GAM）机制。

Result: TeNeRV在率失真性能上 consistently 优于现有基于INR的方法。

Conclusion: TeNeRV通过整合短长期时间依赖关系，在视频压缩中表现出色，验证了其方法的有效性。

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [88] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: UniCD是一个统一的变更检测框架，通过共享编码器和多分支协作学习机制，有效处理监督、弱监督和无监督任务，显著提升弱监督和无监督场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中像素级变化标签获取成本高，现有模型难以适应标注可用性多样化的场景，因此需要一种统一的框架来协同处理监督、弱监督和无监督任务。

Method: UniCD框架包含三个监督特定分支：监督分支引入时空感知模块（STAM）实现双时相特征的高效协同融合；弱监督分支构建变化表示正则化（CRR）引导模型从粗粒度激活向连贯且可分离的变化建模收敛；无监督分支提出语义先验驱动的变化推断（SPCI）将无监督任务转化为可控的弱监督路径优化。

Result: UniCD在主流数据集上表现优异，尤其在弱监督和无监督场景下，准确率分别比当前最优方法提升了12.72%和12.37%（LEVIR-CD数据集）。

Conclusion: UniCD框架通过共享编码器和多分支协作学习机制，实现了异构监督信号的深度耦合，在监督、弱监督和无监督任务中均表现出色，显著提升了弱监督和无监督场景下的检测精度。

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [89] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: A non-contact method using 3D reconstruction from multi-view RGB images and ensemble regression was developed for cattle weight estimation, showing better performance with classical models under low-data conditions, suitable for farm use.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is to address the limitations of traditional cattle live weight estimation methods, which involve manual handling and can impact productivity from both a stock and economic perspective. The goal is to develop a cost-effective, non-contact method for live weight calculation in cattle.

Method: The proposed method uses multi-view RGB images with SAM 3D-based agreement-guided fusion followed by ensemble regression to generate a single 3D point cloud per animal. It compares classical ensemble models with deep learning models under low-data conditions.

Result: Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R² = 0.69 ± 0.10, MAPE = 2.22 ± 0.56 %), making this practical for on-farm implementation.

Conclusion: The study concludes that improving 3D reconstruction quality is more critical than increasing model complexity for scalable deployment on farms, especially where large volumes of 3D data are challenging to produce.

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [90] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: ViTCoP通过视觉与文本协同剪枝，高效保留关键视觉标记，显著提升模型性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记剪枝方法存在视觉信息过早丢失或标记间信息冗余的问题，ViTCoP旨在解决这些挑战。

Method: 提出了ViTCoP框架，结合视觉编码器中的冗余过滤和基于LLM层次特性的逐步协同剪枝，以保留关键且信息多样的视觉标记。

Result: 在各种大规模视觉语言模型上的实验表明，ViTCoP在图像和视频理解任务上均超越现有方法，并显著提升效率。

Conclusion: ViTCoP框架通过视觉和文本语义协同剪枝，不仅在大规模视觉语言模型上实现了最先进的性能，还显著降低了推理延迟和GPU内存消耗，尤其在极端剪枝率下表现更为突出。

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [91] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: \namex 是一种轻量级扩散训练加速框架，利用预训练VAE特征对齐潜在特征，显著提升训练效率且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散变换器训练收敛效率低，现有方法（如REPA和SRA）因外部依赖导致计算开销大。\namex 旨在通过轻量级内在引导框架解决这一问题。

Method: \namex 利用预训练的VAE特征，通过轻量级投影层将扩散变换器的中间潜在特征与VAE特征对齐，并通过特征对齐损失进行监督。

Result: 实验表明，\namex 在生成质量和训练收敛速度上均优于普通扩散变换器，与现有加速方法相当或更优，且仅增加4%的GFLOPs。

Conclusion: \namex 提出了一种轻量级的内在引导框架，通过利用预训练的VAE特征，有效加速了扩散变换器的训练收敛，同时保持了生成质量，且计算开销极小。

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [92] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: 该论文通过理论化高斯基元为随机固体，解决了形状提取问题，提升了重建质量和多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射（GS）在新视角合成中表现出色，但形状提取仍存在问题，现有方法在多视角一致性和浮点敏感度上表现不佳。

Method: 提出了一种基于随机固体理论的高斯基元几何参数化方法，利用其体积性质高效渲染高质量深度图，用于细粒度几何提取。

Result: 实验表明，该方法在公开数据集上实现了所有基于高斯溅射的形状重建方法中的最佳效果。

Conclusion: 该论文通过将高斯基元理论化为随机固体，为Geometry-Grounded Gaussian Splatting提供了理论基础，显著提升了形状重建的质量和多视角一致性。

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [93] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: SynMind通过文本对齐模块增强fMRI图像重建的语义准确性，优于现有方法且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI图像重建方法在视觉质量上表现优异，但语义对齐不足，常出现对象替换或幻觉。

Method: 利用基于VLMs生成的句子级语义描述，结合预训练的扩散模型（Stable Diffusion 1.4），提出SynMind框架。

Result: SynMind在定量指标和人类评估中优于现有方法，且仅需单个消费级GPU。神经可视化分析显示其激活了更广泛的语义相关脑区。

Conclusion: SynMind通过结合显式语义编码与视觉先验，显著提升了fMRI图像重建的语义对齐能力，并在定量指标和人类评估中优于现有方法。

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [94] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级域泛化框架，结合量子增强协作学习，显著提升了医学影像AI模型在跨中心部署中的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI模型在单中心或单设备设置中表现良好，但在真实世界的跨中心部署中因域偏移而效果下降，限制了临床泛化能力。

Method: 构建了一个基于MobileNetV2的域不变编码器，通过多域成像偏移模拟、域对抗训练和轻量级量子特征增强层进行优化，并在推理时采用测试时适应策略。

Result: 在模拟多中心医学影像数据集上的实验表明，该方法在未见域上显著优于无域泛化或量子增强的基线模型，降低了域特定性能方差，提高了AUC和敏感性。

Conclusion: 该论文提出的量子增强协作学习框架在未见过的目标域上表现出色，显著提升了模型的泛化能力，为混合量子-经典医学影像系统提供了可行的范例。

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [95] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: MV-SAM利用点图实现多视图分割的3D一致性，性能优于SAM2-Video，无需显式3D数据或网络。


<details>
  <summary>Details</summary>
Motivation: 解决现有可提示分割模型（如SAM）在多视图场景中缺乏3D一致性的问题，避免昂贵的逐场景优化。

Method: MV-SAM扩展了SAM，通过将预训练编码器的图像嵌入提升为3D点嵌入，并通过带有3D提示嵌入的交叉注意力解码，实现2D交互与3D几何的对齐。

Result: 在多个基准测试（NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV）上表现优异，泛化能力强。

Conclusion: MV-SAM通过利用点图（pointmaps）实现多视图分割的3D一致性，无需显式3D网络或标注3D数据，性能优于SAM2-Video，并与逐场景优化基线相当。

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [96] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA结合双向注意力和MARS-Cache，提升视频LLM建模效率，推理加速12倍。


<details>
  <summary>Details</summary>
Motivation: 解决传统自回归视频LLM因因果掩码偏差导致的全局时空建模效率低下问题。

Method: 提出VidLaDA，基于扩散语言模型，利用双向注意力捕获双向依赖；引入MARS-Cache框架，通过异步视觉缓存刷新和帧级块注意力加速推理。

Result: VidLaDA性能优于扩散基线模型，媲美先进自回归模型（如Qwen2.5-VL和LLaVA-Video），MARS-Cache实现12倍以上加速且不影响推理准确性。

Conclusion: VidLaDA通过双向注意力机制和MARS-Cache框架，显著提升了视频LLM的全局时空建模能力，并在推理速度上实现了12倍以上的加速。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [97] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD是一个多模态数据集，整合了古兰经的文本和音频，支持多种计算应用和研究。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉古兰经朗诵的丰富口头传统，并提供细粒度的发音、音韵和语义上下文分析，创建了这个多模态数据集。

Method: 数据集整合了古兰经的文本、语言和音频维度，包括每节经文的阿拉伯原文、英文翻译和音标转写，以及32位不同朗诵者的音频。

Result: 数据集支持自然语言处理、语音识别、文本到语音合成等多种应用，并为多模态嵌入、语义检索等任务奠定了基础。

Conclusion: Quran MD数据集为古兰经的多模态研究提供了独特的资源，支持多种计算应用，包括语音识别、文本到语音合成等，并为研究和社区应用奠定了基础。

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [98] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: 论文从信号处理角度提出Jinc核及调制核，优化3D重建中频域保真度与空间效率的平衡，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决离散采样引起的周期性频谱扩展问题，改善传统重建核（如高斯、指数函数等）低通特性不理想导致的高低频分量混叠。

Method: 从信号处理角度重新审视3D重建，提出Jinc核作为理想低通滤波器，并进一步设计调制核以平衡空间衰减速度与频域性能。

Result: Jinc和调制核在渲染性能上表现优越，实验验证了其有效性。

Conclusion: Jinc和调制核在3D重建中实现了空间效率与频域保真度的平衡，实验证明了其有效性。

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [99] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: Gen1S利用基础类和新型类嵌入的结构相似性，通过生成模型学习残差分布，显著提升1-shot FSCIL性能。


<details>
  <summary>Details</summary>
Motivation: 解决在基础训练阶段后仅提供每个新型类别单一样本（1-shot）且不允许进一步训练或模型修改的挑战性FSCIL设置中，新型类别泛化困难的问题。

Method: 提出了一种将原始嵌入空间映射到残差空间的方法，通过减去输入样本的类别原型（即平均类别嵌入），并利用VAE或扩散模型学习基础类残差的多模态分布，作为结构先验来改进新型类别的识别。

Result: Gen1S在多个基准和骨干架构上一致提升了新型类别的识别性能，优于现有技术。

Conclusion: Gen1S方法通过利用基础类和新型类嵌入的结构相似性，结合生成模型学习残差空间的多模态分布，显著提升了新型类别的识别性能，在多个基准和骨干架构上均优于现有技术。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [100] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: 研究评估了DPO在医学LVLM中的效果，发现现有方法存在局限性，并提出新策略提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型（LVLM）在医学应用中因对齐和可靠性不足而受限的问题，探索DPO在高风险医学环境中的有效性。

Method: 评估了九种不同的DPO变体在两种医学LVLM（LLaVA-Med和HuatuoGPT-Vision）上的表现，并提出了针对视觉误解错误的偏好构建策略。

Result: 现有DPO方法在不同任务和模型上表现不一致，且未能解决视觉误解错误。提出的策略在视觉问答任务上比现有DPO基线提升了3.6%。

Conclusion: 当前DPO方法在医学领域的应用存在局限性，但通过针对性的偏好构建策略，可以显著提升视觉问答任务的性能。

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [101] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: RemEdit通过Riemannian流形导航和任务特定注意力剪枝，实现了高效且高保真的图像编辑，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可控图像生成中语义保真度与推理速度之间的关键权衡问题。

Method: RemEdit采用基于扩散的框架，结合Riemannian流形导航、mamba模块学习流形结构、双SLERP混合技术、目标感知提示增强以及任务特定注意力剪枝机制。

Result: 在50%剪枝率下保持实时性能，超越了现有最先进的编辑框架。

Conclusion: RemEdit通过其创新方法在保持实时性能的同时，超越了现有的图像编辑框架，为实用且强大的图像编辑设立了新标准。

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [102] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: SC-SAM通过U-Net与SAM的双向协同训练，有效利用未标记数据，在医学图像分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中领域偏移、标签稀缺及PEFT无法利用未标记数据的问题，探索专家模型（如U-Net）与通用模型（如SAM）的协同潜力。

Method: 提出SC-SAM框架，结合U-Net和SAM的双向协同训练，利用U-Net生成点提示和伪标签指导SAM，同时SAM作为通用监督器规范U-Net。

Result: 在前列腺MRI和息肉分割基准测试中，SC-SAM优于现有的半监督SAM变体及MedSAM等医学基础模型。

Conclusion: SC-SAM框架通过专家-通用模型的双向协同训练，显著提升了医学图像分割的标签效率，并在多个基准测试中取得了最先进的性能。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [103] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: DTC通过学习动态采样位置改进医学图像分割的上采样效果，优于传统固定位置方法。


<details>
  <summary>Details</summary>
Motivation: 传统的固定位置上采样方法（如转置卷积和线性插值）可能无法捕捉预定义采样位置之外的结构信息，导致伪影或细节丢失。

Method: 提出了一种新颖的上采样方法——可变形转置卷积（DTC），通过学习动态坐标来生成高分辨率特征图，适用于2D和3D医学图像分割任务。

Result: 在3D（如BTCV15）和2D数据集（如ISIC18、BUSI）上的实验表明，DTC能有效整合到现有医学图像分割模型中，提升性能。

Conclusion: DTC方法通过动态学习采样位置，显著提升了医学图像分割的解码器特征重建和细节恢复能力，为现有模型提供了有效的改进方案。

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [104] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph 是一种自监督框架，通过物理一致的方法从微流控视频中学习红细胞的力学代理标量k，显著提升了变形性分析的性能。


<details>
  <summary>Details</summary>
Motivation: 红细胞的机械性能是血液和全身疾病的潜在生物标志物，但现有方法依赖于监督分割或手工制作的kymographs，且未编码控制红细胞形状演化的层流斯托克斯流物理。

Method: FlowMorph 通过低维参数化轮廓建模每个红细胞，结合层流平流和曲率正则化弹性松弛的微分“胶囊流动”推进边界点，并优化了多个损失耦合条件。

Result: 在四个公开的红细胞微流控数据集上，FlowMorph 在物理丰富的视频上实现了平均轮廓IoU为0.905，显著改善了面积守恒和壁约束违反情况。标量k能够以AUC 0.863区分翻转和坦克履带动力学。

Conclusion: FlowMorph 是一种物理一致的自监督框架，能够从短时明场微流控视频中学习无标记的力学代理标量k，显著提高了红细胞变形性分析的准确性和适应性。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [105] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: UPLiFT通过局部注意力操作符实现高效特征上采样，性能优于现有方法且成本更低。


<details>
  <summary>Details</summary>
Motivation: 探索任务无关特征上采样的高效方法，以低成本从预训练视觉骨干中生成密集特征。

Method: 提出了UPLiFT架构和高效的Local Attender操作符，采用局部注意力池化公式。

Result: UPLiFT在生成下游任务中表现优异，推理成本低于现有像素密集特征上采样器。

Conclusion: UPLiFT提供了一种多功能且高效的方法来创建更密集的特征，在保持较低推理成本的同时实现了最先进的性能。

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [106] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: DKGH-MoE是一种结合数据驱动和领域专家知识的混合专家模型，旨在提升医学领域的模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医学等专业领域，混合专家模型（MoE）因数据集小而效果受限，而临床实践中的专家知识（如医生诊断启发式）难以从有限数据中可靠学习。结合两者的优势可提升学习的鲁棒性和临床意义。

Method: 提出了一种名为DKGH-MoE的模块，结合了数据驱动的MoE和领域专家引导的MoE，前者从原始影像数据中提取新特征，后者整合临床先验知识（如医生眼动线索）以突出高诊断相关性区域。

Result: DKGH-MoE通过整合领域专家见解和数据驱动特征，提升了性能和可解释性。

Conclusion: DKGH-MoE通过结合数据驱动学习和领域专家知识，提高了模型在医学领域的性能和可解释性。

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [107] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: MorphXAI是一个可解释的寄生虫检测框架，结合形态学监督提升检测性能并提供临床相关的形态特征分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习模型在寄生虫检测中解释性不足的问题，提供更符合临床诊断需求的形态学特征分析。

Method: MorphXAI框架将形态学监督直接集成到预测流程中，使模型能够定位寄生虫并同时表征临床相关的形态特征。

Result: 实验结果表明，MorphXAI在检测性能上优于基线模型，并能提供结构化的生物学解释。

Conclusion: MorphXAI框架通过结合形态学监督，不仅提升了寄生虫检测的性能，还提供了结构化和生物学上有意义的解释，增强了模型的临床实用性。

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [108] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种直接在网络架构中整合拓扑特征的方法，显著提升了医学图像曲线结构分割的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像中曲线结构分割的拓扑特性（如连通性）对提高分割准确性和一致性至关重要，但现有方法依赖手工设计的损失函数，泛化能力差。

Method: 提出了PIs-Regressor模块，学习持久性图像（PI）作为拓扑特征的有限、可微分表示，并结合Topology SegNet在网络的下采样和上采样阶段融合这些特征。

Result: 实验结果表明，该方法在三个曲线结构基准测试中实现了最先进的性能，有效处理了医学图像中的过曝光和模糊等挑战。

Conclusion: 该论文提出了一种名为PIs-Regressor的模块，结合Topology SegNet框架，通过直接在网络架构中整合拓扑特征，而非依赖手工设计的损失函数，显著提升了医学图像中曲线结构分割的准确性和鲁棒性。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [109] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: Strip-Fusion网络通过时空融合和KL散度损失提升多光谱行人检测性能，尤其在遮挡和错位条件下表现突出。


<details>
  <summary>Details</summary>
Motivation: 多光谱行人检测方法主要关注空间融合而忽视时间信息，且RGB与热成像图像对可能存在错位问题。

Method: 提出了Strip-Fusion，一种空间-时间融合网络，采用时间自适应卷积动态权衡时空特征，并设计了KL散度损失以减少模态不平衡。

Result: 在KAIST和CVC-14基准测试中表现优异，尤其在遮挡和错位条件下显著改进。

Conclusion: Strip-Fusion网络在KAIST和CVC-14基准测试中表现优异，尤其在遮挡和错位等挑战性条件下显著优于现有方法。

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [110] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出LungCRCT框架，利用因果表示学习改进肺癌分析，实现高精度分类和因果干预能力。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期症状难以与其他呼吸系统疾病区分，导致早期诊断困难。现有深度学习方法在因果分析和可解释性上存在局限。

Method: 采用基于图自编码器的因果发现算法，结合距离相关性解缠和基于熵的图像重建优化。

Result: LungCRCT在恶性肿瘤分类任务中达到93.91%的AUC得分，并支持因果干预分析。

Conclusion: LungCRCT框架通过潜在因果表示学习，不仅支持肺癌治疗的因果干预分析，还在恶性肿瘤分类任务中实现了93.91%的AUC得分，表现出色且模型轻量。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [111] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间先验和动态学习机制的半监督高光谱分类框架，通过EASLP和DREPL模块优化标签传播和伪标签稳定性，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于高标注成本和样本有限，半监督学习面临边界标签扩散和伪标签不稳定的挑战，本文旨在解决这些问题。

Method: 论文提出了一个集成空间先验信息与动态学习机制的半监督高光谱分类框架，包括EASLP模块（结合边缘强度惩罚与邻域校正策略）、DHP方法（动态历史融合预测）和ATSC策略（自适应三方样本分类）。

Result: 在四个基准数据集上的评估表明，该框架能够保持优越的分类性能。

Conclusion: 该论文提出的动态可靠性增强伪标签框架（DREPL）与边缘感知超像素标签传播（EASLP）模块协同工作，实现了时空一致性优化，并在四个基准数据集上展示了优越的分类性能。

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [112] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: NaVIDA通过逆动力学增强和分层动作分块技术，解决了视觉与语言导航中动作与视觉变化的因果关系建模问题，提升了导航性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉与语言导航方法缺乏对动作如何因果影响后续视觉观察的显式建模，导致行为不稳定、泛化能力弱及轨迹误差累积。

Method: NaVIDA采用基于分块的逆动力学监督学习和分层概率动作分块（HPAC）技术，以建模视觉变化与动作间的因果关系，并通过熵引导机制自适应设置动作分块的执行范围。

Result: NaVIDA在导航性能上优于现有方法（参数更少，3B vs. 8B），并在真实机器人测试中表现优异。

Conclusion: NaVIDA框架通过结合策略学习与视觉动态建模及自适应执行，显著提升了视觉与语言导航任务的性能，并在实际机器人测试中验证了其可行性。

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [113] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出一种自监督跨域迁移框架，通过S2Former和FDC实现光谱-空间协同建模及频域一致性，结合DAFT蒸馏机制在低标签条件下实现高效迁移，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖源域标注且易受分布偏移影响，导致目标域泛化性能下降。本文旨在解决这一问题，提出无需源标签的自监督跨域迁移框架。

Method: 设计了空间-光谱Transformer（S2Former）模块，采用双分支结构并引入双向交叉注意力机制实现光谱-空间协同建模；提出频域约束（FDC）保持频域一致性；在微调阶段引入扩散对齐微调（DAFT）蒸馏机制。

Result: 在四个高光谱数据集上展示了稳定的分类性能和强大的跨域适应性。

Conclusion: 该论文提出的自监督跨域迁移框架在资源受限条件下表现出稳定的分类性能和强大的跨域适应性，验证了方法的有效性。

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [114] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: TPF是一种新型的任意形状文本检测方法，通过模拟带通滤波器直接分割文本，避免了传统方法的局限性，并结合REU和FPU提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于收缩-掩码扩展策略的文本检测方法在收缩操作中丢失了文本边缘的视觉特征，混淆了前景与背景差异，导致文本特征识别存在固有局限性。

Method: TPF通过模拟带通滤波器，为每个文本构建独特的特征-滤波器对，直接分割整个文本，避免了传统方法的局限性。REU和FPU分别用于增强特征一致性和区分前景与背景。

Result: 实验证明了REU和FPU的有效性，同时展示了TPF的优越性。

Conclusion: TPF（Text-Pass Filter）在任意形状文本检测中表现出色，通过REU和FPU的辅助，显著提升了文本特征识别的准确性和效率。

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [115] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: OrthoFoundation 是一种多模态视觉基础模型，通过自监督学习从大规模未标注数据中学习通用放射学表示，在肌肉骨骼影像任务中表现优异且具有跨解剖结构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 肌肉骨骼疾病是全球残疾的主要原因，当前 AI 方法依赖于任务特定的监督学习范式，缺乏通用性且需要大量标注数据。

Method: 利用 Dinov3 骨干网络，通过自监督对比学习训练模型，构建了包含 120 万未标注膝关节 X 光和 MRI 图像的预训练数据集。

Result: OrthoFoundation 在 14 项下游任务中达到最先进性能，在 X 光骨关节炎诊断和 MRI 结构损伤检测中表现优异，仅需 50% 标注数据即可匹配监督基线，并展现出跨解剖结构的泛化能力。

Conclusion: OrthoFoundation 代表了肌肉骨骼影像领域向通用人工智能迈出的重要一步，通过从大规模多模态数据中学习基础的、与关节无关的放射学语义，克服了传统模型的局限性，为减少临床实践中的标注负担和提高诊断准确性提供了强大框架。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [116] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: A zero-training framework for real-time Gaussian model applications, using analytic expressions and similarity measures, achieves low error rates in blur estimation.


<details>
  <summary>Details</summary>
Motivation: To enable real-time applications by introducing a zero-training forward computational framework for Gaussian models, addressing partial blur scenarios between images.

Method: The framework utilizes a discrete calculation of the analytic expression for defocused images, filtering multiple solutions via similarity measures over neighboring points.

Result: Achieves a mean absolute error (MAE) below 1.7% in estimating synthetic blur values and maintains intensity discrepancies under 2%.

Conclusion: The proposed framework effectively estimates synthetic blur values with low error rates, demonstrating its practical applicability in real-time scenarios.

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [117] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: Co-PLNet通过点线协作框架提升线框解析性能，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将线和交点分开预测后处理，导致不匹配和鲁棒性下降，需要一种更协同的方法。

Method: 提出Co-PLNet框架，包含Point-Line Prompt Encoder（PLP-Encoder）和Cross-Guidance Line Decoder（CGL-Decoder），通过空间提示和稀疏注意力机制实现点线一致性。

Result: 在Wireframe和YorkUrban数据集上，Co-PLNet在准确性和鲁棒性上均表现出持续改进，并具备实时效率。

Conclusion: Co-PLNet通过点线协作框架有效提升了线框解析的准确性和鲁棒性，同时保持了实时效率，适用于结构化几何感知任务。

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [118] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 研究探讨了深度图与RGB帧融合对视觉语言模型在长视域自我中心视频中空间推理能力的影响，发现其在提升特定任务性能的同时需权衡通用准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长视域自我中心视频中因视角漂移和缺乏持久几何上下文而导致的视觉导航挑战。

Method: 通过融合深度图与RGB帧，评估其对空间推理的影响，并在Sanpo-D数据集上对多个视觉语言模型进行导航导向的空间查询基准测试。

Result: 揭示了通用准确性与空间专业化之间的权衡关系，表明深度感知和空间基础表示能提升特定任务的性能。

Conclusion: 研究表明，深度感知和空间基础表示可以提升安全关键任务（如行人和障碍物检测）的性能，但需要在通用准确性和空间专业化之间权衡。

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [119] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 3DGesPolicy通过基于动作的框架和GAP融合模块，解决了整体手势生成中的语义和空间一致性问题，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成整体手势时存在语义不连贯的协调问题和空间不稳定的无意义运动，3DGesPolicy旨在解决这些问题。

Method: 3DGesPolicy是一种基于动作的框架，将整体手势生成重新定义为通过机器人学中的扩散策略解决连续轨迹控制问题。通过将帧间变化建模为统一的整体动作，该方法有效学习帧间整体手势运动模式，并确保空间和语义上一致的运动轨迹。此外，提出了Gesture-Audio-Phoneme（GAP）融合模块，深度整合和优化多模态信号。

Result: 3DGesPolicy在BEAT2数据集上表现出色，优于其他最先进方法，能够生成自然、富有表现力且高度语音对齐的整体手势。

Conclusion: 3DGesPolicy在BEAT2数据集上通过广泛的定量和定性实验证明了其在生成自然、富有表现力且高度语音对齐的整体手势方面的有效性。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [120] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA是一种轻量级视频异常检测模型，通过门控上下文聚合和远期一致性学习，在边缘设备上实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法依赖大规模模型且仅使用单帧预测误差，忽略了长期时间信息，限制了在边缘设备上的可行性。

Method: 提出了一种基于Unet的方法，通过连续帧特征提取生成即时和远期预测，并引入门控上下文聚合模块动态融合编码器和解码器特征。

Result: FoGA在实验中显著优于现有方法，运行速度高达155 FPS。

Conclusion: FoGA模型在性能和效率之间取得了优异的平衡，适用于资源有限的边缘设备。

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [121] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: Fair-Eye Net是一种多模态AI系统，通过优化公平性和临床可靠性，显著提升青光眼筛查和随访的准确性与公平性。


<details>
  <summary>Details</summary>
Motivation: 青光眼是全球不可逆失明的主要原因，当前筛查和进展评估依赖单一测试或松散关联的检查，存在主观性和碎片化护理问题，且高质量成像工具和专家资源有限。

Method: 开发了Fair-Eye Net，一种公平、可靠的多模态AI系统，采用双流异构融合架构，结合不确定性感知的分层门控策略进行选择性预测和安全转诊。

Result: 实验结果显示，Fair-Eye Net的AUC为0.912（特异性96.7%），将种族假阴性差异减少了73.4%（12.31%至3.28%），保持稳定的跨域性能，并提供3-12个月的早期风险预警（敏感性92%，特异性88%）。

Conclusion: Fair-Eye Net 通过多任务学习优化公平性和临床可靠性，为全球眼健康公平提供了可复制的临床转化和大规模部署路径。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [122] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent通过实体场景图和混合搜索，提升长时视频理解性能，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长时视频理解中的上下文窗口限制和多跳推理不足的问题。

Method: 采用基于实体场景图的规划代理框架，结合结构化搜索和跨模态推理工具。

Result: 在EgoLifeQA和Video-MME (Long)数据集上分别达到57.5%和74.1%的性能。

Conclusion: EGAgent框架通过实体场景图和混合搜索能力，在长时视频理解任务中实现了最先进的性能。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [123] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: 提出一种粗到细的2D-3D血管配准方法（SA-PnP + TempDiffReg），显著提升TACE手术导航精度，降低误差，助力临床操作。


<details>
  <summary>Details</summary>
Motivation: TACE手术中血管导航复杂且解剖结构多变，现有方法难以满足高精度配准需求，亟需一种更准确的2D-3D血管配准方法以指导手术操作。

Method: 提出了分阶段的配准策略：首先使用SA-PnP模块进行全局对齐，再通过TempDiffReg（一种时间扩散模型）迭代优化血管变形，以捕捉复杂解剖变化。

Result: 实验表明，该方法在配准精度（MSE 0.63 mm，MAE 0.51 mm）和解剖合理性上均优于现有技术，MSE降低66.7%，MAE降低17.7%。

Conclusion: 该方法通过粗到细的配准策略显著提升了TACE手术中的血管配准精度，为经验不足的临床医生提供了高效、安全的操作支持，有望改善手术效果和患者护理。

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [124] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: YOLO-DS通过双统计协同算子（DSO）和轻量级门控模块，显著提升异构物体检测性能，AP增益1.1%-1.7%，延迟几乎不变。


<details>
  <summary>Details</summary>
Motivation: 现有的YOLO检测器在共享特征通道中缺乏对异构物体响应的显式建模，限制了性能的进一步提升。

Method: 提出了一种新颖的双统计协同算子（DSO），并基于此设计了两个轻量级门控模块：双统计协同门控（DSG）模块用于自适应通道特征选择，多路径分段门控（MSG）模块用于深度特征加权。

Result: 在MS-COCO基准测试中，YOLO-DS在五个模型尺度（N, S, M, L, X）上均优于YOLOv8，AP增益为1.1%至1.7%，推理延迟仅轻微增加。

Conclusion: YOLO-DS通过引入DSO和两个轻量级门控模块（DSG和MSG），显著提升了YOLO系列在异构物体检测上的性能，同时在推理延迟上仅增加极小代价。

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [125] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: MPS-CLIP通过关键词引导的细粒度对齐和多视角表示模块，显著提升遥感图像-文本检索性能，计算高效且性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖粗粒度的全局对齐，忽略了遥感图像的多尺度密集语义，且全微调方法计算成本高并存在灾难性遗忘风险。

Method: 利用大型语言模型提取核心语义关键词，指导Segment Anything Model生成语义相关的子视角，并通过Gated Global Attention适配器和Multi-Perspective Representation模块优化模型。

Result: 在RSICD和RSITMD基准测试中，MPS-CLIP分别达到35.18%和48.40%的平均召回率（mR），显著优于基线和其他竞争方法。

Conclusion: MPS-CLIP通过关键词引导的细粒度对齐和多视角表示模块，显著提升了遥感图像-文本检索的性能，并在RSICD和RSITMD基准测试中达到最优效果。

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [126] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: SeNeDiF-OOD是一种层次化OOD检测框架，通过语义嵌套二分融合有效处理异构数据，在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界环境中异构OOD数据（从低级损坏到语义偏移）的检测难题，传统单阶段检测器难以应对。

Method: 提出了一种基于语义嵌套二分融合（SeNeDiF-OOD）的层次化框架，将检测任务分解为与不同语义抽象级别对齐的二元融合节点。

Result: 在MonuMAI实际应用中，SeNeDiF-OOD显著优于传统基线方法，能有效过滤多样OOD类别且保持分布内性能。

Conclusion: SeNeDiF-OOD 通过层次化的语义嵌套二分融合方法，显著提升了在开放世界环境中处理异构OOD数据的能力，并在实际应用中验证了其有效性。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


### [127] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: MindCine通过多模态学习和预训练EEG模型，解决了EEG到视频重建的单模态和数据稀缺问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: EEG的非侵入性和高时间分辨率使其在动态视觉感知重建中具有潜力，但现有方法因单模态和数据稀缺问题面临挑战。

Method: 采用多模态联合学习策略，结合预训练大型EEG模型解码语义信息，并设计带有因果注意力的Seq2Seq模型解码感知信息。

Result: 实验表明，MindCine在质量和数量上均优于现有方法，验证了多模态互补和大规模EEG模型的有效性。

Conclusion: MindCine框架通过多模态联合学习和预训练大型EEG模型，有效解决了EEG到视频重建中的单模态和数据稀缺问题，显著提升了重建质量。

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [128] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: QualiRAG 是一个无需训练的 RAG 框架，通过动态生成知识源和检索增强推理，显著提升视觉质量评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉质量评估方法依赖监督微调或强化学习，存在标注成本高和数据集偏差问题。QualiRAG 旨在利用大型多模态模型的潜在感知知识，避免任务特定训练。

Method: QualiRAG 采用检索增强生成（RAG）框架，动态生成四种互补知识源（视觉元数据、主题定位、全局质量摘要和局部质量描述），并通过相关性感知检索进行证据推理。

Result: 实验表明，QualiRAG 在视觉质量理解任务上优于开源通用 LMM 和 VQA 微调 LMM，并在质量比较任务中表现竞争力。

Conclusion: QualiRAG 是一个无需训练的框架，通过动态生成辅助知识源和相关性感知检索，显著提升了视觉质量理解的性能，并在质量比较任务中表现出色。

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [129] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: HomoFM利用流匹配技术和领域适应策略，显著提升单应性估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂几何变换或跨领域泛化时表现不佳，HomoFM旨在通过流匹配技术和领域适应策略解决这些问题。

Method: 提出HomoFM框架，将流匹配技术首次引入单应性估计任务，通过建模连续点速度场实现高精度变换恢复，并集成梯度反转层（GRL）以学习领域不变表示。

Result: 实验表明HomoFM在标准基准测试中，无论是估计准确性还是鲁棒性均优于现有方法。

Conclusion: HomoFM通过引入流匹配技术和领域适应策略，显著提升了单应性估计的准确性和鲁棒性，超越了现有最先进方法。

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [130] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 提出轻量级EfficientNetB2模型，通过优化策略在FER-2013上实现高效情绪识别，适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中面部情绪识别因图像质量低、光照变化、姿态变化、背景干扰、类间差异小、标注噪声及严重类别不平衡等问题，同时降低计算和内存开销。

Method: 使用EfficientNetB2架构，结合AdamW优化、解耦权重衰减、标签平滑（ε=0.06）、裁剪类别权重等技术，并采用分层训练-验证分割（87.5%/12.5%）。

Result: 在FER-2013数据集上达到68.78%的测试准确率，参数数量比VGG16基线少近十倍，表现出稳定的训练和强泛化能力。

Conclusion: 该论文提出了一种基于EfficientNetB2的轻量级高效面部情绪识别流程，通过两阶段预热和微调策略，结合多种优化技术，显著提升了模型性能，适合实时和边缘计算应用。

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [131] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: V-Loop是一种无需训练的框架，通过逻辑闭环验证医学VQA中答案的事实正确性，显著减少幻觉，且高效兼容现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在医学VQA中存在幻觉问题（即输出与视觉事实矛盾），现有不确定性方法间接且不直接验证答案正确性。

Method: 提出Visual Logical Loop Verification（V-Loop），一种无需训练、即插即用的框架，通过双向推理形成视觉逻辑闭环验证答案的事实正确性。

Result: 在多个医学VQA基准测试中，V-Loop表现优于现有自省方法，保持高效，并能与不确定性方法结合提升效果。

Conclusion: V-Loop框架在医学视觉问答（VQA）中有效检测幻觉，显著优于现有方法，且能与不确定性方法结合进一步提升性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [132] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的框架，通过单张2D深度图像预测内部器官3D位置和形状，实验验证了其在放射学工作流程中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 自动化患者定位在优化扫描程序和提高患者吞吐量方面具有重要作用，利用RGB-D相机捕获的深度信息为估计内部器官位置提供了有前景的方法。

Method: 利用大规模全身MRI扫描数据集合成深度图像与解剖分割配对，训练统一的卷积神经网络架构，无需显式表面重建即可准确定位多种解剖结构。

Result: 实验结果表明，该方法能够准确识别包括骨骼和软组织在内的多种解剖结构。

Conclusion: 本文提出了一种基于学习的框架，通过单张2D深度图像直接预测多个内部器官的3D位置和形状，展示了将深度传感器集成到放射学工作流程中以优化扫描程序和提升患者体验的潜力。

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [133] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: The paper reviews aerial image classification methods and proposes Aerial-Y-Net, a spatial attention-enhanced CNN, which achieves 91.72% accuracy on the AID dataset.


<details>
  <summary>Details</summary>
Motivation: Aerial images are heterogeneous and complex, making robust scene classification challenging. This study aims to address this challenge by reviewing existing methods and proposing an improved model.

Method: The study reviews various machine learning methods for aerial image classification, including handcrafted features (e.g., SIFT, LBP), traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. It then proposes Aerial-Y-Net, which incorporates spatial attention and multi-scale feature fusion.

Result: Aerial-Y-Net achieves 91.72% accuracy on the AID dataset, outperforming several baseline architectures.

Conclusion: Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, demonstrates superior performance in aerial image classification, achieving 91.72% accuracy on the AID dataset.

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [134] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 本文提出CAP和CWAP机制，通过结合实例中心和类别标签信息优化LiDAR点云到2D图像的投影，提升了语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D点云到2D范围图像的投影中仅保留深度最小的点，忽略了语义相关性和对象结构，导致重要上下文信息丢失。

Method: 提出了两种机制：Centerness-Aware Projection (CAP)和Class-Weighted-Aware Projection (CWAP)。CAP根据点与实例中心的距离调整深度，优先保留中心点；CWAP通过用户定义的权重优先处理特定类别。

Result: 在SemanticKITTI数据集上的评估显示，CAP保留了更多实例点，mIoU提升了3.1%；CWAP对目标类别的性能有显著提升，对其他类别影响可忽略。

Conclusion: 本文提出的CAP和CWAP机制通过结合实例中心和类别标签的上下文信息，有效解决了3D LiDAR点云到2D范围图像投影中的多对一冲突问题，显著提升了语义分割性能。

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [135] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 研究提出SwipeGen流程和GUISwiper代理，显著提升GUI代理的滑动交互能力，准确率提高214%。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在处理滑动交互时采用过于简化的策略，无法准确模拟人类行为，这成为任务完成的新瓶颈。

Method: 通过分解人类滑动手势为多个可量化维度，并提出了自动化流程SwipeGen来合成类似人类的滑动交互。基于此流程，构建并发布了首个评估GUI代理滑动执行能力的基准。

Result: GUISwiper的滑动执行准确率为69.07%，比现有VLM基线提高了214%。

Conclusion: GUISwiper显著提升了GUI代理的滑动交互执行能力，其准确率达到了69.07%，比现有VLM基线提高了214%。

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [136] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: EDSH框架通过结合DenseNet和Swin Transformer，高效分析脑肿瘤MRI，在四类肿瘤分类中达到98.50%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤MRI分析中类特异性诊断挑战，如不规则形状、边界模糊的弥漫性胶质瘤和位置明确的脑膜瘤/垂体瘤分类问题。

Method: 提出EDSH框架，包含两个实验设置：1) Boosted Feature Space (BFS) 结合DenseNet和Swin分支学习互补的局部和全局特征；2) 分层DenseNet-Swin架构（DFE和DR）通过双残差连接抑制假阴性。DenseNet和Swin Transformer分别针对MRI空间特征和任务进行了定制化设计。

Result: 在40,260张MRI图像的大规模数据集上，EDSH框架在测试集上达到98.50%的准确率和召回率，优于单独使用CNN、Vision Transformer或其他混合模型。

Conclusion: EDSH框架在脑肿瘤MRI分析中表现出色，通过结合DenseNet和Swin Transformer的优势，实现了高精度和召回率（98.50%），显著优于现有方法。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [137] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: NRVBench是首个评估非刚性视频编辑的基准测试，包含数据集、评估指标和基线方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本驱动视频编辑取得了显著进展，但生成连贯的非刚性变形仍面临物理失真和时间闪烁的挑战。

Method: 提出了NRVBench基准测试，包括高质量数据集、NRVE-Acc评估指标和训练免费基线VM-Edit。VM-Edit采用双区域去噪机制实现结构感知控制。

Result: 实验表明，当前方法在保持物理合理性方面存在不足，而本文方法在标准和提出的指标上均表现出色。

Conclusion: NRVBench作为首个专注于非刚性视频编辑的基准测试，有望成为推动物理感知视频编辑发展的标准测试平台。

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [138] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: Q-Bench-Portrait 是首个针对肖像图像质量感知的基准测试，评估了 25 个 MLLMs，发现其能力有限，呼吁未来研究改进。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在通用图像上表现优异，但在肖像图像这一具有独特结构和感知特性的领域尚未充分探索。

Method: 提出了 Q-Bench-Portrait 基准，包含 2,765 个图像-问题-答案三元组，涵盖多样化的肖像图像来源、全面的质量维度和多种问题格式。

Result: 评估了 25 个开源和闭源 MLLMs，发现其肖像图像感知能力有限且不精确，与人类判断存在明显差距。

Conclusion: Q-Bench-Portrait 是首个专注于肖像图像质量感知的基准测试，揭示了当前 MLLMs 在肖像图像感知上的局限性，并希望促进未来研究。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [139] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS是首个全自动管道，用于从常规MRI中量化内淋巴积水，通过深度学习分割和容积工作流程实现高效、一致的结果，显著优于现有临床软件。


<details>
  <summary>Details</summary>
Motivation: 开发OREHAS的目的是为了从常规3D-SPACE-MRC和3D-REAL-IR MRI中实现内淋巴积水（EH）的容积量化，消除手动干预的需求。

Method: OREHAS整合了三个组件——切片分类、内耳定位和序列特定分割——形成一个单一工作流程，直接从整个MRI容积计算每耳内淋巴至前庭容积比（ELR）。

Result: OREHAS在外部验证队列中与专家手动标注结果高度一致（VSI = 74.3%），显著优于临床syngo.via软件（VSI = 42.5%），并且在19名测试患者中，内淋巴容积更小且更符合生理现实。

Conclusion: OREHAS提供了一种可靠且可重复的内耳内淋巴积水量化方法，通过结合高效的深度学习分割和临床对齐的容积工作流程，减少了操作依赖性，确保了方法的一致性。

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [140] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: 提出结合HMD运动和视觉显著性的注视预测框架，在无直接眼动追踪时提升VR交互体验。


<details>
  <summary>Details</summary>
Motivation: 解决因硬件限制或隐私问题导致的直接眼动追踪不可用问题，提升VR应用中注视预测的准确性。

Method: 采用UniSal轻量级显著性编码器提取视觉特征，融合HMD运动数据，并通过时间序列预测模块（TSMixer和LSTM）预测未来注视方向。

Result: 在EHTask数据集及商用VR硬件上的实验表明，该方法在减少感知延迟和提升交互体验方面优于基线模型（如Center-of-HMD和Mean Gaze）。

Conclusion: 该论文提出的结合HMD运动信号和视觉显著性线索的注视预测框架，在直接眼动追踪受限的情况下，有效减少了感知延迟并提升了VR环境的自然交互体验。

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [141] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 提出一种基于网格引导信号的水印方法，通过Radon变换估计几何变换，有效解决裁剪导致的同步问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法对裁剪操作缺乏鲁棒性，导致水印同步困难，因此需要一种能够准确估计几何变换的方法。

Method: 通过嵌入具有独特水平和垂直值的网格形状引导信号，并利用Radon变换分析图像的失真情况，估计网格的角度和间隔，从而确定变换矩阵。

Result: 实验结果表明，该方法在单次和复合攻击下均能准确估计变换矩阵，误差较低。

Conclusion: 该论文提出的水印方法通过使用网格形状的引导信号，能够准确估计几何变换矩阵，即使在裁剪后也能实现同步，具有较高的鲁棒性。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [142] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: ARMOR框架通过VLM和LLM动态协调多种攻击方法，显著提升对抗攻击的适应性和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化攻击套件缺乏战略适应性和语义感知的问题，提升对抗攻击的灵活性和效果。

Method: ARMOR框架利用VLM引导的代理协同生成扰动，通过LLM实时调参，结合CW、JSMA和STA三种攻击方法，采用混合策略（Mixing Desk）优化攻击效果。

Result: 在标准基准测试中，ARMOR实现了更好的跨架构迁移效果，并能可靠地欺骗目标模型，通过置信度和SSIM评分选择最佳攻击策略。

Conclusion: ARMOR框架通过动态协调和语义感知，显著提升了对抗攻击的适应性和效果，尤其在跨架构迁移和白盒目标攻击中表现优异。

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [143] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: kViT 是一种直接在 k-Space 数据上分类的复数视觉变换器，计算高效且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在 MRI 中仅利用重建后的幅度图像，丢弃了相位信息且计算成本高，且标准神经网络架构不适合处理 k-Space 数据的全局性。

Method: 采用径向 k-Space 分块策略，设计了适用于 k-Space 数据的复数视觉变换器（kViT）。

Result: kViT 在 fastMRI 和内部数据集上表现出与图像域基准方法（ResNet、EfficientNet、ViT）相当的分类性能，同时对高加速因子表现出更强的鲁棒性，训练时 VRAM 消耗减少高达 68 倍。

Conclusion: kViT 提出了一种直接在 k-Space 数据上执行分类的新方法，显著提升了计算效率并减少 VRAM 消耗，为直接利用扫描仪数据的 AI 分析开辟了新途径。

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [144] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: 该研究提出了一种流式处理架构和DSL，用于优化大规模图像分析的I/O性能，显著提升处理效率。


<details>
  <summary>Details</summary>
Motivation: 针对大规模图像分析（如1.4 PB电子显微镜数据或150 TB人体器官图谱）中I/O性能瓶颈的问题，研究如何通过流式处理架构和优化数据访问模式来提升处理效率。

Method: 提出了基于流式处理的架构，结合扫掠执行、窗口操作和重叠感知分块，最小化冗余访问。并开发了一个领域特定语言（DSL），用于编码算法并优化流式处理和内存使用。

Result: 提出的流式处理架构和DSL能够实现近线性的I/O扫描和可预测的内存占用，显著提升了大规模图像处理的吞吐量。

Conclusion: 通过引入领域特定语言（DSL）和流式处理架构，该研究成功解决了大规模图像分析中的I/O瓶颈问题，显著提升了处理效率，同时保持内存使用可控。

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [145] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: 该论文比较了三种深度学习模型在儿童绘画情感分类中的表现，揭示了轻量级与深度架构在移动和实时应用中的权衡。


<details>
  <summary>Details</summary>
Motivation: 早期儿童情感状态的识别具有挑战性，传统方法通常具有侵入性、主观性或难以一致应用。该研究旨在通过机器学习模型从儿童绘画中识别情感状态。

Method: 使用迁移学习在由心理专家标注的儿童绘画数据集上训练和评估三种深度学习模型（MobileNet、EfficientNet、VGG16），分析分类性能、鲁棒性和计算效率。

Result: 研究结果展示了轻量级和深度架构在基于绘画的情感计算任务中的性能权衡，特别是在移动和实时应用场景中。

Conclusion: 该论文通过比较三种深度学习架构（MobileNet、EfficientNet和VGG16）在儿童绘画情感分类任务中的表现，揭示了轻量级与深度架构在移动和实时应用中的权衡。

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [146] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 研究发现GPA预处理可能污染ML模型，提出新对齐方法消除依赖性，并揭示了地标空间与样本量的关系。


<details>
  <summary>Details</summary>
Motivation: 标准实践中，所有样本在分割为训练集和测试集前通过广义Procrustes分析（GPA）对齐，可能导致统计依赖性和下游预测模型的污染。

Method: 通过控制2D和3D模拟实验，研究在不同样本量、地标密度和异速生长模式下的GPA诱导污染效应，并提出一种新的对齐方法，即将测试样本与训练集对齐。

Result: 模拟实验揭示了样本量与地标空间之间的稳健“对角线”关系，并展示了地标间空间自相关的重要性。新提出的对齐方法有效消除了跨样本依赖性。

Conclusion: 本研究强调了在几何形态测量学（GMM）与机器学习（ML）结合应用中预处理的重要性，提出了新的对齐方法，并阐明了Procrustes形状空间固有的统计约束。

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [147] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: DisasterInsight是一个多模态基准，用于评估灾害分析任务中的视觉语言模型，DI-Chat通过LoRA微调在损害分类和报告生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言基准主要关注粗粒度标签和图像级识别，忽视了实际人道主义工作流程中的功能理解和指令鲁棒性。

Method: 通过参数高效的Low-Rank Adaptation (LoRA)对现有VLM主干进行微调，构建了DI-Chat，并在DisasterInsight基准上进行了广泛实验。

Result: DI-Chat在损害级别和灾害类型分类以及报告生成质量方面表现显著提升，但建筑功能分类对所有评估模型仍具挑战性。

Conclusion: DisasterInsight提供了一个统一的基准，用于研究灾害图像中的多模态推理，DI-Chat在损害级别和灾害类型分类以及报告生成质量方面取得了显著改进。

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [148] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 提出结合基础模型嵌入与聚类的冷启动策略及不确定性主动学习框架，显著提升分割准确性，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动标注分割注释耗时且需要专业知识，成为疾病监测的主要瓶颈，主动学习通过优先标注信息量大的样本来减轻这一负担。

Method: 结合基础模型嵌入与聚类的新型冷启动采样策略，包括自动选择聚类数量和按比例采样，构建多样且具代表性的初始训练集，随后采用基于不确定性的主动学习框架，整合空间多样性以指导样本选择。

Result: 在三个数据集（X光和MRI模态）上评估，冷启动策略优于随机选择，显著提升Dice分数并减少Hausdorff距离；主动学习进一步提升了性能。

Conclusion: 提出的框架在低数据量情况下持续优于基线方法，提升了分割准确性。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [149] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent通过代理框架分离视觉理解与生成能力，采用两阶段训练策略显著提升生成模型性能，并具备跨工具泛化等优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有统一模型训练成本高且理解与生成能力难以兼顾的问题，同时避免模块化系统的静态流程限制。

Method: 采用两阶段训练策略：先通过监督微调冷启动代理行为，再通过端到端代理强化学习结合点对和成对奖励进行优化。

Result: GenAgent在GenEval++和WISE基准上分别提升了23.6%和14%的性能。

Conclusion: GenAgent通过代理框架显著提升了生成模型的性能，并展示了跨工具泛化、测试时扩展和任务自适应推理三大特性。

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [150] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: REMAC是一种高效的火星图像压缩方法，通过转移计算复杂度和利用图像间相似性，显著降低编码器负担并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法在火星图像压缩中存在两个关键问题：1）忽视了火星上计算资源的极度有限性；2）未利用火星图像间强烈的相似性来提升压缩性能。

Method: 提出了一种基于参考的火星不对称图像压缩方法（REMAC），包括参考引导的熵模块和ref-decoder，利用参考图像中的有用信息减少编码器的冗余操作。

Result: 实验结果表明，REMAC将编码器复杂度降低了43.51%，同时BD-PSNR增益达到0.2664 dB。

Conclusion: REMAC方法通过将计算复杂度从编码器转移到资源丰富的解码器，并利用图像间的相似性，显著降低了编码器的复杂度，同时提升了压缩性能。

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [151] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本研究验证了MRI与X射线在FAI筛查中的等效性，支持将自动化FAI评估整合到常规MRI工作流程中。


<details>
  <summary>Details</summary>
Motivation: 传统的FAI筛查依赖于X射线上的角度测量，但评估撞击区域的高度和范围需要MRI扫描的3D视图。本研究旨在验证MRI在定位和诊断准确性方面是否与X射线等效。

Method: 采用标准的热图回归架构进行匹配队列验证研究（89名患者，配对MRI/X射线），评估跨模态临床等效性。

Result: MRI在cam型撞击的定位和诊断准确性方面与X射线等效，展示了在3D MRI体积冠状视图中进行FAI评估的临床可行性。

Conclusion: 本研究支持将自动化FAI评估整合到常规MRI工作流程中，为3D MRI体积的冠状视图中的FAI评估提供了临床可行性。

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [152] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: SDA-QEC框架通过扩散增强和量子分类技术，有效解决医学图像分类中的类别不平衡问题，实现高准确率和平衡性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据集中严重的类别不平衡问题，提高少数类别的召回率，减少临床误诊风险。

Method: 提出SDA-QEC框架，结合简化的扩散增强技术和量子增强特征分类，通过生成高质量合成样本和量子特征层增强模型判别能力。

Result: 在冠状动脉造影图像分类任务中，SDA-QEC达到98.33%准确率、98.78% AUC和98.33% F1分数，显著优于传统基线模型。

Conclusion: SDA-QEC框架通过结合简化的扩散增强和量子增强分类，在医学图像分类任务中实现了高准确率和平衡的性能，为小样本、高不平衡和高风险的诊断场景提供了新的研究方向。

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [153] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级AI卫星边缘计算框架，通过两阶段像素级标签传播实现高光谱图像分类，解决了传输瓶颈和图像质量问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像卫星在地球观测系统中具有重要作用，但在灾害监测和应急测绘等需要快速响应的应用中，卫星下行传输速度成为主要瓶颈。

Method: 采用轻量级的非深度学习框架，结合少样本学习策略，并开发了一种新颖的两阶段像素级标签传播方案，利用单像素级别的固有光谱特征，无需考虑空间结构信息。

Result: 提出的方法通过两阶段标签传播方案和基于秩约束的图聚类算法，有效解决了传感器故障和扫描模式错误导致的图像质量下降问题。

Conclusion: 本文提出了一种高效的人工智能卫星边缘计算范式，用于高光谱图像分类，解决了卫星下行传输速度瓶颈问题，并实现了卫星的自主决策能力。

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [154] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出自细化视频采样方法，利用预训练生成器作为自细化器，通过迭代内循环和不确定性感知策略提升视频生成质量，无需额外训练或外部验证器。


<details>
  <summary>Details</summary>
Motivation: 现代视频生成器在复杂物理动态上表现不佳，现有方法依赖外部验证器或增强数据训练，计算成本高且难以捕捉细粒度运动。

Method: 利用预训练的视频生成器作为自细化器，通过将其解释为去噪自编码器，在推理时实现迭代内循环细化，并引入基于自一致性的不确定性感知细化策略。

Result: 实验表明，该方法在运动连贯性和物理对齐性上显著提升，获得超过70%的人类偏好。

Conclusion: 通过自细化视频采样方法，显著提升了视频生成器的运动连贯性和物理对齐性，无需外部验证器或额外训练。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [155] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: AGSP-DSA框架通过双图构建、谱图滤波和多尺度GCNs，实现了异构多模态数据的高效融合，在多个任务和数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构多模态数据（如文本、音频、图像）融合中的鲁棒性问题，提出了一种自适应图信号处理与动态语义对齐框架。

Method: 采用双图构建学习模态内和模态间关系，结合谱图滤波增强信息信号，以及多尺度图卷积网络（GCNs）进行节点嵌入，并引入语义感知注意力机制动态调整各模态贡献。

Result: 在CMU-MOSEI、AVE和MM-IMDB三个基准数据集上，AGSP-DSA达到了最先进的性能，具体表现为95.3%准确率、0.936 F1分数和0.924 mAP（CMU-MOSEI），并在缺失模态场景中展现了良好的泛化能力。

Conclusion: AGSP-DSA框架在多模态数据融合中表现出色，特别是在情感分析、事件识别和多媒体分类任务中验证了其高效性和鲁棒性。

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [156] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR是一种新型检测框架，通过动态频率-空间协同和高效语义提取，显著提升了无人机图像中小物体检测的性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中小物体检测因特征表示有限和多尺度融合效果不佳而具有挑战性，现有方法未能充分利用频率信息且依赖静态卷积操作。

Method: EFSI-DETR包含动态频率-空间统一协同网络（DyFusNet）和高效语义特征集中器（ESFC），结合细粒度特征保留策略（FFR），实现了多尺度特征融合和深层语义提取。

Result: 在VisDrone基准测试中，AP和APs分别提升了1.6%和5.8%，推理速度达到188 FPS。

Conclusion: EFSI-DETR通过动态频率-空间协同网络和高效语义特征提取器，显著提升了无人机图像中小物体检测的性能和实时性，在VisDrone和CODrone基准测试中达到了最先进水平。

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [157] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 该论文提出了一种尺度感知的自监督学习适应方法，通过小窗口裁剪增强预训练，显著提升了小、稀疏或局部不规则对象的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习流程在分割大型、均匀区域时表现良好，但在处理小、稀疏或局部不规则对象时性能下降。

Method: 提出了一种尺度感知的自监督学习适应方法，将小窗口裁剪集成到增强流程中，在预训练期间聚焦于精细尺度结构。

Result: 在两种不同数据模态（地震成像和神经成像）中，该方法在标签限制下相比标准和最先进基线均取得一致改进，断层分割准确率提升高达13%，细胞描绘提升5%。

Conclusion: 该研究强调了自监督学习（SSL）设计需与目标对象的大小和稀疏性对齐，为科学成像领域构建更有效的表示学习流程提供了通用原则。

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [158] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: 论文提出了一种改进的跨模态图像翻译方法，通过动态域转移和局部残差校正，提升了效果和效率。


<details>
  <summary>Details</summary>
Motivation: 标准扩散方法依赖单一的全局线性转移，导致采样器需要遍历非流形的高成本区域，增加了校正负担并引发语义漂移。

Method: 模型在每一步反向过程中预测空间变化的混合场，并将明确的目标一致恢复项注入漂移中，保持了大规模更新在流形上，并将模型的作用从全局对齐转向局部残差校正。

Result: 在医学影像、遥感和电致发光语义映射等翻译任务中，该框架提高了结构保真度和语义一致性，同时收敛速度更快。

Conclusion: 该论文提出了一种改进的跨模态图像翻译方法，通过嵌入域转移动态到生成过程中，显著提升了结构保真度和语义一致性，同时减少了去噪步骤。

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [159] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER 是一个两阶段框架，通过增强跨模态对齐和自适应查询优化，显著提升了基于文本的人物搜索性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于文本的人物搜索（TBPS）中跨模态差异和模糊用户查询的挑战。

Method: CONQUER 采用两阶段框架，包括训练时的多粒度编码、互补对挖掘和基于最优传输的上下文引导匹配，以及推理时的即插即用查询增强模块。

Result: 在 CUHK-PEDES、ICFG-PEDES 和 RSTPReid 数据集上，CONQUER 在 Rank-1 准确率和 mAP 上均显著优于基线方法。

Conclusion: CONQUER 是一种实用且有效的解决方案，适用于现实世界中的基于文本的人物搜索（TBPS）部署。

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [160] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait 是一种基于高斯泼溅的方法，无需3D监督即可生成高质量3D说话头部动画，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有3D说话头部生成方法依赖领域特定启发式方法，导致重建不准确，影响动画真实感。

Method: 基于高斯泼溅的方法，自动将单张肖像图像分解为静态3D重建和预测的2D背景，并通过音频输入生成自然唇部运动。

Result: 实验结果表明，Splat-Portrait 在说话头部生成和新视角合成方面优于先前工作，视觉质量更高。

Conclusion: Splat-Portrait 在3D头部重建和唇部运动合成方面表现出色，无需3D监督或标志点即可生成高质量动画。

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [161] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 研究发现文本到视频模型Sora 2在全球视觉知识表达上地理偏见较弱，表明其具备全球部署潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨文本到视频生成模型是否具备地理公平性及地理视觉知识，以验证其在全球范围内的适用性。

Method: 提出了Geo-Attraction Landmark Probing (GAP)框架，结合全球结构对齐、细粒度关键点对齐和视觉语言模型判断等多维度指标，并构建了GEOATTRACTION-500基准数据集进行评估。

Result: Sora 2模型在区域、发展水平和文化群体中表现出相对均衡的地理视觉知识，仅弱依赖于景点知名度。

Conclusion: 当前文本到视频生成模型（如Sora 2）在全球视觉知识表达上表现出相对均衡的地理分布，挑战了常见的地理偏见假设，并展示了其在全球应用中的潜力。

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [162] [Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models](https://arxiv.org/abs/2601.17295)
*Xinyu Zhu,Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: AST-Masking通过结构感知微调显著提升SQL生成准确性，FLAN-T5和Gemma模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在动态和延迟敏感网络中的适应性不足，且缺乏结构化领域知识，限制了泛化和可解释性。

Method: 引入了AST-Masking，一种结构感知的微调方法，利用SQL AST为关键组件分配权重，实现语法感知学习。

Result: FLAN-T5达到99.6%的执行准确率，Gemma从7.5%提升至72.0%。

Conclusion: AST-Masking方法显著提高了SQL生成的准确性，特别是在FLAN-T5和Gemma模型中表现突出，验证了结构感知微调在可解释SFC编排中的有效性。

Abstract: Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration.

</details>


### [163] [Efficient Self-Learning and Model Versioning for AI-native O-RAN Edge](https://arxiv.org/abs/2601.17534)
*Mounir Bensalem,Fin Gentzen,Tuck-Wai Choong,Yu-Chiao Jhuang,Admela Jukan,Jenq-Shiou Leu*

Main category: cs.NI

TL;DR: 本文提出了一种自学习框架，用于AI原生O-RAN边缘的高效闭环版本管理，通过强化学习决策保证服务质量并平衡多维度性能。


<details>
  <summary>Details</summary>
Motivation: 6G的AI原生愿景要求无线接入网络训练、部署并持续优化数千个机器学习模型，以实现实时无线网络优化。尽管O-RAN架构提供了开放接口和智能控制平面，但未明确这些模型的生命周期管理。

Method: 提出了一种自学习框架，该框架通过中央/区域云的训练管道持续生成新模型，并将这些模型与其资源占用、安全评分和准确性指标一起编目到共享版本库中。更新管理器根据这些信息应用自学习策略，决定何时何地推广每个新模型版本。容器编排器随后在异构工作节点上实现这些决策。

Result: 模拟结果表明，高效的强化学习驱动决策能够保证服务质量，限制延迟，同时平衡模型准确性、系统稳定性和弹性。

Conclusion: 通过模拟结果验证，基于强化学习的决策框架能够在保证服务质量、限制延迟的同时，平衡模型准确性、系统稳定性和弹性。

Abstract: The AI-native vision of 6G requires Radio Access Networks to train, deploy, and continuously refine thousands of machine learning (ML) models that drive real-time radio network optimization. Although the Open RAN (O-RAN) architecture provides open interfaces and an intelligent control plane, it leaves the life-cycle management of these models unspecified. Consequently, operators still rely on ad-hoc, manual update practices that can neither scale across the heterogeneous, multi-layer stack of Cell-Site, Edge-, Regional-, and Central-Cloud domains, nor across the three O-RAN control loops (real-, near-real-, and non-real-time). We present a self-learning framework that provides an efficient closed-loop version management for an AI-native O-RAN edge. In this framework, training pipelines in the Central/Regional Cloud continuously generate new models, which are cataloged along with their resource footprints, security scores, and accuracy metrics in a shared version repository. An Update Manager consults this repository and applies a self-learning policy to decide when and where each new model version should be promoted into operation. A container orchestrator then realizes these decisions across heterogeneous worker nodes, enabling multiple services (rApps, xApps, and dApps) to obtain improved inference with minimal disruption. Simulation results show that an efficient RL-driven decision-making can guarantee quality of service, bounded latencies while balancing model accuracy, system stability, and resilience.

</details>


### [164] [Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control](https://arxiv.org/abs/2601.18069)
*Haoyuan Pan,Sizhao Chen,Zhaorui Wang,Tse-Tin Chan*

Main category: cs.NI

TL;DR: 本文研究了多用户状态更新系统中的平均和尾部风险敏感的VAoI调度，提出了D2SAC和RS-D3SAC算法，后者通过分布型评论家和扩散型行动者显著降低了尾部风险。


<details>
  <summary>Details</summary>
Motivation: 现有的VAoI调度方法主要关注最小化平均VAoI，忽视了罕见但严重的陈旧事件，这些事件在随机数据包到达和不可靠信道下可能损害系统可靠性。

Method: 本文首先将平均VAoI最小化问题建模为约束马尔可夫决策过程，并提出了基于深度扩散的Soft Actor-Critic（D2SAC）算法。随后，进一步提出了RS-D3SAC算法，结合了扩散型行动者和基于分位数的分布型评论家，显式建模了完整的VAoI回报分布。

Result: 广泛的仿真表明，D2SAC降低了平均VAoI，而RS-D3SAC在不牺牲平均性能的情况下，持续实现了CVaR的显著降低。

Conclusion: 本文提出的RS-D3SAC算法在满足长期传输成本约束的同时，显著降低了CVaR，展现了在多用户无线系统中实现稳健且风险感知的VAoI调度的有效性。

Abstract: Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.

</details>


### [165] [Accelerating Update Broadcasts Over LoRaWAN Downlink via D2D Cooperation](https://arxiv.org/abs/2601.18134)
*Anshika Singh,Siddhartha S. Borkotoky*

Main category: cs.NI

TL;DR: Proposes a cooperative broadcast method for LoRaWANs, cutting update delivery time from 42h to 45min for 10KB updates.


<details>
  <summary>Details</summary>
Motivation: Existing broadcast techniques in LoRaWANs suffer from long delivery delays due to low data rates and duty-cycle constraints.

Method: A device-level cooperative mechanism where updated end devices broadcast update fragments to neighbors.

Result: The scheme reduces delivery time from 42 hours to 45 minutes for a 10KB update in a 400-node network.

Conclusion: The proposed cooperative mechanism significantly reduces update delivery time in LoRaWANs, enhancing security and edge intelligence efficiency.

Abstract: Broadcast distribution of updates (e.g., security patches, machine learning models) from a server to end devices (EDs) is a critical requirement in the Internet of Things (IoT). In this paper, we consider the problem of reliable over-the-air broadcast of updates in Long Range Wide Area Networks (LoRaWANs). Existing broadcast techniques for LoRaWANs suffer from long delivery delays due to low data rates and duty-cycle constraints. We address this problem by proposing a device-level cooperative mechanism, in which updated EDs broadcast a few update fragments to accelerate delivery to their neighbors. We demonstrate large reductions in the delivery time compared to conventional methods. For instance, in a 400-node network spanning 1 km radius and operating at 1% duty-cycle, the proposed scheme reduces the time required to deliver a 10 kilobyte update to an ED at the network's edge from 42 hours to 45 minutes. The proposed solution thus provides a pathway toward improved security and efficient realization of edge intelligence in LoRaWAN IoT.

</details>


### [166] [Contact Plan Design For Optical Interplanetary Communications](https://arxiv.org/abs/2601.18148)
*Jason Gerard,Juan A. Fraire,Sandra Cespedes*

Main category: cs.NI

TL;DR: 论文提出首个PAT感知的CPD框架，用于光学星际回程网络，准确建模重定向延迟。结果显示，MILP调度器比贪婪算法提升30%以上容量，最优调度倾向于更少但更长的光学链路。


<details>
  <summary>Details</summary>
Motivation: 空间探索任务产生的科学遥测数据量迅速增长，远超当前手动调度的RF基础架构容量。自由空间光学（FSO）通信虽能提供更高吞吐量，但其窄波束需要精确的指向、捕获和跟踪（PAT）以建立链路，且需严格同步的接触计划。现有接触计划设计（CPD）框架未考虑光学头重定向延迟，这是光学网络特有的主要损害。

Method: 论文提出了一种首个PAT感知的CPD框架，用于光学星际回程网络。该模型捕捉了直接对地光学链路和两跳中继路径上的定向时间流，使用了延迟/中断容忍网络（DTN）卫星。

Result: 结果表明，MILP调度器比贪婪算法提供超过30%的网络容量提升。更重要的是，结果揭示了一个基本行为转变：准确建模重定向延迟时，最优调度倾向于更少但更长的光学链路。

Conclusion: 该论文的结论是，当准确建模重定向延迟时，最优调度倾向于选择更少但更长的光学链路，以最大化吞吐量同时最小化重定向开销。零延迟假设会显著高估可实现性能并产生不切实际的接触计划。

Abstract: Space exploration missions generate rapidly increasing volumes of scientific telemetry that far exceed the capacity of today's manually scheduled, RF-based deep-space infrastructure. Free-space optical (FSO) communications promise orders of magnitude higher throughput, but their narrow beams require precise pointing, acquisition, and tracking (PAT) for link establishment and tightly synchronized contact schedules. Critically, no existing contact plan design (CPD) framework accounts for optical head retargeting delay, the time spent during coarse pointing and link acquisition before data transmission begins, which directly reduces usable contact time. Retargeting delay is the dominant impairment unique to optical networks, which induces a seconds-to-minutes-long mechanical pointing process for an optical terminal's laser from its current partner to the next receiver. This paper introduces the first PAT-aware CPD framework for optical interplanetary backhaul networks. The model captures directional temporal flows across both direct-to-Earth optical links and two-hop relay paths using delay/disruption-tolerant networking (DTN) satellites. We also introduce an optical network duty-cycle metric that quantifies the proportion of time spent transmitting to the contact window duration, exposing capacity lost to retargeting delay. Our results show that our MILP scheduler delivers over 30 percent higher network capacity than a greedy algorithm. More importantly, the results uncover a fundamental behavioral shift: when retargeting delays are modeled accurately, optimal schedules favor fewer but longer optical links that maximize throughput while minimizing retargeting overhead. These findings demonstrate that zero-delay assumptions substantially overestimate achievable performance and yield unrealistic contact plans.

</details>


### [167] [A Mechanical Wi-Fi Antenna Device for Automatic Orientation Tuning with Bayesian Optimization](https://arxiv.org/abs/2601.18256)
*Akihito Taya,Yuuki Nishiyama,Kaoru Sezaki*

Main category: cs.NI

TL;DR: 自动调整方向的Wi-Fi天线设备通过贝叶斯优化显著提升通信性能。


<details>
  <summary>Details</summary>
Motivation: 非专业用户难以确定天线的最佳方向，导致天线常处于无效位置，影响通信性能。

Method: 开发了一种机械Wi-Fi天线设备，并采用贝叶斯优化方法进行方向调整。

Result: 实验显示，天线方向在视距条件下可导致约70 Mbps的吞吐量变化，贝叶斯优化比随机搜索更有效。

Conclusion: 本研究开发了一种能够自动调整方向的机械Wi-Fi天线设备，通过贝叶斯优化有效提升了通信性能。

Abstract: Wi-Fi access points have been widely deployed in homes, offices, and public spaces. Some APs allow users to adjust the antenna orientation to improve communication performance by optimizing antenna polarization. However, it is difficult for non-expert users to determine the optimal orientation, and users often leave the antenna orientation in ineffective positions. To address this issue, we developed a mechanical Wi-Fi antenna device capable of automatically tuning its orientation. Experimental results show that antenna orientation could cause a throughput variation of approximately 70 Mbps under line-of-sight conditions. Furthermore, Bayesian optimization identified better configurations than random search, demonstrating its effectiveness for orientation tuning.

</details>


### [168] [CovertComBench: The First Domain-Specific Testbed for LLMs in Wireless Covert Communication](https://arxiv.org/abs/2601.18315)
*Zhaozhi Liu,Jiaxin Chen,Yuanai Xie,Yuna Jiang,Minrui Xu,Xiao Zhang,Pan Lai,Zan Zhou*

Main category: cs.NI

TL;DR: CovertComBench基准揭示LLMs在隐蔽通信中概念和代码表现优秀，但数学推导能力不足，需外部工具辅助以提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用推理或标准通信任务，未能充分评估LLMs在满足严格安全约束（如Kullback-Leibler散度限制）方面的能力。

Method: 引入了CovertComBench基准，评估LLMs在隐蔽通信（CC）流程中的能力，包括概念理解（MCQs）、优化推导（ODQs）和代码生成（CGQs）。

Result: LLMs在概念识别（81%）和代码实现（83%）上表现优异，但在高阶数学推导（安全保证必需）上表现较差（18%-55%）。

Conclusion: 当前的大型语言模型（LLMs）在安全约束优化中更适合作为实现助手，而非自主解决方案。未来研究应关注外部工具增强，以构建可信的无线AI系统。

Abstract: The integration of Large Language Models (LLMs) into wireless networks presents significant potential for automating system design. However, unlike conventional throughput maximization, Covert Communication (CC) requires optimizing transmission utility under strict detection-theoretic constraints, such as Kullback-Leibler divergence limits. Existing benchmarks primarily focus on general reasoning or standard communication tasks and do not adequately evaluate the ability of LLMs to satisfy these rigorous security constraints. To address this limitation, we introduce CovertComBench, a unified benchmark designed to assess LLM capabilities across the CC pipeline, encompassing conceptual understanding (MCQs), optimization derivation (ODQs), and code generation (CGQs). Furthermore, we analyze the reliability of automated scoring within a detection-theoretic ``LLM-as-Judge'' framework. Extensive evaluations across state-of-the-art models reveal a significant performance discrepancy. While LLMs achieve high accuracy in conceptual identification (81%) and code implementation (83%), their performance in the higher-order mathematical derivations necessary for security guarantees ranges between 18% and 55%. This limitation indicates that current LLMs serve better as implementation assistants rather than autonomous solvers for security-constrained optimization. These findings suggest that future research should focus on external tool augmentation to build trustworthy wireless AI systems.

</details>


### [169] [Integrating HAPS, LEO, and Terrestrial Networks: A Cost-Performance Study for IoT Connectivity](https://arxiv.org/abs/2601.18361)
*Jean Michel de Souza Sant'Ana,Felipe Augusto Tondo,Nurul Huda Mahmood,Aamir Mahmood*

Main category: cs.NI

TL;DR: HAPS和LEO卫星可互补提升IoT连接，尤其在自然灾害等场景中表现突出，成本与现有技术相当。


<details>
  <summary>Details</summary>
Motivation: 评估HAPS和LEO卫星作为替代或互补系统以增强物联网（IoT）连接的潜力。

Method: 分析了不同连接配置下的传输擦除概率，包括HAPS、LEO卫星及混合架构，考虑了LEO卫星运动、仰角和不同衰落模型，并评估了LR-FHSS技术的可扩展性。

Result: 模拟结果显示HAPS能有效提升稀疏地面网络性能，经济分析表明其成本与LEO和地面部署相当，在特定用例中更具竞争力。

Conclusion: HAPS和LEO卫星可以有效互补稀疏地面网络，在特定场景（如自然灾害）中HAPS更具竞争力，尽管其成本较高但与LEO和地面部署仍处于可比范围。

Abstract: This work evaluates the potential of High-Altitude Platform Stations (HAPS) and Low Earth Orbit (LEO) satellites as alternative or complementary systems to enhance Internet of Things (IoT) connectivity. We first analyze the transmission erasure probability under different connectivity configurations, including only HAPS or LEO satellites, as well as hybrid architectures that integrate both aerial/spatial and terrestrial infrastructures. To make the analysis more realistic, we considered movement of LEO satellites regarding a fixed region, elevation angle between gateway and devices, and different fading models for terrestrial and non-terrestrial communication. We also analyze LR-FHSS (Long-Range Frequency Hopping Spread Spectrum) random access uplink technology as a potential use case for IoT connectivity, showing the scalability impact of the scenarios. The simulation results demonstrate that HAPS can effectively complement sparse terrestrial networks and improve the performance of satellite-based systems in specific scenarios. Furthermore, considering the deployment and operational costs, respectively, CAPEX and OPEX, the economic analysis reveals that although HAPS exhibits higher costs, these remain within a comparable order of magnitude to LEO and terrestrial deployments. In addition, specific use cases, such as natural disasters, transform HAPS into a competitive technology for conventional infrastructures.

</details>


### [170] [An LLM-Agent-Based Framework for Age of Information Optimization in Heterogeneous Random Access Networks](https://arxiv.org/abs/2601.18563)
*Fang Liu,Erchao Zhu,Jiedan Tan,Jingwen Tong,Taotao Wang,Shengli Zhang*

Main category: cs.NI

TL;DR: Reflex-Core框架利用LLM代理优化异构网络中的AoI驱动随机访问，RMA协议性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有AoI驱动的随机访问策略存在模型假设理想化、收敛慢和泛化能力差等问题，亟需新型解决方案。

Method: 通过设计“观察-反思-决策-执行”闭环机制，结合监督微调（SFT）和近端策略优化（PPO），开发了Reflex-Core框架及RMA协议。

Result: RMA协议平均AoI降低14.9%，优先级版本收敛速度提升约20%。

Conclusion: Reflex-Core框架及其衍生的RMA协议在异构网络中显著提升了AoI性能，证明了LLM代理在实时访问控制中的潜力。

Abstract: With the rapid expansion of the Internet of Things (IoT) and heterogeneous wireless networks, the Age of Information (AoI) has emerged as a critical metric for evaluating the performance of real-time and personalized systems. While AoI-based random access is essential for next-generation applications such as the low-altitude economy and indoor service robots, existing strategies, ranging from rule-based protocols to learning-based methods, face critical challenges, including idealized model assumptions, slow convergence, and poor generalization. In this article, we propose Reflex-Core, a novel Large Language Model (LLM) agent-based framework for AoI-driven random access in heterogeneous networks. By devising an "Observe-Reflect-Decide-Execute" closed-loop mechanism, this framework integrates Supervised Fine-Tuning (SFT) and Proximal Policy Optimization (PPO) to enable optimal, autonomous access control. Based on the Reflex-Core framework, we develop a Reflexive Multiple Access (RMA) protocol and a priority-based RMA variant for intelligent access control under different heterogeneous network settings. Experimental results demonstrate that in the investigated scenarios, the RMA protocol achieves up to a 14.9% reduction in average AoI compared with existing baselines, while the priority-based version improves the convergence rate by approximately 20%.

</details>


### [171] [COMETS: Coordinated Multi-Destination Video Transmission with In-Network Rate Adaptation](https://arxiv.org/abs/2601.18670)
*Yulong Zhang,Ying Cui,Zili Meng,Abhishek Kumar,Dirk Kutscher*

Main category: cs.NI

TL;DR: COMETS是一种基于信息中心网络原则的多目的地视频传输框架，通过分布式优化和网络内协调，显著提升大规模视频流传输的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视频流事件对现有传输基础设施造成压力，客户端驱动的自适应对共享拥塞反应慢，而基于服务器的协调存在可扩展性瓶颈和单点故障。

Method: COMETS采用了一种新颖的范围兴趣协议和分布式网络内决策过程，结合轻量级分布式优化框架，实现了无需集中控制的质量适配。

Result: 实验表明，COMETS在高并发情况下，相比DASH、MoQ和ICN基线，能持续提升带宽利用率、公平性和用户体验质量。

Conclusion: COMETS提出了一种实用的、可部署的下一代可扩展视频传输方案，显著提升了带宽利用率、公平性和用户体验质量。

Abstract: Large-scale video streaming events attract millions of simultaneous viewers, stressing existing delivery infrastructures. Client-driven adaptation reacts slowly to shared congestion, while server-based coordination introduces scalability bottlenecks and single points of failure. We present COMETS, a coordinated multi-destination video transmission framework that leverages information-centric networking principles such as request aggregation and in-network state awareness to enable scalable, fair, and adaptive rate control. COMETS introduces a novel range-interest protocol and distributed in-network decision process that aligns video quality across receiver groups while minimizing redundant transmissions. To achieve this, we develop a lightweight distributed optimization framework that guides per-hop quality adaptation without centralized control. Extensive emulation shows that COMETS consistently improves bandwidth utilization, fairness, and user-perceived quality of experience over DASH, MoQ, and ICN baselines, particularly under high concurrency. The results highlight COMETS as a practical, deployable approach for next-generation scalable video delivery.

</details>


### [172] [An ISAC-ready Full-Duplex Backscatter Architecture for the mmWave IoT](https://arxiv.org/abs/2601.18727)
*Skanda Harisha,Jimmy G. D. Hester,Aline Eid*

Main category: cs.NI

TL;DR: 低成本毫米波全双工反向散射标签，通过创新电路设计实现远距离、高可靠性通信，覆盖范围和成本显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波硬件高功耗和高成本问题，推动未来泛在感知系统的规模化应用。

Method: 采用新型低功耗再生放大器和再生整流器，分别提供30 dB增益和低至-60 dBm的灵敏度，集成于紧凑PCB上。

Result: 标签在45米上行和200米下行范围内实现了20倍于现有系统的覆盖范围，成本降低100倍，通信质量稳定（下行BER 10^-1，上行BER 10^-2）。

Conclusion: 该论文提出了一种创新的毫米波全双工反向散射标签架构，显著降低了高性能毫米波连接和定位的成本，同时实现了远距离、高可靠性的通信。

Abstract: Achieving long-range, high-rate, concurrent two-way mmWave communication with power-constrained IoT devices is fundamental to scaling future ubiquitous sensing systems, yet the substantial power demands and high cost of mmWave hardware have long stood in the way of practical deployment. This paper presents the first mmWave full-duplex backscatter tag architecture, charting a genuinely low-cost path toward high-performance mmWave connectivity and localization for ISAC systems. The proposed tag operates at ranges beyond 45m on the uplink and beyond 200m on the downlink, delivering 20x the reach of state-of-the-art systems while being over 100x cheaper than existing mmWave backscatter platforms. Enabling this leap is a novel low-power regenerative amplifier that provides 30 dB of gain while consuming only 30 mW, paired with a regenerative rectifier that achieves state-of-the-art sensitivity down to -60 dBm. We integrate our circuits on a compact PCB and evaluate it across diverse uplink and downlink scenarios, where it achieves an downlink BER of $10^{-1}$ at 200 meters and a uplink BER of $10^{-2}$ at 45 meters, demonstrating resilient, high-quality communication even at extended ranges.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [173] [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)
*Julian Bellavita,Matthew Rubino,Nakul Iyer,Andrew Chang,Aditya Devarakonda,Flavio Vella,Giulia Guidi*

Main category: cs.DC

TL;DR: 提出多GPU分布式算法，显著提升Kernel K-means在大规模数据集上的性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决Kernel K-means在处理大规模数据集时因GPU内存限制而无法运行的问题。

Method: 提出了一套分布式内存并行算法，用于多GPU系统上的大规模Kernel K-means聚类，设计了分区方案以实现高效的线性代数原语组合。

Result: 1.5D算法在256个GPU上实现了79.7%的几何平均弱扩展效率和4.2倍的几何平均强扩展加速比，相比1D算法最高提速3.6倍。

Conclusion: 分布式算法设计结合应用特定的线性代数公式可以显著提升性能，1.5D算法在256个GPU上实现了最高性能，将Kernel K-means扩展到比以往大一到两个数量级的数据规模。

Abstract: Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.
  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.
  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\%$ and a geometric mean strong scaling speedup of $4.2\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.

</details>


### [174] [Push Down Optimization for Distributed Multi Cloud Data Integration](https://arxiv.org/abs/2601.17546)
*Ravi Kiran Kodali,Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Balakrishna Pothineni,Aswathnarayan Muthukrishnan Kirubakaran,Sumit Saha,Nachiappan Chockalingam*

Main category: cs.DC

TL;DR: 论文研究了多云ETL管道中的下推优化，展示了其通过本地化下推和数据联邦技术减少跨云流量并提升性能的实际效果。


<details>
  <summary>Details</summary>
Motivation: 企业采用多云架构以利用多样化的数据库引擎、区域可用性和成本模型，但ETL管道在处理大型分布式数据集时面临延迟和传输成本的挑战。

Method: 论文评估了本地化下推、混合模型和数据联邦技术，以减少跨云流量并提高性能。

Result: 通过Redshift和BigQuery的案例研究，论文展示了可衡量的收益，包括降低端到端运行时间、减少传输量和提高成本效率。

Conclusion: 该论文探讨了在多云环境中应用下推优化的可行性，分析了其优势和局限性，并提出了实际策略以提高ETL的可扩展性和可靠性。

Abstract: Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.

</details>


### [175] [A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures](https://arxiv.org/abs/2601.17578)
*Henrik Bengtsson*

Main category: cs.DC

TL;DR: futurize包通过futurize()函数统一R中map-reduce API的并行化，简化代码修改，支持多种函数和包。


<details>
  <summary>Details</summary>
Motivation: 解决R生态系统中多样化map-reduce API并行化代码时需学习多个不兼容API的问题，简化并行计算流程。

Method: futurize包利用R的原生管道操作符，将顺序map-reduce表达式转换为future生态系统中的并行等效代码，支持多种经典和领域特定的map-reduce函数。

Result: futurize包成功实现了对多种map-reduce函数的透明并行化，用户仅需简单修改即可实现代码并行化。

Conclusion: futurize包通过提供统一的futurize()函数，简化了R中多样化map-reduce API的并行化过程，使开发者能够专注于声明并行内容，而用户则通过plan()选择执行方式。

Abstract: The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.

</details>


### [176] [Lightspeed Data Compute for the Space Era](https://arxiv.org/abs/2601.17589)
*Thomas Sandholm,Bernardo A. Huberman,Klas Segeljakt,Paris Carbone*

Main category: cs.DC

TL;DR: SpaceCoMP是一种LEO卫星数据处理模型，通过协同处理和优化调度显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决地面站下载带宽不足导致大量卫星数据无法传输的问题。

Method: 提出了SpaceCoMP，一种受MapReduce启发的LEO卫星网状网络处理模型，利用空间物理加速计算，采用距离感知路由协议和二分图匹配调度策略。

Result: 模拟1,000-10,000颗卫星的星座，显示在映射放置效率上比基线提高61-79%，比贪婪分配提高18-28%，聚合成本降低67-72%。

Conclusion: SpaceCoMP证明了轨道网格不仅可作为通信中继，还能为更快的数据处理提供基础。

Abstract: While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies.

</details>


### [177] [Scaling All-to-all Operations Across Emerging Many-Core Supercomputers](https://arxiv.org/abs/2601.17606)
*Shannon Kinkead,Jackson Wesley,Whit Schonbein,David DeBonis,Matthew G. F. Dosanjh,Amanda Bienz*

Main category: cs.DC

TL;DR: 本文针对多核系统提出优化all-to-all算法，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 高性能的MPI all-to-all集体操作对快速傅里叶变换、转置和机器学习应用至关重要，现有实现在不同系统上的性能表现差异较大。

Method: 通过性能分析对比现有算法和系统MPI，提出针对新兴多核系统的优化all-to-all算法。

Result: 在32节点的Sapphire Rapids系统上，新算法比系统MPI快3倍。

Conclusion: 本文提出的新型all-to-all算法在Sapphire Rapids系统上实现了高达3倍的性能提升，证明了其在多核系统中的高效性。

Abstract: Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.

</details>


### [178] [Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs](https://arxiv.org/abs/2601.17707)
*Mekala Kiran,Apurba Das,Suman Banerjee,Tathagata Ray*

Main category: cs.DC

TL;DR: Parallel algorithms (M-BBC and G-BBC/G-BBC++) for balanced butterfly counting achieve massive speedups (up to 13,320x) on large graphs, enabling efficient signed motif analysis.


<details>
  <summary>Details</summary>
Motivation: Balanced butterfly counting is crucial for analyzing signed bipartite graphs but faces computational bottlenecks on large-scale graphs, necessitating efficient parallel solutions.

Method: The paper presents multi-core (M-BBC) and GPU-based (G-BBC/G-BBC++) algorithms, employing vertex-level parallelism and tile-based approaches with dynamic scheduling for workload balance.

Result: M-BBC achieves up to 71.13x speedup (average 38.13x), while GPU-based methods reach up to 13,320x speedup (average 2,600x) over the sequential baseline BB2K.

Conclusion: The parallel algorithms (M-BBC and G-BBC/G-BBC++) significantly improve the efficiency and scalability of balanced butterfly counting, establishing a foundation for high-performance signed motif analysis on large bipartite graphs.

Abstract: Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.

</details>


### [179] [An MLIR Lowering Pipeline for Stencils at Wafer-Scale](https://arxiv.org/abs/2601.17754)
*Nicolai Stawinoga,David Katz,Anton Lydike,Justs Zarins,Nick Brown,George Bisbas,Tobias Grosser*

Main category: cs.DC

TL;DR: 通过编译器自动化将模板内核优化为WSE适配的CSL代码，性能媲美手动优化，且在WSE3上显著超越传统GPU和CPU超级计算机。


<details>
  <summary>Details</summary>
Motivation: Cerebras WSE架构虽适用于高性能计算（HPC），但其分布式异步编程模型与传统顺序或批量同步程序差异显著，导致现有代码移植需定制化重写。缺乏编译器支持（如MLIR）进一步阻碍了自动化过程。本研究旨在探索利用模板的领域特定信息，通过编译器自动化适配WSE。

Method: 提出了一种编译器流水线，能够将基于模板的内核转换为针对WSE高度优化的CSL代码，填补了数学问题表示与WSE异步执行模型之间的语义鸿沟。

Result: 在五个基准测试中，该方法在Cerebras WSE2和WSE3上的性能与手动优化代码相当或更优。在WSE3上，性能比128个Nvidia A100 GPU快约14倍，比128个CPU节点的Cray-EX超级计算机快20倍。

Conclusion: 本研究通过利用特定领域信息（如模板）的编译器优化，成功实现了在不修改应用层代码的情况下，自动将模板内核转换为高度优化的CSL代码，以适配Cerebras WSE架构。实验结果表明，该方法在性能上不仅与手动优化代码相当甚至更优，且在WSE3上的性能显著优于传统GPU和CPU超级计算机。

Abstract: The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.
  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.

</details>


### [180] [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)
*Zizhao Zhang,Yihan Xue,Haotian Zhu,Sijia Li,Zhijun Wang,Yujie Xiao*

Main category: cs.DC

TL;DR: CondenseGraph 通过动态压缩节点特征和误差反馈机制，显著减少分布式 GNN 训练的通信开销，保持准确性。


<details>
  <summary>Details</summary>
Motivation: 分布式 GNN 训练因图结构数据的邻域依赖性导致严重的通信开销，静态图分区策略无法适应动态网络条件，亟需一种高效的通信优化方案。

Method: 提出了一种动态图压缩机制（CondenseGraph），将边界节点特征动态压缩为紧凑的超级节点，并开发了基于梯度的误差反馈机制以减少信息损失。

Result: 在四个基准数据集上的实验表明，CondenseGraph 减少了 40-60% 的通信量，显著降低了训练时间，同时保持了准确性。

Conclusion: CondenseGraph 是一种新型的分布式 GNN 训练框架，通过动态压缩边界节点特征和梯度误差反馈机制，显著减少了通信开销，同时保持了与全精度基线相当的准确性。

Abstract: Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.

</details>


### [181] [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)
*Zixi Chen,Tianci Bu,Chendong Song,Xin Lu,Yinyu Ye,Zijie Zhou*

Main category: cs.DC

TL;DR: 本文提出了一种通用负载均衡原则，通过优化方法显著减少LLM解码中的不平衡，提高性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）服务中的负载均衡问题，特别是在数据并行解码下，屏障同步导致的空闲时间可超过每个解码步骤计算时间的40%。

Method: 开发了一种逐步有限范围整数优化公式，适用于LLM解码模型和一类非递减工作负载漂移过程。

Result: 理论分析和实验结果表明，该方法显著减少了长期不平衡，提高了吞吐量和延迟，并降低了能耗。

Conclusion: 本文提出了一种通用的负载均衡原则，通过逐步有限范围整数优化公式，为LLM解码模型和一类非递减工作负载漂移过程提供了最坏情况下的保证，显著减少了长期不平衡。实验结果验证了理论，展示了吞吐量和延迟的显著改善以及能耗的降低。

Abstract: Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.

</details>


### [182] [An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX](https://arxiv.org/abs/2601.18158)
*Karame Mohammadiporshokooh,Panagiotis Syskakis,Hartmut Kaiser*

Main category: cs.DC

TL;DR: 研究提出基于HPX的NWGraph分布式实现，BFS优于BGL，PageRank仍有挑战，展示了异步任务模型在图处理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有分布式图框架面临延迟受限、内存访问不规则和同步成本高等问题，限制了可扩展性和效率。

Method: 通过HPX的异步多任务模型，减少同步开销、改善负载平衡，并基于NWGraph库实现分布式图分析。评估采用BFS和PageRank两种算法。

Result: BFS性能优于分布式BGL，但PageRank当前实现尚未超越BGL。

Conclusion: 该研究展示了将NWGraph库与HPX运行时系统集成的分布式实现，虽然在BFS算法上表现优于分布式Boost Graph Library（BGL），但PageRank算法仍有优化空间。这为异步任务模型在图处理中的应用提供了前景和挑战。

Abstract: Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.

</details>


### [183] [On the Bandwidth Consumption of Blockchains](https://arxiv.org/abs/2601.18400)
*Andrei Lebedev,Vincent Gramoli*

Main category: cs.DC

TL;DR: 本文首次对五种区块链协议的带宽消耗进行了实证比较，发现传输协议和节点角色隔离是影响流量的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的兴起，提案数量激增，这些提案带来的网络流量增加了托管节点的成本。目前尚缺乏对区块链带宽消耗的比较研究。

Method: 测量了五种区块链协议（Algorand、Aptos、Avalanche、Redbelly和Solana）的网络流量，研究了随时间的变化，区分了接收和发送流量，并分析了流量如何随节点和验证者数量的变化而变化。

Result: 研究发现，传输协议是影响网络流量的主要因素，隔离节点角色可以减少流量，不同区块链对网络规模的敏感度不同。

Conclusion: 传输协议是影响网络流量的主要因素，隔离节点角色有助于减少流量，不同的区块链受网络规模的影响不同。

Abstract: With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.
  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.
  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [184] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 本文研究噪声对四旋翼无人机系统的影响，采用扩展卡尔曼滤波和期望最大化算法进行参数估计，结果显示在线估计优于离线估计。


<details>
  <summary>Details</summary>
Motivation: 无人机在救援、农业、运输等领域发挥重要作用，但地震等灾害中噪声可能影响其性能。本文旨在研究噪声对四旋翼无人机系统的影响及优化方法。

Method: 本文采用了扩展卡尔曼滤波器进行状态估计，基于SDE系统实现了线性二次高斯控制器，并应用期望最大化算法进行四旋翼无人机的参数估计。

Result: 通过离线与在线参数估计的比较，发现在线估计的收敛值范围更大。

Conclusion: 研究表明，在线参数估计的收敛值范围略大于离线参数估计，为无人机系统的噪声处理提供了优化方向。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [185] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 本文探讨代理AI系统的可解释性挑战，评估现有方法的局限性，并提出未来发展方向以确保安全和责任。


<details>
  <summary>Details</summary>
Motivation: 代理系统在架构和部署上与传统机器学习模型有根本区别，带来独特的AI安全挑战，需要新的可解释性方法。

Method: 评估现有可解释性方法在代理系统中的适用性和局限性，并提出未来发展方向。

Result: 现有可解释性方法在代理系统中存在局限性，需开发新方法以应对其时间动态性和上下文依赖行为。

Conclusion: 为确保代理AI系统的安全和负责任部署，必须开发专门针对代理系统的可解释性技术，以在整个代理生命周期中嵌入监督机制。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [186] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah,Wlodek Zadrozny*

Main category: cs.AI

TL;DR: Tensor Logic 通过数学等价性验证和实验证明，为符号推理与神经网络的统一提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 解决符号系统与神经网络在可扩展性和透明度上的矛盾，探索二者的统一路径。

Method: 通过三个实验验证 Tensor Logic 框架：1) 递归 Datalog 规则与张量收缩的等价性验证；2) 嵌入空间中的推理实现；3) 在大规模知识图谱上的关系矩阵验证。

Result: 实验表明，Tensor Logic 在祖先关系计算、零样本推理和知识图谱链接预测上均表现优异，验证了其数学等价性和实用性。

Conclusion: Tensor Logic 提供了符号推理与神经网络统一的可行路径，并通过实验验证了其在可扩展性和推理能力上的优势。

Abstract: The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [187] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实临床记录开发生成模型，模拟高保真患者未来轨迹，揭示电子健康记录的价值。


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革性潜力，如个性化治疗计划和虚拟临床试验，但由于复杂的生物和社会文化影响，模拟患者轨迹具有挑战性。

Method: 开发了一个生成模拟器模型，利用患者的病史作为输入，合成细粒度、真实的未来轨迹。模型基于超过2亿条临床记录进行了预训练。

Result: 模型生成了高保真的未来时间线，与真实患者未来数据中的事件发生率、实验室测试结果和时间动态紧密匹配，并能准确估计未来事件的概率。

Conclusion: 研究揭示了电子健康记录中真实世界数据的潜在价值，并提出了一个可扩展的临床护理计算机模拟框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [188] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出了一种理论框架，用于预测多智能体系统在有限预算下的性能表现，并通过实验验证了理论的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统在固定推理预算下的性能表现，尤其是其在帮助、饱和或崩溃等不同状态下的行为。

Method: 开发了一个理论框架，结合有限上下文窗口、有损通信和共享错误相关性等约束，通过计算性能缩放指数β、消息长度保真度曲线γ(m)、共享错误相关性ρ和上下文窗口W来分析多智能体系统的性能。

Result: 理论预测了多智能体系统在不同约束条件下的相变行为，并通过实验验证了这些预测。特别是在放大状态下，提出了组织指数s，并展示了预算协同效应的条件。

Conclusion: 论文提出了一个最小化且可校准的理论，用于预测多智能体系统在不同约束条件下的性能表现，并通过实验验证了理论预测的相变边界。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [189] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式数据合成管道，通过分解任务和优化计算利用率，显著提高了数据生成效率。


<details>
  <summary>Details</summary>
Motivation: 解决形式数学中代理工作流的高成本问题，缓解开源语料库稀缺的现状。

Method: 通过将形式化过程分解为五个子任务（陈述形式化、证明生成、前提选择、证明修正和证明草图），并采用解耦提取策略，从全局失败的轨迹中恢复有效的训练信号。

Result: 在2000个问题的基准测试中，TheoremForge实现了12.6%的验证率，超过8.6%的基线，平均成本仅为每次成功轨迹0.481美元。

Conclusion: TheoremForge被证明是一个可扩展的框架，能够有效构建数据飞轮以训练未来的专家模型。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [190] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 研究证明AGI的强分布独立主张未定义，且无法通过自我认证实现通用智能。


<details>
  <summary>Details</summary>
Motivation: 探讨AGI是否支持存在性、鲁棒性或自我验证的绝对主张的理论定义。

Method: 通过将AGI形式化为一个分布性、资源受限的语义谓词，并基于任务族、任务分布、性能函数和明确资源预算进行索引，推导出四类结果。

Result: 证明了通用性是关系性的、任务分布的微小扰动可导致AGI属性失效、有限资源下跨任务族的无界泛化被排除，以及AGI无法通过任何可计算程序（包括代理自身）完全认证。

Conclusion: 研究表明，AGI的强分布独立主张在没有明确形式索引的情况下是未定义的，AI的实证进展并不意味着可实现自我认证的通用智能。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [191] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估协议，解决了现有知识更新方法在编辑效果和特异性评估上的不足，实验证明新协议更敏感且能更精细地区分方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识更新方法在编辑效果和特异性之间存在平衡问题，且现有评估协议不足以有效评估这一平衡。

Method: 系统阐述了现有特异性评估协议的三个基本问题，并提出了一种新的评估协议。

Result: 新协议消除了开放式LLM与确定性答案假设之间的冲突，避免了查询无关的流畅性偏差，并能平滑调整评估严格度。

Conclusion: 提出的新评估协议在多个LLM、数据集和编辑方法上表现出更强的敏感性，并能更精细地区分不同方法的知识保留能力。

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [192] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: MALPP框架利用多智能体协作和LLMs，解决了现有学习路径规划的不足，实验证明其在多个指标上优于基线，为教育AI提供了可信、可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，MALPP旨在解决这些问题。

Method: 提出了一个多智能体学习路径规划（MALPP）框架，利用角色和规则为基础的协作机制，由三个任务特定智能体（学习者分析、路径规划和反思）通过结构化提示和预定义规则协作完成。

Result: 在MOOCCubeX数据集上使用七个LLMs进行的实验表明，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型。消融研究进一步验证了协作机制和理论约束的有效性。

Conclusion: 该研究为教育领域可信、可解释的AI发展做出了贡献，展示了基于LLMs的以学习者为中心的自适应教学的可扩展方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [193] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究显示VLM在残疾描述中易产生解释偏移，针对性干预可改善表现。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型（VLM）在残疾相关描述中的行为，因其在社会敏感应用中的广泛部署而行为未充分探索。

Method: 基于中性提示（NP）和残疾情境提示（DP）的基准，评估15种最先进的开放和封闭源VLM在零样本设置下的表现，结合标准文本指标和LLM作为评判协议。

Result: 引入残疾情境会一致性地降低解释保真度，导致推测性推断、叙事扩展、情感降级和缺陷导向框架的解释偏移。这些效应在种族和性别维度上进一步放大。

Conclusion: 引入针对性提示和偏好微调可有效提升解释保真度并显著减少解释偏移。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [194] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: LLMs在逻辑推理上展现出向现代逻辑的转变，模型规模、思维链和基础模型是主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否像人类逻辑一样，从直觉驱动的推理演变为严格的逻辑体系。

Method: 通过使用存在导入作为探针，评估LLMs在传统和现代逻辑下的三段论表现，并在新的三段论数据集上对SOTA LLMs进行广泛实验。

Result: 实验发现：(i) 模型规模扩大促进向现代逻辑的转变；(ii) 思维链技术是超越参数规模的高效加速器；(iii) 基础模型对这一转变的稳定性和易发性起关键作用。

Conclusion: 大型语言模型（LLMs）在逻辑推理上表现出从传统逻辑向现代逻辑的转变，模型规模、思维链技术及基础模型是影响这一转变的关键因素。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [195] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh是一种协议无关的执行控制平面，通过AAB和CAR确保自主代理动作的执行时授权和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理系统缺乏执行前的确定性检查点，可能导致不可控的现实世界副作用，如基础设施部署、数据库修改等。

Method: Faramesh设计了协议无关的执行控制平面，将代理意图规范化为CAR，通过策略和状态确定性评估动作，并生成决策结果（PERMIT/DEFER/DENY）。

Result: Faramesh实现了框架和模型无关的多代理、多租户部署，支持决策中心的追加日志记录，确保审计和验证能力。

Conclusion: Faramesh通过引入Action Authorization Boundary（AAB）和Canonical Action Representation（CAR），为自主代理系统提供了执行时授权的确定性控制，确保了可审计性和可验证性。

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [196] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice是一种自我构建和持续改进的AI防护栏框架，通过两阶段方法在ProsocialDialog数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用静态规则，无法适应新威胁或部署环境，因此需要一种能自我构建和持续改进的防护栏框架。

Method: Lattice分两阶段运作：构建阶段通过迭代模拟和优化从标记示例中构建初始防护栏；持续改进阶段通过风险评估、对抗测试和整合自主调整部署的防护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1，优于关键词基线43个百分点、LlamaGuard 25个百分点和NeMo 4个百分点。持续改进阶段通过闭环优化在跨域数据上实现了7个百分点的F1提升。

Conclusion: Lattice框架通过迭代优化展示了有效的防护栏可以自我构建。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [197] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 论文提出了一种名为'认知平台工程'的新范式，通过四层架构将智能嵌入平台生命周期，实现了更高效的云环境管理。


<details>
  <summary>Details</summary>
Motivation: 现代DevOps实践虽然通过自动化、CI/CD管道和可观测性工具加速了软件交付，但在云原生系统的规模和动态性面前显得力不从心，传统规则驱动的自动化导致反应性操作、延迟修复和依赖手动专业知识。

Method: 论文提出了一个四层参考架构，统一了数据收集、智能推理、策略驱动的编排和人类体验层，并通过基于Kubernetes、Terraform、Open Policy Agent和基于ML的异常检测的原型实现进行验证。

Result: 原型实现展示了在平均解决时间、资源效率和合规性方面的改进。

Conclusion: 论文总结了将智能嵌入平台操作能够实现弹性、自我调整且意图一致的云环境，并提出了强化学习、可解释治理和可持续自我管理云生态系统等研究方向。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [198] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX的高性能RL环境，大幅提升了ARC任务的执行效率，支持大规模研究。


<details>
  <summary>Details</summary>
Motivation: 现有的Gymnasium-based RL环境由于计算瓶颈限制了实验规模，无法满足大规模RL研究的需求。

Method: 通过JAX实现的功能性、无状态架构，支持大规模并行处理，提供灵活的action空间和可组合的wrapper。

Result: JaxARC在匹配的batch size下实现了38-5,439倍的加速，峰值吞吐量达到790M steps/second。

Conclusion: JaxARC是一个高性能的RL环境，解决了现有Gymnasium-based RL环境在计算上的瓶颈，支持大规模RL研究。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [199] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel,Nathaniel W. Zuckschwerdt,Aryan Deshwal,Susmita Bose,Amit Bandyopadhyay,Jana Doppa*

Main category: cs.AI

TL;DR: 利用AI和领域知识优化金属合金增材制造参数配置，显著提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 金属合金增材制造参数配置复杂，传统试错方法效率低下且资源消耗大。

Method: 结合AI驱动的自适应实验设计与领域知识，构建替代模型以智能选择少量输入配置进行验证。

Result: 在三个月内成功获得多个无缺陷输出，显著缩短了实验周期并减少了资源消耗。

Conclusion: 该方法成功实现了高质量GRCop-42合金的制造，显著降低了时间和资源消耗，为该合金的普及和航空航天应用的低成本、分散化生产铺平了道路。

Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [200] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 论文认为智能需要基础化而非具体化，并通过定义智能属性和思想实验支持这一观点。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的最新进展是否重新引发了关于智能是否需要具体化的科学辩论。

Method: 通过定义智能的四个属性（动机、预测能力、因果理解力和从经验中学习的能力），论证非具体化但基础化的代理可以实现这些属性。

Result: 论证了基础化而非具体化是实现智能的必要条件，并通过思想实验和反驳潜在反对意见来支持这一结论。

Conclusion: 论文得出结论，智能需要基础（grounding），而非具体化（embodiment），并提出了一个数字环境中智能LLM代理的思想实验来支持这一观点。

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [201] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: Health-ORSC-Bench是首个系统测量医疗AI中过度拒绝和安全完成质量的基准，发现当前LLM在平衡拒绝和合规方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域的安全对齐至关重要，但依赖二元拒绝边界常导致良性查询的过度拒绝或有害查询的不安全合规。现有基准无法评估安全完成能力。

Method: 引入Health-ORSC-Bench，一个大规模基准，用于系统测量医疗领域中的过度拒绝和安全完成质量。包括31,920个良性边界提示，涵盖七个健康类别，并使用自动化流程和人工验证来测试不同意图模糊级别的模型。

Result: 评估了30个最先进的LLM，发现安全优化模型频繁拒绝高达80%的“困难”良性提示，而领域特定模型常牺牲安全性以换取实用性。模型家族和大小显著影响校准。

Conclusion: Health-ORSC-Bench为下一代医疗AI助手提供了一个严格的校准标准，以实现细致、安全和有用的完成。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [202] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: DIML框架通过多智能体学习动态模型和反事实收益生成，成功恢复非结构化激励机制，性能优异且可扩展。


<details>
  <summary>Details</summary>
Motivation: 研究逆向机制学习：从自利学习智能体的战略交互轨迹中恢复未知的激励生成机制，区别于逆向博弈论和多智能体逆向强化学习，目标是包括非结构化机制。

Method: 提出DIML，一种基于似然的框架，通过多智能体学习动态模型进行微分，并使用候选机制生成预测观察行为所需的反事实收益。

Result: DIML在模拟交互中可靠地恢复了可识别的激励差异，支持反事实预测，性能在小环境中与表格枚举预言机相当，在大规模环境中也能收敛。

Conclusion: DIML框架能够可靠地恢复可识别的激励差异，并支持反事实预测，其性能在小环境中可与表格枚举预言机相媲美，在大规模环境中也能收敛。

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [203] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail通过多轮交互式强化学习框架，显著提升Text-to-SQL性能，数据效率最高达18倍，小模型超越大系统5%。


<details>
  <summary>Details</summary>
Motivation: 现有单次生成范式缺乏人类专家的迭代推理和纠错能力，导致在复杂基准（如BIRD-SQL）上性能差距显著。

Method: 采用多轮强化学习（RL）框架SQL-Trail，通过数据库环境交互和执行反馈迭代优化预测，包括自适应轮次预算分配和复合奖励机制。

Result: SQL-Trail在多个基准测试中刷新了最高性能，数据效率提升高达18倍，且较小模型（7B/14B）表现优于更大规模的专有系统5%。

Conclusion: SQL-Trail框架通过多轮强化学习显著提升了Text-to-SQL生成的性能，尤其在复杂查询场景下表现优于现有单次生成方法，验证了交互式工作流的有效性。

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [204] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 论文提出LLM Data Auditor框架，系统评估多模态合成数据的质量和可信度，发现当前评估实践的不足并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成合成数据质量评估的不足，特别是多模态数据的统一评估视角缺失问题。

Method: 通过LLM生成六种不同模态的数据，并系统地从质量和可信度两个维度对合成数据进行内在评估。

Result: 实验评估揭示了当前生成方法在数据质量评估上的显著缺陷，并提出了改进建议。

Conclusion: 论文提出了LLM Data Auditor框架，系统地评估了多模态合成数据的质量和可信度，并指出了当前评估实践的不足，为改进数据生成评估提供了具体建议。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [205] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld 是一个大规模企业级基准，包含 1,756 个任务，通过逆向工程业务逻辑生成真实工作流程，并采用 SQL 验证机制，揭示当前代理在企业环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对消费者场景，无法捕捉企业工作流程的复杂性和严谨性，因此需要专门的企业级基准。

Method: 采用基于模式的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，并引入基于 SQL 的确定性验证机制。

Result: 实验结果显示，最先进的模型（如 GPT-4.1）在 EntWorld 上的成功率仅为 47.61%，远低于人类表现。

Conclusion: EntWorld 作为一个严格的测试平台，旨在促进下一代企业级数字代理的开发和评估，揭示了当前代理在专业企业工作流程中的显著差距。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [206] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: ReFuGe框架通过三个LLM代理迭代生成和优化关系特征，显著提升关系数据库预测任务性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在预测任务中的应用面临复杂模式和组合爆炸特征空间的挑战，缺乏明确监督。

Method: ReFuGe框架利用三个专门的大型语言模型代理：模式选择代理、特征生成代理和特征过滤代理，通过迭代反馈循环生成和优化关系特征。

Result: 在关系数据库基准测试中，ReFuGe显著提升了多种预测任务的性能。

Conclusion: ReFuGe框架通过迭代反馈循环显著提升了关系数据库预测任务的性能，其代码和数据集已开源。

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [207] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出混合AI框架（CNN+LSTM+KNN+XGB），显著提升心血管疾病预测性能，支持早期干预。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，传统预测模型在异构数据集和复杂生理模式上泛化能力不足。

Method: 结合深度学习架构（CNN和LSTM）与传统机器学习算法（KNN和XGB），采用集成投票机制。

Result: 在两个Kaggle数据集上分别达到82.30%和97.10%的准确率，且在精确率、召回率和F1分数上均有提升。

Conclusion: 该研究提出的混合集成框架在预测心血管疾病方面表现出色，支持联合国可持续发展目标3，促进早期诊断和预防。

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [208] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，通过约束满足问题建模指令，显著优于现有方法并提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在重要应用中不总是遵循指令的问题，尤其是在基于LLM的代理工作流中，指令违反可能导致任务失败和系统事故。

Method: NSVIF将指令遵循验证建模为约束满足问题，通过将用户指令建模为约束（包括逻辑和语义约束），并由统一的求解器协调逻辑推理和语义分析来完成约束求解。

Result: 实验表明，NSVIF在VIFBENCH基准测试中显著优于基于LLM的方法，并能提供可解释的反馈。

Conclusion: NSVIF作为一种通用的神经符号验证框架，显著优于基于LLM的方法，并能提供可解释的反馈，帮助提升LLM的指令遵循能力而无需后训练。

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [209] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench 是一个多模态路由基准，通过多模态信号提升路由效率，在低成本下超越单一模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLM）在架构、对齐策略和效率上的异构性，以及在实际部署中如何通过路由策略平衡计算成本和准确性。

Method: 提出了 MMR-Bench，一个包含模态感知输入、可变计算预算和多样化视觉语言任务的基准测试套件，用于评估多模态路由策略。

Result: 实验表明，多模态信号提升了路由质量，路由系统在约 33% 的成本下超越了最强单一模型的准确性，且策略能零样本泛化到新数据集和文本基准。

Conclusion: MMR-Bench 提供了一个统一的基准，用于研究自适应多模态模型选择和高效 MLLM 部署，展示了多模态信号如何提升路由质量，并在成本效率上优于单一模型。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [210] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: 本研究开发了一个AI代理框架，结合RAG和大型语言模型，通过多阶段工作流程和知识图谱优化，实现了从静态代码翻译到动态自适应分析的转变。


<details>
  <summary>Details</summary>
Motivation: 为了促进传统有限差分实现向Devito环境的转换，本研究开发了一个集成的AI代理框架。

Method: 通过多阶段迭代工作流程，结合检索增强生成（RAG）和开源大型语言模型，构建了混合LangGraph架构。系统通过文档解析、结构感知分割、实体关系提取和基于Leiden的社区检测，构建了广泛的Devito知识图谱。GraphRAG优化提升了跨语义社区的查询性能。反向工程组件通过Fortran源代码的静态分析，为RAG检索制定了三级查询策略。

Result: 开发了一个全面的验证框架，结合了传统静态分析和G-Eval方法，覆盖了执行正确性、结构健全性、数学一致性和API合规性。代理工作流程在LangGraph框架上实现，并采用并发处理以支持基于质量的迭代优化和状态感知动态路由。

Conclusion: 本研究的主要贡献在于结合了强化学习的反馈机制，实现了从静态代码翻译向动态自适应分析行为的转变。

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [211] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard是一款AI助手，通过HiSACC和ReLACE技术自动解析复杂监管文本，提升合规效率，降低错误风险。


<details>
  <summary>Details</summary>
Motivation: 跨国制药公司面临监管更新频繁且复杂的挑战，手动解析成本高且易出错，亟需自动化解决方案。

Method: 系统采用HiSACC（分层语义聚合）进行文档语义分割，以及ReLACE（基于开源模型的域适应交叉编码器）进行候选排序优化。

Result: 企业环境评估显示，RegGuard在答案质量（相关性、基础性和上下文聚焦）方面有显著提升，并有效降低幻觉风险。

Conclusion: RegGuard通过其创新的HiSACC和ReLACE组件，显著提升了监管文本解析的准确性、相关性和上下文聚焦能力，同时降低了幻觉风险，适用于高合规要求的领域。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [212] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT通过信息增益奖励和在线RL训练医疗对话AI，显著提升HPI生成效果，超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖昂贵专家标注对话或静态数据集的问题，通过自生成对话和模拟患者实现高效学习。

Method: 采用在线强化学习框架，结合信息增益奖励函数和GPT-4o-mini质量评估，对Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B进行LoRA微调。

Result: DeepSeek-R1-Distill-Qwen-7B在Avey和MIMIC数据上的F1分数分别提升10.9%和12.9%，优于OpenAI模型和医疗领域基线。

Conclusion: IGFT方法通过结合在线GRPO和信息增益奖励，显著提升了医疗对话AI在患者访谈和HPI生成中的表现，超越了现有基线模型。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [213] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 研究发现个性化LLM代理中的意图合法化问题，提出PS-Bench基准和检测-反思方法，证明个性化显著增加安全风险。


<details>
  <summary>Details</summary>
Motivation: 揭示个性化代理中先前未被充分探索的安全故障——意图合法化，即良性个人记忆如何偏注意图推断并导致模型合法化本质上有害的查询。

Method: 引入了PS-Bench基准，用于识别和量化个性化交互中的意图合法化现象，并通过内部表示空间提供机制性证据。

Result: 在多个记忆增强代理框架和基础LLM中，个性化使攻击成功率相对于无状态基线增加了15.8%-243.7%。

Conclusion: 该论文首次系统性地探索和评估了意图合法化这一安全故障模式，强调了在长期个性化背景下评估安全性的重要性，并提出了轻量级的检测-反思方法以有效减少安全退化。

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [214] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog通过潜在心智空间分析LLM认知，揭示共享推理核心与能力特定特征，发现推理失败与潜在激活异常相关，并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中认知能力的参与方面存在局限，需要一种统一框架来分析LLM的认知过程。

Method: 提出UniCog框架，通过潜在变量模型将密集模型激活编码为稀疏、解耦的潜在维度，分析六种先进LLM的认知能力。

Result: 发现LLM认知的帕累托原则，推理失败常表现为潜在激活异常强度，潜在信息候选优先策略使推理性能提升高达7.5%。

Conclusion: UniCog框架为LLM认知分析提供了新的视角，揭示了推理动态的认知基础，并通过潜在信息候选优先策略显著提升了推理性能。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [215] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: EoG框架通过分离LLM的证据挖掘与确定性控制器的图遍历，显著提升了开放调查任务中的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在开放调查中因上下文窗口限制和隐藏依赖结构导致的证据丢失和结果不稳定问题。

Method: 提出EoG框架，将LLM的本地证据挖掘与确定性控制器的图遍历、状态管理和信念传播分离。

Result: 在ITBench诊断任务中，EoG相比ReAct基线提高了准确性和运行一致性，平均Majority-at-k实体F1提升了7倍。

Conclusion: EoG框架通过将LLM的证据挖掘与确定性控制器的图遍历和信念传播分离，显著提高了在开放调查任务中的准确性和一致性。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [216] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 本文综述了自驱动实验室（SDLs）中的AI问题，提出了分类法和评估标准，总结了现有挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 探讨自驱动实验室中AI在昂贵操作、噪声和延迟反馈、严格可行性和安全约束以及非平稳性等挑战下的应用。

Method: 论文通过软物质作为代表性场景，将SDL自主性框架化为代理环境交互问题，并连接常见的SDL流程与AI原则。

Result: 提出了一个基于能力的分类法，组织了系统决策范围、不确定性建模、动作参数化、约束处理、故障恢复和人类参与，并合成了基准任务模板和评估指标。

Conclusion: 论文总结了自驱动实验室（SDLs）的现状，并提出了未来研究方向，包括多模态表示、校准不确定性、安全探索和共享基准基础设施。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [217] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 通过技能图分层课程训练，智能体在复杂实时控制环境中实现高效持续学习，仅需微调部分技能即可适应新阶段。


<details>
  <summary>Details</summary>
Motivation: 研究终身智能体在不从头开始训练或覆盖先前学习行为的情况下，如何随时间扩展其能力。

Method: 通过将战斗表示为有向技能图，并采用分层课程训练其组件，将控制分解为五个可重用技能：相机控制、目标锁定、移动、躲避和治疗-攻击决策策略。

Result: 在环境从阶段1切换到阶段2时，仅需调整部分技能即可快速恢复性能，证明了该方法的高效性。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中不断进化的持续学习智能体提供了一条实用路径。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [218] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 研究提出SSEV和ReCAPAgent-SQL框架，显著提升Text-to-SQL性能，支持高效数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言生成SQL的挑战，如用户查询的歧义、模式链接的复杂性、SQL方言的泛化限制及领域特定理解的需求。

Method: 提出了SSEV（单代理自优化集成投票）流程和ReCAPAgent-SQL（基于代理的SQL框架），结合自优化、加权多数投票（WMV）及其随机变体（RWMA）以及多代理协作。

Result: SSEV在多个基准测试中表现优异，执行准确率在Spider 1.0-Dev达85.5%，Spider 1.0-Test达86.4%，BIRD-Dev达66.3%；ReCAPAgent-SQL在Spider 2.0-Lite前100查询中执行准确率达31%。

Conclusion: 该研究通过SSEV和ReCAPAgent-SQL框架，显著提升了Text-to-SQL技术的实际应用能力，支持更高效、低成本的数据驱动决策。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [219] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: Sentipolis框架通过PAD表示和情感-记忆耦合，提升了LLM代理的情感连续性和社交模拟效果，但效果因模型容量而异。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在社交模拟中情感被视为短暂线索的问题，导致情感遗忘和长期连续性不足。

Method: Sentipolis框架整合了连续的Pleasure-Arousal-Dominance (PAD)表示、双速情感动态和情感-记忆耦合。

Result: Sentipolis提升了情感基础行为、沟通和情感连续性，效果因模型容量而异。高容量模型的可信度提升，但小型模型可能下降。情感意识可能轻微降低对社会规范的遵守。

Conclusion: Sentipolis框架通过整合连续的PAD表示、双速情感动态和情感-记忆耦合，显著提升了情感基础行为、沟通和情感连续性。尽管效果因模型容量而异，但整体上支持了累积社会动态的研究。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [220] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究发现心理健康专家在评估AI回答时存在系统性分歧，建议采用保留分歧的对齐方法而非共识聚合。


<details>
  <summary>Details</summary>
Motivation: 验证在心理健康领域，专家判断是否能作为AI系统训练和评估的有效基准，尤其是在高安全风险情境下。

Method: 三位认证精神病学家使用校准的评分标准独立评估LLM生成的回答，并通过定性访谈分析分歧原因。

Result: 专家间可靠性极低（ICC 0.087--0.295），自杀和自残类别的分歧最大，且分歧是系统性的而非随机。

Conclusion: 专家在安全关键的AI评估中存在系统性分歧，反映了不同的临床框架和哲学取向，建议从基于共识的聚合转向保留并学习专家分歧的对齐方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [221] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE通过进化策略和STG优化Verilog设计，在多个基准测试中表现卓越，显著提升PPA。


<details>
  <summary>Details</summary>
Motivation: Verilog设计周期劳动密集且需专业知识，LLMs因训练数据有限和顺序推理无法满足硬件系统的形式逻辑和并发需求。

Method: 提出了EvolVE框架，分析多种进化策略（如MCTS和IGR），并利用STG加速进化过程。

Result: EvolVE在多个基准测试中表现优异，成为新SOTA，并在IC-RTL上显著优化PPA。

Conclusion: EvolVE框架在VerilogEval v2和RTLLM v2上分别达到98.1%和92%的准确率，并在IC-RTL套件上超越参赛者实现，优化PPA产品高达66%（Huffman Coding）和17%（几何平均值）。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [222] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL调试基准，通过自动注入错误和高效评估，揭示了LLMs在复杂SQL调试中的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决企业数据工程中SQL代码生成和调试的困难，尤其是高级文本到SQL LLMs的不足。

Method: 通过自动化构建工作流程注入现实错误，创建包含语法和语义错误的复杂SQL查询，并采用无执行评估框架。

Result: 评估了近30种LLMs，最佳模型在语法和语义错误上的准确率分别为36.46%和32.17%，多数模型低于20%。

Conclusion: 论文提出了OurBench基准测试，揭示了当前LLMs在企业级SQL调试中的性能差距，并探索了解决方案策略和未来方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [223] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: PPO在截止时间感知控制中表现最优，显著降低热水器能耗，相比传统方法节能26%-69%。


<details>
  <summary>Details</summary>
Motivation: 传统家用浸入式热水器系统在冬季通常连续运行，注重快速加热而非效率，忽略了可预测的需求窗口和环境热损失。研究旨在通过截止时间感知控制，在指定时间达到目标温度的同时最小化能耗。

Method: 研究采用了时间最优的bang-bang控制基线、零样本蒙特卡洛树搜索（MCTS）规划器和近端策略优化（PPO）策略，通过Gymnasium环境模拟热水器的热损失和离散动作。

Result: PPO在60步（2小时）的时间范围内表现最佳，能耗为3.23千瓦时，相比bang-bang控制的4.37至10.45千瓦时和MCTS的4.18至6.46千瓦时，节能效果显著（26%至69%）。在典型场景中，PPO比bang-bang控制节能54%，比MCTS节能33%。

Conclusion: 学习型截止时间感知控制（如PPO）在相同物理假设下显著降低了能耗，而规划器（如MCTS）无需训练即可提供部分节能效果，且训练后的学习策略推理成本近乎为零。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [224] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA通过动态路由和轻量级评分优化Mixture-of-Agents，显著降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有Mixture-of-Agents方法因密集拓扑结构导致的高成本和延迟问题，以及缺乏有效模型筛选标准和大模型池处理困难。

Method: 采用轻量级评分器进行初步筛选，结合自评估和交叉评估的混合评委机制，以及平衡性能、成本和延迟的模型排名机制。

Result: RouteMoA在多种任务和模型池规模下均优于传统MoA，大规模模型池中成本降低89.8%，延迟减少63.6%。

Conclusion: RouteMoA通过动态路由和轻量级评分机制，显著降低了Mixture-of-Agents的成本和延迟，同时保持了性能优势。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [225] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一种罕见病早期筛查系统，通过整合多个LLM的推理信号并校准，显著提高了罕见病风险预测的准确性，适合大规模部署。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是医疗领域的重大挑战，现有初级护理分诊流程无法可靠识别罕见病患者，需要通用筛查以减少诊断延迟。

Method: RareAlert整合了十个LLM生成的推理信号，利用机器学习进行校准和加权，并将对齐的推理蒸馏成一个可本地部署的单一模型。

Result: 在独立测试集上，RareAlert（基于Qwen3-4B模型）的AUC达到0.917，优于所有评估的LLM和机器学习集成模型。

Conclusion: RareAlert通过整合和校准多个LLM的推理信号，成功开发了一种高效、隐私保护且可扩展的罕见病风险筛查系统，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [226] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个针对长期规划任务的新基准，揭示了当前LLM在主动信息获取和约束优化方面的不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有代理评估和LLM规划基准在长期规划和真实世界约束优化方面存在不足，需要一个新的基准来填补这一空白。

Method: 引入了DeepPlanning基准测试，包含多日旅行规划和多产品购物任务，要求主动信息获取、局部约束推理和全局约束优化。

Result: 评估显示，即使前沿代理性LLM在这些任务上也表现不佳，突显了显式推理和并行工具使用的重要性。

Conclusion: DeepPlanning 作为一个具有挑战性的基准测试，揭示了当前前沿代理性LLM在长期规划任务中的不足，并指出了改进方向，如可靠的显式推理模式和并行工具使用。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [227] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: Success conditioning solves a trust-region optimization problem, ensuring conservative policy improvement without performance degradation, with observable minimal changes indicating failure.


<details>
  <summary>Details</summary>
Motivation: To clarify the optimization problem solved by success conditioning, a widely used technique in policy improvement, and its implications.

Method: The paper proves that success conditioning solves a trust-region optimization problem with a $χ^2$ divergence constraint, automatically determined by data.

Result: Success conditioning maximizes policy improvement under a $χ^2$ divergence constraint, establishing an identity linking relative policy improvement, policy change magnitude, and action-influence.

Conclusion: Success conditioning emerges as a conservative improvement operator, ensuring performance does not degrade and observable failure when policy changes minimally.

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [228] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: GAIA框架通过迭代训练批评模型（ICM）提升GUI代理性能，实验验证其有效性且性能可逐步提升。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理操作不可逆性（单一错误动作可能导致灾难性偏离）的问题。

Method: 提出GUI Action Critic's Data Flywheel System (GAIA)，训练Intuitive Critic Model (ICM)评估代理动作正确性，并通过数据循环自我改进。

Result: 实验表明ICM能提升多种闭源和开源模型的测试时性能，且性能随数据循环逐步提高。

Conclusion: GAIA框架通过迭代式批评能力训练，显著提升了GUI代理在测试时的性能，且性能随数据循环使用逐步提高。代码和数据集将公开。

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [229] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: SAGE是一个自动化流水线，通过数据生成器和搜索代理的交互，生成高质量、难度可控的深度搜索问答对，显著提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 由于人工标注深度搜索问题的成本过高，研究团队旨在开发一种自动化方法生成高质量的问答对。

Method: SAGE采用了一个由数据生成器和搜索代理组成的流水线，两者通过多轮交互迭代优化问答对，直至达到目标难度水平。

Result: SAGE生成的问答对需要多样化的推理策略，显著提高了数据的正确性和难度。在外部评估中，使用合成数据训练的深度搜索代理在流行基准测试中性能提升了23%。

Conclusion: SAGE通过自动生成高质量、难度可控的深度搜索问答对，显著提升了深度搜索代理的性能，并在无需额外训练的情况下适应不同的搜索环境。

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [230] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究通用LLM代理在未知领域的后训练挑战，发现状态信息丰富度和规划复杂度是关键，提出随机化技术提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究通用LLM代理在未知测试领域中的后训练挑战，探索哪些RL环境属性和模型选择对跨领域性能影响最大。

Method: 通过分析RL环境和模型选择的影响，识别出状态信息丰富度和规划复杂度是影响跨领域泛化的关键因素，并提出了一种随机化技术来增加状态信息丰富度。

Result: 发现状态信息丰富度和规划复杂度是影响跨领域泛化的关键因素，而领域真实性和文本相似性并非主要因素。提出的随机化技术有效提升了跨领域鲁棒性。

Conclusion: 研究发现，增加状态信息丰富度可以有效提升跨领域鲁棒性，并提出了一种低开销且广泛适用的随机化技术。此外，模型选择中的SFT预热或中期训练有助于防止RL期间的灾难性遗忘，但会削弱对未包含在中期训练数据中的领域的泛化能力。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [231] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: ShopSimulator是一个用于评估和训练LLM在电商购物场景中的统一环境，研究发现结合SFT和RL能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的模拟环境来全面评估LLM在电商购物中的表现，尤其是缺乏训练支持。

Method: 引入ShopSimulator，一个大规模的中文购物环境，用于评估和训练LLM。

Result: 即使表现最佳的模型在ShopSimulator中的全成功率也低于40%，错误分析显示代理在深度搜索、产品选择和用户互动方面存在困难。

Conclusion: 通过ShopSimulator的评估和训练探索，研究发现结合监督微调（SFT）和强化学习（RL）能显著提升LLM在电商购物场景中的表现。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [232] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式和Yunjue Agent系统，通过工具进化和并行批处理策略，在开放环境中动态扩展能力，实验验证了其有效性和知识迁移潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统代理系统在开放环境中因静态工具集或离线训练而无法适应动态任务分布的局限性。

Method: 采用工具进化作为能力扩展的关键路径，提出Parallel Batch Evolution策略优化进化效率。

Result: 在五个不同基准测试中，零起点设置下表现出显著性能提升，补充的热启动评估验证了通用知识的无缝迁移。

Conclusion: 论文提出了一种新颖的In-Situ Self-Evolving范式，通过Yunjue Agent系统实现了在开放环境中动态扩展能力，并通过实验验证了其显著性能提升和知识迁移能力。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [233] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: TAFC通过显式推理增强函数调用，提升准确性和可解释性，无需修改LLMs架构。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在函数调用中缺乏参数生成的显式推理透明度，尤其是对具有相互依赖参数的复杂函数。

Method: 提出Think-Augmented Function Calling (TAFC)框架，引入通用“think”参数增强，动态优化参数描述，并基于复杂性评分触发细粒度推理。

Result: 在ToolBench上的评估显示，TAFC在多参数函数的参数生成准确性和推理连贯性方面有显著提升。

Conclusion: TAFC框架通过显式推理在功能和参数级别提升了函数调用的准确性，同时保持了与现有LLMs的API兼容性，显著提高了多参数函数的参数生成准确性和推理连贯性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [234] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的灾害通信系统，通过个性化建议提高保护行动的执行效率，减少响应时间，并增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 传统的早期预警系统（EWS）虽然能快速传播警报，但往往无法及时触发保护行动，导致可预防的损失和不平等现象加剧。

Method: Climate RADAR整合了气象、水文、脆弱性和社会数据，形成综合风险指数，并采用带有防护栏的大型语言模型（LLMs）提供个性化建议。

Result: 通过模拟、用户研究和市政试点评估，Climate RADAR显示出更高的保护行动执行率、更低的响应延迟以及更高的可用性和信任度。

Conclusion: Climate RADAR通过结合预测分析、行为科学和负责任的人工智能，提供了一个以人为本、透明且公平的早期预警系统，为符合要求的灾害韧性基础设施提供了实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [235] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 生成式AI在模仿作家风格上表现优异，甚至逆转专家评委偏好，引发对人类创意写作独特性的质疑。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI是否能挑战人类在创意写作中的独特性，尤其是在模仿作家风格方面。

Method: 进行了行为实验，28名MFA作家与三个LLM模型竞争模仿50位知名作家风格，通过盲测比较专家和普通评委的偏好。

Result: 专家评委在初始条件下82.7%偏好人类写作，但在AI微调后偏好逆转至62%支持AI；普通评委则始终更偏好AI写作。

Conclusion: 研究挑战了AI在创意写作中的局限性，并引发了对创意劳动未来的根本性思考。

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [236] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: DynTS通过选择性保留关键token的KV缓存，优化了LRMs的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成推理痕迹时产生高内存和计算开销，影响了效率。研究发现只有部分决策关键token对最终答案有显著影响。

Method: 提出了Dynamic Thinking-Token Selection (DynTS)方法，利用注意力图分析推理痕迹的影响，识别决策关键token，并仅保留其KV缓存状态。

Result: DynTS方法有效减少了冗余KV缓存状态，优化了LRMs的推理效率。

Conclusion: DynTS通过识别并保留决策关键token的KV缓存状态，显著优化了LRMs的效率，同时保持了模型的推理能力。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [237] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 离线训练研究代理OffSeeker（8B）通过开源工具和数据集，无需昂贵在线RL，性能媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前高性能研究代理依赖昂贵的在线强化学习，而离线训练因高质量研究轨迹稀缺进展缓慢。本研究旨在证明离线训练的潜力，并提供开源解决方案。

Method: 引入了一个完全开源的工具套件，包括DeepForge任务合成框架和高质量数据集（66k QA对、33k SFT轨迹和21k DPO对），用于离线训练OffSeeker模型。

Result: OffSeeker（8B）在六个基准测试中表现优于同类模型，并与通过在线RL训练的30B参数系统竞争。

Conclusion: 本研究证明，离线训练可以构建强大的研究代理，无需依赖昂贵的在线强化学习。通过开源工具和资源，如DeepForge和高质量数据集，训练出的OffSeeker模型在多个基准测试中表现优异。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [238] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 本文提出了一个三维分类法和AgentDoG框架，用于解决AI代理在工具使用和环境交互中的复杂安全挑战，实现了细粒度监控和风险诊断，性能领先。


<details>
  <summary>Details</summary>
Motivation: 当前护栏模型缺乏代理风险意识和风险诊断的透明度，无法覆盖复杂且众多的风险行为。

Method: 提出了一个统一的三维分类法，按来源（where）、失败模式（how）和后果（what）正交分类代理风险，并基于此引入了细粒度的代理安全基准（ATBench）和诊断性护栏框架（AgentDoG）。

Result: AgentDoG在多样化和复杂的交互场景中实现了最先进的代理安全性调节性能，提供了细粒度和上下文监控，并能诊断不安全行为及看似安全但不合理行为的根本原因。

Conclusion: AgentDoG框架在多样化和复杂的交互场景中实现了最先进的代理安全性调节性能，所有模型和数据集均已公开发布。

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [239] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed通过优化医学数据合成、训练和推理方法，解决了医学推理中的上下文理解和工具调用问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 医学推理模型受限于参数化知识，易出现遗忘和幻觉问题；通用DR模型虽能检索信息但缺乏临床上下文推理能力，且盲目扩展工具调用会引入噪声干扰敏感医学推理。

Method: 采用多跳医学搜索QA合成方法支持DR范式在医学场景中的应用，引入难度感知的轮次惩罚抑制过度工具调用，推理时通过监控验证假设并避免上下文腐化。

Result: DeepMed在七个医学基准测试中平均提升9.79%，优于其他医学推理和DR模型。

Conclusion: DeepMed通过多跳医学搜索QA合成方法、难度感知的轮次惩罚和推理监控，显著提升了医学推理性能，在七个医学基准测试中平均提升了9.79%，优于其他医学推理和DR模型。

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [240] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC框架通过动态数据集评估LLMs的指令遵循能力，发现其表现受约束类型、数量和位置影响，为模型改进提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以反映真实使用情况或区分指令遵循与任务成功，因此需要更精细的评估方法。

Method: 引入MOSAIC框架，使用动态生成的数据集（包含多达20个应用导向的生成约束）进行独立分析。

Result: 评估显示，LLMs的指令遵循能力因约束类型、数量和位置而异，并揭示了模型特定弱点及指令间的协同/冲突。

Conclusion: MOSAIC框架揭示了LLMs在遵循复杂指令方面的能力差异，为诊断模型失败和开发更可靠的LLMs提供了关键见解。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [241] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 研究发现，训练稳定性虽确保平滑损失收敛，但可能导致生成模型集中在少数模式上，输出低熵且重复，表明稳定性与生成质量不直接相关。


<details>
  <summary>Details</summary>
Motivation: 分析训练动态的稳定性如何影响诱导的生成分布。

Method: 使用基于反馈的受控训练框架来稳定内部生成统计量。

Result: 稳定的参数轨迹导致平稳解近似最小化前向KL散度到经验分布，同时隐式减少生成熵，模型可能在有限的实证模式子集上集中概率质量，表现出系统性退化。

Conclusion: 优化稳定性与生成表达能力并非固有对齐，稳定性本身不足以作为生成质量的指标。

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [242] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 结合LLM和逻辑求解器，通过迭代增强常识推理，显著提升逻辑问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在形式推理方面表现出色，但在需要复杂证明规划的问题上表现不佳。现有的逻辑求解器虽然效率高，但无法处理缺失的常识关系。

Method: 使用逻辑求解器的反馈，以迭代方式通过搜索潜在的常识假设来增强逻辑问题，最大化找到有用事实的机会，同时控制成本。

Result: 在去除了部分常识信息的纯逻辑推理数据集上，该方法相比现有技术取得了显著改进。

Conclusion: 论文提出了一种结合神经和符号元素的方法，通过逻辑求解器的反馈迭代增强LLM的常识推理能力，显著提升了在缺乏常识信息的情境下的逻辑推理性能。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [243] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: PolySHAP通过高阶多项式改进KernelSHAP，提供更准确的Shapley值估计，并首次理论证明了配对采样的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管KernelSHAP避免了计算Shapley值的指数成本，但其线性近似可能忽略了特征间的非线性交互。

Method: 扩展KernelSHAP，通过高阶多项式近似游戏，捕捉特征间的非线性交互，提出了PolySHAP方法。

Result: PolySHAP在各种基准数据集上提供了更好的Shapley值估计，并证明了估计的一致性。此外，发现配对采样与二阶PolySHAP输出相同的近似值。

Conclusion: PolySHAP方法通过高阶多项式近似游戏，提供了更好的Shapley值估计，并证明了这些估计的一致性。同时，研究发现配对采样（antithetic sampling）与二阶PolySHAP输出相同的Shapley值近似，为配对采样的优秀实践性能提供了首次理论支持。

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [244] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan,Pablo Diego-Simón,Emmnanuel Chemla,Yair Lakretz,Yves Boubenec,Jean-Rémi King*

Main category: cs.AI

TL;DR: 人工神经网络在训练中会依次形成音素、词汇和句法的表征，但所需数据量远多于儿童，为语言习得的计算机制提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 尽管儿童语言习得的行为发展已有很好的描述，但仍缺乏一个统一的计算框架来解释其背后的神经表征。

Method: 通过研究语音和文本模型在训练过程中神经激活的变化，分析其是否及何时形成音素、词汇和句法表征。

Result: 研究发现，语音和文本模型在训练过程中会依次形成音素、词汇和句法的神经表征，但其所需数据量比儿童语言习得多出2到4个数量级。

Conclusion: 研究结果表明，人工神经网络在训练过程中能够自发形成语言习得的主要阶段，这为理解语言习得的计算基础提供了有希望的路径。

Abstract: During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [245] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 论文评估了LLMs在心理健康对话中的表现，发现其在认知上可靠但情感共鸣不足，提出了需关注关系敏感性的评估框架。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机加剧，治疗缺口大，LLMs作为可扩展支持途径的潜力与挑战并存。

Method: 采用人类基础评估方法，评估了9种不同LLM生成的500个心理健康对话响应，由两名精神病学专家独立评分。

Result: LLMs在认知支持上表现可靠，但在情感共鸣上不稳定，闭源模型表现更平衡，开源模型变异性大。

Conclusion: 论文强调需要在心理健康导向的LLMs中优先考虑关系敏感性，并提出了一个框架来指导负责任的设计和临床监督。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [246] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner通过自适应学习和强化学习，提升多模态模型的工具使用能力，实现泛化和高性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在视觉推理中如何有效选择、调用和组合工具的问题，尤其是在面对新工具或新任务时。

Method: AdaReasoner采用（i）可扩展的数据处理流程，（ii）Tool-GRPO强化学习算法优化工具选择和序列，（iii）自适应学习机制动态调节工具使用。

Result: AdaReasoner在多个挑战性基准测试中表现优异，7B基础模型平均提升24.9%，超越GPT-5等强专有系统。

Conclusion: AdaReasoner通过自适应学习机制和强化学习算法，实现了在未见过的工具和任务上的泛化能力，显著提升了多模态大语言模型的视觉推理性能。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [247] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: FadeMem是一种受人类记忆启发的代理记忆架构，通过选择性遗忘机制优化记忆效率，实验证明其显著提升性能并减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的二元记忆策略（全保留或全丢失）无法平衡记忆的保留与遗忘，导致在上下文边界出现灾难性遗忘或信息过载。人类记忆通过适应性衰减过程自然平衡保留与遗忘，这为改进AI记忆系统提供了灵感。

Method: FadeMem采用双层级记忆架构，结合基于语义相关性、访问频率和时间模式的适应性指数衰减函数，通过LLM引导的冲突解决和智能记忆融合实现选择性遗忘。

Result: 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，FadeMem在多跳推理和检索任务中表现优异，同时减少了45%的存储需求。

Conclusion: FadeMem通过模拟人类记忆的适应性遗忘机制，显著提升了自主代理在多跳推理和检索任务中的表现，同时减少了45%的存储需求，验证了生物启发式遗忘在代理记忆系统中的有效性。

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [248] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: TEA-Bench是首个评估工具增强情感支持对话代理的基准测试，实验显示工具增强对高质量模型效果显著，但弱模型受益有限。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统和基准测试主要关注文本情感支持，忽视了外部工具在多轮情感支持中的作用。

Method: 引入了TEA-Bench，一个交互式基准测试，用于评估工具增强的情感支持对话代理，包括真实情感场景、MCP风格工具环境和过程级指标。

Result: 实验表明，工具增强普遍提高了情感支持质量并减少了幻觉，但效果因模型能力而异。

Conclusion: 工具增强在构建可靠的情感支持对话系统中具有重要作用，尤其是对能力更强的模型效果更显著。

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [249] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE是一个可扩展的量规评估框架，降低开发成本并保持评估质量，适用于医疗领域。


<details>
  <summary>Details</summary>
Motivation: 解决高质量、领域特异性量规开发成本高、难以规模化的问题。

Method: 引入Health-SCORE框架，通过结构化奖励信号指导强化学习，并直接融入提示中以提升响应质量。

Result: Health-SCORE在开放型医疗任务中达到与人工创建量规相当的评估质量，同时显著降低开发成本。

Conclusion: Health-SCORE框架在保持评估质量的同时显著降低了开发成本，使得基于量规的评估和训练更具可扩展性。

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [250] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: AI辅助药物设计方法通过靶向降解Abeta-42，为神经退行性疾病治疗提供了新策略。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的病理特征是Abeta-42的积累，导致突触功能障碍和神经退行性变。尽管细胞外淀粉样斑块已被广泛研究，但越来越多的证据表明细胞内Abeta-42是疾病进展的早期和毒性驱动因素。

Method: 采用基于结构的建模、ADMET筛选和对接技术，系统评估了Abeta-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力，并开发了LC-JT-VAE模型生成连接酶特异性小分子。

Result: 生成模型能够产生化学有效、新颖且靶向特异性分子胶，促进Abeta-42降解。

Conclusion: 本研究提出了一种集成AI辅助药物设计的方法，通过靶向降解Abeta-42为神经退行性疾病治疗提供了新框架。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [251] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Jiawei Yao,Jian Wang,Guanlong Qu,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: Agora框架通过市场机制协调多智能体视觉语言模型，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式代理，忽略成本且破坏不确定性结构，导致协调效率低下且成本不可持续。

Method: Agora将认知不确定性形式化为可交易资产，并基于理性经济规则推动智能体间的盈利驱动交易，同时引入市场感知的经纪人引导系统实现成本高效的均衡。

Result: 在五个多模态基准测试中，Agora表现优于现有方法，如在MMMU上准确率提升8.5%，同时成本降低3倍以上。

Conclusion: Agora框架通过将协调问题重新定义为不确定性市场，提供了一种经济可持续的多智能体视觉智能系统构建范式。

Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [252] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个多模态时间序列推理基准测试，涵盖4个维度和15个任务，实验揭示了规模定律的局限性及多模态融合的不足。


<details>
  <summary>Details</summary>
Motivation: 填补现有通用模型基准测试中时间序列推理能力的空白。

Method: 引入TSRBench，一个多模态基准测试，包含4125个问题，覆盖14个领域，分为4个维度（感知、推理、预测、决策）和15个任务。

Result: 实验发现：1) 规模定律在感知和推理中成立，但在预测中失效；2) 强推理能力不保证准确的上下文感知预测；3) 当前多模态模型未能有效融合文本和视觉表示。

Conclusion: TSRBench作为一个标准化评估平台，不仅突出了现有挑战，还为推动通用模型发展提供了宝贵见解。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [253] [Parallel Algorithm For Finding The Minimum s/t Cut in a Structured 3-Dimensional Proper Order Graph](https://arxiv.org/abs/2601.17026)
*Shridharan Chandramouli*

Main category: cs.DS

TL;DR: 本文提出了两种并行算法（分层合并和推送-重标）用于计算3维适当顺序图中的最小s-t割，推送-重标算法通过级别同步全局重标提高了效率。


<details>
  <summary>Details</summary>
Motivation: 适当顺序图的多列结构自然地出现在地质层位分割的表面提取问题中，需要高效的并行算法来解决。

Method: 开发了两种并行方法：Boykov-Kolmogorov算法的分层合并变体，以及一种新颖的并行推送-重标算法，具有级别同步全局重标功能。

Result: 推送-重标变体通过沿列分割图并消除全局共享队列的需求，实现了高效的并行计算。

Conclusion: 本文提出了一种并行算法，用于计算结构化3维适当顺序图中的最小s-t割，该算法在图像分割问题中表现优异。

Abstract: We present a parallel algorithm for computing the minimum s-t cut in structured 3-dimensional proper order graphs arising from image segmentation problems. Proper order graphs are multi-column structures where vertices are arranged in parallel columns, with each vertex connected to consecutive vertices in adjacent columns. This graph structure naturally arises in surface extraction problems for geological horizon segmentation in seismic imaging volumes. We develop two parallel approaches: a hierarchical merging variant of the Boykov-Kolmogorov algorithm, and a novel parallel push-relabel algorithm with level synchronized global relabeling. Our primary contribution is the push-relabel variant, which partitions the graph into segments along columns with processor affinity, eliminating the need for a global shared queue. We introduce level synchronized global relabeling that enables concurrent label updates while maintaining correctness through barriers at each frontier level.

</details>


### [254] [Minimizing Completion Times of Stochastic Jobs on Parallel Machines is Hard](https://arxiv.org/abs/2601.17425)
*Benjamin Moseley,Kirk Pruhs,Marc Uetz,Rudy Zhou*

Main category: cs.DS

TL;DR: 本文证明了调度随机作业在特定条件下的固有难处理性，填补了研究空白。


<details>
  <summary>Details</summary>
Motivation: 尽管过去二十年有大量关于近似算法的研究，但在输入分布受限的情况下，性能保证仍非常有限。本文旨在填补这一研究空白，证明问题的固有难处理性。

Method: 本文采用理论证明的方法，针对离散两点处理时间分布和单位权重的特殊情况，证明了调度问题的#P-难解性。

Result: 证明了在离散两点处理时间分布和单位权重的情况下，决策问题是#P-难的，且评估标准(W)SEPT贪婪策略的期望目标值也是#P-难的。

Conclusion: 本文通过证明调度随机作业的固有难处理性填补了研究空白，特别是针对离散两点处理时间分布和单位权重的情况，证明了决策问题的#P-难解性。

Abstract: This paper considers the scheduling of stochastic jobs on parallel identical machines to minimize the expected total weighted completion time. While this is a classical problem with a significant body of research on approximation algorithms over the past two decades, constant-factor performance guarantees are currently known only under very restrictive assumptions on the input distributions, even when all job weights are identical. This algorithmic difficulty is striking given the lack of corresponding complexity results: to date, it is conceivable that the problem could be solved optimally in polynomial time.
  We address this gap with hardness results that demonstrate the problem's inherent intractability. For the special case of discrete two-point processing time distributions and unit weights, we prove that deciding whether there exists a scheduling policy with expected cost at most a given threshold is #P-hard. Furthermore, we show that evaluating the expected objective value of the standard (W)SEPT greedy policy is itself #P-hard. These represent the first hardness results for scheduling independent stochastic jobs and min-sum objective that do not merely rely on the intractability of the underlying deterministic counterparts.

</details>


### [255] [Split Algorithm in Linear Time for the Vehicle Routing Problem with Simultaneous Pickup and Delivery and Time Windows](https://arxiv.org/abs/2601.17572)
*Ethan Gibbons,Mario Ventresca,Beatrice M. Ombuki-Berman*

Main category: cs.DS

TL;DR: 本文扩展了线性Split算法，使其能同时处理VRPSPD和VRPTW，验证了其速度优势。


<details>
  <summary>Details</summary>
Motivation: 现有的线性Split算法在CVRP中表现出色，但在其他VRP变体中的应用有限，限制了其通用性。

Method: 通过扩展线性Split算法，同时处理VRPSPD和VRPTW两种变体，并引入容量惩罚和时间扭曲惩罚函数。

Result: 扩展后的线性Split算法在满足三角不等式的情况下保证了最优性，并在计算实验中显示出速度优势。

Conclusion: 本文提出的线性Split算法扩展了其在VRPSPD和VRPTW中的应用，并通过计算实验验证了其速度优势。

Abstract: For many kinds of vehicle routing problems (VRPs), a popular heuristic approach involves constructing a Traveling Salesman Problem (TSP) solution, referred to as a long tour, then partitioning segments of the solution into routes for different vehicles with respect to problem constraints. Previously, a Split algorithm with a worst-case runtime of $Θ(n)$ was proposed for the capacitated VRP (CVRP) that finds the most cost-efficient partition of customers, given a long tour. This was an improvement over the previously fastest-known Split algorithm with a worst-case runtime of $Θ(n^2)$ that was based on Bellman's shortest path algorithm. While this linear Split has been an integral part of modern state-of-the-art CVRP approaches, little progress has been made in extending this algorithm to handle additional VRP variants, limiting the general applicability of the algorithm. In this work, we propose an extension of the linear Split that handles two cardinal VRP variants simultaneously: (i) simultaneous pickups and deliveries (VRPSPD) and (ii) time windows (VRPTW). The resulting $Θ(n)$ algorithm is guaranteed to be optimal, assuming travel times between nodes satisfy the triangle inequality. Additionally, we extend the linear Split to handle a capacity penalty for the VRPSPD. For the VRPTW, we extend the linear Split to handle the CVRP capacity penalty in conjunction with the popular time warp penalty function. Computational experiments are performed to empirically validate the speed gains of these linear Splits against their $Θ$($n^2$) counterparts.

</details>


### [256] [Sampling Sphere Packings with Continuum Glauber Dynamics](https://arxiv.org/abs/2601.18748)
*Aiya Kuchukova,Santosh Vempala,Daniel J. Zhang*

Main category: cs.DS

TL;DR: 通过新技术扩展了连续Glauber动力学的快速混合范围，并改进了球体采样阈值。


<details>
  <summary>Details</summary>
Motivation: 扩展连续Glauber动力学在硬球模型上的快速混合参数范围，并改进固定数量球体在有限域内采样的阈值。

Method: 引入了连续扩展的谱独立性和负场定位技术，适用于具有有限范围排斥对势的一般Gibbs点过程。

Result: 证明了在强空间混合假设下，连续Glauber动力学在硬球模型上具有谱间隙，并改进了球体采样的阈值。

Conclusion: 本研究通过引入连续扩展的谱独立性和负场定位技术，证明了在强空间混合假设下，连续Glauber动力学在硬球模型上具有谱间隙，从而扩展了其快速混合的参数范围。

Abstract: We establish a spectral gap for Continuum Glauber dynamics on the hard sphere model assuming strong spatial mixing, thereby extending the range of parameters in which Continuum Glauber is provably rapidly mixing. To do this, we introduce continuous extensions of spectral independence and negative fields localization. Our techniques apply to general Gibbs point processes with finite-range repulsive pair potentials. As a corollary, we improve the threshold up to which packings of a fixed number of spheres can be sampled from a bounded domain.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [257] [LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction](https://arxiv.org/abs/2601.18475)
*Xinhui Liu,Can Wang,Lei Liu,Zhenghao Chen,Wei Jiang,Wei Wang,Dong Xu*

Main category: cs.GR

TL;DR: StreamLoD-GS 是一个专为 SFVV 设计的高斯泼溅框架，通过分层高斯丢弃、动态静态分离和量化残差精炼，实现了高质量、高效和低存储的实时流媒体。


<details>
  <summary>Details</summary>
Motivation: 为了解决 Free-Viewpoint Video (FVV) 重建中的实时流媒体瓶颈问题，如稀疏视图输入、高昂的训练成本和带宽限制。

Method: StreamLoD-GS 是一个基于 LoD 的高斯泼溅框架，集成了锚点和八叉树结构的 LoD 3DGS、GMM 运动分区机制和量化残差精炼框架。

Result: StreamLoD-GS 在质量、效率和存储方面表现优异，实验证明其性能达到了竞争性或最先进水平。

Conclusion: StreamLoD-GS 通过其创新的分层高斯丢弃技术、动态与静态内容分离机制以及量化残差精炼框架，在质量、效率和存储方面达到了竞争性或最先进的性能。

Abstract: Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [258] [Risk-based test framework for LLM features in regulated software](https://arxiv.org/abs/2601.17292)
*Zhiyin Zhou*

Main category: cs.SE

TL;DR: 本文提出了一种针对受监管软件中LLM特性的风险测试框架，包括风险分类、分层测试策略及案例验证，旨在解决幻觉、隐私等风险问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地嵌入到受监管和安全关键的软件中，带来了幻觉、有害建议、隐私安全问题等多种风险，现有机器学习测试和AI保证方法对此类交互式、产品嵌入式助手的指导有限。

Method: 提出了一种六类风险分类法和分层测试策略，将风险映射到护栏、编排和系统层的具体测试中，并通过案例研究验证方法的有效性。

Result: 通过案例研究验证了所提出的测试框架在临床研究平台知识库助手中的适用性和有效性。

Conclusion: 本文提出了一个基于风险的测试框架，用于规范软件中的LLM特性，包括六类风险分类法、分层测试策略以及在一个临床研究平台的知识库助手上的案例研究。

Abstract: Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.

</details>


### [259] [YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group](https://arxiv.org/abs/2601.17390)
*Yayi Wang,Shenao Wang,Jian Zhao,Shaosen Shi,Ting Li,Yan Cheng,Lizhong Bian,Kan Yu,Yanjie Zhao,Haoyu Wang*

Main category: cs.SE

TL;DR: YASA是一个多语言静态污点分析框架，通过UAST和统一语义模型提升分析效率，在工业级应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代企业采用多语言技术栈，现有静态分析工具在中间表示设计、分析精度和扩展性上存在局限，难以满足工业级需求。

Method: YASA引入了统一抽象语法树（UAST）进行点对点分析和污点传播，结合统一语义模型和语言特定模型处理多语言特性。

Result: YASA在行业基准测试中优于6种单语言和2种多语言分析工具，实际部署中分析了1亿行代码，发现314条未知污点路径，其中92条被确认为0-day漏洞。

Conclusion: YASA框架通过统一的抽象语法树（UAST）和多语言语义模型，显著提升了静态污点分析的效率和扩展性，适用于大规模工业级应用。

Abstract: Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.

</details>


### [260] [Fingerprinting AI Coding Agents on GitHub](https://arxiv.org/abs/2601.17406)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究通过分析33,580个PR，识别了五大AI编码代理的独特行为指纹，为识别AI贡献提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 研究AI编码代理在代码贡献中的行为特征，以解决代码作者归属问题，对仓库治理、研究有效性和理解现代开发实践具有重要意义。

Method: 分析了33,580个来自五大AI编码代理（OpenAI Codex、GitHub Copilot、Devin、Cursor、Claude Code）的PR，使用41个特征（包括提交消息、PR结构和代码特征）进行多类代理识别。

Result: 在多类代理识别中达到了97.2%的F1分数，发现了独特的行为指纹：Codex显示独特的多行提交模式（67.5%特征重要性），Claude Code表现出独特的代码结构（条件语句占27.2%重要性）。

Conclusion: AI编码工具在代码贡献中产生了可检测的行为模式，这为识别软件仓库中的AI贡献提供了可能性。

Abstract: AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.

</details>


### [261] [When AI Agents Touch CI/CD Configurations: Frequency and Success](https://arxiv.org/abs/2601.17413)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: AI代理很少修改CI/CD配置（主要改GitHub Actions），变更可靠性高。Copilot在CI/CD中表现突出，暗示配置专业化趋势。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在CI/CD配置中的交互行为，填补现有研究的空白。

Method: 分析了8,031个AI代理的PR（来自1,605个GitHub仓库），重点关注YAML配置的修改情况。统计了CI/CD变更的比例、目标平台、合并率及构建成功率。

Result: CI/CD配置变更占AI代理修改的3.25%，96.77%针对GitHub Actions。CI/CD变更的PR合并率略低（67.77% vs. 71.80%），但Copilot例外（高15.63%）。构建成功率相近（75.59% vs. 74.87%），但部分代理在CI/CD变更中表现更优。

Conclusion: AI agents rarely modify CI/CD配置，主要集中于GitHub Actions，但其配置变更与常规代码同样可靠。Copilot在CI/CD中的出色表现暗示了配置专业化的趋势，对代理训练和DevOps自动化有重要启示。

Abstract: AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.

</details>


### [262] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: DALIA是一种声明式架构层，通过分离发现、规划和执行，解决多智能体系统的可靠性问题，确保工作流程可验证和可复现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统存在可靠性问题，如幻觉行为、不可执行计划和脆弱协调，根源在于缺乏明确的目标、能力和执行之间的架构联系。

Method: 提出了一种声明式、模型无关的架构层DALIA，通过形式化可执行能力、声明式发现协议、维护联邦目录及构建确定性任务图，分离发现、规划和执行阶段。

Result: DALIA架构通过声明式设计约束智能体行为到可验证的操作空间，减少了推测推理和自由协调的依赖，实现了更可靠的智能体工作流程。

Conclusion: DALIA架构通过声明式的基础层设计，解决了多智能体系统中的可靠性问题，实现了可验证和可复现的工作流程。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [263] [Data-driven Test Generation for Fuzzing AI Compiler](https://arxiv.org/abs/2601.17450)
*Qingchao Shen*

Main category: cs.SE

TL;DR: 提出统一的数据驱动测试框架OPERA、OATest和HARMONY，分阶段测试AI编译器，检测出266个未知bug。


<details>
  <summary>Details</summary>
Motivation: AI编译器在部署AI模型时至关重要，但其存在的bug可能影响编译器可靠性和模型正确性，因此需要提升其质量。

Method: OPERA迁移AI库的测试以验证模型加载阶段的算子转换逻辑；OATest合成多样化的优化感知计算图以测试高级优化；HARMONY生成和变异多样化的低级IR种子以测试低级优化。

Result: 该框架在四种广泛使用的AI编译器中检测出266个未知bug。

Conclusion: OPERA、OATest和HARMONY共同构成了一个全面的、阶段感知的测试框架，显著提升了AI编译器的测试覆盖率和有效性，成功检测出266个未知bug。

Abstract: Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.

</details>


### [264] [LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression](https://arxiv.org/abs/2601.17482)
*Yang Liu,Kaiming Zhang,Zhuangbin Chen,Jinyang Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: LogPrism通过统一冗余树动态整合日志解析与压缩，显著提升压缩效率与速度，成为新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有‘解析-压缩’分离范式限制了日志压缩效率，忽略了静态模板与动态变量的深层关联。

Method: 提出LogPrism框架，构建统一冗余树（URT）动态整合结构提取与变量编码，挖掘‘结构+变量’共现模式。

Result: 在16个基准数据集上，LogPrism在13个数据集上实现最高压缩比（领先基线4.7%~80.9%），吞吐量达29.87 MB/s（比竞争对手快1.68×~43.04×）。单归档模式下，压缩比提升19.39%，速度优势保持2.62×。

Conclusion: LogPrism通过统一冗余编码框架显著提升了日志压缩效率，成为新的最先进技术。

Abstract: The prevailing "parse-then-compress" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines "structure+variable" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\times$~43.04$\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\times$ speed advantage.

</details>


### [265] [Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification](https://arxiv.org/abs/2601.17558)
*J. P. Fleischer,Tanchanok Sirikanchittavon,Chonlachart Jeenprasom,Nooshin Yousefzadeh,Sanjay Ranka,Mohammed Hadi*

Main category: cs.SE

TL;DR: 该论文介绍了一个开源软件，通过地面平面单应性估计分析交通摄像头 footage，无需校准即可获取车辆轨迹和制动数据。案例研究展示了其在信号化交叉口的应用效果。


<details>
  <summary>Details</summary>
Motivation: 开发一个开源软件应用，用于分析交通摄像头 footage，重点关注信号化城市高速公路上的车辆行为和制动事件，无需摄像头校准。

Method: 该软件应用采用MAGSAC++估计器建立地面平面单应性，将YOLO11对象检测转换为校正后的俯视坐标系，所有检测和轨迹数据存储在ClickHouse数据库中。

Result: 在佛罗里达州基韦斯特的两个信号化交叉口的案例研究中，白天 footage 显示高流量交叉口的制动活动在下午4点达到峰值（约57.5次/小时），而第二个交叉口在上午10点达到峰值（约15.5次/小时）。空间分析显示，大多数制动事件在上游启动，轻度至中度制动多发生在距离停车线30至45+米处，而严重制动则分布更广，但在交互和合并活动较多的车道中尤为集中。

Conclusion: 该研究展示了集中式安全信息系统的巨大潜力，可支持联网车辆，促进主动交通管理、事故缓解以及数据驱动的道路设计和安全分析。

Abstract: This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.

</details>


### [266] [How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests](https://arxiv.org/abs/2601.17581)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: AI编码代理的PR与人类PR在提交次数、修改文件数量和删除行数上存在差异，且描述与代码变更更一致。


<details>
  <summary>Details</summary>
Motivation: 缺乏关于AI编码代理生成的PR与人类贡献差异的实证证据，需评估其对开发工作流的可靠性和影响。

Method: 使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个合并的Agentic PR（440,295次提交）和5,081个合并的Human PR（23,242次提交），通过词法和语义相似性评估PR描述与代码变更的一致性。

Result: Agentic PR在提交次数上差异显著（Cliff's $δ= 0.5429$），在修改文件数量和删除行数上差异中等，且PR描述与代码变更的一致性略高。

Conclusion: AI编码代理在开源开发中的贡献与人类存在显著差异，特别是在提交次数、修改文件数量和删除行数上，且其PR描述与代码变更的一致性略高。

Abstract: AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.

</details>


### [267] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: 研究表明，现代LLMs能通过提示驱动开发构建生产级工具，支持新兴语言的软件开发。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在软件开发中的应用日益广泛，但其通过自然语言交互生成和维护大型多模块系统的能力尚未得到充分表征。

Method: 研究采用纯提示驱动的工作流程，使用Claude Code, Opus 4.5，通过107个提示（包括特征请求、错误修复、架构指导等）开发了一个7420行的终端用户界面框架。

Result: 开发出的框架包括完整的窗口子系统、事件驱动架构、交互式小部件、分层菜单、网格和树组件、标签控件以及多窗口桌面环境。

Conclusion: 本研究通过实证分析表明，现代大型语言模型（LLMs）能够保持架构一致性，并支持为新兴编程语言构建生产级工具，提示驱动开发在软件工程实践中是一种可行的方法论。

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [268] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 研究探讨LLM如何通过整合用户反馈改进编程答案，开发了AUTOCOMBAT工具，其表现接近人类质量，并受到实践者高度认可。


<details>
  <summary>Details</summary>
Motivation: 技术问答平台上，约三分之一的用户反馈未被处理，导致答案不完整或过时。本研究探讨LLM能否通过解释和整合基于评论的反馈来改进编程答案。

Method: 本研究引入了ReSOlve基准，评估了四种最先进的LLM在识别可操作问题上的能力，并开发了AUTOCOMBAT工具，该工具结合用户评论和问题上下文改进编程答案。

Result: AUTOCOMBAT在改进编程答案方面接近人类质量，显著优于基线，用户研究显示84.5%的实践者表示会采用或推荐该工具。

Conclusion: AUTOCOMBAT展示了通过反馈驱动的答案改进在提升技术知识平台可靠性和可信度方面的潜力。

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [269] [Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests](https://arxiv.org/abs/2601.17627)
*Dung Pham,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: AI代理生成的PR在代码变更上更频繁但短暂，提交消息更精准，但PR总结稍弱，显示其在宏观沟通上的不足。


<details>
  <summary>Details</summary>
Motivation: 探究AI编码代理生成的PR与人类PR在代码变更和消息质量上的差异，以评估代理的实际贡献。

Method: 分析了33,596个代理生成的PR（APR）和6,618个人类PR（HPR），比较了代码变更特征和消息质量。

Result: APR引入的符号（函数和类）比HPR更快被移除（中位数3天 vs. 34天）且移除率更高（7.33% vs. 4.10%）。代理在提交级消息上表现更好（语义相似度0.72 vs. 0.68），但在PR级总结上略逊于人类（相似度0.86 vs. 0.88）。

Conclusion: AI编码代理在微观层面的代码变更精确性上表现优异，但在宏观层面的沟通（如PR级总结）上仍有不足，这为改进代理驱动的工作流程提供了机会。

Abstract: AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.

</details>


### [270] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: MAVM是一个多智能体漏洞管理框架，通过整合历史知识库和上下文检索工具，显著提升了漏洞检测与修复的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化漏洞管理方法在上下文依赖和历史知识利用方面存在不足，导致检测和修复效果不佳。

Method: MAVM是一个多智能体框架，包含漏洞知识库、检测、确认、修复和验证五个组件，并设计了上下文检索工具以增强信息提取能力。

Result: 在包含78个真实补丁移植案例的数据集上，MAVM成功检测并修复了51个漏洞，修复准确率比基线方法高出31.9%-45.2%。

Conclusion: MAVM框架通过整合历史漏洞知识库和多智能体协作，显著提升了漏洞检测与修复的准确率，验证了其在漏洞管理中的有效性。

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [271] [iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement](https://arxiv.org/abs/2601.17888)
*Monika Santra,Bokai Zhang,Mark Lim,Vishnu Asutosh Dasu,Dongrui Zeng,Gang Tan*

Main category: cs.SE

TL;DR: iResolveX 通过混合静态和机器学习方法优化间接调用解析，显著减少误报并保持高召回率。


<details>
  <summary>Details</summary>
Motivation: 间接调用解析在逆向工程和控制流图恢复中是一个关键挑战，静态分析虽然全面但会产生大量误报，而机器学习方法可能牺牲完整性和泛化能力。

Method: iResolveX 结合了保守的值集分析（BPA）和基于学习的软签名评分器（iScoreGen）以及选择性过程间反向分析与内存检查（iScoreRefine）。

Result: 在 SPEC CPU2006 和真实二进制文件上，iScoreGen 平均减少了 19.2% 的预测目标，同时保持 98.2% 的召回率；结合 iScoreRefine 后，总减少率达到 44.3%，召回率为 97.8%。

Conclusion: iResolveX 是一个结合保守静态分析和基于学习的优化的混合多层框架，显著提高了间接调用解析的精度和召回率，优于现有系统。

Abstract: Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.

</details>


### [272] [Prompt-Based REST API Test Amplification in Industry: An Experience Report](https://arxiv.org/abs/2601.17903)
*Tolgahan Bardakci,Andreas Faes,Mutlu Beyazit,Serge Demeyr*

Main category: cs.SE

TL;DR: LLM-based test amplification is effective for REST API testing in industrial contexts, improving coverage and detecting anomalies.


<details>
  <summary>Details</summary>
Motivation: Address the lack of evidence on LLM effectiveness for REST API testing in industrial settings.

Method: Replicated earlier work on LLM-based REST API test amplification within an industrial context, applying it to six endpoints of a production microservice.

Result: Increased test coverage and revealed various observations and anomalies.

Conclusion: LLM-based test amplification proves practically useful in industrial settings, enhancing test coverage and uncovering anomalies.

Abstract: Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.

</details>


### [273] [RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models](https://arxiv.org/abs/2601.18044)
*Melika Sepidband,Hamed Taherkhani,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: 论文提出了一种新型项目级故障定位方法，通过分层推理和两阶段排名显著提高了定位准确性，从而提升了修复成功率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLM）的修复代理的兴起，故障定位（FL）在自动化程序修复（APR）中的重要性增加。由于软件仓库通常包含数百万个令牌，远超过当前LLM的上下文限制，因此需要先识别一个小的相关代码子集，使得准确的FL对有效修复至关重要。

Method: 引入了一个分层推理模块，该模块生成结构化、特定于错误的解释，并在两阶段排名方案中结合基于LLM和嵌入的信号。

Result: 在Python和Java项目上的评估显示，文件级Hit@1从71.4%提高到85%，MRR从81.8%提高到88.8%；元素级Exact Match在top-3文件下从36%提高到69%。集成到Agentless后，端到端修复成功率提高了12.8%。

Conclusion: 该论文提出的分层推理模块和两阶段排名方案显著提高了项目级故障定位的准确性，尤其在文件级和元素级定位上表现优异，最终提升了端到端修复的成功率。

Abstract: Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.

</details>


### [274] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: TAM-Eval是一个评估LLMs在测试套件维护中表现的框架，结果显示当前LLMs能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在单元测试中的应用局限于孤立测试生成或预言预测，忽视了测试套件维护的更广泛挑战。

Method: 提出了TAM-Eval框架，包含1,539个自动提取和验证的场景，支持系统无关的评估，基于测试套件通过率、代码覆盖率和变异测试。

Result: 实证结果表明，最先进的LLMs在现实测试维护过程中能力有限，仅能带来边际改进。

Conclusion: 论文介绍了TAM-Eval框架和基准测试，用于评估模型在测试套件维护中的表现，并指出当前最先进的LLMs在实际测试维护过程中的能力有限，仅能带来边际改进。

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [275] [Agentic Much? Adoption of Coding Agents on GitHub](https://arxiv.org/abs/2601.18341)
*Romain Robbes,Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 编码代理在GitHub上的采用率高达15.85%-22.60%，影响广泛，需进一步研究其实际使用。


<details>
  <summary>Details</summary>
Motivation: 编码代理作为一种高度自主的开发工具，其运作模式可能比传统代码补全LLMs对开发格局产生更大影响，因此研究其影响至关重要。

Method: 通过分析GitHub上129,134个项目，利用编码代理在软件工程工件中留下的显式痕迹（如同提交或拉取请求）进行大规模研究。

Result: 编码代理的采用率在短时间内达到15.85%-22.60%，且采用范围广泛，辅助提交的规模和内容显著不同于纯人工提交。

Conclusion: 研究发现编码代理的采用率高达15.85%-22.60%，且采用范围广泛，涵盖不同成熟度的项目、组织及编程语言。编码代理辅助的提交规模更大，且包含大量功能和错误修复。这些发现强调了对编码代理实际使用进一步研究的必要性。

Abstract: In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.

</details>


### [276] [Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries](https://arxiv.org/abs/2601.18344)
*Alexandros Tsakpinis,Efe Berk Ergülec,Emil Schwenger,Alexander Pretschner*

Main category: cs.SE

TL;DR: 研究预测OpenSSF Maintained分数的未来维护活动，通过多元时间序列预测模型，发现简单模型与深度学习表现相当，预测准确率高。


<details>
  <summary>Details</summary>
Motivation: OpenSSF Scorecard的Maintained指标仅反映过去90天的活动，无法预测未来维护，限制了其在主动风险评估中的实用性。

Method: 分析3,220个GitHub仓库，重构三年内的Maintained分数，将任务建模为多元时间序列预测，比较VARMA、随机森林和LSTM模型在不同训练窗口和预测周期下的表现。

Result: 未来维护活动可以以有意义的准确性预测，特别是对于聚合表示（如分桶分数和趋势类型），准确率分别超过0.95和0.80。简单的统计和机器学习模型与深度学习方法表现相当。

Conclusion: 预测模型可以有效补充现有的Scorecard指标，实现更主动的开源维护风险评估。

Abstract: The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.

</details>


### [277] [Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity](https://arxiv.org/abs/2601.18345)
*Romain Robes Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 研究分析了GitHub上编码代理的活动，揭示了其使用中的潜在好处、风险及实用经验。


<details>
  <summary>Details</summary>
Motivation: 编码代理的快速普及及其与LLM代码补全的显著差异，使得研究其对软件工程实践的影响变得至关重要。

Method: 通过分析GitHub上的编码代理活动痕迹，应用MSR技术进行研究。

Result: 研究发现编码代理在GitHub上的活动提供了丰富的可研究痕迹，揭示了其在实际应用中的效果和挑战。

Conclusion: 本文总结了编码代理在GitHub上的活动研究，揭示了其潜在的承诺、风险以及实用的启发式方法。

Abstract: In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.

</details>


### [278] [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418)
*Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu*

Main category: cs.SE

TL;DR: 代理性中期训练通过原生轨迹数据提升LLM在软件工程中的自主能力，资源效率高且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索代理性中期训练在大型语言模型中的应用，以解决静态训练数据与动态开发环境之间的分布不匹配问题，提供比单纯依赖强化学习更可扩展的解决方案。

Method: 提出了一种系统化的代理性中期训练方法，包括数据合成原则和训练方法，特别强调了两种原生轨迹（contextually-native和environmentally-native）的结合使用。

Result: 在SWE-Bench Verified上的测试表明，该方法优于现有中期训练方法Kimi-Dev，且资源消耗更低（73.1B tokens），32B和72B模型的解决率分别达到56.1%和58.5%。

Conclusion: 研究表明，代理性中期训练（agentic mid-training）通过结合上下文原生轨迹和环境原生轨迹，能够有效提升大型语言模型在软件工程任务中的自主能力，且在资源效率上优于现有方法。

Abstract: Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...

</details>


### [279] [An Audit of Machine Learning Experiments on Software Defect Prediction](https://arxiv.org/abs/2601.18477)
*Giuseppe Destefanis,Leila Yousefi,Martin Shepperd,Allan Tucker,Stephen Swift,Steve Counsell,Mahir Arzoky*

Main category: cs.SE

TL;DR: 审计2019-2023年SDP研究，发现实验设计和报告实践差异大，近半研究细节不足，可复现性有限，改进空间大。


<details>
  <summary>Details</summary>
Motivation: 评估软件缺陷预测（SDP）研究的实验设计、分析和报告实践，以描述当前实践并评估已发表结果的可复现性。

Method: 我们审计了2019至2023年间SCOPUS索引的SDP研究，重点关注设计和分析选择，如结果度量、样本外验证策略和统计推断的使用。评估了九个研究问题，并使用González Barahona和Robles提出的工具评估了可复现性。

Result: 搜索确定了约1,585个SDP实验。随机抽样101篇论文，观察到研究实践存在显著差异。约45%的研究应用了正式统计推断。样本中发现了427个问题，中位数为每篇论文四个问题。可复现性从接近完整到严重受限不等。

Conclusion: 实验设计和报告实践差异很大，近一半的研究提供的细节不足以支持复现。审计表明有大幅改进空间。

Abstract: Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by González Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.

</details>


### [280] [On the Abolition of the "ICSE Paper" and the Adoption of the "Registered Proposal" and the "Results Report"](https://arxiv.org/abs/2601.18566)
*Fabio Massacci,Winnie Mbaka*

Main category: cs.SE

TL;DR: 废除传统ICSE论文，改为两阶段评审（注册提案+结果报告），解决领域问题。


<details>
  <summary>Details</summary>
Motivation: 应对软件工程领域的‘新颖性恶性循环’和‘可重复性危机’，基于社区调研反馈。

Method: 引入‘注册提案’和‘结果报告’两阶段评审机制，前者聚焦新想法和方法论，后者关注实证结果。

Result: 提出颠覆性改革方案，将注册提案和结果报告均视为主流会议的一等公民。

Conclusion: 提出废除传统ICSE论文形式，采用两阶段系统（注册提案和结果报告）以解决领域内的‘新颖性恶性循环’和‘可重复性危机’。

Abstract: To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the "ICSE paper" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a "Registered Proposal" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) "Results Reports" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey

</details>


### [281] [How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization](https://arxiv.org/abs/2601.18591)
*Fiorella Zampetti,Federico Stocchetti,Federica Razzano,Damian Andrew Tamburri,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 研究显示，开发者更倾向于通过API自定义MLOps框架功能，而非直接使用，且用户需求集中在核心功能改进和CI/CD集成。


<details>
  <summary>Details</summary>
Motivation: 研究MLOps框架的实际使用情况和用户期望的功能改进，以帮助开发者更好地利用这些框架。

Method: 分析了GitHub上依赖项目对八个流行开源MLOps框架的使用情况，包括API和命令的调用方式，并从问题跟踪器中挖掘了功能请求和改进。

Result: MLOps框架通常不被直接使用，开发者通过API实现自定义功能；用户主要需求集中在核心功能增强、API改进和CI/CD集成。

Conclusion: MLOps框架很少被直接使用，开发者更倾向于通过API实现自定义功能，同时用户主要希望增强核心功能、API暴露和CI/CD集成。

Abstract: Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.

</details>


### [282] [Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests](https://arxiv.org/abs/2601.18749)
*Haruhiko Yoshioka,Takahiro Monno,Haruka Tokumasu,Taiki Wakamatsu,Yuki Ota,Nimmi Weeraddana,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究分析40,214个PR，发现提交者属性对合并结果影响最大，审查特征在人工与AI生成PR中作用相反，为人机协作优化PR质量提供依据。


<details>
  <summary>Details</summary>
Motivation: 尽管AI生成的PR创建速度快且便捷，但其合并率低于人工PR，因此需通过大规模实证分析探究影响合并结果的关键因素。

Method: 从AIDev数据集中收集40,214个PR，提取六类共64个特征，并拟合统计回归模型，比较人工和AI生成PR的合并结果及三种AI代理间的差异。

Result: 提交者属性主导了人工和AI生成PR的合并结果，而审查相关特征在两者间表现出相反效应。

Conclusion: 研究结果表明，提交者属性对人工和AI生成的PR合并结果均有显著影响，而审查相关特征在两者间表现出相反效应。这些发现为通过人机协作提升PR质量提供了见解。

Abstract: The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [283] [Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics](https://arxiv.org/abs/2601.16985)
*Pierrick Lorang*

Main category: cs.RO

TL;DR: 提出神经符号框架，结合TAMP和强化学习，提升机器人适应能力，验证了快速收敛和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决自主系统在开放世界环境中适应不可预见新事物的挑战，尤其是混合规划和强化学习方法存在的样本效率低、适应慢和灾难性遗忘问题。

Method: 采用神经符号框架，整合分层抽象、任务与运动规划（TAMP）和强化学习，结合符号目标导向学习和基于世界模型的探索。

Result: 在机器人操作和自动驾驶任务中验证，该方法实现了更快的收敛速度、更高的样本效率和优于现有混合方法的鲁棒性。

Conclusion: 该论文提出的神经符号框架结合了分层抽象、任务与运动规划（TAMP）和强化学习，显著提升了机器人在开放世界环境中的快速适应能力，验证了其在实际部署中的潜力。

Abstract: Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.

</details>


### [284] [Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap](https://arxiv.org/abs/2601.17219)
*David Wireko Atibila,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 研究提出六层次分类法，揭示建筑机器人人机协作即兴能力的现状与差距，建议未来研究加强沟通技术以实现真正协作。


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临生产力停滞、熟练劳动力短缺和安全问题，机器人自动化虽提供解决方案，但难以适应非结构化、动态的工地环境。人机协作即兴能力是维持工作流程连续性的关键。

Method: 通过系统综述214篇文章（2010-2025年），开发了一个六层次分类法，分类了建筑机器人的人机协作能力，并提出了五维雷达框架。

Result: 分析显示当前研究集中在较低层次，存在经验学习和协作即兴方面的关键差距。五维雷达框架展示了人机互补能力如何提升团队表现。

Conclusion: 研究提出了一个六层次分类法，揭示了当前研究主要集中在较低层次，并指出了技术、概念和方法上的三大障碍。建议未来研究应加强人机沟通，利用AR/VR界面、大语言模型和云端知识系统，以实现真正的协作即兴。

Abstract: The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.

</details>


### [285] [Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization](https://arxiv.org/abs/2601.17227)
*Avraiem Iskandar,Shamak Dutta,Kevin Murrant,Yash Vardhan Pant,Stephen L. Smith*

Main category: cs.RO

TL;DR: 分层框架结合全局与局部优化，高效减少不确定性，计算速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂环境中基于高斯过程的路径规划问题，平衡全局保证与计算效率。

Method: 采用三阶段分层框架：图基础的全局规划、基于几何和核边界的段预算分配，以及带硬约束和障碍物修剪的样条细化。

Result: 在合成杂乱环境和北极数据集中，该方法比图基础和连续基线方法降低了后验不确定性，计算速度比梯度方法快9倍，比黑盒优化器快20倍。

Conclusion: 该论文提出了一种分层框架，结合了全局规划和局部优化，在预算限制下有效减少了目标位置的后验不确定性，同时在计算效率上显著优于连续空间求解器。

Abstract: We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.

</details>


### [286] [Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration](https://arxiv.org/abs/2601.17231)
*Tanmay Desai,Brian Plancher,R. Iris Bahar*

Main category: cs.RO

TL;DR: FPGA优化的MPPI设计显著提升AMR的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 解决GPU和CPU实现无法满足电池受限AMR平台在嵌入式计算上的严格能耗和延迟预算的问题。

Method: 提出了一种针对FPGA优化的MPPI设计，通过深度流水线化和跨算法阶段的并行化，消除了同步瓶颈。

Result: 相比嵌入式GPU和CPU优化实现，平均提速3.1倍至7.5倍，同时能耗降低2.5倍至5.4倍。

Conclusion: FPGA架构在能效和高性能边缘机器人领域展现出巨大潜力。

Abstract: Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.

</details>


### [287] [Quantifying Ergonomics in the Elevate Soft Robotic Suit](https://arxiv.org/abs/2601.17249)
*Peter Bryan,Rejin John Varghese,Dario Farina*

Main category: cs.RO

TL;DR: Elevate软机器人服通过实验验证了其在肩部辅助运动中的人体工程学和舒适性设计，为未来患者研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 软机器人服在康复、辅助和增强人体方面具有潜力，但数据驱动、用户特定和舒适优先的设计挑战限制了其广泛应用。

Method: 使用运动捕捉系统和力传感器，测量了在辅助肩部抬高至70度时的服用人机工程学性能，进行了两次4小时的实验，涉及高达200N的电缆张力传输。

Result: 实验结果显示，辅助运动时施加在肩部的压力在人类抓握范围内（约69.1-85.1kPa），躯干和上臂的体积压缩分别小于3%和8%，且无不适报告。

Conclusion: Elevate软机器人服的设计在人体工程学和舒适性方面表现良好，为未来患者群体的研究奠定了基础。

Abstract: Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.

</details>


### [288] [EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects](https://arxiv.org/abs/2601.17251)
*Yunuo Chen,Yafei Hu,Lingfeng Sun,Tushar Kusnur,Laura Herlant,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: EMPM是一种基于可微分MPM模拟器的框架，通过多视角RGB-D视频重建和优化，实现了对复杂可变形物体的物理精确建模，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在建模可变形物体时过度简化动态特性或需要大量训练数据的问题，提升物理建模的准确性和泛化能力。

Method: EMPM采用可微分的Material Point Method (MPM)模拟器，结合多视角RGB-D视频数据，重建物体的几何和外观，并通过最小化预测与观测视觉数据的不匹配来模拟物体行为。

Result: EMPM在实验中表现优于传统的弹簧-质量基线模型，展示了其在复杂可变形物体建模和机器人操作中的潜力。

Conclusion: EMPM框架通过结合可微分的MPM模拟器和多视角RGB-D视频数据，成功实现了对复杂可变形物体的物理精确建模，并在实验中优于传统的弹簧-质量模型。

Abstract: Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.

</details>


### [289] [Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots](https://arxiv.org/abs/2601.17287)
*Yanrong Chen,Xihan Bian*

Main category: cs.RO

TL;DR: 该论文提出了一种实时框架，通过双通道情感引擎、动态时间规整和闭环验证，实现了NAO机器人语音与动作的同步，提升了情感对齐能力，适用于多种动态应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人越来越多地进入社交场景，实现情感同步的多模态交互仍是一个重大挑战。为了促进人形机器人在服务角色中的进一步采用和集成，研究提出了这一框架。

Method: 提出了一个实时框架，包含三个关键创新：(1) 双通道情感引擎，利用大语言模型（LLM）生成上下文感知的文本响应和生物力学可行的动作描述；(2) 持续时间感知的动态时间规整，用于语音输出和运动关键帧的精确时间对齐；(3) 闭环可行性验证，通过实时调整确保动作符合NAO机器人的物理关节限制。

Result: 评估显示，与基于规则的系统相比，该框架的情感对齐能力提高了21%，通过协调声调（由兴奋驱动）与上肢运动学，同时保持下半身的稳定性来实现。

Conclusion: 该框架通过实现语音与全身动作的实时同步，显著提升了人形机器人在社交场景中的情感对齐能力，为个性化医疗、互动教育和响应式客服等动态应用提供了技术支持。

Abstract: As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.

</details>


### [290] [Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms](https://arxiv.org/abs/2601.17404)
*Anke Fischer-Janzen,Thomas M. Wendt,Kristof Van Laerhoven*

Main category: cs.RO

TL;DR: 该论文提出了一种基于眼动追踪的控制框架，帮助严重身体残疾人士独立完成任务，准确率达97.9%，并开源了框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于眼动追踪的方法在3D视线估计和任务区分方面存在挑战。为了减少用户的工作量并提高机器人的自主性，作者提出了一种新的控制框架，旨在帮助严重身体残疾人士独立完成任务。

Method: 该系统使用任务图标作为基准标记，结合特征匹配方法，通过眼在手配置传输选定对象的数据，以完成必要的任务相关测量。这种方法不需要了解用户与对象的相对位置。

Result: 框架在对象和任务选择方面的正确解释率高达97.9%。评估中发现的问题已得到改进，并作为经验教训分享。开源框架的灵活性使其能够适应新任务和对象。

Conclusion: 该论文提出了一个基于眼动追踪的控制框架，旨在帮助严重身体残疾人士独立完成日常任务。通过结合任务图标和特征匹配方法，系统能够高效识别用户选择的对象和任务，准确率高达97.9%。尽管评估中发现了一些问题，但已通过改进并提出经验教训。开源框架的灵活性使其能够适应新任务和对象。

Abstract: Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.

</details>


### [291] [DiffusionCinema: Text-to-Aerial Cinematography](https://arxiv.org/abs/2601.17412)
*Valerii Serpiva,Artem Lykov,Jeffrin Sam,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的无人机创意捕捉系统，用户通过自然语言描述即可生成并执行最优飞行轨迹，显著降低了操作负担。


<details>
  <summary>Details</summary>
Motivation: 传统无人机操控需要手动操作，用户需具备专业技能且操作复杂。本文旨在通过自然语言描述实现无人机自动飞行，降低用户操作负担。

Method: 系统通过扩散模型对高级自然语言提示进行解释，并结合初始视觉快照，采样出满足场景几何和镜头语义的时空运动计划，生成无人机飞行轨迹。

Result: 用户评估显示，与传统遥控器相比，该系统显著降低了整体工作量（M = 21.6 vs. 58.1），心理需求（M = 11.5 vs. 60.5）和挫败感（M = 14.0 vs. 54.5）也明显减少。

Conclusion: 该项目展示了一种新的交互范式：文本到电影飞行，其中扩散模型作为“创意操作员”直接将故事意图转换为空中运动。

Abstract: We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., "orbit around me slowly from the right and reveal the background waterfall"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the "creative operator" converting story intentions directly into aerial motion.

</details>


### [292] [Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.17428)
*Ziming Li,Chenhao Li,Marco Hutter*

Main category: cs.RO

TL;DR: LP-ACRL通过自适应任务采样实现自动课程生成，显著提升四足机器人在复杂地形中的运动性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务空间中缺乏明确难度结构的问题，传统方法难以定义难度排序。

Method: 提出了基于学习进度的自动课程强化学习（LP-ACRL）框架，通过在线估计代理的学习进度并自适应调整任务采样分布，无需先验知识即可生成课程。

Result: LP-ACRL训练的策略使ANYmal D四足机器人在多样地形（如楼梯、斜坡、砾石和低摩擦平面）上实现并保持2.5 m/s线速度和3.0 rad/s角速度的高速稳定运动。

Conclusion: LP-ACRL框架展示了在复杂、广泛任务空间中自动生成课程的能力，为未来机器人学习研究提供了稳健的基线。

Abstract: Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.

</details>


### [293] [PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes](https://arxiv.org/abs/2601.17440)
*Xinru Cui,Linxi Feng,Yixuan Zhou,Haoqi Han,Zhe Liu,Hesheng Wang*

Main category: cs.RO

TL;DR: PILOT是一种新型强化学习框架，通过融合感知与运动控制，提升人形机器人在复杂环境中的稳定性和精确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有全身控制器缺乏对外部环境的感知能力，无法在复杂非结构化场景中稳定执行任务的问题。

Method: 提出PILOT，一种统一的单阶段强化学习框架，结合感知运动与全身控制，设计跨模态上下文编码器和Mixture-of-Experts策略架构。

Result: 在仿真和物理机器人Unitree G1上的广泛实验验证了PILOT的优越性。

Conclusion: PILOT框架在非结构化场景中展现出卓越的稳定性、命令跟踪精度和地形穿越能力，有望成为鲁棒的基础低级控制器。

Abstract: Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.

</details>


### [294] [EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds](https://arxiv.org/abs/2601.17486)
*Zhiyuan Zhang,Yu She*

Main category: cs.RO

TL;DR: EquiForm 是一个噪声鲁棒的SE(3)-等变策略学习框架，通过几何去噪和对齐目标提升点云模仿学习的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的等变方法主要将对称约束编码到神经架构中，但未显式纠正噪声引起的几何偏差或强制学习表示的等变一致性。

Method: EquiForm 提出了一个几何去噪模块来恢复噪声或部分观测下的3D结构一致性，并引入对比等变对齐目标来增强表示一致性。

Result: EquiForm 在16个模拟任务和4个真实世界操作任务中表现优异，显著提升了性能。

Conclusion: EquiForm 在模拟和真实世界的操作任务中表现出色，相比现有方法在模拟中平均提升了17.2%，在真实世界中提升了28.1%，展示了强大的噪声鲁棒性和空间泛化能力。

Abstract: Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.

</details>


### [295] [MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions](https://arxiv.org/abs/2601.17507)
*Yutong Shen,Hangxu Liu,Kailin Pei,Ruizhe Xia,Tongtong Feng*

Main category: cs.RO

TL;DR: MetaWorld通过分层模型和专家策略库，解决了人形机器人操作中的语义-物理鸿沟问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人操作存在强化学习样本效率低、模仿学习泛化性差以及VLM物理不一致性等问题，亟需新方法。

Method: 提出MetaWorld分层世界模型，结合VLM驱动的语义层和潜在动态模型，采用动态专家选择和运动先验融合机制，利用预训练的多专家策略库进行知识迁移。

Result: 在Humanoid-Bench上的实验表明，MetaWorld在任务完成率和运动连贯性上优于基于世界模型的强化学习方法。

Conclusion: MetaWorld通过整合语义规划和物理控制，有效解决了人形机器人操作中的语义-物理鸿沟问题，显著提升了任务完成率和运动连贯性。

Abstract: Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/

</details>


### [296] [AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation](https://arxiv.org/abs/2601.17550)
*Deepak Singh,Shreyas Khobragade,Nitin J. Sanket*

Main category: cs.RO

TL;DR: 研究提出了一种在完全黑暗中自主导航的微型无人机系统，结合红外摄像头和结构化光源，通过仿真训练的深度估计模型实现高效导航，成功率达95.5%。


<details>
  <summary>Details</summary>
Motivation: 解决灾后搜救中因电力中断导致的黑暗环境下微型无人机导航难题，提升搜救效率和安全性。

Method: 通过结合红外单目摄像头和大孔径编码镜头及结构化光源，利用深度相关的离焦线索作为AsterNet深度估计模型的先验知识。模型在仿真环境中训练，无需微调即可直接应用于现实场景。

Result: 在仅使用机载传感和计算的真实实验中，系统成功应对暗色障碍物和细绳（直径6.25mm），未知物体形状、位置和材质的情况下总体成功率达95.5%。

Conclusion: 该研究成功开发了一种在完全黑暗环境中自主导航的微型无人机系统，结合红外单目摄像头和编码镜头，无需依赖外部基础设施如GPS或动作捕捉系统，实现了95.5%的成功率。

Abstract: Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.

</details>


### [297] [Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models](https://arxiv.org/abs/2601.17556)
*Ulices Santa Cruz,Mahmoud Elfar,Yasser Shoukry*

Main category: cs.RO

TL;DR: 论文提出可认证神经网络框架，结合物理模型与学习估计，用于姿态估计，并在杂乱环境中保持认证保证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在视觉任务中表现出色，但缺乏对其输出正确性的可证明保证，这在安全关键应用中至关重要。

Method: 框架首先利用环境中常见平面物体的已知几何形状（如交通标志和跑道标记），引入几何生成模型（GGM），其参数来自目标物体被相机观察的图像形成过程。GGM用于训练具有认证误差保证的基于神经网络的姿态估计器。

Result: 通过合成和真实图像评估，框架在杂乱环境中有效估计目标物体的姿态，并符合认证误差界限。

Conclusion: 该论文提出了一个可认证的神经网络框架，用于基于感知的姿态估计，结合了物理驱动模型和学习估计方法，并在杂乱环境中扩展了其应用，同时保持了认证保证。

Abstract: We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.

</details>


### [298] [Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction](https://arxiv.org/abs/2601.17812)
*Mingtian Du,Suhas Raghavendra Kulkarni,Bernardo Noronha,Domenico Campolo*

Main category: cs.RO

TL;DR: 论文提出了一种延迟补偿的刚度估计方法，通过代数估计器和NWLS过滤动态偏差，实验验证其在远程治疗中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决因网络延迟导致的触觉信号时间错位问题，提升远程物理治疗中患者刚度感知的准确性。

Method: 通过基于准静态平衡的代数估计器明确考虑延迟，并引入归一化加权最小二乘法（NWLS）来过滤动态偏差。

Result: 实验证明，该方法在多种延迟条件下均能保持一致的跟踪精度，显著优于标准估计器。

Conclusion: 该论文提出了一种延迟补偿的刚度估计框架，显著优于传统方法，为远程治疗中的高保真触觉感知提供了可行解决方案。

Abstract: Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.

</details>


### [299] [Less Is More: Scalable Visual Navigation from Limited Data](https://arxiv.org/abs/2601.17815)
*Yves Inglin,Jonas Frey,Changan Chen,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出利用几何规划器生成合成轨迹以增强模仿学习的训练数据，显著提升了视觉导航策略LiMo的性能，强调了数据质量和多样性的重要性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在目标条件视觉导航中表现优异，但其效果高度依赖于训练数据的质量和多样性。本文旨在通过几何规划器生成的合成轨迹来增强有限的专家演示数据。

Method: 利用经典几何规划器生成合成轨迹以补充昂贵的人类演示数据，训练基于Transformer的视觉导航策略LiMo，预测单次RGB观察的目标条件SE(2)轨迹。

Result: 通过实验证明，将规划器生成的监督数据与有限的专家演示数据结合，可显著提升性能。数据集规模和多样性对规划性能的影响通过消融实验和定性定量分析得到验证。

Conclusion: 通过战略性地筛选多样且高质量的数据集，而非简单地收集更多演示，可以实现鲁棒的视觉导航。结果表明，可扩展的、特定于具体化的几何监督是实现数据高效视觉导航的实用途径。

Abstract: Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.

</details>


### [300] [NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi](https://arxiv.org/abs/2601.17991)
*Roman Akinshin,Elizaveta Lopatina,Kirill Bogatikov,Nikolai Kiz,Anna V. Makarova,Mikhail Lebedev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Valerii Kangler*

Main category: cs.RO

TL;DR: 该论文提出了一种结合sEMG和视线引导视觉的神经形态控制架构，实现了高效、安全的假肢控制，识别准确率高达95%。


<details>
  <summary>Details</summary>
Motivation: 开发一种轻量级、可穿戴的神经形态控制架构，以解决传统假肢控制中能耗高和安全性不足的问题。

Method: 结合表面肌电图（sEMG）和视线引导的计算机视觉，采用部署在神经形态处理器AltAi上的脉冲神经网络实时分类EMG模式，同时通过眼追踪头显和场景摄像头识别用户注视的物体。

Result: 系统对六种功能手势的识别性能与最先进的肌电接口相当，当视觉管线将决策空间限制为当前物体相关的三种手势时，识别准确率提升至约95%，同时排除了不安全的抓握动作。

Conclusion: 该论文提出的神经形态、上下文感知控制器能够为上肢假肢提供高效且可靠的控制，有望提升上肢截肢患者的日常活动安全性和可用性。

Abstract: This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.

</details>


### [301] [Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization](https://arxiv.org/abs/2601.18121)
*Byeonggyeol Choi,Woojin Oh,Jongwoo Lim*

Main category: cs.RO

TL;DR: A framework refines visually aligned hand-object trajectories into physically valid ones using spline-based motion parameterization and CMA-ES optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing datasets like DexYCB and HO3D yield physically implausible interactions in simulators, necessitating a method to refine these trajectories for physical validity.

Method: The method involves parameterizing hand motion with a low-dimensional, spline-based representation and using CMA-ES as a gradient-free optimizer to treat the physics engine as a black-box objective function.

Result: The approach outperforms MANIPTRANS in reducing hand and object pose errors and more accurately recovers hand-object physical interactions.

Conclusion: The proposed simulation-in-the-loop refinement framework successfully converts visually aligned trajectories into physically executable ones, offering a scalable solution for generating high-fidelity data for robust policy learning.

Abstract: Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.
  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.
  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.

</details>


### [302] [Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation](https://arxiv.org/abs/2601.18289)
*Jialong Li,Zhenguo Wang,Tianci Wang,Maj Stenmark,Volker Krueger*

Main category: cs.RO

TL;DR: Quest2ROS2是一个开源的ROS2框架，通过相对运动控制方法改进双手机器人远程操作，支持多种控制模式并集成关键功能。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人远程操作中的工作空间限制和操作直观性问题，Quest2ROS2通过相对运动控制方法，提升了双手机器人远程操作的灵活性和用户体验。

Method: 该框架采用模块化架构，支持‘Side-by-Side’和‘Mirror’两种控制模式，集成了实时RViz可视化、简化的夹持器控制和暂停重置功能，以优化操作体验。

Result: Quest2ROS2成功实现了双手机器人远程操作，支持多种控制模式，并集成了关键的安全和可用性功能，代码已开源。

Conclusion: Quest2ROS2是一个开源的ROS2框架，专为双手机器人远程操作设计，旨在扩展机器人数据收集的能力。通过改进Quest2ROS，它克服了工作空间限制，采用基于相对运动的控制方法，计算VR控制器姿态变化以驱动机器人运动，实现了直观且姿态无关的操作。

Abstract: Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports "Side-by-Side" and "Mirror" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.

</details>


### [303] [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323)
*Weishi Mi,Yong Bao,Xiaowei Chi,Xiaozhu Ju,Zhiyuan Qin,Kuangzhi Ge,Kai Tang,Peidong Jia,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: TC-IDM 通过工具轨迹的中间表示，显著提升视觉语言动作范式的泛化能力和任务执行效果。


<details>
  <summary>Details</summary>
Motivation: 解决生成世界模型的像素级规划与物理可执行动作之间的关键差距。

Method: TC-IDM 通过分割和3D运动估计提取工具的点云轨迹，并采用解耦的动作头将轨迹投影为6自由度末端执行器运动和对应控制信号。

Result: 在真实世界评估中，TC-IDM 的平均成功率为61.11%，在简单任务中为77.7%，在零样本可变形物体任务中为38.46%。

Conclusion: TC-IDM 通过工具为中心的逆向动力学模型，有效连接了视觉规划与物理控制，显著提升了任务执行的成功率和泛化能力。

Abstract: The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

</details>


### [304] [SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation](https://arxiv.org/abs/2601.18442)
*Hongyi Zhao,Shuo Wang,Qijie He,Ziyuan Pu*

Main category: cs.RO

TL;DR: SG-CADVLM通过上下文感知解码和多模态输入处理，从事故报告生成高保真安全关键场景，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全验证需要测试安全关键场景，但这些场景在现实驾驶中罕见且测试成本高。事故报告提供了真实的安全关键事件描述，是稀缺的真实碰撞轨迹数据的重要替代来源。

Method: SG-CADVLM框架结合了上下文感知解码和多模态输入处理，从事故报告和道路网络图中生成安全关键场景，同时生成道路几何和车辆轨迹。

Result: 实验结果显示，SG-CADVLM生成关键风险场景的成功率为84.4%，相比基线方法的12.5%提升了469%，同时生成了可执行的自动驾驶测试仿真。

Conclusion: SG-CADVLM框架通过整合上下文感知解码与多模态输入处理，显著提升了安全关键场景生成的准确性和多样性，为自动驾驶测试提供了高效的仿真解决方案。

Abstract: Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.

</details>


### [305] [DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation](https://arxiv.org/abs/2601.18492)
*Zijun Li,Shijie Li,Zhenxi Zhang,Bin Li,Shoujun Zhou*

Main category: cs.RO

TL;DR: DV-VLN是一种新型视觉与语言导航框架，通过生成-验证范式提升导航可靠性，实验显示其在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLM）的导航代理多依赖单次动作决策，容易因局部不匹配和不完善的中间推理偏离正确路径，导致错误累积和在未见环境中可靠性下降。

Method: DV-VLN采用参数高效的领域内适配方法，基于开源的LLaMA-2模型生成结构化的导航链式思考，并通过True-False Verification (TFV)和Masked-Entity Verification (MEV)两种互补通道验证候选动作。

Result: 在R2R、RxR（英文子集）和REVERIE数据集上的实验表明，DV-VLN在语言驱动的导航任务中表现优异，甚至与部分跨模态系统相比具有竞争力。

Conclusion: DV-VLN框架通过生成-验证范式显著提升了语言驱动的导航性能，在多个数据集上表现优于直接预测和仅采样基线，展示了其在未见环境中的可靠性和竞争力。

Abstract: Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.

</details>


### [306] [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537)
*Linyong Gan,Zimo Li,Wenxin Xu,Xingjian Li,Jianhua Z. Huang,Enmei Tu,Shuhang Chen*

Main category: cs.RO

TL;DR: 通过语义关键点（NKP）分解长时预测，结合预训练-微调策略，显著提升船舶轨迹预测的准确性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长时预测中难以保持全局方向一致性，导致轨迹漂移或不合理。

Method: 采用预训练-微调策略，通过Next Key Point（NKP）捕捉导航意图，将长时预测分解为全局语义决策和局部运动建模。

Result: 在真实AIS数据上的实验表明，该方法在长时预测、方向准确性和细粒度预测上均优于现有技术。

Conclusion: 提出的语义关键点条件轨迹建模框架在长时预测中表现优异，显著提升了方向准确性和细粒度轨迹预测。

Abstract: Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.

</details>


### [307] [Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field](https://arxiv.org/abs/2601.18548)
*Yulin Li,Zhiyuan Song,Yiming Li,Zhicheng Song,Kai Chen,Chunxin Zheng,Zhihai Bi,Jiahang Cao,Sylvain Calinon,Fan Shi,Jun Ma*

Main category: cs.RO

TL;DR: GCDF扩展了CDF，适用于移动机械臂，通过神经GCDF和优化框架实现了高效的碰撞推理和轨迹优化。


<details>
  <summary>Details</summary>
Motivation: 解决移动机械臂在复杂、受限空间中进行全身轨迹优化时的高维非凸性和快速准确碰撞推理的挑战。

Method: 通过开发数据生成和训练流程，生成具有准确值和梯度的连续神经GCDF，支持高效的GPU批量查询，并在此基础上构建了一个高性能的顺序凸优化框架。

Result: GCDF保留了类似欧几里得的局部距离结构，并在构型空间中准确编码了全身几何形状，支持高效的GPU批量查询和快速重新规划。

Conclusion: GCDF（广义构型空间距离场）成功扩展了CDF的能力，使其适用于具有平移和旋转关节的移动机械臂，在无界工作空间中实现了高效的碰撞推理和优化。

Abstract: Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.

</details>


### [308] [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569)
*Seokju Lee,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: AttenNKF结合注意力机制和神经补偿器，有效提升腿式机器人在足部滑动时的状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 足部滑动是腿式机器人状态估计误差的主要来源，现有方法在滑动发生时无法有效处理，导致估计偏差。

Method: 提出了一种基于注意力机制的神经增强卡尔曼滤波器（AttenNKF），通过在不变扩展卡尔曼滤波器（InEKF）中引入神经补偿器，利用注意力机制推断滑动引起的误差并进行补偿。

Result: 实验表明，AttenNKF在易滑动条件下相比现有方法表现更优。

Conclusion: AttenNKF通过注意力机制的神经补偿器有效减少了足部滑动引起的估计误差，提升了腿式机器人在易滑动条件下的状态估计性能。

Abstract: In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.

</details>


### [309] [ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection](https://arxiv.org/abs/2601.18629)
*Yiming Wang,Ruogu Zhang,Minyang Li,Hao Shi,Junbo Wang,Deyi Li,Jieji Ren,Wenhai Liu,Weiming Wang,Hao-Shu Fang*

Main category: cs.RO

TL;DR: ExoGS通过机器人无源外骨骼和4D Real-to-Sim-to-Real技术，高效捕获真实世界交互并提升仿真数据质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在交互转移上的不足，尤其是接触密集型任务中仿真数据获取的低效问题。

Method: 使用AirExo-3外骨骼捕获毫米级精度的运动轨迹和同步RGB观测，通过3D高斯溅射重建可编辑的机器人、物体和环境资产，并结合轻量级Mask Adapter增强视觉域偏移下的鲁棒性。

Result: 实验表明，ExoGS在数据效率和策略泛化上优于基于遥操作的基线方法。

Conclusion: ExoGS提出了一种新型的机器人无源外骨骼框架，通过4D Real-to-Sim-to-Real技术，显著提升了数据采集效率和策略泛化能力。

Abstract: Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.

</details>


### [310] [Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](https://arxiv.org/abs/2601.18639)
*Ojasva Mishra,Xiaolong Wu,Min Xu*

Main category: cs.RO

TL;DR: 论文提出了一种实现感知的饱和离散时间关节控制分析和调优方法，通过混合认证贝叶斯优化显著提升了系统鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 精确调节旋转驱动在自主机器人技术中至关重要，但实际PID回路因离散时间执行、执行器饱和、小延迟和测量不完美而偏离连续时间理论。

Method: （i）使用Jury准则推导PI稳定性区域，（ii）评估离散反计算抗饱和实现，（iii）提出混合认证贝叶斯优化工作流程。

Result: 在随机模型族模拟下，鲁棒性导向的调优将中值IAE从0.843提高到0.430，同时保持中值超调低于2%。

Conclusion: 论文提出了一个实现感知的分析和调优工作流程，用于饱和离散时间关节控制，通过混合认证的贝叶斯优化工作流程，显著提高了控制系统的鲁棒性和性能。

Abstract: The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.

</details>


### [311] [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692)
*Wei Wu,Fan Lu,Yunnan Wang,Shuai Yang,Shi Liu,Fangjing Wang,Qian Zhu,He Sun,Yong Wang,Shuailei Ma,Yiyu Ren,Kejia Zhang,Hui Yu,Jingmei Zhao,Shuai Zhou,Zhenqi Qiu,Houlong Xiong,Ziyu Wang,Zechen Wang,Ran Cheng,Yong-Lu Li,Yongtao Huang,Xing Zhu,Yujun Shen,Kecheng Zheng*

Main category: cs.RO

TL;DR: LingBot-VLA是一个高效且泛化能力强的VLA基础模型，通过大量真实数据和系统评估展示了优越性能，并开放资源以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在任务和平台间忠实泛化且成本高效的Vision-Language-Action (VLA)基础模型，以提升机器人操作的潜力。

Method: 开发了LingBot-VLA，利用约20,000小时的真实世界数据，来自9种流行的双臂机器人配置，并通过系统评估在3个机器人平台上完成100个任务，每个任务130次训练后测试。

Result: 模型在性能上明显优于竞争对手，展示了强大的泛化能力；代码库实现了每GPU每秒261样本的吞吐量，比现有VLA代码库快1.5~2.8倍。

Conclusion: LingBot-VLA展示了强大的性能和广泛的泛化能力，适合实际部署，并通过开放代码、基础模型和基准数据推动了机器人学习领域的发展。

Abstract: Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.

</details>


### [312] [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723)
*Mengyuan Liu,Juyi Sheng,Peiming Li,Ziyi Wang,Tianming Xu,Tiantian Xu,Hong Liu*

Main category: cs.RO

TL;DR: 论文提出Eval-Actions基准和AutoEval架构，通过整合多维监督信号和时空评估，显著提升机器人操作信任评估的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法依赖二元成功率，无法解决信任的关键维度（如来源真实性和执行质量），因此需要一种更全面的评估框架。

Method: 通过构建Eval-Actions基准（包含VA和VLA策略执行轨迹及人类遥控数据，并涵盖失败场景）和设计AutoEval架构（利用时空聚合进行语义评估，并辅以运动平滑度校准），以及AutoEval-P（引入GRPO范式增强逻辑推理能力）。

Result: AutoEval在EG和RG协议下的SRCC分别达到0.81和0.84，并能以99.6%的准确率区分策略生成和遥控视频。

Conclusion: 该论文提出了一种结合Eval-Actions基准和AutoEval架构的解决方案，显著提升了机器人操作的信任评估标准，并通过实验验证了其高效性和准确性。

Abstract: Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.

</details>


### [313] [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733)
*Li Kang,Heng Zhou,Xiufeng Song,Rui Li,Bruno N. Y. Chen,Ziye Wang,Ximeng Meng,Stone Tao,Yiran Qin,Xiaohong Liu,Ruimao Zhang,Lei Bai,Yilun Du,Hao Su,Philip Torr,Zhenfei Yin,Ruihao Gong,Yejun Zeng,Fengjun Zhong,Shenghao Jin,Jinyang Guo,Xianglong Liu,Xiaojun Jia,Tianqi Shan,Wenqi Ren,Simeng Qin,Jialing Yang,Xiaoyu Ma,Tianxing Chen,Zixuan Li,Zijian Cai,Yan Qin,Yusen Qin,Qiangyu Chen,Kaixuan Wang,Zhaoming Han,Yao Mu,Ping Luo,Yuanqi Yao,Haoming Song,Jan-Nico Zaech,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.RO

TL;DR: 论文提出MARS挑战赛，探索多智能体协作在规划和控制中的应用，推动协作AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着Embodied AI领域向更复杂的任务场景过渡，多智能体系统框架成为实现可扩展、高效和协作解决方案的关键。这一转变受到三个主要因素的推动：智能体能力的提升、通过任务委派增强系统效率以及实现高级人机交互。

Method: 论文提出了Multi-Agent Robotic System (MARS) Challenge，重点关注规划和控制两个关键领域，参与者使用视觉语言模型（VLMs）协调任务并执行策略以在动态环境中进行机器人操作。

Result: 通过评估参与者提交的解决方案，挑战赛为多智能体机器人系统的设计和协调提供了宝贵见解。

Conclusion: 该论文通过提出MARS挑战赛，为多智能体机器人系统的设计和协调提供了宝贵见解，推动了高级协作AI系统的未来发展。

Abstract: Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.

</details>


### [314] [Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery](https://arxiv.org/abs/2601.18765)
*Shutong Chen,Adnan Aijaz,Yansha Deng*

Main category: cs.RO

TL;DR: 提出目标导向通信框架，通过3D场景图和优化的小型语言模型实现快速可靠的机器人故障检测与恢复，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有FDR框架在通信计算延迟和机器人运动生成可靠性方面存在不足，需设计更高效的通信-计算-控制（3C）循环以优化FDR性能。

Method: 通过创新的3D场景图（3D-SG）语义表示进行故障检测，结合小型语言模型（SLM）和知识蒸馏进行故障恢复，并设计轻量级目标导向数字孪生模块优化恢复动作。

Result: GoC框架将FDR时间减少高达82.6%，任务成功率提升高达76%，优于依赖视觉语言模型和大型语言模型的现有框架。

Conclusion: 提出的目标导向通信（GoC）框架显著降低了故障检测与恢复（FDR）时间并提高了任务成功率，验证了其在动态、不确定和人机交互环境中的有效性。

Abstract: Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.

</details>
