<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 29]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.RO](#cs.RO) [Total: 31]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection](https://arxiv.org/abs/2601.18845)
*Zeineb Dridi,Jihen Bennaceur,Amine Ben Hassouna*

Main category: cs.CV

TL;DR: 本文提出了一种针对目标检测模型的动态掩码后门攻击方法，利用SAM生成动态触发器，展示了其在YOLOv7上的高效攻击能力，并呼吁加强防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在计算机视觉领域的广泛应用，后门攻击等对抗性威胁日益凸显，尤其是在关键现实领域（如蘑菇检测）中，此类攻击可能带来严重后果。

Method: 利用数据集中毒技术嵌入恶意触发器，并特别采用SAM（一种强大的图像分割AI模型）来创建动态触发器放置的掩码。

Result: 提出的动态掩码后门攻击方法在YOLOv7模型上保持了高准确率，同时在中毒样本上实现了高攻击成功率，优于传统的静态后门注入方法。

Conclusion: 该研究强调了针对深度学习模型的动态后门攻击的严重性，并呼吁开发更强大的防御措施以应对此类不断演变的对抗性威胁。

Abstract: Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.

</details>


### [2] [Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding](https://arxiv.org/abs/2601.18849)
*Yuhui Zhang,Hui Yu,Wei Liang,Sunjie Zhang*

Main category: cs.CV

TL;DR: 本研究提出一种结合眨眼嵌入和哈希网格地标编码的自动方法，显著提升了说话人脸的逼真度和生成效率。


<details>
  <summary>Details</summary>
Motivation: 尽管动态神经辐射场（NeRF）在说话肖像的3D建模上取得了显著进展，但在准确高效捕捉嘴部动作方面仍存在挑战。

Method: 利用面部特征作为条件特征编码，并通过动态地标变换器将音频特征作为残差项整合到模型中，同时采用神经辐射场对整个面部进行建模。

Result: 实验评估验证了该方法在说话人脸生成质量和效率上的优越性。

Conclusion: 本研究提出的基于眨眼嵌入和哈希网格地标编码的自动方法显著提升了说话人脸的逼真度，实验验证了该方法优于现有技术。

Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.

</details>


### [3] [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/abs/2601.18851)
*Wei Liang,Hui Yu,Derui Ding,Rachael E. Jack,Philippe G. Schyns*

Main category: cs.CV

TL;DR: 研究提出3DMM+StyleGAN方法，用自拍视频实现高细节头部虚拟形象重演，解决了现有方法在实时性和数据依赖上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以实时捕捉整个头部（包括非面部区域和背景细节），且依赖大量训练数据，无法仅通过简单自拍视频实现高质量重演。

Method: 结合3DMM与基于StyleGAN的生成器，提出详细重建模型，采用混合损失函数进行对抗训练，以恢复高频细节。

Result: 在自重演和交叉重演任务中，定性定量评估表明，该方法在纹理细节还原上优于现有方法。

Conclusion: 该研究提出了一种结合3DMM与StyleGAN生成器的方法，通过自拍视频实现高质量头部虚拟形象重演，显著提升了纹理细节的还原能力。

Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.

</details>


### [4] [Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)](https://arxiv.org/abs/2601.18891)
*Ghazaleh Serati,Samuel Foucher,Jerome Theau*

Main category: cs.CV

TL;DR: 弱监督预训练HerdNet在驯鹿检测中表现优异，克服背景异质性等问题，F1分数达93.7%，但仍有误检漏检限制。


<details>
  <summary>Details</summary>
Motivation: 北极驯鹿数量下降，需要可扩展且准确的监测方法以支持保护决策，但手动解译耗时且易错，需自动化检测方法。

Method: 提出了一种基于检测网络架构的弱监督块级预训练方法，利用空与非空标签学习，增强模型对背景异质性、类别不平衡等挑战的鲁棒性。

Result: 在多群驯鹿图像（2017）和独立年份（2019）测试集上取得高准确率（F1: 93.7%/92.6%），检测任务中弱监督预训练初始化优于通用权重（F1提升显著）。

Conclusion: 通过弱监督预训练方法（HerdNet）在有限标记数据下实现了高精度检测，结果与通用权重初始化相当，但仍存在背景干扰和低密度遮挡的误检漏检问题。

Abstract: Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.

</details>


### [5] [RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection](https://arxiv.org/abs/2601.18900)
*Haim Zisman,Uri Shaham*

Main category: cs.CV

TL;DR: 提出基于统计的假图像检测框架，结合多种检测器优势，无需训练，适用于多样化环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有检测方法缺乏形式可解释性、依赖对虚假内容的隐含假设，以及在分布变化中鲁棒性不足的问题。

Method: 通过结合多种现有检测器的优势，利用无训练统计量计算p值，并使用经典统计集成方法评估与统一真实图像分布的一致性。

Result: 开发了一个通用、灵活且无需训练的框架，适用于在不同和动态环境中进行鲁棒的假图像检测。

Conclusion: 该论文提出了一个统计基础框架，用于生成可解释的假图像检测概率分数，适用于多样化和不断变化的环境。

Abstract: As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.

</details>


### [6] [On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training](https://arxiv.org/abs/2601.18929)
*John J. Han,Adam Schmidt,Muhammad Abdullah Jamal,Chinedu Nwoye,Anita Rau,Jie Ying Wu,Omid Mohareri*

Main category: cs.CV

TL;DR: 研究表明，结合深度信息的多模态预训练能显著提升手术视觉系统的性能和数据效率，且无需额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前手术场景理解主要依赖单模态RGB预训练，忽略了手术环境中固有的复杂3D几何信息。尽管通用计算机视觉中有支持多模态或几何感知输入的架构，但在手术场景中结合深度信息的优势尚未充分探索。

Method: 通过大规模实证研究比较了八种基于ViT的视觉基础模型（VFMs），这些模型在预训练领域、学习目标和输入模态（RGB vs. RGB-D）上有所不同。使用包含140万机器人手术图像及深度图的数据集进行预训练，并在八种手术数据集上评估了冻结主干和端到端微调协议。

Result: 结合显式几何标记化的模型（如MultiMAE）在所有任务中显著优于单模态基线。几何感知预训练显著提升了数据效率：仅使用25%标注数据微调的模型表现优于完全数据训练的RGB模型，且推理时无需改变架构或运行时。

Conclusion: 多模态预训练（尤其是结合深度信息）为构建更强大的手术视觉系统提供了可行路径，无需在推理时改变架构或增加运行时负担。

Abstract: Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding. However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments. Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored. We conduct a large-scale empirical study comparing eight ViT-based VFMs that differ in pre-training domain, learning objective, and input modality (RGB vs. RGB-D). For pre-training, we use a curated dataset of 1.4 million robotic surgical images paired with depth maps generated from an off-the-shelf network. We evaluate these models under both frozen-backbone and end-to-end fine-tuning protocols across eight surgical datasets spanning object detection, segmentation, depth estimation, and pose estimation. Our experiments yield several consistent findings. Models incorporating explicit geometric tokenization, such as MultiMAE, substantially outperform unimodal baselines across all tasks. Notably, geometric-aware pre-training enables remarkable data efficiency: models fine-tuned on just 25% of labeled data consistently surpass RGB-only models trained on the full dataset. Importantly, these gains require no architectural or runtime changes at inference; depth is used only during pre-training, making adoption straightforward. These findings suggest that multimodal pre-training offers a viable path towards building more capable surgical vision systems.

</details>


### [7] [Smart Split-Federated Learning over Noisy Channels for Embryo Image Segmentation](https://arxiv.org/abs/2601.18948)
*Zahra Hafezi Kafshgari,Ivan V. Bajic,Parvaneh Saeedi*

Main category: cs.CV

TL;DR: SplitFed学习通过智能平均策略提升了对通信信道噪声的鲁棒性，实验显示其能在高噪声环境下保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 研究通信信道噪声对SplitFed学习过程及最终模型质量的影响，旨在提升模型在噪声环境下的表现。

Method: 提出了一种智能平均策略，用于SplitFed学习中，以增强对通信信道噪声的抵抗能力。

Result: 在胚胎图像分割模型上的实验表明，智能平均策略能够容忍比传统平均方法高两个数量级的噪声，同时保持最终模型的准确性。

Conclusion: 智能平均策略显著提升了SplitFed学习在通信信道噪声下的鲁棒性，能够在保持模型精度的同时容忍比传统平均方法高两个数量级的噪声。

Abstract: Split-Federated (SplitFed) learning is an extension of federated learning that places minimal requirements on the clients computing infrastructure, since only a small portion of the overall model is deployed on the clients hardware. In SplitFed learning, feature values, gradient updates, and model updates are transferred across communication channels. In this paper, we study the effects of noise in the communication channels on the learning process and the quality of the final model. We propose a smart averaging strategy for SplitFed learning with the goal of improving resilience against channel noise. Experiments on a segmentation model for embryo images shows that the proposed smart averaging strategy is able to tolerate two orders of magnitude stronger noise in the communication channels compared to conventional averaging, while still maintaining the accuracy of the final model.

</details>


### [8] [FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction](https://arxiv.org/abs/2601.18993)
*Wei Cao,Hao Zhang,Fengrui Tian,Yulun Wu,Yingying Li,Shenlong Wang,Ning Yu,Yaoyao Liu*

Main category: cs.CV

TL;DR: FreeOrbit4D通过几何完整的4D代理解决单目视频大角度重定向的几何模糊问题，实验证明其优越性并拓展了应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频在大角度相机重定向中因视觉基础缺失导致的几何模糊和时间不一致性问题。

Method: 通过解耦前景和背景重建，将单目视频反投影到静态背景和几何不完整的前景点云中，利用对象中心的多视图扩散模型合成多视图图像并在规范对象空间中重建几何完整的前景点云。

Result: FreeOrbit4D在挑战性的大角度轨迹下生成更忠实于原始场景的重定向视频，其几何完整的4D代理为后续应用提供了可能。

Conclusion: FreeOrbit4D通过恢复几何完整的4D代理作为视频生成的结构基础，有效解决了大角度相机重定向中的几何模糊问题，并在实验中展现出优越性能，同时为编辑传播和4D数据生成等应用提供了潜在途径。

Abstract: Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.

</details>


### [9] [Pay Attention to Where You Look](https://arxiv.org/abs/2601.18970)
*Alex Beriand,JhihYang Wu,Daniel Brignac,Natnael Daba,Abhijit Mahalanobis*

Main category: cs.CV

TL;DR: 提出了一种自适应视角加权机制，通过调整源视角的重要性来优化少样本新颖视图合成，显著提升了结果质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在少样本NVS中假设所有输入视角对目标同等重要导致结果次优的问题。

Method: 提出了两种方法：一种基于几何属性（如欧氏距离和角度差异）的确定性加权方案，另一种是基于交叉注意力的学习方案，优化视角加权。

Result: 实验结果表明，自适应视角加权提高了合成视图的质量和真实感。

Conclusion: 自适应视角加权机制显著提升了新颖视图合成的准确性和真实感，为NVS领域提供了有前景的改进方向。

Abstract: Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.

</details>


### [10] [Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)](https://arxiv.org/abs/2601.19519)
*Ofir Abramovich,Ariel Shamir,Andreas Aristidou*

Main category: cs.CV

TL;DR: 提出基于UWB传感器的Wild-Poser系统，通过稀疏距离测量实现全身体3D运动捕捉，适用于各种环境和物种，无需外部设备。


<details>
  <summary>Details</summary>
Motivation: 传统光学或惯性系统受环境限制（如光照和磁干扰），且需要外部设备。本文旨在开发一种无需外部摄像头、对环境约束具有鲁棒性的运动捕捉系统。

Method: 采用基于Transformer的紧凑实时架构Wild-Poser，直接从噪声或损坏的PWD测量中预测3D关节位置，并通过学习方法重建关节旋转。

Result: WiP在实时操作下实现了低关节位置误差，并能准确重建人类和动物在野外的3D运动，且无需个体身体测量或形状拟合。

Conclusion: 该论文提出的Wild-Poser（WiP）系统通过稀疏成对距离测量实现了全身体3D运动捕捉，具有低成本、可扩展性和通用性，适用于现实场景。

Abstract: We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.

</details>


### [11] [Anatomically-aware conformal prediction for medical image segmentation with random walks](https://arxiv.org/abs/2601.18997)
*Mélanie Gaillochet,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: RW-CP通过随机游走扩散提升分割预测集的空间连贯性和解剖学有效性，显著优于标准CP。


<details>
  <summary>Details</summary>
Motivation: 解决标准CP在分割中忽视解剖学上下文，导致预测集碎片化、空间不连贯和过度分割的问题。

Method: RW-CP构建了一个基于预训练视觉基础模型特征的k近邻图，并应用随机游走来扩散不确定性。

Result: 在允许错误率为α=0.1的情况下，相比标准CP基线，RW-CP在多模态公共数据集上提升了高达35.4%的分割质量。

Conclusion: RW-CP通过随机游走扩散增强了空间一致性，生成了更具解剖学意义的预测集，同时保持了严格的边际覆盖。

Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $λ$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $α=0.1$.

</details>


### [12] [Non-Invasive 3D Wound Measurement with RGB-D Imaging](https://arxiv.org/abs/2601.19014)
*Lena Harkämper,Leo Lebrat,David Ahmedt-Aristizabal,Olivier Salvado,Mattias Heinrich,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: 本文提出了一种基于RGB-D成像的快速、非侵入性3D伤口测量算法，结合RGB-D里程计和B样条曲面重建，实现高精度伤口测量，适用于临床和远程医疗环境。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口的监测和管理需要准确且高效的伤口测量方法。

Method: 该方法结合了RGB-D里程计与B样条曲面重建，生成详细的3D伤口网格，实现临床相关伤口测量（如周长、表面积和尺寸）的自动计算。

Result: 在真实的硅胶伤口模型上评估系统，测量结果显示与高分辨率地面真实扫描相比，3D重建精度达到亚毫米级。提取的测量值在重复捕获中表现出低变异性，并与手动评估结果高度一致。该流程在保持适合实时临床部署的运行时间的同时，优于最先进的以对象为中心的RGB-D重建方法。

Conclusion: 该论文提出的基于RGB-D成像的快速、非侵入性3D伤口测量算法，为临床和远程医疗环境中的自动化伤口评估提供了一个有前景的工具。

Abstract: Chronic wound monitoring and management require accurate and efficient wound measurement methods. This paper presents a fast, non-invasive 3D wound measurement algorithm based on RGB-D imaging. The method combines RGB-D odometry with B-spline surface reconstruction to generate detailed 3D wound meshes, enabling automatic computation of clinically relevant wound measurements such as perimeter, surface area, and dimensions. We evaluated our system on realistic silicone wound phantoms and measured sub-millimetre 3D reconstruction accuracy compared with high-resolution ground-truth scans. The extracted measurements demonstrated low variability across repeated captures and strong agreement with manual assessments. The proposed pipeline also outperformed a state-of-the-art object-centric RGB-D reconstruction method while maintaining runtimes suitable for real-time clinical deployment. Our approach offers a promising tool for automated wound assessment in both clinical and remote healthcare settings.

</details>


### [13] [NC-Reg : Neural Cortical Maps for Rigid Registration](https://arxiv.org/abs/2601.19042)
*Ines Vati,Pierrick Bourgeat,Rodrigo Santa Cruz,Vincent Dore,Olivier Salvado,Clinton Fookes,Léo Lebrat*

Main category: cs.CV

TL;DR: 神经皮质地图是一种连续紧凑的神经表示，替代传统离散结构，优化效率高，运行速度快30倍。NC-Reg算法结合神经特征地图和模拟退火，实现亚度精度的皮质表面配准，适用于临床。


<details>
  <summary>Details</summary>
Motivation: 传统离散结构（如网格和网格）在处理皮质特征地图时存在局限性，需要一种更高效、连续且紧凑的表示方法。神经皮质地图的提出旨在解决这一问题，并提供更快的运行时间和更高的优化效率。

Method: 本文提出神经皮质地图（neural cortical maps），一种连续且紧凑的神经表示，能够从任意大小的网格中学习并在任何分辨率下提供学习特征。此外，提出了NC-Reg算法，结合神经皮质特征地图、梯度下降优化和模拟退火策略，用于皮质表面的刚性配准。

Result: 神经皮质地图在球面上的优化效率显著提升，运行时间比经典重心插值快30倍（相同迭代次数下）。NC-Reg算法在皮质表面刚性配准中实现亚度精度（<1°），并通过消融实验和主体到模板实验验证了其稳健性。

Conclusion: 神经皮质地图作为一种连续且紧凑的神经表示，为皮质特征地图提供了一种替代传统离散结构（如网格和网格）的方法。通过NC-Reg算法，该方法在皮质表面刚性配准中表现出亚度精度（<1°），并成为一种有前景的稳健预对齐策略，适用于临床环境。

Abstract: We introduce neural cortical maps, a continuous and compact neural representation for cortical feature maps, as an alternative to traditional discrete structures such as grids and meshes. It can learn from meshes of arbitrary size and provide learnt features at any resolution. Neural cortical maps enable efficient optimization on the sphere and achieve runtimes up to 30 times faster than classic barycentric interpolation (for the same number of iterations). As a proof of concept, we investigate rigid registration of cortical surfaces and propose NC-Reg, a novel iterative algorithm that involves the use of neural cortical feature maps, gradient descent optimization and a simulated annealing strategy. Through ablation studies and subject-to-template experiments, our method demonstrates sub-degree accuracy ($<1^\circ$ from the global optimum), and serves as a promising robust pre-alignment strategy, which is critical in clinical settings.

</details>


### [14] [NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation](https://arxiv.org/abs/2601.19048)
*Han-Hung Lee,Cheng-Yu Yang,Yu-Lun Liu,Angel X. Chang*

Main category: cs.CV

TL;DR: NuiWorld通过生成式引导和可变场景块表示，解决了世界生成的可控性、可扩展性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有世界生成方法在可控性、可扩展性和效率方面存在不足，数据稀缺和固定分辨率表示限制了模型性能。

Method: 提出生成式引导策略，从少量输入图像开始，结合3D重建和可扩展场景生成技术，合成不同大小和布局的场景。采用可变场景块的扁平向量集表示，减少大型场景的标记长度。

Result: NuiWorld能够生成不同大小和布局的场景，通过伪草图标签实现可控性，并在训练和推理效率上有所提升。

Conclusion: NuiWorld框架通过生成式引导策略和可变场景块表示，解决了现有世界生成方法在可控性、可扩展性和效率方面的挑战。

Abstract: World generation is a fundamental capability for applications like video games, simulation, and robotics. However, existing approaches face three main obstacles: controllability, scalability, and efficiency. End-to-end scene generation models have been limited by data scarcity. While object-centric generation approaches rely on fixed resolution representations, degrading fidelity for larger scenes. Training-free approaches, while flexible, are often slow and computationally expensive at inference time. We present NuiWorld, a framework that attempts to address these challenges. To overcome data scarcity, we propose a generative bootstrapping strategy that starts from a few input images. Leveraging recent 3D reconstruction and expandable scene generation techniques, we synthesize scenes of varying sizes and layouts, producing enough data to train an end-to-end model. Furthermore, our framework enables controllability through pseudo sketch labels, and demonstrates a degree of generalization to previously unseen sketches. Our approach represents scenes as a collection of variable scene chunks, which are compressed into a flattened vector-set representation. This significantly reduces the token length for large scenes, enabling consistent geometric fidelity across scenes sizes while improving training and inference efficiency.

</details>


### [15] [Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models](https://arxiv.org/abs/2601.19060)
*Jeonghwan Kim,Renjie Tao,Sanat Sharma,Jiaqi Wang,Kai Sun,Zhaojiang Lin,Seungwhan Moon,Lambert Mathias,Anuj Kumar,Heng Ji,Xin Luna Dong*

Main category: cs.CV

TL;DR: PixSearch是首个统一区域感知与检索增强推理的端到端Segmenting LMM，通过像素级掩码作为视觉查询，显著提升VQA任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有MM-RAG系统缺乏对检索时机和方式的内部策略，限制了其在VQA任务中的事实基础和性能。PixSearch旨在通过统一区域感知与检索增强推理来解决这一问题。

Method: PixSearch是首个端到端的Segmenting LMM，通过<search>标记触发检索、选择查询模态（文本、图像或区域），并生成像素级掩码作为视觉查询，避免依赖模块化流水线。采用两阶段监督微调策略，在保持分割能力的同时学习检索时机和查询选择。

Result: 在自我中心和实体中心VQA基准测试中，PixSearch显著提升了事实一致性和泛化能力，CRAG-MM准确率相对提升19.7%，同时在多种VQA和纯文本QA任务中保持竞争力。

Conclusion: PixSearch通过统一的区域级感知和检索增强推理，显著提升了VQA任务的事实一致性和泛化能力，相比全图像检索在CRAG-MM上实现了19.7%的相对准确率提升。

Abstract: Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits <search> tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.

</details>


### [16] [m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning](https://arxiv.org/abs/2601.19099)
*Yosub Shin,Michael Buriek,Igor Molybog*

Main category: cs.CV

TL;DR: 研究开发了m2sv基准测试，评估视觉-语言模型在空间推理任务中的表现，发现模型在几何对齐等方面存在显著差距，未来需改进跨视角推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在多模态基准测试中表现优异，但在需要将抽象地图与街景图像对齐的空间推理任务中表现脆弱，因此需要开发更有效的评估方法和模型。

Method: 研究引入了m2sv基准测试，包括m2sv-20k和m2sv-sft-11k数据集，用于评估模型在从地图到街景的空间推理能力。通过监督微调和强化学习提升模型性能，并进行了跨基准测试和失败分析。

Result: 最佳视觉-语言模型在m2sv上的准确率仅为65.2%，远低于人类基准的95%。监督微调和强化学习虽能提升性能，但跨基准测试显示迁移能力有限。

Conclusion: 研究揭示了视觉-语言模型在空间推理任务中的局限性，特别是在几何对齐、证据聚合和推理一致性方面的差距，为未来跨视角的空间推理研究提供了方向。

Abstract: Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.

</details>


### [17] [Glance and Focus Reinforcement for Pan-cancer Screening](https://arxiv.org/abs/2601.19103)
*Linshan Wu,Jiaxin Zhuang,Hao Chen*

Main category: cs.CV

TL;DR: GF-Screen通过Glance和Focus强化学习框架，高效解决大规模CT扫描中泛癌筛查的挑战，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在大规模CT扫描中难以定位多样化的微小病变，且前景-背景极度不平衡导致效率低下和假阳性率高。受放射科医生诊断策略启发，提出GF-Screen框架。

Method: 采用Glance模型定位病变区域，Focus模型精确分割病变，并通过强化学习将分割结果反馈给Glance模型进行优化。引入群体相对学习范式，优先处理高优势预测。

Result: 在16个内部和7个外部数据集上验证了GF-Screen的有效性，其在MICCAI FLARE25挑战赛中大幅领先（DSC提升25.6%，NSD提升28.2%）。

Conclusion: GF-Screen通过创新的Glance和Focus强化学习框架，成功解决了大规模CT扫描中泛癌筛查的挑战，显著提升了病变检测的准确性和效率。

Abstract: Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).

</details>


### [18] [Reg-TTR, Test-Time Refinement for Fast, Robust and Accurate Image Registration](https://arxiv.org/abs/2601.19114)
*Lin Chen,Yue He,Fengting Zhang,Yaonan Wang,Fengming Lin,Xiang Chen,Min Liu*

Main category: cs.CV

TL;DR: Reg-TTR结合深度学习和传统配准技术，通过测试时细化提升配准精度，保持快速推理，性能接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统图像配准方法稳健但速度慢，深度学习推理快但难以应对领域偏移。注册基础模型在速度和稳健性之间取得平衡，但无法达到专用数据集训练的峰值精度。

Method: 提出Reg-TTR，一种测试时细化框架，通过在推理时细化预训练模型的预测结果，结合深度学习和传统配准技术的互补优势。

Result: Reg-TTR在两个不同任务上评估，实现了SOTA性能，同时推理速度接近之前的深度学习方法，仅需额外21%的推理时间（0.56秒）。

Conclusion: Reg-TTR通过结合深度学习和传统配准技术的优势，显著提升了配准精度，同时保持了接近深度学习方法的推理速度，为缩小基础模型与专用数据集训练的SOTA方法之间的性能差距提供了高效策略。

Abstract: Traditional image registration methods are robust but slow due to their iterative nature. While deep learning has accelerated inference, it often struggles with domain shifts. Emerging registration foundation models offer a balance of speed and robustness, yet typically cannot match the peak accuracy of specialized models trained on specific datasets. To mitigate this limitation, we propose Reg-TTR, a test-time refinement framework that synergizes the complementary strengths of both deep learning and conventional registration techniques. By refining the predictions of pre-trained models at inference, our method delivers significantly improved registration accuracy at a modest computational cost, requiring only 21% additional inference time (0.56s). We evaluate Reg-TTR on two distinct tasks and show that it achieves state-of-the-art (SOTA) performance while maintaining inference speeds close to previous deep learning methods. As foundation models continue to emerge, our framework offers an efficient strategy to narrow the performance gap between registration foundation models and SOTA methods trained on specialized datasets. The source code will be publicly available following the acceptance of this work.

</details>


### [19] [FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation](https://arxiv.org/abs/2601.19115)
*Xiang Gao,Yunpeng Jia*

Main category: cs.CV

TL;DR: FBSDiff++是一种基于频率域视角的I2I翻译框架，通过动态频率带替换实现高效、多功能和可控的图像翻译，相比现有方法具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着大规模文本到图像（T2I）扩散模型的显著进展，研究如何将其自然扩展到文本驱动的图像到图像（I2I）翻译领域，以源图像作为视觉指导。

Method: FBSDiff通过动态频率带替换扩散特征，实现无需训练或微调的即插即用I2I翻译。FBSDiff++进一步优化模型架构，提升推理速度、支持任意分辨率输入，并扩展功能支持局部图像操作和风格化内容生成。

Result: FBSDiff++在I2I翻译中实现了8.9倍的推理加速，支持任意分辨率输入，并扩展了功能。定性和定量实验验证了其优越性。

Conclusion: FBSDiff++在I2I翻译的视觉质量、效率、多功能性和可控性方面表现出色，相比现有先进方法具有显著优势。

Abstract: With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.

</details>


### [20] [Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection](https://arxiv.org/abs/2601.19127)
*Zhilong Zhang,Lei Zhang,Qing He,Shuyin Xia,Guoyin Wang,Fuxiang Huang*

Main category: cs.CV

TL;DR: GB-DAL通过改进的领域对抗学习方法，解决了开放世界目标检测中隐式非因果因素的挑战，提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于领域对抗学习（DAL）的领域泛化方法在处理隐式非因果因素方面存在局限，主要原因是领域标签稀疏和无法识别隐式数据偏差。

Method: 提出了GB-DAL方法，包含PGBS模块用于生成更密集的领域标签，以及SNF模块通过数据增强减少非因果因素的隐式性。

Result: 在多个基准测试中，GB-DAL方法展现了在新环境下更好的泛化性能。

Conclusion: GB-DAL方法通过PGBS模块和SNF模块的引入，显著提升了在开放世界目标检测中的领域泛化性能，尤其在处理隐式非因果因素方面表现出色。

Abstract: Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.

</details>


### [21] [Resolving Primitive-Sharing Ambiguity in Long-Tailed Industrial Point Cloud Segmentation via Spatial Context Constraints](https://arxiv.org/abs/2601.19128)
*Chao Yin,Qing Han,Zhiwei Hou,Yue Liu,Anjin Dai,Hongda Hu,Ji Yang,Wei Yao*

Main category: cs.CV

TL;DR: 该论文提出了一种解决工业点云分割中类别不平衡和几何模糊问题的方法，通过空间上下文约束显著提升尾类性能，同时保持头类准确性。


<details>
  <summary>Details</summary>
Motivation: 工业点云分割中，安全关键组件（如减速器和阀门）由于训练数据稀少且与主导结构（如管道）共享局部几何形状，导致系统性误分类。这一问题源于极端的类别不平衡（215:1）和几何模糊性。

Method: 提出了两种架构无关的机制：(1) Boundary-CB，基于熵的约束，强调模糊边界；(2) Density-CB，基于密度的约束，补偿扫描依赖性变化。这两种机制作为即插即用模块集成，仅需替换损失函数。

Result: 在Industrial3D数据集（6.1亿点）上，方法实现了55.74%的mIoU，尾类性能相对提升21.7%（29.59% vs. 24.32%基线），同时保持头类准确性（88.14%）。减速器IoU从0%提升至21.12%，阀门相对提升24.3%。

Conclusion: 该方法通过空间上下文约束解决了工业点云分割中的几何模糊问题，显著提升了尾类（如减速器和阀门）的性能，同时保持了头类的准确性，为数字孪生应用中的安全关键组件识别提供了可靠解决方案。

Abstract: Industrial point cloud segmentation for Digital Twin construction faces a persistent challenge: safety-critical components such as reducers and valves are systematically misclassified. These failures stem from two compounding factors: such components are rare in training data, yet they share identical local geometry with dominant structures like pipes. This work identifies a dual crisis unique to industrial 3D data extreme class imbalance 215:1 ratio compounded by geometric ambiguity where most tail classes share cylindrical primitives with head classes. Existing frequency-based re-weighting methods address statistical imbalance but cannot resolve geometric ambiguity. We propose spatial context constraints that leverage neighborhood prediction consistency to disambiguate locally similar structures. Our approach extends the Class-Balanced (CB) Loss framework with two architecture-agnostic mechanisms: (1) Boundary-CB, an entropy-based constraint that emphasizes ambiguous boundaries, and (2) Density-CB, a density-based constraint that compensates for scan-dependent variations. Both integrate as plug-and-play modules without network modifications, requiring only loss function replacement. On the Industrial3D dataset (610M points from water treatment facilities), our method achieves 55.74% mIoU with 21.7% relative improvement on tail-class performance (29.59% vs. 24.32% baseline) while preserving head-class accuracy (88.14%). Components with primitive-sharing ambiguity show dramatic gains: reducer improves from 0% to 21.12% IoU; valve improves by 24.3% relative. This resolves geometric ambiguity without the typical head-tail trade-off, enabling reliable identification of safety-critical components for automated knowledge extraction in Digital Twin applications.

</details>


### [22] [CLIP-Guided Unsupervised Semantic-Aware Exposure Correction](https://arxiv.org/abs/2601.19129)
*Puzhen Wu,Han Weng,Quan Zheng,Yi Zhan,Hewei Wang,Yiming Li,Jiahui Han,Rui Xu*

Main category: cs.CV

TL;DR: 提出无监督语义感知曝光校正网络，结合FastSAM和CLIP先验，避免手动标注，显著提升曝光修正效果。


<details>
  <summary>Details</summary>
Motivation: 解决曝光校正中的两个关键挑战：（1）忽略区域语义信息导致色彩偏移；（2）真实世界曝光图像缺乏地面真实标签且手动标注成本高。

Method: 提出了一种新的无监督语义感知曝光校正网络，包含自适应语义感知融合模块和多尺度残差空间Mamba组，利用FastSAM和CLIP的丰富先验进行训练。

Result: 综合实验结果表明，该方法在修正曝光图像时有效，细节恢复和曝光调整效果显著。

Conclusion: 该方法在修正真实世界曝光图像方面表现出色，数值和视觉上均优于当前最先进的无监督方法。

Abstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.

</details>


### [23] [QA-ReID: Quality-Aware Query-Adaptive Convolution Leveraging Fused Global and Structural Cues for Clothes-Changing ReID](https://arxiv.org/abs/2601.19133)
*Yuxiang Wang,Kunming Jiang,Tianxiang Zhang,Ke Tian,Gaozhe Jiang*

Main category: cs.CV

TL;DR: QA-ReID通过结合RGB和解析特征，利用多模态注意力模块和QAConv-QA，有效解决了服装变化带来的行人重识别挑战，并在多个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别在服装变化时面临严重挑战，因为服装变化会引入显著的外观变化。

Method: 提出了Quality-Aware Dual-Branch Matching (QA-ReID)，结合RGB特征和解析表示，通过多模态注意力模块自适应融合异构特征，并在匹配阶段设计了Quality-Aware Query Adaptive Convolution (QAConv-QA)。

Result: QA-ReID在PRCC、LTCC和VC-Clothes等多个基准测试中表现优异。

Conclusion: QA-ReID在多个基准测试中实现了最先进的性能，显著优于现有方法，特别是在跨服装场景下。

Abstract: Unlike conventional person re-identification (ReID), clothes-changing ReID (CC-ReID) presents severe challenges due to substantial appearance variations introduced by clothing changes. In this work, we propose the Quality-Aware Dual-Branch Matching (QA-ReID), which jointly leverages RGB-based features and parsing-based representations to model both global appearance and clothing-invariant structural cues. These heterogeneous features are adaptively fused through a multi-modal attention module. At the matching stage, we further design the Quality-Aware Query Adaptive Convolution (QAConv-QA), which incorporates pixel-level importance weighting and bidirectional consistency constraints to enhance robustness against clothing variations. Extensive experiments demonstrate that QA-ReID achieves state-of-the-art performance on multiple benchmarks, including PRCC, LTCC, and VC-Clothes, and significantly outperforms existing approaches under cross-clothing scenarios.

</details>


### [24] [TFFM: Topology-Aware Feature Fusion Module via Latent Graph Reasoning for Retinal Vessel Segmentation](https://arxiv.org/abs/2601.19136)
*Iftekhar Ahmed,Shakib Absar,Aftar Ahmad Sami,Shadman Sakib,Debojyoti Biswas,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 提出拓扑感知框架，结合TFFM和图注意力网络，显著提升视网膜血管分割的拓扑连贯性，减少断裂率38%。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构在视网膜血管分割中常产生拓扑不连贯的结果，影响临床分析的可靠性，因此需要一种能保持血管连通性的方法。

Method: 采用拓扑特征融合模块（TFFM）将局部特征映射到潜在图空间，结合图注意力网络捕捉全局结构依赖，并使用混合损失函数（Tversky损失和soft clDice损失）优化学习过程。

Result: 在Fundus-AVSeg数据集上取得了90.97%的Dice分数和3.50像素的95% Hausdorff距离，血管断裂率比基线降低了约38%。

Conclusion: 该论文提出了一种拓扑感知框架，显著减少了视网膜血管分割中的断裂问题，提高了血管树的拓扑连贯性，为自动化生物标志物量化提供了可行方案。

Abstract: Precise segmentation of retinal arteries and veins carries the diagnosis of systemic cardiovascular conditions. However, standard convolutional architectures often yield topologically disjointed segmentations, characterized by gaps and discontinuities that render reliable graph-based clinical analysis impossible despite high pixel-level accuracy. To address this, we introduce a topology-aware framework engineered to maintain vascular connectivity. Our architecture fuses a Topological Feature Fusion Module (TFFM) that maps local feature representations into a latent graph space, deploying Graph Attention Networks to capture global structural dependencies often missed by fixed receptive fields. Furthermore, we drive the learning process with a hybrid objective function, coupling Tversky loss for class imbalance with soft clDice loss to explicitly penalize topological disconnects. Evaluation on the Fundus-AVSeg dataset reveals state-of-the-art performance, achieving a combined Dice score of 90.97% and a 95% Hausdorff Distance of 3.50 pixels. Notably, our method decreases vessel fragmentation by approximately 38% relative to baselines, yielding topologically coherent vascular trees viable for automated biomarker quantification. We open-source our code at https://tffm-module.github.io/.

</details>


### [25] [GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2601.19157)
*Yongsong Huang,Tzu-Hsuan Peng,Tomo Miyazaki,Xiaofeng Liu,Chun-Ting Chou,Ai-Chun Pang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: GTFMN通过解耦光照估计和纹理恢复，结合动态调制机制，在低光图像超分辨率任务中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 低光图像超分辨率任务因低分辨率和光照不足的耦合退化而具有挑战性，需要一种能够解耦并分别处理这两个问题的解决方案。

Method: 提出了GTFMN框架，包含专用的Illumination Stream和Texture Stream，通过Illumination Guided Modulation Block动态调制特征，实现空间自适应的纹理恢复。

Result: 在OmniNormal5和OmniNormal15数据集上，GTFMN在定量指标和视觉质量上均优于其他竞争方法。

Conclusion: GTFMN在低光图像超分辨率任务中表现出色，通过解耦光照估计和纹理恢复两个子问题，实现了优于现有方法的性能。

Abstract: Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.

</details>


### [26] [SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing](https://arxiv.org/abs/2601.19180)
*Lifan Jiang,Boxi Wu,Yuhang Pei,Tianrun Wu,Yongyuan Chen,Yan Zhao,Shiyu Yu,Deng Cai*

Main category: cs.CV

TL;DR: SNR-Edit是一种无需训练的轻量级框架，通过自适应噪声控制修正潜在轨迹，显著提升图像编辑质量，计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定高斯噪声的方法导致轨迹动态偏差，引发结构退化或质量损失，需一种无需训练或反演的轻量级解决方案。

Method: 采用结构感知噪声校正技术，将分割约束注入初始噪声，固定源轨迹的随机分量至真实图像的隐式反演位置，减少轨迹漂移。

Result: 在SD3和FLUX模型上，SNR-Edit在PIE-Bench和SNR-Bench评估中表现出色，像素级指标和VLM评分均优，单图仅增加约1秒开销。

Conclusion: SNR-Edit通过自适应噪声控制实现了无训练的潜在轨迹校正，显著提升了图像编辑的保真度和结构完整性，且计算开销极小。

Abstract: Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.

</details>


### [27] [Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP](https://arxiv.org/abs/2601.19210)
*Sen Nie,Jie Zhang,Zhuo Wang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: CSR是一种高效测试时防御方法，通过光谱引导的对比目标优化校正扰动，显著提升对抗样本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时防御方法在面对强攻击时鲁棒性不足，且存在高推理延迟和任务特定适用性问题。

Method: 提出了一种名为对比光谱校正（CSR）的高效测试时防御方法，通过优化校正扰动来重新对齐输入与自然流形。

Result: 在16个分类基准测试中，CSR平均优于SOTA方法18.1%，且推理开销适中。

Conclusion: CSR通过光谱引导的对比目标优化校正扰动，有效提升了对抗样本的鲁棒性，并在多个视觉任务中展现出广泛适用性。

Abstract: Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.

</details>


### [28] [UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection](https://arxiv.org/abs/2601.19222)
*Fuxiang Sun,Xi Jiang,Jiansheng Wu,Haigang Zhang,Feng Zheng,Jinfeng Yang*

Main category: cs.CV

TL;DR: UniPCB是首个针对PCB质量检测的统一视觉语言基准，结合新模型PCB-GPT（采用渐进式课程训练），显著提升缺陷定位性能，计划开源推动研究。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂场景（如PCB检测）中表现不足，缺乏高质量、统一的视觉语言基准来量化评估，主要由于数据稀缺、碎片化和标准不一致。

Method: 通过系统化流程整合和标准化不同来源的数据，构建UniPCB基准，并基于新生成的指令数据集训练PCB-GPT模型，采用模拟人类专家学习过程的渐进式课程。

Result: 在UniPCB基准测试中，PCB-GPT在细粒度缺陷定位上性能翻倍，显著优于现有MLLMs，尤其在定位和分析任务中表现突出。

Conclusion: UniPCB和PCB-GPT为PCB质量检测领域提供了首个统一的视觉语言基准和新基线，显著提升了细粒度缺陷定位性能，并计划发布数据、基准和模型以推动未来研究。

Abstract: Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research.

</details>


### [29] [Towards Pixel-Level VLM Perception via Simple Points Prediction](https://arxiv.org/abs/2601.19228)
*Tianhui Song,Haoyu Lu,Hao Yang,Lin Sui,Haoning Wu,Zaida Zhou,Zhiqi Huang,Yiping Bao,Y. Charles,Xinyu Zhou,Limin Wang*

Main category: cs.CV

TL;DR: SimpleSeg通过简单序列生成实现像素级分割，性能媲美复杂方法，展示了MLLMs在低层次感知上的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在低层次感知上的潜在能力，证明无需专门架构即可实现高精度的空间理解，简化现有分割方法的复杂性。

Method: 将分割任务重新定义为简单的序列生成问题，模型直接预测对象边界的点序列（文本坐标），并通过两阶段SF→RL训练流程（基于IoU奖励的强化学习）优化点序列以匹配真实轮廓。

Result: 在分割基准测试中，SimpleSeg的性能与依赖复杂任务特定设计的方法相当甚至更优。

Conclusion: SimpleSeg通过简单的序列生成方法，成功解锁了MLLMs在像素级感知上的潜力，挑战了现有方法对复杂辅助组件的依赖，为更统一的视觉语言模型铺平了道路。

Abstract: We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/

</details>


### [30] [VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics](https://arxiv.org/abs/2601.19236)
*Zhiyu Yin,Zhipeng Liu,Kehai Chen,Lemao Liu,Jin Liu,Hong-Dong Li,Yang Xiang,Min Zhang*

Main category: cs.CV

TL;DR: 提出了VC-Bench基准测试，用于评估视频连接任务，发现现有模型在一致性和平滑性上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成主要关注文本或图像条件，但实际应用中需要无缝连接独立视频片段，缺乏标准化评估基准。

Method: 提出了VC-Bench基准测试，包含1,579个高质量视频，覆盖15个主类别和72个子类别，并定义了三个核心评估指标：VQS、SECS和TSS。

Result: 实验表明现有视频生成模型在保持起始-结束一致性和过渡平滑性方面存在显著不足。

Conclusion: VC-Bench作为首个专门为视频连接任务设计的基准测试，预期将推动该领域的未来研究。

Abstract: While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.

</details>


### [31] [TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment](https://arxiv.org/abs/2601.19247)
*Jiarun Liu,Qifeng Chen,Yiru Zhao,Minghua Liu,Baorui Ma,Sheng Yang*

Main category: cs.CV

TL;DR: TIGaussian利用3D高斯泼溅特性，通过多分支标记器和双向对齐策略，提升了3D与文本/图像模态的跨模态对齐效果。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型已成功连接文本和图像特征，但3D模态数据的整合仍面临特征提取和模态间差距的挑战。

Method: 提出TIGaussian框架，包括多分支3DGS标记器和双向跨模态对齐策略（多视图特征融合机制和文本-3D投影模块）。

Result: 在多个数据集上的实验表明，TIGaussian在跨模态检索、零样本分类和场景识别等任务中表现优异。

Conclusion: TIGaussian框架通过多分支3DGS标记器和模态特定的3D特征对齐策略，显著提升了跨模态对齐效果，并在多个任务中实现了最先进的性能。

Abstract: While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.

</details>


### [32] [Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images](https://arxiv.org/abs/2601.19262)
*Syed Mehedi Hasan Nirob,Moqsadur Rahman,Shamim Ehsan,Summit Haque*

Main category: cs.CV

TL;DR: 研究评估了多种手工特征在检测合成图像中的表现，发现LightGBM结合混合特征效果最佳，强调了手工特征和集成学习在可解释性和效率场景中的价值。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展使得高度逼真的合成图像成为可能，引发了人们对数字媒体真实性和信任的担忧。可靠检测此类虚假内容是一个紧迫的挑战。

Method: 使用CIFAKE数据集对七种手工特征（包括原始像素、颜色直方图、DCT、HOG、LBP、GLCM和小波特征）进行系统评估，并比较了从逻辑回归到高级梯度提升集成（LightGBM、XGBoost、CatBoost）的七种分类器。

Result: LightGBM在混合特征下表现最佳，PR-AUC为0.9879，ROC-AUC为0.9878，F1为0.9447，Brier得分为0.0414，显示出在校准和判别方面的显著优势。

Conclusion: 研究强调了精心设计的特征和集成学习在检测合成图像中的持续相关性，特别是在可解释性和计算效率至关重要的场景中。

Abstract: The rapid progress of generative models has enabled the creation of highly realistic synthetic images, raising concerns about authenticity and trust in digital media. Detecting such fake content reliably is an urgent challenge. While deep learning approaches dominate current literature, handcrafted features remain attractive for their interpretability, efficiency, and generalizability. In this paper, we conduct a systematic evaluation of handcrafted descriptors, including raw pixels, color histograms, Discrete Cosine Transform (DCT), Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gray-Level Co-occurrence Matrix (GLCM), and wavelet features, on the CIFAKE dataset of real versus synthetic images. Using 50,000 training and 10,000 test samples, we benchmark seven classifiers ranging from Logistic Regression to advanced gradient-boosted ensembles (LightGBM, XGBoost, CatBoost). Results demonstrate that LightGBM consistently outperforms alternatives, achieving PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, and a Brier score of 0.0414 with mixed features, representing strong gains in calibration and discrimination over simpler descriptors. Across three configurations (baseline, advanced, mixed), performance improves monotonically, confirming that combining diverse handcrafted features yields substantial benefit. These findings highlight the continued relevance of carefully engineered features and ensemble learning for detecting synthetic images, particularly in contexts where interpretability and computational efficiency are critical.

</details>


### [33] [Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods](https://arxiv.org/abs/2601.19461)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 论文首次系统性评估了八种立体深度估计方法在植被密集环境中的表现，发现DEFOM跨域一致性最佳，将作为未来基准的伪真实值。


<details>
  <summary>Details</summary>
Motivation: 自主无人机林业操作需要具有强跨域泛化能力的深度估计，但现有评估主要关注城市和室内场景，植被密集环境存在关键空白。

Method: 对八种立体方法（包括迭代细化、基础模型、基于扩散和3D CNN范式）进行了首次系统性零样本评估，使用官方发布的预训练权重（在Scene Flow上训练），并在四个标准基准（ETH3D、KITTI 2012/2015、Middlebury）及新提出的Canterbury Tree Branches数据集上进行测试。

Result: 结果显示场景依赖性模式：基础模型在结构化场景表现优异（如BridgeDepth在ETH3D上为0.23 px；DEFOM在Middlebury上为4.65 px），而迭代方法在不同基准上表现不一（如IGEV++在ETH3D上为0.36 px但在Middlebury上为6.77 px）。DEFOM在Tree Branches数据集上表现最佳，平均排名1.75。

Conclusion: DEFOM被确立为植被深度估计的黄金标准基线，其跨域一致性表现最佳，将作为未来基准测试的伪真实值。

Abstract: Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.

</details>


### [34] [A Multi-View Consistency Framework with Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2601.19266)
*Yuting Hong,Li Dong,Xiaojie Qiu,Hui Xiao,Baochen Yao,Siming Zheng,Chengbin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种多视图一致性框架，通过去偏策略和伪负标签解决半监督域适应中的预测偏差问题，并引入跨域亲和力学习提升性能，实验证明其在DomainNet和Office-Home数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于目标域中标记样本数量有限，特征空间中可能存在类别的内在相似性，即使在平衡数据集上训练模型，也可能导致预测偏差。

Method: 引入了一个多视图一致性框架，包括两个视图用于训练强增强数据。一个是用于根据模型的预测性能校正类间预测概率的去偏策略，另一个涉及利用模型预测得到的伪负标签。此外，还引入了跨域亲和力学习，旨在对齐不同域中同一类别的特征。

Result: 实验结果表明，该方法在两个标准域适应数据集DomainNet和Office-Home上优于竞争方法。

Conclusion: 结合无监督域适应和半监督学习的方法在工业领域提供了不可或缺的贡献，增强了模型的适应性，降低了标注成本，并提高了性能。

Abstract: Semi-Supervised Domain Adaptation (SSDA) leverages knowledge from a fully labeled source domain to classify data in a partially labeled target domain. Due to the limited number of labeled samples in the target domain, there can be intrinsic similarity of classes in the feature space, which may result in biased predictions, even when the model is trained on a balanced dataset. To overcome this limitation, we introduce a multi-view consistency framework, which includes two views for training strongly augmented data. One is a debiasing strategy for correcting class-wise prediction probabilities according to the prediction performance of the model. The other involves leveraging pseudo-negative labels derived from the model predictions. Furthermore, we introduce a cross-domain affinity learning aimed at aligning features of the same class across different domains, thereby enhancing overall performance. Experimental results demonstrate that our method outperforms the competing methods on two standard domain adaptation datasets, DomainNet and Office-Home. Combining unsupervised domain adaptation and semi-supervised learning offers indispensable contributions to the industrial sector by enhancing model adaptability, reducing annotation costs, and improving performance.

</details>


### [35] [The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments](https://arxiv.org/abs/2601.19557)
*Riccardo Giubilato,Marcus Gerhard Müller,Marco Sewtz,Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel*

Main category: cs.CV

TL;DR: S3LI Vulcano数据集支持多模态SLAM与地点识别算法，提供多样化环境数据与开源工具包。


<details>
  <summary>Details</summary>
Motivation: 为了促进依赖视觉和LiDAR模态的SLAM与地点识别算法的发展与基准测试。

Method: 通过记录火山岛Vulcano上的多个序列，采集了不同环境、纹理和地形（如玄武岩、富含铁的岩石、古老熔岩通道的地质构造、干燥植被和水域）的数据。

Result: 发布了包含多样化环境数据的S3LI Vulcano数据集，并提供了生成地面真实位姿和标记样本的开源工具包。

Conclusion: S3LI Vulcano数据集为依赖视觉和LiDAR模态的SLAM与地点识别算法的开发和基准测试提供了多模态数据支持，并附带开源工具包。

Abstract: We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.

</details>


### [36] [ProMist-5K: A Comprehensive Dataset for Digital Emulation of Cinematic Pro-Mist Filter Effects](https://arxiv.org/abs/2601.19295)
*Yingtie Lei,Zimeng Li,Chi-Man Pun,Wangyu Wu,Junke Yang,Xuhang Chen*

Main category: cs.CV

TL;DR: ProMist-5K是一个专注于电影风格模拟的数据集，通过物理启发的管道构建，包含20,000个图像对，支持多种图像翻译模型和学习范式。


<details>
  <summary>Details</summary>
Motivation: Pro-Mist滤镜在电影摄影中广泛使用，但其效果难以通过数字方式复现，因为光扩散行为复杂。

Method: 使用物理启发的管道在场景参考线性空间中构建，包含20,000个高分辨率图像对，涵盖四种配置（两种滤镜密度和两种焦距）。采用多层模糊和精心调整的权重来模拟光学扩散的不同强度和范围。

Result: 实验表明，该数据集在不同训练设置下表现良好，有助于捕捉微妙和强烈的电影外观。

Conclusion: ProMist-5K提供了一个实用且基于物理的资源，用于电影风格的图像转换，弥合了数字灵活性和传统镜头美学之间的差距。

Abstract: Pro-Mist filters are widely used in cinematography for their ability to create soft halation, lower contrast, and produce a distinctive, atmospheric style. These effects are difficult to reproduce digitally due to the complex behavior of light diffusion. We present ProMist-5K, a dataset designed to support cinematic style emulation. It is built using a physically inspired pipeline in a scene-referred linear space and includes 20,000 high-resolution image pairs across four configurations, covering two filter densities (1/2 and 1/8) and two focal lengths (20mm and 50mm). Unlike general style datasets, ProMist-5K focuses on realistic glow and highlight diffusion effects. Multiple blur layers and carefully tuned weighting are used to model the varying intensity and spread of optical diffusion. The dataset provides a consistent and controllable target domain that supports various image translation models and learning paradigms. Experiments show that the dataset works well across different training settings and helps capture both subtle and strong cinematic appearances. ProMist-5K offers a practical and physically grounded resource for film-inspired image transformation, bridging the gap between digital flexibility and traditional lens aesthetics. The dataset is available at https://www.kaggle.com/datasets/yingtielei/promist5k.

</details>


### [37] [VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction](https://arxiv.org/abs/2601.19887)
*Dominic Maggio,Luca Carlone*

Main category: cs.CV

TL;DR: VGGT-SLAM 2.0通过新因子图设计和注意力层优化，显著提升SLAM性能，减少姿态误差23%，并在多种环境中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 改进VGGT-SLAM系统，解决其存在的漂移、平面退化及重建模糊性问题，同时提升图像检索验证能力。

Method: 提出了新的因子图设计以消除高维漂移和平面退化，并利用VGGT的注意力层进行图像检索验证，无需额外训练。

Result: 实验表明，VGGT-SLAM 2.0在开放集目标检测和实时性能上表现优异，且在TUM数据集上精度最高，姿态误差减少23%。

Conclusion: VGGT-SLAM 2.0显著提升了VGGT-SLAM的性能，通过新的因子图设计解决了漂移和平面退化问题，并在TUM数据集上实现了最高精度，姿态误差减少了约23%。

Abstract: We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.

</details>


### [38] [Beyond Shadows: A Large-Scale Benchmark and Multi-Stage Framework for High-Fidelity Facial Shadow Removal](https://arxiv.org/abs/2601.19309)
*Tailong Luo,Jiesong Bai,Jinyang Huang,Junyu Xia,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: ASFW是首个大规模真实世界面部阴影去除数据集，结合FSE方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂光照条件下难以去除阴影并保留纹理，且缺乏真实世界的配对数据集。

Method: 提出了Augmented Shadow Face in the Wild (ASFW)数据集和Face Shadow Eraser (FSE)方法。

Result: ASFW数据集包含1,081对阴影和无阴影图像，训练出的深度模型在真实场景中表现更优。

Conclusion: ASFW数据集和FSE方法显著提升了面部阴影去除模型的性能，为该任务设定了新标准。

Abstract: Facial shadows often degrade image quality and the performance of vision algorithms. Existing methods struggle to remove shadows while preserving texture, especially under complex lighting conditions, and they lack real-world paired datasets for training. We present the Augmented Shadow Face in the Wild (ASFW) dataset, the first large-scale real-world dataset for facial shadow removal, containing 1,081 paired shadow and shadow-free images created via a professional Photoshop workflow. ASFW offers photorealistic shadow variations and accurate ground truths, bridging the gap between synthetic and real domains. Deep models trained on ASFW demonstrate improved shadow removal in real-world conditions. We also introduce the Face Shadow Eraser (FSE) method to showcase the effectiveness of the dataset. Experiments demonstrate that ASFW enhances the performance of facial shadow removal models, setting new standards for this task.

</details>


### [39] [Instance-Guided Radar Depth Estimation for 3D Object Detection](https://arxiv.org/abs/2601.19314)
*Chen-Chou Lo,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 该论文提出了一种通过InstaRadar增强雷达密度和语义对齐，并集成RCDPT到BEVDepth框架的方法，显著提升了单目3D物体检测性能，尽管在BEV特征提取上仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 单目相机在3D检测中存在深度模糊性和在挑战性条件下鲁棒性降低的问题，而雷达虽然对光照不足和恶劣天气具有鲁棒性，但其稀疏性和低分辨率限制了其在检测框架中的直接使用。这促使需要有效的雷达-相机融合以及改进的预处理和深度估计策略。

Method: 提出了一个端到端框架，包含两个关键组件：InstaRadar（一种实例分割引导的扩展方法）和将预训练的RCDPT集成到BEVDepth框架中作为其深度模块的替代。

Result: InstaRadar在雷达引导的深度估计中取得了最先进的结果，RCDPT集成持续提升了3D检测性能，整体上这些组件相对于基线BEVDepth模型带来了稳定的增益。

Conclusion: 尽管该框架在直接提取BEV特征的雷达-相机融合模型上稍显不足，但雷达仅作为指导而非独立特征流的局限性也凸显了改进潜力。未来工作将扩展InstaRadar至类似点云的表示，并整合带有时间线索的专用雷达分支以增强BEV融合。

Abstract: Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.

</details>


### [40] [Innovator-VL: A Multimodal Large Language Model for Scientific Discovery](https://arxiv.org/abs/2601.19325)
*Zichen Wen,Boxue Yang,Shuang Chen,Yaojie Zhang,Yuhang Han,Junlong Ke,Cong Wang,Yicheng Fu,Jiawang Zhao,Jiangchao Yao,Xi Fang,Zhen Wang,Henxing Cai,Lin Yao,Zhifeng Gao,Yanhui Hong,Nang Yuan,Yixuan Li,Guojiang Zhao,Haoyi Tao,Nan Wang,Han Lyu,Guolin Ke,Ning Liao,Xiaoxing Wang,Kai Chen,Zhiyu Li,Feiyu Xiong,Sihan Hu,Kun Chen,Yanfeng Wang,Weinan E,Linfeng Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: Innovator-VL是一种科学多模态大语言模型，通过透明方法和原则性设计，在少量数据下实现高效科学推理和通用任务性能。


<details>
  <summary>Details</summary>
Motivation: 挑战依赖大规模领域特定预训练和不透明流程的趋势，证明通过原则性设计和透明方法可以实现高效的科学智能。

Method: 提供完全透明、端到端可复现的训练流程，包括数据收集、清理、预处理、监督微调、强化学习和评估，并详细优化方案。

Result: Innovator-VL在科学任务上表现出卓越的数据效率，仅用不到五百万样本即达到竞争性性能，同时在通用视觉、多模态推理和科学基准上表现优异。

Conclusion: Innovator-VL展示了通过原则性训练设计和透明方法，可以在减少数据需求的同时实现强大的科学智能和通用视觉任务性能，为未来研究提供了实用基础。

Abstract: We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.

</details>


### [41] [Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation](https://arxiv.org/abs/2601.19365)
*Jinming Zhang,Xi Yang,Youpeng Yang,Haosen Shi,Yuyao Yan,Qiufeng Wang,Guangliang Cheng,Kaizhu Huang*

Main category: cs.CV

TL;DR: 论文提出了一种针对医学图像分割边界模糊性的区域课程策略和帕累托一致损失函数，实验证明其在多种配置下均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中的不确定性在边界区域尤为显著，传统训练方法对所有像素平等对待，导致早期训练不稳定，阻碍帕累托最优解的收敛。

Method: 论文引入了区域课程策略，优先从确定性区域学习并逐步纳入不确定区域，减少梯度方差。此外，开发了帕累托一致损失函数和模糊标记机制，平衡区域间的不确定性并稳定梯度。

Result: 在脑转移和非转移肿瘤分割任务中，该方法在所有肿瘤子区域均优于传统方法，展示了其一致性和有效性。

Conclusion: 该论文提出了一种区域课程策略和帕累托一致损失函数，通过自适应调整损失景观和约束收敛动态，有效解决了医学图像分割中边界区域的高模糊性问题。实验证明该方法在脑转移和非转移肿瘤分割任务中均优于传统方法。

Abstract: Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.

</details>


### [42] [Establishing dermatopathology encyclopedia DermpathNet with Artificial Intelligence-Based Workflow](https://arxiv.org/abs/2601.19378)
*Ziyang Xu,Mingquan Lin,Yiliang Zhou,Zihan Xu,Seth J. Orlow,Zihan Xu,Shane A. Meehan,Alexandra Flamm,Ata S. Moshiri,Yifan Peng*

Main category: cs.CV

TL;DR: 开发了DermpathNet，一个开放访问的皮肤病理学图像数据集，采用混合工作流程整理，验证结果显示高准确率，数据集可用于教育和机器学习。


<details>
  <summary>Details</summary>
Motivation: 解决临床医生和皮肤病理学学员在获取高质量、开放访问的皮肤病理学图像数据集用于学习和交叉参考时的常见挑战。

Method: 采用混合工作流程整理和分类来自PubMed Central（PMC）存储库的图像，结合基于深度学习的图像模态分类和图形标题分析的新方法。

Result: 在651个手动注释的图像上验证了工作流程的稳健性，深度学习方法的F分数为89.6%，基于关键词的检索方法为61.0%，混合方法为90.4%。检索了超过7,772张图像，涵盖166个诊断，并发布了这个完全注释的数据集。

Conclusion: 研究人员开发了一个大型、经过同行评审、开放访问的皮肤病理学图像数据集DermpathNet，并采用了半自动化的整理工作流程。

Abstract: Accessing high-quality, open-access dermatopathology image datasets for learning and cross-referencing is a common challenge for clinicians and dermatopathology trainees. To establish a comprehensive open-access dermatopathology dataset for educational, cross-referencing, and machine-learning purposes, we employed a hybrid workflow to curate and categorize images from the PubMed Central (PMC) repository. We used specific keywords to extract relevant images, and classified them using a novel hybrid method that combined deep learning-based image modality classification with figure caption analyses. Validation on 651 manually annotated images demonstrated the robustness of our workflow, with an F-score of 89.6\% for the deep learning approach, 61.0\% for the keyword-based retrieval method, and 90.4\% for the hybrid approach. We retrieved over 7,772 images across 166 diagnoses and released this fully annotated dataset, reviewed by board-certified dermatopathologists. Using our dataset as a challenging task, we found the current image analysis algorithm from OpenAI inadequate for analyzing dermatopathology images. In conclusion, we have developed a large, peer-reviewed, open-access dermatopathology image dataset, DermpathNet, which features a semi-automated curation workflow.

</details>


### [43] [Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT](https://arxiv.org/abs/2601.19380)
*Fakrul Islam Tushar,Joseph Y. Lo*

Main category: cs.CV

TL;DR: Tri-Reader是一个免费的三阶段工作流，整合肺部分割、结节检测和恶性分类，优先敏感性和减少标注负担，经多数据集验证表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合肺部分割、结节检测和恶性分类的统一流程，以提升敏感性并减轻标注负担。

Method: 使用多个基于公共数据集训练的开源模型，开发了Tri-Reader流程，并评估其在多个内部和外部数据集上的表现。

Result: Tri-Reader在多个数据集上表现出色，与专家标注和参考标准相比具有高准确性和泛化性。

Conclusion: Tri-Reader是一个综合、免费的流程，通过整合肺部分割、结节检测和恶性分类三阶段工作流，优先考虑敏感性并减少标注者负担。在多个内部和外部数据集上的评估验证了其准确性和泛化能力。

Abstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards.

</details>


### [44] [Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection](https://arxiv.org/abs/2601.19430)
*Yao Xiao,Weiyan Chen,Jiahao Chen,Zijie Cao,Weijian Deng,Binbin Yang,Ziyi Dong,Xiangyang Ji,Wei Ke,Pengxu Wei,Liang Lin*

Main category: cs.CV

TL;DR: X-AIGD是一个细粒度的AIGI检测基准，提供像素级注释，研究发现现有检测器对伪影依赖低，但显式对齐注意力可提升可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AIGI检测方法主要依赖二元分类，缺乏可解释或令人信服的证据支持其决策，现有基准在伪影多样性和详细注释方面受限。

Method: 引入了一个细粒度的基准X-AIGD，提供像素级、分类的感知伪影注释，涵盖低层失真、高层语义和认知级反事实。

Result: X-AIGD基准为可解释性评估和模型决策过程提供了更深入的洞察。研究发现现有检测器对伪影的依赖极低，但通过显式对齐注意力可提升性能。

Conclusion: 现有的AIGI检测器对感知伪影的依赖极低，即使是最基础的失真级别。虽然可以训练这些检测器识别特定伪影，但其判断仍主要基于不可解释的特征。显式对齐模型注意力与伪影区域可提升检测器的可解释性和泛化能力。

Abstract: Current AI-Generated Image (AIGI) detection approaches predominantly rely on binary classification to distinguish real from synthetic images, often lacking interpretable or convincing evidence to substantiate their decisions. This limitation stems from existing AIGI detection benchmarks, which, despite featuring a broad collection of synthetic images, remain restricted in their coverage of artifact diversity and lack detailed, localized annotations. To bridge this gap, we introduce a fine-grained benchmark towards eXplainable AI-Generated image Detection, named X-AIGD, which provides pixel-level, categorized annotations of perceptual artifacts, spanning low-level distortions, high-level semantics, and cognitive-level counterfactuals. These comprehensive annotations facilitate fine-grained interpretability evaluation and deeper insight into model decision-making processes. Our extensive investigation using X-AIGD provides several key insights: (1) Existing AIGI detectors demonstrate negligible reliance on perceptual artifacts, even at the most basic distortion level. (2) While AIGI detectors can be trained to identify specific artifacts, they still substantially base their judgment on uninterpretable features. (3) Explicitly aligning model attention with artifact regions can increase the interpretability and generalization of detectors. The data and code are available at: https://github.com/Coxy7/X-AIGD.

</details>


### [45] [RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming](https://arxiv.org/abs/2601.19433)
*Jisheng Chu,Wenrui Li,Rui Zhao,Wangmeng Zuo,Shifeng Chen,Xiaopeng Fan*

Main category: cs.CV

TL;DR: RoamScene3D利用语义推理和动态修复模型，解决了现有3D场景生成方法的局限性，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在空间盲点且依赖预定义轨迹，无法理解语义布局或自适应推断遮挡内容。

Method: 采用视觉语言模型（VLM）构建场景图编码对象关系，并引入Motion-Injected Inpainting模型以适应相机运动。

Result: 实验证明该方法在生成一致且逼真的3D场景方面表现优异。

Conclusion: RoamScene3D通过语义推理和几何约束，显著优于现有方法，生成一致且逼真的3D场景。

Abstract: Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.

</details>


### [46] [DSTCS: Dual-Student Teacher Framework with Segment Anything Model for Semi-Supervised Pubic Symphysis Fetal Head Segmentation](https://arxiv.org/abs/2601.19446)
*Yalin Luo,Shun Long,Huijin Wang,Jieyun Bai*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和SAM的双学生-教师框架（DSTCS），通过协作学习、专用数据增强和新颖损失函数，显著提升PSFH分割的准确性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于类别不平衡、边界模糊和超声图像中的噪声干扰，以及高质量标注数据的稀缺性，PSFH的准确分割仍是一个重大挑战。当前研究主要依赖CNN和Transformer架构，更强大模型的潜力尚未充分探索。

Method: 提出了一个结合CNN和SAM的双学生-教师框架（DSTCS），通过CNN和SAM分支之间的协作学习机制显著提高了分割准确性。方案还包含针对边界处理的专用数据增强策略和新颖的损失函数。

Result: 在MICCAI 2023和2024 PSFH分割基准测试上的大量实验表明，该方法表现出卓越的鲁棒性，并显著优于现有技术。

Conclusion: 提出的Dual-Student and Teacher框架结合CNN和SAM（DSTCS）在PSFH分割任务中表现出卓越的鲁棒性，显著优于现有技术，为临床实践提供了可靠的分割工具。

Abstract: Segmentation of the pubic symphysis and fetal head (PSFH) is a critical procedure in intrapartum monitoring and is essential for evaluating labor progression and identifying potential delivery complications. However, achieving accurate segmentation remains a significant challenge due to class imbalance, ambiguous boundaries, and noise interference in ultrasound images, compounded by the scarcity of high-quality annotated data. Current research on PSFH segmentation predominantly relies on CNN and Transformer architectures, leaving the potential of more powerful models underexplored. In this work, we propose a Dual-Student and Teacher framework combining CNN and SAM (DSTCS), which integrates the Segment Anything Model (SAM) into a dual student-teacher architecture. A cooperative learning mechanism between the CNN and SAM branches significantly improves segmentation accuracy. The proposed scheme also incorporates a specialized data augmentation strategy optimized for boundary processing and a novel loss function. Extensive experiments on the MICCAI 2023 and 2024 PSFH segmentation benchmarks demonstrate that our method exhibits superior robustness and significantly outperforms existing techniques, providing a reliable segmentation tool for clinical practice.

</details>


### [47] [Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes](https://arxiv.org/abs/2601.19484)
*Yin Wang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: Dyn-HSI是首个动态人-场景交互认知架构，通过视觉、记忆和控制组件，在动态场景中生成高质量交互动作。


<details>
  <summary>Details</summary>
Motivation: 现有的人-场景交互生成方法通常将场景视为静态，这与现实世界中的动态变化不符，因此需要一种能够适应动态场景的认知架构。

Method: Dyn-HSI结合了动态场景感知导航、分层经验记忆和人-场景交互扩散模型，通过多模态输入生成高保真交互动作。

Result: 在动态基准Dyn-Scenes上的实验表明，Dyn-HSI在静态和动态场景中均能生成高质量的人-场景交互动作，且性能优于现有方法。

Conclusion: Dyn-HSI通过赋予虚拟人类视觉、记忆和控制三个类人组件，显著提升了动态场景中的人-场景交互生成质量，并在静态和动态场景中均优于现有方法。

Abstract: Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.

</details>


### [48] [Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation](https://arxiv.org/abs/2601.19488)
*Yizhao Han,Tianxing Shi,Zhao Wang,Zifan Xu,Zhiyuan Pu,Mingxiao Li,Qian Zhang,Wei Yin,Xiao-Xiao Long*

Main category: cs.CV

TL;DR: ENkG是一种自适应采样策略，通过动态调整候选标记数量，有效提升视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频标记的低语义密度和高时空冗余使得静态的top-k/top-p采样策略在视频解码器中效果不佳，导致生成质量下降。

Method: 提出了一种名为ENkG的自适应采样策略，根据每个标记的预测分布熵值动态调整候选标记数量，以应对视频生成中的冗余和误差累积。

Result: 实验表明，ENkG策略在感知质量和结构稳定性上均优于静态的top-k/top-p策略。

Conclusion: ENkG采样策略通过自适应调整候选标记数量，有效解决了视频生成中的冗余和误差累积问题，显著提升了生成视频的感知质量和结构稳定性。

Abstract: Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.

</details>


### [49] [Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction](https://arxiv.org/abs/2601.19489)
*Ziyu Zhang,Tianle Liu,Diantao Tu,Shuhan Shen*

Main category: cs.CV

TL;DR: 提出一种快速3DGS重建管道，两阶段处理异构设置，一分钟内实现高保真重建，竞赛排名第一。


<details>
  <summary>Details</summary>
Motivation: 为SIGGRAPH Asia 3DGS快速重建挑战设计，需在异构设置（SLAM生成的有噪声轨迹和COLMAP的高精度姿态）下实现快速重建。

Method: 采用两阶段解决方案：第一阶段使用反向逐高斯并行优化、紧凑前向溅射等技术；第二阶段禁用姿态优化，回归标准3DGS并引入多视角一致性引导的高斯分割。

Result: 在竞赛中取得PSNR 28.43的顶级性能，排名第一。

Conclusion: 该方法在严格的一分钟预算内实现了高保真重建，并在竞赛中以PSNR 28.43的成绩排名第一。

Abstract: We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.

</details>


### [50] [Cortex-Grounded Diffusion Models for Brain Image Generation](https://arxiv.org/abs/2601.19498)
*Fabian Bongratz,Yitong Li,Sama Elbaroudy,Christian Wachinger*

Main category: cs.CV

TL;DR: Cor2Vox是一种基于皮层结构的脑MRI生成框架，通过扩散过程和统计形状模型实现高保真合成，适用于多种医学影像任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型依赖弱条件信号（如标签或文本）导致的生物不真实性，提升合成数据的解剖学准确性。

Method: 利用高分辨率皮层表面引导3D形状到图像的布朗桥扩散过程，开发了基于33,000多个UK Biobank扫描的大规模统计形状模型。

Result: 在图像质量指标、皮层表面重建和全脑分割质量上优于基线方法，并在三个应用中表现出色。

Conclusion: Cor2Vox通过结合皮层结构先验，实现了高保真、可控的脑MRI合成，展示了在多种应用中的强大适应性和鲁棒性。

Abstract: Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.

</details>


### [51] [Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration](https://arxiv.org/abs/2601.19506)
*Zhengjian Yao,Jiakui Hu,Kaiwen Li,Hangzhou He,Xinliang Zhang,Shuang Zeng,Lei Zhu,Yanye Lu*

Main category: cs.CV

TL;DR: Pref-Restore通过增强输入密度和修剪输出分布，解决了盲脸恢复中的信息不对称问题，实现了确定性恢复。


<details>
  <summary>Details</summary>
Motivation: 当前生成方法在信息不对称（低质量输入与高质量输出之间的信息密度差异）下导致一对多映射，引发随机不确定性和幻觉伪影。

Method: 采用自回归积分器将文本指令转化为密集潜在查询，增强输入密度；并在扩散恢复循环中直接集成策略强化学习，修剪输出分布。

Result: Pref-Restore在合成和真实世界基准测试中达到了最先进的性能，偏好对齐策略显著降低了解决方案熵。

Conclusion: Pref-Restore通过结合离散语义逻辑和连续纹理生成，实现了确定性且偏好对齐的盲脸恢复，显著降低了解决方案的熵，为可靠和确定性的盲恢复建立了稳健的路径。

Abstract: Blind face restoration remains a persistent challenge due to the inherent ill-posedness of reconstructing holistic structures from severely constrained observations. Current generative approaches, while capable of synthesizing realistic textures, often suffer from information asymmetry -- the intrinsic disparity between the information-sparse low quality inputs and the information-dense high quality outputs. This imbalance leads to a one-to-many mapping, where insufficient constraints result in stochastic uncertainty and hallucinatory artifacts. To bridge this gap, we present \textbf{Pref-Restore}, a hierarchical framework that integrates discrete semantic logic with continuous texture generation to achieve deterministic, preference-aligned restoration. Our methodology fundamentally addresses this information disparity through two complementary strategies: (1) Augmenting Input Density: We employ an auto-regressive integrator to reformulate textual instructions into dense latent queries, injecting high-level semantic stability to constrain the degraded signals; (2) Pruning Output Distribution: We pioneer the integration of on-policy reinforcement learning directly into the diffusion restoration loop. By transforming human preferences into differentiable constraints, we explicitly penalize stochastic deviations, thereby sharpening the posterior distribution toward the desired high-fidelity outcomes. Extensive experiments demonstrate that Pref-Restore achieves state-of-the-art performance across synthetic and real-world benchmarks. Furthermore, empirical analysis confirms that our preference-aligned strategy significantly reduces solution entropy, establishing a robust pathway toward reliable and deterministic blind restoration.

</details>


### [52] [A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder](https://arxiv.org/abs/2601.19526)
*Fouad Boutaleb,Emery Pierson,Mohamed Daoudi,Clémence Nineuil,Ali Amad,Fabien D'Hondt*

Main category: cs.CV

TL;DR: 该论文提出了一种非侵入性方法，通过单目RGB视频提取步态特征，结合机器学习框架，实现了抑郁症心理运动迟缓的客观检测，并揭示了步态特征与抑郁严重程度的关联。


<details>
  <summary>Details</summary>
Motivation: 抑郁症的临床评估通常依赖主观方法，而现有的3D运动捕捉技术因依赖专业硬件难以在临床常规使用。因此，需要一种非侵入性、客观且可解释的方法来评估抑郁症患者的运动症状。

Method: 提出了一种非侵入性计算框架，结合重力视图坐标和轨迹校正算法，从单目RGB视频中提取297个步态生物力学标志物，并采用基于稳定性的机器学习框架防止过拟合。

Result: 在CALYPSO数据集上验证，该方法检测心理运动迟缓（PMR）的准确率达83.3%，并能解释64%的抑郁严重程度方差（R²=0.64）。研究发现踝关节推进力减少和骨盆活动受限与抑郁运动表型密切相关。

Conclusion: 该研究通过非侵入性计算框架从单目RGB视频中提取临床相关的3D步态运动学特征，成功识别了与抑郁运动表型相关的生物标志物，为抑郁症的客观监测提供了透明且可扩展的工具。

Abstract: Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.
  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.
  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.

</details>


### [53] [MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation](https://arxiv.org/abs/2601.19577)
*Ronglai Zuo,Rolandos Alexandros Potamias,Qi Sun,Evangelos Ververas,Jiankang Deng,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: MaDiS是一种基于掩码扩散的手语生成模型，通过三层次跨模态预训练和高效解掩策略，显著提升性能并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归语言模型在手语生成中存在的单向上下文建模和慢速逐令牌推理问题。

Method: 采用掩码扩散模型（MaDiS）和三层次跨模态预训练方案，结合解掩策略和部分混合嵌入层。

Result: 在CSL-Daily、Phoenix-2014T和How2Sign数据集上，MaDiS在多个指标上表现优异，推理延迟降低近30%。

Conclusion: MaDiS通过掩码扩散模型和三层次跨模态预训练方案，显著提升了手语生成的性能，并在多个数据集上验证了其优越性。

Abstract: Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.

</details>


### [54] [QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture](https://arxiv.org/abs/2601.19580)
*Cuong Le,Pavlo Melnyk,Urs Waldmann,Mårten Wadenbäck,Bastian Wandt*

Main category: cs.CV

TL;DR: QuaMo利用四元数微分方程解决运动捕捉中的不连续性问题，实验显示其在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D姿态估计方法忽视帧间时间一致性，导致运动不连贯；现有基于运动学的方法依赖欧拉角，存在不连续性问题。

Method: 提出QuaMo方法，利用四元数微分方程（QDE）和状态空间模型进行实时运动估计，结合元PD控制器和加速度增强技术。

Result: 在Human3.6M、Fit3D、SportsPose和AIST数据集上，QuaMo表现优于现有方法，运动估计连续且准确。

Conclusion: QuaMo通过四元数微分方程（QDE）和加速度增强，显著提升了3D人体运动捕捉的准确性和连续性，优于现有方法。

Abstract: Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo

</details>


### [55] [ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.19582)
*Yujin Wang,Yutong Zheng,Wenxian Fan,Tianyi Wang,Hongqing Chu,Daxin Tian,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: ScenePilot-Bench是一个大规模的第一人称驾驶基准，用于评估VLMs在自动驾驶场景中的能力，基于ScenePilot-4K数据集，包含多粒度标注和四轴评估套件。


<details>
  <summary>Details</summary>
Motivation: 为了评估VLMs在自动驾驶场景中的能力，并明确当前性能边界和驾驶导向推理的差距。

Method: 基于ScenePilot-4K数据集，构建了一个包含3,847小时驾驶视频的大规模基准，标注了多粒度信息，并设计了一个四轴评估套件。

Result: 通过基准测试代表性VLMs，提供了实证分析，阐明了当前性能边界并识别了驾驶导向推理的差距。

Conclusion: ScenePilot-Bench提供了一个全面的框架，用于评估和推进视觉语言模型（VLMs）在安全关键自动驾驶场景中的应用。

Abstract: In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.

</details>


### [56] [Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning](https://arxiv.org/abs/2601.19593)
*Estèphe Arnaud,Mohamed Daoudi,Pierre Guerreschi*

Main category: cs.CV

TL;DR: 研究提出了一种基于生成模型的肉毒杆菌毒素注射效果模拟框架，通过局部潜在轨迹学习和剂量响应建模，结合临床医生交互，优化注射计划。


<details>
  <summary>Details</summary>
Motivation: 肉毒杆菌毒素注射在面部不对称和美学年轻化管理中虽为黄金标准，但剂量确定仍依赖直觉，常导致效果不佳。因此，需要一种更精确的注射计划模拟方法。

Method: 研究通过区域特异性潜在轴发现方法，在StyleGAN2的潜在空间中学习局部肌肉松弛轨迹，并结合剂量响应建模，预测注射效果。比较了直接度量回归与基于图像的生成模拟两种方法。

Result: 在保留测试集上，生成模型对几何不对称指标表现出中等到强的结构相关性，验证了模型能正确捕捉形态变化方向。

Conclusion: 该研究提出了一个结合生成模型与临床医生交互的混合工作流，以弥补生物变异性带来的绝对精度限制，为肉毒杆菌毒素注射计划提供更精确的模拟工具。

Abstract: Botulinum toxin (Botox) injections are the gold standard for managing facial asymmetry and aesthetic rejuvenation, yet determining the optimal dosage remains largely intuitive, often leading to suboptimal outcomes. We propose a localized latent editing framework that simulates Botulinum Toxin injection effects for injection planning through dose-response modeling. Our key contribution is a Region-Specific Latent Axis Discovery method that learns localized muscle relaxation trajectories in StyleGAN2's latent space, enabling precise control over specific facial regions without global side effects. By correlating these localized latent trajectories with injected toxin units, we learn a predictive dose-response model. We rigorously compare two approaches: direct metric regression versus image-based generative simulation on a clinical dataset of N=360 images from 46 patients. On a hold-out test set, our framework demonstrates moderate-to-strong structural correlations for geometric asymmetry metrics, confirming that the generative model correctly captures the direction of morphological changes. While biological variability limits absolute precision, we introduce a hybrid "Human-in-the-Loop" workflow where clinicians interactively refine simulations, bridging the gap between pathological reconstruction and cosmetic planning.

</details>


### [57] [GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining](https://arxiv.org/abs/2601.19606)
*Shentong Mo,Zehua Chen,Jun Zhu*

Main category: cs.CV

TL;DR: GMS-CAVP通过多尺度对比学习和扩散生成目标，提升了视频-音频对应建模，实验证明其在生成和检索任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CAVP在建模视频和音频信号的多尺度密集特性上表现不足，导致性能欠佳。

Method: 提出GMS-CAVP框架，结合多尺度对比学习策略和基于扩散的生成目标，以增强视频-音频对应建模。

Result: 在VGGSound、AudioSet和Panda70M数据集上的实验表明，GMS-CAVP在生成和检索任务上优于之前的方法。

Conclusion: GMS-CAVP通过结合多尺度视频-音频对齐和基于扩散的预训练目标，显著提升了视频-音频对应建模的性能，在生成和检索任务上优于现有方法。

Abstract: Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.

</details>


### [58] [The role of self-supervised pretraining in differentially private medical image analysis](https://arxiv.org/abs/2601.19618)
*Soroosh Tayebi Arasteh,Mina Farajiamiri,Mahshad Lotfinia,Behrus Hinrichs-Puladi,Jonas Bienzeisler,Mohamed Alhaskir,Mirabela Rusu,Christiane Kuhl,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 差分隐私医学影像分析中，初始化策略（如DINOv3或领域特定预训练）显著影响诊断效用、公平性和泛化性，领域特定预训练表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨现代自监督学习在完整模型差分隐私下的作用，以减轻差分隐私对诊断性能的负面影响。

Method: 使用DP-SGD训练的最先进的ConvNeXt模型，在真实的隐私制度下，比较了非领域特定的监督ImageNet初始化、非领域特定的自监督DINOv3初始化和领域特定的监督预训练。

Result: DINOv3初始化在差分隐私下相对于ImageNet初始化一致提高了诊断效用，但仍不如领域特定的监督预训练，后者实现了最接近非隐私基线的性能。

Conclusion: 初始化策略是差分隐私医学影像中效用、公平性和泛化性的核心决定因素。

Abstract: Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images. Using state-of-the-art ConvNeXt models trained with DP-SGD across realistic privacy regimes, we compare non-domain-specific supervised ImageNet initialization, non-domain-specific self-supervised DINOv3 initialization, and domain-specific supervised pretraining on MIMIC-CXR, the largest publicly available chest radiograph dataset. Evaluations are conducted across five external datasets spanning diverse institutions and acquisition settings. We show that DINOv3 initialization consistently improves diagnostic utility relative to ImageNet initialization under DP, but remains inferior to domain-specific supervised pretraining, which achieves performance closest to non-private baselines. We further demonstrate that initialization choice strongly influences demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity under privacy constraints. The results establish initialization strategy as a central determinant of utility, fairness, and generalization in differentially private medical imaging.

</details>


### [59] [Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework](https://arxiv.org/abs/2601.19640)
*Hao Chang,Zhihui Wang,Lingxiang Wu,Peijin Wang,Wenhui Diao,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出了首个管理导向的多模态基准GovLA-10K和统一视觉语言推理框架GovLA-Reasoner，通过高效特征适配器协调视觉与语言模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的以对象为中心的感知范式与松散的视觉语言管道难以支持现实城市治理中所需的管理导向异常理解。

Method: 引入了一个高效的特征适配器，隐式协调视觉检测器与大语言模型（LLM）之间的判别性表示共享。

Result: 实验表明，该方法显著提高了性能，同时避免了针对任何任务特定组件的微调需求。

Conclusion: 本文提出了GovLA-10K基准和GovLA-Reasoner框架，为管理感知的低空视觉语言系统提供了新的视角和基础。

Abstract: Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.

</details>


### [60] [KeepLoRA: Continual Learning with Residual Gradient Adaptation](https://arxiv.org/abs/2601.19659)
*Mao-Lin Luo,Zi-Hao Zhou,Yi-Lin Zhang,Yuanyu Wan,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: KeepLoRA通过限制LoRA参数在残差子空间中的更新，有效平衡持续学习的三个目标，性能最优。


<details>
  <summary>Details</summary>
Motivation: 持续学习需要在保留预训练知识、保持已学任务知识和获取新知识三个目标之间取得平衡。KeepLoRA的动机是通过分析模型参数空间中的知识保留机制，发现通用知识主要编码在主空间，而任务特定知识编码在残差空间。

Method: KeepLoRA通过限制LoRA参数在残差子空间中的更新，避免干扰先前学习的能力。具体方法是将新任务的梯度投影到与预训练模型主空间和先前任务特征主导方向正交的子空间。

Result: 理论和实证分析证实，KeepLoRA平衡了三个目标，并实现了最先进的性能。

Conclusion: KeepLoRA通过将新任务的梯度投影到与预训练模型主空间和先前任务特征主导方向正交的子空间，有效平衡了保留预训练知识、保持已学任务知识和获取新知识三个目标，实现了最先进的性能。

Abstract: Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.

</details>


### [61] [A new Image Similarity Metric for a Perceptual and Transparent Geometric and Chromatic Assessment](https://arxiv.org/abs/2601.19680)
*Antonio Di Marino,Vincenzo Bevilacqua,Emanuel Di Nardo,Angelo Ciaramella,Ivanoe De Falco,Giovanna Sannino*

Main category: cs.CV

TL;DR: 提出了一种结合纹理和色度评估的新感知度量，优于现有技术，并提供视觉解释。


<details>
  <summary>Details</summary>
Motivation: 现有图像相似性度量在评估纹理失真时表现不佳，且缺乏解释性。

Method: 提出了一种新的感知度量，包含两个部分：使用Earth Mover's Distance评估纹理差异，以及在Oklab感知颜色空间中评估色度差异。

Result: 在Berkeley-Adobe Perceptual Patch Similarity数据集上的实验表明，新度量在形状失真情况下表现优于现有技术，并具有更高的感知性。

Conclusion: 该论文提出的新感知度量在评估图像相似性时，尤其是在处理形状和颜色失真时，优于现有技术。此外，该度量还提供了视觉解释，增强了评估的透明度和合理性。

Abstract: In the literature, several studies have shown that state-of-the-art image similarity metrics are not perceptual metrics; moreover, they have difficulty evaluating images, especially when texture distortion is also present. In this work, we propose a new perceptual metric composed of two terms. The first term evaluates the dissimilarity between the textures of two images using Earth Mover's Distance. The second term evaluates the chromatic dissimilarity between two images in the Oklab perceptual color space. We evaluated the performance of our metric on a non-traditional dataset, called Berkeley-Adobe Perceptual Patch Similarity, which contains a wide range of complex distortions in shapes and colors. We have shown that our metric outperforms the state of the art, especially when images contain shape distortions, confirming also its greater perceptiveness. Furthermore, although deep black-box metrics could be very accurate, they only provide similarity scores between two images, without explaining their main differences and similarities. Our metric, on the other hand, provides visual explanations to support the calculated score, making the similarity assessment transparent and justified.

</details>


### [62] [SharpNet: Enhancing MLPs to Represent Functions with Controlled Non-differentiability](https://arxiv.org/abs/2601.19683)
*Hanting Niu,Junkai Deng,Fei Hou,Wencheng Wang,Ying He*

Main category: cs.CV

TL;DR: SharpNet是一种改进的MLP架构，通过辅助特征函数解决传统MLP无法处理尖锐特征的问题，在2D和3D任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统MLP由于其全局平滑性，难以表示具有预设尖锐特征的连续但非可微函数，而无需依赖临时后处理。

Method: SharpNet通过将网络与辅助特征函数（定义为具有跳跃Neumann边界条件的Poisson方程的解）结合，实现了对尖锐特征的精确控制。该方法通过局部积分高效评估，并允许联合优化特征位置和MLP参数。

Result: 在2D问题和3D CAD模型重建任务中，SharpNet能准确恢复尖锐边缘和角落，同时保持其他区域的平滑性，优于现有方法。

Conclusion: SharpNet通过引入辅助特征函数，成功解决了传统MLP在处理具有预设尖锐特征（即C^0连续性）的函数时的局限性，同时在2D和3D任务中表现优于现有方法。

Abstract: Multi-layer perceptrons (MLPs) are a standard tool for learning and function approximation, but they inherently yield outputs that are globally smooth. As a result, they struggle to represent functions that are continuous yet deliberately non-differentiable (i.e., with prescribed $C^0$ sharp features) without relying on ad hoc post-processing. We present SharpNet, a modified MLP architecture capable of encoding functions with user-defined sharp features by enriching the network with an auxiliary feature function, which is defined as the solution to a Poisson equation with jump Neumann boundary conditions. It is evaluated via an efficient local integral that is fully differentiable with respect to the feature locations, enabling our method to jointly optimize both the feature locations and the MLP parameters to recover the target functions/models. The $C^0$-continuity of SharpNet is precisely controllable, ensuring $C^0$-continuity at the feature locations and smoothness elsewhere. We validate SharpNet on 2D problems and 3D CAD model reconstruction, and compare it against several state-of-the-art baselines. In both types of tasks, SharpNet accurately recovers sharp edges and corners while maintaining smooth behavior away from those features, whereas existing methods tend to smooth out gradient discontinuities. Both qualitative and quantitative evaluations highlight the benefits of our approach.

</details>


### [63] [Video-KTR: Reinforcing Video Reasoning via Key Token Attribution](https://arxiv.org/abs/2601.19686)
*Ziyue Wang,Sheng Jin,Zhongrong Zuo,Jiawei Wu,Han Qiu,Qi She,Hao Zhang,Xudong Jiang*

Main category: cs.CV

TL;DR: Video-KTR是一种多模态感知的策略框架，通过选择性强化关键标记提升视频推理的准确性和可解释性，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推理方法通常依赖粗粒度的序列级奖励或单因素标记选择，忽视了视觉输入、时间动态和语言输出之间的细粒度联系，限制了准确性和可解释性。

Method: 提出Video-KTR框架，通过视觉感知标记、时间感知标记和高熵标记三种信号进行选择性、标记级的强化学习。

Result: 在五个挑战性基准测试中，Video-KTR取得了最先进或极具竞争力的结果，如在Video-Holmes上达到42.7%（超过GPT-4o），并在推理和通用视频理解任务中表现一致提升。

Conclusion: Video-KTR通过结合视觉感知、时间敏感性和预测不确定性的多模态信号，实现了在视频推理任务中的高准确性和可解释性，为复杂视频推理提供了一种简单有效的强化学习扩展方法。

Abstract: Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.

</details>


### [64] [DSVM-UNet : Enhancing VM-UNet with Dual Self-distillation for Medical Image Segmentation](https://arxiv.org/abs/2601.19690)
*Renrong Shao,Dongyang Li,Dong Xia,Lin Shao,Jiangdong Lu,Fen Zheng,Lulu Zhang*

Main category: cs.CV

TL;DR: DSVM-UNet通过双自蒸馏方法简化设计，在医学图像分割任务中实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有Vision Mamba模型在医学图像分割中依赖复杂结构的问题，简化设计并提升性能。

Method: 提出双自蒸馏方法，分别在全局和局部层面进行特征对齐。

Result: 在ISIC2017、ISIC2018和Synapse基准测试中达到最先进性能，同时保持计算效率。

Conclusion: DSVM-UNet通过双自蒸馏方法在全局和局部特征对齐上取得了显著效果，无需复杂架构设计即可实现高效性能。

Abstract: Vision Mamba models have been extensively researched in various fields, which address the limitations of previous models by effectively managing long-range dependencies with a linear-time overhead. Several prospective studies have further designed Vision Mamba based on UNet(VM-UNet) for medical image segmentation. These approaches primarily focus on optimizing architectural designs by creating more complex structures to enhance the model's ability to perceive semantic features. In this paper, we propose a simple yet effective approach to improve the model by Dual Self-distillation for VM-UNet (DSVM-UNet) without any complex architectural designs. To achieve this goal, we develop double self-distillation methods to align the features at both the global and local levels. Extensive experiments conducted on the ISIC2017, ISIC2018, and Synapse benchmarks demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. Code is available at https://github.com/RoryShao/DSVM-UNet.git.

</details>


### [65] [Self-Supervised Weight Templates for Scalable Vision Model Initialization](https://arxiv.org/abs/2601.19694)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: SWEET是一种自监督框架，通过共享权重模板和尺寸特定权重缩放器，高效初始化可变尺寸视觉模型，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代模型参数规模和复杂性增加，传统预训练和微调方法在部署不同架构尺寸时存在局限性。

Method: 提出SWEET框架，采用基于Tucker分解的共享权重模板和尺寸特定权重缩放器，结合宽度随机缩放技术，实现模型的模块化和灵活适应。

Result: 在分类、检测、分割和生成任务中，SWEET展示了初始化可变尺寸视觉模型的最先进性能。

Conclusion: SWEET框架通过共享权重模板和尺寸特定权重缩放器的自监督预训练，实现了视觉任务中可变尺寸模型的高效初始化，并在多个任务中展示了最先进的性能。

Abstract: The increasing scale and complexity of modern model parameters underscore the importance of pre-trained models. However, deployment often demands architectures of varying sizes, exposing limitations of conventional pre-training and fine-tuning. To address this, we propose SWEET, a self-supervised framework that performs constraint-based pre-training to enable scalable initialization in vision tasks. Instead of pre-training a fixed-size model, we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. Target models are subsequently initialized by composing and reweighting the template through lightweight weight scalers, whose parameters can be efficiently learned from minimal training data. To further enhance flexibility in width expansion, we introduce width-wise stochastic scaling, which regularizes the template along width-related dimensions and encourages robust, width-invariant representations for improved cross-width generalization. Extensive experiments on \textsc{classification}, \textsc{detection}, \textsc{segmentation} and \textsc{generation} tasks demonstrate the state-of-the-art performance of SWEET for initializing variable-sized vision models.

</details>


### [66] [DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization](https://arxiv.org/abs/2601.19717)
*Yitong Yang,Xuexin Liu,Yinglin Wang,Jing Wang,Hao Dou,Changshuo Wang,Shuting He*

Main category: cs.CV

TL;DR: DiffStyle3D是一种新型扩散式3D风格迁移方法，通过潜在空间优化和几何引导的多视角一致性，显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有VGG和CLIP方法难以建模多视角一致性，而基于扩散的方法虽能捕捉一致性但依赖去噪方向导致训练不稳定。DiffStyle3D旨在解决这些局限性。

Method: 提出DiffStyle3D，一种基于扩散的3DGS风格迁移新范式，包括注意力感知损失（通过自注意力空间对齐风格特征并保留内容特征）和几何引导的多视角一致性方法（整合几何信息到自注意力中建模跨视角对应关系）。

Result: 大量实验表明，DiffStyle3D在风格化质量和视觉真实感上优于现有方法。

Conclusion: DiffStyle3D通过直接在潜在空间优化，结合注意力感知损失和几何引导的多视角一致性方法，显著提升了3D风格迁移的质量和视觉真实感，超越了现有技术。

Abstract: 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.

</details>


### [67] [WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration](https://arxiv.org/abs/2601.19753)
*Xinrui Zhang,Yufeng Wang,Shuangkang Fang,Zesheng Wang,Dacheng Qi,Wenrui Ding*

Main category: cs.CV

TL;DR: WaterClear-GS是首个纯3DGS框架，通过显式集成水下光学特性，实现高性能实时渲染，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有NeRF方法渲染速度慢、颜色恢复不理想，以及3DGS无法建模复杂体积散射效应的问题。

Method: 采用双分支优化策略，结合深度引导的几何正则化、感知驱动的图像损失、曝光约束、空间自适应正则化和物理引导的光谱正则化，确保局部3D一致性和自然视觉感知。

Result: 在标准基准和新收集的数据集上，WaterClear-GS在新视角合成（NVS）和水下图像恢复（UIR）任务中表现出色，并保持实时渲染。

Conclusion: WaterClear-GS通过将水下光学特性显式集成到高斯基元中，实现了实时渲染的高性能水下3D重建和外观恢复，无需辅助介质网络。

Abstract: Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.

</details>


### [68] [PaW-ViT: A Patch-based Warping Vision Transformer for Robust Ear Verification](https://arxiv.org/abs/2601.19771)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: PaW-ViT是一种基于解剖学知识的预处理方法，通过对齐token与耳朵特征边界，提升ViT在耳朵识别中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉Transformer中矩形token因包含识别对象外部信息而影响性能的问题，特别是在耳朵生物识别中形态变化与Transformer架构位置敏感性之间的不匹配。

Method: 提出了一种基于解剖学知识的预处理方法PaW-ViT，通过准确对齐token边界与检测到的耳朵特征边界，增强ViT的效能。

Result: 实验证实PaW-ViT在多种ViT模型（ViT-T、ViT-S、ViT-B、ViT-L）上有效，对形状、大小和姿态变化具有合理的对齐鲁棒性。

Conclusion: PaW-ViT通过将token边界与耳朵特征边界对齐，提高了ViT模型在形状、大小和姿态变化下的鲁棒性，为生物识别认证提供了一种可能的解决方案。

Abstract: The rectangular tokens common to vision transformer methods for visual recognition can strongly affect performance of these methods due to incorporation of information outside the objects to be recognized. This paper introduces PaW-ViT, Patch-based Warping Vision Transformer, a preprocessing approach rooted in anatomical knowledge that normalizes ear images to enhance the efficacy of ViT. By accurately aligning token boundaries to detected ear feature boundaries, PaW-ViT obtains greater robustness to shape, size, and pose variation. By aligning feature boundaries to natural ear curvature, it produces more consistent token representations for various morphologies. Experiments confirm the effectiveness of PaW-ViT on various ViT models (ViT-T, ViT-S, ViT-B, ViT-L) and yield reasonable alignment robustness to variation in shape, size, and pose. Our work aims to solve the disconnect between ear biometric morphological variation and transformer architecture positional sensitivity, presenting a possible avenue for authentication schemes.

</details>


### [69] [GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance](https://arxiv.org/abs/2601.19785)
*Haozhi Zhu,Miaomiao Zhao,Dingyao Liu,Runze Tian,Yan Zhang,Jie Guo,Fenggen Yu*

Main category: cs.CV

TL;DR: GeoDiff3D是一种自监督框架，通过几何约束和扩散模型高效生成高质量3D场景，减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结构建模和依赖大规模真实监督方面存在局限，导致结构伪影、几何不一致和复杂场景中高频细节退化。GeoDiff3D旨在解决这些问题。

Method: 使用粗几何作为结构锚点，结合几何约束的2D扩散模型生成参考图像，并引入体素对齐的3D特征聚合和双重自监督以保持场景连贯性和细节。

Result: 在复杂场景上的实验显示，GeoDiff3D在泛化能力和生成质量上优于现有基线。

Conclusion: GeoDiff3D提出了一种高效的自监督框架，通过粗几何作为结构锚点和几何约束的2D扩散模型提供纹理丰富的参考图像，显著减少了对标注数据的依赖，并在低计算成本下实现了快速、高质量的3D场景生成。

Abstract: 3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.

</details>


### [70] [Diffusion for De-Occlusion: Accessory-Aware Diffusion Inpainting for Robust Ear Biometric Recognition](https://arxiv.org/abs/2601.19795)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 扩散基础的耳部修复技术能有效解决耳饰遮挡问题，提升Transformer耳部识别系统的性能。


<details>
  <summary>Details</summary>
Motivation: 耳饰遮挡在无约束成像条件下会对基于耳部的生物识别系统性能产生负面影响，因此需要一种有效的预处理方法来缓解这一问题。

Method: 使用扩散基础的耳部修复技术，结合自动生成的耳饰遮挡掩码，重建干净且解剖学上合理的耳部区域，同时保持关键耳部结构的局部几何一致性。

Result: 实验表明，扩散基础的修复技术能够有效改善耳饰遮挡问题，提升多种视觉Transformer模型在不同基准数据集上的识别性能。

Conclusion: 扩散基础的耳部修复技术可以作为预处理辅助手段，有效缓解耳饰遮挡问题，提升基于Transformer的耳部识别系统性能。

Abstract: Ear occlusions (arising from the presence of ear accessories such as earrings and earphones) can negatively impact performance in ear-based biometric recognition systems, especially in unconstrained imaging circumstances. In this study, we assess the effectiveness of a diffusion-based ear inpainting technique as a pre-processing aid to mitigate the issues of ear accessory occlusions in transformer-based ear recognition systems. Given an input ear image and an automatically derived accessory mask, the inpainting model reconstructs clean and anatomically plausible ear regions by synthesizing missing pixels while preserving local geometric coherence along key ear structures, including the helix, antihelix, concha, and lobule. We evaluate the effectiveness of this pre-processing aid in transformer-based recognition systems for several vision transformer models and different patch sizes for a range of benchmark datasets. Experiments show that diffusion-based inpainting can be a useful pre-processing aid to alleviate ear accessory occlusions to improve overall recognition performance.

</details>


### [71] [Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision](https://arxiv.org/abs/2601.19798)
*Zhixiang Wei,Yi Li,Zhehan Kan,Xinghua Jiang,Zuwei Long,Shifeng Liu,Hongze Shen,Wei Liu,Xiaoyu Tan,Haojia Lin,Yubo Zhu,Qianyu Li,Di Yin,Haoyu Cao,Weibo Gu,Xin Li,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Mingkong Tang,Shuangyin Liu,Lexiang Tang,Haodong Lin,Junru Lu,Jiarui Qin,Lingfeng Qiao,Ruizhi Qiao,Bo Ke,Jianfeng He,Ke Li,Yangning Li,Yunhang Shen,Mengdan Zhang,Peixian Chen,Kun Yin,Bing Liu,Yunfei Wu,Huang Chen,Zhongpeng Cai,Xiaotian Li*

Main category: cs.CV

TL;DR: Youtu-VL通过VLUAS范式改进视觉语言模型，将视觉信号作为监督目标，提升了多模态任务的细粒度理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）因训练范式的文本主导优化偏差，导致视觉信息保留不足，多模态理解粗粒度。

Method: 引入Vision-Language Unified Autoregressive Supervision (VLUAS)范式，将视觉令牌直接整合到预测流中，实现视觉细节和语言内容的统一自回归监督。

Result: Youtu-VL在通用多模态任务和视觉中心任务中均取得竞争性表现。

Conclusion: Youtu-VL通过VLUAS范式，将视觉信号从被动输入转变为监督目标，显著提升了多模态理解的细粒度，并在通用多模态任务和视觉中心任务中表现出色。

Abstract: Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.

</details>


### [72] [Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering](https://arxiv.org/abs/2601.19821)
*Kun Li,Michael Ying Yang,Sami Sebastian Brandt*

Main category: cs.CV

TL;DR: 本文提出了一种新的查询引导的时空频交互方法（QSTar）和查询上下文推理模块（QCR），通过整合问题引导线索和音频频域特性，显著提升了AVQA任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的AVQA方法主要关注视觉信息处理，音频输入被视为视频分析的补充，文本问题信息在音频-视觉理解中的贡献有限。为了克服这些限制，需要更有效地整合问题引导线索和音频信号的频域特性。

Method: 提出了一种新颖的查询引导的时空频（QSTar）交互方法，结合问题引导线索和音频信号的频域特性，以及空间和时间感知，以增强音频-视觉理解。此外，引入了受提示启发的查询上下文推理（QCR）模块，指导模型更精确地关注语义相关的音频和视觉特征。

Result: 在多个AVQA基准测试上的广泛实验表明，所提出的方法显著优于现有的音频QA、视觉QA、视频QA和AVQA方法。

Conclusion: 提出的QSTar方法和QCR模块显著提升了AVQA任务的性能，超越了现有的音频QA、视觉QA、视频QA和AVQA方法。

Abstract: Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.

</details>


### [73] [HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation](https://arxiv.org/abs/2601.19849)
*Haya Alyoussef,Ahmad Bdeir,Diego Coello de Portugal Mecke,Tom Hanika,Niels Landwehr,Lars Schmidt-Thieme*

Main category: cs.CV

TL;DR: HexFormer是一种双曲视觉Transformer，通过指数映射聚合提升图像分类性能，混合变体表现最佳，且双曲模型训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 跨模态数据（如图像、文本和图）常包含层次和关系结构，这些结构在欧几里得几何中难以建模。双曲几何为表示此类结构提供了自然框架。

Method: 提出了HexFormer，一种用于图像分类的双曲视觉Transformer，在其注意力机制中融入了指数映射聚合。探索了两种设计：纯双曲ViT（HexFormer）和混合变体（HexFormer-Hybrid），后者结合了双曲编码器和欧几里得线性分类头。

Result: 在多数据集上的实验表明，HexFormer性能优于欧几里得基线和先前的双曲ViT，混合变体表现最佳。双曲模型还表现出更稳定的梯度和对预热策略的较低敏感性。

Conclusion: 双曲几何可以增强视觉Transformer架构，通过提高梯度稳定性和准确性。相对简单的机制（如指数映射聚合）也能带来显著的实践优势。

Abstract: Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.

</details>


### [74] [EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning](https://arxiv.org/abs/2601.19850)
*Binzhu Xie,Shi Qiu,Sicheng Zhang,Yinqiao Wang,Hao Xu,Muzammal Naseer,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: EgoHandICL是首个基于上下文学习的3D手部重建框架，通过视觉语言模型和MAE架构提升性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视角下3D手部重建的深度模糊、自遮挡和复杂手物交互问题，现有方法在未知场景中表现不佳。

Method: 提出了EgoHandICL框架，包括视觉语言模型引导的互补示例检索、ICL定制分词器和基于MAE的架构，结合手部引导的几何与感知目标进行训练。

Result: 在ARCTIC和EgoExo4D数据集上表现优于现有方法，并展示了在真实场景中的泛化能力，同时提升了EgoVLM的手物交互推理能力。

Conclusion: EgoHandICL 通过结合视觉语言模型、ICL定制分词器和基于MAE的架构，显著提升了3D手部重建在复杂第一人称视角下的性能，并在多个数据集上优于现有方法。

Abstract: Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL

</details>


### [75] [SONIC: Spectral Oriented Neural Invariant Convolutions](https://arxiv.org/abs/2601.19884)
*Gijs Joppe Moens,Regina Beets-Tan,Eduardo H. P. Pooch*

Main category: cs.CV

TL;DR: SONIC通过连续光谱参数化方法，结合CNN和ViT优势，实现了全局感受野和跨分辨率自然适应的滤波器，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决CNN固定大小核的限制和ViT缺乏空间归纳偏差的问题，需要一种既结构化又全局的表示。

Method: 引入SONIC（Spectral Oriented Neural Invariant Convolutions），一种连续光谱参数化方法，使用少量共享的、方向选择性组件来建模卷积算子。

Result: 在合成基准、大规模图像分类和3D医学数据集上，SONIC表现出对几何变换、噪声和分辨率变化的更强鲁棒性，且参数数量少一个数量级。

Conclusion: 连续、方向感知的光谱参数化为传统的空间和光谱算子提供了一种原则性和可扩展的替代方案。

Abstract: Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.

</details>


### [76] [DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding](https://arxiv.org/abs/2601.19898)
*Shubham Patle,Sara Ghaboura,Hania Tariq,Mohammad Usman Khan,Omkar Thawakar,Rao Muhammad Anwer,Salman Khan*

Main category: cs.CV

TL;DR: DuwatBench是一个针对阿拉伯书法多模态处理的基准数据集，评估显示现有模型在艺术化文本处理上存在挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多模态模型在处理阿拉伯脚本，尤其是艺术化和风格化书法形式方面的不足。

Method: 提出了DuwatBench基准，包含1,272个精选样本，涵盖六种古典与现代书法风格，每个样本配有句子级检测标注。评估了13种领先的阿拉伯及多语言多模态模型。

Result: 尽管这些模型在干净文本上表现良好，但在书法变体、艺术扭曲和精确的视觉-文本对齐方面表现不佳。

Conclusion: 通过公开DuwatBench数据集及其标注，旨在推动文化基础的多模态研究，促进阿拉伯语言及视觉遗产在AI系统中的公平包容，并支持该领域的持续进展。

Abstract: Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [77] [Intent2QoS: Language Model-Driven Automation of Traffic Shaping Configurations](https://arxiv.org/abs/2601.18974)
*Sudipta Acharya,Burak Kantarci*

Main category: cs.NI

TL;DR: 自动化框架将自然语言流量整形意图转化为Linux流量控制配置，LLaMA3表现最佳，AQM引导提示显著减少变异性。


<details>
  <summary>Details</summary>
Motivation: 传统流量整形和QoS依赖低层手动设置，技术门槛高。本文旨在通过自动化框架简化这一过程，将自然语言或声明式语言意图转化为可部署配置。

Method: 框架分为三步：(1) 基于排队理论和优先级调度的队列模拟构建语义模型；(2) 语言模型生成子意图和配置规则；(3) 基于规则的批评器检查和调整规则。

Result: 实验表明，LLaMA3在100个意图测试中达到0.88语义相似性和0.87语义覆盖率，优于其他模型30%以上。AQM引导提示将变异性降低三倍。

Conclusion: 该论文提出的自动化框架成功地将高级流量整形意图转化为可部署的Linux流量控制配置，显著提高了语义相似性和覆盖率，并通过AQM引导提示减少了变异性。

Abstract: Traffic shaping and Quality of Service (QoS) enforcement are critical for managing bandwidth, latency, and fairness in networks. These tasks often rely on low-level traffic control settings, which require manual setup and technical expertise. This paper presents an automated framework that converts high-level traffic shaping intents in natural or declarative language into valid and correct traffic control rules. To the best of our knowledge, we present the first end-to-end pipeline that ties intent translation in a queuing-theoretic semantic model and, with a rule-based critic, yields deployable Linux traffic control configuration sets. The framework has three steps: (1) a queuing simulation with priority scheduling and Active Queue Management (AQM) builds a semantic model; (2) a language model, using this semantic model and a traffic profile, generates sub-intents and configuration rules; and (3) a rule-based critic checks and adjusts the rules for correctness and policy compliance. We evaluate multiple language models by generating traffic control commands from business intents that comply with relevant standards for traffic control protocols. Experimental results on 100 intents show significant gains, with LLaMA3 reaching 0.88 semantic similarity and 0.87 semantic coverage, outperforming other models by over 30\. A thorough sensitivity study demonstrates that AQM-guided prompting reduces variability threefold compared to zero-shot baselines.

</details>


### [78] [Optimizing Network Topology Efficiency: A Resource-Centric Analysis of Non-Blocking Architectures](https://arxiv.org/abs/2601.19008)
*Jia Xu Wei,Wei Wei*

Main category: cs.NI

TL;DR: 本文重新定义网络效率为硬件成本，通过建模发现高基数直接网络适合中小规模，大规模需间接网络，冗余处理推荐并行网络实例。


<details>
  <summary>Details</summary>
Motivation: 现代网络设计中，‘效率’常与原始性能指标（如延迟或总吞吐量）混淆，本文提出以硬件成本为核心的效率定义，以维持非阻塞吞吐量约束。

Method: 通过将网络成本建模为流量乘数（跳数）和路由器复杂度（基数）的函数，分析链路接口成本（$α$）、交叉开关成本（$β$）与网络集中比率的技术比例，确定最优拓扑结构。

Result: 研究表明，高基数直接网络在小到中等规模下优化效率，而大规模网络需要间接网络来限制路由器复杂度。

Conclusion: 在高规模网络中，间接网络（如Fat Trees）能有效限制路由器复杂度，而冗余处理最有效的方式是通过并行网络实例（如多平面星型网络）而非内在拓扑路径多样性。

Abstract: In modern network design, "efficiency" is often conflated with raw performance metrics like latency or aggregate throughput. This paper proposes a resource-centric definition of efficiency, isolating the hardware cost required to maintain a non-blocking throughput constraint. By modeling network cost as a function of the Traffic Multiplier (Hop Count) and Router Complexity (Radix), we demonstrate that the optimal topology is determined by the technological ratio between link interface costs ($α$), crossbar switching costs ($β$), and the network concentration ratio. We conclude that while high-radix direct networks optimize efficiency at small to medium scales, indirect networks (e.g., Fat Trees) are required to cap router complexity at massive scales. Furthermore, we posit that redundancy is most efficiently handled via parallel network instances (e.g., multi-plane Star networks) rather than intrinsic topological path diversity.

</details>


### [79] [Design and Evaluation of Next-Generation Cellular Networks through Digital and Physical Open and Programmable Platforms](https://arxiv.org/abs/2601.19027)
*Davide Villa*

Main category: cs.NI

TL;DR: 论文探讨了5G/6G中Open RAN的挑战与解决方案，开发了Colosseum和X5G实验平台，验证了数字孪生技术，并实现了智能RAN应用。


<details>
  <summary>Details</summary>
Motivation: 5G和6G技术中无线接入网络（RAN）的演进推动了开放、可编程和软件化架构的发展，但这一转变带来了设计互操作解决方案、获取数据集以训练和测试AI/ML算法等挑战。

Method: 开发并评估了互补的实验平台：Colosseum（全球最大的Open RAN数字孪生）和X5G（一个开放、可编程、多供应商的私有5G O-RAN测试床，带有GPU加速）。

Result: 主要贡献包括：CaST（通过3D建模、光线追踪和信道探测实现数字孪生无线场景的自动化创建和验证）、Colosseum数字孪生的大规模验证、X5G的集成、GPU加速的dApp框架以及智能RAN应用的开发。

Conclusion: 本论文提供了一种端到端的方法论，连接数字和物理实验，为下一代蜂窝网络的发展提供了支持。

Abstract: The evolution of the Radio Access Network (RAN) in 5G and 6G technologies marks a shift toward open, programmable, and softwarized architectures, driven by the Open RAN paradigm. This approach emphasizes open interfaces for telemetry sharing, intelligent data-driven control loops for network optimization, and virtualization and disaggregation of multi-vendor RAN components. While promising, this transition introduces significant challenges, including the need to design interoperable solutions, acquire datasets to train and test AI/ML algorithms for inference and control, and develop testbeds to benchmark these solutions. Experimental wireless platforms and private 5G deployments play a key role, providing architectures comparable to real-world systems and enabling prototyping and testing in realistic environments. This dissertation focuses on the development and evaluation of complementary experimental platforms: Colosseum, the world's largest Open RAN digital twin, and X5G, an open, programmable, multi-vendor private 5G O-RAN testbed with GPU acceleration. The main contributions include: (i) CaST, enabling automated creation and validation of digital twin wireless scenarios through 3D modeling, ray-tracing, and channel sounding; (ii) validation of Colosseum digital twins at scale, demonstrating that emulated environments closely reproduce real-world setups; (iii) X5G, integrating NVIDIA Aerial GPU-accelerated PHY processing with OpenAirInterface higher layers; (iv) a GPU-accelerated dApp framework for real-time RAN inference, enabling sub-millisecond control loops for AI-native applications including ISAC; and (v) intelligent RAN applications spanning spectrum sharing, interference detection, network slicing, security, and CSI-based sensing. Overall, this dissertation provides an end-to-end methodology bridging digital and physical experimentation for next-generation cellular networks.

</details>


### [80] [FTA-NTN: Fairness and Throughput Assurance in Non-Terrestrial Networks](https://arxiv.org/abs/2601.19078)
*Sachin Ravikant Trankatwar,Heiko Straulino,Petar Djukic,Burak Kantarci*

Main category: cs.NI

TL;DR: FTA-NTN是一個多目標優化框架，旨在同時最大化非地面網絡的吞吐量和公平性，通過整合先進技術實現了高效且公平的全球連接。


<details>
  <summary>Details</summary>
Motivation: 設計最佳的非地面網絡（NTN）星座對於最大化吞吐量和確保公平資源分配至關重要。

Method: 該框架整合了多層Walker Delta星座、加拿大陸地區域的參數化移動模型、自適應K-Means聚類用於波束成形和用戶關聯，以及貝葉斯優化用於參數調整。

Result: 模擬結果顯示，FTA-NTN在500用戶的情況下實現了超過9.88 Gbps的總吞吐量，平均公平性為0.42，對應於LEO中9個平面每平面15顆衛星和MEO中7個平面每平面3顆衛星的最佳配置。

Conclusion: FTA-NTN展示了在實際約束下可以同時優化吞吐量和公平性，超越了文獻中以吞吐量為中心的設計，並為下一代NTN部署提供了可擴展的方法論，支持高效且公平的全球連接。

Abstract: Designing optimal non-terrestrial network (NTN) constellations is essential for maximizing throughput and ensuring fair resource distribution. This paper presents FTA-NTN (Fairness and Throughput Assurance in Non-Terrestrial Networks), a multi-objective optimization framework that jointly maximizes throughput and fairness under realistic system constraints. The framework integrates multi-layer Walker Delta constellations, a parametric mobility model for user distributions across Canadian land regions, adaptive K-Means clustering for beamforming and user association, and Bayesian optimization for parameter tuning. Simulation results with 500 users show that FTA-NTN achieves over 9.88 Gbps of aggregate throughput with an average fairness of 0.42, corresponding to an optimal configuration of 9 planes with 15 satellites per plane in LEO and 7 planes with 3 satellites per plane in MEO. These values align with 3GPP NTN evaluation scenarios and representative system assumptions, confirming their relevance for realistic deployments. Overall, FTA-NTN demonstrates that throughput and fairness can be jointly optimized under practical constraints, advancing beyond throughput-centric designs in the literature and offering a scalable methodology for next-generation NTN deployments that supports efficient and equitable global connectivity.

</details>


### [81] [In-Network Collective Operations: Game Changer or Challenge for AI Workloads?](https://arxiv.org/abs/2601.19132)
*Torsten Hoefler,Mikhail Khalilov,Josiah Clark,Surendra Anubolu,Mohan Kalkunte,Karen Schramm,Eric Spada,Duncan Roweth,Keith Underwood,Adrian Caulfield,Abdul Kabbani,Amirreza Rastegari*

Main category: cs.NI

TL;DR: 本文探讨了网络内集体操作（INC）在AI中的加速潜力，分析了Edge-INC和Core-INC的性能优势与障碍，并预测了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 促进AI和网络领域非专家之间的理解与连接，推动INC技术的普及与应用。

Method: 通过分析两种INC类型（Edge-INC和Core-INC），概述了潜在的性能优势及六大关键障碍。

Result: 详细阐述了INC的性能优势及可能阻碍其采用的障碍，并提供了未来发展的预测。

Conclusion: 本文总结了网络内集体操作（INC）在AI工作负载中的加速潜力，并提出了对未来INC发展和应用的一系列预测。

Abstract: This paper summarizes the opportunities of in-network collective operations (INC) for accelerated collective operations in AI workloads. We provide sufficient detail to make this important field accessible to non-experts in AI or networking, fostering a connection between these communities. Consider two types of INC: Edge-INC, where the system is implemented at the node level, and Core-INC, where the system is embedded within network switches. We outline the potential performance benefits as well as six key obstacles in the context of both Edge-INC and Core-INC that may hinder their adoption. Finally, we present a set of predictions for the future development and application of INC.

</details>


### [82] [Enabling SLO-Aware 5G Multi-Access Edge Computing with SMEC](https://arxiv.org/abs/2601.19162)
*Xiao Zhang,Daehyeok Kim*

Main category: cs.NI

TL;DR: SMEC通过解耦RAN和边缘服务器调度，利用5G协议信息显著提升MEC的SLO满足率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 商业MEC部署中频繁的SLO违规和高尾部延迟问题，主要由RAN和边缘服务器的资源争用及SLO无关调度器引起。

Method: 利用标准5G协议和应用行为提供的信息，设计了一种无需大规模基础设施或应用变更的SLO感知管理框架。

Result: 在5G MEC测试平台上，SMEC实现了90-96%的SLO满足率，相比现有方法的不足6%，尾部延迟降低了122倍。

Conclusion: SMEC是一种实用的、SLO感知的资源管理框架，通过完全解耦的RAN和边缘服务器操作实现基于截止时间的调度，显著提高了SLO满足率并降低了尾部延迟。

Abstract: Multi-access edge computing (MEC) promises to enable latency-critical applications by bringing computational power closer to mobile devices, but our measurements on commercial MEC deployments reveal frequent SLO violations due to high tail latencies. We identify resource contention at the RAN and the edge server as the root cause, compounded by SLO-unaware schedulers. Existing SLO-aware approaches require RAN--edge coordination, making them impractical for deployment and prone to poor performance due to coordination delays, limited heterogeneous application support, and ignoring edge resource contention. This paper introduces SMEC, a practical, SLO-aware resource management framework that facilitates deadline-aware scheduling through fully decoupled operations at the RAN and edge servers. Our key insight is that standard 5G protocols and application behaviors naturally provide information exploitable for SLO-aware management without extensive infrastructure or application changes. Evaluation on our 5G MEC testbed shows that SMEC achieves 90-96% SLO satisfaction versus under 6% for existing approaches, while reducing tail latency by up to 122$\times$. We have open-sourced SMEC at https://github.com/smec-project.

</details>


### [83] [Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction](https://arxiv.org/abs/2601.19216)
*Chaozheng Wen,Jingwen Tong,Zehong Lin,Chenghong Bian,Jun Zhang*

Main category: cs.NI

TL;DR: URF-GS是一种统一无线电-光学辐射场表示框架，通过融合视觉和无线传感观测，显著提升了3D无线电地图的构建精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将光学和无线知识视为独立模态，未能利用光和电磁传播的基本物理原理。

Method: 基于3D高斯散射（3D-GS）和逆渲染的统一无线电-光学辐射场表示框架URF-GS，融合视觉和无线传感观测。

Result: URF-GS在空间频谱预测精度上比基于NeRF的方法提高了24.7%，在3D无线电地图构建的样本效率上提高了10倍。

Conclusion: URF-GS 为下一代无线网络奠定了基础，通过整体辐射场重建整合感知、交互和通信。

Abstract: The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence. 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation. However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation. Existing approaches typically treat optical and wireless knowledge as distinct modalities, failing to exploit the fundamental physical principles governing both light and electromagnetic propagation. To bridge this gap, we propose URF-GS, a unified radio-optical radiation field representation framework for accurate and generalizable 3D radio map construction based on 3D Gaussian splatting (3D-GS) and inverse rendering. By fusing visual and wireless sensing observations, URF-GS recovers scene geometry and material properties while accurately predicting radio signal behavior at arbitrary transmitter-receiver (Tx-Rx) configurations. Experimental results demonstrate that URF-GS achieves up to a 24.7% improvement in spatial spectrum prediction accuracy and a 10x increase in sample efficiency for 3D radio map construction compared with neural radiance field (NeRF)-based methods. This work establishes a foundation for next-generation wireless networks by integrating perception, interaction, and communication through holistic radiation field reconstruction.

</details>


### [84] [NET4EXA: Pioneering the Future of Interconnects for Supercomputing and AI](https://arxiv.org/abs/2601.19413)
*Michele Martinelli,Roberto Ammendola,Andrea Biagioni,Carlotta Chiarini,Ottorino Frezza,Francesca Lo Cicero,Alessandro Lonardo,Pier Stanislao Paolucci,Elena Pastorelli,Pierpaolo Perticaroli,Luca Pontisso,Cristian Rossi,Francesco Simula,Piero Vicini,David Colin,Grégoire Pichon,Alexandre Louvet,John Gliksberg,Claire Chen,Matteo Turisini,Andrea Monterubbiano,Jean-Philippe Nominé,Denis Dutoit,Hugo Taboada,Lilia Zaourar,Mohamed Benazouz,Angelos Bilas,Fabien Chaix,Manolis Katevenis,Nikolaos Chrysos,Evangelos Mageiropoulos,Christos Kozanitis,Thomas Moen,Steffen Persvold,Einar Rustad,Sandro Fiore,Fabrizio Granelli,Simone Pezzuto,Raffaello Potestio,Luca Tubiana,Philippe Velha,Flavio Vella,Daniele De Sensi,Salvatore Pontarelli*

Main category: cs.NI

TL;DR: NET4EXA开发下一代高性能互连BXIv3，结合商用与定制技术，满足HPC和AI需求，为未来BXIv4做准备。


<details>
  <summary>Details</summary>
Motivation: 满足HPC和AI系统对高性能互连日益增长的需求，特别是大规模基础设施（如训练大型语言模型）的需求。

Method: 采用混合开发和协同设计方法，结合商用交换机技术与定制IP及基于FPGA的网卡。

Result: 项目将集成一个功能齐全的TRL 8试点系统，并通过广泛的基准测试、科学可扩展应用和AI工作负载评估性能。

Conclusion: NET4EXA项目旨在开发下一代高性能互连技术BXIv3，为2025年及以后的百亿亿次及后百亿亿次系统部署做好准备，并为未来的BXIv4奠定基础。

Abstract: NET4EXA aims to develop a next-generation high-performance interconnect for HPC and AI systems, addressing the increasing demands of large-scale infrastructures, such as those required for training Large Language Models. Building upon the proven BXI (Bull eXascale Interconnect) European technology used in TOP15 supercomputers, NET4EXA will deliver the new BXI release, BXIv3, a complete hardware and software interconnect solution, including switch and network interface components. The project will integrate a fully functional pilot system at TRL 8, ready for deployment into upcoming exascale and post-exascale systems from 2025 onward. Leveraging prior research from European initiatives like RED-SEA, the previous achievements of consortium partners and over 20 years of expertise from BULL, NET4EXA also lays the groundwork for the future generation of BXI, BXIv4, providing analysis and preliminary design. The project will use a hybrid development and co-design approach, combining commercial switch technology with custom IP and FPGA-based NICs. Performances of NET4EXA BXIv3 interconnect will be evaluated using a broad portfolio of benchmarks, scientific scalable applications, and AI workloads.

</details>


### [85] [Quantum Takes Flight: Two-Stage Resilient Topology Optimization for UAV Networks](https://arxiv.org/abs/2601.19724)
*Huixiang Zhang,Mahzabeen Emu,Octavia A. Dobre*

Main category: cs.NI

TL;DR: 论文提出量子辅助的两阶段框架，提升动态无人机网络拓扑控制的效率和弹性，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 下一代无人机通信网络需要在快速拓扑变化、链路质量波动和时间关键数据交换下保持可靠连接，现有方法在动态环境中适应性不足。

Method: 论文提出了一个两阶段量子辅助框架：离线阶段通过量子退火并行生成多个高质量且结构多样的候选拓扑；在线阶段使用轻量级经典选择机制实时选择最合适的拓扑。

Result: 模拟结果显示，相比单一静态最优拓扑，该框架在30秒动态窗口内性能保持提升6.6%；量子退火相比经典方法进一步降低目标值5.15%，并增加解决方案多样性28.3%。

Conclusion: 该论文展示了量子辅助框架在动态无人机通信网络中实现高效和弹性拓扑控制的潜力，显著提升了性能保持和解决方案多样性。

Abstract: Next-generation Unmanned Aerial Vehicle (UAV) communication networks must maintain reliable connectivity under rapid topology changes, fluctuating link quality, and time-critical data exchange. Existing topology control methods rely on global optimization to produce a single optimal topology or involve high computational complexity, which limits adaptability in dynamic environments. This paper presents a two-stage quantum-assisted framework for efficient and resilient topology control in dynamic UAV networks by exploiting quantum parallelism to generate a set of high-quality and structurally diverse candidate topologies. In the offline stage, we formulate the problem as a Quadratic Unconstrained Binary Optimization (QUBO) model and leverage quantum annealing (QA) to parallelly sample multiple high-quality and structurally distinct topologies, providing a rich solution space for adaptive decision-making. In the online stage, a lightweight classical selection mechanism rapidly identifies the most suitable topology based on real-time link stability and channel conditions, substantially reducing the computation delay. The simulation results show that, compared to a single static optimal topology, the proposed framework improves performance retention by 6.6% in a 30-second dynamic window. Moreover, relative to the classic method, QA achieves an additional 5.15% reduction in objective value and a 28.3% increase in solution diversity. These findings demonstrate the potential of QA to enable fast and robust topology control for next-generation UAV communication networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Agentic Business Process Management Systems](https://arxiv.org/abs/2601.18833)
*Marlon Dumas,Fredrik Milani,David Chapela-Campa*

Main category: cs.AI

TL;DR: 本文探讨了AI驱动的BPM新浪潮，提出了整合自主性、推理和学习的A-BPMS架构愿景，重新定义流程自动化和治理。


<details>
  <summary>Details</summary>
Motivation: 随着生成式和代理式人工智能的兴起，BPM领域正经历从自动化到自主性、从设计驱动到数据驱动的转变，本文旨在探索这一新浪潮的可能性。

Method: 基于2025年AI for BPM研讨会上的主题演讲，本文探讨了流程挖掘如何为代理感知流程状态、推理改进机会和优化性能奠定基础。

Result: 本文提出了A-BPMS的架构愿景，支持从人工驱动到完全自主的连续流程，为流程管理带来新的可能性。

Conclusion: 本文提出了Agentic Business Process Management Systems (A-BPMS)的架构愿景，这是一种整合了自主性、推理和学习能力的新型平台，旨在重新定义流程自动化和治理的边界。

Abstract: Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.

</details>


### [87] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: 研究利用LLaMEA框架和LLM生成具有特定景观特征的优化问题，通过ELA-based评分和多样性机制验证其有效性，丰富了BBOB测试套件的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有测试套件（如BBOB）在连续黑盒优化基准测试中因结构多样性有限而受到阻碍，研究旨在探索是否可以通过大型语言模型在进化循环中设计具有明确高层景观特征的优化问题。

Method: 研究采用LLaMEA框架，通过进化循环中的大型语言模型（LLM）从自然语言描述生成问题代码，并结合ELA-based属性预测器对候选问题进行评分。引入ELA-space fitness-sharing机制增加种群多样性，避免冗余景观。通过盆地吸引分析、统计测试和视觉检查验证生成函数的结构特性。

Result: 生成的函数确实表现出预期的结构特征，t-SNE嵌入显示它们扩展了BBOB实例空间而非形成无关集群。

Conclusion: 该研究通过LLaMEA框架成功生成了具有明确高层景观特征的优化问题，丰富了BBOB测试套件的结构多样性，为景观分析和自动化算法选择等下游任务提供了广泛、可解释且可复现的基准问题库。

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [88] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: IT2-ANFIS框架通过模糊规则结构提供可解释的预测区间，支持污水处理厂的能源预测和风险感知决策。


<details>
  <summary>Details</summary>
Motivation: 污水处理厂消耗全球1-3%的电力，准确的能源预测对运营优化和可持续性至关重要。现有机器学习模型缺乏可解释的不确定性量化，而这对安全关键基础设施的风险感知决策至关重要。

Method: 本研究开发了一种Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS)，通过模糊规则结构生成可解释的预测区间，并在三个层次上分解不确定性：特征级、规则级和实例级。

Result: 在Melbourne Water的Eastern Treatment Plant数据集上验证，IT2-ANFIS的预测性能与一阶ANFIS相当，且训练运行的方差显著降低，同时提供了可解释的不确定性估计。

Conclusion: 本研究开发的IT2-ANFIS框架不仅提供了与一阶ANFIS相当的预测性能，还通过模糊规则结构生成了可解释的预测区间，为安全关键基础设施中的风险感知决策提供了支持。

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [89] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: 研究发现LLMs在非顺序指令遵循上表现显著下降，揭示了其对位置连续性的强依赖性及结构敏感性的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在指令遵循上的能力，尤其是结构顺序对性能的影响，现有基准测试混淆了任务复杂性与结构顺序。

Method: 引入了RIFT（重新排序指令遵循测试平台），通过解构结构与内容，使用重新表述的Jeopardy!问答对，测试LLMs在两种提示结构（线性提示和跳跃提示）下的表现。

Result: 在10,000次评估中，跳跃条件下的准确率下降高达72%，50%的失败源于指令顺序违反和语义漂移。

Conclusion: 当前大型语言模型（LLMs）在非顺序指令遵循上表现不佳，揭示了其在结构敏感性上的根本局限性，对需要非顺序控制流的应用（如工作流自动化和多代理系统）有直接影响。

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [90] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: NTP4VC是首个针对验证条件证明的多语言基准测试，评估显示LLMs在此任务中具有潜力，但程序验证仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 程序验证中验证条件（VCs）的自动证明是主要瓶颈，现有自动定理证明器（ATPs）难以处理，亟需探索机器学习方法在此领域的应用。

Method: 引入NTP4VC，首个针对验证条件证明的多语言基准测试，利用Why3和Frama-C等工业管道从Linux和Contiki-OS内核等实际项目中生成语义等效的测试用例。

Result: 评估了通用及针对定理证明微调的大型语言模型（LLMs），结果显示LLMs在VC证明中具有潜力，但仍存在显著挑战。

Conclusion: 尽管大型语言模型（LLMs）在验证条件（VC）证明中显示出潜力，但程序验证仍面临重大挑战，为未来研究提供了广阔的空间。

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [91] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: 研究LLMs在重复社会困境中的战略行为，发现其行为受激励和语言环境影响，为AI治理提供新见解。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在交互和多代理环境中越来越多地作为自主代理，理解其战略行为对安全、协调及AI驱动的社会和经济学系统至关重要。

Method: 使用经过报酬调整的囚徒困境来隔离对激励强度的敏感性，训练监督分类器分析LLM决策中的行为意图。

Result: 观察到跨模型和语言的持续行为模式，包括对激励敏感的条策略和跨语言差异，语言框架有时与架构效应相当或超过。

Conclusion: 该研究为审计LLMs作为战略代理提供了一个统一框架，并突出了合作偏见对AI治理和多代理系统设计的直接影响。

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [92] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk通过先验蒸馏和不确定性感知多视图融合，解决了情感对齐和渲染质量问题，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D情感说话人脸合成方法在音频-视觉情感对齐和多视图融合策略上存在不足，导致情感表达不精确和渲染质量下降。

Method: 提出UA-3DTalk框架，包含先验提取模块、情感蒸馏模块和基于不确定性的变形模块，分别处理音频-视觉情感对齐、精细情感控制和多视图融合。

Result: 在常规和情感数据集上，UA-3DTalk在情感对齐（E-FID提升5.2%）、唇同步（SyncC提升3.1%）和渲染质量（LPIPS提升0.015）上均优于现有方法。

Conclusion: UA-3DTalk通过先验提取、情感蒸馏和基于不确定性的变形三个核心模块，显著提升了情感对齐、唇同步和渲染质量，优于现有方法。

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [93] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出对抗性数据增强方法，通过强化学习生成对抗查询，提升LLMs函数调用的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动标注或自动生成的数据，缺乏针对性设计，且受限于固定模式和数据分布，限制了函数调用LLMs的泛化性和鲁棒性。

Method: 提出了一种新颖的对抗性数据增强方法，利用强化学习训练查询模型生成针对性的对抗查询，通过零和博弈框架进行迭代交替训练。

Result: 该方法有效提升了函数调用模型的性能，并系统性识别和纠正了LLMs与外部工具交互的弱点。

Conclusion: 该方法通过对抗性数据增强和强化学习，显著提升了LLMs在函数调用任务中的泛化性和鲁棒性，为识别和纠正模型弱点提供了系统性解决方案。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [94] [Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction](https://arxiv.org/abs/2601.19142)
*Zhicheng Zhang,Zhaocheng Du,Jieming Zhu,Jiwei Tang,Fengyuan Lu,Wang Jiaheng,Song-Li Wu,Qianhui Zhu,Jingyu Li,Hai-Tao Zheng,Zhenhua Dong*

Main category: cs.AI

TL;DR: LAIN是一种长度自适应兴趣网络，通过将序列长度作为条件信号，平衡长短序列建模，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统中的用户行为序列长度差异显著，现有CTR模型在处理长序列时因注意力极化和训练数据长度不平衡而性能下降。

Method: LAIN由三个轻量级组件组成：光谱长度编码器、长度条件提示和长度调制注意力，这些组件共同平衡长短序列建模。

Result: 在三个真实世界基准测试中，LAIN在五个强大的CTR骨干上一致提升了整体性能，最高实现了1.15%的AUC增益和2.25%的对数损失减少。

Conclusion: LAIN提供了一种通用、高效且可部署的解决方案，用于减轻顺序推荐中的长度诱导偏差，显著提升了短序列用户的准确性，同时不牺牲长序列的有效性。

Abstract: User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models paradoxically degrades performance for short-sequence users due to attention polarization and length imbalance in training data. To address this, we propose LAIN(Length-Adaptive Interest Network), a plug-and-play framework that explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling. LAIN consists of three lightweight components: a Spectral Length Encoder that maps length into continuous representations, Length-Conditioned Prompting that injects global contextual cues into both long- and short-term behavior branches, and Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length. Extensive experiments on three real-world benchmarks across five strong CTR backbones show that LAIN consistently improves overall performance, achieving up to 1.15% AUC gain and 2.25% log loss reduction. Notably, our method significantly improves accuracy for short-sequence users without sacrificing longsequence effectiveness. Our work offers a general, efficient, and deployable solution to mitigate length-induced bias in sequential recommendation.

</details>


### [95] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate通过多专家代理协作和结构化辩论协议，显著提升零样本时间序列推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在时间序列分析中的数值保真度、模态干扰和跨模态整合问题。

Method: TS-Debate采用模态专业化、协作多代理辩论框架，包括文本上下文、视觉模式和数值信号的专家代理，并通过验证-冲突-校准机制协调其交互。

Result: 在三个公共基准的20个任务中，TS-Debate显著优于强基线，包括标准多模态辩论。

Conclusion: TS-Debate框架通过多专家代理协作和结构化辩论协议，显著提升了零样本时间序列推理的性能，解决了大型语言模型在数值保真度和跨模态整合方面的脆弱性。

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [96] [LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge](https://arxiv.org/abs/2601.19155)
*Qiujun Li,Zijin Xiao,Xulin Wang,Zhidan Ma,Cheng Yang,Haifeng Li*

Main category: cs.AI

TL;DR: LocationAgent通过分层推理和外部工具验证，解决了图像地理定位中的事实幻觉和泛化问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法因静态记忆和动态知识不足，易在开放世界或需动态知识的场景中出现事实幻觉和泛化瓶颈。

Method: 设计了RER架构（Reasoner-Executor-Recorder）实现分层推理，并构建了一套线索探索工具用于证据验证。

Result: 在零样本设置下，LocationAgent性能显著优于现有方法至少30%。

Conclusion: LocationAgent通过外部工具验证地理证据和分层推理架构（RER），显著提升了图像地理定位的准确性和泛化能力，尤其在零样本设置下表现优异。

Abstract: Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.

</details>


### [97] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: \model{} 是一个多代理框架，通过分阶段的推理和反馈机制，显著提升了从自然语言中提取程序图的质量。


<details>
  <summary>Details</summary>
Motivation: 从自然语言中自动提取程序图是一个有前景但尚未充分探索的领域，需要保证结构的有效性和逻辑的一致性。现有大型语言模型（LLMs）在提取时常常生成结构错误或逻辑不一致的结果。

Method: \model{} 采用多轮推理过程，包括图形构建代理、结构反馈代理和语义反馈代理三个阶段，通过自然语言反馈进行迭代优化。

Result: 实验表明，\model{} 在结构正确性和逻辑一致性方面均取得了显著提升。

Conclusion: \model{} 通过多代理框架显著提升了程序图提取的结构正确性和逻辑一致性，优于现有基线方法。

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [98] [CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation](https://arxiv.org/abs/2601.19178)
*Jingyu Li,Zhaocheng Du,Qianhui Zhu,kaiyuan Li,Zhicheng Zhang,Song-Li Wu,Chaolang Li,Pengwen Dai*

Main category: cs.AI

TL;DR: CollectiveKV通过共享KV机制，大幅减少存储开销且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 发现KV序列在不同用户间存在显著相似性，表明KV中存在协作信号，且大部分信息可跨用户共享。

Method: 提出CollectiveKV，利用可学习的全局KV池捕获用户间共享信息，并与低维用户特定KV拼接。

Result: 在五个顺序推荐模型和三个数据集上验证，KV缓存压缩至原大小的0.8%，性能保持或提升。

Conclusion: CollectiveKV通过跨用户共享KV机制，显著减少了KV缓存的存储开销（压缩至原大小的0.8%），同时保持甚至提升了模型性能。

Abstract: Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.

</details>


### [99] [CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning](https://arxiv.org/abs/2601.19193)
*Van-Quang Nguyen,Takayuki Okatani*

Main category: cs.AI

TL;DR: CoReTab是一个代码驱动的推理框架，通过结合多步推理和可执行Python代码，提升了多模态表格理解的准确性、可解释性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如MMTab）提供的简短事实性答案缺乏显式的多步推理监督，导致模型生成的回答准确性不足且解释性有限。

Method: 通过三阶段管道对开源MLLMs进行微调，并利用CoReTab框架整理了一个包含115K已验证样本的数据集。

Result: 在17个MMTab基准测试中，模型在表格问答、事实验证和表格结构理解任务上分别取得了+6.2%、+5.7%和+25.6%的显著提升。

Conclusion: CoReTab被确立为一个稳健且可推广的监督框架，用于提升多模态表格理解中的多步推理能力。

Abstract: Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.

</details>


### [100] [MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution](https://arxiv.org/abs/2601.19199)
*Libo Sun,Jiwen Zhang,Siyuan Wang,Zhongyu Wei*

Main category: cs.AI

TL;DR: MAGNET框架通过双级记忆和动态进化机制，有效应对GUI界面变化，提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI代理因界面频繁更新而导致的历史数据训练失效问题，利用功能语义和任务意图的稳定性。

Method: 引入了MAGNET，一个具有双级记忆（静态记忆和程序记忆）的记忆驱动自适应代理框架，并提出了动态记忆进化机制。

Result: 在AndroidWorld在线基准测试中表现优于基线，离线基准测试在分布变化下也显示出一致增益。

Conclusion: MAGNET框架通过利用界面变化中的稳定结构，显著提升了代理在动态软件环境中的性能和泛化能力。

Abstract: Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.

</details>


### [101] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一种多代理分层可训练自动机，通过超级代理选择最优代理解决视觉推理任务，显著提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型具有强大的感知能力，但其隐式推理难以解释，且在复杂查询上容易产生幻觉。组合方法提高了可解释性，但大多数依赖单一代理或手工制作的流程，无法决定何时在互补代理之间协作或在重叠代理之间竞争。

Method: MATA是一个多代理系统，以分层有限状态自动机形式呈现，用于视觉推理。其顶层转换由可训练的超级代理选择，每个代理对应超自动机中的一个状态，并运行小型基于规则的子自动机以实现可靠的微控制。所有代理读写共享内存，生成透明的执行历史。

Result: MATA通过监督微调（SFT）数据集MATA-SFT-90K训练超级代理的转换策略，使其能够理解查询和代理能力，高效选择最优代理解决问题。

Conclusion: MATA在多个视觉推理基准测试中取得了最先进的结果，超越了单一和组合基线方法。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [102] [Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection](https://arxiv.org/abs/2601.19245)
*Yongxin Deng,Zhen Fang,Yixuan Li,Ling Chen*

Main category: cs.AI

TL;DR: 本文提出SpikeScore方法，通过量化多轮对话中的不确定性波动，显著提升跨领域幻觉检测性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在跨领域泛化上表现不佳，研究通用幻觉检测（GHD）问题，旨在单领域训练数据上实现跨领域鲁棒性能。

Method: 提出SpikeScore评分方法，通过理论分析和实证验证，量化多轮对话中的不确定性波动，用于区分幻觉和非幻觉响应。

Result: 实验表明，基于SpikeScore的检测方法在跨领域泛化上优于现有基线方法，并在多个LLM和基准测试中表现优异。

Conclusion: SpikeScore作为一种新的评分方法，在多轮对话中量化不确定性波动，显著提升了跨领域幻觉检测的性能，验证了其在跨领域泛化中的有效性。

Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.

</details>


### [103] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架通过主动探测记忆不一致性，实现无监督记忆更新，显著提升LLM在动态环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法依赖外部评估或模型内省，难以应对动态环境中的非平稳性。

Method: 提出Global Verifier (GLOVE)框架，通过主动探测记忆与观察间的不一致性，实现无监督记忆验证与更新。

Result: 在多种基准测试中，GLOVE显著提高了智能体成功率。

Conclusion: GLOVE框架通过引入相对真理概念，显著提升了LLM在动态环境中的适应性和成功率，为认知智能体的自我进化提供了稳健路径。

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [104] [Curiosity Driven Knowledge Retrieval for Mobile Agents](https://arxiv.org/abs/2601.19306)
*Sijia Li,Xiaoyu Tan,Shahir Ali,Niels Schmidt,Gengchen Ma,Xihe Qiu*

Main category: cs.AI

TL;DR: 通过好奇心驱动的知识检索和结构化AppCards，移动代理在复杂任务中的性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决移动代理在复杂应用中因知识不完整和泛化能力弱而导致的性能限制问题。

Method: 提出了一种基于好奇心驱动的知识检索框架，将执行中的不确定性量化为好奇心分数，并利用外部信息（如文档、代码库和历史轨迹）生成结构化AppCards。

Result: 在AndroidWorld基准测试中，平均性能提升6个百分点，结合GPT-5时达到88.8%的最新成功率。

Conclusion: AppCards框架通过结构化外部知识显著提升了移动代理在复杂应用中的性能，特别是在多步骤和跨应用任务中表现突出。

Abstract: Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.

</details>


### [105] [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)
*Anh Khoa Ngo Ho,Martin Chauvin,Simon Gosset,Philippe Cordier,Boris Gamazaychikov*

Main category: cs.AI

TL;DR: 研究发现小规模开源语言模型能在保证性能的同时减少能源消耗，并提出了可持续AI设计的实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在智能AI系统中的广泛应用，其推理过程中的能源需求可能带来显著的可持续性挑战。

Method: 通过对不同规模的语言模型进行对比分析，量化了效率与性能之间的权衡。

Result: 结果表明，较小规模的开源模型在保持任务质量的同时可以降低能源消耗。

Conclusion: 研究提出了一套实用的可持续人工智能设计指南，包括最优批次大小配置和计算资源分配，为开发可扩展且环境友好的AI系统提供了可行策略。

Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.

</details>


### [106] [SETA: Statistical Fault Attribution for Compound AI Systems](https://arxiv.org/abs/2601.19337)
*Sayak Chowdhury,Meenakshi D'Souza*

Main category: cs.AI

TL;DR: 论文提出了一种适用于多网络AI系统的模块化鲁棒性测试框架，支持组件级分析和错误传播推理，并在实际系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒性测试技术主要针对单网络模型，无法有效扩展到多网络系统，因此需要新的测试方法。

Method: 采用模块化测试框架，支持对测试数据应用扰动集，进行组件级系统分析和错误传播推理。

Result: 在真实世界的自主铁路检查系统中成功应用该框架，实现了超越传统端到端指标的细粒度鲁棒性分析。

Conclusion: 该论文提出了一个模块化的鲁棒性测试框架，能够有效分析和隔离多网络AI系统中的错误，并推理错误在网络模块间的传播。

Abstract: Modern AI systems increasingly comprise multiple interconnected neural networks to tackle complex inference tasks. Testing such systems for robustness and safety entails significant challenges. Current state-of-the-art robustness testing techniques, whether black-box or white-box, have been proposed and implemented for single-network models and do not scale well to multi-network pipelines. We propose a modular robustness testing framework that applies a given set of perturbations to test data. Our testing framework supports (1) a component-wise system analysis to isolate errors and (2) reasoning about error propagation across the neural network modules. The testing framework is architecture and modality agnostic and can be applied across domains. We apply the framework to a real-world autonomous rail inspection system composed of multiple deep networks and successfully demonstrate how our approach enables fine-grained robustness analysis beyond conventional end-to-end metrics.

</details>


### [107] [PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems](https://arxiv.org/abs/2601.19402)
*Amit Singh Bhatti,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: PROTEUS是一种新型LLM路由器，通过拉格朗日对偶控制直接接受运行时准确性目标，无需重新训练即可实现高效路由，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由器需要离线调整参数且无法直接指定准确性目标，导致操作者难以预测实际效果。PROTEUS旨在解决这一问题，允许直接设定准确性目标。

Method: PROTEUS采用拉格朗日对偶控制，通过训练期间跟踪约束违规的学习对偶变量λ来调整策略网络，从而将指定的准确性目标τ转化为满足这些目标的路由决策。

Result: 在RouterBench和SPROUT数据集上的评估显示，PROTEUS在准确性目标τ∈[0.85,0.95]范围内均能稳定满足或超过目标，目标响应相关性达0.97-0.98，成本节省高达89.8%。

Conclusion: PROTEUS是一种能够接受准确性目标作为运行时输入的LLM路由器，通过拉格朗日对偶控制实现目标响应，无需重新训练即可满足广泛的准确性要求，显著降低成本。

Abstract: Production LLM deployments serve diverse workloads where cost and quality requirements vary by customer tier, time of day, and query criticality. Model serving systems accept latency SLOs directly. LLM routers do not. They force operators to tune parameters offline and guess what accuracy might result. The relationship between parameters and outcomes is indirect, non-monotonic, and dataset-dependent. Operators need to specify accuracy targets, not infer them from opaque settings. We present PROTEUS (Polymorphic Router for Operational Target Enforcement with Unified SLA), a router that accepts accuracy targets tau as runtime input. PROTEUS uses Lagrangian dual control. A learned dual variable lambda tracks constraint violations during training and conditions the policy network. This lets the router translate specified tau values into routing decisions that satisfy them. A single trained model serves the full accuracy spectrum without retraining.We evaluate on RouterBench (11 models, 405K queries) and SPROUT (14 models, 45K queries). PROTEUS achieves consistent floor compliance where accuracy meets or exceeds tau. The target-response correlation reaches 0.97 to 0.98. The closest baseline, OmniRouter, meets floors only 22% of the time despite also using Lagrangian optimization. PROTEUS operates across tau in [0.85, 0.95] from a single model. On RouterBench it achieves 90.1% accuracy, within 1.3% of oracle. On SPROUT it achieves 94.0% accuracy, within 4.6% of oracle. Cost savings reach 89.8% versus the best fixed model.

</details>


### [108] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO通过部分推理优化减少训练时间开销，性能与原始算法相当。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调算法在训练阶段生成完整推理路径导致计算开销大，RPO旨在减少这一开销。

Method: 提出了Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO)，一种基于经验缓存生成推理路径后缀的强化微调算法。

Result: RPO在训练阶段减少了约95%的token生成，1.5B模型训练时间降低90%，7B模型降低72%。

Conclusion: RPO算法通过部分推理优化显著降低了训练时间开销，同时保持了与原始算法相当的性能，并可与其他典型算法（如GRPO和DAPO）集成。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [109] [Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach](https://arxiv.org/abs/2601.19527)
*Temirbolat Maratuly,Pakizar Shamoi,Timur Samigulin*

Main category: cs.AI

TL;DR: 模糊专家系统结合数字孪生技术，有效处理酸性水，减少排放和腐蚀风险，适用于多种工业场景。


<details>
  <summary>Details</summary>
Motivation: 酸性水处理对减少排放、降低腐蚀风险、实现水回用及降低运营成本至关重要。自动化处理还能减少工人接触风险。

Method: 开发了一个模糊专家系统，结合定制的数字孪生技术，使用Honeywell UniSim Design R492模拟工业行为，MATLAB建模阀门动态，并通过OPC DA实现实时数据交换。

Result: 系统在105种测试场景下表现良好，通过误差和动态响应指标评估，验证了其有效性。

Conclusion: 本文提出的模糊专家系统结合数字孪生技术，不仅适用于酸性水处理，还具有通用性，可应用于其他工业过程。

Abstract: Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.

</details>


### [110] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是一个经过手动修订的数据集，旨在减少噪声并提供更准确的模型性能评估，同时揭示了评估者可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集和评估方法的不准确性影响了大型语言模型（LLM）发展的进展跟踪。

Method: 通过手动修订Omni-MATH数据集，创建了清洁精确答案子集（n=4181）和标记非标准子集（n=247），并对每个问题进行了审核以确保可编译性、可解性和可验证性。

Result: 修订后的数据集显著减少了由数据集引起的噪声，并揭示了评估者之间的显著差异（Omni-Judge在96.4%的评估分歧中错误）。

Conclusion: 数据集质量和评估者可靠性对于开发准确的模型性能基准至关重要。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [111] [Learning Adaptive Parallel Execution for Efficient Code Localization](https://arxiv.org/abs/2601.19568)
*Ke Xu,Siyang Xiao,Ming Liang,Yichen Yu,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: FuseSearch通过联合优化质量与效率，动态调整并行搜索策略，显著提升了代码定位的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前代理存在34.9%的冗余调用率，抵消了并行化的优势，因此需要优化并行代码定位的质量与效率。

Method: 采用两阶段的SFT和RL训练方法，学习自适应并行策略，动态调整搜索广度。

Result: 在SWE-bench Verified上，FuseSearch-4B达到了SOTA性能（文件级84.7%和函数级56.4%的F1分数），速度提升93.6%，调用次数减少67.7%，令牌使用减少68.9%。

Conclusion: 效率感知训练通过消除冗余信号自然提升了质量，实现了高性能且成本效益高的代码定位代理。

Abstract: Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\% redundant invocation rate, which negates parallelism benefits. We propose \textbf{FuseSearch}, reformulating parallel code localization as a \textbf{joint quality-efficiency optimization} task. Through defining \textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\% file-level and 56.4\% function-level $F_1$ scores) with 93.6\% speedup, utilizing 67.7\% fewer turns and 68.9\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.

</details>


### [112] [ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks](https://arxiv.org/abs/2601.19607)
*Haoyun Li,Ming Xiao,Kezhi Wang,Robert Schober,Dong In Kim,Yong Liang Guan*

Main category: cs.AI

TL;DR: ComAgent是一个多LLM代理框架，通过闭环循环协调专业代理，自主生成6G网络优化公式，性能媲美专家并优于单一LLM。


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中手动将高层次意图转化为数学公式的瓶颈，以及单一LLM方法在领域基础、约束意识和验证能力上的不足。

Method: 采用闭环的感知-规划-行动-反思循环，协调文献搜索、编码和评分等专业代理，自主生成可求解的公式和可复现的模拟。

Result: 评估显示，ComAgent在复杂波束成形优化中达到专家可比性能，并在多样化无线任务中优于单一LLM方法。

Conclusion: ComAgent通过多LLM代理框架有效解决了6G网络中高层次意图到数学公式的转换瓶颈，展示了在新兴无线网络中自动化设计的潜力。

Abstract: Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.

</details>


### [113] [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)
*Thomas Bömer,Nico Koltermann,Max Disselnmeyer,Bastian Amberg,Anne Meyer*

Main category: cs.AI

TL;DR: A-CEoH框架通过将A*代码融入提示，自动化生成高质量启发式，在UPMP和SPP实验中表现优于专家设计。


<details>
  <summary>Details</summary>
Motivation: 传统启发式函数需手工设计且依赖专家知识，近期大型语言模型（LLMs）和进化框架的发展为自动化启发式设计提供了可能。

Method: 扩展了启发式进化（EoH）框架，提出了一种新颖的领域无关提示增强策略A-CEoH，将A*代码融入提示中以利用上下文学习。

Result: 在UPMP和SPP两个问题领域的计算实验中，A-CEoH显著提升了生成启发式的质量，部分情况下超越专家设计的启发式。

Conclusion: A-CEoH框架在自动化启发式设计方面表现出色，不仅能提升生成启发式的质量，甚至在某些情况下超越专家设计的启发式。

Abstract: Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.

</details>


### [114] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文提出系统理论框架和12种设计模式，解决代理性AI系统的不可靠性问题，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有代理设计模式缺乏严格的系统理论基础，导致难以实现高层次或基于便利性的分类，本文旨在填补这一空白。

Method: 通过将代理性AI系统解构为五个核心功能子系统，并基于此架构提出12种设计模式，分类为基础型、认知与决策型、执行与交互型以及适应与学习型。

Result: 案例研究展示了所提模式如何纠正ReAct框架的系统架构缺陷，验证了框架的实用性。

Conclusion: 本文提出了一个系统理论框架和12种代理设计模式，为研究人员和工程师提供了标准化的代理设计语言和方法论，从而构建更模块化、可理解和可靠的自主系统。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


### [115] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 本文提出基于规则的激活安全方法，通过可组合的认知元素（CEs）和谓词规则，提高精确度和灵活性，支持AI治理的可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全方法在广泛误用数据集上训练，存在精确度低、灵活性有限和缺乏可解释性的问题。

Method: 提出了一种新的范式：基于规则的激活安全，将激活建模为可组合的认知元素（CEs），并定义谓词规则以实时检测违规行为。

Result: 组合式基于规则的激活安全提高了精确度，支持领域定制化，并实现了透明和可审计的AI治理。

Conclusion: 基于规则的激活安全方法通过提高精确度、支持领域定制化，为可扩展、可解释和可审计的AI治理奠定了基础。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


### [116] [CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing](https://arxiv.org/abs/2601.19793)
*Shanyv Liu,Xuyang Yuan,Tao Chen,Zijun Zhan,Zhu Han,Danyang Zheng,Weishan Zhang,Shaohua Cao*

Main category: cs.AI

TL;DR: CASTER通过动态模型选择优化图基多智能体系统的计算效率，减少推理成本72.4%，同时保持高成功率。


<details>
  <summary>Details</summary>
Motivation: 解决图基多智能体系统中静态模型分配效率低下的问题，避免在简单子任务上浪费计算资源。

Method: CASTER采用双信号路由器，结合语义嵌入和结构元特征来估计任务难度，并通过冷启动到迭代进化的自优化范式进行训练。

Result: 实验表明，CASTER相比强模型基线减少了高达72.4%的推理成本，同时成功率相当，且在软件工程、数据分析、科学发现和网络安全等领域均优于启发式路由和FrugalGPT。

Conclusion: CASTER（Context-Aware Strategy for Task Efficient Routing）通过动态模型选择显著降低了图基多智能体系统的推理成本，同时保持了高成功率，并在多个领域表现优于基线方法。

Abstract: Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.

</details>


### [117] [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](https://arxiv.org/abs/2601.19824)
*Andre Paulino de Lima,Paula Castro,Suzana Carvalho Vaz de Andrade,Rosa Maria Marcucci,Ruth Caldeira de Melo,Marcelo Garcia Manzato*

Main category: cs.AI

TL;DR: A recommender system for gerontological care uses psychometric data to offer interpretable visual explanations, showing promise in addressing healthcare challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in healthcare recommender systems, including lack of public clinical data, user understanding difficulties, recommendation risks, and effectiveness uncertainty.

Method: A recommendation model leveraging psychometric data structure to provide visual explanations, evaluated through offline performance tests and user studies.

Result: The model demonstrated effectiveness in assisting care professionals with personalized care plans and provided interpretable visual explanations.

Conclusion: The proposed recommendation model shows potential in advancing the application of recommender systems in gerontological primary care, addressing key challenges such as data availability, interpretability, and effectiveness.

Abstract: There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.

</details>


### [118] [Routing End User Queries to Enterprise Databases](https://arxiv.org/abs/2601.19825)
*Saikrishna Sudarshan,Tanay Kulkarni,Manasi Patwardhan,Lovekesh Vig,Ashwin Srinivasan,Tanmay Tulsidas Verlekar*

Main category: cs.AI

TL;DR: 论文提出了一种基于推理的模块化重新排序策略，用于在多数据库环境中路由自然语言查询，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在多数据库企业环境中，随着数据库库规模增大、领域重叠和查询歧义性增加，路由任务变得更具挑战性，需要更结构化和鲁棒的基于推理的解决方案。

Method: 通过明确建模模式覆盖、结构连接性和细粒度语义对齐，构建了一种模块化、推理驱动的重新排序策略。

Result: 所提出的策略在所有指标上均优于嵌入和直接LLM提示的基线方法。

Conclusion: 论文提出了一种模块化、基于推理的重新排序策略，该策略在路由自然语言查询任务中表现优于仅嵌入和直接LLM提示的基线方法。

Abstract: We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.

</details>


### [119] [Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models](https://arxiv.org/abs/2601.19834)
*Jialong Wu,Xiaoying Zhang,Hongyi Yuan,Xiangcheng Zhang,Tianhao Huang,Changjing He,Chaoyi Deng,Renrui Zhang,Youbin Wu,Mingsheng Long*

Main category: cs.AI

TL;DR: 本文首次系统研究了视觉生成如何及何时辅助推理，提出视觉优势假设，并通过理论和实验验证了其在特定任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉生成在推理中的优势和适用条件，特别是在物理和空间智能等需要丰富表征和先验知识的领域。

Method: 通过理论分析，将内部世界建模形式化为CoT推理的核心组成部分，并实证研究构建了新的评估套件VisWorld-Eval，在先进的多模态模型上进行控制实验。

Result: 实验表明，在视觉世界建模优势的任务中，交替视觉-语言CoT推理显著优于纯语言CoT，而在其他任务中无明显优势。

Conclusion: 本文阐明了多模态世界建模在实现更强大、更类似人类的多模态AI方面的潜力，特别是在需要视觉世界建模的任务中，视觉生成显著优于纯语言推理。

Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [120] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: X-Sched通过可解释性技术和机器学习优化容器化环境中的任务调度，提供透明且可操作的资源配置指导。


<details>
  <summary>Details</summary>
Motivation: 现有调度器在决策过程透明度和满足服务级别目标（SLOs）的具体行动指导方面存在不足，X-Sched旨在填补这一空白。

Method: X-Sched是一个中间件，利用可解释性技术生成资源配置的行动指南，结合随机森林等机器学习模型高效识别最优配置。

Result: 实验结果表明，X-Sched在真实执行环境中验证了其效率、优势和实用性。

Conclusion: X-Sched通过结合反事实解释和机器学习模型（如随机森林），为容器化环境中的任务执行提供了可行的资源配置指导，不仅满足了性能目标，还增强了调度决策的透明度。

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [121] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: Axe Layout通过统一的多轴映射和编译器设计，优化深度学习工作负载布局，性能接近手工调优。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载需要跨设备网格、内存层次和异构加速器协调数据和计算布局。

Method: 提出了Axe Layout抽象和多粒度、分布感知的DSL及编译器，结合线程本地控制与集体操作符。

Result: 实验表明，该统一方法在最新GPU设备和多设备环境及加速器后端上能接近手工调优内核的性能。

Conclusion: Axe Layout通过统一的多轴物理空间映射，结合DSL和编译器设计，实现了接近手工调优内核的性能，适用于多种GPU设备和加速器后端。

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [122] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT是一个基于Kubernetes的FaaS集群管理器，通过直接消息传递和窄腰状态管理，实现了高效和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案需要重新设计集群管理器，牺牲了与现有生态系统的兼容性，且工程成本高昂。

Method: KUBEDIRECT采用直接消息传递的方式绕过API Server，利用窄腰作为分层写回缓存来管理状态。

Result: 实验表明，KUBEDIRECT比Knative降低了26.7倍的延迟，性能与Dirigent相当。

Conclusion: KUBEDIRECT通过利用FaaS平台中的窄腰特性，实现了高效性和外部兼容性的平衡，同时通过新颖的状态管理方案确保了系统的一致性和收敛性。

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [123] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: ODC通过点对点通信优化LLM后训练中的不平衡工作负载，相比FSDP提升36%效率。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）后训练中序列长度的高方差导致工作负载不平衡，集体通信在此情况下效率低下。

Method: 提出了按需通信（ODC），通过将集体通信替换为直接点对点通信，将同步屏障从每层一次减少到每小批次一次。

Result: ODC在多样化的LLM后训练任务中持续提高了设备利用率和训练吞吐量，相比标准FSDP实现了高达36%的加速。

Conclusion: ODC是一种更适合LLM后训练中普遍存在的不平衡工作负载的解决方案，通过开源实现与FSDP集成，展示了其优越性。

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


### [124] [Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization](https://arxiv.org/abs/2601.19563)
*Juan Zhu,Zixin Wang,Shenghui Song,Jun Zhang,Khaled Ben Letaief*

Main category: cs.DC

TL;DR: 提出微服务FM推理框架，通过静态+动态两层级部署，在边缘实现高效实时服务，仿真验证其高完成率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决云中心部署FM导致的实时性差和隐私问题，以及边缘单一体执行在资源限制和网络动态下的不可行性。

Method: 采用静态部署核心服务（基于网络感知的整数规划）和动态编排轻服务（结合有效容量理论和Lyapunov优化的在线控制器）的两层级策略。

Result: 仿真显示，该框架在适度部署成本下实现了超过84%的平均按时任务完成率，并在系统负载增加时保持强鲁棒性。

Conclusion: 该论文提出了一种基于微服务的FM推理框架，通过两层级部署策略在资源受限的边缘环境中实现高效、实时的服务，同时保证了QoS和用户隐私。

Abstract: Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [125] [Analysis of Shuffling Beyond Pure Local Differential Privacy](https://arxiv.org/abs/2601.19154)
*Shun Takagi,Seng Pei Liew*

Main category: cs.DS

TL;DR: 本文通过洗牌指数和渐近分析，为洗牌机制提供了紧致的DP保证，并开发了高效计算毯散度的FFT算法。


<details>
  <summary>Details</summary>
Motivation: 现有分析大多将局部差分隐私参数$\varepsilon_0$作为唯一调节参数，给出的通用上界可能宽松且未能表征洗牌如何放大高斯机制等基本机制的隐私性。

Method: 本文重新审视了Balle等人的隐私毯界（毯散度），并开发了一种适用于广泛局部随机化器的渐近分析，无需纯局部DP。

Result: 研究发现毯散度的主导项仅通过一个标量参数$χ$（洗牌指数）依赖于局部机制，并推导了局部随机化器的结构条件。

Conclusion: 本文通过引入洗牌指数$χ$，为洗牌机制提供了紧致的$(\varepsilon_n,δ_n)$-DP保证，并通过FFT算法实现了高效计算。

Abstract: Shuffling is a powerful way to amplify privacy of a local randomizer in private distributed data analysis, but existing analyses mostly treat the local differential privacy (DP) parameter $\varepsilon_0$ as the only knob and give generic upper bounds that can be loose and do not even characterize how shuffling amplifies privacy for basic mechanisms such as the Gaussian mechanism. We revisit the privacy blanket bound of Balle et al. (the blanket divergence) and develop an asymptotic analysis that applies to a broad class of local randomizers under mild regularity assumptions, without requiring pure local DP. Our key finding is that the leading term of the blanket divergence depends on the local mechanism only through a single scalar parameter $χ$, which we call the shuffle index. By applying this asymptotic analysis to both upper and lower bounds, we obtain a tight band for $δ_n$ in the shuffled mechanism's $(\varepsilon_n,δ_n)$-DP guarantee. Moreover, we derive a simple structural necessary and sufficient condition on the local randomizer under which the blanket-divergence-based upper and lower bounds coincide asymptotically. $k$-RR families with $k\ge3$ satisfy this condition, while for generalized Gaussian mechanisms the condition may not hold but the resulting band remains tight. Finally, we complement the asymptotic theory with an FFT-based algorithm for computing the blanket divergence at finite $n$, which offers rigorously controlled relative error and near-linear running time in $n$, providing a practical numerical analysis for shuffle DP.

</details>


### [126] [Preprocessing Uncertain Data into Supersequences for Sorting and Gaps](https://arxiv.org/abs/2601.19453)
*Maarten Löffler,Benjamin Raichel*

Main category: cs.DS

TL;DR: 本文提出用超序列作为辅助结构处理不确定数据，简化预处理并增强阶段解耦，适用于排序和间隔计算问题。


<details>
  <summary>Details</summary>
Motivation: 探索超序列作为辅助结构在不确定数据处理中的潜力，以简化现有方法并增强预处理与重构阶段的解耦。

Method: 提出使用输入项的超序列作为辅助结构，应用于排序、计算最小或最大间隔的问题。

Result: 超序列比以往专用辅助结构更简单，并能灵活组合不同解决方案。

Conclusion: 使用超序列作为辅助结构简化了预处理框架，并允许预处理阶段与重构阶段更强地解耦，为已知和新结果提供了灵活的组合方式。

Abstract: In the preprocessing framework for dealing with uncertain data, one is given a set of regions that one is allowed to preprocess to create some auxiliary structure such that when a realization of these regions is given, consisting of one point per region, this auxiliary structure can be used to reconstruct some desired output structure more efficiently than would have been possible without preprocessing. The framework has been successfully applied to several, mostly geometric, computational problems.
  In this note, we propose using a supersequence of input items as the auxiliary structure, and explore its potential on the problems of sorting and computing the smallest or largest gap in a set of numbers. That is, our uncertainty regions are intervals on the real line, and in the preprocessing phase we output a supersequence of the intervals such that the sorted order / smallest gap / largest gap of any realization is a subsequence of this sequence.
  We argue that supersequences are simpler than specialized auxiliary structures developed in previous work. An advantage of using supersequences as the auxiliary structures is that it allows us to decouple the preprocessing phase from the reconstruction phase in a stronger sense than was possible in previous work, resulting in two separate algorithmic problems for which different solutions may be combined to obtain known and new results. We identify one key open problem which we believe is of independent interest.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [127] [Automated structural testing of LLM-based agents: methods, framework, and case studies](https://arxiv.org/abs/2601.18827)
*Jens Kohl,Otto Kruse,Youssef Mostafa,Andre Luckow,Karsten Schroer,Thomas Riedl,Ryan French,David Katz,Manuel P. Luitz,Tanrajbir Takher,Ken E. Friedl,Céline Laurent-Winter*

Main category: cs.SE

TL;DR: 本文提出了一种自动化结构化测试方法，通过轨迹追踪和模拟行为，降低了LLM代理的测试成本并提升了质量。


<details>
  <summary>Details</summary>
Motivation: 现有的测试方法依赖手动评估，难以自动化，不利于根因分析，且测试环境成本高昂。

Method: 利用基于OpenTelemetry的轨迹追踪、模拟LLM行为以确保可重复性，并添加断言来自动化测试验证。

Result: 在案例研究中展示了自动化执行和更快的根因分析，提高了测试覆盖率、可重用性和早期缺陷检测。

Conclusion: 本文提出了一种结构化的测试方法，通过自动化测试流程降低了成本并提升了LLM代理的质量。

Abstract: LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.

</details>


### [128] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: LLMs在企业级SAT误报减少中表现高效且成本低，混合技术可消除94-98%误报。


<details>
  <summary>Details</summary>
Motivation: 解决企业级系统中SATs高误报率导致的低效手动检查问题，验证LLMs在实际工业环境中的有效性。

Method: 通过腾讯企业定制SAT的数据集（433个警报，含328个误报和105个真阳性），结合开发者访谈和数据分析，评估多种基于LLM的误报减少技术。

Result: 混合LLM与静态分析的技术可消除94-98%的误报，且成本极低（每警报2.1-109.5秒，$0.0011-$0.12）。

Conclusion: LLM-based技术在企业环境中具有显著潜力，能够高效减少静态分析工具（SATs）的高误报率，同时成本效益显著。

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [129] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: MulVul是一个检索增强的多代理框架，通过粗到细策略和跨模型提示进化，显著提升漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化真实世界漏洞检测时面临异构漏洞模式和手动提示工程不可扩展的挑战。

Method: MulVul采用粗到细的策略，包括路由代理和检测代理，并结合检索工具和跨模型提示进化机制来自动生成和优化提示。

Result: 在130种CWE类型上评估，MulVul的Macro-F1达到34.79%，比最佳基线提高了41.5%。消融实验验证了跨模型提示进化的有效性，性能比手动提示提高了51.6%。

Conclusion: MulVul框架通过检索增强的多代理架构和跨模型提示进化，显著提升了漏洞检测的准确性和覆盖范围，证明了其在处理异构漏洞模式方面的有效性。

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [130] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种安全视角下的Transformer架构，通过多模态融合和冗余设计提升自动驾驶系统的容错性和鲁棒性，为可认证AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在视觉和语言任务中表现出色，但在安全关键应用中面临独特挑战。本文旨在从安全角度探讨如何将Transformer集成到汽车系统中。

Method: 提出了一种结合多个独立模态特定编码器的架构，这些编码器将它们的表示融合到一个共享的潜在空间中，支持在某一模态退化时的故障操作行为。

Result: 展示了如何融合不同输入模态以保持一致的场景理解，通过多模态基础模型利用传感器多样性和冗余性来提高容错性和鲁棒性。

Conclusion: 通过将冗余性和多样性嵌入到表示层面，该方法弥合了现代深度学习与成熟功能安全实践之间的差距，为自动驾驶中可认证的AI系统铺平了道路。

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [131] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 论文构建了Tricky$^2$数据集，研究人类与LLM生成错误的交互，支持混合代码可靠性分析。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在软件开发中引入的逻辑或数据误用错误与人类错误的交互作用。

Method: 采用分类学引导的提示框架，在保留原始人类缺陷和程序结构的同时，生成机器源错误，构建包含人类、LLM及混合错误的数据集。

Result: 构建了Tricky$^2$数据集，支持混合来源错误行为分析、多错误修复鲁棒性及混合人机代码可靠性研究。

Conclusion: 该论文通过构建Tricky$^2$数据集，展示了人类与LLM生成错误的交互作用，为混合人机代码的可靠性研究提供了基础。

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [132] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种Python化的PIMPL模式，用于稳定公共API并隔离内部实现，适用于大型长期维护的Python库。


<details>
  <summary>Details</summary>
Motivation: 解决Python库在长期维护中因用户依赖未公开的内部实现而导致的难以重构和维护缓慢的问题。

Method: 通过重新解释C++的PIMPL惯用法，提出了一种Python化的不透明委托模式，结合模块级间接、门面对象和后端调度等现有实践，展示了其在Python标准库和科学计算生态系统中的应用。

Result: 展示了Pythonic PIMPL在现有代码库中的应用，包括隔离重依赖、支持延迟导入和运行时选择后端，同时不改变公共API。

Conclusion: 本文提出了一种Python化的PIMPL模式，用于在保持公共API稳定的同时，隔离内部实现的重依赖和可选后端，并提供了实际应用指南。

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [133] [Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair](https://arxiv.org/abs/2601.19066)
*Runxiang Cheng,Michele Tufano,José Cambronero,Renyao Wei,Sherry Shi,Grant Uy,Pat Rondon,Franjo Ivančić*

Main category: cs.SE

TL;DR: 研究APR代理同时生成修复和BRT的策略，证明其在不影响修复率的情况下，能生成与专用BRT代理相同数量的BRT。


<details>
  <summary>Details</summary>
Motivation: 开发者希望在AI生成的补丁中包含BRT以增加信心，但传统APR系统倾向于单独生成BRT和修复。

Method: 评估不同cogeneration策略在120个Google报告的人类bug上的有效性，开发并评估考虑测试变更信息的补丁选择器。

Result: Cogeneration策略使APR代理能够为至少与专用BRT代理相同数量的bug生成BRT，且不影响修复率。

Conclusion: Cogeneration策略允许APR代理在不影响修复率的情况下，生成与专用BRT代理相同数量的BRT，从而减少维护和协调单独生成管道的工程努力。

Abstract: Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.

</details>


### [134] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: HalluJudge 通过多策略评估 LLM 生成的代码审查评论的幻觉问题，成本低且与开发者偏好高度一致。


<details>
  <summary>Details</summary>
Motivation: 解决 LLM 在代码审查自动化中生成的评论存在幻觉（未基于实际代码）的问题，以提升 LLM 在代码审查工作流程中的可信度。

Method: 设计了 HalluJudge，通过四种关键策略（从直接评估到结构化多分支推理，如 Tree-of-Thoughts）来评估生成的审查评论的上下文对齐性。

Result: HalluJudge 的 F1 得分为 0.85，平均成本为 0.009 美元，67% 的评估结果与在线生产中开发者对 LLM 生成评论的偏好一致。

Conclusion: HalluJudge 被证明是一种成本效益高的方法，能够有效检测 LLM 生成的代码审查评论中的幻觉问题，并在实际生产中与开发者偏好高度一致。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [135] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: PyTation 是一种针对 Python 的突变测试工具，通过新操作符和混合分析方法，有效生成独特突变体并减少等效突变体。


<details>
  <summary>Details</summary>
Motivation: 现有突变测试技术在动态类型语言（如 Python）中无法捕捉许多常见错误类型。

Method: 结合静态和动态分析的混合方法，引入七种新的突变操作符，针对 Python 程序的常见反模式。

Result: PyTation 在 13 个开源 Python 应用上的评估显示，其生成的突变体与通用工具互补，表现出独特行为，并揭示了测试套件的不足。

Conclusion: PyTation 是一种有效的突变测试工具，能够生成独特的突变体并揭示高覆盖率测试套件的不足，同时减少了等效突变体的数量。

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [136] [Reward Engineering for Reinforcement Learning in Software Tasks](https://arxiv.org/abs/2601.19100)
*Md Rayhanul Masud,Azmine Toushik Wasi,Salman Rahman,Md Rizwan Parvez*

Main category: cs.SE

TL;DR: 本文系统综述了软件任务中强化学习的奖励设计方法，总结了现有技术并提出了未来挑战和建议。


<details>
  <summary>Details</summary>
Motivation: 软件任务中强化学习的奖励设计缺乏系统性综述，现有研究分散且不全面。

Method: 通过系统性文献综述，整理并分类现有奖励设计方法，沿三个互补维度进行总结。

Result: 首次提供了软件任务中强化学习奖励设计的系统性综述，总结了现有方法和技术。

Conclusion: 本文总结了软件任务中强化学习奖励设计的挑战和建议，为未来研究提供了方向。

Abstract: Reinforcement learning is increasingly used for code-centric tasks. These tasks include code generation, summarization, understanding, repair, testing, and optimization. This trend is growing faster with large language models and autonomous agents. A key challenge is how to design reward signals that make sense for software. In many RL problems, the reward is a clear number. In software, this is often not possible. The goal is rarely a single numeric objective. Instead, rewards are usually proxies. Common proxies check if the code compiles, passes tests, or satisfies quality metrics. Many reward designs have been proposed for code-related tasks. However, the work is scattered across areas and papers. There is no single survey that brings these approaches together and shows the full landscape of reward design for RL in software. In this survey, we provide the first systematic and comprehensive review of reward engineering for RL in software tasks. We focus on existing methods and techniques. We structure the literature along three complementary dimensions, summarizing the reward-design choices within each. We conclude with challenges and recommendations in the reward design space for SE tasks.

</details>


### [137] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文提出了一种静态分析框架，通过AST和动态知识库验证，可靠检测并自动纠正代码生成中的知识冲突幻觉，展示了确定性方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成中常引入知识冲突幻觉（KCHs），现有缓解方法如约束解码或非确定性LLM修复往往不可靠。

Method: 通过将生成的代码解析为抽象语法树（AST），并与通过库内省动态生成的知识库（KB）进行验证，使用确定性规则来发现和修复API及标识符级别的冲突。

Result: 在200个Python代码片段的手动整理数据集上，该框架检测KCHs的精度为100%，召回率为87.6%（F1分数0.934），并成功自动纠正了77.0%的幻觉。

Conclusion: 本文提出了一种确定性后处理框架，用于可靠地检测和自动纠正知识冲突幻觉（KCHs），为可信代码生成提供了可行的替代方案。

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [138] [The Promise and Reality of Continuous Integration Caching: An Empirical Study of Travis CI Builds](https://arxiv.org/abs/2601.19146)
*Taher A. Ghaleb,Daniel Alencar da Costa,Ying Zou*

Main category: cs.SE

TL;DR: 研究发现CI缓存采用率低且维护复杂，虽能减少构建时间，但需开发者更多关注和维护。


<details>
  <summary>Details</summary>
Motivation: 探究CI缓存在实践中的采用情况及其带来的挑战，以解决长构建时间对开发效率的影响。

Method: 通过对Travis CI中513,384次构建和1,279个GitHub项目的大规模实证研究，包括提交拉取请求启用缓存以观察采纳情况。

Result: 仅30%的项目采用CI缓存，早期采用与项目成熟度相关；近一半非采用项目在拉取请求后被接受；缓存维护活动占24%的项目，33%的项目存在过时缓存。

Conclusion: CI缓存虽然能显著减少构建时间，但并非对所有项目都有效，需要持续维护，且实际应用比开发者预期的更复杂。

Abstract: Continuous Integration (CI) provides early feedback by automatically building software, but long build durations can hinder developer productivity. CI services offer caching mechanisms to speed up builds by reusing infrequently changing artifacts, yet little is known about how caching is adopted in practice and what challenges it entails. In this paper, we conduct a large-scale empirical study of CI caching in Travis CI, analyzing 513,384 builds from 1,279 GitHub projects. We find that only 30% of projects adopt CI caching, and early adoption is strongly associated with project maturity, such as more dependencies, more commits, and longer CI lifespans. To understand why many projects do not adopt caching, we submitted pull requests enabling caching in non-adopting projects, and nearly half were accepted or merged. Developer feedback suggests that non- or late adoption mainly stems from limited awareness of CI caching support. We also examine cache maintenance and identify five common activities, performed by 24% of cache-enabled projects. Although one-third of projects see substantial build-time reductions, cache uploads occur in 97% of builds, and 33% of projects contain stale cached artifacts. Finally, our analysis of reported caching issues shows developers mainly struggle with corrupted or outdated caches or request broader caching features. Overall, CI caching does not help all projects, needs ongoing maintenance, and is more complex in practice than many developers expect.

</details>


### [139] [SE Journals in 2036: Looking Back at the Future We Need to Have](https://arxiv.org/abs/2601.19217)
*Tim Menzies,Paris Avgeriou,Robert Feldt,Mauro Pezzè,Abhik Roychoudhury,Miroslaw Staron,Sebastian Uchitel,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 论文探讨了软件工程出版危机的解决方案，包括流程改革和文化转变，以应对全球化和方法论快速变化的挑战。


<details>
  <summary>Details</summary>
Motivation: 软件工程出版面临的可扩展性危机，传统的同行评审制度无法应对全球社区增长和快速变化的方法论（如LLMs）。

Method: 通过期刊联盟（The Journal Alliance）、流程改革（如抽签制度、解绑和基准问题修复）以及文化变革（如大教堂与市集模式）来应对挑战。

Result: 提出了一套从流程到文化的系统性解决方案，以应对出版危机。

Conclusion: 论文提出了解决软件工程出版危机的综合方案，包括流程改革和文化转变。

Abstract: In 2025, SE publishing faces an existential crisis of scalability. As our communities swell globally and integrate fast-moving methodologies like LLMs, traditional peer-review practices are collapsing under the strain. The "bureaucratic anomaly" of monolithic review has become mathematically unsustainable, creating a stochastic "lottery" that punishes novelty and exhausts researchers.
  This paper, written from the perspective of 2036, documents potential solutions. Here, the editors of ASE, EMSE, IST, JSS, TOSEM and TSE dream a collective dream of a brighter future. In summary first we stopped fighting (The Journal Alliance). Then we fixed the process (The Lottery / Unbundling / Fixing the Benchmark Graveyard). And then we fixed the culture (Cathedrals/Bazaars).

</details>


### [140] [LLM-based Vulnerability Detection at Project Scale: An Empirical Study](https://arxiv.org/abs/2601.19239)
*Fengjie Li,Jiajun Jiang,Dongchi Chen,Yingfei Xiong*

Main category: cs.SE

TL;DR: LLM检测器在漏洞检测中召回率低但发现独特漏洞，误报率高且计算成本大，需改进鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 首次全面实证研究基于LLM的专用检测器，并与传统静态分析器在项目规模上进行比较。

Method: 通过内部基准（222个已知真实漏洞）和24个活跃开源项目（手动检查385个警告）评估5种最新LLM方法和2种传统工具。

Result: 1. LLM检测器在基准上召回率低，但发现更多独特漏洞；2. 两者在开源项目中警告多但误报率高；3. LLM方法计算成本高。

Conclusion: 当前基于LLM的检测器在鲁棒性、可靠性和可扩展性方面存在关键局限性，需要未来研究以实现更有效和实用的项目规模漏洞检测。

Abstract: In this paper, we present the first comprehensive empirical study of specialized LLM-based detectors and compare them with traditional static analyzers at the project scale. Specifically, our study evaluates five latest and representative LLM-based methods and two traditional tools using: 1) an in-house benchmark of 222 known real-world vulnerabilities (C/C++ and Java) to assess detection capability, and 2) 24 active open-source projects, where we manually inspected 385 warnings to assess their practical usability and underlying root causes of failures. Our evaluation yields three key findings: First, while LLM-based detectors exhibit low recall on the in-house benchmark, they still uncover more unique vulnerabilities than traditional tools. Second, in open-source projects, both LLM-based and traditional tools generate substantial warnings but suffer from very high false discovery rates, hindering practical use. Our manual analysis further reveals shallow interprocedural reasoning and misidentified source/sink pairs as primary failure causes, with LLM-based tools exhibiting additional unique failures. Finally, LLM-based methods incurs substantial computational costs-hundreds of thousands to hundreds of millions of tokens and multi-hour to multi-day runtimes. Overall, our findings underscore critical limitations in the robustness, reliability, and scalability of current LLM-based detectors. We ultimately summarize a set of implications for future research toward more effective and practical project-scale vulnerability detection.

</details>


### [141] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 论文研究了AI增强软件开发工具的能源效率，提出结合RAG和PETs的框架，并在多种LLM上验证了其提升代码生成质量和能源效率的效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件开发工具中的普及，其能源消耗对软件开发生命周期的影响日益显著，研究旨在探索如何提升AI工具的能源效率。

Method: 采用Retrieval-Augmented Generation (RAG)与Prompt Engineering Techniques (PETs)结合的方法，构建了一个实时测量能源消耗和推理时间的框架，测试了包括GPT-2、CodeLlama等在内的多种模型架构。

Result: 通过实验验证了所提框架在不同规模LLM（从125M到7B参数）上的有效性，为未来更深入的分析提供了概念验证。

Conclusion: 论文提出了一个结合RAG和PETs的框架，旨在提高基于LLM的代码生成的质量和能源效率，并通过不同参数规模的模型验证了其有效性。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [142] [Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code](https://arxiv.org/abs/2601.19264)
*Syed Mehedi Hasan Nirob,Shamim Ehsan,Moqsadur Rahman,Summit Haque*

Main category: cs.SE

TL;DR: 论文比较了基于特征和嵌入的两种方法，用于区分人类和AI生成的代码。结果显示两者均高效，但特征方法更易解释，嵌入方法语义理解更强，为实际应用提供了权衡参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）能够轻松从自然语言提示生成源代码，学术诚信、作者归属和负责任AI使用面临新风险。论文旨在解决如何区分人类编写和机器生成代码的问题。

Method: 论文采用了两种互补的方法：基于轻量级、可解释的代码风格和结构特征构建的检测器，以及利用预训练代码编码器的基于嵌入的检测器。使用了包含60万个人类编写和AI生成代码样本的大规模基准数据集进行实验。

Result: 基于特征的模型表现优异（ROC-AUC 0.995，PR-AUC 0.995，F1 0.971），基于嵌入的模型（使用CodeBERT嵌入）也非常有竞争力（ROC-AUC 0.994，PR-AUC 0.994，F1 0.965）。分析表明，缩进和空格相关特征提供显著区分线索，而嵌入捕获更深层语义模式且精度略高。

Conclusion: 研究发现，基于特征的模型和基于嵌入的模型在区分人类编写和机器生成的代码方面均表现出色，但各有优劣。基于特征的方法在可解释性上更优，而基于嵌入的方法在语义模式捕获上更强。这为学术和工业场景中部署代码来源检测提供了实用指导。

Abstract: Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.

</details>


### [143] [Understanding Dominant Themes in Reviewing Agentic AI-authored Code](https://arxiv.org/abs/2601.19287)
*Md. Asif Haider,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 本文通过大规模实证研究发现，AI生成的代码审查主要关注文档缺失、重构需求、样式和格式问题，以及测试和安全相关的问题，表明AI加速代码生产但仍需人工审查。


<details>
  <summary>Details</summary>
Motivation: 先前的研究探讨了Agentic AI系统的生成能力，但对审阅者如何实际响应AI生成的代码知之甚少。本文旨在通过大规模实证研究填补这一空白。

Method: 使用AIDev数据集的精选子集，分析了来自真实GitHub仓库的3,177个AI生成的PR中的19,450条内联评论。通过主题建模结合大型语言模型（LLM）辅助的语义聚类和整合，得出了12个评论主题的分类法。然后研究了零样本提示LLM是否能可靠地标注评论。

Result: 开源LLM在评论标注方面表现出较高的准确度（78.63%精确匹配，0.78宏F1分数），并且在PR级别上也能正确识别主导评论主题（78% Top-1准确度，0.76平均Jaccard相似度）。

Conclusion: 研究结果表明，尽管AI代理可以加速代码生产，但仍存在需要针对性人工审查监督的差距。

Abstract: While prior work has examined the generation capabilities of Agentic AI systems, little is known about how reviewers respond to AI-authored code in practice. In this paper, we present a large-scale empirical study of code review dynamics in agent-generated PRs. Using a curated subset of the AIDev dataset, we analyze 19,450 inline review comments spanning 3,177 agent-authored PRs from real-world GitHub repositories. We first derive a taxonomy of 12 review comment themes using topic modeling combined with large language model (LLM)-assisted semantic clustering and consolidation. According to this taxonomy, we then investigate whether zero-shot prompts to LLM can reliably annotate review comments. Our evaluation against human annotations shows that open-source LLM achieves reasonably high exact match (78.63%), macro F1 score (0.78), and substantial agreement with human annotators at the review comment level. At the PR level, the LLM also correctly identifies the dominant review theme with 78% Top-1 accuracy and achieves an average Jaccard similarity of 0.76, indicating strong alignment with human judgments. Applying this annotation pipeline at scale, we find that apart from functional correctness and logical changes, reviews of agent-authored PRs predominantly focus on documentation gaps, refactoring needs, styling and formatting issues, with testing and security-related concerns. These findings suggest that while AI agents can accelerate code production, there remain gaps requiring targeted human review oversight.

</details>


### [144] [Modeling Sampling Workflows for Code Repositories](https://arxiv.org/abs/2601.19316)
*Romain Lefeuvre,Maïwenn Le Goasteller,Jessie Galasso,Benoit Combemale,Quentin Perez,Houari Sahraoui*

Main category: cs.SE

TL;DR: 论文提出一种DSL，用于明确描述和推理代码仓库采样策略的代表性，并通过案例研究验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中采样策略的设计和评估对研究结果的泛化性至关重要，但目前采样仍被低估，存在设计和代表性不足的挑战。

Method: 提出了一种基于Python的流畅API实现的DSL，通过可组合的采样操作符明确描述复杂采样策略，并利用统计指标支持代表性推理。

Result: DSL能够建模近期文献中的采样策略，并通过MSR论文的案例研究验证了其有效性。

Conclusion: 该论文提出的DSL能够有效建模近期文献中报告的采样策略，并通过案例研究验证了其可行性。

Abstract: Empirical software engineering research often depends on datasets of code repository artifacts, where sampling strategies are employed to enable large-scale analyses. The design and evaluation of these strategies are critical, as they directly influence the generalizability of research findings. However, sampling remains an underestimated aspect in software engineering research: we identify two main challenges related to (1) the design and representativeness of sampling approaches, and (2) the ability to reason about the implications of sampling decisions on generalizability. To address these challenges, we propose a Domain-Specific Language (DSL) to explicitly describe complex sampling strategies through composable sampling operators. This formalism supports both the specification and the reasoning about the generalizability of results based on the applied sampling strategies. We implement the DSL as a Python-based fluent API, and demonstrate how it facilitates representativeness reasoning using statistical indicators extracted from sampling workflows. We validate our approach through a case study of MSR papers involving code repository sampling. Our results show that the DSL can model the sampling strategies reported in recent literature.

</details>


### [145] [High-quality data augmentation for code comment classification](https://arxiv.org/abs/2601.19383)
*Thomas Borsani,Andrea Rosani,Giuseppe Di Fatta*

Main category: cs.SE

TL;DR: 论文提出Q-SYNTH技术，通过合成数据增强和过采样解决代码注释数据集规模小和类别不平衡问题，提升分类器性能2.56%。


<details>
  <summary>Details</summary>
Motivation: 现有代码注释数据集因依赖人工标注而存在规模小和类别不平衡问题，无法准确反映真实代码库中的注释分布。

Method: 采用合成过采样和增强技术（Q-SYNTH），基于高质量数据生成来增强NLBSE'26挑战数据集。

Result: Q-SYNTH技术将基础分类器的性能提高了2.56%。

Conclusion: Q-SYNTH技术通过合成高质量数据增强和过采样，显著提升了分类器的性能，为解决代码注释数据集规模小和类别不平衡问题提供了有效方案。

Abstract: Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\%$.

</details>


### [146] [Bridging the Socio-Emotional Gap: The Functional Dimension of Human-AI Collaboration for Software Engineering](https://arxiv.org/abs/2601.19387)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 研究表明，AI在协作中的社交情感差距是功能性而非情感性的，建议通过功能性设计而非模仿人类SEI特质来提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI模型被用于支持软件工程师及其开发团队，理解有效的人机协作（HAIC）变得越来越重要。当前AI系统缺乏人类团队协作中的社交情感智能（SEI）能力，可能导致协作动态中的潜在差距。

Method: 通过半结构化访谈10名从业者，探讨了他们与人类及AI队友协作时的社交情感智能（SEI）期望和AI能力设想。

Result: 从业者目前将AI模型视为智力队友而非社交伙伴，对它们的SEI属性期望低于人类队友。他们认为社交情感差距是AI在协作能力（如协商责任、情境适应或维持持续伙伴关系）上的功能性差距，而非缺乏SEI特质。研究提出了功能性等价物的概念，即通过技术能力（如内部认知、情境智能、自适应学习和协作智能）实现与人类SEI属性相当的协作效果。

Conclusion: 研究发现，与AI的有效协作可能更依赖于功能性设计而非复制人类的社交情感智能（SEI）特质，从而将协作重新定义为功能对齐。

Abstract: As GenAI models are adopted to support software engineers and their development teams, understanding effective human-AI collaboration (HAIC) is increasingly important. Socio-emotional intelligence (SEI) enhances collaboration among human teammates, but its role in HAIC remains unclear. Current AI systems lack SEI capabilities that humans bring to teamwork, creating a potential gap in collaborative dynamics. In this study, we investigate how software practitioners perceive the socio-emotional gap in HAIC and what capabilities AI systems require for effective collaboration. Through semi-structured interviews with 10 practitioners, we examine how they think about collaborating with human versus AI teammates, focusing on their SEI expectations and the AI capabilities they envision. Results indicate that practitioners currently view AI models as intellectual teammates rather than social partners and expect fewer SEI attributes from them than from human teammates. However, they see the socio-emotional gap not as AIs failure to exhibit SEI traits, but as a functional gap in collaborative capabilities (AIs inability to negotiate responsibilities, adapt contextually, or maintain sustained partnerships). We introduce the concept of functional equivalents: technical capabilities (internal cognition, contextual intelligence, adaptive learning, and collaborative intelligence) that achieve collaborative outcomes comparable to human SEI attributes. Our findings suggest that effective collaboration with AI for SE tasks may benefit from functional design rather than replicating human SEI traits for SE tasks, thereby redefining collaboration as functional alignment.

</details>


### [147] [AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context](https://arxiv.org/abs/2601.19494)
*Lei Zhang,Yongda Yu,Minghui Yu,Xinxin Guo,Zhengqi Zhuang,Guoping Rong,Dong Shao,Haifeng Shen,Hongyu Kuang,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobing Xu*

Main category: cs.SE

TL;DR: AACR-Bench是一个多语言、跨文件的自动化代码审查基准，通过AI辅助和专家验证提升缺陷覆盖率，为LLM评估提供更严格标准和新见解。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准在多语言支持和噪声数据上的局限性，提升评估结果的通用性和缺陷覆盖率。

Method: 采用“AI辅助、专家验证”的注释流程，构建支持多语言和跨文件上下文的综合基准AACR-Bench。

Result: AACR-Bench显著提高了缺陷覆盖率（285%），并揭示了主流LLM在传统评估中可能被低估或部分捕捉的能力。

Conclusion: AACR-Bench 为自动化代码审查（ACR）评估设立了更严格的标准，并提供了关于LLM在ACR中应用的新见解，如上下文粒度、检索方法选择对性能的影响。

Abstract: High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an "AI-assisted, Expert-verified" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .

</details>


### [148] [From Scattered to Structured: A Vision for Automating Architectural Knowledge Management](https://arxiv.org/abs/2601.19548)
*Jan Keim,Angelika Kaplan*

Main category: cs.SE

TL;DR: 提出自动化管道，从多样软件制品中提取并整合架构知识，构建结构化知识库以支持架构维护和问答。


<details>
  <summary>Details</summary>
Motivation: 软件架构知识分散在多种异构制品中，难以有效访问和利用，且系统演化过程中制品间的不一致性导致架构侵蚀和维护困难。

Method: 开发针对不同制品类型的专门提取器，设计统一的知识表示模式，实现一致性检查机制，并整合检索增强生成技术以实现对话式知识访问。

Result: 通过自动化管道提取、链接和整合架构知识，构建结构化知识库，支持架构一致性检查、变更影响分析及自然语言问答。

Conclusion: 该论文提出了一个自动化管道，用于系统地从多样化的软件制品中提取架构知识，并通过链接、解决不一致性及整合知识到结构化知识库中，以支持架构一致性检查和变更影响分析等关键活动。

Abstract: Software architecture is inherently knowledge-centric. The architectural knowledge is distributed across heterogeneous software artifacts such as requirements documents, design diagrams, code, and documentation, making it difficult for developers to access and utilize this knowledge effectively. Moreover, as systems evolve, inconsistencies frequently emerge between these artifacts, leading to architectural erosion and impeding maintenance activities. We envision an automated pipeline that systematically extracts architectural knowledge from diverse artifacts, links them, identifies and resolves inconsistencies, and consolidates this knowledge into a structured knowledge base. This knowledge base enables critical activities such as architecture conformance checking and change impact analysis, while supporting natural language question-answering to improve access to architectural knowledge. To realize this vision, we plan to develop specialized extractors for different artifact types, design a unified knowledge representation schema, implement consistency checking mechanisms, and integrate retrieval-augmented generation techniques for conversational knowledge access.

</details>


### [149] [Toward Architecture-Aware Evaluation Metrics for LLM Agents](https://arxiv.org/abs/2601.19583)
*Débora Souza,Patrícia Machado*

Main category: cs.SE

TL;DR: 本文提出一种架构感知的评估方法，通过关联代理组件与行为和指标，改进基于LLM的代理评估的针对性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法过于分散且以模型为中心，忽视了架构组件对代理行为的影响，限制了诊断能力。

Method: 采用架构感知的方法，明确代理组件（如规划器、记忆和工具路由器）如何影响行为，并确定相应的评估指标。

Result: 通过实际代理的应用展示了该方法，明确了应测量什么及为何测量，从而提升了评估的针对性和透明度。

Conclusion: 本文提出了一种轻量级、基于架构的方法，通过将代理组件与其可观察行为及评估指标联系起来，实现了对基于LLM的代理更有针对性、透明和可操作的评估。

Abstract: LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.

</details>


### [150] [The Competence Crisis: A Design Fiction on AI-Assisted Research in Software Engineering](https://arxiv.org/abs/2601.19628)
*Mairieli Wessel,Daniel Feitosa,Sangeeth Kochanthara*

Main category: cs.SE

TL;DR: 论文利用设计虚构方法，分析生成式AI和出版压力对软件工程研究的潜在负面影响，呼吁社区反思未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨出版压力上升和生成式AI工具普及对软件工程研究产生的潜在影响，特别是技能退化、责任归属和学术信任问题。

Method: 采用设计虚构（Design Fiction）作为方法论视角，构建一个近未来研究场景的推测性案例。

Result: 通过虚构场景揭示了自动化辅助可能削弱领域知识能力、验证和指导实践的风险。

Conclusion: 论文通过设计虚构的方法探讨了当前出版压力和生成式AI工具的使用可能对软件工程研究社区带来的负面影响，呼吁社区对未来研究能力、责任分配和学习支持进行深入讨论。

Abstract: Rising publication pressure and the routine use of generative AI tools are reshaping how software engineering research is produced, assessed, and taught. While these developments promise efficiency, they also raise concerns about skill degradation, responsibility, and trust in scholarly outputs. This vision paper employs Design Fiction as a methodological lens to examine how such concerns might materialise if current practices persist. Drawing on themes reported in a recent community survey, we construct a speculative artifact situated in a near future research setting. The fiction is used as an analytical device rather than a forecast, enabling reflection on how automated assistance might impede domain knowledge competence, verification, and mentoring practices. By presenting an intentionally unsettling scenario, the paper invites discussion on how the software engineering research community in the future will define proficiency, allocate responsibility, and support learning.

</details>


### [151] [Who Said CVE? How Vulnerability Identifiers Are Mentioned by Humans, Bots, and Agents in Pull Requests](https://arxiv.org/abs/2601.19636)
*Pien Rooijendijk,Christoph Treude,Mairieli Wessel*

Main category: cs.SE

TL;DR: 研究比较了GitHub拉取请求中机器人、自主代理和人类开发者对漏洞标识符的使用差异，发现机器人使用频率最高但用途单一，人类和代理用途更广泛。


<details>
  <summary>Details</summary>
Motivation: 了解漏洞标识符在实际开发中的使用情况，尤其是不同作者类型的使用差异。

Method: 使用AIDev pop数据集及增强的拉取请求集，分析漏洞标识符的提及者及出现位置。

Result: 机器人占所有提及的69.1%，主要用于自动化依赖更新和审计；人类和代理提及较少但用途多样，涉及修复、维护和讨论。

Conclusion: 漏洞标识符（如CVE、CWE、GHSA）在GitHub拉取请求中的使用因作者类型（机器人、自主代理、人类开发者）而异，机器人使用频率最高但用途单一，人类和代理使用较少但用途更广泛。

Abstract: Vulnerability identifiers such as CVE, CWE, and GHSA are standardised references to known software security issues, yet their use in practice is not well understood. This paper compares vulnerability ID use in GitHub pull requests authored by autonomous agents, bots, and human developers. Using the AIDev pop dataset and an augmented set of pull requests from the same repositories, we analyse who mentions vulnerability identifiers and where they appear. Bots account for around 69.1% of all mentions, usually adding few identifiers in pull request descriptions, while human and agent mentions are rarer but span more locations. Qualitative analysis shows that bots mainly reference identifiers in automated dependency updates and audits, whereas humans and agents use them to support fixes, maintenance, and discussion.

</details>


### [152] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 研究显示LLM在评估架构文档时表现与文档质量正相关，但存在不一致性，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在软件工程活动中的实际效益，特别是在支持软件架构师改进架构文档方面。

Method: 在开发数字市场的研究项目中，使用不同LLM分析架构文档质量，并与软件架构师的评估结果进行比较。

Result: 架构文档的质量对LLM评估的准确性有显著影响，文档质量越高，LLM与人类专家的评估结果越一致。

Conclusion: 使用LLM支持架构文档评估具有潜力，但结果存在不一致性，需进一步分析才能推广。

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [153] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AlignCoder通过查询增强和强化学习训练检索器，显著提升了仓库级代码补全的性能，并在多个测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型在仓库级代码补全任务中表现不佳，主要由于对仓库特定上下文和领域知识的理解有限，以及检索增强生成方法存在的查询与目标代码不对齐和检索信息利用不足的问题。

Method: 提出了AlignCoder框架，包括查询增强机制和基于强化学习的检索器训练方法，以解决现有方法在查询与目标代码对齐和检索信息利用方面的不足。

Result: 在CrossCodeEval和RepoEval两个基准测试上，AlignCoder在五种骨干代码大语言模型上实现了18.1%的EM分数提升，显示出卓越的性能和通用性。

Conclusion: AlignCoder框架通过引入查询增强机制和基于强化学习的检索器训练方法，显著提升了仓库级代码补全的性能，并在多个基准测试和编程语言中展现出高度的通用性。

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


### [154] [Future of Software Engineering Research: The SIGSOFT Perspective](https://arxiv.org/abs/2601.19731)
*Massimiliano Di Penta,Kelly Blincoe,Marsha Chechik,Claire Le Goues,David Lo,Emerson Murphy-Hill,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 论文提出通过提升透明度、实验混合展示和扩大推广，解决软件工程会议参与障碍，促进包容性。


<details>
  <summary>Details</summary>
Motivation: 软件工程会议规模扩大导致成本上升和形式过时，阻碍了许多研究人员的参与，威胁社区的包容性和全球多样性。

Method: 基于调查数据，识别具体行动建议。

Result: 提出了包括提高会议资金透明度、尝试混合海报展示和扩大对 underrepresented 地区推广的具体措施。

Conclusion: 通过实施透明度提升、混合海报展示实验及扩大对 underrepresented 地区的推广，SIGSOFT 可以确保软件工程社区的持续可及性和包容性。

Abstract: As software engineering conferences grow in size, rising costs and outdated formats are creating barriers to participation for many researchers. These barriers threaten the inclusivity and global diversity that have contributed to the success of the SE community. Based on survey data, we identify concrete actions the ACM Special Interest Group on Software Engineering (SIGSOFT) can take to address these challenges, including improving transparency around conference funding, experimenting with hybrid poster presentations, and expanding outreach to underrepresented regions. By implementing these changes, SIGSOFT can help ensure the software engineering community remains accessible and welcoming.

</details>


### [155] [Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow](https://arxiv.org/abs/2601.19787)
*Elena Masserini,Diego Clerissi,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文介绍了TOFU-D和COD两个数据集，用于聊天机器人质量和安全性的实证研究，初步评估显示存在测试覆盖不足和安全漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模精选数据集限制了聊天机器人质量和可靠性的研究。

Method: 提出了TOFU-D和COD两个数据集，并使用Botium测试框架和Bandit静态分析器进行了初步评估。

Result: 初步评估揭示了测试覆盖率的不足和多个聊天机器人中频繁出现的安全漏洞。

Conclusion: 本文强调了系统性、多平台研究对提升聊天机器人质量和安全性的必要性。

Abstract: In recent years, chatbots have gained widespread adoption thanks to their ability to assist users at any time and across diverse domains. However, the lack of large-scale curated datasets limits research on their quality and reliability. This paper presents TOFU-D, a snapshot of 1,788 Dialogflow chatbots from GitHub, and COD, a curated subset of TOFU-D including 185 validated chatbots. The two datasets capture a wide range of domains, languages, and implementation patterns, offering a sound basis for empirical studies on chatbot quality and security. A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities in several chatbots, highlighting the need for systematic, multi-Platform research on chatbot quality and security.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [156] [The Last Mile to Production Readiness: Physics-Based Motion Refinement for Video-Based Capture](https://arxiv.org/abs/2601.19036)
*Tianxin Tao,Han Liu,Hung Yu Ling*

Main category: cs.GR

TL;DR: 本文总结了运动捕捉中的关键问题，并提出一个基于物理的框架以减少手动清理工作，提升视觉和物理真实感。


<details>
  <summary>Details</summary>
Motivation: 尽管基于视觉的运动捕捉工具取得了显著进展，但在物理真实性和生产准备度方面仍存在不足，主要由于捕捉过程中引入的各种伪影。

Method: 通过案例研究和专业动画师的反馈总结关键问题，提出了一个物理基础的运动精炼框架。

Result: 提出的框架能够有效减少手动清理工作，提升运动的视觉质量和物理真实感，支持单人和多人角色序列。

Conclusion: 本文提出了一个基于物理的运动精炼框架，旨在减少劳动密集型的手动清理工作，并提升视觉质量和物理真实感。该框架支持单人和多人角色序列，并可集成到动画师的工作流程中，用于进一步的细化，如通过关键帧编辑实现运动风格化。

Abstract: High-quality motion data underpins games, film, XR, and robotics. Vision-based motion capture tools have made significant progress, offering accessible and visually convincing results, yet often fall short in the final stretch -- the last mile -- when it comes to physical realism and production readiness, due to various artifacts introduced during capture. In this paper, we summarize key issues through case studies and feedback from professional animators to set a stepping stone for future research in motion cleanup. We then present a physics-based motion refinement framework to bridge the gap, with the goal of reducing labor-intensive manual cleanup and enhancing visual quality and physical realism. Our framework supports both single- and multi-character sequences and can be integrated into animator workflows for further refinement, such as stylizing motions via keyframe editing.

</details>


### [157] [UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation](https://arxiv.org/abs/2601.19233)
*Zeyu Xiao,Mingyang Sun,Yimin Cong,Lintao Wang,Dongliang Kou,Zhenyi Wu,Dingkang Yang,Peng Zhai,Zeyu Wang,Lihua Zhang*

Main category: cs.GR

TL;DR: UniMGS是首个统一网格和3D高斯泼溅的单通道抗锯齿渲染框架，通过新型绑定策略解决变形伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究因表示和渲染管道的差异，难以准确处理网格和3DGS的遮挡和透明效果，且变形后的3DGS因对代理网格拓扑质量的敏感性而产生视觉伪影，阻碍了二者的联合使用。

Method: UniMGS采用单通道抗锯齿α混合技术，将三角形和高斯片段的颜色混合，实现视觉一致的结果，并通过高斯中心绑定策略将高斯与网格面空间关联，减少渲染伪影。

Result: UniMGS在单通道中实现了网格和3DGS的视觉一致渲染，显著减少了变形3DGS的视觉伪影，为图形应用提供了统一框架。

Conclusion: UniMGS提出了一种统一框架，首次实现了网格和3D高斯泼溅（3DGS）的单通道抗锯齿渲染，并通过基于代理网格的新型绑定策略解决了3DGS变形中的视觉伪影问题，为图形应用开辟了新可能性。

Abstract: Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions. However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency. Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh. These issues pose serious obsta cles to the joint use of 3DGS and meshes, making it diffi cult to adapt 3DGS to conventional mesh-oriented graphics pipelines. We propose UniMGS, the first unified framework for rasterizing mesh and 3DGS in a single-pass anti-aliased manner, with a novel binding strategy for 3DGS deformation based on proxy mesh. Our key insight is to blend the col ors of both triangle and Gaussian fragments by anti-aliased α-blending in a single pass, achieving visually coherent re sults with precise handling of occlusion and transparency. To improve the visual appearance of the deformed 3DGS, our Gaussian-centric binding strategy employs a proxy mesh and spatially associates Gaussians with the mesh faces, signifi cantly reducing rendering artifacts. With these two compo nents, UniMGS enables the visualization and manipulation of 3D objects represented by mesh or 3DGS within a unified framework, opening up new possibilities in embodied AI, vir tual reality, and gaming. We will release our source code to facilitate future research.

</details>


### [158] [Words have Weight: Comparing the use of pressure and weight as a metaphor in a User Interface in Virtual Reality](https://arxiv.org/abs/2601.19294)
*Joffrey Guilmet,Suzanne Sorli,Diego Vilela Monteiro*

Main category: cs.GR

TL;DR: 研究发现压力增强的重量能丰富VR触觉渲染，但对传达通知紧急性效果有限，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨重量和压力如何作为触觉隐喻支持VR中的用户界面通知，填补现有研究中未充分探索的重量模拟与气动反馈结合在传达信息方面的空白。

Method: 开发了一种可穿戴触觉设备，通过向用户手背上的柔性容器输送液体和空气，独立操纵重量和压力。通过三种条件（无反馈、仅重量、重量加压力）的初步评估，研究了这些信号如何影响感知重量、与视觉提示的一致性以及通知的感知紧急性。

Result: 验证了压力增强了重量的感知，但这种增加的重量并未转化为更高的感知紧急性。

Conclusion: 压力增强的重量可以丰富VR中UI元素的触觉渲染，但在传达紧急性方面的贡献可能需要进一步研究、替代压力配置或不同类型的通知。

Abstract: This work investigates how weight and pressure can function as haptic metaphors to support user interface notifications in Virtual Reality (VR). While prior research has explored ungrounded weight simulation and pneumatic feedback, their combined role in conveying information through UI elements remains underexplored. We developed a wearable haptic device that transfers liquid and air into flexible containers mounted on the back of the user's hand, allowing us to independently manipulate weight and pressure. Through an initial evaluation using three conditions-no feedback, weight only, and weight combined with pressure-we examined how these signals affect perceived heaviness, coherence with visual cues, and the perceived urgency of notifications. Our results validate that pressure amplifies the perception of weight, but this increased heaviness does not translate into higher perceived urgency. These findings suggest that while pressure___enhanced weight can enrich haptic rendering of UI elements in VR, its contribution to communicating urgency may require further investigation, alternative pressure profiles, or different types of notifications.

</details>


### [159] [ClipGS-VR: Immersive and Interactive Cinematic Visualization of Volumetric Medical Data in Mobile Virtual Reality](https://arxiv.org/abs/2601.19310)
*Yuqi Tong,Ruiyang Li,Chengkun Li,Qixuan Liu,Shi Qiu,Pheng-Ann Heng*

Main category: cs.GR

TL;DR: ClipGS-VR通过整合预计算切片和梯度调制，在移动VR上实现了高保真任意角度切片，性能优于离线结果。


<details>
  <summary>Details</summary>
Motivation: 移动VR设备上的高保真医学可视化仍具挑战性，ClipGS虽支持横截面探索，但缺乏在消费级VR头显上的任意角度切片功能。

Method: 我们重构了ClipGS的神经推断为一个整合的数据集，将多个预计算切片状态的高保真层集成到一个统一的渲染结构中，并通过基于梯度的不透明度调制支持任意角度切片。

Result: 评估证实，我们的方法在保持与离线结果相当的视觉保真度的同时，提供了更优的可用性和交互效率。

Conclusion: ClipGS-VR通过整合预计算切片状态和基于梯度的不透明度调制，实现了在移动VR设备上高保真且交互式的任意角度切片渲染，其视觉保真度与离线结果相当，同时提供了更高的可用性和交互效率。

Abstract: High-fidelity cinematic medical visualization on mobile virtual reality (VR) remains challenging. Although ClipGS enables cross-sectional exploration via 3D Gaussian Splatting, it lacks arbitrary-angle slicing on consumer-grade VR headsets. To achieve real-time interactive performance, we introduce ClipGS-VR and restructure ClipGS's neural inference into a consolidated dataset, integrating high-fidelity layers from multiple pre-computed slicing states into a unified rendering structure. Our framework further supports arbitrary-angle slicing via gradient-based opacity modulation for smooth, visually coherent rendering. Evaluations confirm our approach maintains visual fidelity comparable to offline results while offering superior usability and interaction efficiency.

</details>


### [160] [It's Not Just a Phase: Creating Phase-Aligned Peripheral Metamers](https://arxiv.org/abs/2601.19425)
*Sophie Kergaßner,Piotr Didyk*

Main category: cs.GR

TL;DR: 本文提出一种利用局部图像统计合成高频细节的方法，减少精确渲染需求，提升感知质量，成本效益高。


<details>
  <summary>Details</summary>
Motivation: 新型显示技术能提供广视角的高质量图像，但渲染成本高。由于人眼在周边视觉对细节的敏感度较低，利用视觉感知的局限性可以实现高效渲染。

Method: 从中心凹内容中提取多种局部图像统计信息，并将其外推到高频范围，利用这些统计信息合成信号以增强初始渲染的感知质量。特别关注相位信息在空间和频率上的对齐。

Result: 该方法与现有先进策略相比，显著减少了需要精确渲染的内容，且合成额外信号的额外成本相对较小。

Conclusion: 本文提出了一种基于局部图像统计的方法，通过合成高频细节信号来替代昂贵的渲染过程，显著减少了需要精确渲染的内容，同时以较小的额外成本提升了感知质量。

Abstract: Novel display technologies can deliver high-quality images across a wide field of view, creating immersive experiences. While rendering for such devices is expensive, most of the content falls into peripheral vision, where human perception differs from that in the fovea. Consequently, it is critical to understand and leverage the limitations of visual perception to enable efficient rendering. A standard approach is to exploit the reduced sensitivity to spatial details in the periphery by reducing rendering resolution, so-called foveated rendering. While this strategy avoids rendering part of the content altogether, an alternative promising direction is to replace accurate and expensive rendering with inexpensive synthesis of content that is perceptually indistinguishable from the ground-truth image. In this paper, we propose such a method for the efficient generation of an image signal that substitutes the rendering of high-frequency details. The method is grounded in findings from image statistics, which show that preserving appropriate local statistics is critical for perceived image quality. Based on this insight, we extrapolate several local image statistics from foveated content into higher spatial frequency ranges that are attenuated or omitted in the rendering process. This rich set of statistics is later used to synthesize a signal that is added to the initial rendering, boosting its perceived quality. We focus on phase information, demonstrating the importance of its alignment across space and frequencies. We calibrate and compare our method with state-of-the-art strategies, showing a significant reduction in the content that must be accurately rendered at a relatively small extra cost for synthesizing the additional signal.

</details>


### [161] [Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty](https://arxiv.org/abs/2601.19843)
*Doga Yilmaz,Jialin Zhu,Deshan Gong,He Wang*

Main category: cs.GR

TL;DR: GraphiXS是一个新的概率框架，用于将数据不确定性纳入高斯泼溅，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅作为神经渲染的新范式，数据不确定性（如视图稀疏性、缺失帧、相机异步等）是一个未被充分探索但亟需解决的领域。

Method: 提出了GraphiXS，一个新的概率框架，考虑多种类型的数据不确定性，将当前的4D高斯泼溅范式扩展至概率设置。

Result: GraphiXS能够系统地建模数据不确定性，在数据缺失或污染的情况下表现优于现有方法。

Conclusion: GraphiXS是一个重要的4D高斯泼溅研究的泛化，能够系统地建模数据中的各种不确定性，并在数据缺失或污染的情况下优于现有方法。

Abstract: We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting. Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed. However, one orthogonal, much needed, but under-explored area is data uncertainty. In standard 4D Gaussian Splatting, data uncertainty can manifest as view sparsity, missing frames, camera asynchronization, etc. So far, there has been little research to holistically incorporating various types of data uncertainty under a single framework. To this end, we propose Graphical X Splatting, or GraphiXS, a new probabilistic framework that considers multiple types of data uncertainty, aiming for a fundamental augmentation of the current 4D Gaussian Splatting paradigm into a probabilistic setting. GraphiXS is general and can be instantiated with a range of primitives, e.g. Gaussians, Student's-t. Furthermore, GraphiXS can be used to `upgrade' existing methods to accommodate data uncertainty. Through exhaustive evaluation and comparison, we demonstrate that GraphiXS can systematically model various uncertainties in data, outperform existing methods in many settings where data are missing or polluted in space and time, and therefore is a major generalization of the current 4D Gaussian Splatting research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity](https://arxiv.org/abs/2601.19144)
*Tzvika Geft,William Zhang,Jingjin Yu,Kostas Bekris*

Main category: cs.RO

TL;DR: 本文提出了一个自动化存储系统框架，通过$k$-有界扰动处理不确定性，证明$Θ(k)$网格宽度可消除重定位，并开发高效求解器。实验显示，该框架显著减少重定位。


<details>
  <summary>Details</summary>
Motivation: 自动化存储系统在物流应用中（如最后一英里配送中心和船厂）需要处理存储和检索序列的不确定性。此前的研究保证了在已知序列下的零重定位解决方案，但在实际中检索序列可能会变化。因此，本研究旨在解决这种不确定性，特别是在$k$-有界扰动下的存储和检索效率问题。

Method: 本文提出了一种基于2D网格的存储系统框架，用于处理均匀尺寸负载的存储和检索。通过考虑$k$-有界扰动，研究证明了$Θ(k)$网格宽度的必要性，并开发了一个高效的求解器来计算对扰动鲁棒的存储排列。对于超出$k$的扰动，引入了一种策略以最小化重定位。

Result: 实验表明，对于$k$值高达网格宽度一半的情况，提出的框架几乎消除了重定位。对于$k$值高达全网格宽度的情况，重定位减少了50%以上。

Conclusion: 本研究提出了一个框架，用于在不确定性下提高自动化存储系统的操作效率，特别是在面对$k$-有界扰动时。通过证明$Θ(k)$网格宽度的必要性和充分性，并提供高效的求解器，该框架在最大容量下几乎消除了重定位。对于超出$k$的高不确定性情况，提出的策略也能有效减少重定位。

Abstract: This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty. It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid. The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards. The objective is to minimize the load relocations to ensure efficient retrieval. A previous result guarantees a zero-relocation solution for known storage and retrieval sequences, even for storage at full capacity, provided that the side of the grid through which loads are stored/retrieved is at least 3 cells wide. However, in practice, the retrieval sequence can change after the storage phase. To address such uncertainty, this work investigates \emph{$k$-bounded perturbations} during retrieval, under which any two loads may depart out of order if they are originally at most $k$ positions apart. We prove that a $Θ(k)$ grid width is necessary and sufficient for eliminating relocations at maximum capacity. We also provide an efficient solver for computing a storage arrangement that is robust to such perturbations. To address the higher-uncertainty case where perturbations exceed $k$, a strategy is introduced to effectively minimize relocations. Extensive experiments show that, for $k$ up to half the grid width, the proposed storage-retrieval framework essentially eliminates relocations. For $k$ values up to the full grid width, relocations are reduced by $50\%+$.

</details>


### [163] [Learning the Pareto Space of Multi-Objective Autonomous Driving: A Modular, Data-Driven Approach](https://arxiv.org/abs/2601.18913)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.RO

TL;DR: 该研究提出一个框架，通过自然轨迹数据平衡自动驾驶的安全、效率和交互，发现帕累托最优状态罕见但表现更优。


<details>
  <summary>Details</summary>
Motivation: 设计自动驾驶代理和理解其在现实世界中的行为需要平衡安全、效率和交互。研究旨在直接从自然轨迹数据中推导这些权衡关系。

Method: 研究引入了一个统一的客观空间，通过安全、效率和交互的复合分数表示每个自动驾驶时间步。应用帕累托优势识别非支配状态，形成实证前沿。

Result: 结果显示，仅有0.23%的自动驾驶实例达到帕累托最优状态，且这些状态在安全、效率和交互方面的平均分数显著高于非最优状态，其中交互的改进潜力最大。

Conclusion: 该研究提出了一个基于自然轨迹数据的实证学习框架，用于平衡自动驾驶代理的安全、效率和交互性能。通过帕累托优势识别非支配状态，形成可实现的平衡性能区域。结果表明，同时优化多个目标的情况极为罕见，但帕累托最优状态在安全、效率和交互方面表现显著更好。

Abstract: Balancing safety, efficiency, and interaction is fundamental to designing autonomous driving agents and to understanding autonomous vehicle (AV) behavior in real-world operation. This study introduces an empirical learning framework that derives these trade-offs directly from naturalistic trajectory data. A unified objective space represents each AV timestep through composite scores of safety, efficiency, and interaction. Pareto dominance is applied to identify non-dominated states, forming an empirical frontier that defines the attainable region of balanced performance.
  The proposed framework was demonstrated using the Third Generation Simulation (TGSIM) datasets from Foggy Bottom and I-395. Results showed that only 0.23\% of AV driving instances were Pareto-optimal, underscoring the rarity of simultaneous optimization across objectives. Pareto-optimal states showed notably higher mean scores for safety, efficiency, and interaction compared to non-optimal cases, with interaction showing the greatest potential for improvement.
  This minimally invasive and modular framework, which requires only kinematic and positional data, can be directly applied beyond the scope of this study to derive and visualize multi-objective learning surfaces

</details>


### [164] [DeFM: Learning Foundation Representations from Depth for Robotics](https://arxiv.org/abs/2601.18923)
*Manthan Patel,Jonas Frey,Mayank Mittal,Fan Yang,Alexander Hansson,Amir Bar,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: DeFM是一个自监督的深度图像基础模型，通过大规模训练和独特归一化策略，实现了跨任务和环境的泛化，适用于资源受限的机器人系统。


<details>
  <summary>Details</summary>
Motivation: 深度传感器的广泛应用与深度模拟技术的进步促使机器人策略在深度观测上实现鲁棒的仿真到现实迁移，但深度模态的表征学习仍落后于RGB模态。

Method: 采用DINO风格的自蒸馏目标在6000万深度图像数据集上训练，并引入新颖的输入归一化策略以保持多尺度度量感知。

Result: DeFM在分类、分割、导航、运动及操作等基准测试中达到最先进性能，并展示了从仿真到现实环境的强泛化能力。

Conclusion: DeFM作为深度模态的自监督基础模型，在多种任务和环境中展现了强大的泛化能力，无需任务特定微调即可直接应用于基于深度的机器人学习。

Abstract: Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/

</details>


### [165] [Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot](https://arxiv.org/abs/2601.18963)
*Fauna Robotics,:,Diego Aldarondo,Ana Pervan,Daniel Corbalan,Dave Petrillo,Bolun Dai,Aadhithya Iyer,Nina Mortensen,Erik Pearson,Sridhar Pandian Arunachalam,Emma Reznick,David Weis,Jacob Davison,Samuel Patterson,Tess Carella,Michael Suguitan,David Ye,Oswaldo Ferro,Nilesh Suriyarachchi,Spencer Ling,Erik Su,Daniel Giebisch,Peter Traver,Sam Fonseca,Mack Mor,Rohan Singh,Sertac Guven,Kangni Liu,Yaswanth Kumar Orru,Ashiq Rahman Anwar Batcha,Shruthi Ravindranath,Silky Arora,Hugo Ponte,Dez Hernandez,Utsav Chaudhary,Zack Walker,Michael Kelberman,Ivan Veloz,Christina Santa Lucia,Kat Casale,Helen Han,Michael Gromis,Michael Mignatti,Jason Reisman,Kelleher Guerin,Dario Narvaez,Christopher Anderson,Anthony Moschella,Robert Cochran,Josh Merel*

Main category: cs.RO

TL;DR: Sprout是一个专注于安全性、表达性和开发者可访问性的类人机器人开发平台，旨在解决现有平台难以在人类环境中部署的问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏适合在人类环境中安全、表达性强且长期部署的通用机器人控制器平台，现有类人机器人多为封闭工业系统或难以在人群中部署的学术原型。

Method: Sprout采用轻量化外形、顺应控制、有限的关节扭矩和柔软外壳，支持在共享人类空间中的安全操作。平台集成了全身控制、带有集成夹持器的操作以及基于虚拟现实的远程操作。

Result: Sprout平台通过降低部署的物理和技术障碍，扩展了对高性能类人机器人平台的访问。

Conclusion: Sprout平台通过强调安全性、表达性和开发者可访问性，为在真实人类环境中开发具身智能提供了实用基础。

Abstract: Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.

</details>


### [166] [A Switching Nonlinear Model Predictive Control Strategy for Safe Collision Handling by an Underwater Vehicle-Manipulator System](https://arxiv.org/abs/2601.18971)
*Ioannis G. Polyzos,Konstantinos J. Kyriakopoulos*

Main category: cs.RO

TL;DR: 本文提出了一种NMPC策略，用于水下车辆-机械臂系统的安全碰撞处理，通过虚拟实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境中自主车辆的主动干预任务是一个新兴研究领域，机器人可能因各种原因与障碍物发生碰撞，需要一种安全有效的控制策略。

Method: 采用切换非线性模型预测控制（NMPC）策略，结合机械臂的主动干预能力，处理水下环境中的碰撞问题。

Result: 虚拟实验表明，该算法能成功检测碰撞，并通过避免或利用机械臂适当处理碰撞，保护车辆的敏感区域。

Conclusion: 本文提出了一种切换非线性模型预测控制（NMPC）策略，用于水下车辆-机械臂系统（UVMS）的安全碰撞处理。通过虚拟实验验证了该算法在检测碰撞并避免或利用机械臂适当处理碰撞方面的有效性。

Abstract: For active intervention tasks in underwater environments, the use of autonomous vehicles is just now emerging as an active area of research. During operation, for various reasons, the robot might find itself on a collision course with an obstacle in its environment. In this paper, a switching Nonlinear Model Predictive Control (NMPC) strategy is proposed to safely handle collisions for an Underwater Vehicle-Manipulator System (UVMS). When avoiding the collision is impossible, the control algorithm takes advantage of the manipulator, using it to push against the obstacle, and deflect away from the collision. Virtual experiments are performed to demonstrate the algorithm's capability to successfully detect collisions and either avoid them, or use the manipulator to handle them appropriately without damaging sensitive areas of the vehicle.

</details>


### [167] [Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing](https://arxiv.org/abs/2601.19079)
*Naqash Afzal,Niklas Funk,Erik Helmut,Jan Peters,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 提出了一种基于神经形态事件触觉传感器的高精度实时连续盲文识别系统，通过时空分割和轻量级分类器实现快速、稳健的阅读。


<details>
  <summary>Details</summary>
Motivation: 传统机器人盲文阅读器依赖离散的逐字符扫描，限制了阅读速度并破坏了自然流程；基于视觉的替代方案通常需要大量计算，引入延迟，并在真实条件下性能下降。

Method: 结合时空分割与轻量级ResNet分类器处理稀疏事件流，实现了对不同压痕深度和扫描速度的稳健字符识别。

Result: 在标准深度下达到接近完美的准确率（>=98%），在包含日常生活词汇的物理盲文板上实现超过90%的词级准确率。

Conclusion: 神经形态触觉传感为机器人盲文阅读提供了一种可扩展、低延迟的解决方案，并在辅助和机器人应用的触觉感知中具有更广泛的意义。

Abstract: Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.

</details>


### [168] [SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers](https://arxiv.org/abs/2601.19098)
*Kurt Enkera,Josh Pinskier,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: SimTO框架通过自动提取负载案例，实现高分辨率拓扑优化，生产出高度定制化的软夹具，适用于特征丰富的物体。


<details>
  <summary>Details</summary>
Motivation: 现有夹具难以抓取具有高拓扑变异性的特征丰富物体，且缺乏明确的“最佳”接触表面，导致抓取困难且易损坏。

Method: 引入了SimTO框架，该框架通过从接触式物理模拟器中自动提取负载案例，消除了手动指定负载的需求，实现了高分辨率拓扑优化。

Result: SimTO设计的夹具不仅高度专业化于特征丰富的物体，还能推广到未见过的物体。

Conclusion: SimTO框架通过自动从基于接触的物理模拟器中提取负载案例，实现了高分辨率拓扑优化，无需手动指定负载，从而为特征丰富的物体生产高度定制的软夹具。

Abstract: Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear "optimal" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.

</details>


### [169] [Agree to Disagree: Consensus-Free Flocking under Constraints](https://arxiv.org/abs/2601.19119)
*Peter Travis Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 论文提出一种新方法，通过局部观察协商代理间距离，解决了半信任场景下的群体协调问题，无需全局信息或通信。


<details>
  <summary>Details</summary>
Motivation: 随着多代理系统在社会中的普及，代理类型和配置的多样性增加，传统群体运动假设的统一代理间距离不再适用，且代理常在不保证信任或安全通信的环境中运作。

Method: 通过更新传统的群体运动框架，引入一种新的约束集体势函数，允许代理通过局部观察协商不同的代理间距离和约束。

Result: 通过一系列模拟验证了该方法的有效性，特别是在半信任场景下，相邻代理追求冲突目标时的鲁棒性。

Conclusion: 该论文提出了一种新的约束集体势函数方法，允许在不依赖全局信息或代理间通信的情况下，通过局部观察协商不同的代理间距离和约束，有效解决了半信任场景下的群体协调问题。

Abstract: Robots sometimes have to work together with a mixture of partially-aligned or conflicting goals. Flocking - coordinated motion through cohesion, alignment, and separation - traditionally assumes uniform desired inter-agent distances. Many practical applications demand greater flexibility, as the diversity of types and configurations grows with the popularity of multi-agent systems in society. Moreover, agents often operate without guarantees of trust or secure communication. Motivated by these challenges we update well-established frameworks by relaxing this assumption of shared inter-agent distances and constraints. Through a new form of constrained collective potential function, we introduce a solution that permits negotiation of these parameters. In the spirit of the traditional flocking control canon, this negotiation is achieved purely through local observations and does not require any global information or inter-agent communication. The approach is robust to semi-trust scenarios, where neighbouring agents pursue conflicting goals. We validate the effectiveness of the approach through a series of simulations.

</details>


### [170] [iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations](https://arxiv.org/abs/2601.19234)
*Youndo Do,Chad Meece,Marc Zebrowitz,Spencer Banks,Myeongjun Choi,Xiaoxu Diao,Kai Tan,Michael Doran,Jason Reed,Fan Zhang*

Main category: cs.RO

TL;DR: iFAN是一个用于核设施的数字孪生框架，提供高保真虚拟测试环境，支持多种新兴技术的验证。


<details>
  <summary>Details</summary>
Motivation: 核设施数字化转型和先进反应堆发展中，缺乏专用虚拟测试平台，阻碍了AI集成、网络物理安全等新兴技术的评估与部署。

Method: 开发了一个全面的数字孪生框架iFAN，包含基于物理的模拟和实时数据交换功能。

Result: iFAN生态系统作为高保真虚拟测试平台，支持工厂操作、网络安全、物理安全和机器人操作，具备虚拟现实、强化学习、辐射模拟等核心功能。

Conclusion: iFAN生态系统为下一代自主和网络弹性核操作提供了一个多功能且安全的验证架构。

Abstract: As nuclear facilities experience digital transformation and advanced reactor development, AI integration, cyber-physical security, and other emerging technologies such as autonomous robot operations are increasingly developed. However, evaluation and deployment is challenged by the lack of dedicated virtual testbeds. The Immersive Framework for Advanced Nuclear (iFAN) ecosystem is developed, a comprehensive digital twin framework with a realistic 3D environment with physics-based simulations. The iFAN ecosystem serves as a high-fidelity virtual testbed for plant operation, cybersecurity, physical security, and robotic operation, as it provides real-time data exchange for pre-deployment verification. Core features include virtual reality, reinforcement learning, radiation simulation, and cyber-physical security. In addition, the paper investigates various applications through potential operational scenarios. The iFAN ecosystem provides a versatile and secure architecture for validating the next generation of autonomous and cyber-resilient nuclear operations.

</details>


### [171] [Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist](https://arxiv.org/abs/2601.19275)
*Tatsuya Kamijo,Mai Nishimura,Cristian C. Beltran-Hernandez,Nodoka Shibasaki,Masashi Hamaya*

Main category: cs.RO

TL;DR: TaMeSo-bot系统通过软腕和触觉记忆实现安全操作，MAT$^\text{3}$模型在多样化peg-in-hole任务中表现优于基线，适应性强。


<details>
  <summary>Details</summary>
Motivation: 触觉记忆对接触密集型任务（如不确定条件下的钥匙插入）至关重要，但现有系统缺乏灵活适应新场景的能力。

Method: 系统采用Masked Tactile Trajectory Transformer (MAT$^\text{3}$)模型，通过掩码标记预测学习丰富的时空表征，无需显式子任务分割即可提取任务相关特征。

Result: 在真实机器人实验中，MAT$^\text{3}$在所有条件下均比基线方法取得更高的成功率，并能显著适应未见过的peg和条件。

Conclusion: TaMeSo-bot系统通过结合软腕和触觉检索控制，成功实现了在不确定环境下的安全、鲁棒操作，MAT$^\text{3}$模型在多样化的peg-in-hole任务中表现出色，展现了强大的适应能力。

Abstract: Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.

</details>


### [172] [Perception-to-Pursuit: Track-Centric Temporal Reasoning for Open-World Drone Detection and Autonomous Chasing](https://arxiv.org/abs/2601.19318)
*Venkatakrishna Reddy Oruganti*

Main category: cs.RO

TL;DR: P2P框架通过时间推理和运动模式分析，显著提升了无人机轨迹预测和拦截规划的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪方法忽略了拦截可行性，导致99.9%的轨迹无法实际拦截，需要一种兼顾预测准确性和拦截可行性的方法。

Method: 提出了Perception-to-Pursuit (P2P)框架，使用8维令牌表示无人机运动，并通过12帧因果变换器进行未来行为推理。

Result: 在Anti-UAV-RGBT数据集上，P2P实现了28.12像素的平均位移误差和0.597的拦截成功率，比基线方法提升了77%的预测准确性和597倍的拦截可行性。

Conclusion: P2P框架通过时间推理实现了无人机轨迹的准确预测和可行的拦截规划，显著提升了预测准确性和拦截成功率。

Abstract: Autonomous drone pursuit requires not only detecting drones but also predicting their trajectories in a manner that enables kinematically feasible interception. Existing tracking methods optimize for prediction accuracy but ignore pursuit feasibility, resulting in trajectories that are physically impossible to intercept 99.9% of the time. We propose Perception-to-Pursuit (P2P), a track-centric temporal reasoning framework that bridges detection and actionable pursuit planning. Our method represents drone motion as compact 8-dimensional tokens capturing velocity, acceleration, scale, and smoothness, enabling a 12-frame causal transformer to reason about future behavior. We introduce the Intercept Success Rate (ISR) metric to measure pursuit feasibility under realistic interceptor constraints. Evaluated on the Anti-UAV-RGBT dataset with 226 real drone sequences, P2P achieves 28.12 pixel average displacement error and 0.597 ISR, representing a 77% improvement in trajectory prediction and 597x improvement in pursuit feasibility over tracking-only baselines, while maintaining perfect drone classification accuracy (100%). Our work demonstrates that temporal reasoning over motion patterns enables both accurate prediction and actionable pursuit planning.

</details>


### [173] [Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection](https://arxiv.org/abs/2601.19354)
*Ziqian Wang,Chenxi Fang,Zhen Zhang*

Main category: cs.RO

TL;DR: 提出一种自监督框架，通过可微分硬约束投影层和全局引导人工势场解决自主导航中的安全性和数据稀缺问题，实验验证了高成功率和实时可行性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中部署深度学习代理进行自主导航面临安全性、数据稀缺和有限计算资源的关键挑战。传统求解器通常存在高延迟问题，而新兴的基于学习的方法难以确保确定性可行性。

Method: 提出了一种自监督框架，包含可微分硬约束投影层用于运行时保证。构建了全局引导人工势场（G-APF）以缓解数据稀缺问题，并采用自适应神经投影层高效执行执行器限制和几何约束。

Result: 在20,000个场景的测试集上，成功率达到88.75%，验证了操作安全性的提升。CARLA中的闭环实验进一步验证了动态约束下规划路径的物理可实现性。在NVIDIA Jetson Orin NX上的部署验证显示推理延迟为94毫秒，证明了在资源受限的嵌入式硬件上的实时可行性。

Conclusion: 该框架提供了一个将物理定律嵌入神经架构的通用范式，为解决机电系统中的约束优化问题提供了可行方向。

Abstract: Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: https://github.com/wzq-13/SSHC.git.

</details>


### [174] [Teaching Machine Learning Fundamentals with LEGO Robotics](https://arxiv.org/abs/2601.19376)
*Viacheslav Sydora,Guner Dilsad Er,Michael Muehlebach*

Main category: cs.RO

TL;DR: A web-based platform using LEGO robotics teaches machine learning to teens via hands-on activities, showing improved understanding and engagement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make machine learning concepts accessible and engaging for students aged 12 to 17, using a hands-on, robotics-based approach.

Method: The method involves a web-based platform combining interactive visualizations with LEGO robotics to teach KNN, linear regression, and Q-learning through programming-free activities.

Result: Results show significant improvements in students' conceptual understanding, positive shifts in AI perception, high platform usability, and increased motivation for continued learning.

Conclusion: The paper demonstrates that tangible, visualization-based approaches like Machine Learning with Bricks effectively teach machine learning to young learners, maintaining technical depth while being engaging.

Abstract: This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at https://learning-and-dynamics.github.io/ml-with-bricks/, with video tutorials guiding students through the experiments at https://youtube.com/playlist?list=PLx1grFu4zAcwfKKJZ1Ux4LwRqaePCOA2J.

</details>


### [175] [Judgelight: Trajectory-Level Post-Optimization for Multi-Agent Path Finding via Closed-Subwalk Collapsing](https://arxiv.org/abs/2601.19388)
*Yimin Tang,Sven Koenig,Erdem Bıyık*

Main category: cs.RO

TL;DR: Judgelight是一种后优化方法，通过ILP形式化MAPF-Collapse问题，减少学习型MAPF求解器轨迹中的冗余移动，实验显示成本降低20%。


<details>
  <summary>Details</summary>
Motivation: 学习型MAPF求解器虽快速且可扩展，但常产生包含不必要或振荡运动的可行轨迹，需后优化提升质量。

Method: 通过将MAPF-Collapse问题形式化为整数线性规划（ILP）问题，提出了一种精确优化方法。

Result: 实验表明，Judgelight能稳定降低约20%的解成本，尤其对学习型求解器效果显著。

Conclusion: Judgelight作为一种后优化方法，能够有效提升MAPF求解器生成的轨迹质量，尤其适用于学习型求解器，显著减少不必要的移动，更适合实际部署。

Abstract: Multi-Agent Path Finding (MAPF) is an NP-hard problem with applications in warehouse automation and multi-robot coordination. Learning-based MAPF solvers offer fast and scalable planning but often produce feasible trajectories that contain unnecessary or oscillatory movements. We propose Judgelight, a post-optimization method that improves trajectory quality after a MAPF solver generates a feasible schedule. Judgelight collapses closed subwalks in agents' trajectories to remove redundant movements while preserving all feasibility constraints. We formalize this process as MAPF-Collapse, prove that it is NP-hard, and present an exact optimization approach by formulating it as integer linear programming (ILP) problem. Experimental results show Judgelight consistently reduces solution cost by around 20%, particularly for learning-based solvers, producing trajectories that are better suited for real-world deployment.

</details>


### [176] [Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.19406)
*Kaipeng Fang,Weiqing Liang,Yuyang Li,Ji Zhang,Pengpeng Zeng,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.RO

TL;DR: SimHum框架利用模拟机器人动作和真实人类观察数据的互补性，显著提升机器人操作的泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 模拟数据和真实人类数据各有局限性，前者存在视觉差距，后者缺乏机器人动作数据。两者互补性未被充分探索。

Method: 提出了SimHum共训练框架，同时从模拟机器人动作中提取运动学先验和从真实人类观察中提取视觉先验。

Result: SimHum在相同数据收集预算下性能提升40%，仅用80个真实数据实现62.5%的OOD成功率，比仅用真实数据的基线高7.1倍。

Conclusion: SimHum框架通过结合模拟机器人动作和真实人类观察数据，显著提升了机器人操作任务的数据效率和泛化能力。

Abstract: Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\mathbf{40\%}$ under the same data collection budget, and achieves a $\mathbf{62.5\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\times$. Videos and additional information can be found at \href{https://kaipengfang.github.io/sim-and-human}{project website}.

</details>


### [177] [Task-Centric Policy Optimization from Misaligned Motion Priors](https://arxiv.org/abs/2601.19411)
*Ziang Zheng,Kai Feng,Yi Nie,Shentao Qin*

Main category: cs.RO

TL;DR: TCMP框架通过任务优先的对抗模仿学习，平衡任务性能与自然运动风格，解决了模仿学习中的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类演示与机器人任务之间的不对齐问题（如体现差异、重定向误差等），避免模仿学习对任务性能的负面影响。

Method: 提出Task-Centric Motion Priors (TCMP)，一种任务优先的对抗模仿框架，通过条件正则化处理模仿信号，仅在兼容任务进展时引入模仿。

Result: 实验验证了TCMP在噪声演示下仍能保持稳健的任务性能和一致的运动风格。

Conclusion: TCMP框架通过将模仿作为条件正则化而非平等目标，有效解决了任务与运动先验之间的冲突，实现了任务性能与自然运动风格的平衡。

Abstract: Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.

</details>


### [178] [Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots](https://arxiv.org/abs/2601.19496)
*Jie Gu,Hongrun Gao,Zhihao Xia,Yirun Sun,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: 提出了一种保证稳定连接的四边形MSRRs自重构规划算法，通过虚拟图和DRTree解决动作依赖，验证了其高效性和实际可行性。


<details>
  <summary>Details</summary>
Motivation: 对于模块化自重构机器人（MSRRs），在重构过程中保持稳定连接对物理可行性和可部署性至关重要。

Method: 该方法首先使用虚拟图表示构建可行的连接/断开动作，然后通过基于依赖的反向树（DRTree）将这些动作组织成有效的执行序列，解决相互依赖问题。

Result: 论文证明了对于包含七个或更多模块（不包括线性拓扑）的任何配置对，存在满足运动特性的重构序列。与改进的BiRRT算法相比，该方法在效率和稳定性上表现更优。

Conclusion: 该论文提出了一种新颖的自重构规划算法，确保四边形模块化自重构机器人在重构过程中保持稳定连接，并通过物理机器人平台的部署验证了其实际可行性。

Abstract: For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.

</details>


### [179] [Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots](https://arxiv.org/abs/2601.19499)
*Mehdi Heydari Shahna,Seyed Adel Alizadeh Kolagar,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习和Lyapunov类稳定器的控制框架，显著提升了机器人在非结构化环境中的目标到达率。


<details>
  <summary>Details</summary>
Motivation: 强化学习通常无法提供目标到达的正式保证，而现有的安全约束机制可能过于保守，影响学习和探索效果。

Method: 设计了一个包含15个奖励项的实时RL策略，并结合Lyapunov类稳定器层作为策略监督器，以增强目标到达控制。

Result: 实验结果显示，Lyapunov类稳定器显著提升了基准RL策略的性能，目标到达率从84.6%提高到99.0%。

Conclusion: 本文提出的基于强化学习的控制框架，结合Lyapunov类稳定器层，显著提升了轮式移动机器人在非结构化环境中的目标到达率，从84.6%提升至99.0%，同时减少了失败并提高了效率。

Abstract: Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.

</details>


### [180] [A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation](https://arxiv.org/abs/2601.19509)
*Jin Huang,Zichen Liu,Haoda Li,Zhikun Wang,Ying Chen*

Main category: cs.RO

TL;DR: 论文针对SINS/DVL导航系统中长期姿态误差累积导致性能下降的问题，提出了两种改进方法：姿态误差感知的速度转换模型和方差传播方法，实验证明其显著提升了导航精度。


<details>
  <summary>Details</summary>
Motivation: 传统的SINS/DVL松耦合架构在长期运行中，由于姿态估计误差累积导致速度投影偏差，进而降低导航性能。为解决这一问题，论文提出了两种互补的改进方法。

Method: 1. 提出了一种车辆姿态误差感知的DVL速度转换模型，通过将姿态误差项纳入观测方程来减少投影引起的速度偏差；2. 开发了一种基于协方差矩阵的方差传播方法，通过引入基于期望的姿态误差补偿项实现统计一致的噪声建模。

Result: 仿真和现场实验结果表明，两种改进方法单独应用均能提升导航精度，联合应用时能有效抑制长期误差发散。现场实验显示，相比基线IMU+DVL方法，3D位置RMSE改善了78.3%，最大分量位置误差减少了71.8%。

Conclusion: 论文提出的两种改进方法有效抑制了长期误差发散，显著提升了SINS/DVL导航系统的长期性能，3D位置RMSE改善了78.3%，最大分量位置误差减少了71.8%。

Abstract: In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.

</details>


### [181] [ALRM: Agentic LLM for Robotic Manipulation](https://arxiv.org/abs/2601.19510)
*Vitor Gaboardi dos Santos,Ibrahim Khadraoui,Ibrahim Farhat,Hamza Yous,Samy Teffahi,Hakim Hacid*

Main category: cs.RO

TL;DR: ALRM是一个LLM驱动的机器人操控框架，通过ReAct循环支持CaP和TaP模式，实验显示其在多任务基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在机器人控制中的局限性：缺乏模块化执行机制和现有基准未系统评估多步推理与语言多样性。

Method: ALRM通过ReAct式推理循环整合策略生成与代理执行，支持Code-asPolicy（CaP）和Tool-as-Policy（TaP）两种模式。

Result: 实验表明ALRM在56个任务的模拟基准上表现优异，Claude-4.1-Opus和Falcon-H1-7B在CaP模式下表现最佳。

Conclusion: ALRM提供了一个可扩展、可解释和模块化的方法，将自然语言推理与可靠的机器人执行相结合。实验显示Claude-4.1-Opus和Falcon-H1-7B分别在CaP模式下表现最佳。

Abstract: Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.

</details>


### [182] [PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment](https://arxiv.org/abs/2601.19514)
*Ruiyu Wang,Zheyu Zhuang,Danica Kragic,Florian T. Pokorny*

Main category: cs.RO

TL;DR: PALM通过模块化策略和局部一致性，显著提升图像行为克隆在训练域外的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像行为克隆中训练域外泛化的挑战，现有方法通常孤立处理单个泛化轴且依赖复杂流程，PALM旨在无需额外输入模态、模型修改或数据收集的情况下，同时应对多种OOD偏移。

Method: PALM将操作策略模块化为全局粗粒度组件和局部细粒度策略，通过强化局部视觉焦点和一致的本体感受表示，减少训练域外输入的差异。

Result: PALM在仿真和现实环境中将OOD性能下降分别限制在8%和24%，显著优于基线方法的45%和77%。

Conclusion: PALM通过模块化操作策略和强化局部视觉焦点与一致的本体感受表示，有效减少了训练域外（OOD）条件下的性能下降，展现了在仿真和现实环境中的优越泛化能力。

Abstract: Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.

</details>


### [183] [Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion](https://arxiv.org/abs/2601.19529)
*Jie Gu,Yirui Sun,Zhihao Xia,Tin Lun Lam,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: Rhombot是一种新型可变形平面晶格模块化自重构机器人，通过简化设计和morphpivoting运动原语实现稳定重构，适用于多样化环境。


<details>
  <summary>Details</summary>
Motivation: 旨在以最小控制复杂度实现MSRR的基本功能（变形、对接、移动），并在多样化环境中稳定运行。

Method: 设计了基于平行四边形骨架和中央执行器的模块，引入了morphpivoting运动原语以实现连续重构。

Result: 物理实验验证了模块的稳定重构能力及位置与对接精度。

Conclusion: Rhombot通过其菱形模块和简化控制实现了稳定的重构能力，适用于多样化环境。

Abstract: In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.

</details>


### [184] [Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation](https://arxiv.org/abs/2601.19536)
*Hongji Liu,Linwei Zheng,Yongjian Li,Mingkai Tang,Xiaoyang Yan,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: 低成本框架利用优化IPM和车辆姿态，实现高精度矢量道路映射，支持多种地面标记，测试显示接近厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 提出一种低成本、统一的矢量道路映射框架，以解决IPM在道路映射中的局限性，并提升精度。

Method: 利用Catmull-Rom样条描述车道线，其他地面标记统一用多边形表示，通过实例分割结果优化样条控制点和多边形角点的三维位置，同时优化IPM的单应矩阵和车辆姿态。

Result: 在两个实际场景中测试，方法能自动生成高精度地图（接近厘米级精度），优化的IPM矩阵精度接近手动校准，车辆姿态精度也显著提升。

Conclusion: 该框架显著降低了IPM相关的映射误差，提高了初始IPM单应矩阵和预测车辆姿态的准确性，并解决了IPM中平面假设的限制。

Abstract: In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.

</details>


### [185] [AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation](https://arxiv.org/abs/2601.19634)
*Wenda Yu,Tianshi Wang,Fengling Li,Jingjing Li,Lei Zhu*

Main category: cs.RO

TL;DR: AC^2-VLA通过动作上下文感知的自适应计算，显著提升VLA模型效率，减少计算成本，同时保持任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在机器人操作中表现出色，但其闭环部署受到高延迟和高计算成本的限制。此外，大多数效率优化方法忽视了动作上下文的重要性。

Method: 提出了一个统一的框架AC^2-VLA，该框架基于当前视觉观察、语言指令和先前动作状态，自适应地执行跨时间步的认知重用、令牌修剪和模型组件的选择性执行。

Result: 在机器人操作基准测试中，AC^2-VLA实现了高达1.79倍的加速，并将FLOPs减少至密集基线的29.4%，同时保持了可比较的任务成功率。

Conclusion: AC^2-VLA框架通过动作上下文感知的自适应计算，显著提升了VLA模型在机器人操作任务中的效率，同时保持了与密集基线相当的完成任务成功率。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.

</details>


### [186] [Enhancing Worker Safety in Harbors Using Quadruped Robots](https://arxiv.org/abs/2601.19643)
*Zoe Betta,Davide Corongiu,Carmine Tommaso Recchiuto,Antonio Sgorbissa*

Main category: cs.RO

TL;DR: The paper proposes using a quadruped robot to inspect critical areas in harbors, aiming to improve safety by addressing the challenges of complex operations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing relevance of infrastructure inspection in robotics to enhance workers' safety, especially in challenging harbor environments.

Method: The method involves an initial phase to identify critical areas within the port environment, followed by the analysis of a quadruped robot solution for inspection.

Result: The result is the identification of critical areas in harbor environments and a preliminary robotic solution for inspection.

Conclusion: The paper concludes with an analysis of a preliminary solution using a quadruped robot for inspecting critical areas in harbor environments, highlighting its potential to improve workers' safety.

Abstract: Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.

</details>


### [187] [SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects](https://arxiv.org/abs/2601.19742)
*Ali Jnadi,Hadi Salloum,Yaroslav Kholodov,Alexander Gasnikov,Karam Almaghout*

Main category: cs.RO

TL;DR: SCOPE是一个快速高效的框架，通过凸近似方法降低计算成本，适用于实时或近实时响应的变形线性物体建模与操纵。


<details>
  <summary>Details</summary>
Motivation: 传统的基于能量的方法在建模和操纵变形线性物体时计算成本高，难以满足实时或近实时响应的需求。

Method: SCOPE采用凸近似方法，替代传统的基于能量的方法，以实现快速和高效的变形线性物体建模与操纵。

Result: 通过全面的模拟实验验证了SCOPE框架的有效性，展示了其在几何和长度约束下生成平滑形状轨迹的能力。

Conclusion: SCOPE框架通过凸近似方法在保持平滑和物理合理变形的同时显著降低了计算成本，适用于需要实时或近实时响应的应用。

Abstract: We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.

</details>


### [188] [Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications](https://arxiv.org/abs/2601.19761)
*Jin Huang,Fethiye Irmak Doğan,Hatice Gunes*

Main category: cs.RO

TL;DR: 论文提出利用推荐系统技术增强社交机器人的个性化能力，通过范式对齐、关键技术识别和模块化设计，解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如大型语言模型和强化学习）无法全面捕捉用户偏好（包括长期、短期和细粒度方面），并用于排名和选择行动、主动个性化交互及确保道德负责任的适应。

Method: 通过（i）对齐社交机器人与推荐系统的范式，（ii）识别增强社交机器人个性化的关键技术，（iii）将其设计为模块化即插即用组件。

Result: 提出了一个整合推荐系统技术的框架，为社交机器人和HRI社区的深度合作开辟了途径。

Conclusion: 该论文提出了一个框架，将推荐系统技术整合到社交机器人中，以更全面地捕捉用户偏好，并促进RS与HRI社区的深度合作。

Abstract: Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.

</details>


### [189] [Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse](https://arxiv.org/abs/2601.19826)
*Fan Yang,Renkai Ma,Yaxin Hu,Lingyao Li*

Main category: cs.RO

TL;DR: 拟人化程度和道德基础共同影响人们对机器人虐待的反应，低进步性个体和高进步性个体在道德推理上表现出不同模式。


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益融入日常生活，理解人们对机器人虐待的反应具有重要的伦理和设计意义。

Method: 采用混合方法研究（N = 201），通过展示不同拟人化程度（蜘蛛形、双足形、人形）机器人遭受物理虐待的视频，并测量参与者的道德基础、愤怒情绪和社会距离。

Result: 结果显示，拟人化程度影响人们对机器人的道德关怀，而道德基础则决定了他们如何推理这种关怀。定性分析揭示了不同的推理模式：低进步性个体采用基于性格的判断，而高进步性个体则进行未来导向的道德思考。

Conclusion: 研究发现，拟人化程度决定了人们是否将道德关怀扩展到机器人，而道德基础则影响了他们对此类关怀的推理方式。这为机器人设计和政策沟通提供了重要启示。

Abstract: As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.

</details>


### [190] [Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation](https://arxiv.org/abs/2601.19832)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出一种基于单次RGB视频的双臂任务演示处理方法，利用信息论和场景图生成模块化行为树，显著提升双臂协调效率。


<details>
  <summary>Details</summary>
Motivation: 针对双臂任务演示中手部协调复杂且数据记录困难的问题，简化非专家用户的机器人编程过程。

Method: 应用香农信息论分析场景元素间的信息流，并利用场景图属性检测手部协调策略，生成基于不同协调需求的模块化行为树。

Result: 通过多个主题视频演示和公开数据集的验证，相比现有方法在生成双臂系统集中执行计划方面有显著改进。

Conclusion: 本文提出了一种新颖的一次性方法，通过分析单次RGB视频演示生成双臂机器人系统的执行计划，显著提升了双臂协调任务的编程效率。

Abstract: Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.

</details>


### [191] [HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs](https://arxiv.org/abs/2601.19839)
*Jeanne Malécot,Hamed Rahimi,Jeanne Cattoni,Marie Samson,Mouad Abrini,Mahdi Khoramshahi,Maribel Pino,Mohamed Chetouani*

Main category: cs.RO

TL;DR: HARMONI是一个多模态个性化框架，通过四大模块实现多用户环境中机器人长期交互的个性化和动态适应，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统在多用户环境中缺乏持续个性化和动态适应的机制，限制了实际应用效果。

Method: HARMONI框架包含四个核心模块：感知模块（识别说话者及多模态输入）、世界建模模块（环境及短期对话上下文）、用户建模模块（更新长期用户档案）和生成模块（生成情境化且符合伦理的响应）。

Result: 在四个数据集和养老院实际场景的用户研究中，HARMONI在说话者识别、在线记忆更新和伦理对齐个性化方面表现优异，用户建模准确性、个性化质量和用户满意度均优于基线方法。

Conclusion: HARMONI框架通过整合多模态输入和长期用户建模，显著提升了多用户环境中人机交互的个性化和适应性，优于现有基线方法。

Abstract: Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.

</details>


### [192] [Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability](https://arxiv.org/abs/2601.19856)
*Giulio Campagna,Marta Lagomarsino,Marta Lorenzini,Dimitrios Chrysostomou,Matthias Rehm,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本研究提出数据驱动框架评估人机协作信任，通过行为指标和机器学习模型实现高准确率分类。


<details>
  <summary>Details</summary>
Motivation: 工业5.0关注人机协作中的人本中心，强调安全、舒适和信任。本研究旨在通过行为指标评估信任。

Method: 研究引入了一个基于行为指标的数据驱动框架，采用偏好优化算法生成增强信任的轨迹，并利用操作员反馈作为训练机器学习模型的基础事实。

Result: 机器学习模型在化学工业场景中分类信任的准确率超过80%，其中投票分类器达到84.07%的准确率和0.90的AUC-ROC分数。

Conclusion: 研究强调了数据驱动方法在评估人机协作信任中的有效性，并突出了行为指标在预测人类信任动态中的重要作用。

Abstract: Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\% accuracy, with the Voting Classifier achieving 84.07\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.

</details>
