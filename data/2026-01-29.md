<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.RO](#cs.RO) [Total: 20]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种从单目图像恢复真实比例3D重建的方法，显著提升了食物摄入量估计的准确性，适用于精确营养领域。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病（如肥胖和糖尿病）的增多凸显了准确监测食物摄入量的需求，而现有方法在恢复食物真实比例方面存在不足。

Method: 利用在大规模数据集上训练的模型提取丰富的视觉特征，估计重建对象的比例，从而将单视角3D重建转换为真实比例的物理模型。

Result: 在两个公开数据集上的实验表明，该方法平均绝对体积估计误差减少了近30%，优于现有技术。

Conclusion: 本文提出了一种从单目图像中恢复真实比例的3D重建对象的方法，填补了3D计算机视觉与数字健康之间的空白，显著提高了精确营养领域的食物摄入量估计准确性。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa通过显著性感知解耦和分层细化，解决了开放词汇语义分割中的前景偏见和空间定位问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（如CLIP）的方法存在前景偏见和空间定位模糊的问题，DiSa旨在解决这些局限性。

Method: DiSa采用显著性感知的前景-背景解耦框架（SDM）和分层细化模块（HRM），分别建模前景和背景特征，并通过多级更新细化空间定位。

Result: 在六个基准测试中，DiSa consistently outperforms state-of-the-art methods。

Conclusion: DiSa框架通过显式结合显著性线索和分层细化模块，显著提升了开放词汇语义分割的性能，在六个基准测试中均优于现有方法。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: SSMAE框架通过动态伪标签和门控机制，在低标签数据下显著提升ViT性能，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 解决在标记数据稀缺但未标记数据丰富时训练Vision Transformers的挑战。

Method: 提出Semi-Supervised Masked Autoencoder (SSMAE)框架，联合优化掩码图像重建和分类任务，利用未标记和标记样本，并通过动态选择伪标签及验证驱动的门控机制减少确认偏差。

Result: 在CIFAR-10和CIFAR-100上，SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels).

Conclusion: SSMAE框架通过动态选择伪标签和验证驱动的门控机制，在低标签数据情况下显著提升了Vision Transformers的性能，证明了伪标签引入时机的重要性。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer 是一种新颖的 STVSR 框架，通过一步扩散过程和混合专家策略，在真实场景中实现了高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在视频超分辨率中表现出色，但其在时空视频超分辨率（STVSR）中的潜力尚未充分探索，尤其是在复杂未知退化的真实场景中。

Method: OSDEnhancer 通过线性预插值策略初始化关键的时空结构，并依赖于训练时间精化和空间增强的混合专家（TR-SE MoE），以及双向可变形变分自编码器（VAE）解码器进行循环时空聚合和传播。

Result: 实验证明，该方法在真实场景中实现了最先进的性能，并保持了卓越的泛化能力。

Conclusion: OSDEnhancer 提出了一种新颖的框架，首次通过高效的一步扩散过程实现了真实世界的 STVSR，并在实验中展示了最先进的性能和卓越的泛化能力。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [5] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 该论文提出了一种在CLIP训练中直接整合稀疏性的方法，生成既解释性强又高性能的表示，挑战了传统观念，展示了解释性和性能可共同优化。


<details>
  <summary>Details</summary>
Motivation: CLIP的密集且不透明的潜在表示带来了显著的解释性挑战，传统方法认为解释性和性能之间存在矛盾，但后处理方法（如稀疏自编码器）往往导致性能下降和多模态能力丧失。

Method: 提出了一种简单而有效的方法，将稀疏性直接整合到CLIP训练中，生成既具有解释性又高性能的表示。

Result: 稀疏CLIP表示在保持强大下游任务性能的同时，实现了更优的解释性，并保留了多模态能力。稀疏特征还能实现直接的语义概念对齐，并揭示跨模态知识的训练动态。

Conclusion: 该研究挑战了传统观念，证明解释性和性能可以共同优化，为未来模型设计提供了有前景的原则。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [6] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 本研究专注于H&E染色图像中的核实例分割数据集，通过标准化和评估公开数据集，提出了统一的测试和训练集，为模型评估提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种机器学习和深度学习方法用于核实例分割，但大多数研究集中在开发新算法并在有限数量的公开数据集上进行基准测试，本研究则专注于数据集本身。

Method: 基于广泛的文献综述，识别并标准化了公开可用的H&E染色图像数据集，使用两种先进的分割模型（CNN和混合CNN与视觉Transformer架构）系统评估了这些数据集，并提出了统一的测试集（NucFuse-test）和训练集（NucFuse-train）。

Result: 通过系统评估和排名数据集，生成了融合数据集并进行了外部验证，为核实例分割模型的训练、测试和评估提供了新基准。

Conclusion: 本研究通过评估和排名数据集、生成融合数据集、进行外部验证并公开实现代码，为H&E染色组织图像中的核实例分割模型提供了新的基准。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [7] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: SAP是一种无需训练的剪枝方法，通过识别中间层关键视觉补丁实现高性能压缩，在ViDoRe基准测试中减少90%索引向量且保持高检索保真度。OSR协议揭示了中间层语义结构锚补丁的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的无需训练剪枝方法（如基于EOS注意力的方法）在高压缩场景（>80%）下表现不佳，甚至低于随机选择，原因在于视觉令牌的重要性本质上是查询依赖的。本研究旨在探索无需训练剪枝的可行性。

Method: 提出了Structural Anchor Pruning (SAP)方法，通过识别中间层的关键视觉补丁实现高性能压缩，并引入了Oracle Score Retention (OSR)协议来评估分层信息对压缩效率的影响。

Result: 在ViDoRe基准测试中，SAP方法将索引向量减少了超过90%，同时保持了强大的检索保真度。OSR分析进一步揭示了语义结构锚补丁在中间层的持续存在。

Conclusion: SAP（Structural Anchor Pruning）作为一种无需训练的剪枝方法，通过识别中间层的关键视觉补丁，实现了高性能压缩，并在ViDoRe基准测试中展示了超过90%的索引向量缩减，同时保持了强大的检索保真度。此外，OSR（Oracle Score Retention）协议揭示了语义结构锚补丁在中间层持续存在，与传统剪枝方法聚焦于结构信号消散的最后一层形成对比。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [8] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 论文提出针对LLaDA-V中后期层的结构化令牌剪枝策略，显著降低计算开销且保持性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式大型多模态模型（如LLaDA-V）的双向注意力机制和迭代去噪范式导致显著计算开销，需要优化。

Method: 通过深入注意力分析，发现LLaDA-V的跨模态信息主要在中间到后期层聚合，因此提出在首次去噪步骤中对中后期层进行选择性令牌剪枝。

Result: 在多个基准测试中，最佳配置将计算成本降低高达65%，同时平均保留95%的任务性能。

Conclusion: 该论文提出了一种结构化令牌剪枝策略，有效降低了扩散式大型多模态模型的计算开销，同时保持了关键语义信息，为高效LLaDA-V推理提供了实证基础。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [9] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle是一种轻量级但高效的模型，通过课程持续学习框架和高质量数据集，在图像和视频风格化任务中实现了内容保留和风格定制的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决DiTs中内容和风格特征纠缠的问题，实现高质量的内容保留和风格定制。

Method: 基于Qwen-Image-Edit构建TeleStyle，利用高质量数据集和合成三元组进行训练，并引入课程持续学习框架和视频到视频风格化模块。

Result: TeleStyle在风格相似性、内容一致性和美学质量三个核心评估指标上均达到最先进性能。

Conclusion: TeleStyle通过结合高质量数据集和课程持续学习框架，在保持内容精确性的同时实现了对未见风格的泛化，同时在图像和视频风格化任务中均达到了最先进的性能。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [10] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 研究探讨了自动分类生物污损严重程度的方法，发现计算机视觉模型和LLMs各有优势，混合方法最具潜力。


<details>
  <summary>Details</summary>
Motivation: 船舶船体上的海洋生物污损带来重大生态、经济和生物安全风险，传统潜水检查方法危险且难以扩展。

Method: 评估了卷积神经网络、基于变换器的分割模型和零样本大型多模态语言模型（LLMs），使用新西兰初级产业部的专家标记数据集。

Result: 计算机视觉模型在极端污损级别上表现出高准确性，但在中间级别因数据集不平衡和图像构图而表现不佳；LLMs通过结构化提示和检索实现了竞争性性能，无需训练且输出可解释。

Conclusion: 混合方法结合分割覆盖和LLM推理为可扩展且可解释的生物污损评估提供了有前景的途径。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [11] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO通过密集奖励和自适应探索空间校准，解决了GRPO方法中的稀疏奖励问题，提升了文本到图像生成的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在稀疏奖励问题，全局反馈信号与中间去噪步骤的细粒度贡献不匹配，导致训练效果不佳。

Method: 1. 预测每个去噪步骤的逐步奖励增益作为密集奖励；2. 提出奖励感知方案，通过自适应调整SDE采样器中的时间步特定随机性注入来校准探索空间。

Result: 在多个标准基准上的实验证明了DenseGRPO的有效性，并验证了密集奖励在流匹配模型对齐中的关键作用。

Conclusion: DenseGRPO通过密集奖励和奖励感知方案有效解决了GRPO方法中的稀疏奖励问题，显著提升了文本到图像生成中的人类偏好对齐效果。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [12] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: FPL通过特征投影学习将分类问题转化为特征投影问题，结合CLIP预测，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言预训练模型（如CLIP）在下游任务适应中存在性能有限、可学习参数过多或训练时间过长的问题，FPL旨在高效解决这些问题。

Method: 提出了一种名为特征投影学习（FPL）的方法，通过投影模型将类原型特征投影到查询图像特征空间，并重构查询图像特征图，使用负平均平方重建误差作为类别分数。

Result: FPL在全面实证评估中表现出卓越的准确性，显著超越了当前最先进的方法。

Conclusion: FPL方法通过将分类问题转化为特征投影问题，结合预训练CLIP模型的预测，显著提升了下游任务的准确率，超越了现有最先进方法。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [13] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: PAE 通过频率域初始化和全局协调优化视觉提示调优，显著提升稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有 VPT 方法存在训练不稳定（梯度振荡）和跨层不匹配问题，导致收敛慢且性能下降。

Method: PAE 通过任务感知的频率捷径模式初始化提示，使用共享 Koopman 算子协调跨层演化，并引入 Lyapunov 稳定性理论的正则化约束误差放大。

Result: PAE 在 25 个数据集上平均加速收敛 1.41 倍，准确率提升 1-3%，且具有轻量化和通用性优势。

Conclusion: PAE 通过频率域视角和全局线性变换显著提升了视觉提示调优的稳定性和性能，适用于多种 VPT 变体且无需修改主干网络。

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [14] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: BLenDeR是一种扩散采样方法，通过集合论操作增强类内多样性，显著提升深度度量学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在深度度量学习中难以可控地增强类内多样性，BLenDeR旨在解决这一关键限制。

Method: BLenDeR利用去噪残差的并集和交集操作，通过并集操作鼓励多提示中的任何属性，交集操作通过主成分替代提取共同方向，从而可控地合成类内多样属性组合。

Result: 在标准DML基准测试中，BLenDeR在CUB-200上Recall@1提升3.7%，在Cars-196上提升1.8%。

Conclusion: BLenDeR通过引入基于集合论的扩散采样方法，显著提升了深度度量学习中的类内多样性，并在多个数据集和骨干网络上优于现有最先进方法。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [15] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: RED模型通过显式监督训练框架，解决了扩散模型在多模态图像融合中的细节丢失问题，同时保持了高效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但在图像融合任务中常因马尔可夫过程中的噪声误差累积导致细节丢失。显式监督训练虽能解决这一问题，但会带来计算效率的挑战。

Method: 提出了可逆高效扩散（RED）模型，这是一种显式监督训练框架，旨在解决扩散模型在图像融合任务中的细节丢失问题。

Result: RED模型在保持扩散模型生成能力的同时，避免了分布估计问题，提高了图像融合的质量和效率。

Conclusion: RED模型通过显式监督训练框架，继承了扩散模型的强大生成能力，同时避免了分布估计问题，有效解决了多模态图像融合中的细节丢失问题。

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [16] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: LVLMs-Saliency框架通过融合注意力与梯度信号，提出双机制（SGRS和LocoRE）有效减少大型视觉语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖前向传递注意力模式，无法可靠区分幻觉与事实基础输出，因此需要结合梯度信号来改进。

Method: 提出LVLMs-Saliency框架，结合注意力权重和输入梯度量化输出令牌的视觉基础强度，并引入双机制推理时框架（SGRS和LocoRE）来减少幻觉。

Result: 实验表明，该方法显著减少了幻觉率，同时保持了模型的流畅性和任务性能。

Conclusion: LVLMs-Saliency框架通过融合注意力权重和输入梯度，显著降低了大型视觉语言模型的幻觉率，同时保持了模型的流畅性和任务性能。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [17] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出了一种无源域自适应方法，通过多视图增强和潜在空间一致性直接从目标域学习特征，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的域自适应方法通常需要访问源域数据、对抗训练或复杂的伪标签技术，计算成本高。本文旨在解决这些挑战，提出一种无需源-目标对齐或伪标签优化的方法。

Method: 该方法利用多视图增强和潜在空间一致性技术，设计了一个基于ConvNeXt的编码器，并结合分类和一致性目标的损失函数，直接从目标域学习可迁移表示。

Result: 在Office-31、Office-Home和Office-Caltech数据集上，平均分类准确率分别达到90.72%、84%和97.12%，比现有方法平均提升1.23%、7.26%和1.77%。

Conclusion: 该论文提出了一种新颖的无源域自适应方法，通过多视图增强和潜在空间一致性技术直接从目标域学习域不变特征，显著提升了分类准确率。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [18] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出一种针对生成视频的综合评估协议，通过构建大规模数据集和开发DVAR框架，显著提升了人工痕迹检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评估方法通常仅提供粗略质量评分，缺乏对特定人工痕迹的详细定位和分类。

Method: 通过定义10种常见人工痕迹类别的分类法，构建了包含8万视频的大规模数据集GenVID，并开发了密集视频人工痕迹识别框架DVAR。

Result: 实验表明，该方法显著提高了人工痕迹检测的准确性，并能有效过滤低质量内容。

Conclusion: 本文提出的综合评估协议和DVAR框架显著提升了生成视频中人工痕迹的检测准确性，并有效过滤低质量内容。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [19] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: C-SAM通过扰动剪枝掩码优化模型结构的平坦性，解决了SAM与压缩的冲突，显著提升鲁棒性且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 探索SAM与模型压缩之间的相互作用，发现单纯剪枝SAM训练的模型会削弱鲁棒性，而剪枝后应用SAM可能受限于早期剪枝模式。

Method: 提出了Compression-aware ShArpness Minimization (C-SAM)框架，通过显式扰动剪枝掩码来优化模型结构的平坦性。

Result: 在多个数据集和模型上，C-SAM显著提升了认证鲁棒性（最高42%），同时保持了与未剪枝模型相当的任务准确性。

Conclusion: C-SAM通过将锐度感知学习从参数扰动转移到掩码扰动，有效解决了SAM与模型压缩之间的冲突，实现了在保持模型紧凑性的同时提升鲁棒性。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [20] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 通过整合少量WA数据与大量NA数据，双域学习策略显著提升了膀胱分割的性能，解决了协变量偏移和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，协变量偏移导致的性能下降是一个主要挑战。特别是在CT引导的妇科近距离放射治疗中，WA数据稀缺且存在显著的解剖变形和成像伪影，使得自动分割尤为困难。

Method: 采用双域学习策略，结合NA和WA CT数据进行训练，通过系统实验在轴向、冠状和矢状平面上验证了多种深度学习架构的性能。

Result: 提出的方法在Dice相似系数和IoU得分上分别达到0.94和0.92，显示出有效的域适应和临床可靠性提升。

Conclusion: 本研究提出了一种双域学习策略，通过整合NA和WA CT数据，显著提升了膀胱分割在协变量偏移下的鲁棒性和泛化能力。实验结果表明，仅需少量WA数据（10%-30%）即可达到与全WA数据训练相当的性能，为克服数据稀缺问题提供了有效解决方案。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [21] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 论文提出了一种物理结构化框架，通过结合几何体积和材料语义信息，从单张RGB图像中估计物体质量，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于质量取决于几何体积和材料密度，而这两者无法直接从RGB外观中观测到，因此从像素预测质量是一个不适定问题，需要物理上有意义的表示来约束解空间。

Method: 通过单目深度估计恢复物体三维几何信息以计算体积，利用视觉语言模型提取粗略材料语义以指导密度推理，最后通过实例自适应门控机制融合这些表示，并通过两个回归头预测体积和密度相关因素。

Result: 在image2mass和ABO-500数据集上的实验表明，该方法在质量估计任务中 consistently 优于现有技术。

Conclusion: 论文提出的物理结构化框架通过结合视觉线索与物理因素，显著提升了单图像质量估计的准确性，并在实验中优于现有方法。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [22] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++ introduces Query-Gated Deformable Fusion for end-to-end camera-LiDAR fusion in query space, improving perception and prediction metrics on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Modular pipelines in autonomous driving restrict information flow and amplify errors. Existing query-based PnP models lack sufficient exploration of camera-LiDAR complementarity, often using heuristic fusion schemes that introduce bias and limit information utilization.

Method: Proposes Li-ViP3D++, a query-based multimodal PnP framework with Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF includes masked attention for image evidence, differentiable BEV sampling for LiDAR context, and query-conditioned gating for adaptive cue weighting.

Result: On nuScenes, Li-ViP3D++ improves EPA (0.335), mAP (0.502), reduces false positives (FP ratio 0.147), and is faster (139.82 ms vs. 145.91 ms) than prior Li-ViP3D.

Conclusion: Query-space, fully differentiable camera-LiDAR fusion enhances robustness of end-to-end perception-and-prediction (PnP) without compromising deployability.

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [23] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: SLDM是一种结合结构约束和语言信息的扩散模型，用于从低剂量碘对比剂生成正常剂量CT图像，解决了现有方法在不完全配对图像中精确增强的难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不完全配对的图像中实现精确增强，主要因为模型识别特定结构的能力有限。

Method: 提出了一种结构约束和语言信息扩散模型（SLDM），通过提取图像结构先验信息约束模型推理过程，并引入语义监督策略和空间智能，最终应用减影血管增强模块。

Result: 定性和定量分析证明了SLDM在低剂量对比剂CT血管造影中的有效性。

Conclusion: SLDM方法在低剂量对比剂CT血管造影的血管重建中表现出有效性，通过结构约束和语言信息扩散模型实现了精确增强。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [24] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff是一种结合退化、结构和语义先验的统一图像恢复方法，通过分层互补的引导机制，在多样退化场景中实现优异恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖退化先导但难以重建严重退化区域的内容，且语义信息集成到扩散模型浅层会破坏空间结构。因此，需要一种分层互补的先导引导方法来解决这一问题。

Method: TPGDiff网络在扩散轨迹中引入退化先验，同时在浅层和深层分别加入结构先验和语义先验，实现了分层互补的先导引导。具体包括多源结构线索作为结构先验捕捉细节，以及蒸馏驱动的语义提取器提供鲁棒的语义先验。

Result: 在单退化和多退化基准测试中，TPGDiff表现出卓越的性能和泛化能力。

Conclusion: TPGDiff通过结合退化先验、结构先验和语义先验，实现了在多样退化场景下的统一图像恢复，并在实验中展示了优异的性能和泛化能力。

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [25] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi是一种通道排列不变的多变量时间序列预测框架，通过时空解耦和排列不变训练，解决了通道顺序依赖问题，实现了高性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测模型分为通道依赖和独立两类，前者易受通道顺序影响，后者忽略通道间依赖。CPiRi旨在通过通道排列不变性解决这些问题，适应通道结构变化。

Method: CPiRi结合了时空解耦架构和排列不变正则化训练策略：预训练的时序编码器提取高质量时序特征，轻量级空间模块学习内容驱动的通道间关系，通道洗牌策略在训练中强制实现排列不变性。

Result: 实验表明，CPiRi在多个基准测试中达到最先进性能，在通道顺序变化时保持稳定，并在仅训练一半通道时对未见通道表现出强泛化能力，同时在大规模数据集上保持高效。

Conclusion: CPiRi提出了一种通道排列不变（CPI）框架，通过解耦时空架构和排列不变正则化策略，有效解决了现有多变量时间序列预测模型中通道依赖和独立模型的局限性，实现了在通道结构变化下的稳定预测和强归纳泛化能力。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [26] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 提出新方法改进3D高斯溅射的表面重建，通过多视角几何一致性和单目深度约束提升几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角几何一致性在大几何差异下不可靠，而单目深度先验存在尺度模糊和局部不一致的问题，导致高斯深度监督不准确。

Method: 提出了一种高斯可见性感知的多视角几何一致性约束，以及渐进式四叉树校准的单目深度约束，以解决现有方法在几何监督上的不足。

Result: 在DTU和TNT数据集上的实验表明，该方法在几何精度上优于现有方法。

Conclusion: 通过引入高斯可见性感知的多视角几何一致性约束和渐进式四叉树校准的单目深度约束，该方法在几何精度上优于现有的基于高斯和隐式表面重建的方法。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [27] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT通过拓扑感知的最优传输框架，显著提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用深度拓扑数据分析（TDA）捕捉跨尺度的结构不变性，解决传统阈值二值化在分布偏移下的脆弱性问题。

Method: 结合多过滤持久图（PDs）与测试时适应（TTA），提出Optimal Transport Chaining技术，通过顺序对齐PDs生成稳定性评分。

Result: 在2D数据集上平均F1提升24.1%，3D异常分割基准上提升10.2%。

Conclusion: TopoOT框架在2D和3D异常检测基准测试中表现出色，性能显著优于现有方法。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [28] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: MMSF是一种多任务多模态框架，通过特征融合和Mamba编码器显著提升了计算病理学的预测性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中，多模态证据（如全幻灯片图像和临床描述符）的整合面临特征空间统计和尺度差异的挑战，因此需要一种有效的多模态融合方法。

Method: MMSF框架包含四个模块：图特征提取模块（嵌入组织拓扑结构）、临床数据嵌入模块（标准化患者属性）、特征融合模块（对齐模态共享和模态特定表示）以及基于Mamba的MIL编码器（带多任务预测头）。

Result: 在CAMELYON16和TCGA-NSCLC数据集上，MMSF相比基线方法提升了2.1-6.6%的准确率和2.2-6.9%的AUC；在五个TCGA生存队列中，C-index提升了7.1-9.8%（优于单模态方法）和5.6-7.1%（优于多模态方法）。

Conclusion: MMSF框架通过多任务和多模态的监督学习，显著提升了计算病理学中的预后预测性能，特别是在准确率、AUC和C-index等关键指标上优于现有方法。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [29] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge是一种基于向量量化的特征空间对齐框架，通过混合代表向量抑制领域偏移，提升开放集掌纹识别的性能。


<details>
  <summary>Details</summary>
Motivation: 解决掌纹识别中因异构部署条件导致的特征分布偏移问题，避免传统数据增强方法在显著领域不匹配时的失效。

Method: PalmBridge利用向量量化学习训练特征的紧凑代表向量集，通过最小距离准则将特征向量映射到最近的代表向量，并混合原始向量以抑制领域偏移带来的噪声变化。

Result: PalmBridge在多个掌纹数据集和架构上一致降低了EER，提升了跨数据集的泛化能力，且运行时开销可忽略至适度。

Conclusion: PalmBridge通过特征空间对齐框架有效提升了开放集掌纹识别的性能，减少了特征分布偏移的影响，并在多个数据集和架构上验证了其有效性。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [30] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SpatialGenEval 是评估 T2I 模型空间智能的新基准，通过信息密集的提示和数据集显著提升了模型在空间关系上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前 T2I 模型在复杂空间关系（如空间感知、推理或交互）上表现不佳，现有基准因提示设计简短或信息稀疏而无法有效评估这些方面。

Method: 通过设计包含 1,230 个信息密集的长提示和 15,400 个文本-图像对的数据集，覆盖 25 个真实场景和 10 个空间子领域，评估并微调了 21 个先进 T2I 模型。

Result: 评估显示高阶空间推理是主要瓶颈，微调后的模型性能显著提升（Stable Diffusion-XL +4.2%，Uniworld-V1 +5.7%，OmniGen2 +4.4%），空间关系表现更真实。

Conclusion: SpatialGenEval 和 SpatialT2I 数据集的引入不仅系统评估了 T2I 模型的空间智能，还通过数据中心的范式显著提升了模型在空间关系处理上的性能。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [31] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE是一个因果启发的框架，通过变分不确定性建模和结构正则化，提升场景图在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 场景图在场景理解中提供了结构化抽象，但常因过拟合虚假相关性而严重阻碍分布外泛化。

Method: CURVE采用原型条件去偏方法，解耦不变交互动态与环境依赖的变异，促进稀疏且领域稳定的拓扑结构。

Result: 在零样本迁移和低数据模拟到真实适应任务中，CURVE验证了其学习领域稳定稀疏拓扑的能力，并提供可靠的 uncertainty 估计以支持分布偏移下的风险预测。

Conclusion: CURVE框架通过整合变分不确定性建模和不确定性引导的结构正则化，有效抑制了高方差、环境特定的关系，提升了场景图在分布偏移下的泛化能力。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [32] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: RAW-Flow通过潜在传输和跨尺度引导，高效解决了RGB-to-RAW重构中的细节和颜色问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法将RGB-to-RAW重构视为直接回归任务，存在细节不一致和颜色偏差问题。

Method: 提出RAW-Flow框架，利用流匹配学习潜在空间中的确定性向量场，并结合跨尺度上下文引导模块和双域潜在自编码器。

Result: RAW-Flow在定量和视觉评估上均优于现有最先进方法。

Conclusion: RAW-Flow通过将RGB-to-RAW重构问题重新定义为确定性潜在传输问题，并引入跨尺度上下文引导模块和双域潜在自编码器，显著提升了重构的准确性和视觉质量。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [33] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 本文提出了一种双模态物联网框架，集成RFID访问控制与多传感器环境监测，通过云架构实现高效协同。实验显示高精度（99.2% RFID认证）与低成本（比商业方案低82%），适用于广泛场景。


<details>
  <summary>Details</summary>
Motivation: 传统方法将物理安全系统与环境安全监测系统作为独立孤岛运行，导致操作效率低下、应急响应延迟和管理复杂性增加。本文旨在通过集成这些系统，提高智能基础设施管理的效率和响应能力。

Method: 本文提出了一种全面的双模态物联网框架，通过统一的云架构无缝集成基于RFID的访问控制与多传感器环境安全监测。系统包括两个协调子系统：子系统1实现RFID认证与伺服驱动门控及实时Google Sheets日志记录，子系统2提供全面的安全监测，包括火焰检测、水流测量、LCD状态显示和人员识别。两个子系统均使用ESP32微控制器进行边缘处理和无线连接。

Result: 45天的实验评估显示，系统性能指标优异：RFID认证准确率99.2%，平均响应时间0.82秒；火焰检测可靠性98.5%（5米范围内）；云数据记录成功率99.8%。在网络中断时，系统通过智能本地缓存机制保持运行完整性，总实现成本为5,400 BDT（约48美元），比商业集成解决方案降低了82%。

Conclusion: 该研究建立了一个实用的安全-环境协同集成框架，通过精心设计的架构和组件优化，实现了专业级性能，同时保持了卓越的成本效益和广泛应用的可行性。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [34] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: RepSFNet是一种轻量级人群计数网络，通过高效特征提取和融合模块，在降低计算成本的同时保持高准确性，适用于实时边缘计算。


<details>
  <summary>Details</summary>
Motivation: 解决人群计数在变密度场景中因尺度变化、遮挡和现有模型高计算成本带来的挑战。

Method: RepSFNet采用RepLK-ViT骨干网络和大重参数化核进行多尺度特征提取，结合ASPP和CAN的特征融合模块实现密度自适应的上下文建模，并通过Concatenate Fusion模块保持空间分辨率生成高质量密度图。

Result: 在ShanghaiTech、NWPU和UCF-QNRF数据集上，RepSFNet在保持竞争力的准确性的同时，推理延迟降低了34%。

Conclusion: RepSFNet通过轻量级架构和高效的多尺度特征提取，在保持竞争力的准确性的同时显著降低了计算复杂度和推理延迟，适用于实时和低功耗的边缘计算应用。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [35] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA通过去除冗余图像块和文本描述，提升视觉-文本对齐效果，显著改进CLIP的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，细粒度文本描述和局部图像块常包含冗余信息，导致文本-视觉对齐效果不佳。

Method: BiFTA采用两种细化策略：视图细化（去除高IoU比的冗余图像块）和描述细化（去除高余弦相似度的冗余文本描述）。

Result: BiFTA在基于ViT和ResNet的CLIP模型上，均实现了优越的零样本性能。

Conclusion: BiFTA通过去除视觉和文本中的冗余信息，显著提升了预训练视觉语言模型（如CLIP）的零样本性能，在6个基准数据集上表现优异。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [36] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT是首个用于多人生成运动的自回归框架，通过分层交互建模和滑动窗口策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线方法在处理长文本或可变代理数量时存在固有局限性，这促使了自回归框架的开发，以逐步预测未来运动。

Method: HINT采用了一种解耦的运动表示方法，并在规范化的潜在空间中操作，将局部运动语义与人际交互分离。此外，它使用滑动窗口策略进行高效在线生成，并聚合局部和全局条件以保持长期一致性。

Result: 在InterHuman基准测试中，HINT实现了FID为3.100，显著优于之前的5.154。

Conclusion: HINT框架通过分层交互建模和扩散方法，在多人生成运动任务中表现出色，不仅匹配了离线模型的性能，还显著超越了现有的自回归基线。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [37] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 研究探讨了六种策略，利用部分标记数据训练WMH和ISL分割模型，发现伪标签方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 由于WMH和ISL在FLAIR序列中视觉上容易混淆，且常出现在同一受试者中，开发并验证能够分割和区分这两种特征的深度学习模型具有挑战性。

Method: 研究了六种训练策略，用于开发一个结合WMH和ISL分割的模型，并利用了部分标记数据。

Result: 结合私有和公开的部分标记数据集，共使用了2052个MRI体积数据，其中1341个包含WMH的真实标注，1152个包含ISL的真实标注。研究发现，多种方法能有效利用部分标记数据提升模型性能。

Conclusion: 研究发现，利用伪标签的方法在结合部分标记数据时表现最佳，能够有效提升模型性能。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [38] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 监督模型在训练领域表现优异但跨领域泛化能力差，语言对齐模型意外展现出跨领域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨不同训练范式在跨域应用中的鲁棒性，以及基础模型（如SigLIP2）在ReID任务中的作用。

Method: 比较了监督、自监督和语言对齐三种训练范式，分析了11种模型在9个数据集上的表现。

Result: 监督模型在训练领域表现最佳，但在跨领域数据中表现不佳；语言对齐模型展现出跨领域鲁棒性。

Conclusion: 监督模型在训练领域表现优异但在跨领域场景中表现不佳，而语言对齐模型展现出意外的跨领域鲁棒性。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [39] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: Quartet of Diffusions通过四个扩散模型解耦点云生成过程，首次实现对称性和部件先验的完全集成，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在形状生成中仅支持整体过程或部件组合的局限性，确保对称性、部件放置的连贯性和高质量输出。

Method: 通过四个协调的扩散模型（全局形状潜在、对称性、语义部件及其空间组装）来学习分布，解耦生成过程为可解释的组件。

Result: 实验表明，该框架在保证对称性、部件连贯性和多样性方面表现优异，支持对形状属性的细粒度控制。

Conclusion: Quartet of Diffusions框架首次在3D点云生成过程中完全集成并强制执行对称性和部件先验，实现了最先进的性能。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [40] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: CLEAR-Mamba 通过改进的架构和训练策略，提升了医学图像分类的泛化性和可靠性，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在泛化和高置信度预测方面的局限性，尤其是在眼底血管造影（FFA 和 ICGA）中的挑战。

Method: 提出了 CLEAR-Mamba 框架，包括基于超网络的 HaC 层和基于证据不确定性学习的 RaP 训练策略。

Result: CLEAR-Mamba 在多种指标上优于基线模型，尤其在多疾病分类和可靠性预测方面表现突出。

Conclusion: CLEAR-Mamba 提供了一种平衡泛化性和可靠性的有效解决方案，适用于特定模态的医学图像分类任务。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [41] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing是一种高效的文档解析模型，结合ViT和LLM，通过并行解码策略显著提升速度，适用于多种文档元素，并在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一个高效且多功能的文档解析模型，用于高性能内容提取。

Method: 采用原生Vision Transformer（ViT）和动态分辨率视觉编码器提取共享文档特征，结合提示引导的Youtu-LLM-2B语言模型进行布局分析和区域提示解码。

Result: 在OmniDocBench和olmOCR-bench基准测试中实现了最先进的性能。

Conclusion: Youtu-Parsing展示了在大规模文档智能应用中的显著实验价值和实用效用。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [42] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: GDCNet利用MLLMs生成稳定图像标题作为锚点，通过语义和情感差异检测多模态讽刺，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉和文本内容松散关联或语义间接时表现不佳，且大语言模型生成的讽刺线索因多样性和主观性引入噪声。

Method: 提出Generative Discrepancy Comparison Network (GDCNet)，利用多模态大语言模型（MLLMs）生成描述性图像标题作为语义锚点，计算语义、情感差异及视觉-文本保真度，并通过门控模块自适应平衡模态贡献。

Result: 在MSD基准测试（如MMSD2.0）中取得最优性能。

Conclusion: GDCNet通过生成稳定的语义锚点（描述性图像标题）并计算语义和情感差异，有效解决了多模态讽刺检测中的噪声问题，在MSD基准测试中表现出卓越的准确性和鲁棒性。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [43] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE利用多模态对齐和强化学习提升视觉语言模型的Deepfake检测能力，通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，Deepfake检测面临新的挑战，现有方法主要局限于分类或空间定位，需要更准确可靠的检测方法。

Method: MARE设计了全面的奖励函数，结合人类反馈的强化学习（RLHF），激励生成与人类偏好一致的文本-空间对齐推理内容，并引入了伪造解耦模块以捕获高级面部语义中的伪造痕迹。

Result: 定量和定性实验结果表明，MARE在准确性和可靠性方面达到了最先进的性能。

Conclusion: MARE通过多模态对齐和强化学习，显著提升了视觉语言模型在Deepfake检测中的准确性和可靠性，并通过实验验证了其优越性能。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [44] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF是一种标签高效的图像质量评估框架，通过从MLLM教师模型中蒸馏感知质量先验到轻量级学生回归器，减少了人工标注需求，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在图像质量评估（IQA）任务中表现出强大的能力，但其适应过程计算成本高昂且仍依赖大量MOS标注。作者认为，MLLM-based IQA的核心瓶颈在于MOS尺度校准，而非MLLMs的质量感知能力。

Method: LEAF框架利用MLLM教师模型进行密集监督，通过点级判断和成对偏好以及决策可靠性的估计，指导学生回归器学习教师的质量感知模式，并在少量MOS子集上进行校准以与人类标注对齐。

Result: 在用户生成和AI生成的IQA基准测试中，LEAF方法显著减少了人工标注的需求，同时保持了与MOS对齐的强相关性。

Conclusion: LEAF框架通过从MLLM教师模型中提取感知质量先验，并将其蒸馏到轻量级学生回归器中，显著减少了人工标注的需求，同时保持了与MOS对齐的强相关性，使得在有限标注预算下实现轻量级IQA成为可能。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [45] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 利用生成器最终组件‘污染’真实图像，训练检测器在未见过的生成器上实现高准确率（98.83%）。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器对未见过的生成器泛化能力差，而现代图像生成器尽管训练范式不同，但共享相似的最终架构组件。利用这一共性提升检测器的泛化能力。

Method: 提出一种方法，通过‘污染’真实图像（利用生成器的最终组件）并训练检测器来区分原始图像与‘污染’图像。基于生成器最终组件的分类法对21种生成器进行分类，以验证方法的泛化能力。

Result: 在22个未见过的生成器测试集上，仅用每类100个样本训练的检测器平均准确率达98.83%。

Conclusion: 通过利用图像生成器的最终组件‘污染’真实图像并训练检测器，该方法在未见过的生成器上表现出卓越的泛化能力，平均准确率达98.83%。

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [46] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON是一个专注于STEM讲座视频的多模态评估基准，旨在测试长时推理和跨模态整合能力，揭示了当前MLLMs在时间推理和教学预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在长形式、知识密集和时间结构化的教育内容上的表现尚未充分探索，因此需要一个新的基准测试来填补这一空白。

Method: LEMON包含2,277个视频片段，覆盖5个学科和29门课程，平均时长196.1秒，生成4,181个高质量问答对，包括3,413个多项选择题和768个开放性问题。

Result: 实验结果显示，即使是如GPT-4o这样的先进MLLMs，在时间推理和教学预测任务上也存在显著的性能差距。

Conclusion: LEMON被设计为一个可扩展且具有挑战性的基准测试，旨在推动多模态感知、推理和生成在长形式教学内容中的发展。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [47] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: Dummy Forcing通过优化多头自注意力的历史帧利用和缓存压缩，显著提升视频生成速度，几乎不影响质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型中的多头自注意力存在历史帧利用率低的问题，约25%的头几乎只关注当前帧，丢弃其KV缓存仅导致轻微性能下降。

Method: 提出Dummy Forcing方法，包括异质内存分配和动态头编程，以减少多头自注意力中的冗余，并结合上下文打包技术实现更激进的缓存压缩。

Result: 在不增加训练的情况下，Dummy Forcing实现了比基线快2.0倍的速度，支持24.3 FPS的视频生成，质量损失小于0.5%。

Conclusion: Dummy Forcing方法通过减少多头自注意力中的冗余并动态分类头类型，实现了高效的视频生成，速度提升2.0倍且质量损失小于0.5%。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [48] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V是一种无需训练的去偏框架，通过中和提示嵌入减少文本到视频生成中的性别偏见，保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 探索文本到视频扩散模型中的性别偏见，并提出无需微调的去偏框架。

Method: FairT2V通过基于锚点的球面测地变换中和提示嵌入，并在早期身份形成步骤中应用去偏，以保持时间连贯性。

Result: FairT2V显著减少了职业相关的性别偏见，且对视频质量影响最小。

Conclusion: FairT2V通过无需训练的方法有效减少了文本到视频生成中的性别偏见，同时保持了视频质量。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [49] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 提出LTD运动先验，通过动态损失权重提升动态视频生成质量，实验显示显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在静态场景中表现良好，但在动态视频生成中因噪声干扰时间连贯性和动态区域学习困难，导致质量下降。

Method: 提出潜在时间差异（LTD）作为运动先验，通过测量潜在空间中帧间差异来动态调整损失权重，对高差异区域施加更大惩罚，同时保持稳定区域的常规优化。

Result: 在通用基准VBench和运动专注的VMBench上，该方法分别以3.31%和3.58%的优势超越基线模型，显著提升了运动质量。

Conclusion: 通过引入潜在时间差异（LTD）作为运动先验指导损失权重分配，该方法显著提升了动态视频生成的质量，尤其在剧烈动态变化场景中表现优异。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [50] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI是一个无训练框架，通过功能感知推理和优化，生成功能正确且物理合理的3D人-场景交互。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D人类与场景功能交互中缺乏对对象功能性和人-场景接触的显式推理，导致交互不自然或功能错误的问题。

Method: FunHSI框架结合功能感知接触推理、3D几何重建、接触图建模、视觉语言模型合成及阶段优化，实现了从开放词汇任务提示到3D人体姿态的生成与优化。

Result: 实验表明，FunHSI在多样化的室内外场景中一致生成功能正确且物理上合理的人-场景交互。

Conclusion: FunHSI框架通过功能驱动的无训练方法，成功生成了功能正确且物理上合理的3D人类与场景交互，适用于多样化的室内外场景。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [51] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 提出PCG任务及解决方案CHEESE数据集和SCheese框架，通过文本引导和分层保留实现高质量肖像集合生成。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台用户对高质量、多样化肖像集合的需求，克服现有方法在复杂多属性修改和高保真细节保留上的挑战。

Method: 提出了CHEESE数据集（24K肖像集合和573K样本）和SCheese框架，结合文本引导生成与分层身份和细节保留，采用自适应特征融合机制和ConsistencyNet。

Result: SCheese在PCG任务中实现了最先进的性能，CHEESE数据集有效推动了该领域的发展。

Conclusion: CHEESE数据集和SCheese框架在肖像集合生成（PCG）任务中表现出色，SCheese通过自适应特征融合和ConsistencyNet实现了身份和细节的一致性，达到了最先进的性能。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [52] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出了一种融合图像和惯性测量的多模态框架，并引入了一个包含真实世界、视觉和合成数据的新数据集ROAD，显著提升了路面分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的路面分类技术由于传感模态有限和数据集缺乏环境多样性，难以在狭窄的操作条件之外泛化。

Method: 引入了一个多模态框架，通过轻量级双向交叉注意力模块和自适应门控层融合图像和惯性测量数据，调整模态贡献以适应领域偏移。

Result: 在PVS基准测试上比之前的最先进方法提高了1.4个百分点，在多模态ROAD子集上提高了11.6个百分点，且在少数类上F1分数持续更高。

Conclusion: 结合经济型摄像头和IMU传感器与多模态注意力机制，为路面理解提供了一个可扩展且稳健的基础，特别适用于环境多变且成本受限的地区。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [53] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 本文提出CoTA方法，通过分析信息流机制解决dMLLMs中的重复生成问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式多模态大语言模型（dMLLMs）因高推理延迟依赖缓存技术，但缓存机制常导致重复文本生成（即“重复诅咒”），需探究其机制并解决。

Method: 通过分析信息流视角下的重复生成机制，提出了CoTA方法，该方法通过增强上下文令牌的注意力和引入解码时的惩罚项来避免重复。

Result: CoTA在缓解重复生成方面表现出显著效果，并在通用任务上实现了性能提升。

Conclusion: CoTA方法通过增强上下文令牌的注意力并引入惩罚项，有效缓解了重复生成问题，并在通用任务上实现了性能提升。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [54] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: AnomalyVFM通过合成数据集和高效适应机制，显著提升了零样本异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉基础模型（VFMs）的零样本异常检测方法性能落后于基于视觉语言模型（VLMs）的方法，主要原因是辅助数据集多样性不足和VFM适应策略过于浅层。

Method: 提出了AnomalyVFM框架，包括三阶段合成数据集生成和参数高效适应机制（低秩特征适配器和置信加权像素损失）。

Result: 使用RADIO作为骨干网络，AnomalyVFM在9个不同数据集上平均图像级AUROC达到94.1%，比之前方法提升了3.3个百分点。

Conclusion: AnomalyVFM通过结合三阶段合成数据集生成方案和参数高效适应机制，显著提升了基于视觉基础模型（VFMs）的零样本异常检测性能，超越了现有最先进方法。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [55] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA框架结合数据驱动和知识驱动模块，通过纠正知识提升预训练模型的下游任务适应能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效调优方法将预训练模型视为黑盒，仅依赖数据驱动优化，未能充分利用其内在先验知识，限制了模型在下游任务中的适应潜力。

Method: 提出了一种新颖的黑白盒提示学习框架（IOTA），通过白盒模块从错误预测与正确认知的对比中提取纠正知识，并将其转化为可解释的人类提示，通过纠正知识引导的提示选择策略指导黑盒模块。

Result: 在12个图像分类基准测试中，IOTA在少样本和易到难适应设置下均表现出色，验证了纠正知识的有效性和方法的优越性。

Conclusion: IOTA框架通过结合数据驱动的黑盒模块和知识驱动的白盒模块，有效提升了预训练模型在下游任务中的适应能力，实验证明了其在少样本和易到难适应设置下的优越性。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [56] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World是一个开源的世界模拟器，具备高保真度、长期记忆和实时交互性，适用于多种环境和应用场景。


<details>
  <summary>Details</summary>
Motivation: 开发LingBot-World的目的是提供一个开源的高性能世界模型，以推动开源社区在内容创作、游戏和机器人学习等领域的创新和应用。

Method: 基于视频生成技术，LingBot-World在多种环境中保持高保真度和动态鲁棒性，支持分钟级时间跨度的上下文一致性（长期记忆），并实现实时交互（每秒16帧，延迟低于1秒）。

Result: LingBot-World成功实现了高保真度、长期记忆和实时交互性，代码和模型已公开，为社区提供了实用的工具。

Conclusion: LingBot-World作为一个开源的世界模拟器，通过视频生成技术提供了高保真度、长期记忆和实时交互性，旨在缩小开源与闭源技术之间的差距，并在内容创作、游戏和机器人学习等领域具有广泛应用潜力。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [57] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2提出了一种动态重排视觉token的编码器DeepEncoder V2，旨在模拟人类视觉感知的因果推理能力，为2D图像理解提供新思路。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型（VLMs）以固定的光栅扫描顺序处理视觉token，这与人类灵活且语义连贯的视觉感知模式相矛盾。

Method: 设计了DeepEncoder V2，赋予编码器因果推理能力，使其能够在基于LLM的内容解释前智能地重排视觉token。

Result: 探索了通过两个级联的1D因果推理结构有效实现2D图像理解的可能性，并公开了代码和模型权重。

Conclusion: DeepSeek-OCR 2通过动态重排视觉token的机制，探索了2D图像理解的新范式，提出了一种可能实现真正2D推理的架构方法。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [58] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: DiffVC-RT 是首个实时扩散式感知神经视频压缩框架，通过高效架构、一致性建模和并行解码技术，显著提升性能并实现实时处理。


<details>
  <summary>Details</summary>
Motivation: 解决扩散式神经视频压缩在实际部署中面临的信息丢失、高延迟和时间一致性差等关键挑战。

Method: 采用高效信息模型架构、显式和隐式一致性建模以及异步并行解码管道，结合混合半精度技术。

Result: DiffVC-RT 在 HEVC 数据集上实现了 80.1% 的比特率节省（基于 LPIPS），并在 NVIDIA H800 GPU 上达到 206/30 fps 的实时编码解码速度（720p 视频）。

Conclusion: DiffVC-RT 在扩散式神经视频压缩领域实现了实时性能，显著降低了比特率并提高了处理速度，标志着该技术的重要里程碑。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [59] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: StructAlign是一种结构化跨模态对齐方法，通过ETF几何和关系保持损失解决CTVR中的特征漂移问题，显著减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: CTVR（持续文本-视频检索）在多模态持续学习环境中面临灾难性遗忘的挑战，特别是特征漂移问题，包括模态内和跨模态的特征漂移。

Method: StructAlign采用等角紧框架（ETF）几何作为统一几何先验，设计了跨模态ETF对齐损失和跨模态关系保持损失，以解决模态间非合作特征漂移和模态内特征漂移问题。

Result: StructAlign在基准数据集上的实验表明，其性能优于现有的持续检索方法。

Conclusion: StructAlign通过引入结构化跨模态对齐方法，有效缓解了CTVR中的灾难性遗忘问题，并在基准数据集上表现优于现有方法。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [60] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon 是一个包含242个长周期重复任务的评估基准，通过少量示例教授代理工作流逻辑，有效提升任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 解决长周期重复工作流评估基准的缺失问题，提升计算机使用代理（CUAs）在这些任务中的表现。

Method: 提出了一种成本效益高的方法，通过少量示例构建浓缩演示，教授代理工作流逻辑，使其能够在未见过的数据集合上有效执行类似工作流。

Result: 实验证明了这些任务的固有挑战以及所提方法的有效性。

Conclusion: OS-Marathon 提供了一个有效的评估基准，并通过少量示例构建的演示方法，显著提升了计算机使用代理（CUAs）处理长周期重复任务的能力。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [61] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: A lightweight, frequency-based approach improves morph detection in cross-dataset scenarios by analyzing spectral residuals and combining regional evidence, outperforming deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Face morphing attacks present a significant threat to face recognition systems, especially in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. Existing methods struggle in cross-dataset scenarios.

Method: A region-aware frequency-based morph detection strategy that decouples the frequency of the signal from the natural spectral decay and combines evidence from different facial regions in a Markov Random Field.

Result: The proposed method achieves an average EER of 1.85% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12%, demonstrating strong performance in cross-dataset and cross-morph settings.

Conclusion: Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [62] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: ProSkill是首个针对程序性任务中动作级技能评估的基准数据集，通过创新的标注协议提供绝对和成对评估标注，现有算法表现不佳，突显其挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估研究主要集中在体育领域，缺乏针对复杂程序性活动的大规模数据集，且评估方式有限（成对或二元标签）。

Method: 采用瑞士锦标赛方案进行高效的成对比较，并通过基于ELO的评分系统将这些比较聚合成一致的连续全局分数。

Result: ProSkill数据集为程序性任务中的动作级技能评估提供了基准，现有最先进算法在该数据集上的表现不佳，突显了其挑战性和价值。

Conclusion: ProSkill数据集通过引入绝对技能评估标注和创新的标注协议，为程序性视频中的技能评估提供了首个基准数据集，突显了当前最先进算法的不足及其价值。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [63] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: BiMoRS是一种针对遥感任务的轻量级双模态提示学习框架，通过融合文本和视觉特征生成上下文提示，显著提升了在遥感数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感数据具有多标签场景、高类内变异性和多样化的空间分辨率等独特挑战，现有的提示学习方法难以直接应用。BiMoRS旨在解决这些挑战，提升遥感场景中新类别的泛化能力。

Method: BiMoRS利用冻结的图像描述模型（如BLIP-2）从遥感图像中提取文本语义摘要，通过BERT分词器进行标记化，并与CLIP编码器的高级视觉特征融合。轻量级交叉注意力模块基于融合的文本-视觉表示生成可学习的查询提示，从而在不改变CLIP主干的情况下实现上下文化的提示。

Result: 在四个遥感数据集上的三个领域泛化任务中，BiMoRS表现优于强基线方法，平均性能提升高达2%。

Conclusion: BiMoRS提出了一种轻量级的双模态提示学习框架，针对遥感任务中的独特挑战进行了优化，显著提升了在多个遥感数据集上的性能表现。

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [64] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 本文综述并统一了视觉编码和视觉令牌技术，探讨了压缩效率与模型性能的权衡，并展望了下一代技术及其在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究压缩效率与模型性能之间的关系，以推动多模态大语言模型（MLLMs）和生成式多模态大模型的发展。

Method: 首先综述了视觉编码和视觉令牌技术，然后从优化的角度统一了它们，讨论了压缩效率和模型性能之间的权衡。

Result: 实验证明了任务导向令牌在多模态LLMs、AI生成内容（AIGC）和具身AI等实际任务中的潜力。

Conclusion: 本文通过统一视觉编码和视觉令牌技术，提出了下一代视觉编解码和令牌技术的展望，并展示了任务导向令牌在实际应用中的巨大潜力。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix是一种免微调方法，利用预训练图像扩散模型优化外推视图渲染，在泛化性和保真度之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在泛化性与保真度之间的权衡问题，即微调扩散模型可能过拟合，而免微调方法保真度较低。

Method: 提出了一种免微调方法FreeFix，结合预训练图像扩散模型和2D-3D交错优化策略，并引入逐像素置信度掩码以定位不确定区域进行针对性优化。

Result: 实验表明，FreeFix在多帧一致性和渲染质量上表现优异，性能接近或超越基于微调的方法，同时保持强泛化能力。

Conclusion: FreeFix通过引入预训练图像扩散模型和2D-3D交错优化策略，在保持泛化能力的同时，显著提升了外推视图的渲染质量，性能媲美甚至超越基于微调的方法。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 本文探讨神经科学与AI的协同潜力，提出NeuroAI概念，认为其能提升AI效率并深化对神经计算的理解，附有专家观点和SWOT分析。


<details>
  <summary>Details</summary>
Motivation: 神经科学与AI在过去几年取得了显著进展，但两者之间的联系较为松散。本文旨在探讨这两个领域的协同潜力。

Method: 基于2025年8月的一次研讨会，识别了神经科学与AI之间当前和未来的协同领域，并聚焦于具身性、语言与通信、机器人学、人类与机器学习及神经形态工程等子领域。

Result: 提出了NeuroAI的概念，并附有领先研究者的个人观点及研究人员和学员的两项SWOT分析，描述了NeuroAI的益处与风险。

Conclusion: 本文主张发展NeuroAI，一种神经科学启发的人工智能，认为其有潜力显著提升AI算法的范围和效率，同时改变我们对生物神经计算的理解。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [67] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP通过双向搜索和验证器，有效解决了部分可观测下的计划生成问题，显著降低违规率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在部分可观测条件下容易产生幻觉或违反硬约束的问题。

Method: SQ-BCP采用双向搜索，利用基于距离的分数进行排序和剪枝，并通过拉回验证器作为目标兼容性的分类证明。

Result: 在WikiHow和RecipeNLG任务中，SQ-BCP将资源违规率降至14.9%和5.8%，优于基线。

Conclusion: SQ-BCP通过双向搜索和基于拉回的验证器，在有限分支和深度下能够找到符合目标的计划，显著降低了资源违规率。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [68] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: FCP是一种模糊范畴论规划方法，通过分级适用性和质量组合改进自然语言规划，在食谱替代任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言规划常涉及模糊谓词（如“合适的替代品”、“足够稳定”），其满足程度本质上是分级的。现有范畴论规划器虽然提供了组合结构和基于拉回的硬约束验证，但将适用性视为明确的，导致阈值化处理会抹去有意义的区别，且无法跟踪多步计划中的质量退化。

Method: FCP通过为每个动作（态射）标注[0,1]区间内的程度值，使用Lukasiewicz t-范数组合计划质量，并通过拉回验证保留严格的执行检查。

Result: FCP在（i）公共PDDL3偏好/超额订阅基准和（ii）RecipeNLG-Subs（一个基于RecipeNLG构建的缺失替代品食谱规划基准）上进行了评估，结果显示其在RecipeNLG-Subs上提高了成功率并减少了硬约束违反。

Conclusion: Fuzzy Category-theoretic Planning (FCP) 在RecipeNLG-Subs基准测试中表现优于纯LLM和ReAct风格的基线方法，同时在经典PDDL3规划器中保持竞争力。

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [69] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: 开发了Insight Agents (IA)系统，通过分层多代理结构和ML优化，为电商卖家提供高效、准确的数据洞察，实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决电商卖家在发现和利用现有工具及数据时的困难，通过自动化信息检索提供个性化数据和业务洞察。

Method: 采用基于计划和执行范式的分层多代理结构，包括管理代理和两个工作代理（数据展示和洞察生成），结合轻量级编码器-解码器模型进行OOD检测和BERT分类器进行代理路由，优化准确性和延迟。

Result: IA系统在美国亚马逊卖家中的实际应用显示，人工评估准确率达90%，P90延迟低于15秒。

Conclusion: Insight Agents (IA) 系统通过其分层多代理结构和高效的ML解决方案，显著提升了电商卖家在数据洞察和业务决策方面的效率，实现了高准确性和低延迟。

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [70] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: 论文介绍了一个框架，用于在LLM驱动的代理控制场景中进行反事实推理，并通过SCM和概率溯因提供可靠性保证，实验证明其在无线网络控制中的有效性。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM驱动的代理时，可能会好奇如果意图表达方式不同会有什么结果，因此需要一个支持反事实推理的框架。

Method: 利用结构因果模型（SCM）建模用户、LLM代理和环境之间的闭环交互，通过概率溯因生成多个候选反事实结果，并通过离线校准阶段确保结果的可靠性。

Result: 在无线网络控制用例中，提出的CCG方法相比简单的重新执行基线显示出显著优势。

Conclusion: 论文提出了一个框架，通过结构因果模型（SCM）和概率溯因，在LLM驱动的代理控制场景中实现反事实推理，并提供了形式化的可靠性保证。

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [71] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: 本文提出一种多模态LLM代理框架，通过数据对齐和工具包设计，解决了城市公园发展监测中的多模态数据分析挑战。


<details>
  <summary>Details</summary>
Motivation: 传统遥感影像变化检测方法在高水平和智能化分析方面存在明显局限性，难以满足当前城市规划管理的需求。

Method: 设计了一种通用的水平和垂直数据对齐机制，并构建了特定工具包以缓解LLM因缺乏领域知识而产生的幻觉问题。

Result: 与普通GPT-4o及其他代理相比，该方法实现了稳健的多模态信息融合与分析。

Conclusion: 本研究提出的多模态LLM代理框架为城市公园发展监测提供了可靠且可扩展的解决方案，显著提升了高水平和智能化分析能力。

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [72] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 提出了一种迭代查询外部医学语料库的验证框架，显著提升医学推理准确性并降低采样成本。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型方法在医学推理验证中存在两个局限性：仅产生标量奖励值而无明确理由，且依赖单次检索无法适应验证过程中的知识访问需求。

Method: 训练医学推理验证器在评估过程中迭代查询外部医学语料库，结合工具增强验证和迭代强化学习范式，仅需轨迹级监督，并采用自适应课程机制动态调整训练数据分布。

Result: 在四个医学推理基准测试中，相比基线方法，MedQA准确率提高了23.5%，MedXpertQA提高了32.0%，采样预算需求减少了8倍。

Conclusion: 该方法通过动态检索证据为医学推理系统提供了更可靠的验证路径，显著提高了准确性并降低了采样预算需求。

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [73] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: SEER通过内生循环和自我对齐描述符，有效提升多模态模型的生成质量与评估能力。


<details>
  <summary>Details</summary>
Motivation: 解决UMMs在理解与生成之间的认知差距问题，提升模型生成过程的自我指导能力。

Method: 提出了Endogenous Reprompting机制和SEER训练框架，结合RLVR和RLMT两阶段内生循环，仅需300个样本即可激活模型的潜在评估能力并优化生成策略。

Result: 实验表明SEER在评估准确性、重新提示效率和生成质量上均优于现有基线，且不损害多模态能力。

Conclusion: SEER框架通过内生循环和自我对齐描述符的生成，有效弥合了认知差距，显著提升了生成质量和评估准确性，同时保持了通用的多模态能力。

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [74] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: ECG-Agent是首个基于LLM的多轮ECG对话代理，解决了现有模型的不足，并通过实验验证了其高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在ECG领域缺乏多轮对话能力、设备端效率和精确的ECG测量理解（如PQRST间期），因此需要开发更适应实际场景的解决方案。

Method: 开发了不同规模的ECG-Agent，从适用于设备端的轻量级到大型代理，并利用ECG-MTD数据集进行开发和评估。

Result: 实验结果表明，ECG-Agent在响应准确性、工具调用能力和减少幻觉方面表现优异，设备端代理在多项评估中与大型代理性能相当。

Conclusion: ECG-Agent作为首个基于LLM的多轮ECG对话工具调用代理，不仅在响应准确性上优于基线ECG-LLMs，还展示了在设备端应用的可行性。

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [75] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA是一种通过多智能体协作动态管理记忆的框架，解决了现有记忆系统的不足，显著提升了检索精度和一致性，并大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统存在检索粒度僵化、维护策略积累负担重、更新机制粗粒度等问题，导致存储信息与任务需求不匹配及逻辑不一致性累积。

Method: AMA采用分层记忆设计，通过Constructor和Retriever实现多粒度记忆构建与自适应查询路由，Judge验证内容相关性与一致性，Refresher执行针对性更新或删除过时条目。

Result: 在长上下文基准测试中，AMA显著优于现有基线方法，同时相比全上下文方法减少约80%的token消耗。

Conclusion: AMA框架通过多智能体协作动态管理不同粒度的记忆，显著提高了检索精度和长期记忆一致性，同时大幅降低了token消耗。

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [76] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: PoT框架通过实时策略优化和反馈学习，显著提升LLM在复杂推理任务中的表现，4B模型性能超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在复杂长时推理任务中因固定策略假设导致的不稳定性问题，提出动态策略优化的必要性。

Method: PoT采用高效的探索机制生成多样化候选解决方案，并通过Group Relative Policy Optimization (GRPO)更新瞬态LoRA适配器，实现实时策略优化。

Result: 实验结果显示，4B参数的PoT模型在LiveCodeBench上达到49.71%的准确率，优于GPT-4o和DeepSeek-V3。

Conclusion: PoT框架通过实时优化和反馈学习，显著提升了大型语言模型在复杂长时推理任务中的表现，证明了动态策略调整的有效性。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [77] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: OmegaUse是一款跨终端GUI代理模型，通过高质量数据合成和两阶段训练方法，在多个基准测试中实现领先性能。


<details>
  <summary>Details</summary>
Motivation: 提升GUI代理模型的通用性和跨终端任务执行能力，以革新人机交互并提升生产力。

Method: 采用两阶段训练策略：监督微调（SFT）建立基础交互语法，随后通过组相对策略优化（GRPO）提升空间定位和序列规划能力；基于混合专家（MoE）架构平衡计算效率与推理能力。

Result: 在多个基准测试中表现优异，包括ScreenSpot-V2（96.3%）、AndroidControl（79.1%步骤成功率）、ChiM-Nav（74.24%步骤成功率）和Ubu-Nav（55.9%平均成功率）。

Conclusion: OmegaUse作为一款通用GUI代理模型，在移动和桌面平台上均表现出色，通过高质量数据和创新训练方法实现了跨终端任务的高效执行，并在多个基准测试中达到领先水平。

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [78] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT通过多粒度压缩和逻辑保留，显著提升CoT推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在语义层面保守或token剪枝激进，导致准确率下降或任务关键线索丢失。

Method: 提出了CtrlCoT框架，包含层次化推理抽象、逻辑保留蒸馏和分布对齐生成三个组件。

Result: 在MATH-500数据集上，CtrlCoT减少了30.7%的token使用，同时准确率比最强基线提高了7.6个百分点。

Conclusion: CtrlCoT通过多粒度语义抽象和逻辑保留剪枝，实现了更高效可靠的推理，显著减少了token使用并提高了准确性。

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [79] [Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups](https://arxiv.org/abs/2601.20487)
*Nico Mutzner,Taha Yasseri,Heiko Rauhut*

Main category: cs.AI

TL;DR: 研究发现AI代理在群体合作中与人类行为无显著差异，合作规范可扩展到AI，模糊人机界限。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能代理如何影响小群体中合作规范的出现和维持，填补了之前研究主要集中在二元互动上的空白。

Method: 通过在线实验，使用重复的四玩家公共物品游戏（PGG），每组由三名人类参与者和一个机器人组成，机器人被标记为人类或AI，并遵循三种预定义的决策策略之一。

Result: 合作水平在人类和AI标签之间没有显著差异，合作依赖于群体行为而非伙伴身份，支持规范性等效模式。

Conclusion: 该研究发现合作规范具有足够的灵活性，可以扩展到人工智能代理，模糊了人类与AI在集体决策中的界限。

Abstract: The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.

</details>


### [80] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: PathWise是一个多智能体推理框架，通过状态感知规划和序列决策提升LLM在组合优化问题中的启发式设计效率，实验证明其高效且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计框架依赖固定的进化规则和静态提示模板，导致启发式生成短视、评估冗余且缺乏对新启发式推导的推理能力。

Method: 提出了一种名为PathWise的多智能体推理框架，将启发式生成建模为基于蕴含图的序列决策过程，包含策略智能体、世界模型智能体和批评智能体，分别负责规划进化动作、生成启发式展开和提供路由反思。

Result: 实验表明，PathWise在多样化的组合优化问题上能更快收敛到更优启发式，且具备跨LLM骨干和更大问题规模的泛化能力。

Conclusion: PathWise框架通过多智能体推理和状态感知规划，显著提升了组合优化问题中启发式设计的效率和效果，能够更快收敛到更优启发式，并具备跨不同LLM骨干和更大问题规模的泛化能力。

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [81] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 论文提出了一种基于ICVaR的风险敏感规划方法，扩展了三种在线规划算法，并通过实验验证了其在降低尾部风险方面的优势。


<details>
  <summary>Details</summary>
Motivation: 研究在部分可观测环境下进行风险敏感规划的需求，通过引入ICVaR动态风险度量来解决这一问题。

Method: 开发了一种针对ICVaR的策略评估算法，并扩展了三种在线规划算法（Sparse Sampling、PFT-DPW、POMCPOW）以优化ICVaR价值函数。

Result: 实验结果表明，提出的ICVaR规划器在基准POMDP领域中相比风险中性规划器能够有效降低尾部风险。

Conclusion: 论文通过引入动态风险度量ICVaR，成功开发了一种在部分可观测环境下进行风险敏感规划的方法，并通过实验验证了其在降低尾部风险方面的有效性。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [82] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 论文提出了一种通过多模型对话测试AI对齐策略的方法，初步证明AI能参与复杂对话并生成新见解，但存在局限性，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 通过借鉴和平研究传统，将AI对齐问题从控制问题重新定义为通过对话式推理发展的关系问题，以验证当前AI系统是否能实质性参与复杂对齐框架。

Method: 采用结构化多模型对话方法，将四种不同角色分配给不同AI系统，在六种条件下测试大型语言模型是否能有效参与复杂对齐框架。使用了Claude、Gemini和GPT-4o进行72轮对话。

Result: 结果表明，AI系统能有效参与和平研究概念，提出不同架构视角的互补异议，并生成初始框架中未出现的新见解。不同模型关注点各异：Claude强调验证挑战，Gemini关注偏见和可扩展性，GPT-4o突出实施障碍。

Conclusion: 该论文提出了一个可复制的方法框架，用于在实际实施前测试AI对齐提案，并初步证明了AI进行对话式推理的能力。同时指出了当前对话更多关注过程元素而非AI本质的局限性，并提出了未来研究方向。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [83] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge框架通过DGPO算法和MQR策略，针对数学推理中的难题进行优化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在算法和数据层面均缺乏对更具挑战性问题的关注，这限制了模型在数学推理方面的能力提升。

Method: 提出了MathForge框架，包含Difficulty-Aware Group Policy Optimization (DGPO)算法和Multi-Aspect Question Reformulation (MQR)策略。DGPO通过难度平衡的组优势估计纠正GRPO的隐式不平衡，并通过难度感知的问题级权重优先处理更难的问题。MQR通过多方面的重新表述增加问题难度，同时保留原始正确答案。

Result: MathForge在多种数学推理任务上显著优于现有方法。

Conclusion: MathForge框架通过结合DGPO算法和MQR策略，显著提升了数学推理任务的性能，并提供了开源代码和增强数据。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [84] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: 研究发现LLM代理能开发高效且隐蔽的任务导向通信协议，参考游戏框架是未来研究的有效测试平台。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理是否能开发出不同于标准自然语言的任务导向通信协议，重点关注其效率和隐蔽性。

Method: 研究采用了参考游戏框架，通过视觉语言模型（VLM）代理在协作推理任务中的通信行为，评估语言变体的效率和隐蔽性。

Result: 实验显示，VLM代理能够开发出高效且任务适应的通信模式，同时也能形成难以被人类或外部代理解读的隐蔽协议。相似模型之间还能自发协调，无需显式共享协议。

Conclusion: 研究结果表明，基于LLM的代理能够开发出既高效又隐蔽的任务导向通信协议，这既展示了其潜力，也带来了透明度和控制方面的风险。参考游戏框架被证明是未来研究的有效测试平台。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [85] [Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry](https://arxiv.org/abs/2601.20696)
*Samira Yazdanpourmoghadam,Mahan Balal Pour,Vahid Partovi Nia*

Main category: cs.AI

TL;DR: 论文提出使用多类型Transformer（MTT）解决组合优化问题，实验证明其在JSP和KP基准数据集上表现优异，并首次在实际制造中应用。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题（如JSP和KP）在运筹学和物流等领域具有重要挑战，传统算法难以在有限时间内达到近优解。

Method: 利用多类型Transformer（MTT）架构，对JSP和KP标准基准数据集进行了广泛的实验评估。

Result: MTT在不同规模的基准问题上表现优异，并在实际制造应用中展示了多类型注意力的潜力。

Conclusion: 该论文展示了多类型Transformer（MTT）架构在解决组合优化问题（如JSP和KP）中的有效性，并在实际制造应用中验证了其潜力。

Abstract: Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.

</details>


### [86] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 提出了一种处理度量ASP中时间约束的方法，利用差异约束外部处理时间，避免因时间粒度导致的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 为了在度量答案集编程中表达定量时间约束（如持续时间和截止时间），同时保持处理细粒度时间约束时的可扩展性。

Method: 利用ASP的扩展，特别是差异约束（一种简化的线性约束），来处理时间相关方面，从而避免因细粒度时间约束而加剧的ASP基础瓶颈问题。

Result: 开发了一种不受时间精度影响的解决方案，有效地解耦了度量ASP与时间粒度的关系。

Conclusion: 该论文提出了一种计算方法来处理度量答案集编程（ASP）中的定量时间约束，通过外部处理时间相关方面，有效地解耦了度量ASP与时间粒度的关系，从而不受时间精度的影响。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [87] [REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence](https://arxiv.org/abs/2601.20784)
*Zishen Wan,Che-Kai Liu,Jiayi Qian,Hanchen Yang,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AI

TL;DR: REASON是一个针对神经符号AI中概率逻辑推理的加速框架，通过统一的DAG表示和优化的硬件架构，显著提升了效率和能效。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI系统在部署时面临符号和概率推理效率低下的挑战，尤其是在概率逻辑推理方面存在瓶颈。

Method: REASON引入了一种统一的DAG表示方法，结合自适应剪枝和正则化，并设计了一种可重构的树状处理结构，优化了不规则遍历、符号推理和概率聚合。

Result: 在六个神经符号工作负载上，REASON实现了12-50倍的加速和310-681倍的能效提升，在28纳米工艺下完成端到端任务仅需0.8秒。

Conclusion: REASON通过针对概率逻辑推理的加速，为神经符号AI提供了实用且可扩展的解决方案，并成为下一代认知智能的基础系统架构。

Abstract: Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.
  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.

</details>


### [88] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: MemCtrl框架通过在线记忆修剪提升具身代理任务完成能力，尤其在复杂指令上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有记忆压缩和检索系统通常将记忆视为大型离线存储空间，不利于在严格内存和计算约束下运行的具身代理。

Method: 提出MemCtrl框架，利用可训练的记忆头μ在线修剪记忆，并通过离线专家和在线强化学习两种方式训练μ。

Result: 在EmbodiedBench基准测试中，μ增强的MLLMs平均提升16%，特定指令子集提升超过20%。

Conclusion: MemCtrl框架通过在线修剪记忆显著提升了多模态大语言模型在具体任务中的表现，尤其在复杂指令类型上效果更佳。

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [89] [Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)](https://arxiv.org/abs/2601.20843)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 论文提出Deep Researcher架构，通过顺序优化和交叉算法，在博士级任务中超越并行方法，得分46.21。


<details>
  <summary>Details</summary>
Motivation: 动机是解决并行扩展范式的固有局限性，如知识孤岛问题，提升研究代理的效率和适应性。

Method: 论文采用了Sequential Research Plan Refinement via Reflection和Candidates Crossover算法，结合Gemini 2.5 Pro模型，动态调整研究计划并高效搜索。

Result: 在DeepResearch Bench上，Deep Researcher以46.21分超越其他领先研究代理，验证了顺序扩展的优越性。

Conclusion: 该论文的结论是，通过Sequential Research Plan Refinement via Reflection和Candidates Crossover算法，Deep Researcher架构在博士级研究任务中表现优于并行范式，验证了顺序扩展的优越性。

Abstract: This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.

</details>


### [90] [SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models](https://arxiv.org/abs/2601.20856)
*Sebastiano Monti,Carlo Nicolini,Gianni Pellegrini,Jacopo Staiano,Bruno Lepri*

Main category: cs.AI

TL;DR: 论文通过Sokoban基准测试发现大型推理模型在长时程规划上存在性能下降，表明其架构限制难以仅通过扩展方法解决。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂推理任务上的能力已得到广泛测试，但其长时程规划能力尚未被深入研究。

Method: 提出了一个基于Sokoban谜题的新基准，旨在隔离长时程规划与状态持久性，并评估了最先进的大型推理模型的规划和长时程推理能力。

Result: 研究发现，当解决方案需要超过25步时，规划性能会持续下降。通过配备PDDL解析、验证和求解工具，可以实现适度改进。

Conclusion: 研究表明，大型推理模型在长时程规划能力上存在根本性限制，仅通过测试时的扩展方法难以克服其架构上的局限性。

Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [91] [A Cache-Aware Hybrid Sieve Combining Segmentation and Bit-Packing for Fast Prime Generation](https://arxiv.org/abs/2601.19909)
*Kathi Lakshmi Mani Thirdhana*

Main category: cs.DS

TL;DR: 提出缓存感知混合筛法，优化内存和缓存效率，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 经典筛法在现代CPU上受限于内存访问效率，需要优化内存带宽和缓存局部性。

Method: 提出了一种缓存感知的混合筛法，结合了分段、位压缩和缓存行对齐的块处理技术。

Result: 内存使用减少八倍，运行时间提升2.4倍（相比经典筛法）和1.7倍（相比分段筛法）。

Conclusion: 架构感知的算法设计可以显著提升实际性能，实验结果显示内存使用减少了八倍，运行时间提升了2.4倍。

Abstract: Prime generation is a fundamental task in cryptography, number theory, and randomized algorithms. While the classical Sieve of Eratosthenes is simple and efficient in theory, its practical performance on modern central processing units is often limited by memory access inefficiencies. This paper introduces a cache-aware hybrid sieve that integrates segmentation, bit-packing, and cache-line-aligned block processing to optimize memory bandwidth and level one and level two cache locality.
  The proposed approach reduces memory usage by storing only odd numbers and using one bit per value. The sieve range is divided into cache-sized blocks to minimize cache misses, while primes up to the square root of the limit are reused across blocks. Experimental results demonstrate up to an eight times reduction in memory usage and runtime improvements of up to two point four times compared to the classical sieve and one point seven times compared to the segmented sieve. Benchmarks up to ten to the power of nine illustrate that architecture-aware algorithm design can yield substantial practical performance gains.

</details>


### [92] [Node-Weighted Multicut in Planar Digraphs](https://arxiv.org/abs/2601.20038)
*Chandra Chekuri,Rhea Jain*

Main category: cs.DS

TL;DR: 扩展[KS22]的算法，解决节点加权Multicut问题，实现确定性算法并简化分析。


<details>
  <summary>Details</summary>
Motivation: 研究节点加权Multicut问题，以解决一般有向图中无法通过黑盒方式将节点加权问题转化为边加权问题的问题，并实现确定性算法和算法简化。

Method: 通过扩展[KS22]的LP松弛方法，解决了节点加权Multicut问题，并利用该问题实现了确定性算法和算法简化。

Result: 提出了一个确定性算法，解决了节点加权Multicut问题，并通过标准技术进一步解决了Nonuniform Sparsest Cut问题。

Conclusion: 本文扩展了[KS22]的算法和分析，解决了节点加权Multicut问题，并实现了确定性算法，同时简化了原算法的某些方面。

Abstract: Kawarabayashi and Sidiropoulos [KS22] obtained an $O(\log^2 n)$-approximation algorithm for Multicut in planar digraphs via a natural LP relaxation, which also establishes a corresponding upper bound on the multicommodity flow-cut gap. Their result is in contrast to a lower bound of $\tildeΩ(n^{1/7})$ on the flow-cut gap for general digraphs due to Chuzhoy and Khanna [CK09]. We extend the algorithm and analysis in [KS22] to the node-weighted Multicut problem. Unlike in general digraphs, node-weighted problems cannot be reduced to edge-weighted problems in a black box fashion due to the planarity restriction. We use the node-weighted problem as a vehicle to accomplish two additional goals: (i) to obtain a deterministic algorithm (the algorithm in [KS22] is randomized), and (ii) to simplify and clarify some aspects of the algorithm and analysis from [KS22]. The Multicut result, via a standard technique, implies an approximation for the Nonuniform Sparsest Cut problem with an additional logarithmic factor loss.

</details>


### [93] [Hypergraph Samplers: Typical and Worst Case Behavior](https://arxiv.org/abs/2601.20039)
*Vedat Levi Alev,Uriya A. First*

Main category: cs.DS

TL;DR: 论文研究了$k$-均匀超图在随机算法错误减少中的效用，发现超图边数有下限，但在多数情况下与独立样本效果相近。


<details>
  <summary>Details</summary>
Motivation: 探索在随机算法中，使用$k$-均匀超图进行错误减少的潜力和限制，特别是在减少单侧和双侧错误概率方面的效果。

Method: 通过采样超图的随机超边并重复算法$k$次，使用超边顶点作为种子，研究其在错误减少中的表现。分析了超图的边数和顶点度分布对错误减少的影响。

Result: 研究发现，超图的边数不能太少，否则无法有效减少错误概率。同时，在大多数情况下，使用合理的超图与独立同分布样本相比，错误减少的效果相近。

Conclusion: 该论文揭示了在随机算法中，使用$k$-均匀超图进行错误减少的效用和局限性，特别是在单侧和双侧错误的情境下。研究结果表明，尽管存在下限，但在大多数情况下，使用合理的超图与独立同分布样本相比，优势可以忽略不计。

Abstract: We study the utility and limitations of using $k$-uniform hypergraphs $H = ([n], E)$ ($n \ge \mathrm{poly}(k)$) in the context of error reduction for randomized algorithms for decision problems with one- or two-sided error. Our error reduction idea is sampling a uniformly random hyperedge of $H$, and repeating the algorithm $k$ times using the hyperedge vertices as seeds. This is a general paradigm, which captures every pseudorandom method generating $k$ seeds without repetition. We show two results which imply a gap between the typical and the worst-case behavior of using $H$ for error-reduction.
  First, in the context of one-sided error reduction, if using a random hyperedge of $H$ decreases the error probability from $p$ to $p^k + ε$, then $H$ cannot have too few edges, i.e., $|E| = Ω(n k^{-1} ε^{-1})$. Thus, the number of random bits needed for reducing the error from $p$ to $p^k + ε$ cannot be reduced below $\lg n+\lg(ε^{-1})-\lg k+O(1)$. This is also true for hypergraphs of average uniformity $k$. Our result implies new lower bounds for dispersers and vertex-expanders.
  Second, if the vertex degrees are reasonably distributed, we show that in a $(1-o(1))$-fraction of the cases, choosing $k$ pseudorandom seeds using $H$ will reduce the error probability to at most $o(1)$ above the error probability of using $k$ IID seeds, for both algorithms with one- or two-sided error. Thus, despite our lower bound, for a $(1-o(1))$-fraction of randomized algorithms (and inputs) for decision problems, the advantage of using IID samples over samples obtained from a uniformly random edge of a reasonable hypergraph is negligible. A similar statement holds true for randomized algorithms with two-sided error.

</details>


### [94] [Dynamic framework for edge-connectivity maintenance of simple graphs](https://arxiv.org/abs/2601.20137)
*Blazej Wrobel*

Main category: cs.DS

TL;DR: 本文提出动态维护无向图k边连通性的框架，通过冗余消除和连通性恢复任务，结合稀疏证书和Dinic算法，高效解决了边更新带来的连通性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的动态图问题（如动态最小割）仅关注报告最小割值，而本文旨在主动修改图以维持k边连通性不变，解决边添加和删除带来的连通性挑战。

Method: 结合Nagamochi-Ibaraki稀疏证书和Link-Cut Trees实现冗余边的消除，时间复杂度为O(k log n)；采用局部化增强策略和Dinic算法在稀疏化图上进行最小边集计算，恢复连通性的时间为O(k·n^(5/3))。

Result: 提出的方法在冗余消除和连通性恢复任务中均表现出色，分别实现了O(k log n)和O(k·n^(5/3))的时间复杂度。

Conclusion: 本文提出了一个动态框架，用于在结构更新（如边的添加和删除）时维护无向简单图的k边连通性。通过冗余消除和连通性恢复两种基本维护任务，确保了图的k边连通性不变。

Abstract: We present a dynamic framework for maintaining $k$-edge-connectivity of undirected, simple graphs subject to structural updates, specifically single edge additions and removals. The required edge-connectivity $k$ is a chosen, constant parameter. Unlike standard dynamic graph problems, such as dynamic minimum-cut, which focus solely on reporting the value of the minimum cut, our approach actively modifies the graph $G$ to maintain the edge-connectivity invariant $λ(G) \ge k$. We address two fundamental maintenance tasks: redundancy elimination, which identifies and removes an existing edge rendered redundant for $k$-edge-connectivity by new edge insertion, and connectivity restoration, which computes and inserts a minimal set of augmenting edges to restore graph's $k$-edge-connectivity following an old edge deletion. To preclude trivial reversals, we strictly enforce that the eliminated edge is distinct from the inserted edge and that restoration excludes the already deleted edge. Our solution of the first problem integrates Nagamochi-Ibaraki sparse certificates [Nagamochi and Ibaraki 1992] with Link-Cut Trees [Sleator and Tarjan 1983] to remove redundant edges in $O(k \log n)$ amortized time. For restoration, we propose a localized augmentation strategy that exploits the residual graph structure to bridge the minimum cut. By executing Dinic's [Dinic 1970] algorithm on the sparsified input graph, we identify the minimal edge set required to reconnect the graph in $O(k \cdot n^{5/3})$ time.

</details>


### [95] [Fully Dynamic Algorithms for Graph Spanners via Low-Diameter Router Decomposition](https://arxiv.org/abs/2601.20718)
*Julia Chuzhoy,Merav Parter*

Main category: cs.DS

TL;DR: 论文提出了一种确定性算法，用于在完全动态图中维护稀疏子图（spanner），通过低直径路由器分解技术实现了拉伸、大小和更新时间的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决完全动态环境下维护稀疏子图（spanner）的问题，尤其是在拉伸、更新时间和子图大小之间取得平衡。目前尚无算法能同时满足亚对数拉伸、亚线性更新时间和强亚二次子图大小。

Method: 论文提出了一种确定性算法，利用低直径路由器分解技术，将完全动态图分解为边不相交的簇，每个簇具有有界的顶点重叠和直径。

Result: 算法在拉伸为poly(k)⋅2^O(1/δ^6)、大小为O(n^(1+O(1/k)))的情况下，实现了最坏情况下n^O(δ)的更新时间和n^O(1/k)的补救。

Conclusion: 该论文的主要成果是提出了一种确定性算法，能够在完全动态图中维护具有特定拉伸、大小和更新时间的稀疏子图（spanner），并引入了低直径路由器分解这一新技术工具。

Abstract: A $t$-spanner of an undirected $n$-vertex graph $G$ is a sparse subgraph $H$ of $G$ that preserves all pairwise distances between its vertices to within multiplicative factor $t$, also called the \emph{stretch}. We investigate the problem of maintaining spanners in the fully dynamic setting with an adaptive adversary. Despite a long line of research, this problem is still poorly understood: no algorithm achieving a sublogarithmic stretch, a sublinear in $n$ update time, and a strongly subquadratic in $n$ spanner size is currently known.
  One of our main results is a deterministic algorithm, that, for any $512 \leq k \leq (\log n)^{1/49}$ and $1/k\leq δ\leq 1/400$, maintains a spanner $H$ of a fully dynamic graph with stretch $poly(k)\cdot 2^{O(1/δ^6)}$ and size $|E(H)|\leq O(n^{1+O(1/k)})$, with worst-case update time $n^{O(δ)}$ and recourse $n^{O(1/k)}$. Our algorithm relies on a new technical tool that we develop, called low-diameter router decomposition. We design a deterministic algorithm that maintains a decomposition of a fully dynamic graph into edge-disjoint clusters with bounded vertex overlap, where each cluster $C$ is a bounded-diameter router, meaning that any reasonable multicommodity demand over the vertices of $C$ can be routed along short paths and with low congestion. A similar graph decomposition notion was introduced by [Haeupler et al., STOC 2022] and strengthened by [Haeupler et al., FOCS 2024]. However, in contrast to these and other prior works, the decomposition that our algorithm maintains is proper, ensuring that the routing paths between the pairs of vertices of each cluster $C$ are contained inside $C$, rather than in the entire graph $G$. We show additional applications of our router decomposition, including dynamic algorithms for fault-tolerant spanners and low-congestion spanners.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [96] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: Google分享了优化AI开发者工具的经验，重点解决了代码补全和自然语言驱动代码转换的挑战，提升了企业生产力。


<details>
  <summary>Details</summary>
Motivation: 开发和完善基于AI的IDE功能，以提升开发者生产力。

Method: 通过严格的实验解决延迟、用户体验和建议质量等挑战。

Result: 成功优化了代码补全和自然语言驱动的代码转换功能，实现了生产力提升。

Conclusion: Google通过优化AI开发者工具（如代码补全和自然语言驱动的代码转换）在用户界面、后端和模型层的表现，显著提升了企业环境中的生产力。

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [97] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 论文提出TRACE基准测试代码环境中的奖励漏洞，发现对比性检测优于孤立分类，并揭示语义漏洞的检测难度更高。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码生成强化学习中作为评估者的普及，其检测奖励漏洞的能力尚未充分研究，因此需要构建更健壮的测试环境。

Method: 引入了一个包含54类奖励漏洞的新分类法，并创建了TRACE基准（包含517条测试轨迹），采用对比性异常检测与孤立分类设置对比。

Result: 实验表明，对比性设置下模型检测率更高（GPT-5.2达63%，孤立设置仅45%），且语义上下文漏洞比语法上下文更具挑战性。

Conclusion: 论文提出了TRACE基准，展示了对比性异常检测设置优于孤立分类设置的效果，并揭示了模型在语义上下文奖励漏洞检测上的挑战。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [98] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: 研究显示核心与外围开发者在使用自主编码代理时行为差异显著，核心开发者更注重文档、测试和严格验证，而外围开发者更依赖代理但审查较宽松。


<details>
  <summary>Details</summary>
Motivation: 探索自主AI代理时代中，核心与外围开发者使用AI工具的差异动态。

Method: 通过定性定量分析，研究了9,427个代理生成的PR，探讨开发者如何使用、审查、修改和验证代理贡献。

Result: 外围开发者更频繁使用代理，核心开发者更关注文档和测试，且其代理PR更易被合并；两者在审查中均关注可演化问题；代理PR较少被修改，但修改时多为重构；外围开发者更可能跳过CI检查合并，核心开发者则更严格。

Conclusion: 本文提供了关于核心和外围开发者如何与自主编码代理协作的全面视角，为两者提供了有效合作的见解。

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [99] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: 研究发现AI生成的错误修复PR合并后的代码质量问题，尤其是代码异味和严重错误，表明合并成功不等于高质量，需系统性检查。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码代理生成的PR承诺提高生产力，但其合并后的代码质量尚未得到充分研究，此前工作主要依赖基准测试和控制任务而非大规模合并后分析。

Method: 使用SonarQube对AIDev数据集中的1,210个合并的AI生成的错误修复PR进行差异分析，比较基础提交和合并提交之间的代码质量问题。

Result: 结果显示，原始问题数量在不同代理间的明显差异在按代码变更量归一化后基本消失，表明较高的问题数量主要由较大的PR驱动。在所有代理中，代码异味占主导地位，尤其是在关键和主要严重性级别，而错误较少但通常较严重。

Conclusion: 合并成功并不能可靠地反映合并后的代码质量，强调了需要对AI生成的错误修复PR进行系统性质量检查的必要性。

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [100] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 论文调查了AI编码助手和CodeLLMs在实际项目中的适用性，通过开发者调查和用户反馈分析，提出了未来发展的需求。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在软件工程任务中的自动化应用，并探讨其在企业环境中的实际适用性和影响。

Method: 通过调查57名来自不同领域和具有不同软件工程技能的开发者，以及回顾35份关于专业人士和学生使用AI编码助手和CodeLLMs的用户调查。

Result: 研究发现和现有调查分析揭示了AI编码助手的需求和用户体验。

Conclusion: 论文总结了AI编码助手和CodeLLMs在实际项目和企业用例中的适用性，并提出了未来发展的需求。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [101] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 研究发现系统提示对代码生成模型性能的影响随模型规模增加而增强，Few-shot提示可减少这种影响，且Java比Python更敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管ILMs（尤其是CLMs）在代码生成方面表现出色，但系统提示对其性能的影响尚未被充分探索。本研究旨在填补这一空白。

Method: 本研究通过评估120种模型配置，系统分析了系统提示的不同教学细节、模型规模、提示策略和编程语言对ILMs和CLMs在代码生成任务中的影响。

Result: 研究发现：(1) 系统提示的影响随模型规模增加而增强；(2) Few-shot提示相比零样本提示能减少这种影响；(3) 编程语言的选择影响敏感性，Java比Python更敏感。

Conclusion: 系统提示对ILMs和CLMs在代码生成任务中的性能有显著影响，尤其是随着模型规模的增加。Few-shot提示策略可以减少这种影响，而编程语言的选择（如Java比Python更敏感）也是一个关键因素。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [102] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LogSieve是一种针对CI日志的轻量级语义保留减少技术，显著降低日志体积和计算成本，同时保持高语义保真度，助力绿色CI自动化。


<details>
  <summary>Details</summary>
Motivation: CI日志的快速增长和冗余性使得手动检查和自动化分析变得昂贵、耗时且对环境不友好。现有方法主要针对结构化系统日志，而非CI工作流中典型的非结构化、嘈杂和冗长日志。

Method: LogSieve是一种基于嵌入的分类器，能够自动化检测日志相关性，实现语义感知的过滤。

Result: 在20个开源Android项目的GitHub Actions CI日志上评估，LogSieve平均减少42%的行数和40%的token数，语义损失最小。与基线方法相比，LogSieve在语义和分类保真度上表现更优（Cosine=0.93，GPTScore=0.93，80%精确匹配准确率）。

Conclusion: LogSieve为持续集成（CI）工作流提供了一种轻量级、语义保留的日志减少技术，有效降低了计算成本和能源消耗，同时保持了高语义保真度。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [103] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: 论文提出基于异构图的SBOM安全分析方法，用HGAT预测组件漏洞，多层感知机解决漏洞链稀缺问题，准确率达91.03%。


<details>
  <summary>Details</summary>
Motivation: 当前软件安全分析工具孤立评估漏洞，而复杂的供应链安全威胁常源于跨组件的漏洞链。此外，不同SBOM生成器和分析工具的结果差异较大，亟需统一方法。

Method: 论文提出将SBOM表示为异构图，节点包括SBOM组件、依赖关系、已知漏洞和安全弱点，并训练HGAT模型预测组件漏洞。对于稀缺的多漏洞链，采用多层感知机神经网络进行链接预测。

Result: HGAT组件分类器的准确率达到91.03%，F1分数为74.02%，表明该方法在漏洞预测方面具有高效性。

Conclusion: 该论文提出了一种基于异构图的SBOM驱动安全分析方法，通过HGAT模型有效预测组件漏洞，并采用多层感知机神经网络解决漏洞链稀缺问题，显著提升了漏洞检测的准确性和F1分数。

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [104] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 研究比较了代理与开发者在Java重构中的表现，发现代理主要进行注解变更，而开发者更注重结构改进，Cursor是唯一显著增加重构异味的代理。


<details>
  <summary>Details</summary>
Motivation: 探讨软件代理在Java重构实践中的表现、变更类型及其对代码质量的影响。

Method: 使用RefactoringMiner和DesigniteJava 3.0工具，识别重构类型并检测重构提交前后的代码异味。

Result: 代理重构以注解变更为主导，而开发者重构则更注重结构改进；Cursor是唯一显著增加重构异味的模型。

Conclusion: 研究表明，代理重构主要集中在注解变更上，而开发者则倾向于多样化的结构改进。尽管重构类型存在差异，但Cursor是唯一显示出重构异味显著增加的模型。

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [105] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI agents contribute heavily to documentation in SE3.0, but minimal human review of their edits raises quality and collaboration concerns.


<details>
  <summary>Details</summary>
Motivation: To understand the extent of AI agents' contributions to documentation tasks and how human developers review and intervene, given the risks of delegating work to AI agents.

Method: Analysis of 1,997 documentation-related pull requests (PRs) using the AIDev tool, comparing AI agent-authored and human-authored PRs.

Result: AI agents submit more documentation-related PRs than humans, and their edits are often integrated with minimal human follow-up, raising concerns about review practices and reliability.

Conclusion: AI agents significantly contribute to documentation workflows in SE3.0, but concerns arise regarding documentation quality assurance and human-AI collaboration.

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [106] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 研究提出控制模型优化LLM驱动的代码补全，通过分类器触发推断并过滤建议，实验显示其提升效率与质量。


<details>
  <summary>Details</summary>
Motivation: 通过控制模型更好地对齐用户需求，减少不必要的请求，提升代码补全的建议质量与效率。

Method: 评估了基于boosting和transformer的架构在真实代码补全离线数据集（n=98用户）上的表现，并在生产环境中进行了A/B测试。

Result: 离线分类性能在多语言环境下表现良好，生产环境中的A/B测试显示补全效率和质量指标均有提升。

Conclusion: 研究表明，辅助模型在IDE中更智能地集成LLM驱动功能具有潜力，并指出了未来的研究方向与开放问题。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [107] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: npm开发者重视安全但面临诸多挑战，如供应链攻击和工具不足，需改进工具和教育以提升生态系统安全性。


<details>
  <summary>Details</summary>
Motivation: npm生态系统在现代软件开发中扮演着重要角色，但第三方包中的漏洞导致了严重的安全漏洞，影响了依赖它们的应用程序的完整性。

Method: 我们通过在线调查对75名npm包开发者进行了研究，并采用混合方法分析他们的回答。

Result: 开发者虽然重视安全，但认为他们的包仅具有中等安全性，担心供应链攻击、依赖漏洞和恶意代码。只有40%对当前npm安全工具满意，主要问题包括警报疲劳。自动化方法如双因素认证和npm审计优于代码审查。许多开发者因依赖项被弃用或存在漏洞而放弃使用，通常通过快速发布补丁来应对漏洞。主要障碍包括时间限制和高误报率。开发者希望改进检测工具、文档、账户保护和教育措施。

Conclusion: 研究结果将为npm包的贡献者和维护者提供帮助，突出普遍存在的安全挑战，并促进关于最佳实践的讨论，以增强npm生态系统中的安全性和可信度。

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [108] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 论文通过分析ICSE FOSE调查，指出小型经济体在软件工程社区中的挑战，并提出改善研究与工业合作的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨小型经济体（特别是非英语国家）在软件工程社区中的代表性及其面临的研究与行业差距问题。

Method: 通过反思性主题分析（reflexive thematic analysis）对ICSE FOSE社区调查进行分析。

Result: 分析表明，研究与行业之间的差距对小型社区和本地公司影响尤为显著。

Conclusion: 论文提出了一系列建议，旨在改善小型经济体中软件工程研究与工业合作的关系。

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [109] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 研究评估模块化语言工作台Neverlang的用户可理解性与接受度，发现用户认可其有用性但易用性待改进，且理解度与接受度无直接关联。


<details>
  <summary>Details</summary>
Motivation: 现有文献常忽视语言工作台评估中的用户中心因素（如可理解性和接受度），本研究填补这一空白。

Method: 采用定制版方法评估模型（MEM），通过三次实验迭代评估元语言和程序的可理解性及用户接受度（易用性、有用性和使用意愿）。

Result: 用户对Neverlang的元语言理解良好，认可其有用性并有意使用，但易用性不足。易用性和有用性感知影响使用意愿，但可理解性与接受度无显著关联。

Conclusion: 用户对Neverlang的元语言理解充分，尤其是在语法方面，且认为其有用并有意使用，但易用性仍是挑战。易用性和有用性的感知差异影响使用意愿，但理解度与接受度之间无显著相关性。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [110] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: AGENTS$.$md文件能显著提升AI编码代理的运行效率，减少令牌消耗，且不影响任务完成效果。


<details>
  <summary>Details</summary>
Motivation: 研究仓库级配置文件（如AGENTS$.$md）如何影响AI编码代理的运行效率和令牌消耗。

Method: 分析了10个代码仓库和124个拉取请求，分别在有无AGENTS$.$md文件的情况下运行AI编码代理，测量执行时间和令牌消耗。

Result: AGENTS$.$md文件的存在使运行时间中位数降低28.64%，输出令牌消耗减少16.58%，且任务完成行为基本不变。

Conclusion: 研究表明，AGENTS$.$md文件的存在显著降低了AI编码代理的运行时间和令牌消耗，同时对任务完成行为影响不大。这为AI编码代理的配置和部署提供了实践指导，并为进一步研究仓库级指令在软件开发中的作用奠定了基础。

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [111] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 该论文评估了四种MLOps工具在两类ML任务中的表现，为开发者选择工具提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI解决方案在专业环境中的日益普及，开发者需要了解当前工具生态以做出明智决策。

Method: 实证评估了MLflow、Metaflow、Apache Airflow和Kubeflow Pipelines等工具，基于安装便捷性、配置灵活性、互操作性、代码复杂性、结果可解释性和文档质量等标准。

Result: 评估了两种常见ML场景（MNIST数字分类器和IMDB+BERT情感分类器）下的工具表现，并提供了加权结果。

Conclusion: 该论文通过加权评估结果，为不同场景下的MLOps工具选择提供了实用建议。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [112] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 研究发现开发者在完成Google Play的Data Safety Section表单时面临分类隐私数据的挑战，缺乏信心，需更清晰指导和工具支持。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者在Google Play商店的Data Safety Section（DSS）表单中准确报告数据收集的体验和挑战。

Method: 首先调查了41名Android开发者，了解他们如何将隐私相关数据分类到DSS类别中以及完成DSS表单时的信心；随后分析了172个在线开发者讨论，捕获了642名开发者的观点。

Result: 开发者通常手动分类隐私数据或完全省略分类，并依赖在线资源完成表单；他们对识别数据收集有信心，但对转换为DSS合规披露缺乏信心。关键挑战包括识别隐私相关数据、对表单理解有限及担心应用因隐私要求不符而被拒绝。

Conclusion: 研究结果强调了需要更清晰的指导和更易用的工具来帮助开发者满足隐私报告义务。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [113] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: DrainCode是一种针对RAG代码生成系统的对抗攻击，通过毒化检索上下文增加LLM的计算开销，实验证明其显著提升延迟、能耗和输出长度。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成中表现出色，但其计算成本（如延迟和能耗）在安全领域的关注不足。

Method: 通过基于突变的策略毒化检索上下文，迫使LLM生成更长的输出，从而增加GPU延迟和能耗。

Result: 实验显示，DrainCode使延迟增加85%，能耗增加49%，输出长度增加3倍以上，且攻击对不同提示策略具有通用性。

Conclusion: DrainCode作为一种针对RAG代码生成系统计算效率的对抗攻击方法，能显著增加LLM的计算开销，适用于评估资源受限环境下的LLM安全性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [114] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: Lila是一个去中心化系统，用于大规模监控软件构建的可重现性，解决了现有监控基础设施的不足。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程中，软件构建产物的完整性日益重要，但大规模可重现性监控基础设施仍是一个未解决的挑战。

Method: 提出了Lila系统，采用分布式报告构建结果并聚合到可重现性数据库中的方法。

Result: Lila系统能够支持分布式报告和聚合构建结果，为实践者和未来的实证研究提供便利。

Conclusion: Lila作为一种去中心化的可重现性评估系统，为功能性包管理模型提供了有效的监控解决方案，有助于提升软件构建的透明度和信任。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [115] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 开发了一个低开销、高保真的LLM推理分析框架，通过eBPF技术实现细粒度性能分析，提升透明度和可诊断性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）从研究转向生产，实时理解推理引擎的行为变得至关重要但难以实现，现有系统缺乏操作员级别的可见性。

Method: 利用扩展的Berkeley Packet Filter（eBPF）技术，动态附加探针到运行时函数的多层，无需修改或重新编译源代码，将收集的跟踪数据转化为丰富的可视化信息。

Result: 该框架运行时开销低于4%，具有高分析保真度，能够展示密集推理、混合专家路由和操作员卸载的实际行为。

Conclusion: 该论文开发了一个基于eBPF技术的细粒度、非侵入式分析框架，显著提高了LLM推理引擎的透明度和可诊断性，使其性能分析成为优化、调度和资源感知部署的实用工具。

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [116] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: PKG通过细粒度检索和重排序机制提升LLMs处理复杂代码问题的能力，显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成中表现优秀，但处理复杂问题时存在困难，RAG虽能缓解但检索模型常遗漏相关上下文且生成模型易产生无关数据。

Method: 提出了编程知识图（PKG）用于代码和文本的语义表示与细粒度检索，通过树剪枝提升检索精度，并利用重排序机制减少幻觉。

Result: 在HumanEval和MBPP上的评估显示，pass@1准确率提升高达20%，MBPP上基线提升34%。

Conclusion: PKG方法与重排序机制有效解决了复杂问题，同时最小化了对非RAG解决方案的负面影响。

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [117] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为\method的样本高效人类在环强化学习框架，通过主动选择信息性样本减少人类干预，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类在环强化学习框架样本效率低，需要大量人类干预才能收敛，导致高劳动成本。

Method: 通过构建不同样本对策略熵的影响函数，并选择中等影响值的样本，剔除导致熵急剧下降的捷径样本和影响微弱的噪声样本。

Result: 在四个真实世界操作任务中，\method 实现了42.1%更高的成功率，同时需要比最先进的HiL-RL方法少10.1%的人类干预。

Conclusion: \method 框架通过主动选择信息性样本，显著提高了样本效率，减少了人类干预的需求，并在四个真实世界操作任务中验证了其有效性。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [118] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过动态采样和运动优化，显著提升高维机械臂路径规划的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统采样方法在高维复杂环境中效率低下，尤其在机械臂应用中面临运动奇异性和自碰撞风险。

Method: JIT*算法包含两个核心模块：Just-in-Time模块（动态优化边缘连接和采样密度）和Motion Performance模块（平衡可操作性和轨迹成本）。

Result: JIT*在$\mathbb{R}^4$至$\mathbb{R}^{16}$维度中均优于传统方法，并在单臂和双臂操作任务中验证了其有效性。

Conclusion: JIT*算法通过动态调整采样密度和边缘连接性，在复杂环境中显著提高了路径规划的效率和安全性，尤其在多自由度机械臂应用中表现优异。

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [119] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: REMAC通过校正动作分块内部不一致性，提升机器人异步推理的可靠性和实时性。


<details>
  <summary>Details</summary>
Motivation: 解决异步推理中因动作分块内部不一致性导致的执行失败问题。

Method: 提出REMAC方法，通过掩码动作分块学习校正调整，并引入前缀保留采样程序增强分块间连续性。

Result: 实验表明，REMAC在仿真和真实环境中均能实现更快任务执行、更高完成率，并保持延迟变化下的鲁棒性。

Conclusion: REMAC通过掩码动作分块学习校正调整，提升了策略在异步推理中的鲁棒性，同时保持了实时性。

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [120] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 针对移动机器人定位误差导致的高斯过程模型退化问题，提出了一种基于二阶校正的实时优化方法，显著提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在回归标量场时假设测量位置精确已知且噪声为高斯分布，但实际应用中移动机器人定位不准确会引入状态不确定性，导致模型估计退化。

Method: 利用核函数的可微性，开发了一种基于预计算雅可比矩阵和海森矩阵的二阶校正算法，用于实时根据测量位置误差数据优化高斯过程模型。

Result: 仿真结果表明，与完全重新训练模型相比，所提方法在预测精度和计算效率上均有显著提升。

Conclusion: 本文提出了一种基于二阶校正算法的高斯过程模型更新方法，有效解决了移动机器人定位不准确导致的测量位置误差问题，显著提升了预测精度和计算效率。

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [121] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER通过分层语义推理和物理一致性优化，提升了可变形物体操作的功能区域预测精度和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于视觉的功能预测方法在可变形物体操作中因边界溢出和功能区域碎片化导致的问题。

Method: 提出了TRACER框架，包括Tree-structured Affordance Chain-of-Thought (TA-CoT)、Spatial-Constrained Boundary Refinement (SCBR)机制和Interactive Convergence Refinement Flow (ICRF)。

Result: 在Fine-AGDDO15数据集和真实机器人平台上验证了TRACER的优越性能，显著提升了功能区域预测的精度和长时任务的成功率。

Conclusion: TRACER显著提高了可变形物体在不同纹理和模式下的功能区域预测精度，并成功连接了高层语义推理与低层物理执行之间的鸿沟。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [122] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide是一种新型视觉触觉融合范式，通过两阶段动作生成和触觉引导，显著提升机器人在精细任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决机器人因触觉反馈利用不足而在精细和接触密集任务中面临的挑战。

Method: TouchGuide采用两阶段策略：首先使用预训练的扩散或流匹配视觉运动策略生成粗略动作，然后通过对比学习训练的Contact Physical Model（CPM）提供触觉引导，细化动作以满足物理接触条件。此外，TacUMI系统用于高效收集高质量触觉数据。

Result: 在五项挑战性任务（如系鞋带和芯片交接）中，TouchGuide表现显著优于现有视觉触觉策略。

Conclusion: TouchGuide通过结合视觉和触觉反馈，显著提升了机器人在精细和接触密集任务中的表现，并在多个挑战性任务中超越了现有技术。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [123] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi通过知识蒸馏压缩VLA模型深度，提升推理速度且性能损失极小，适用于实时机器人部署。


<details>
  <summary>Details</summary>
Motivation: 实时机器人部署对快速、设备端推理的需求日益增长，而现有VLA模型在层数减少方面的系统性研究不足。

Method: 提出了Shallow-pi框架，通过知识蒸馏技术将VLA模型的深度从18层压缩至6层，包括VLM主干和基于流的动作头。

Result: Shallow-pi在标准操作基准上实现了推理速度提升两倍以上，成功率下降不到1%，并在工业级机器人平台上验证了其性能。

Conclusion: Shallow-pi框架通过知识蒸馏显著减少了VLA模型的深度，实现了推理速度的大幅提升且性能损失极小，在工业级机器人平台上验证了其有效性。

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [124] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: TaF-VLA框架通过触觉-力对齐提升接触密集型任务表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要依赖视觉模态，缺乏对接触密集型任务所需的精确力调节和物理推理的物理直觉。

Method: 提出了TaF-VLA框架，开发了自动触觉-力数据采集设备，并构建了TaF-Dataset。核心组件是Tactile-Force Adapter（TaF-Adapter），用于将触觉观测与交互力对齐。

Result: TaF-VLA策略在真实世界实验中显著优于现有的触觉-视觉对齐和仅视觉基线。

Conclusion: TaF-VLA框架通过触觉-力对齐显著提升了在接触密集型任务中的表现，验证了其通过跨模态物理推理实现稳健、力感知操作的能力。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [125] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将通用LLM代理框架应用于机器人操作，无需任务特定演示或微调，在多个基准上接近VLA模型性能，展示了通用代理在规划密集型任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索通用大型语言模型（LLM）代理框架是否能替代传统视觉-语言-动作（VLA）模型，解决后者需要任务特定演示、微调及领域迁移性差的问题。

Method: FAEA将LLM代理框架直接应用于机器人操作，利用迭代推理策略进行任务规划，并在LIBERO、ManiSkill3和MetaWorld基准上进行评估。

Result: FAEA在LIBERO、ManiSkill3和MetaWorld上的成功率分别为84.9%、85.7%和96%，接近VLA模型的性能，且无需演示或微调；加入人类反馈后，LIBERO上的性能提升至88.2%。

Conclusion: FAEA展示了通用代理框架在机器人操作任务中的潜力，无需特定任务的演示或微调，即可达到与VLA模型相近的性能，为机器人系统利用前沿模型基础设施开辟了新路径。

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [126] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID是一个开源的大规模RF数据集，用于细粒度材料识别，填补了当前RF方法在数据集和基准测试上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别方法受限于光学传感器的固有约束，而RF方法虽能揭示材料内在属性，但缺乏大规模公开数据集和基于学习方法的基准测试。

Method: 提出了RF-MatID，首个开源、大规模、宽频带且几何多样化的RF数据集，包含16个细粒度类别和142k样本，涵盖频率和时间域表示，并系统性地引入了几何扰动。

Result: 建立了多设置、多协议的基准测试，评估了最先进的深度学习模型，分析了分布内性能和跨角度、跨距离的分布外鲁棒性。

Conclusion: RF-MatID数据集旨在促进可重复研究、加速算法进步、增强跨领域鲁棒性，并支持RF材料识别在实际应用中的发展。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [127] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM是一个轻量级对象中心适应模块，通过多阶段训练增强冻结视觉基础模型，提升机器人操作的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型的密集表示缺乏明确的对象级结构，限制了机器人操作任务的鲁棒性和可操作性。

Method: STORM采用多阶段训练策略：首先通过语言嵌入进行视觉-语义预训练稳定对象中心槽，然后与下游操作策略联合适应。

Result: 实验表明，STORM在对象发现基准和模拟操作任务中，相比直接使用冻结的基础模型特征或端到端训练对象中心表示，能更好地泛化视觉干扰并提升控制性能。

Conclusion: STORM通过多阶段训练策略，将通用的视觉基础模型特征转化为任务感知的对象中心表示，显著提升了机器人控制的泛化能力和性能。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [128] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出一个评估多机器人月球探测任务的KPI框架，强调效率、鲁棒性和精确性，验证了其实际应用性，但精确性评估受限于数据获取。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器人平台和实验设置差异导致的性能评估不一致问题，建立科学目标与现场性能之间的明确联系。

Method: 通过三个现实的多机器人月球场景，从科学目标和操作约束中推导出KPI框架，并在多机器人现场测试中验证。

Result: 框架在效率与鲁棒性相关的KPI上表现实用且易于应用，但精确性相关的KPI需要可靠的地面真实数据，这在户外模拟环境中难以获取。

Conclusion: 本文提出了一个结构化框架，用于评估多机器人月球探测任务的性能，强调效率、鲁棒性和精确性，并验证了其在实际部署中的适用性。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [129] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 本文提出了一种基于振动声学传感的高精度全身触觉定位方法，通过低成本压电麦克风和音频频谱图变换器实现，静态定位误差低于5毫米，并对不同材料特性表现出适应性。


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤成本高且集成复杂，本文旨在提供一种可扩展的高精度全身触觉定位替代方案。

Method: 通过为机器人手配备七个低成本压电麦克风，并利用音频频谱图变换器（Audio Spectrogram Transformer）解码物理交互过程中产生的振动特征。

Result: 在静态条件下定位误差低于5毫米，且系统对机器人自身运动具有鲁棒性，即使在主动操作期间也能保持有效跟踪。

Conclusion: 本文的主要贡献是证明了复杂的物理接触动力学可以通过简单的振动信号有效解码，为机器人领域提供了广泛且经济实惠的接触感知途径。

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [130] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo是一个相似性感知的多机器人协作框架，通过缓存和重用机制减少冗余计算，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的多机器人协作方法在遇到相似任务时需重新规划，效率低下，缺乏对任务相似性的利用。

Method: MeCo提出了一种新的相似性测试方法，用于检索高相关性的已解决任务，并引入MeCoBench基准来评估相似任务协作性能。

Result: 实验结果表明，MeCo相比现有方法显著降低了规划成本并提高了成功率。

Conclusion: MeCo框架通过引入相似性测试和计划重用机制，显著降低了多机器人协作中的冗余计算，提高了任务成功率和效率。

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [131] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO框架通过动态调整动作空间，有效解决了有腿机器人强化学习中的探索和训练难题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基于扭矩的控制中效果不佳，主要因动作空间探索困难和梯度信号不足。

Method: GPO通过时间变化的动作变换限制早期有效动作空间，促进有效数据收集和政策学习，随后逐步扩展以增强探索。

Result: GPO训练的政策在四足和六足机器人上表现更优，包括仿真训练政策的零次硬件部署。

Conclusion: GPO提供了一个通用的、与环境无关的优化框架，用于学习有腿机器人的运动控制，并在仿真和硬件部署中表现优异。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [132] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 论文提出了一种通过肌腱位移和张力估计关节位置的计算方法，并结合闭环控制实现无直接关节传感的手势跟踪，仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决肌腱驱动仿人机器人手因缺乏直接关节角度传感而导致的机械紧凑性和灵活性受限问题。

Method: 基于Denavit-Hartenberg约定引入高效的运动学建模框架，通过非线性优化方法求解肌腱状态与关节位置的非线性方程组，并采用Jacobian-based PI控制器增强前馈项进行闭环控制。

Result: 在MuJoCo仿真环境中使用Anatomically Correct Biomechatronic Hand验证了估计和控制框架的有效性与局限性。

Conclusion: 该论文提出的计算方法和控制框架在仿真环境中验证了其有效性，展示了无需直接关节传感即可实现手势跟踪的潜力。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [133] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO通过单步推理、分散正则化和RL微调，实现了实时机器人控制的高频率生成，性能优于多步方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，无法满足实时机器人控制的高频率需求。

Method: 提出了Dispersive MeanFlow Policy Optimization (DMPO)框架，包含MeanFlow（数学推导的单步推理）、分散正则化（防止表示崩溃）和RL微调（超越专家演示）三个关键组件。

Result: 在RoboMimic和OpenAI Gym基准测试中表现优于多步基线，推理速度提升5-20倍，达到数百赫兹，并在Franka-Emika-Panda机器人上验证了实际应用。

Conclusion: DMPO框架通过MeanFlow、分散正则化和RL微调三个关键组件，实现了真正的一步生成，满足了实时控制的高频率需求（>120Hz），并在实际机器人部署中验证了其有效性。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [134] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: 该论文提出一种弱监督框架，通过预热轨迹提取空间信息，显著提高显微镜引导微操作的精度和工作效率。


<details>
  <summary>Details</summary>
Motivation: 重新思考稳态机械手操作，旨在减少传统自动化中依赖劳动密集型2D标注的问题，提高操作的准确性和效率。

Method: 通过融合校准感知的感知与导纳控制，利用可重复使用的预热轨迹提取隐含的空间信息，实现无需外部基准或手动深度注释的校准感知、深度分辨感知。

Result: 系统在95%置信度下实现了约49微米的横向闭环精度和≤291微米的深度精度（最坏测试子集）。在用户研究中，学习代理相对于简单稳态辅助基线减少了77.1%的NASA-TLX工作负荷。

Conclusion: 该论文提出的弱监督框架显著提高了显微镜引导的生物医学微操作的可靠性，且无需复杂的设置要求，为显微镜引导的干预提供了实用框架。

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [135] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 论文提出了一种在ROS 2系统中实现知识图谱的方法，通过模拟搜救任务验证其提升自主机器人任务效率和智能性的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过知识图谱提升ROS 2系统中自主机器人任务的效率和智能性。

Method: 方法包括定义初始和目标条件、构建任务和子任务结构、规划执行顺序、在知识图谱中表示任务相关数据，以及使用高级语言设计任务。

Result: 在Aerostack2框架中的模拟搜救任务中，无人机成功自主定位目标，验证了方法的有效性。

Conclusion: 该论文提出的方法通过知识图谱有效提升了ROS 2系统中自主机器人任务的决策效率和性能。

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [136] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出了一种基于神经风格转移的模拟到现实强化学习策略转移方法，通过变分自编码器生成更真实的训练数据，在机器人切割任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability.

Method: We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories.

Result: Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data.

Conclusion: Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [137] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: Discontinuous DLS 是一种基于数据驱动的误差有界压缩方法，通过局部子空间提升压缩效率，适用于多种科学数据，显著减少存储需求且不损害数据保真度。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据的快速增长对存储和传输提出了重大挑战，误差有界的有损压缩成为关键解决方案。

Method: Discontinuous DLS 利用局部化的时空子空间，基于底层数据结构，提升压缩效率并保留关键特征。该方法适用于多种科学数据，并在分布式计算环境中通过 MPI 实现。

Result: Discontinuous DLS 显著降低了存储需求，同时保持了关键数据的保真度，其性能在压缩比和重建准确性上优于现有误差有界压缩方法。

Conclusion: Discontinuous DLS 是一种在大规模科学数据压缩中具有前景的方法，能够有效管理现代科学模拟中不断增长的数据需求。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [138] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion是一种高效的DiT服务引擎，通过拓扑感知序列并行、Torus Attention和单边通信，解决了现有序列并行技术的局限性，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像和长视频需求的增加，单GPU推理因延迟增加和大激活尺寸变得低效。现有框架的序列并行技术存在通信模式不优化、all-to-all操作延迟瓶颈以及双边通信库的开销问题。

Method: StreamFusion引入了三种关键技术：(1) 拓扑感知序列并行技术，考虑机器间和机器内带宽差异；(2) Torus Attention，一种新型SP技术，实现机器间all-to-all操作与计算的重叠；(3) 单边通信实现，减少GPU发送-接收同步和计算开销。

Result: 实验表明，StreamFusion平均性能提升1.35倍（最高1.77倍），优于现有最佳方法。

Conclusion: StreamFusion通过拓扑感知的序列并行技术、Torus Attention和单边通信实现，显著提升了DiT服务的效率，平均性能提升1.35倍（最高1.77倍）。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [139] [SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips](https://arxiv.org/abs/2601.20309)
*Jiahuan Yu,Mingtao Hu,Zichao Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: SuperInfer是一个高性能LLM推理系统，通过新型调度器和内存优化，显著提升了响应性，特别适用于Superchips。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM推理系统在高请求率下因KV缓存预算耗尽导致的严重队头阻塞问题，提升TTFT和TBT SLO的达标率。

Method: SuperInfer引入了RotaSched（首个主动、SLO感知的旋转调度器）和DuplexKV（优化的旋转引擎），支持NVLink-C2C全双工传输。

Result: 在GH200上的评估显示，SuperInfer将TTFT SLO达标率提升高达74.7%，同时保持与现有系统相当的TBT和吞吐量。

Conclusion: SuperInfer通过SLO-aware调度和内存协同设计，充分发挥了Superchips的潜力，显著提升了LLM推理的响应性。

Abstract: Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.

</details>


### [140] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出了一种统一竞争分类框架，通过表示变换、结构建模和任务解耦机制，显著提升高维系统环境中的多任务竞争识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统环境中准确识别多任务竞争类型的挑战。

Method: 该方法首先从高维指标序列构建系统状态表示，应用非线性变换提取跨维度动态特征，并在共享表示空间中整合资源利用率、调度行为和任务负载变化等多源信息。随后引入基于图的建模机制捕捉指标间的潜在依赖关系，学习资源链路上的竞争传播模式和结构干扰。在此基础上，设计任务特定的映射结构以建模竞争类型差异，增强分类器区分多种竞争模式的能力。

Result: 在公开系统跟踪数据集上的实验显示，该方法在准确率、召回率、精确率和F1分数上具有优势，对批量大小、训练样本规模和指标维度的敏感性分析进一步证实了模型的稳定性和适用性。

Conclusion: 结构化表示和基于高维指标的多任务分类能显著提高竞争模式识别，为复杂计算环境中的性能管理提供可靠技术途径。

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [141] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT是一个自动化LLM优化框架，解决了企业部署中的可扩展性和专业知识稀缺问题，显著提升GPU吞吐量，并已开源。


<details>
  <summary>Details</summary>
Motivation: 企业LLM部署面临可扩展性挑战，特别是在有限的计算预算下优化模型，且专业优化技能稀缺。

Method: OptiKIT是一个分布式LLM优化框架，通过自动化复杂优化流程为非专家团队提供支持，包括动态资源分配、分阶段管道执行和自动清理。

Result: 在生产环境中，OptiKIT实现了超过2倍的GPU吞吐量提升，并帮助应用团队在不具备深度LLM优化专业知识的情况下获得稳定的性能改进。

Conclusion: 论文开源了OptiKIT系统，以促进外部贡献和更广泛的可重复性。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [142] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: USF是一个用户空间调度框架，通过协作策略减少干扰，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统OS调度器在过载情况下因周期性抢占引入干扰，影响性能。

Method: 通过扩展GNU C库并集成nOS-V运行时，实现了用户空间调度框架USF及其默认协作策略SCHED_COOP。

Result: 在过载多进程场景下，包括嵌套BLAS工作负载、多进程PyTorch推理和分子动力学模拟，性能提升最高达2.4倍。

Conclusion: USF和SCHED_COOP通过用户空间调度框架显著减少了多进程场景下的干扰，提升了性能，最高可达2.4倍。

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [143] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap通过细粒度重叠通信与计算，显著提升多GPU性能。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU工作负载中通信成为首要瓶颈，现有分布式编译器主要通过流级别重叠整个计算和通信内核，这种粗粒度方法导致额外内核启动、内核边界处设备级同步及通信尾部延迟。

Method: AutoOverlap通过通信块抽象解耦通信粒度与内核结构及后端机制，支持从现有分布式编译器移植、用户直接编写或可重用模板实例化块级计划，并对本地Triton内核和块调度进行转换以对齐计算与块可用性。

Result: 作为Triton的源到源编译器实现，AutoOverlap在多GPU工作负载上平均端到端加速1.3倍，最高达4.7倍。

Conclusion: AutoOverlap作为一种编译器及运行时，通过引入通信块抽象，实现了在单个融合内核内的自动细粒度重叠，显著提升了多GPU工作负载的性能。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [144] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece是一种基于RDMA优化的分布式AIGC推理系统，通过微服务架构和动态资源管理，显著提升效率并降低GPU消耗。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC系统在并发工作负载下存在吞吐量、资源利用率和可扩展性方面的效率问题，亟需一种高效的分布式解决方案。

Method: 论文提出了一种名为OnePiece的大规模分布式推理系统，采用RDMA优化多阶段AIGC工作流，通过微服务分解、单边RDMA通信、双环缓冲区设计和动态节点管理器实现资源弹性分配。

Result: 实验结果表明，OnePiece在Wan2.1图像到视频生成任务中，相比传统单体推理流水线，GPU资源消耗降低了16倍。

Conclusion: OnePiece通过其分布式推理系统、RDMA优化和动态资源管理，显著提升了AIGC工作流的效率、可扩展性和容错能力，为生产环境提供了高效的解决方案。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [145] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: AF模型通过策略驱动的自主代理和p2p交互实现雾计算的高效协调，在动态条件下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 雾和边缘计算需要能够处理部分可观测性、严格延迟要求和动态变化工作负载的自适应控制方案。现有基于大型语言模型的Agentic AI（AAI）工具因高计算成本、随机性和较差的形式可分析性不适用于基础设施级系统。

Method: 提出了一种通用模型Agentic Fog（AF），其中雾节点表示为基于共享内存和局部协调的p2p交互策略驱动的自主代理。架构将系统目标分解为抽象策略指导，并将去中心化雾协调形式化为精确潜在博弈。

Result: 模拟显示AF系统在动态条件下比贪婪启发式和整数线性规划具有更低的平均延迟和更高的需求适应效率。敏感性分析还展示了在不同内存和协调条件下实现最优性能的能力。

Conclusion: AF系统在异步更新、有限理性最佳响应动态和节点故障情况下保证收敛和稳定，且在动态条件下比贪婪启发式和整数线性规划具有更低的平均延迟和更高的需求适应效率。

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [146] [Light Field Display Point Rendering](https://arxiv.org/abs/2601.19901)
*Ajinkya Gavane,Benjamin Watson*

Main category: cs.GR

TL;DR: LFDPR通过改进的渲染技术，显著提升了光场显示器的实时渲染速度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 光场显示器需要渲染数十甚至数百个视图，实时渲染难度大，因此需要高效且高质量的渲染解决方案。

Method: LFDPR结合了基于纹理的splatting和LFD-biased采样技术，引入了多视图mipmapping、角度超采样和重建来优化图像质量。

Result: LFDPR比多视图渲染快2-8倍，且图像质量相当。

Conclusion: LFDPR通过改进的渲染技术显著提升了光场显示器的实时渲染性能，同时保持了高质量的图像输出。

Abstract: Rendering for light field displays (LFDs) requires rendering of dozens or hundreds of views, which must then be combined into a single image on the display, making real-time LFD rendering extremely difficult. We introduce light field display point rendering (LFDPR), which meets these challenges by improving eye-based point rendering [Gavane and Watson 2023] with texture-based splatting, which avoids oversampling of triangles mapped to only a few texels; and with LFD-biased sampling, which adjusts horizontal and vertical triangle sampling to match the sampling of the LFD itself. To improve image quality, we introduce multiview mipmapping, which reduces texture aliasing even though compute shaders do not support hardware mipmapping. We also introduce angular supersampling and reconstruction to combat LFD view aliasing and crosstalk. The resulting LFDPR is 2-8x times faster than multiview rendering, with similar comparable quality.

</details>


### [147] [GRTX: Efficient Ray Tracing for 3D Gaussian-Based Rendering](https://arxiv.org/abs/2601.20429)
*Junseo Lee,Sangyun Jeon,Jungi Lee,Junyong Park,Jaewoong Sim*

Main category: cs.GR

TL;DR: GRTX通过精简加速结构和硬件检查点优化，高效提升了3D高斯渲染的射线追踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有高斯射线追踪方法存在加速结构臃肿和节点遍历冗余等问题，导致性能下降。

Method: 提出了两种优化方法：1) 通过射线空间变换将各向异性高斯视为单位球体，构建精简的加速结构；2) 在射线追踪单元中引入专用硬件支持，实现遍历检查点功能，减少冗余节点访问。

Result: 评估显示，GRTX相比基线射线追踪方法显著提升了性能。

Conclusion: GRTX通过软件和硬件优化显著提升了3D高斯渲染的射线追踪性能，且硬件成本几乎可忽略。

Abstract: 3D Gaussian Splatting has gained widespread adoption across diverse applications due to its exceptional rendering performance and visual quality. While most existing methods rely on rasterization to render Gaussians, recent research has started investigating ray tracing approaches to overcome the fundamental limitations inherent in rasterization. However, current Gaussian ray tracing methods suffer from inefficiencies such as bloated acceleration structures and redundant node traversals, which greatly degrade ray tracing performance.
  In this work, we present GRTX, a set of software and hardware optimizations that enable efficient ray tracing for 3D Gaussian-based rendering. First, we introduce a novel approach for constructing streamlined acceleration structures for Gaussian primitives. Our key insight is that anisotropic Gaussians can be treated as unit spheres through ray space transformations, which substantially reduces BVH size and traversal overhead. Second, we propose dedicated hardware support for traversal checkpointing within ray tracing units. This eliminates redundant node visits during multi-round tracing by resuming traversal from checkpointed nodes rather than restarting from the root node in each subsequent round. Our evaluation shows that GRTX significantly improves ray tracing performance compared to the baseline ray tracing method with a negligible hardware cost.

</details>


### [148] [Rendering Portals in Virtual Reality](https://arxiv.org/abs/2601.20722)
*Milan van Zanten*

Main category: cs.GR

TL;DR: 本文介绍了一种使VR门户过渡无缝的技术，分析了其性能影响并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 门户在计算机图形学中有广泛应用，尤其是在虚拟现实环境中用于人工扩展可用空间。

Method: 研究包括开发一种技术来隐藏门户过渡，并在测试场景中测量门户渲染的性能影响。

Result: 研究结果表明，该技术能有效隐藏门户过渡，并识别了门户渲染的性能瓶颈及优化潜力。

Conclusion: 本文提出了一种使虚拟现实环境中门户过渡对用户不可察觉的技术，并评估了门户渲染对性能的影响，提供了可能的优化方向。

Abstract: Portals have many applications in the field of computer graphics. Recently, they have found use as a way of artificially increasing the available space in a virtual reality (VR) environment. In this paper, we will cover a technique for making the transition through a portal unnoticeable to the user. Additionally, we will measure the performance impact of rendering portals in a test scene and provide some insight into possible optimisations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [149] [Proactive SFC Provisioning with Forecast-Driven DRL in Data Centers](https://arxiv.org/abs/2601.20229)
*Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 论文提出了一种结合深度强化学习和预测模型的混合框架，用于动态优化数据中心资源分配，显著提升服务接受率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态资源分配因流量和需求动态变化导致的资源过载或不足问题。

Method: 结合预测智能与SFC配置，利用深度强化学习生成数据集，并通过Optuna优化的集成模型（包括时空图神经网络、时序图神经网络和LSTM）进行预测，进而指导动态资源分配。

Result: 实验结果表明，该方法显著提升了资源密集型服务（如云游戏、VoIP）和延迟敏感服务（如增强现实、工业4.0）的接受率，并降低了VoIP、视频流和云游戏的端到端延迟。

Conclusion: 该论文提出的基于深度强化学习的混合预测框架显著提高了数据中心资源利用率和服务接受率，同时降低了端到端延迟，实现了更均衡的资源分配。

Abstract: Service Function Chaining (SFC) requires efficient placement of Virtual Network Functions (VNFs) to satisfy diverse service requirements while maintaining high resource utilization in Data Centers (DCs). Conventional static resource allocation often leads to overprovisioning or underprovisioning due to the dynamic nature of traffic loads and application demands. To address this challenge, we propose a hybrid forecast-driven Deep reinforcement learning (DRL) framework that combines predictive intelligence with SFC provisioning. Specifically, we leverage DRL to generate datasets capturing DC resource utilization and service demands, which are then used to train deep learning forecasting models. Using Optuna-based hyperparameter optimization, the best-performing models, Spatio-Temporal Graph Neural Network, Temporal Graph Neural Network, and Long Short-Term Memory, are combined into an ensemble to enhance stability and accuracy. The ensemble predictions are integrated into the DC selection process, enabling proactive placement decisions that consider both current and future resource availability. Experimental results demonstrate that the proposed method not only sustains high acceptance ratios for resource-intensive services such as Cloud Gaming and VoIP but also significantly improves acceptance ratios for latency-critical categories such as Augmented Reality increases from 30% to 50%, while Industry 4.0 improves from 30% to 45%. Consequently, the prediction-based model achieves significantly lower E2E latencies of 20.5%, 23.8%, and 34.8% reductions for VoIP, Video Streaming, and Cloud Gaming, respectively. This strategy ensures more balanced resource allocation, and reduces contention.

</details>


### [150] [Dependable Connectivity for Industrial Wireless Communication Networks](https://arxiv.org/abs/2601.20580)
*Nurul Huda Mahmood,Onel L. A. Lopez,David Ruiz-Guirola,Frank Burkhardt,Mehdi Rasti,Matti Latva-aho*

Main category: cs.NI

TL;DR: 本文提出了一个6G时代工业无线网络的可依赖性框架，结合理论与实际方案，展示了智能唤醒协议的优势，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 5G超可靠低延迟通信（URLLC）虽部分解决了可依赖性问题，但6G需更全面地涵盖可靠性、可用性、安全性和安全性。

Method: 首先建立了可依赖性的理论基础，包括其关键属性和分析工具；随后探讨了实际实现手段，如自适应多址接入方案和实时监控技术。

Result: 通过案例研究表明，智能唤醒协议比传统占空比方案显著提高了事件检测概率。

Conclusion: 本文提出了一个全面的框架，用于实现工业无线通信网络（IWCNs）的可依赖性，并展望了6G时代在可靠性、可用性、安全性和安全性方面的未来方向。

Abstract: Dependability - a system's ability to consistently provide reliable services by ensuring safety and maintainability in the face of internal or external disruptions - is a fundamental requirement for industrial wireless communication networks (IWCNs). While 5G ultra-reliable low-latency communication (URLLC) addresses some aspects of this challenge, its evolution toward holistic dependability in 6G must encompass reliability, availability, safety, and security. This paper provides a comprehensive framework for dependable IWCNs, bridging theory and practice. We first establish the theoretical foundations of dependability, including outlining its key attributes and presenting analytical tools to study it. Next, we explore practical enablers, such as adaptive multiple access schemes leveraging real-time monitoring and time-sensitive networking to ensure end-to-end determinism. A case study demonstrates how intelligent wake-up protocols improve event detection probability by orders of magnitude compared to conventional duty cycling. Finally, we outline open challenges and future directions for a 6G-driven dependable IWCN.

</details>


### [151] [Immersive Volumetric Video Playback: Near-RT Resource Allocation and O-RAN-based Implementation](https://arxiv.org/abs/2601.20625)
*Yao Wen,Luping Xiang,Kun Yang*

Main category: cs.NI

TL;DR: An O-RAN-integrated framework using SAC optimizes radio-compute-content resources, reducing latency by 11% and improving QoE for immersive streaming.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ultra-low MTP latency in immersive volumetric video streaming, which conventional edge-centric architectures struggle to meet.

Method: The system uses a Soft Actor-Critic (SAC) agent to optimize the rendered-pixel ratio over O-Cloud compute, gNB transmit power, and bandwidth, under a Weber-Fechner QoE model.

Result: Experiments show that SAC reduces median MTP latency by above 11% and improves both mean QoE and fairness.

Conclusion: The proposed O-RAN-integrated playback framework successfully reduces MTP latency and improves QoE, demonstrating the feasibility of joint radio-compute-content control for immersive streaming.

Abstract: Immersive volumetric video streaming in extended reality (XR) demands ultra-low motion-to-photon (MTP) latency, which conventional edge-centric architectures struggle to meet due to per-frame computationally intensive rendering tightly coupled with user motion. To address this challenge, we propose an Open Radio Access Network (O-RAN)-integrated playback framework that jointly orchestrates radio, compute, and content resources in near real time (Near-RT) control loop. The system formulates the rendered-pixel ratio as a continuous control variable and jointly optimizes it over the Open Cloud (O-Cloud) compute, gNB transmit power, and bandwidth under a Weber-Fechner quality of experience (QoE) model, explicitly balancing resolution, computation, and latency. A Soft Actor-Critic (SAC) agent with structured action decomposition and QoE-aware reward shaping resolves the resulting high-dimensional control problem. Experiments on a 5G O-RAN testbed and system simulations show that SAC reduces median MTP latency by above $11\%$ and improves both mean QoE and fairness, demonstrating the feasibility of RIC-driven joint radio-compute-content control for scalable, latency-aware immersive streaming.

</details>
