<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 研究提出基于高斯溅射的数字孪生方法，通过多尺度重建策略实现高效三维损伤可视化，适用于土木基础设施检测。


<details>
  <summary>Details</summary>
Motivation: 传统二维图像损伤识别方法无法满足三维可视化需求，而现代方法如NeRF和GS在场景表示和渲染质量上表现更优，其中GS因其高效性尤为突出。

Method: 该方法利用GS进行三维重建，结合多尺度重建策略平衡效率与细节，并支持随时间演变的损伤更新。

Result: 在开源合成地震后检测数据集上的实验表明，该方法能有效减少分割误差，实现高效且详细的三维损伤可视化。

Conclusion: 该研究提出了一种基于高斯溅射（GS）的数字孪生方法，有效实现了土木基础设施中三维损伤的可视化，为损伤的全面监测和更新提供了有前景的解决方案。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: UltraVQA数据集和ASO方法提升了视频质量评估的多维性和可解释性，实验表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 视频质量评估正从单一的平均意见分数向更丰富的多维度评估发展，需要更全面的数据集和方法。

Method: 提出了Analytic Score Optimization (ASO)，一种基于理论的多维VQA后训练目标，通过将质量评估重新定义为正则化决策过程，获得闭式解，确保与人类排名偏好一致。

Result: 实验表明，ASO方法在质量预测中优于大多数基线（包括闭源API和开源模型），并减少了平均绝对误差（MAE）。

Conclusion: 本文通过引入UltraVQA数据集和Analytic Score Optimization (ASO)方法，强调了多维、可解释的标注和基于强化的对齐在视频质量评估中的重要性。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: DODO利用块离散扩散技术，解决了OCR任务中自回归解码速度慢的问题，实现了高效并行解码和3倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在OCR任务中依赖自回归解码，计算成本高且速度慢，而扩散模型理论上可实现高效并行解码，但现有掩码扩散模型无法满足OCR的严格匹配要求。

Method: DODO采用块离散扩散技术，将生成过程分解为块，以减少全局扩散的同步错误。

Result: DODO在保持接近最先进准确性的同时，推理速度比自回归基线快3倍。

Conclusion: DODO通过引入块离散扩散方法，成功解决了现有扩散模型在OCR任务中的结构不稳定问题，实现了接近最先进的准确性和3倍的推理速度提升。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter-2通过新型ConvSS2D操作符和大规模合成数据集，显著提升水下立体深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决水下立体深度估计中因波长依赖性光衰减、散射和折射导致的严重域偏移问题。

Method: 提出了一种基于选择性状态空间模型的ConvSS2D操作符，取代传统的ConvGRU更新器，并构建了UW-StereoDepth-80K数据集。

Result: 在TartanAir-UW和SQUID基准测试中分别实现了17%和7.2%的性能提升，并在BlueROV2平台上验证了其鲁棒性。

Conclusion: StereoAdapter-2通过引入ConvSS2D操作符和构建大规模合成数据集UW-StereoDepth-80K，显著提升了水下立体深度估计的性能，并在真实世界验证中展示了其鲁棒性。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: SemCovNet通过动态调整语义特征权重和对齐视觉与语义，有效解决了语义覆盖不平衡问题，提升了模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在语义覆盖不平衡（SCI）问题，导致模型在学习罕见但有意义的语义时出现偏差，影响了模型的推理能力。

Method: SemCovNet结合了Semantic Descriptor Map（SDM）、Descriptor Attention Modulation（DAM）模块和Descriptor-Visual Alignment（DVA）损失函数，动态调整视觉与概念特征的权重，并优化语义对齐。

Result: 实验表明，SemCovNet显著降低了CDI，提升了模型的公平性和可靠性。

Conclusion: 本研究提出了SemCovNet模型，有效解决了语义覆盖不平衡（SCI）问题，并通过Coverage Disparity Index（CDI）量化了语义公平性，为提升视觉模型的可靠性和公平性奠定了基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一个基于Vision Transformer的统一视觉模型，通过三阶段训练和高效令牌重组，在图像和视频理解任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个统一的视觉模型架构，用于大规模图像和视频理解，利用行业规模的社交媒体数据进行训练。

Method: 采用三阶段训练流程，结合自监督MAE、半监督标签分类和CLIP风格对比学习，优化图像和视频模态。架构基于Vision Transformer，并通过高效令牌重组（EViT）提升计算效率。

Result: 在多个基准测试中达到最先进性能，包括ImageNet、Kinetics、HMDB51和MSCOCO，并展示了对领域转移和对抗扰动的强鲁棒性。

Conclusion: Xray-Visual 确立了可扩展、多模态视觉模型的新基准，同时保持了卓越的准确性和计算效率。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: HSI-SC-NeRF是一种静止相机多通道NeRF框架，用于农业产品的高通量高光谱3D重建，解决了传统方法的硬件复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱成像和3D重建在农业应用中集成时的硬件复杂性和自动化兼容性问题。

Method: 采用静止相机和多通道NeRF框架，结合ArUco校准标记和模拟姿态变换，通过两阶段训练协议优化重建。

Result: 在三种农业产品样本上实现了高空间重建精度和强光谱保真度。

Conclusion: HSI-SC-NeRF 框架在农业产品的高通量高光谱3D重建中表现出高空间重建精度和强光谱保真度，适合集成到自动化农业工作流程中。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 动态分块策略通过调整分块大小优化Diffusion Transformers的计算效率，显著加速生成过程且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 固定分块处理导致计算效率低下，尤其在去噪过程中无法适应不同阶段的需求。

Method: 提出动态分块策略，根据内容复杂度和去噪时间步动态调整分块大小。

Result: 在FLUX-1.Dev和Wan 2.1数据集上分别实现了3.52倍和3.2倍的加速，且未影响生成质量和提示遵从性。

Conclusion: 动态分块策略在保持生成质量的同时显著降低了计算成本，实现了高达3.52倍和3.2倍的加速效果。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO通过潜在变量填补缺失模态，在多模态数据不完整时仍保持高性能，并能量化模态对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据不完整（缺失、异步或部分可用）的问题，提升多模态学习的实用性。

Method: PRIMO是一种监督潜在变量填补模型，通过潜在变量建模缺失模态与观测模态的关系，并在推理时从学习分布中采样以获取预测分布。

Result: 在合成XOR数据集、Audio-Vision MNIST和MIMIC-III上，PRIMO在模态缺失或完整时均表现优异，并能量化模态的预测影响。

Conclusion: PRIMO在模态缺失或完整的情况下均能取得与单模态或多模态基线相当的性能，并通过基于方差的度量在实例级别量化模态的预测影响。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 提出了一种基于补丁的框架，用于人类-机器人协作绘画中的空间作者归属，准确率高达88.8%，并量化了风格重叠的不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在创意生产中的参与度增加，记录作者身份对艺术家、收藏家和法律场景变得至关重要。

Method: 使用基于补丁的框架，结合商品平板扫描仪和留一画交叉验证方法，进行空间作者归属。

Result: 该方法在补丁级别达到88.8%的准确率（通过多数投票在画作级别达到86.7%），优于基于纹理和预训练特征的基线（68.0%-84.7%）。

Conclusion: 该研究为人类-机器人协作绘画中的空间作者归属提供了一个高效的框架，未来有望扩展到任何人类-机器人协作绘画的作者归属。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG是一个检索增强的3D生成框架，通过层次对比检索和部件级编辑器，显著提升了部件级3D生成的质量和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 解决单图像3D生成中部件级结构的挑战，包括长尾部件几何覆盖和多视图一致性，以及精确局部编辑的支持不足。

Method: PartRAG采用检索增强框架，结合外部部件数据库和扩散Transformer，通过层次对比检索模块和多视图一致的掩码部件级编辑器实现。

Result: PartRAG在Objaverse上将Chamfer距离从0.1726降至0.1528，F-Score从0.7472提升至0.844，推理时间为38秒，交互编辑时间为5-8秒。

Conclusion: PartRAG在Objaverse、ShapeNet和ABO数据集上取得了竞争性结果，显著降低了Chamfer距离并提高了F-Score，同时保持了快速推理和交互式编辑能力。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 提出了Amber-Image系列轻量级T2I模型，通过高效压缩框架显著降低计算成本，保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决Diffusion Transformer架构在文本到图像生成中的高计算成本和部署障碍问题。

Method: 提出了一个高效的压缩框架，包括时间步敏感的深度剪枝策略、局部权重平均重新初始化、层级蒸馏和全参数微调，以及混合流架构和渐进蒸馏。

Result: Amber-Image系列模型参数减少了70%，无需大规模数据工程，整个压缩和训练流程成本极低（少于2000 GPU小时），在DPG-Bench和LongText-Bench等基准测试中表现出色。

Conclusion: Amber-Image系列模型通过高效的压缩框架，显著降低了计算成本和部署障碍，同时保持了高保真度的图像合成和卓越的文本渲染能力，与更大的模型相媲美。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: StructCore是一种无需训练的结构感知评分方法，通过捕捉异常得分图的结构特征，显著提升了异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 最大池化方法在异常检测中依赖单一极端响应，忽略了异常证据的分布和结构信息，导致正常与异常得分重叠。

Method: StructCore是一种无需训练、结构感知的图像级评分方法，通过计算低维结构描述符phi(S)并利用对角Mahalanobis校准来优化评分。

Result: StructCore在MVTec AD和VisA数据集上分别实现了99.6%和98.4%的图像级AUROC分数。

Conclusion: StructCore通过捕捉异常得分图的结构特征，显著提升了图像级异常检测的性能，超越了传统的最大池化方法。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 提出Cholec80-port数据集和SOP，解决trocar端口标注问题，提升跨数据集鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Trocar端口在腹腔镜视野中会持续遮挡，并因镜面纹理表面吸引过多特征点，对基于几何的下游任务（如图像拼接、3D重建和视觉SLAM）产生负面影响。现有公共数据集缺乏明确的端口标签，且标注常违反几何一致性。

Method: 提出Cholec80-port数据集，并制定严格的SOP，定义不包括中心开口的套管袖口掩模。同时清理并统一现有公共数据集。

Result: 实验表明，几何一致的标注显著提升了跨数据集的鲁棒性。

Conclusion: Cholec80-port数据集和统一的标准操作程序（SOP）显著提升了跨数据集的鲁棒性，超越了仅靠数据集规模带来的改进。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD通过双分支框架和交叉伪标签，在弱监督视频异常检测中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督视频异常检测旨在仅使用视频级标签检测异常并识别异常类别。

Method: 提出了一种双分支框架（CPL-VAD），包括二进制异常检测分支和类别分类分支，通过交叉伪标签交换互补优势。

Result: 在XD-Violence和UCF-Crime数据集上的实验表明，CPL-VAD在异常检测和异常类别分类中均达到最优性能。

Conclusion: CPL-VAD通过交叉伪标签的双分支框架，在视频异常检测和异常类别分类中实现了最先进的性能。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet是一种混合深度学习框架，用于在低光子统计和强背景噪声下准确定位微弱伽马射线暴，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微弱伽马射线暴（GRBs）的探测和定位存在挑战，现有机器学习模型难以在统计鲁棒性和噪声抑制之间取得平衡。

Method: 提出了一种混合深度学习框架ComptonUNet，结合直接重建模型的统计效率和基于图像架构的去噪能力。

Result: ComptonUNet在模拟的低统计量和高背景噪声环境下表现出色，定位精度显著提升。

Conclusion: ComptonUNet显著优于现有方法，在低统计量和高背景噪声场景下实现了更高的定位精度。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 论文提出了一种结合射频传感与3D高斯溅射的多模态框架，解决了视觉方法在恶劣条件下的局限性，通过RF信号高效生成3D点云，实现了高保真度的渲染。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的GS方法依赖于大量相机视图来初始化高斯基元并训练参数，但在恶劣天气、低光照或部分遮挡条件下表现不佳。RF信号对天气、光照和遮挡具有鲁棒性，因此论文提出结合RF传感与GS渲染，以提高渲染的效率和鲁棒性。

Method: 论文提出了一种多模态框架，整合了射频（RF）传感（如汽车雷达）与GS渲染技术。该方法通过RF信号提供的深度测量，高效预测深度并生成3D点云，用于初始化高斯函数，从而优化GS渲染流程。

Result: 数值测试表明，将RF传感引入GS流程能够显著提升3D场景渲染的保真度，尤其是在视觉线索不可靠的条件下。RF提供的结构信息准确性驱动了高质量的渲染效果。

Conclusion: 该论文提出了一种结合射频（RF）传感与3D高斯溅射（GS）的多模态框架，以解决传统基于视觉的GS方法在恶劣天气、低光照或部分遮挡条件下的局限性。通过RF信号的鲁棒性，该方法能够高效地从稀疏的RF深度测量中预测深度，为GS架构提供高质量的3D点云初始化，从而实现高保真度的3D场景渲染。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: B$^3$-Seg 是一种快速、无相机、无训练的3D高斯分割方法，通过贝叶斯更新和预期信息增益实现高效交互式分割。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯分割方法依赖于预定义的相机视角、真实标签或昂贵的重新训练，这使得它们在低延迟使用中不切实际。

Method: 该方法将分割重新表述为顺序的Beta-Bernoulli贝叶斯更新，并通过分析预期信息增益（EIG）主动选择下一个视图。这种贝叶斯公式保证了EIG的自适应单调性和子模性，产生了对最优视图采样策略的贪心$(1{-}1/e)$近似。

Result: 在多个数据集上的实验表明，B$^3$-Seg 实现了与高成本监督方法相竞争的结果，同时在几秒内完成端到端分割。

Conclusion: B$^3$-Seg 提出了一种快速且理论可靠的方法，用于在无相机和无训练条件下进行开放词汇的3D高斯分割，实现了实用的交互式分割，并证明了信息效率。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++ 是一种针对多模态对比学习模型的后门攻击框架，解决了隐蔽性和持久性问题，实验证明其在低中毒率下仍能保持高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习模型的后门攻击研究面临隐蔽性和持久性两大挑战，现有方法在强检测或持续微调下容易失效。

Method: 采用语义融合 QR 微触发器嵌入不可察觉的触发模式，并通过半径收缩和质心对齐稳定触发器嵌入，同时通过曲率控制和弹性权重巩固稳定模型参数。

Result: 在仅 0.3% 中毒率下，BadCLIP++ 在数字环境中实现了 99.99% 的攻击成功率（ASR），并且在 19 种防御下 ASR 仍高于 99.90%。

Conclusion: BadCLIP++ 通过语义融合 QR 微触发器和目标对齐子集选择解决了隐蔽性和持久性问题，实验证明其在低中毒率下仍能保持高攻击成功率。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM是一种基于3D高斯点云的内窥镜非刚性SLAM系统，通过变形感知地图和贝叶斯优化，显著提升了姿态估计和重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜场景中由于软组织持续变形导致的相机运动与固有变形耦合问题，提升非刚性SLAM的性能。

Method: 提出了基于3D高斯点云的变形感知地图，结合贝叶斯自监督策略优化变形概率，设计了从粗到精的姿态估计和变形更新模块。

Result: 实验显示NRGS-SLAM在相机姿态估计误差（RMSE降低50%）和重建质量上优于现有方法。

Conclusion: NRGS-SLAM在多个公共内窥镜数据集上表现出色，显著提升了相机姿态估计的准确性和重建质量。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出VIG指标量化视觉依赖性，设计选择性训练方案，显著提升LVLMs性能并减少语言偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs存在语言偏见问题，缺乏对视觉依赖性的定量评估，需开发新指标和训练方法以提升视觉依赖性。

Method: 提出基于困惑度的VIG指标，量化视觉输入对预测不确定性的减少，并设计VIG引导的选择性训练方案，优先训练高VIG样本和标记。

Result: VIG指标能精细分析样本和标记级别的视觉依赖性，选择性训练方案有效提升视觉依赖性并减少语言偏见。

Conclusion: 引入视觉信息增益（VIG）指标和选择性训练方案，显著提升了LVLMs的视觉依赖性，减少了语言偏见，并在减少监督的情况下实现了更优性能。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: EntropyPrune 是一种基于矩阵熵的令牌剪枝框架，显著提升 MLLM 推理效率，在 LLaVA-1.5-7B 上减少 68.2% FLOPs 并保持 96% 性能。


<details>
  <summary>Details</summary>
Motivation: 现有令牌剪枝方法通常依赖静态、经验选择的层，缺乏解释性和跨模型的可迁移性。本文旨在通过矩阵熵视角提供一种原则性的剪枝阶段选择标准。

Method: 提出基于矩阵熵的令牌剪枝框架 EntropyPrune，通过量化视觉令牌的信息价值并剪枝冗余令牌，避免了依赖注意力图的传统方法。利用双 Gram 矩阵的谱等价性降低熵计算复杂度，实现高达 64 倍的理论加速。

Result: 在 LLaVA-1.5-7B 上，EntropyPrune 实现了 68.2% 的 FLOPs 减少，同时保留了 96.0% 的原始性能。该方法在高分辨率和视频模型中同样有效，展示了其鲁棒性和可扩展性。

Conclusion: EntropyPrune 通过矩阵熵视角识别“熵崩溃层”（ECL），并提出了一种新颖的矩阵熵引导的令牌剪枝框架，显著提升了多模态大语言模型（MLLM）的推理效率，同时保持了高性能。该方法在多样化的多模态基准测试中表现优异，并展示了强大的鲁棒性和可扩展性。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: GASS方法通过几何分解提升文本到图像生成的多样性，兼顾保真度和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像生成模型在语义对齐上表现良好，但生成的图像多样性不足，限制了用户选择并可能加剧社会偏见。

Method: 引入了Geometry-Aware Spherical Sampling（GASS），通过显式控制与提示相关和无关的变异源来增强多样性。具体包括在CLIP嵌入中分解多样性度量，并沿两个正交方向（文本嵌入和无关方向）增加几何投影分布。

Result: 在不同冻结的文本到图像骨干网络（U-Net和DiT，扩散和流）和基准测试中，GASS方法有效提升了多样性，且对图像保真度和语义对齐影响最小。

Conclusion: 通过几何感知球形采样（GASS）方法，成功提升了文本到图像生成模型的多样性，同时保持了图像保真度和语义对齐。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [HiMAP: History-aware Map-occupancy Prediction with Fallback](https://arxiv.org/abs/2602.17231)
*Yiming Xu,Yi Yang,Hao Cheng,Monika Sester*

Main category: cs.CV

TL;DR: HiMAP是一种无需跟踪的轨迹预测框架，在MOT失败时仍可靠，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决因遮挡、身份切换或漏检导致的MOT失败问题，提升预测质量与安全性。

Method: HiMAP将过去检测转换为时空不变的历史占用地图，引入历史查询模块，结合当前代理状态迭代检索未标记占用表示中的代理特定历史，并通过DETR风格解码器生成多模态未来轨迹。

Result: 在Argoverse 2上，HiMAP在无跟踪设置下相对QCNet提升11% FDE、12% ADE，并降低4% MR，同时为所有代理提供稳定预测。

Conclusion: HiMAP是一种无需跟踪的轨迹预测框架，在MOT失败时仍能保持可靠性，性能与基于跟踪的方法相当，并在无跟踪设置下显著优于基线方法。

Abstract: Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.

</details>


### [25] [Inferring Height from Earth Embeddings: First insights using Google AlphaEarth](https://arxiv.org/abs/2602.17250)
*Alireza Hamoudzadeh,Valeria Belloni,Roberta Ravanelli*

Main category: cs.CV

TL;DR: AlphaEarth Embeddings能有效指导深度学习模型进行高度映射，U-Net++表现优于U-Net，但需解决泛化偏差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨Earth Embeddings中编码的地理空间和多模态特征是否能有效指导深度学习回归模型进行区域表面高度映射。

Method: 研究采用了U-Net和U-Net++作为轻量级卷积解码器，评估了AlphaEarth Embeddings中编码的地理空间信息如何转化为准确的地表高度估计。

Result: U-Net和U-Net++在训练集上表现优异（R²=0.97），但在测试集上性能下降。U-Net++显示出更好的泛化能力（R²=0.84）和鲁棒性。

Conclusion: 研究证实了AlphaEarth Embeddings在指导基于深度学习的区域表面高度映射工作流程中的潜力，尤其是在结合空间感知卷积架构时，但同时也指出了需要解决偏差问题以提高区域可转移性。

Abstract: This study investigates whether the geospatial and multimodal features encoded in \textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.

</details>


### [26] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文介绍了一种基于基础设施的多模态货运车辆检测系统，结合LiDAR和摄像头传感器，通过混合感知架构实现高精度实时监测，为FSP应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 货运车辆接近信号灯交叉口需要可靠的检测和运动估计，以支持基于基础设施的货运信号优先（FSP）。准确及时地感知车辆类型、位置和速度对于实现有效的优先控制策略至关重要。

Method: 采用混合感知架构，结合了基于聚类和深度学习的检测方法，并利用卡尔曼滤波进行跟踪，以实现稳定的实时性能。

Result: 现场评估表明，该系统能够以高时空分辨率可靠地监测货运车辆的运动。

Conclusion: 该论文的设计和部署为开发支持FSP应用的基础设施感知系统提供了实用见解。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [27] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: EA-Swin 是一种新型视频检测方法，通过嵌入无关的 Swin Transformer 和因子化窗口注意力设计，显著提升了检测准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖于浅层嵌入轨迹、基于图像的适应或计算密集的 MLLM，面对高度逼真的合成视频时表现不足。

Method: EA-Swin 是一种嵌入无关的 Swin Transformer，采用因子化窗口注意力设计，直接在预训练视频嵌入上建模时空依赖关系。同时构建了 EA-Video 数据集，包含 130K 视频，覆盖多样化的商业和开源生成器。

Result: EA-Swin 在主要生成器上实现了 0.97-0.99 的准确率，比现有方法（通常为 0.8-0.9）高出 5-20%，并在未见分布上表现出强泛化能力。

Conclusion: EA-Swin 提出了一种嵌入无关的 Swin Transformer，通过因子化窗口注意力设计直接在预训练视频嵌入上建模时空依赖关系，为现代 AI 生成视频检测提供了可扩展且稳健的解决方案。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [28] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: PESTGAN结合物理动力学和生成对抗网络，显著提升了热带气旋图像的超分辨率效果，尤其在结构保真度和物理真实性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超分辨率方法往往将卫星图像序列视为普通视频，忽略了云运动的物理规律，因此需要一种能结合物理动力学的方法来提升热带气旋图像的超分辨率效果。

Method: 提出了一种物理编码的空间和时间生成对抗网络（PESTGAN），设计了包含PhyCell模块的解耦生成器架构，通过约束卷积近似涡度方程，并将物理动力学编码为隐式潜在表示。此外，引入了双判别器框架，利用时间判别器增强运动一致性和空间真实性。

Result: 在Digital Typhoon数据集上的4倍放大实验中，PESTGAN在结构保真度和感知质量上表现优异，同时在重建气象学上合理的云结构方面具有显著优势。

Conclusion: PESTGAN通过结合物理动力学和视觉纹理分离的方法，在热带气旋图像超分辨率任务中实现了更好的结构保真度和感知质量，同时保持了像素级准确性。

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [29] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: The paper proposes attachment anchors to improve grasping point prediction in colorectal surgery, showing better results than image-only methods, especially in unfamiliar scenarios.


<details>
  <summary>Details</summary>
Motivation: Colorectal surgeries are complex and underrepresented in current research, yet they offer a promising learning environment due to repetitive tissue manipulation, making them ideal for autonomous, machine learning-driven support.

Method: The paper introduces attachment anchors, a structured representation that encodes local geometric and mechanical relationships between tissue and its anatomical attachments. This representation is predicted from laparoscopic images and integrated into a machine learning-based grasping framework.

Result: Experiments on 90 colorectal surgeries show that attachment anchors improve grasping point prediction compared to image-only baselines, with notable gains in out-of-distribution scenarios.

Conclusion: Attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery, particularly improving grasping point prediction in out-of-distribution settings.

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [30] [Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline](https://arxiv.org/abs/2602.17322)
*Mohamed Dhouib,Davide Buscaldi,Sonia Vanier,Aymen Shabou*

Main category: cs.CV

TL;DR: 提出了一种生成高质量篡改文档图像的新方法，通过对比学习和裁剪评估网络提升数据质量，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的方法生成的篡改文档多样性不足且视觉质量差，导致模型在真实数据上表现不佳。

Method: 通过训练两个辅助网络（一个用于比较文本裁剪，另一个用于评估裁剪质量），并结合精心设计的生成流程，提出了一种新的篡改文档图像生成框架。

Result: 在不同架构和数据集上，新方法均带来了性能提升。

Conclusion: 提出的方法能够生成多样且高质量的篡改文档图像，显著提升了模型在真实数据上的性能。

Abstract: Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.

</details>


### [31] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini是一种基于解剖学特征的图像配准框架，利用深度学习分割模型的质心点实现高效仿射匹配，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习预训练分割模型提供的可靠解剖学分割，克服传统特征点提取的挑战，开发新的解剖学基础图像配准算法。

Method: Polaffini通过提取分割区域的质心获得解剖学基础特征点，利用闭式解实现高效的全局和局部仿射匹配，并生成从仿射到多仿射的可调平滑变换。

Result: Polaffini在结构对齐方面优于现有强度配准方法，并为后续非线性配准提供了更好的初始化。

Conclusion: Polaffini是一个快速、稳健且准确的解剖学基础图像配准框架，特别适合集成到医学图像处理流程中。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [32] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 研究利用深度学习模型生成南美洲10米分辨率树作物地图，发现现有监管地图误分类农业为森林，可能导致不公平处罚，提供高分辨率基线以减少风险。


<details>
  <summary>Details</summary>
Motivation: 监测树作物扩张对零毁林政策（如欧盟无毁林产品法规）至关重要，但缺乏高分辨率数据区分农业系统和森林阻碍了这些努力。

Method: 使用基于Sentinel-1和Sentinel-2卫星影像时间序列的多模态时空深度学习模型，生成了南美洲首个10米分辨率的树作物地图。

Result: 地图识别出约1100万公顷的树作物，其中23%与2000-2020年森林覆盖损失相关。分析显示，现有支持欧盟法规的地图常将已建立的农业（尤其是小农农林复合系统）误分类为“森林”。

Conclusion: 该研究通过提供高分辨率的树作物地图，支持了有效、包容和公平的保护政策，减少了因现有监管地图误分类导致的不公平处罚风险。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [33] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: CAG 通过双分支设计优化 VLAs 的语言遵循能力，减少视觉捷径依赖，显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有 VLAs 在缺乏场景特定监督时，易受数据集偏差影响，产生反事实失败，即忽略语言意图而依赖视觉捷径。

Method: 提出 CAG 方案，结合标准 VLA 策略和无语言条件的 Vision-Action (VA) 模块，通过反事实比较优化动作选择。

Result: CAG 在 LIBERO-CF 基准上显著提升语言遵循准确率（9.7%）和任务成功率（3.6%），真实场景中平均减少 9.4% 的反事实失败并提升 17.2% 的任务成功率。

Conclusion: Counterfactual Action Guidance (CAG) 是一种简单有效的双分支推理方案，通过显式调节语言条件，显著减少了视觉捷径的依赖，提高了语言遵循能力和任务成功率。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [34] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR是一种基于RetNet的Decoder-only模型，通过softmax-free retention和多尺度序列先验，显著提升了手写文本识别的解码速度和内存效率，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的手写文本识别系统由于KV缓存的增长导致解码速度慢且内存占用高，因此需要一种更高效的替代方案。

Method: DRetHTR通过替换softmax注意力为softmax-free retention，并注入多尺度序列先验，避免了KV缓存的增长，实现了线性的时间和内存解码。此外，通过层级的gamma缩放，逐步扩大有效保留范围，恢复了注意力的局部到全局归纳偏差。

Result: DRetHTR在IAM-A（2.26%）、RIMES（1.81%）、Bentham（3.46%）和READ-2016（4.21%）数据集上取得了最佳或竞争性的测试字符错误率，同时实现了1.6-1.9倍的推理加速和38-42%的内存节省。

Conclusion: DRetHTR展示了基于Retentive Networks的Decoder-only模型在保持Transformer级别准确性的同时，显著提升了解码速度和内存效率。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [35] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD 是一种高效的多模态方法，通过统一的跨模态表示和知识蒸馏，在广义类别发现任务中实现高性能和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态时独立对待各模态且计算成本高，SpectralGCD 旨在通过统一的跨模态表示和高效的学习策略解决这些问题。

Method: SpectralGCD 使用 CLIP 跨模态图像-概念相似性作为统一的跨模态表示，并通过 Spectral Filtering 保留相关概念，结合前向和反向知识蒸馏确保学生模型的跨模态表示既语义充足又对齐良好。

Result: SpectralGCD 在多个基准测试中达到或超越现有方法的性能，同时显著降低计算成本。

Conclusion: SpectralGCD 在六个基准测试中表现出与或显著优于现有方法的准确度，且计算成本大幅降低。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [36] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文综述了光学遥感领域的进展，为研究者提供全面指南，填补了现有文献的空白。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和RGB相机的普及，光学遥感领域发展迅速，但缺乏一个全面的综述来指导新进入者。

Method: 通过综述光学遥感领域的现有文献，涵盖多样化的任务、能力和方法，提供高层次见解。

Result: 论文总结了光学遥感的能力、数据集和关键见解，为研究者提供了实用的指南。

Conclusion: 该论文为计算机视觉和遥感领域的研究者提供了一个全面的指南，突出了数据集和关键见解，填补了现有文献中缺乏整体视角的空白。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [37] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: EAGLE是一种无需调参的多模态大语言模型框架，通过专家模型引导实现高精度且可解释的工业异常检测，性能媲美调参方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有工业异常检测方法仅提供二元决策且缺乏语义解释的问题，同时避免MLLMs需要昂贵调参且性能提升有限的问题。

Method: 提出了一种无需调参的框架EAGLE，通过整合专家模型的输出来引导MLLMs，同时研究了EAGLE对MLLMs内部注意力分布的影响。

Result: 在MVTec-AD和VisA数据集上的实验表明，EAGLE无需参数更新即可提升多种MLLMs的异常检测性能，达到与调参方法相当的结果。

Conclusion: EAGLE框架无需调参即可在多模态大语言模型（MLLMs）中实现高精度的工业异常检测，并提供可解释的异常描述，其性能与基于调参的方法相当。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [38] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: Local-EndoGS 是一种针对单目内窥镜序列的4D重建框架，通过窗口化策略和从粗到精的优化，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在单目内窥镜序列中因依赖固定视角或深度先验而难以处理大范围相机运动的问题。

Method: 提出了一种基于窗口的全局表示方法，结合从粗到精的策略，整合多视图几何、跨窗口信息和单目深度先验。

Result: 在三个公开数据集上，Local-EndoGS 在外观质量和几何重建上均优于现有方法。

Conclusion: Local-EndoGS 在单目内窥镜序列中表现出色，显著提升了外观质量和几何重建效果，适用于大范围相机运动的临床场景。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [39] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 该研究提出了一种物理感知的多模态框架，通过合成数据、指令数据集和物理感知模型，解决了二维量子材料光学显微镜图像表征的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于二维量子材料光学显微镜图像的表征存在层间对比度微弱、标记数据有限以及实验室间差异大的问题，现有视觉模型缺乏物理先验且泛化能力不足。

Method: 研究提出了Synthia（物理基础的合成数据生成器）、QMat-Instruct（大规模指令数据集）和QuPAINT（物理感知指令调优架构），并结合QF-Bench（标准化评估基准）。

Result: 提出的框架通过合成数据、指令数据集和物理感知模型，显著提升了量子材料表征的准确性和泛化能力。

Conclusion: 该研究通过提出一个物理感知的多模态框架，成功解决了二维量子材料光学显微镜图像表征中的挑战，包括数据依赖、模型泛化能力和跨实验室一致性。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [40] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 论文提出PixTrace和CopyNCE方法，通过几何可追踪性和对比学习提升图像复制检测性能，实验显示显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的视图级对比方法在处理复杂编辑时由于缺乏细粒度对应学习而表现不佳，论文旨在通过几何可追踪性解决这一问题。

Method: 提出了PixTrace像素坐标跟踪模块和CopyNCE几何引导对比损失，结合像素级可追踪性和块级相似性学习，优化了自监督学习中的监督噪声。

Result: 在DISC21数据集上，匹配器达到88.7% uAP / 83.9% RP90，描述符达到72.6% uAP / 68.4% RP90，性能优于现有方法。

Conclusion: 该论文通过PixTrace和CopyNCE的创新方法，成功提升了图像复制检测的性能和可解释性，实现了在DISC21数据集上的最优表现。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [41] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 该研究通过深度增强姿态估计和NICP方法，简化了腹腔镜肝脏手术中的肿瘤定位流程，实现了临床精度，并降低了工程复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的配准流程通常依赖器官轮廓，且变形（非刚性）对齐常采用有限元模型结合降维或机器学习组件，这增加了工程/建模复杂性和专业知识要求。

Method: 研究整合了腹腔镜深度图与基础姿态估计器进行相机-肝脏姿态估计，并采用非刚性迭代最近点（NICP）替代传统的有限元（FE）变形模型。

Result: 在真实患者数据上，深度增强的基础姿态方法在3个案例中实现了9.91毫米的平均配准误差。刚性-NICP联合配准优于仅刚性配准，证明了NICP作为有限元变形模型的高效替代方案。

Conclusion: 该研究提出了一种轻量级、工程友好的替代方案，用于腹腔镜肝脏手术中的肿瘤定位，通过结合深度增强的基础姿态估计和非刚性迭代最近点（NICP）方法，实现了临床相关精度。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [42] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: LATA是一种无需训练和标签的改进方法，通过平滑零样本概率和改进预测集效率，显著提升了医疗VLMs在域转移下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型（VLMs）在零样本识别中表现强大，但在域转移下的可靠性依赖于校准的不确定性。现有的分割符合预测（SCP）方法在少数样本和不平衡情况下效率低且类间覆盖不平衡。

Method: LATA（Laplacian-Assisted Transductive Adaptation）通过图像-图像k-NN图上的少量CCCP平均场更新平滑零样本概率，并引入故障感知的符合分数来改进预测集效率和类间平衡。

Result: 在三个医疗VLM和九个下游任务中，LATA一致减少了集合大小和类间覆盖差距，同时保持或收紧目标覆盖，优于先前的转导基线，且计算成本低。

Conclusion: LATA方法在保持SPC有效性的同时，通过平滑零样本概率和改进预测集效率，显著减少了集合大小和类间覆盖差距，且在计算上轻量且无需更新VLM。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [43] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker通过构建事件级场景图和强化视觉基础，有效减少视频推理中的幻觉，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频推理需要理解事件间的因果关系，但现有MLLM通过密集标注或视频摘要推断关系，缺乏因果结构建模，导致推理中出现幻觉。

Method: 提出GraphThinker方法，利用MLLM构建事件视频场景图（EVSG），显式建模事件内和事件间关系，并通过强化微调引入视觉注意力奖励以增强视频基础。

Result: 在RexTime和VidHalluc数据集上，GraphThinker能够更精确地捕捉对象和事件关系，减少幻觉现象。

Conclusion: GraphThinker通过强化微调方法构建结构化事件级场景图并增强视觉基础，显著减少了视频推理中的幻觉现象，并在RexTime和VidHalluc数据集上表现出优于现有方法的能力。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [44] [RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward](https://arxiv.org/abs/2602.17558)
*Qiucheng Wu,Jing Shi,Simon Jenni,Kushal Kafle,Tianyu Wang,Shiyu Chang,Handong Zhao*

Main category: cs.CV

TL;DR: RetouchIQ是一个通过MLLM代理和通用奖励模型实现指令图像编辑的框架，显著提升了编辑质量。


<details>
  <summary>Details</summary>
Motivation: 解决专业图像编辑中缺乏可靠、可验证奖励信号的挑战，以支持创意编辑的主观性。

Method: 引入RetouchIQ框架，通过MLLM代理执行基于指令的图像编辑，并采用通用奖励模型提供高质量、指令一致的梯度反馈。

Result: RetouchIQ在语义一致性和感知质量上显著优于之前的MLLM和基于扩散的编辑系统。

Conclusion: RetouchIQ展示了通用奖励驱动的MLLM代理在专业图像编辑中的潜力，作为灵活、可解释且可执行的助手。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>


### [45] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: ArtToMus是一个直接从艺术品生成音乐的框架，避免了图像到文本的转换，实验结果显示了良好的音乐连贯性和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的音乐生成系统通常依赖自然照片训练，且通过图像到文本转换阶段简化条件，无法直接学习视觉到音频的映射。

Method: 提出了ArtToMus框架，通过将视觉嵌入投影到潜在扩散模型的调节空间，实现仅由视觉信息引导的音乐合成。

Result: ArtToMus生成的音乐在音乐连贯性和风格一致性上表现良好，能够反映源艺术品的显著视觉线索。

Conclusion: ArtToMus框架首次实现了直接从艺术品到音乐的生成，无需图像到文本的转换或基于语言的语义监督，为视觉到音乐生成这一独特且具有挑战性的研究方向奠定了基础。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [46] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 该论文提出了一种结合主动学习、在线元学习和概念引导推理的地理空间发现框架，通过概念加权采样和元批处理策略，在资源有限和动态环境中高效发现隐藏目标，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在环境监测、灾害响应或公共卫生等现实场景中，数据收集成本高且环境动态变化，如何在资源有限的情况下高效发现隐藏目标是一个关键挑战。现有基于学习的方法（如强化学习）因地理空间地面真值稀疏且有偏差而受限。

Method: 论文提出了两个关键创新：基于概念相关性的概念加权不确定性采样策略和相关性感知元批处理形成策略。这些策略通过利用领域特定概念（如土地覆盖、源接近度）来调节不确定性，并在在线元更新中促进语义多样性。

Result: 实验结果表明，该方法在PFAS污染的真实世界数据集上表现可靠，能够在数据有限和环境变化的情况下有效发现目标。

Conclusion: 该论文提出了一种统一的地理空间发现框架，通过结合主动学习、在线元学习和概念引导推理，有效解决了在资源有限和动态环境下发现隐藏目标的问题。实验证明，该方法在真实世界数据集上具有可靠性和高效性。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [47] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: CORAL是一种基于DiT的虚拟试衣框架，通过显式对齐查询-键匹配和引入互补损失函数，显著提升了细节保留效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在非配对设置下难以保持服装细节，且未明确解释DiT中人物-服装对应关系的形成机制。

Method: 提出CORAL框架，包括对应蒸馏损失和熵最小化损失，以优化DiT架构中的查询-键匹配。

Result: CORAL在基线上持续改进，提升了全局形状转换和局部细节保留，并通过大量消融实验验证了设计选择。

Conclusion: CORAL框架通过显式对齐查询-键匹配和引入互补损失函数，显著提升了虚拟试衣中的全局形状转换和局部细节保留效果。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [48] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec通过用户反馈交互优化对象检索，显著提升准确率，尤其在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有开放词汇检测器一次性预测的不足，无法根据用户反馈优化预测结果。

Method: IntRec采用Intent State（IS）维护正负记忆集，通过对比对齐函数对候选对象进行排名，实现细粒度消歧。

Result: 在LVIS数据集上，IntRec的AP达到35.4，优于OVMR、CoDet和CAKE；在LVIS-Ambiguous基准测试中，单次反馈后性能提升7.9 AP，每次交互延迟小于30毫秒。

Conclusion: IntRec框架通过用户反馈交互式优化对象检索，显著提升了检索准确率，且在复杂场景中表现出色。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [49] [Human-level 3D shape perception emerges from multi-view learning](https://arxiv.org/abs/2602.17650)
*Tyler Bonnen,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 研究者开发了一种新型神经网络，通过自然视觉数据训练，成功模拟人类从二维图像推断三维结构的能力，首次达到人类准确度。


<details>
  <summary>Details</summary>
Motivation: 模拟人类从二维视觉输入推断三维结构的能力，这是视觉智能科学与工程的长期目标。

Method: 开发了一种新型神经网络框架，利用视觉空间目标在自然感官数据上进行训练，无需依赖任何对象相关的归纳偏差。

Result: 该模型首次在三维形状推断任务上达到人类准确度，且无需任务特定训练或微调。

Conclusion: 人类级别的三维感知能力可以通过简单、可扩展的学习目标从自然视觉空间数据中涌现。

Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.

</details>


### [50] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: OpenEarthAgent是一个工具增强的地理空间代理框架，通过监督微调和结构化推理轨迹训练，在遥感领域实现多模态推理，性能优于基线并与现有模型竞争。


<details>
  <summary>Details</summary>
Motivation: 将多模态推理能力扩展到遥感领域面临挑战，需模型在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步骤逻辑。

Method: OpenEarthAgent采用监督微调方法，基于结构化推理轨迹训练模型，结合卫星图像、自然语言查询和详细推理轨迹，支持多步骤工具交互。

Result: 训练集包含14,538个实例和100K+推理步骤，评估集有1,169个实例和7K+推理步骤。模型在多个领域（城市、环境、灾害、基础设施）表现优异，支持GIS操作和指数分析（如NDVI、NBR、NDBI）。

Conclusion: OpenEarthAgent通过工具增强的地理空间代理，在多样化的地理空间分析任务中展现了结构化推理、稳定的空间理解和可解释的行为，性能优于基线模型并与现有开源和闭源模型竞争。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 该研究提出了一个数字孪生框架，用于施工期间的元素级质量保证和决策支持，通过整合多种数据流实现早期质量评估，补充而非替代现有流程。


<details>
  <summary>Details</summary>
Motivation: 大型公路和桥梁项目中的质量保证通常依赖于工作完成后几天或几周才可用的检查记录和实验室测试结果，这种延迟限制了早期干预，增加了返工、进度影响和文档碎片化的风险。

Method: 该框架将检查记录、材料生产和放置数据、早期感知和预测强度模型与单个施工元素联系起来，通过整合这些数据流，系统表示每个元素的不断变化的质量状态，并在标准年龄测试结果可用之前支持结构化的释放或保留决策。

Result: 该框架通过提高可追溯性和实现更早的、数据驱动的质量评估，补充了现有的工作流程，而不取代既定的检查和测试程序。

Conclusion: 该研究提出了一个施工阶段的数字孪生框架，旨在支持主动施工期间的元素级质量保证和基于准备情况的决策，为从延迟的、文档驱动的审查过渡到主动的、元素级决策支持提供了结构化途径。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [52] [Hybrid-Gym: Training Coding Agents to Generalize Across Tasks](https://arxiv.org/abs/2602.16819)
*Yiqing Xie,Emmy Liu,Gaokai Zhang,Nachiket Kotalwar,Shubham Gandhi,Sathwik Acharya,Xingyao Wang,Carolyn Rose,Graham Neubig,Daniel Fried*

Main category: cs.SE

TL;DR: Hybrid-Gym 通过合成任务训练编码代理，显著提升其在多样化现实任务中的表现，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如 SWE-Bench）局限于单一 GitHub 问题，而实际任务更复杂多样，需要探索代码库、测试软件等技能。

Method: 通过分解轨迹为细粒度组件，识别可迁移技能，并设计辅助训练任务（如函数定位和依赖搜索）来训练语言模型。

Result: 在 SWE-Bench Verified、SWT-Bench Verified 和 Commit-0 Lite 上分别实现 25.4%、7.9% 和 5.1% 的绝对提升，且能补充下游任务数据集。

Conclusion: Hybrid-Gym 训练环境通过合成任务有效提升了编码代理在多样化现实任务中的泛化能力，显著优于现有基准。

Abstract: When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.

</details>


### [53] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: 研究利用LLM从高保真原型自动生成用户故事，发现结合LEL词汇表提示可显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合用户故事和高保真原型，利用LLM实现敏捷、自动化的用户故事生成，改善用户与开发者之间的沟通。

Method: 通过案例研究分析LLM从高保真原型中提取用户故事的能力，比较了包含和不包含LEL词汇表提示的效果。

Result: 结果显示，包含LEL词汇表的提示能显著提升生成用户故事的准确性和适用性。

Conclusion: 结合LEL的提示能显著提高LLM生成用户故事的准确性和适用性，为AI在需求工程中的集成提供了新思路。

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>


### [54] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究Obsidian非开发者社区的插件生态系统，发现其仍能形成工程结构，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究Obsidian这一以知识为中心的平台，其社区虽非开发者主导，却建立了庞大的插件生态系统，旨在探索此类混合生态系统的维护方式及插件类型。

Method: 通过仓库挖掘和基于LLM的主题建模，对396个插件样本进行分析，识别出六个主题，并分析这些插件的Pull Requests。

Result: 识别出六个与知识管理和工具相关的主题，并发现这些生态系统中有大量软件演化活动。

Conclusion: 研究发现，即使在非开发者主导的混合社区中，插件生态系统也能形成可识别的工程结构，为未来研究提供了三个方向及六个相关问题，关注这些非开发者生态系统的健康与可持续性。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [55] [Wink: Recovering from Misbehaviors in Coding Agents](https://arxiv.org/abs/2602.17037)
*Rahul Nanda,Chandra Maddila,Smriti Jha,Euna Mehnaz Khan,Matteo Paltenghi,Satish Chandra*

Main category: cs.SE

TL;DR: 本文提出Wink系统，用于自动修复LLM驱动的编码代理的不当行为，通过轻量级干预显著减少失败率，提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 自主编码代理（LLM驱动）在软件行业中广泛使用，但存在多种不当行为，如偏离用户指令、陷入重复循环或工具使用失败，这些行为需要资源密集型的手动干预。

Method: 开发了一个轻量级、异步的自我干预系统Wink，通过观察代理轨迹并提供针对性的纠正指导。

Result: 在10,000多个真实代理轨迹上评估，Wink成功解决了90%需要单次干预的不当行为；生产环境中的A/B测试显示，Tool Call Failures、Tokens per Session和Engineer Interventions per Session显著减少。

Conclusion: 本文总结了设计和部署Wink系统的经验，提供了构建大规模弹性代理系统的挑战和见解。

Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

</details>


### [56] [What to Cut? Predicting Unnecessary Methods in Agentic Code Generation](https://arxiv.org/abs/2602.17091)
*Kan Watanabe,Tatsuya Shirai,Yutaro Kashiwa,Hajimu Iida*

Main category: cs.SE

TL;DR: 论文提出一个预测模型，帮助评审者高效识别PR中可能被删除的代码，AUC达87.1%。


<details>
  <summary>Details</summary>
Motivation: AI生成的代码在评审过程中有较大比例被删除，但评审者仍需检查这些代码，缺乏高效识别将被删除代码的方法。

Method: 提出一个预测模型，用于识别在PR评审中可能被删除的函数。

Result: 模型AUC达到87.1%，显示不同删除原因的函数具有明显特征。

Conclusion: 预测模型能有效帮助评审者优先处理重要代码，提高评审效率。

Abstract: Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.

</details>


### [57] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 研究通过社会技术追踪框架开发了可持续性模型和项目分类方法，能有效预测开源项目在基金会内外的可持续性，并提供了失败项目的恢复策略。


<details>
  <summary>Details</summary>
Motivation: 开源项目加入基金会（如Apache、Eclipse、OSGeo）以获取治理建议、孵化支持和社区建设机制，但基金会政策、资助模式和支持策略各异，项目需求也因生命周期阶段不同而多样，如何匹配项目与基金会并制定可持续性计划具有挑战性。

Method: 通过实证研究和定量分析，开发了基于项目社会技术追踪特征的基金会特定可持续性模型和项目分类方法。

Result: 开发的模型和分类方法能有效预测项目可持续性，且框架通用性强，可应用于GitHub上的非基金会项目。研究还通过案例分析了失败孵化项目的恢复策略。

Conclusion: 研究表明，基于社会技术追踪框架的可持续性模型和项目分类方法不仅能有效预测基金会内部项目的可持续性，还能跨基金会应用。此外，该框架的通用性使其适用于GitHub上的非基金会项目。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [58] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 提出MIAO方法量化开源软件竞争关系，预测项目终止准确率高达81%，帮助维护者理解生态系统动态。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的竞争关系对其生存影响尚不明确，存在被竞争对手超越的风险，因此需要量化这些竞争关系。

Method: 采用结构向量自回归模型和脉冲响应函数（通常用于宏观经济分析）来分析开源软件项目间的相互作用。

Result: MIAO在分析187个开源项目组时，识别因竞争影响被迫终止开发的项目准确率达81%，并支持提前一年预测项目终止的准确率达77%。

Conclusion: MIAO方法能够有效量化开源软件项目间的竞争关系，预测项目终止的准确率高达81%，为项目维护者提供了理解生态系统动态和预测项目兴衰的宝贵工具。

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [59] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 研究发现LLMs在长代码上下文中的鲁棒性不足，特别是在干扰项和无关信息下表现脆弱，为代码推理评估提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需要处理长代码上下文的软件工程任务中应用广泛，但其在不同输入条件下的鲁棒性尚不明确。

Method: 通过控制消融实验测试答案格式、干扰项和上下文规模的敏感性，扩展LongCodeBench Python数据集，并评估最先进模型在三种设置下的表现。

Result: 结果显示在打乱的多选选项和开放式问题中性能显著下降，且在存在无关信息时表现脆弱。

Conclusion: 当前的长上下文评估方法存在局限性，研究结果为评估传统和现代系统中的代码推理提供了更广泛的基准。

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [60] [The Case for HTML First Web Development](https://arxiv.org/abs/2602.17193)
*Juho Vepsäläinen*

Main category: cs.SE

TL;DR: HTML First方法通过强调HTML的优先使用，结合超媒体和服务器端逻辑，为开发者带来代码精简和维护优势，但具体益处大小及与AI驱动开发的契合度仍需探讨。


<details>
  <summary>Details</summary>
Motivation: 探讨HTML在网页开发中的重新重视，尤其是在现代网页框架盛行后，HTML First方法如何带来开发和维护上的优势。

Method: 通过htmx项目的观察和Yle网站的案例研究，采用HTML First原则进行开发。

Result: HTML First方法显著减少了代码库规模，并因概念简化而带来了开发和维护上的好处。

Conclusion: HTML First 方法为网页开发者提供了明显优势，尤其在代码库精简和维护方面，但仍存在关于其益处大小以及与AI驱动网页开发趋势的契合度等开放性问题。

Abstract: Since its introduction in the early 90s, the web has become the largest application platform available globally. HyperText Markup Language (HTML) has been an essential part of the web since the beginning, as it allows defining webpages in a tree-like manner, including semantics and content. Although the web was never meant to be an application platform, it evolved as such, especially since the early 2000s, as web application frameworks became available. While the emergence of frameworks made it easier than ever to develop complex applications, it also put HTML on the back burner. As web standards caught up, especially with milestones such as HTML5, the gap between the web platform and frameworks was reduced. HTML First development emphasizes this shift and puts focus on literally using HTML first when possible, while encouraging minimalism familiar from the early days of the web. It seems HTML-oriented web development can provide clear benefits to developers, especially when it is combined with comple- mentary approaches, such as embracing hypermedia and moving a large part of application logic to the server side. In the context of the htmx project, it was observed that moving towards HTML can reduce the size of a codebase greatly while leading to maintenance and development benefits due to the increased conceptual simplicity. Holotype-based comparisons for content-oriented websites show performance benefits, and the same observation was confirmed by a small case study where the Yle website was converted to follow HTML First principles. In short, the HTML First approach seems to have clear advantages for web developers, while there are open questions related to the magnitude of the benefits and the alignment with the recent trend of AI-driven web development.

</details>


### [61] [Disjunction Composition of BDD Transition Systems for Model-Based Testing](https://arxiv.org/abs/2602.17237)
*Tannaz Zameni,Petra van den Bos,Arend Rensink*

Main category: cs.SE

TL;DR: 本文提出了一种BDD中基于模型的测试生成方法，通过析取组合结合过渡系统，证明了其在保持测试能力的同时有效集成行为，并通过案例验证。


<details>
  <summary>Details</summary>
Motivation: 为了在行为驱动开发（BDD）中更有效地建模和测试集成行为，同时确保原始场景的测试能力不被削弱。

Method: 采用基于模型的测试生成方法，将BDD文本场景转换为过渡系统，并引入析取组合来结合代表替代系统行为的BDD过渡系统。

Result: 通过符号语义证明了析取组合能保证两个BDD过渡系统在测试失败案例上的一致性，工业案例研究验证了其实际应用价值。

Conclusion: 本文通过形式化定义析取组合，证明了其在保持原始场景测试能力的同时，能够有效建模和测试集成行为，并通过工业案例研究展示了其潜力。

Abstract: We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study.

</details>


### [62] [Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells](https://arxiv.org/abs/2602.17320)
*Stefano Lambiase,Manuel De Stefano,Fabio Palomba,Filomena Ferrucci,Andrea De Lucia*

Main category: cs.SE

TL;DR: 该研究首次分析量子开源社区的社会技术健康状况，旨在减轻社区异味风险，促进可持续发展。


<details>
  <summary>Details</summary>
Motivation: 量子开源社区面临社会技术挑战，如社区异味，但缺乏相关研究。本研究旨在填补这一空白。

Method: 通过横断面研究分析量子开源社区的社会技术健康状况。

Result: 研究发现社会技术因素对量子开源社区的产品质量和社区健康有显著影响。

Conclusion: 该研究为量子开源社区的长期可持续发展提供了基础，通过分析社会技术动态，旨在减轻社区异味带来的风险。

Abstract: Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.

</details>


### [63] [Computer-Using World Model](https://arxiv.org/abs/2602.17365)
*Yiming Guan,Rui Yu,John Zhang,Lu Wang,Chaoyun Zhang,Liqun Li,Bo Qiao,Si Qin,He Huang,Fangkai Yang,Pu Zhao,Lukas Wutschitz,Samuel Kessler,Huseyin A Inan,Robert Sim,Saravan Rajmohan,Qingwei Lin,Dongmei Zhang*

Main category: cs.SE

TL;DR: CUWM是一种桌面软件世界模型，通过两阶段分解和强化学习优化代理的决策和执行效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂的软件环境中，代理需要预测其操作的后果，以避免因单一错误操作而中断长期工作流程。由于真实执行不支持反事实探索，大规模试错学习和规划不切实际。

Method: CUWM采用两阶段分解方法：首先预测与代理相关的状态变化的文本描述，然后通过视觉合成生成下一个屏幕截图。模型基于离线UI转换数据训练，并通过轻量级强化学习阶段进一步优化。

Result: 通过测试时动作搜索评估，CUWM在多个Office任务中显著提升了决策质量和执行鲁棒性。

Conclusion: CUWM通过两阶段分解和轻量级强化学习阶段，显著提升了代理在桌面软件环境中的决策质量和执行鲁棒性。

Abstract: Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.

</details>


### [64] [The Runtime Dimension of Ethics in Self-Adaptive Systems](https://arxiv.org/abs/2602.17426)
*Marco Autili,Gianluca Filippone,Mashal Afzal Memon,Patrizio Pelliccione*

Main category: cs.SE

TL;DR: 本文探讨了自适应性系统在动态环境中进行运行时伦理推理的必要性，提出了处理伦理多样性、冲突和谈判的关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将伦理编码为固定的、基于规则的约束或在设计时嵌入的单一伦理理论，忽视了人类与系统交互环境中伦理偏好的多样性和动态性。

Method: 通过识别伦理不确定性、伦理价值冲突（包括人类、社会和环境驱动因素）以及多维度/多方/多驱动因素的谈判等关键挑战，提出了伦理自适应性系统的研究方向。

Result: 提出了将伦理偏好视为运行时需求的观点，并强调了基于伦理的明确谈判在管理多人类交互中的伦理权衡中的重要性。

Conclusion: 本文主张从静态伦理规则转向运行时伦理推理，以满足自适应性系统在动态环境中的伦理需求，并提出了实现这一目标的研究方向和关键问题。

Abstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems.

</details>


### [65] [Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering](https://arxiv.org/abs/2602.17498)
*Julian Frattini,Quim Motger*

Main category: cs.SE

TL;DR: 本文提出将NLP4RE工具从单一架构转变为模块化生态系统，通过SRA和焦点小组会议确定了36个关键需求，以提升工具的开发和维护效率。


<details>
  <summary>Details</summary>
Motivation: 当前NLP4RE工具缺乏互操作性和维护，导致开发效率低下、工具比较和基准测试困难、文档复杂化，以及长期可持续性问题。

Method: 采用标准方法论框架开发软件参考架构（SRA），并通过利益相关者驱动的焦点小组会议获取36个关键系统需求。

Result: 提出了一个愿景和研究路线图，并初步贡献了36个关键系统需求，为NLP4RE工具的改进奠定了基础。

Conclusion: 本文提出了一个愿景和研究路线图，旨在从单一的自然语言处理（NLP）工具转变为可重用、互操作的模块生态系统，以提高NLP4RE工具的开发和长期维护效率。

Abstract: Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [66] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: 本文证明了仅使用读/写操作也能实现RMWable快照，并提出了两种具体算法。


<details>
  <summary>Details</summary>
Motivation: 探索在仅使用读/写操作（比compare&swap等弱）的情况下，是否可以实现RMWable快照。

Method: 提出了两种算法：第一种适用于进程数量有限且已知的标准模型；第二种适用于进程数量无限但执行时有限的变体模型。

Result: 成功设计出两种仅依赖读/写操作的RMWable快照算法。

Conclusion: 本文提出了两种仅使用读/写操作的RMWable快照算法，分别在标准并发共享内存模型和无限并发模型下实现，证明了RMWable快照在弱操作下的可行性。

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [67] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: Fed-PLoRA是一种轻量级异构联邦微调框架，通过PLoRA和Select-N-Fold策略优化资源分配，提升LLM在联邦学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因客户端资源异构导致的不同LoRA秩初始化与聚合噪声问题。

Method: 提出Fed-PLoRA框架，包含Parallel One-Rank Adaptation (PLoRA)模块和Select-N-Fold策略，以适配异构客户端资源。

Result: 在多样化LLM微调任务中，Fed-PLoRA在准确性和效率上均优于现有方法。

Conclusion: Fed-PLoRA通过引入PLoRA和Select-N-Fold策略，有效解决了异构资源下的初始化与聚合噪声问题，显著提升了联邦学习中LLM微调的准确性和效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [68] [Trivance: Latency-Optimal AllReduce by Shortcutting Multiport Networks](https://arxiv.org/abs/2602.17254)
*Anton Juerss,Vamsi Addanki,Stefan Schmid*

Main category: cs.DC

TL;DR: Trivance是一种新型AllReduce算法，通过优化通信步骤和距离，在保持延迟最优的同时减少拥塞，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: AllReduce是分布式计算中的基本集体操作，但其完成时间受通信步骤和通信距离的影响，现有算法在延迟最优性和带宽最优性之间存在权衡。

Method: Trivance利用双向环的两个传输端口在每个步骤中同时增加通信距离，并通过联合减少步骤和网络拥塞来优化性能。

Result: 实验评估表明，Trivance在消息大小达8MiB、高带宽设置达32MiB以及3D环面网络达128MiB的情况下，性能优于现有算法5-30%。

Conclusion: Trivance是一种新型的AllReduce算法，在保持带宽最优性的同时，显著减少了网络拥塞，并在多维环面网络中保持了其延迟优势。

Abstract: AllReduce is a fundamental collective operation in distributed computing and a key performance bottleneck for large-scale training and inference. Its completion time is determined by the number of communication steps, which dominates latency-sensitive workloads, and the communication distance affecting both latency- and bandwidth-bound regimes. Direct-connect topologies, such as torus networks used in Google's TPUv4, are particularly prone to large communication distances due to limited bisection bandwidth. Latency-optimal algorithms such as Bruck's complete AllReduce in $\log_3 n$ steps on a bidirectional ring, but incur large communication distances that result in substantial congestion. In contrast, recent approaches such as Swing reduce communication distance and congestion, but are inherently required to perform $\log_2 n$ steps to complete AllReduce, sacrificing latency-optimality.
  In this paper, we present Trivance, a novel AllReduce algorithm that completes within $\log_3 n$ steps, while reducing congestion compared to Bruck's algorithm by a factor of three and preserving bandwidth-optimality. Trivance exploits both transmission ports of a bidirectional ring within each step to triple the communication distance along both directions simultaneously. Furthermore, by performing joint reductions, Trivance improves both the number of steps and network congestion. We further show that Trivance extends naturally to multidimensional torus networks, retaining its latency advantage while achieving performance comparable to bandwidth-optimal algorithms for large messages.
  Our empirical evaluation shows that Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8\,MiB, in high-bandwidth settings up to 32MiB and for 3D tori up to 128MiB. Throughout the evaluation, Trivance remains the best-performing latency-optimal algorithm.

</details>


### [69] [Visual Insights into Agentic Optimization of Pervasive Stream Processing Services](https://arxiv.org/abs/2602.17282)
*Boris Sedlak,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 论文提出了一种边缘设备上流处理服务的动态扩展方法，通过平台和智能体解决资源波动和竞争问题。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上流处理服务面临的资源波动、个性化扩展策略需求以及服务间资源竞争的问题。

Method: 通过开发一个上下文感知的自动扩展平台，并连接一个智能体来探索和优化每个服务的操作空间。

Result: 实现了一个允许开发者监控和调整服务执行的平台，并通过智能体优化服务执行。

Conclusion: 该论文提出了一个平台和智能体，用于在边缘设备上动态调整流处理服务的执行，以应对资源波动和应用需求变化，同时避免服务间的资源竞争。

Abstract: Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository.

</details>


### [70] [Informative Trains: A Memory-Efficient Journey to a Self-Stabilizing Leader Election Algorithm in Anonymous Graphs](https://arxiv.org/abs/2602.17541)
*Lelia Blin,Sylvain Gay,Isabella Ziccardi*

Main category: cs.DC

TL;DR: 提出了一种概率性自稳定领导者选举算法，适用于任意匿名网络，每个节点仅需$O(\log \log n)$位内存，系统几乎肯定会收敛并在多项式轮数内稳定。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案在一般最坏情况图下需要$Ω(\log n)$位内存，或仅在受限网络拓扑（如环、树或有界度图）下实现低状态复杂度。本文旨在设计一种适用于任意匿名网络的低内存复杂度算法。

Method: 算法在状态模型下运行，利用全局参数$N = Θ(\log n)$，通过概率性方法实现自稳定领导者选举。算法在收敛后继续传输信息，不验证静默性，且不提供显式终止检测。

Result: 算法使用$O(\log \log n)$位内存，系统几乎肯定会收敛到唯一领导者配置，并在$O(\mathrm{poly}(n))$轮内稳定。

Conclusion: 本文提出了一种概率性的自稳定领导者选举算法，适用于任意匿名网络，每个节点仅需使用$O(\log \log n)$位内存。该算法在同步调度器下运行，并假设已知全局参数$N = Θ(\log n)$。系统几乎肯定会收敛到一个稳定配置，且在高概率下在$O(\mathrm{poly}(n))$轮内稳定。

Abstract: We study the self-stabilizing leader election problem in anonymous $n$-nodes networks. Achieving self-stabilization with low space memory complexity is particularly challenging, and designing space-optimal leader election algorithms remains an open problem for general graphs. In deterministic settings, it is known that $Ω(\log \log n)$ bits of memory per node are necessary [Blin et al., Disc. Math. \& Theor. Comput. Sci., 2023], while in probabilistic settings the same lower bound holds for some values of $n$, but only for an unfair scheduler [Beauquier et al., PODC 1999]. Several deterministic and probabilistic protocols have been proposed in models ranging from the state model to the population protocols. However, to the best of our knowledge, existing solutions either require $Ω(\log n)$ bits of memory per node for general worst case graphs, or achieve low state complexity only under restricted network topologies such as rings, trees, or bounded-degree graphs.
  In this paper, we present a probabilistic self-stabilizing leader election algorithm for arbitrary anonymous networks that uses $O(\log \log n)$ bits of memory per node. Our algorithm operates in the state model under a synchronous scheduler and assumes knowledge of a global parameter $N = Θ(\log n)$. We show that, under our protocol, the system converges almost surely to a stable configuration with a unique leader and stabilizes within $O(\mathrm{poly}(n))$ rounds with high probability. To achieve $O(\log \log n)$ bits of memory, our algorithm keeps transmitting information after convergence, i.e. it does not verify the silence property. Moreover, like most works in the field, our algorithm does not provide explicit termination detection (i.e., nodes do not detect when the algorithm has converged).

</details>


### [71] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: 该论文研究了HPC集群中资源弹性的优势，通过模拟和评估多种调度策略，证明了可塑性作业能显著提升系统效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 优化高性能计算（HPC）集群的资源利用对于最大化系统效率和用户满意度至关重要，但传统的刚性作业调度往往导致资源利用不足和作业等待时间增加。

Method: 使用来自Cori、Eagle和Theta超级计算机的真实工作负载跟踪，通过ElastiSim软件模拟不同比例（0-100%）的可塑性作业，评估了五种作业调度策略，包括一种新颖的策略，即在可能时保持可塑性作业的首选资源分配。

Result: 结果显示，与完全刚性工作负载相比，可塑性作业在所有关键指标上均有显著改善。最佳调度策略下，作业周转时间减少37-67%，作业完成时间减少16-65%，作业等待时间减少73-99%，节点利用率提高5-52%。

Conclusion: 该研究强调了资源弹性在高性能计算（HPC）集群中的潜力，展示了即使有限采用也能带来显著优势，鼓励将其整合到HPC资源管理中。

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [72] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: TopoSZp 是一种高效拓扑感知压缩器，显著提升关键点保留和速度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模 HPC 模拟产生的海量数据需要误差有界的有损压缩，但现有压缩器（如 SZ 和 ZFP）虽然提供强数值误差保证，却无法保留对科学分析至关重要的拓扑结构（如极值点和鞍点）。

Method: TopoSZp 基于高吞吐量的 SZp 压缩器，集成了高效的关键点检测、局部顺序保留和针对性的鞍点优化，所有操作均在宽松但严格执行的误差范围内进行。

Result: 实验结果表明，TopoSZp 在真实科学数据集上实现了比现有拓扑感知压缩器少 3 到 100 倍的非保留关键点，无假阳性或错误关键点类型，压缩和解压速度分别快 100 到 10000 倍和 10 到 500 倍，同时保持竞争力的压缩比。

Conclusion: TopoSZp 是一种轻量级、拓扑感知的误差控制有损压缩器，能够在保持高压缩和解压性能的同时，保留关键点及其关系。

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


### [73] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: 研究评估了DAOS和Ceph对象存储系统在NWP和HPC/AI应用中的性能，发现DAOS在可扩展性和灵活性上优于Ceph和Lustre，有望在未来HPC中心广泛采用。


<details>
  <summary>Details</summary>
Motivation: 科学和工业需求推动HPC和AI应用（如NWP）需要更快处理海量数据，而传统POSIX文件系统在大规模I/O工作负载中存在性能限制，需探索新存储解决方案。

Method: 开发了新的软件适配器，使ECMWF的NWP能够利用DAOS和Ceph，并在多个计算机系统上进行了广泛的I/O基准测试，与相同硬件上的Lustre文件系统进行性能对比。

Result: DAOS和Ceph均表现优异，但DAOS在可扩展性和灵活性上显著优于Ceph和Lustre，适合大规模I/O操作。

Conclusion: DAOS和Ceph均表现出色，但DAOS在可扩展性和灵活性方面优于Ceph和Lustre，为大规模I/O操作提供了更优解决方案，预示着对象存储（尤其是DAOS）在未来HPC中心的广泛应用前景。

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [74] [HyRA: A Hybrid Resource Allocation Framework for RAN Slicing](https://arxiv.org/abs/2602.16952)
*Mohammad Zangooei,Bo Sun,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: HyRA是一种混合RAN切片资源分配框架，结合专用和共享资源池，在保证性能隔离的同时提高效率，仿真显示可节省50-75%频谱。


<details>
  <summary>Details</summary>
Motivation: 5G和6G网络对RAN资源管理的灵活性和效率提出了更高要求，现有基于每切片资源预留的框架在突发流量下效率低下。

Method: HyRA采用双层随机优化问题设计，外层确定专用和共享资源预算，内层通过新型水位填充方法进行每用户调度，并使用样本平均近似、KKT条件和Big-M编码将问题转化为可处理的混合整数规划。

Result: 仿真显示，HyRA在多样化需求模式、SLA配置和流量突发性下，相比纯专用或纯共享基线可实现50-75%的频谱节省。

Conclusion: HyRA框架通过结合专用和共享资源分配，在保证性能隔离的同时显著提高了资源效率，为未来移动网络的RAN切片提供了一种可行的解决方案。

Abstract: The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks.

</details>


### [75] [Robust and Extensible Measurement of Broadband Plans with BQT+](https://arxiv.org/abs/2602.16969)
*Laasya Koduru,Sylee Beltiukov,Alexander Nguyen,Eugene Vuong,Jaber Daneshamooz,Tejas Narechania,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: BQT+ 是一个宽带计划测量框架，通过声明式规范和NFA建模满足BEAD计划评估需求，成功应用于政策研究。


<details>
  <summary>Details</summary>
Motivation: 评估宽带基础设施投资（如BEAD计划）需要独立的、街道地址级别的宽带数据，现有系统无法满足对频繁界面演化、跨数百提供商的可扩展性以及非专家用户低技术门槛的三个基本要求。

Method: BQT+ 采用声明式状态/动作规范，将查询意图建模为交互状态空间，形式化为抽象非确定性有限自动机（NFA），并在运行时选择执行路径以适应不同的交互流程和局部界面变化。

Result: BQT+ 能够持续监测64家ISP，支持对超过100家ISP的查询，并成功应用于两项政策研究：构建BEAD预拨款基线和在四个州的124,000多个地址上评估宽带可负担性。

Conclusion: BQT+ 是一个有效的宽带计划测量框架，能够满足对互联网基础设施投资评估的需求，特别是在BEAD计划中。

Abstract: Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.
  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states.

</details>


### [76] [RIS Control through the Lens of Stochastic Network Calculus: An O-RAN Framework for Delay-Sensitive 6G Applications](https://arxiv.org/abs/2602.17198)
*Oscar Adamuz-Hinojosa,Lanfranco Zanzi,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.NI

TL;DR: DARIO是一个动态RIS分配框架，通过SNC模型和NIP优化，显著降低上行延迟，满足6G网络的异构需求。


<details>
  <summary>Details</summary>
Motivation: 现有RIS控制方案对快速变化的网络条件响应不足，限制了其在超可靠低延迟通信中的应用。

Method: 提出了一种基于SNC模型的非线性整数规划（NIP）方法，并通过在线启发式算法实现低计算开销的近优性能。

Result: 仿真和真实流量跟踪评估显示，DARIO在高负载或RIS可用性下可实现高达95.7%的延迟降低。

Conclusion: DARIO框架通过动态RIS分配和SNC模型，显著降低了多RIS场景下的上行延迟，满足了异构用户的延迟和可靠性需求。

Abstract: Reconfigurable Intelligent Surfaces (RIS) enable dynamic electromagnetic control for 6G networks, but existing control schemes lack responsiveness to fast-varying network conditions, limiting their applicability for ultra-reliable low latency communications. This work addresses uplink delay minimization in multi-RIS scenarios with heterogeneous per-user latency and reliability demands. We propose Delay-Aware RIS Orchestrator (DARIO), an O-RAN-compliant framework that dynamically assigns RIS devices to users within short time windows, adapting to traffic fluctuations to meet per-user delay and reliability targets. DARIO relies on a novel Stochastic Network Calculus (SNC) model to analytically estimate the delay bound for each possible user-RIS assignment under specific traffic and service dynamics. These estimations are used by DARIO to formulate a Nonlinear Integer Program (NIP), for which an online heuristic provides near-optimal performance with low computational overhead. Extensive evaluations with simulations and real traffic traces show consistent delay reductions up to 95.7% under high load or RIS availability.

</details>


### [77] [Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare](https://arxiv.org/abs/2602.17209)
*Alejandro Flores,Danial Shafaie,Konstantinos Ntontin,Elli Kartsakli,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 分层非地面网络架构优化远程医疗任务计算，通过HAPS和LEO卫星实现高效资源分配和延迟控制。


<details>
  <summary>Details</summary>
Motivation: 针对远程医疗设施或医疗物联网设备任务计算的延迟和资源分配问题，提出分层网络架构以提高计算效率和资源利用率。

Method: 研究采用高海拔平台站（HAPS）和低地球轨道（LEO）卫星作为边缘计算和远程云计算的桥梁，通过博弈论方法优化各层级的任务成本和带宽分配。

Result: 通过优化带宽分配和任务卸载策略，实现了HAPS和云服务器对地面用户的高效服务，同时最大化各层级的效用。

Conclusion: 本文提出了一种分层非地面网络架构，用于远程医疗设备和物联网医疗设备的任务计算，通过优化带宽分配和任务卸载策略，实现了各层级效用的最大化。

Abstract: In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.

</details>


### [78] [End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2602.17381)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.NI

TL;DR: 本文提出了一种测量CAVs远程操作中M2M、G2G和E2E延迟的新框架，发现M2M延迟占E2E延迟的60%。


<details>
  <summary>Details</summary>
Motivation: 现有延迟评估方法主要关注G2G延迟，忽略了M2M延迟对整体E2E延迟的贡献。本文旨在填补这一空白。

Method: 使用陀螺仪、光电晶体管和两个GPS同步的Raspberry Pi 5单元，结合低通滤波和阈值检测技术，识别远程操作端和车辆端的转向动作。

Result: 初步测量显示，在商用4G和5G网络上，E2E延迟平均约为500毫秒（测量精度+/-4毫秒），其中M2M延迟占比高达60%。

Conclusion: 本文提出了一个测量框架，能够量化M2M、G2G和E2E延迟，为远程操作CAVs的系统性能评估提供了新方法。

Abstract: Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera's Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value.

</details>


### [79] [Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks](https://arxiv.org/abs/2602.17394)
*Nuno Saavedra,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: SIREN是一个AI驱动的框架，通过整合ASR、LLM和NLP技术，将紧急语音通信转换为结构化信息，支持无人机辅助网络的语音驱动管理。


<details>
  <summary>Details</summary>
Motivation: 在紧急响应场景中，语音通信因其鲁棒性仍是关键，但其非结构化特性阻碍了与无人机辅助网络管理的直接集成。

Method: SIREN框架整合了自动语音识别（ASR）、基于大型语言模型（LLM）的语义提取和自然语言处理（NLP）验证，将紧急语音通信转换为结构化、机器可读的信息。

Result: 实验结果表明，SIREN在多样化操作条件下实现了稳健的转录和可靠的语义提取，但说话人分离和地理模糊性仍是主要限制因素。

Conclusion: 研究证实了SIREN框架在无人机辅助网络中实现语音驱动情境感知的可行性，为人机协同决策支持和自适应网络管理提供了实践基础。

Abstract: Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.

</details>


### [80] [ACOS: Arrays of Cheap Optical Switches](https://arxiv.org/abs/2602.17449)
*Daniel Amir,Ori Cohen,Jakob Krebs,Mark Silberstein*

Main category: cs.NI

TL;DR: ACOS利用低基数OCS构建可扩展、经济的ML训练网络，性能媲美包交换网络，成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统高基数OCS成本高且重新配置慢，限制了可扩展性和性能，需要更经济高效的解决方案。

Method: 采用低基数OCS构建ACOS，支持拓扑选择、工作负载适应和故障恢复等重新配置形式。

Result: ACOS在训练大规模LLM时性能与全配置包交换网络相当，且成本显著降低。

Conclusion: ACOS通过使用低基数OCS作为构建块，打破了当前专用ML网络的可扩展性障碍，实现了与全配置包交换网络相当的性能，同时显著降低了成本。

Abstract: Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.
  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future.

</details>


### [81] [HAP Networks for the Future: Applications in Sensing, Computing, and Communication](https://arxiv.org/abs/2602.17534)
*Sultan Çoğay,T. Tolga Sari,Muhammad Nadeem Ali,Byung-Seo Kim,Gökhan Seçinti*

Main category: cs.NI

TL;DR: 本文综述了HAPs在先进空中通信、集成感知和空中信息学中的应用，评估了当前状态并展望了未来方向。


<details>
  <summary>Details</summary>
Motivation: HAPs作为非地面网络的重要进展，提供了广泛的覆盖和独特能力，是卫星系统与地面网络之间的关键纽带。

Method: 通过审查数据处理、网络性能、计算与存储需求、经济可行性和监管挑战，评估了HAP中心应用的当前状态。

Result: 分析突出了HAPs在全球通信中的演变角色，并确定了支持其部署的未来研究方向。

Conclusion: HAPs在下一代通信技术中扮演关键角色，本研究总结了其应用现状并指出了未来研究方向。

Abstract: High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment.

</details>


### [82] [EDRP: Enhanced Dynamic Relay Point Protocol for Data Dissemination in Multi-hop Wireless IoT Networks](https://arxiv.org/abs/2602.17619)
*Jothi Prasanna Shanmuga Sundaram,Magzhan Gabidolla,Luis Fujarte,Shawn Duong,Jianlin Guo,Toshiaki Koike-Akino,Pu,Wang,Kieran Parsons,Philip V. Orlik,Takenori Sumi,Yukimasa Nagai,Miguel A. Carreira-Perpinan,Alberto E. Cerpa*

Main category: cs.NI

TL;DR: EDRP通过动态调整退避延迟和机器学习优化块大小，解决了DRP在链路波动下的性能问题，提升吞吐量39.43%。


<details>
  <summary>Details</summary>
Motivation: DRP协议在真实链路质量波动下表现不佳，导致多发送者传输重叠，降低吞吐量。

Method: 设计了LQ-CSMA动态调整退避延迟范围，并开发了ML-BSS算法预测链路质量以优化块大小选择。

Result: EDRP在实地评估中平均吞吐量比竞争协议提高了39.43%。

Conclusion: EDRP通过整合LQ-CSMA和ML-BSS算法，显著提升了在真实链路质量波动下的性能，平均吞吐量提高了39.43%。

Abstract: Emerging IoT applications are transitioning from battery-powered to grid-powered nodes. DRP, a contention-based data dissemination protocol, was developed for these applications. Traditional contention-based protocols resolve collisions through control packet exchanges, significantly reducing goodput. DRP mitigates this issue by employing a distributed delay timer mechanism that assigns transmission-start delays based on the average link quality between a sender and its children, prioritizing highly connected nodes for early transmission. However, our in-field experiments reveal that DRP is unable to accommodate real-world link quality fluctuations, leading to overlapping transmissions from multiple senders. This overlap triggers CSMA's random back-off delays, ultimately degrading the goodput performance.
  To address these shortcomings, we first conduct a theoretical analysis that characterizes the design requirements induced by real-world link quality fluctuations and DRP's passive acknowledgments. Guided by this analysis, we design EDRP, which integrates two novel components: (i) Link-Quality Aware CSMA (LQ-CSMA) and (ii) a Machine Learning-based Block Size Selection (ML-BSS) algorithm for rateless codes. LQ-CSMA dynamically restricts the back-off delay range based on real-time link quality estimates, ensuring that nodes with stronger connectivity experience shorter delays. ML-BSS algorithm predicts future link quality conditions and optimally adjusts the block size for rateless coding, reducing overhead and enhancing goodput. In-field evaluations of EDRP demonstrate an average goodput improvement of 39.43\% than the competing protocols.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: A control method using ICP algorithm for real-time fork alignment on inclined surfaces, verified by simulations and experiments, prevents pallet dragging during unloading.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unloading pallets on inclined surfaces without dragging them, which is crucial for efficient and damage-free operations.

Method: The method uses the Iterative Closest Point (ICP) algorithm on point clouds from the pallet's upper region to track and align the fork's position and attitude angle in real-time during unloading.

Result: The method successfully aligns the fork parallel to the target surface, allowing smooth withdrawal along the tilt, preventing pallet dragging.

Conclusion: The proposed control method effectively enables autonomous forklifts to unload pallets on inclined surfaces without dragging, as verified by simulations and real-world experiments.

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [84] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: 提出一种结合B样条和四元数插值的平滑轨迹生成方法，通过Bezier曲线和顺序二次规划优化，显著提升机器人轨迹精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统插值方法在四自由度并联铣削机器人轨迹生成中精度不足、速度波动大及计算效率低的问题。

Method: 整合B样条和四元数插值技术，利用平滑分段Bezier曲线同步位置和方向数据，通过顺序二次规划解决非线性关系，并采用最小急动时间最优Bezier曲线优化时间轨迹。

Result: 实验结果表明，相比传统方法，该方法在精度、速度稳定性和计算效率方面均有显著提升。

Conclusion: 该方法通过B样条和四元数插值技术，结合平滑分段Bezier曲线和顺序二次规划，实现了四自由度并联铣削机器人的平滑轨迹生成，显著提升了精度和计算效率。

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [85] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: RRT$^η$结合AGM鲁棒性度量，优化STL任务规划，显著提升多约束场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于STL的运动规划方法依赖非平滑的min-max鲁棒性度量，导致优化效率低下，无法有效探索高维配置空间。

Method: 提出了RRT$^η$框架，结合AGM鲁棒性度量、高效的增量监控算法和基于FPL的满意度提升向量。

Result: 在双积分器点机器人、独轮移动机器人和7自由度机械臂上验证了RRT$^η$在多约束场景中的性能优势。

Conclusion: RRT$^η$框架通过集成AGM鲁棒性度量，显著提升了在复杂时空约束下的运动规划性能，并在多个机器人系统上验证了其优越性。

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [86] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: Sound of Touch 是一种基于弦振动的触觉传感方法，通过音频信号实现接触定位、力估计和滑动检测，适用于大面积机器人表面。


<details>
  <summary>Details</summary>
Motivation: 分布式触觉传感在大面积扩展上仍面临挑战：密集传感器阵列增加布线、成本和脆弱性，而许多替代方案提供有限的覆盖范围或错过快速交互动态。

Method: 使用振动张力弦作为传感元件，通过电磁连续激励弦，少量拾音器（接触麦克风）观察由接触引起的光谱变化。系统从短时音频信号中估计接触位置、法向力并检测滑动。

Result: 实验证明毫米级定位、可靠的力估计和实时滑动检测。

Conclusion: Sound of Touch 提供了一种轻量级、可扩展的基于弦的触觉传感硬件概念，适用于机器人表面的扩展，同时提供了物理基础的仿真工具和实时推理管道。

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [87] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: 本文提出Walk-Along with Robots（WawR）方法，弥补现有研究方法的不足，适用于动态公共空间中自主机器人的研究。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人在公共空间的普及，现有研究方法（如受控实验或Wizard of Oz技术）无法充分应对机器人动态、不可预测的操作环境，因此需要更有效的方法。

Method: 受城市研究、地理学和社会学中的公共领域民族志启发，本文提出了WawR方法，详细介绍了其关键特征、应用步骤、独特见解及评估方式。

Result: WawR方法为研究自主机器人在公共空间的行为提供了新视角，并展示了其在实践中的应用潜力。

Conclusion: 本文提出了一种名为Walk-Along with Robots（WawR）的新方法，旨在更有效地研究公共空间中自主移动机器人的行为，并希望促进关于此类研究方法的进一步讨论。

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [88] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: SimToolReal 是一种通用化模拟到现实强化学习方法，通过程序化生成工具基元和单一策略训练，实现无需特定训练的灵巧工具操作，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工具操作是机器人任务扩展的重要能力，但传统方法需要大量工程努力建模对象和调整奖励函数，SimToolReal 旨在实现通用化的模拟到现实强化学习策略。

Method: 提出 SimToolReal 方法，通过在模拟中程序化生成大量工具类物体基元，训练单一强化学习策略，目标是随机目标姿态下操作每个物体。

Result: SimToolReal 在测试时无需对象或任务特定训练，性能优于现有重定向和固定抓取方法 37%，并与特定对象和任务训练的专家策略性能相当。在 120 次现实世界测试中，覆盖 24 任务、12 对象实例和 6 工具类别，展示了强大的零样本性能。

Conclusion: SimToolReal 通过程序化生成多样化的工具类物体基元并在模拟中训练单一强化学习策略，实现了无需特定对象或任务训练的通用灵巧工具操作能力，其性能优于现有方法，并在多种日常工具上展示了强大的零样本性能。

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [89] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT扩展了Boreas数据集，提供多传感器数据用于评估自动驾驶算法在多样化道路条件下的表现，现有算法在复杂环境中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 扩展多季节Boreas数据集至新地点，以挑战现代自动驾驶算法在多样化环境中的表现。

Method: 数据集包含60个序列，覆盖643公里驾驶里程，使用多传感器（相机、雷达、激光雷达、IMU等）采集数据，并提供厘米级地面真值。

Result: 基准测试显示，现有里程计和定位算法在简单环境中过拟合，在Boreas-RT的复杂路线上性能显著下降。

Conclusion: Boreas-RT数据集为评估多模态算法在多样化道路条件下的性能提供了统一平台，现有算法在复杂环境中表现显著下降。

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [90] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: MALLVi是一个多智能体框架，通过闭环反馈和协作提升机器人操作的鲁棒性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一模型或开放循环，缺乏环境反馈，导致动态环境下脆弱性。

Method: MALLVi采用多智能体框架（Decomposer、Localizer、Thinker、Reflector和可选的Descriptor），通过闭环反馈驱动操作，避免完全重新规划。

Result: 实验表明，MALLVi在零样本操作任务中提升了泛化能力和成功率。

Conclusion: MALLVi通过多智能体协作和闭环反馈机制，显著提高了机器人操作的泛化能力和成功率。

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [91] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: 该论文提出了一种通过图形化对象关系表示场景状态的方法，结合演示分割和池化技术，有效学习长时程操作任务，并在多环境中实现可靠执行。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从演示中学习长时程操作任务的效率问题，关注任务的目标而非具体操作方式。

Method: 提出了一种演示分割和池化方法，提取操作图并估计任务阶段中对象状态的分布，同时使用预训练视觉特征进行对象匹配以提高鲁棒性。

Result: 在实验中验证了演示分割的准确性，以及从多个演示中学习对找到最小任务模型的实用性，并在仿真和真实机器人上部署了模型。

Conclusion: 该方法通过图形化对象关系表示场景状态，并通过演示分割和池化提取操作图，能够跨环境可靠执行任务。

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [92] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: 本文通过物理基准测试评估3D重建对机器人抓取的影响，发现重建伪影减少抓取候选但准确姿态下性能影响小，空间误差主导抓取成功。


<details>
  <summary>Details</summary>
Motivation: 现代3D重建方法虽能生成视觉和几何上令人印象深刻的网格，但标准几何评估未反映重建质量对下游任务（如机器人操作性能）的影响。

Method: 通过大规模物理基准测试，评估6D姿态估计器和3D网格模型在抓取功能上的有效性，分析模型保真度的影响。

Result: 结果表明，重建伪影显著减少抓取候选姿势数量，但在准确估计姿态时对抓取性能影响有限；抓取成功与姿态误差的关系主要由空间误差主导。

Conclusion: 本研究揭示了3D重建质量对机器人抓取任务的影响，指出重建伪影会减少抓取候选姿势数量，但在准确估计姿态时对抓取性能影响有限。

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [93] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: 提出基于CFM的框架，将刚性夹具抓取姿态映射到软夹具，显著提升抓取成功率并展示可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决刚性夹具抓取合成方法在软夹具上应用时无法捕捉其独特顺应行为的问题，减少数据依赖并提高准确性。

Method: 采用条件流匹配（CFM）生成模型，结合U-Net自动编码器，从深度图像中提取物体几何信息，学习从初始Anygrasp姿态到稳定Fin-ray夹具姿态的连续映射。

Result: CFM生成的姿态在7-DOF机器人上验证，对已见和未见物体的总体成功率（34%和46%）显著高于基线刚性姿态（6%和25%），尤其在圆柱和球形物体上表现突出。

Conclusion: 本文提出了一种利用条件流匹配（CFM）生成模型将刚性夹具的抓取姿态映射到软Fin-ray夹具的新框架，显著提高了抓取成功率，并展示了该方法对其他软机器人系统的可扩展性。

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [94] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: AR framework enables teleoperation of hybrid rigid-soft robots by integrating simulation with real-world deployment, using parameter identification for accurate modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in coordinating hybrid rigid-soft robots due to difficulties in modeling, perception, and cross-domain kinematics.

Method: Developed an AR-based framework integrating a simulated model of the robotic system with a physics engine, and introduced a real-to-simulation parameter identification pipeline leveraging the soft robot's geometric properties.

Result: The framework allows users to interact with a simulated model via AR, ensuring consistent behavior between virtual and physical robots through accurate modeling and control.

Conclusion: AR-based physical human-robot interaction framework successfully enables direct teleoperation of hybrid rigid-soft robots, bridging the gap between simulation and real-world deployment through accurate modeling and parameter identification.

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [95] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: 该论文提出了一种无坐标的固定翼飞机逆飞行动力学公式，通过几何定义气动方向并推导闭合形式的轨迹到输入映射，为轨迹设计和可行性检查提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 旨在将航空中的逆仿真与机器人学中的几何建模相结合，解决固定翼飞机在SO(3)上的逆飞行动力学问题。

Method: 通过在世界坐标系中写出平移力平衡和在体坐标系中写出旋转动力学，几何定义气动方向（阻力、升力、侧向），避免局部姿态坐标。在协调飞行（无侧滑）条件下，推导出闭合形式的轨迹到输入映射，得到姿态、角速度、推力-攻角对，并逐分量恢复气动力矩系数。

Result: 应用于球形平行线上的系留飞行，获得了所需倾斜角的解析表达式，并识别出一个特定的零倾斜轨迹，其中系绳张力恰好平衡离心效应，突出了气动协调与表观重力矢量的解耦。在简单的升力/阻力定律下，最小推力攻角具有闭合形式。

Conclusion: 该论文提出了一种机器人学导向的、无坐标的固定翼飞机逆飞行动力学公式，为轨迹设计和可行性检查提供了严格的构建模块。

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [96] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种结合高保真PDE模型与降阶表示的框架，用于无人机携带柔性电缆的实时控制，仿真验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机携带柔性电缆时的实时控制问题，需要一种既能高精度建模又能高效计算的方法。

Method: 采用基于偏微分方程（PDEs）的高保真模型与适用于实时控制的降阶表示相结合的方法。PDEs通过有限差分法离散化，并利用适当正交分解提取降阶模型（ROM），保留主要变形模式的同时显著降低计算复杂度。基于此ROM，设计了非线性模型预测控制方案。

Result: 仿真结果验证了ROM的稳定性、效率和鲁棒性，以及控制器在各种操作条件下调节电缆动态的有效性。

Conclusion: 该框架实现了无人机携带悬挂柔性电缆的实时动态感知控制。

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [97] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: 提出一种多会话框架，通过拓扑感知决策机制优化建图和定位，减少误差并提升一致性，适用于重复访问环境。


<details>
  <summary>Details</summary>
Motivation: 重复访问同一环境对自主系统的建图和定位提出了重大挑战，现有方法通常贪婪地运行完整的SLAM会话并尝试在结果地图间寻找对应关系，效果不佳。

Method: 该方法基于地图定位，采用拓扑感知和不确定性感知的决策机制，分析位姿图结构以检测低连通性区域，并选择性触发建图和闭环模块。

Result: 在数据集重叠序列和真实矿山类环境中验证了该方法的有效性，展示了其在减少累积误差和增强全局一致性方面的优势。

Conclusion: 该论文提出了一种新颖的多会话框架，通过拓扑感知和不确定性感知的决策机制，有效减少了累积误差并增强了全局一致性，适用于重复访问环境中的自主系统。

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [98] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: FRAPPE通过两阶段微调策略提升世界建模效率，减少对动作标注数据的依赖，在多项测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前方法在像素级重建上过度强调和推理过程中依赖未来观测导致误差积累的问题。

Method: 采用两阶段微调策略：中期训练阶段学习预测未来观测的潜在表示；后期训练阶段并行扩展计算工作量，并与多个视觉基础模型对齐表示。

Result: 在RoboTwin基准测试和实际任务中表现优异，尤其在长周期和未见场景中展现出强泛化能力。

Conclusion: FRAPPE方法通过两阶段微调策略显著提升了世界建模的效率和泛化能力，在RoboTwin基准测试和实际任务中表现优于现有方法。

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [99] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 提出纯本体感知状态估计器，通过接触腿锚点和高度校正抑制漂移，在多种机器人上验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决无相机或LiDAR的腿式机器人里程计因IMU漂移和关节速度噪声导致的不可靠问题。

Method: 使用IMU和电机测量，通过接触腿作为运动学锚点，结合关节扭矩估计可靠接触点，利用脚部位置提供世界坐标系约束。引入高度聚类和时间衰减校正抑制高度漂移，应用逆运动学立方卡尔曼滤波器改善脚部速度观测。

Result: 在多个机器人平台上测试，水平环路误差为0.1638m至7.68m，垂直环路误差为0.1m至0.540m，验证了方法的有效性。

Conclusion: 该论文提出了一种仅依赖IMU和电机测量的纯本体感知状态估计器，适用于双足、四足和轮腿机器人，通过接触腿作为运动学锚点、高度聚类和时间衰减校正等方法，有效抑制了长期漂移，并在多个平台上验证了其性能。

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [100] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: 提出了一种基于虚拟模型控制的分散式人机协作框架，通过虚拟弹簧和阻尼器实现直观控制，有效消除死锁，实验验证了其安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决人机协作中机器人易陷入死锁的问题，同时保持操作的直观性和安全性。

Method: 采用基于虚拟弹簧和阻尼器的虚拟模型控制（VMC），结合分散式力基停滞检测器识别并解决死锁问题。

Result: 实验中将机器人陷入死锁的概率从61.2%降至零，并展示了与最多两个机器人和两个人类的安全协作，模拟中扩展到四个机器人。

Conclusion: 该框架通过虚拟模型控制实现了人机协作的无死锁操作，并在实验中验证了其可扩展性和安全性。

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [101] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: SOLen方法通过3D打印透镜和Y形波导实现软光学传感，避免了多材料界面，为软机器人提供了新的传感功能。


<details>
  <summary>Details</summary>
Motivation: 解决单材料一步制造兼容的传感需求，避免多材料界面带来的复杂性。

Method: 使用改性丙烯酸聚氨酯树脂，通过单层光学表征确定波长依赖的折射率和透射率，设计并打印具有亚毫米精度的透镜轮廓。

Result: 旋转测试显示可重复的分支选择性信号切换，验证了材料到光学的工作流程的可转移性。

Conclusion: 该研究提出了一种名为SOLen的3D打印软光学传感方法，通过透镜旋转和焦点平移实现运动方向和幅度的编码，为下一代软机器人提供了新的功能。

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [102] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: 论文提出了一种低成本硬件方案，提升自动车辆在雨天的感知性能，支持多摄像头兼容，并将行人检测准确率显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案如亲水或疏水镜头和喷雾仅提供部分缓解，而工业保护系统成本高且不适合汽车部署，因此需要一种更经济、可扩展的解决方案。

Method: 设计了一种兼容多摄像头同时使用的硬件解决方案，无需额外高成本传感器或硬件更换。

Result: 该系统将深度学习模型的行人检测准确率从8.3%提升至41.6%。

Conclusion: 该论文提出了一种成本效益高的硬件解决方案，适用于雨天条件，旨在提高自动车辆在恶劣天气下的感知性能，同时支持可持续交通目标。

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [103] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: OS-ERA通过光学传感器实现高精度弯曲状态分类，解决了ERAs的传感瓶颈，为闭环控制铺平道路。


<details>
  <summary>Details</summary>
Motivation: 传统ERAs的电容传感器精度有限，阻碍了精确控制，因此需要一种不影响驱动的可靠传感解决方案。

Method: 设计并嵌入了两个软光学波导传感器，训练分类器以区分八种弯曲状态，并在六次独立试验中验证模型。

Result: OS-ERA在所有测试中均表现出高重复性和速度不变性，分类准确率稳定，信号轨迹保持形状一致。

Conclusion: OS-ERA通过光学传感器解决了传统ERAs电容传感器精度不足的问题，实现了高保真度的弯曲状态分类，为闭环控制奠定了基础。

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [104] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: 研究发现，动力假肢膝关节的膝上放置比膝下放置更有效，能提高行走速度和步频，同时保持步态功能。优化质量分布是关键，需进一步研究验证。


<details>
  <summary>Details</summary>
Motivation: 下肢截肢影响全球数百万人，导致行动能力受损、行走速度降低以及日常和社交活动受限。动力假肢膝关节可以通过主动辅助膝关节扭矩来部分恢复行动能力，改善步态对称性、坐站转换和行走速度。然而，动力组件增加的质量可能会削弱这些好处，对步态力学产生负面影响并增加代谢成本。因此，优化质量分布而非简单地最小化总质量可能提供更有效和实用的解决方案。

Method: 在这项探索性研究中，我们评估了动力假肢膝关节膝上放置的可行性，并与膝下放置进行了比较。测试包括行走速度、步频、步态对称性、膝关节活动范围和峰值速度的测量，以及在斜坡和楼梯上的额外测试。

Result: 与膝下放置相比，膝上配置显示出改善的行走速度（一名参与者提高9.2%）和步频（提高3.6%），但对步态对称性的影响不一。运动学测量表明，两种配置的膝关节活动范围和峰值速度相似。在斜坡和楼梯上的额外测试证实了控制策略在多种运动任务中的稳健性。

Conclusion: 初步研究结果表明，膝上放置动力假肢膝关节在功能上是可行的，精心设计的质量分布可以保留动力辅助的好处，同时减轻额外重量的不利影响。需要进一步研究以确认这些趋势并为设计和临床建议提供指导。

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [105] [RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments](https://arxiv.org/abs/2602.17515)
*Ziyi Zong,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: RA-Nav是一个基于语义分割的风险感知导航框架，通过实时分类障碍物并预测风险，生成安全高效的路径，在突发障碍物移动场景中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有空中机器人导航系统无法适应静态障碍物突然移动的情况，通过引入环境语义感知来估计潜在风险，提升导航的适应性和安全性。

Method: 提出RA-Nav框架，包括轻量级多尺度语义分割网络实时识别障碍物类别，分类为静止、暂时静态和动态三类，并设计相应的风险估计函数构建局部风险地图，结合风险感知路径搜索算法和轨迹优化生成安全、平滑且动态可行的轨迹。

Result: 比较模拟显示，RA-Nav在突发障碍物状态变化场景中的成功率高于基线方法，真实数据模拟进一步验证了其有效性。

Conclusion: RA-Nav框架通过语义分割和风险预测，在突发障碍物状态变化场景中显著提高了导航成功率，并在真实数据模拟中验证了其有效性。

Abstract: Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.

</details>


### [106] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: IRIS是一个低成本、轻量化的6自由度机器人相机系统，通过模仿学习实现自主电影级运动控制，成本低于1000美元，性能优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决工业级机器人平台的高成本和操作复杂性问题，设计一个低成本、易操作的智能机器人成像系统。

Method: IRIS采用全3D打印的轻量化硬件设计，结合基于Action Chunking with Transformers (ACT)的目标条件视觉运动模仿学习框架，直接从人类示范中学习对象感知和感知平滑的相机轨迹。

Result: IRIS平台成本低于1000美元，支持1.5公斤负载，重复精度约1毫米，实验显示其能准确跟踪轨迹、可靠自主执行并泛化多种电影运动。

Conclusion: IRIS系统通过低成本、轻量化的设计和模仿学习框架，成功实现了自主、学习驱动的电影级运动控制，展示了在多样化电影运动中的泛化能力。

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [107] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: 论文提出了一个基于手势的UGV控制数据集FR-GESTURE，专为第一响应者设计，包含12个手势命令和3312个RGBD数据对，数据已公开。


<details>
  <summary>Details</summary>
Motivation: 随着灾害强度和频率的增加，第一响应者的工作变得更加困难。人工智能和机器人技术可以协助他们的操作，缓解这些困难。

Method: 研究团队设计了一套12个手势命令，基于第一响应者的现有手势和战术手势信号，并通过经验丰富的第一响应者反馈进行优化。随后进行了数据收集，共获得3312个RGBD数据对，涵盖2个视角和7个距离。

Result: 创建了FR-GESTURE数据集，并定义了评估协议，进行了基线实验，数据已公开以促进未来研究。

Conclusion: 该论文提出了一个专门为第一响应者设计的基于手势的UGV控制数据集FR-GESTURE，并进行了基线实验，为未来研究提供了公开数据。

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [108] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: 提出混合zonotopes与ADMM结合的混合系统规划框架，提升计算效率与收敛速度，实验验证于自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 解决混合整数规划在嵌入式优化规划中的计算密集性和数值敏感性挑战。

Method: 结合混合zonotopes（高级集合表示法）和新的ADMM混合整数规划启发式方法，提出了处理PWA系统可达性分析及优化规划问题的通用方法。

Result: 所提方法在内存复杂度和凸松弛紧密度上优于现有技术，ADMM启发式方法在混合zonotopes结构下实现了更高的收敛速度。

Conclusion: 本文提出了一种基于混合zonotopes和ADMM启发式方法的混合系统运动规划框架，显著提升了计算效率和收敛速度，并在自动驾驶行为与运动规划中进行了实验验证。

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [109] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow是一个无监督框架，通过OT-CFM和PCA瓶颈检测安全关键异常，显著提升自动驾驶安全验证的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Level 4自动驾驶车辆的安全验证目前受限于无法扩展检测罕见、高风险的长尾场景，传统基于规则的启发式方法无法满足需求。

Method: Deep-Flow利用OT-CFM表征专家人类驾驶行为的连续概率密度，通过PCA瓶颈将生成过程约束到低秩谱流形，确保运动平滑性，并使用Early Fusion Transformer编码器解决多模态歧义。

Result: 在Waymo Open Motion Dataset上，Deep-Flow实现了0.766的AUC-ROC，显著揭示了运动危险与语义不合规之间的根本区别。

Conclusion: Deep-Flow提供了一个数学上严谨的基础，用于定义统计安全门，实现自动驾驶车队安全部署的客观、数据驱动验证。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>


### [110] [Graph Neural Model Predictive Control for High-Dimensional Systems](https://arxiv.org/abs/2602.17601)
*Patrick Benito Eberhard,Luis Pabon,Daniele Gammelli,Hugo Buurmeijer,Amon Lahr,Mark Leone,Andrea Carron,Marco Pavone*

Main category: cs.RO

TL;DR: 结合GNN和MPC的框架实现了高维系统的实时控制，实验显示其高效性和准确性，性能优于基线63.6%。


<details>
  <summary>Details</summary>
Motivation: 高维系统（如软机器人）的控制需要既能捕捉复杂动力学又保持计算可行性的模型。

Method: 通过将系统表示为具有局部交互的图，GNN保持了稀疏性，同时采用定制化的压缩算法消除控制问题中的状态变量，确保计算高效。算法复杂度与系统节点数线性相关，并利用GPU并行化实现实时性能。

Result: 该方法在仿真和物理软机器人躯干实验中验证，可扩展到1,000个节点并以100 Hz闭环运行，硬件上实现了亚厘米精度的实时参考跟踪，性能优于基线63.6%。此外，展示了其实现有效全身避障的能力。

Conclusion: 该论文提出了一种结合图神经网络（GNN）和模型预测控制（MPC）的框架，成功实现了高维系统（如软机器人）的实时控制，并在仿真和硬件实验中验证了其高效性和准确性。

Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [111] [Offline green bin packing and its constrained variant](https://arxiv.org/abs/2602.16867)
*Mingyang Gong,Brendan Mumey*

Main category: cs.DS

TL;DR: 本文研究绿色装箱问题，提出APTAS和3/2近似算法，优化装箱数量和能耗。


<details>
  <summary>Details</summary>
Motivation: 研究绿色装箱问题（GBP）及其约束版本（CGBP），旨在最小化装箱数量和总能耗，或在能耗不超过上限的情况下最小化装箱数量。

Method: 提出了APTAS（渐进多项式时间近似方案）和3/2近似算法。

Result: 提出的算法在GBP和CGBP中均有效，3/2近似算法与经典装箱问题的下界一致。

Conclusion: 本文提出了针对绿色装箱问题（GBP）和约束绿色装箱问题（CGBP）的APTAS和3/2近似算法，其中3/2的比率与经典装箱问题的下界匹配。

Abstract: In this paper, we study the {\em green bin packing} (GBP) problem where $β\ge 0$ and $G \in [0, 1]$ are two given values as part of the input. The energy consumed by a bin is $\max \{0, β(x-G) \}$ where $x$ is the total size of the items packed into the bin. The GBP aims to pack all items into a set of unit-capacity bins so that the number of bins used plus the total energy consumption is minimized. When $β= 0$ or $G = 1$, GBP is reduced to the classic bin packing (BP) problem. In the {\em constrained green bin packing} (CGBP) problem, the objective is to minimize the number of bins used to pack all items while the total energy consumption does not exceed a given upper bound $U$. We present an APTAS and a $\frac 32$-approximation algorithm for both GBP and CGBP, where the ratio $\frac 32$ matches the lower bound of BP. Keywords: Green bin packing; constrained green bin packing; approximation scheme; offline algorithms

</details>


### [112] [Adaptive encodings for small and fast compressed suffix arrays](https://arxiv.org/abs/2602.17201)
*Diego Díaz-Domínguez,Veli Mäkinen*

Main category: cs.DS

TL;DR: VLB, a new encoding technique for BWT-based CSAs, balances space and query speed by adapting indexing to local compressibility, outperforming existing methods in efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing compressed suffix arrays (CSAs) like the r-index and its derivatives face limitations in scalability as their size grows quickly with increased variation, often forcing a tradeoff between space and query speed.

Method: The paper introduces variable-length blocking (VLB), a technique that recursively divides the BWT into blocks of at most w runs, organized into a tree. It adapts indexing information based on local compressibility, balancing space and query speed. Additionally, a sampling technique ensures correctness in backward-search states without affecting query performance.

Result: Experiments demonstrate that VLB-based techniques achieve faster query times than the r-index and sr-index, while maintaining space efficiency close to the sr-index, providing a superior space-time tradeoff compared to the move data structure.

Conclusion: VLB-based techniques outperform existing methods like the r-index and sr-index in query time while maintaining comparable space efficiency, offering a better space-time tradeoff.

Abstract: Compressed suffix arrays (CSAs) index large repetitive collections and are key in many text applications. The r-index and its derivatives combine the run-length Burrows-Wheeler Transform (BWT) with suffix array sampling to achieve space proportional to the number of equal-symbol runs in the BWT. While effective for near-identical strings, their size grows quickly as variation increases, since the number of BWT runs is sensitive to edits. Existing approaches typically trade space for query speed, or vice versa, limiting their practicality at large scale.
  We introduce variable-length blocking (VLB), an encoding technique for BWT-based CSAs that adapts the amount of indexing information to local compressibility. The BWT is recursively divided into blocks of at most w runs (a parameter) and organized into a tree. Compressible regions appear near the root and store little auxiliary data, while incompressible regions lie deeper and retain additional information to speed up access. Queries traverse a short root-to-leaf path followed by a small run scan. This strategy balances space and query speed by transferring bits saved in compressible areas to accelerate access in incompressible ones.
  Backward search relies on rank and successor queries over the BWT. We introduce a sampling technique that guarantees correctness only along valid backward-search states, reducing space without affecting query performance.
  We extend VLB to encode the subsampled r-index (sr-index). Experiments show that VLB-based techniques outperform the r-index and sr-index in query time, while retaining space close to that of the sr-index. Compared to the move data structure, VLB offers a more favorable space-time tradeoff.

</details>


### [113] [Simultaneous Blackwell Approachability and Applications to Multiclass Omniprediction](https://arxiv.org/abs/2602.17577)
*Lunjia Hu,Kevin Tian,Chutong Yang*

Main category: cs.DS

TL;DR: 本文扩展了二元全预测算法到多类设置，样本复杂性或遗憾范围为≈ε^{-(k+1)}，并设计了解决Blackwell逼近问题的框架。


<details>
  <summary>Details</summary>
Motivation: 研究多类设置中的全预测问题，其中比较器家族可能是无限的。

Method: 设计了一个解决Blackwell逼近问题的框架，其中多个集合必须通过耦合动作同时逼近。

Result: 扩展了二元全预测算法到多类设置，样本复杂性或遗憾范围为≈ε^{-(k+1)}。

Conclusion: 本文的主要结果是将二元全预测算法扩展到多类设置，证明了在统计设置中的样本复杂性或在线设置中的遗憾范围为≈ε^{-(k+1)}。

Abstract: Omniprediction is a learning problem that requires suboptimality bounds for each of a family of losses $\mathcal{L}$ against a family of comparator predictors $\mathcal{C}$. We initiate the study of omniprediction in a multiclass setting, where the comparator family $\mathcal{C}$ may be infinite. Our main result is an extension of the recent binary omniprediction algorithm of [OKK25] to the multiclass setting, with sample complexity (in statistical settings) or regret horizon (in online settings) $\approx \varepsilon^{-(k+1)}$, for $\varepsilon$-omniprediction in a $k$-class prediction problem. En route to proving this result, we design a framework of potential broader interest for solving Blackwell approachability problems where multiple sets must simultaneously be approached via coupled actions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [114] [Fuse3D: Generating 3D Assets Controlled by Multi-Image Fusion](https://arxiv.org/abs/2602.17040)
*Xuancheng Jin,Rengan Xie,Wenting Zheng,Rui Wang,Hujun Bao,Yuchi Huo*

Main category: cs.GR

TL;DR: Fuse3D提出了一种新方法，通过多图像控制生成3D资产，解决了现有方法无法多区域独立控制的问题，实现了高质量3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法仅能处理单一控制目标，无法利用多图像独立控制3D资产的不同区域，限制了应用灵活性。

Method: 提出Multi-Condition Fusion Module整合多图像区域的视觉特征；自动对齐用户选择的2D图像区域与3D区域；引入Local Attention Enhancement Strategy平衡区域特定特征融合。

Result: Fuse3D能够灵活融合多个2D图像区域为连贯的3D结构，生成高质量的3D资产。

Conclusion: Fuse3D 是首个能够从多张条件图像生成可控3D资产的方法，实验结果表明其能够灵活融合多个2D图像区域为连贯的3D结构，生成高质量的3D资产。

Abstract: Recently, generating 3D assets with the control of condition images has achieved impressive quality. However, existing 3D generation methods are limited to handling a single control objective and lack the ability to utilize multiple images to independently control different regions of a 3D asset, which hinders their flexibility in applications. We propose Fuse3D, a novel method that enables generating 3D assets under the control of multiple images, allowing for the seamless fusion of multi-level regional controls from global views to intricate local details. First, we introduce a Multi-Condition Fusion Module to integrate the visual features from multiple image regions. Then, we propose a method to automatically align user-selected 2D image regions with their associated 3D regions based on semantic cues. Finally, to resolve control conflicts and enhance local control features from multi-condition images, we introduce a Local Attention Enhancement Strategy that flexibly balances region-specific feature fusion. Overall, we introduce the first method capable of controllable 3D asset generation from multiple condition images. The experimental results indicate that Fuse3D can flexibly fuse multiple 2D image regions into coherent 3D structures, resulting in high-quality 3D assets. Code and data for this paper are at https://jinnmnm.github.io/Fuse3d.github.io/.

</details>


### [115] [InstantRetouch: Personalized Image Retouching without Test-time Fine-tuning Using an Asymmetric Auto-Encoder](https://arxiv.org/abs/2602.17044)
*Temesgen Muruts Weldengus,Binnan Liu,Fei Kou,Youwei Lyu,Jinwei Chen,Qingnan Fan,Changqing Zou*

Main category: cs.GR

TL;DR: InstantRetouch是一个无需测试时微调的个性化图像修饰框架，通过不对称自编码器和RAR技术实现高效风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化图像修饰方法需要用户特定微调或泛化能力不足的问题。

Method: 采用不对称自编码器将修饰风格编码为内容解耦的潜在表示，并提出检索增强修饰（RAR）技术，通过检索与查询图像内容最相似的参考对来聚合风格潜在表示。

Result: InstantRetouch在单参考、多参考和混合风格设置中表现出色，并能直接应用于逼真风格迁移。

Conclusion: InstantRetouch通过不对称自编码器和检索增强修饰（RAR）技术，实现了无需测试时微调的个性化图像修饰，适用于多种场景，并展示了优异的泛化能力。

Abstract: Personalized image retouching aims to adapt retouching style of individual users from reference examples, but existing methods often require user-specific fine-tuning or fail to generalize effectively. To address these challenges, we introduce $\textbf{InstantRetouch}$, a general framework for personalized image retouching that instantly adapts to user retouching styles without any test-time fine-tuning. It employs an $\textit{asymmetric auto-encoder}$ to encode the retouching style from paired examples into a content disentangled latent representation that enables faithful transfer of the retouching style to new images. To adaptively apply the encoded retouching style to new images, we further propose $\textit{retrieval-augmented retouching}$ (RAR), which retrieves and aggregates style latents from reference pairs most similar in content to the query image. With these components, $\textbf{InstantRetouch}$ enables superior and generic content-aware retouching personalization across diverse scenarios, including single-reference, multi-reference, and mixed-style setups, while also generalizing out of the box to photorealistic style transfer.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [116] [The Compute ICE-AGE: Invariant Compute Envelope under Addressable Graph Evolution](https://arxiv.org/abs/2602.16736)
*Raymond Jay Martin*

Main category: cs.OS

TL;DR: 论文提出了一种基于确定性语义状态基底的内存图系统，实验证明其在25M节点范围内实现不变计算包络（Compute ICE-AGE），计算成本仅受内存容量限制。


<details>
  <summary>Details</summary>
Motivation: 传统基于推理的AI架构通过概率重组语义状态，计算成本随标记量和上下文范围增加。本文提出一种基于持久可寻址内存图的语义连续性表示方法，以局部算子g(t)演化，工作受限于局部语义变化Delta s，与总内存基数M无关。

Method: 通过生产级C++实现，基于Bounded Local Generator Classes的数学规范，构建了一个CPU驻留的图引擎，在有限局部状态演化下运行。

Result: 在Apple M2芯片上的实验显示，遍历延迟不变（约0.25至0.32毫秒），CPU利用率稳定（基线约17.2%，Delta CPU约0至0.2%），在1M至25M节点范围内无规模相关热特征。节点密度从约1.3 KB（Float64基线）压缩至约687字节（Float32压缩），二进制内存核算下可在1 TiB内支持16亿节点。

Conclusion: 该论文展示了在确定性语义状态基底上的实证结果，证明了在内存容量而非推理限制下实现可扩展性的可行性，并定义了Compute ICE-AGE（可寻址图演化下的不变计算包络）。

Abstract: This paper presents empirical results from a production-grade C++ implementation of a deterministic semantic state substrate derived from prior formal work on Bounded Local Generator Classes (Martin, 2026). The system was mathematically specified prior to implementation and realized as a CPU-resident graph engine operating under bounded local state evolution.
  Contemporary inference-driven AI architectures reconstruct semantic state through probabilistic recomposition, producing compute cost that scales with token volume and context horizon. In contrast, the substrate described here represents semantic continuity as a persistent, addressable memory graph evolved under a time-modulated local operator g(t). Work is bounded by local semantic change Delta s, independent of total memory cardinality M.
  Empirical measurements on Apple M2-class silicon demonstrate invariant traversal latency (approximately 0.25 to 0.32 ms), stable CPU utilization (approximately 17.2 percent baseline with Delta CPU approximately 0 to 0.2 percent), and no scale-correlated thermal signature across 1M to 25M node regimes under sustained operation. Measured per-node density ranges from approximately 1.3 KB (Float64 baseline) to approximately 687 bytes (compressed Float32 accounting). Under binary memory accounting, this yields a 1.6 billion node capacity projection within a 1 TiB envelope.
  These results indicate an empirically invariant thermodynamic regime in which scaling is governed by memory capacity rather than inference-bound recomposition. The Compute ICE-AGE is defined as the Invariant Compute Envelope under Addressable Graph Evolution, and the empirical evidence presented demonstrates this regime up to 25M nodes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE 本体通过标准化框架解决了牙科年龄评估中的方法异质性和数据碎片化问题，增强了透明度和可解释性，适用于法医和司法决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前牙科年龄评估方法存在方法学异质性、数据表示碎片化以及临床、法医和法律信息系统间互操作性有限的问题，这些问题影响了透明度和可重复性，尤其是在AI方法日益普及的背景下。

Method: AIdentifyAGE 本体整合了司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和基于AI的估计方法，并基于上层和已建立的生物医学、牙科及机器学习本体进行开发。

Result: AIdentifyAGE 本体提供了一个标准化的框架，支持手动和AI辅助的法医牙科年龄评估工作流程，并实现了观察、方法、参考数据和报告结果之间的可追溯链接。

Conclusion: AIdentifyAGE 本体为法医牙科年龄评估提供了一个标准化、语义一致的框架，增强了透明度、可重复性和可解释性，为法医和司法决策支持系统奠定了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [118] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 研究测试了LLMs、RAG和GraphRAG在生成DSM上的表现，发现自动化DSM生成的潜力，并公开了代码。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs、RAG和GraphRAG在生成DSM上的潜力，以评估其在确定预定义组件间关系及识别组件及其关系的复杂任务中的表现。

Method: 研究测试了大型语言模型（LLMs）、检索增强生成（RAG）和基于图的RAG（GraphRAG）在生成设计结构矩阵（DSM）上的表现，应用于电动螺丝刀和CubeSat两个用例。

Result: 通过评估DSM的每个元素及整体架构，研究发现这些方法在自动化DSM生成上具有潜力。

Conclusion: 尽管存在设计和计算上的挑战，研究发现了自动化生成设计结构矩阵（DSM）的潜力，并公开了所有代码以便复现和领域专家的进一步反馈。

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [119] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 本文证明了上下文性是经典概率表示中单一状态重用的必然结果，揭示了其作为适应性智能的一般表示约束，并展示了非经典框架如何避免这一限制。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探讨单一状态重用（由于内存、表示或物理资源的限制）在自然和人工智能中的普遍性及其基本的表示后果，尤其是在上下文性方面的理解不足。

Method: 本文通过将上下文建模为作用于共享内部状态的干预，证明了任何经典模型在再现上下文结果统计时必须承担不可约的信息理论成本。作者提供了一个最小构造示例来明确实现这一成本并阐明其操作意义。

Result: 研究结果表明，上下文性在经典概率表示中是不可避免的，且上下文依赖性不能仅通过内部状态来调解。非经典概率框架通过放松单一全局联合概率空间的假设避免了这一障碍。

Conclusion: 本文的结论是，上下文性不仅是量子力学的特性，也是经典概率表示中单一状态重用的必然结果。这一发现揭示了上下文性作为适应性智能的一般表示约束，与物理实现无关。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [120] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache框架通过重构性缓存和轻量级解码器，高效实现了大规模人类移动模拟，性能与LLM方法相当。


<details>
  <summary>Details</summary>
Motivation: 大规模人类移动模拟在多个领域至关重要，但现有基于LLM的方法计算成本高，限制了可扩展性。

Method: 设计了一个名为MobCache的移动感知缓存框架，包括一个推理组件（将推理步骤编码为潜在空间嵌入并重用）和一个解码组件（使用轻量级解码器将潜在空间推理链转换为自然语言）。

Result: 实验表明，MobCache在多维度显著提升了效率，同时性能与最先进的LLM方法相当。

Conclusion: MobCache框架通过重构性缓存显著提升了大规模人类移动模拟的效率，同时保持了与最先进LLM方法相当的性能。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [121] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 研究分析了60个LLM基准测试的饱和现象，发现近半数饱和，专家策划的基准测试更抗饱和，隐藏测试数据无效果。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试快速饱和，无法区分最佳模型，降低其长期价值。

Method: 分析了60个大型语言模型（LLM）基准测试，沿14个属性（任务设计、数据构建和评估格式）进行特征化，并测试了五个假设以考察各属性对饱和率的影响。

Result: 揭示哪些设计选择能延长基准测试寿命，并为更持久的评估策略提供信息。

Conclusion: 研究发现近半数基准测试出现饱和现象，且饱和率随基准测试年龄增长而增加。专家策划的基准测试比众包基准测试更能抵抗饱和，而隐藏测试数据（公开与私有）无显著保护效果。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [122] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 论文通过测试简单基线在三个领域的表现，发现它们匹配或超过复杂方法，并分析了代码进化技术的不足，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 许多提出的代码进化管道表现出令人印象深刻的性能，但往往没有与更简单的基线进行比较。

Method: 测试了两个简单基线在三个领域的表现：寻找更好的数学界限、设计代理支架和机器学习竞赛。

Result: 发现简单基线在所有三个领域都匹配或超过了更复杂的方法。

Conclusion: 论文总结了代码进化技术在开发和使用中的不足，并提出了未来工作中更严格的代码进化的途径和最佳实践。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [123] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 本文改进了n维超立方体切片边数的最小超平面上界，并利用自动工具CPro1辅助构造证明。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体$Q_n$中切片所有边所需的最小超平面数$S(n)$的上界，以改进已知结果。

Method: 通过构造8个超平面切片$Q_{10}$，并借助最近引入的CPro1工具（一种结合推理LLMs和自动超参数调优的自动工具）来创建搜索算法，以发现数学构造。

Result: 证明了$S(n) \leq \lceil \frac{4n}{5} \rceil$（n非5的奇数倍时）或$\frac{4n}{5} +1$（n是5的奇数倍时），并获得了关于使用$k<n$个超平面切片$Q_n$边数的新的下界。

Conclusion: 本文证明了在n维超立方体$Q_n$中，切片所有边所需的最小超平面数$S(n)$的上界为$\lceil \frac{4n}{5} \rceil$（当n不是5的奇数倍时），或$\frac{4n}{5} +1$（当n是5的奇数倍时）。这一结果改进了Paterson在1971年提出的上界$\lceil\frac{5n}{6} \rceil$。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [124] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流程，显著提升了晶体学数据分析效率，将处理时间缩短4.6-5.0倍，并生成高质量验证结果。


<details>
  <summary>Details</summary>
Motivation: 大规模设施在科学产出中面临分析和报告延迟的瓶颈，特别是对于结构和磁性复杂的样品，需要迭代的减少、整合、优化和验证步骤。NeuDiff Agent旨在提高结果获取时间和分析效率。

Method: NeuDiff Agent作为一个受治理的、使用工具的AI工作流程，通过限制操作到允许列表工具、在关键工作流边界实施故障关闭验证门，并捕获完整的来源信息以供检查、审计和控制回放，执行了从仪器数据产品到验证晶体结构和发布准备CIF的既定流程。

Result: 在参考案例基准测试中，NeuDiff Agent将人工操作时间从435分钟缩短至86.5(4.7)至94.4(3.5)分钟（4.6-5.0倍速度提升），同时生成了无checkCIF级别A或B警报的验证CIF。

Conclusion: NeuDiff Agent成功地将人工操作时间从435分钟大幅缩短至86.5至94.4分钟（4.6-5.0倍速度提升），同时生成了经过验证的CIF文件，无任何checkCIF级别A或B警报，为设施晶体学中部署代理AI提供了实用路径。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [125] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: Node Learning是一种去中心化学习范式，通过边缘节点的本地学习和选择性知识交换，解决集中式智能的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 集中式智能在数据传播、延迟、能耗和对大型数据中心的依赖方面存在瓶颈，无法适应异构、移动和资源受限的环境。

Method: 论文介绍了Node Learning的概念基础，包括节点从本地数据持续学习、维护自身模型状态，并在有益时选择性交换知识。学习通过重叠和扩散传播，而非全局同步或中央聚合。

Result: Node Learning将自主和协作行为统一在一个抽象中，适应了数据、硬件、目标和连接性的异构性。

Conclusion: Node Learning提出了一种去中心化的学习范式，将智能置于边缘节点，通过选择性对等交互扩展智能，解决了集中式智能在异构、移动和资源受限环境中的瓶颈问题。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [126] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出基于序理论的统一评分框架，证明对称序评分满足规范性准则，并引入优势函数类支持群体决策。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的形式化基础，导致评分机制不够灵活和一致。本文旨在填补这一空白，提供更严谨的理论支持。

Method: 通过分析经典序在犹豫模糊元素上的表现，证明了它们不构成格结构，并基于对称序定义评分函数。进一步引入优势函数类，提供了两种具体示例（离散优势函数和相对优势函数）用于有限集。

Result: 证明了对称序定义的评分满足强单调性和Gärdenfors条件，优势函数类可用于构建模糊偏好关系并支持群体决策。

Conclusion: 本文提出了一个基于序理论的统一框架来定义犹豫模糊集的评分机制，证明了对称序定义的评分满足关键规范性准则，并引入了优势函数类用于排名犹豫模糊元素，支持群体决策。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [127] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR基准测试揭示了多语言环境下LLMs的安全漏洞，特别是在南亚语言中，合同和拼写法对模型安全性的影响显著。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）的安全对齐主要在英语和合同绑定下进行，多语言漏洞研究不足。

Method: 引入了Indic Jailbreak Robustness (IJR)基准，覆盖12种印度和南亚语言的45216个提示，分为JSON（合同绑定）和Free（自然主义）两个轨道。

Result: IJR揭示了三种模式：(1)合同增加拒绝但不阻止越狱；(2)英语到印度语的攻击转移性强；(3)拼写法重要，罗马化或混合输入降低JSR。

Conclusion: IJR提供了一个可重复的多语言压力测试，揭示了仅以英语和合同为中心评估所隐藏的风险，特别是对于经常进行代码转换和罗马化的南亚用户。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [128] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5 是一款多平台 GUI 代理模型，通过创新技术在多任务基准测试中取得领先成绩，并开源模型和提供在线演示。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个支持云边协作和实时交互的多平台 GUI 代理模型，以提升 GUI 自动化、基础任务、工具调用、记忆和知识任务等方面的性能。

Method: 论文采用了混合数据飞轮构建 UI 理解和轨迹生成的数据管道，使用统一思维合成管道增强模型推理能力，并提出新的环境 RL 算法 MRPO 以解决多平台冲突和长时任务训练效率低的问题。

Result: GUI-Owl-1.5 在多个基准测试中表现优异，如 GUI 自动化任务（OSWorld 56.5，AndroidWorld 71.6，WebArena 48.4）、基础任务（ScreenSpotPro 80.3）、工具调用任务（OSWorld-MCP 47.6，MobileWorld 46.8）以及记忆和知识任务（GUI-Knowledge Bench 75.5）。

Conclusion: GUI-Owl-1.5 是一款支持多平台（桌面、移动、浏览器等）的本地 GUI 代理模型，通过混合数据飞轮、统一增强代理能力和多平台环境 RL 扩展等创新技术，在超过 20 个 GUI 基准测试中取得了最先进的成果。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [129] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage 是首个支持LLM自动创建智能体的ADK，具有自生成拓扑、工具集和结构化内存，实验证明其优于现有ADK。


<details>
  <summary>Details</summary>
Motivation: 当前ADK在功能支持不足或依赖人工设计，限制了智能体的泛化能力和整体性能。

Method: OpenSage 是首个支持LLM自动创建具有自生成拓扑和工具集的智能体的ADK，并提供全面结构化的内存支持。

Result: 在三个最先进的基准测试中，OpenSage 展现了优于现有ADK的优势，并通过消融研究验证了各组件设计的有效性。

Conclusion: OpenSage 为下一代智能体开发铺平了道路，从以人为中心转向以AI为中心的范式。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [130] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现文本安全评估无法准确反映工具调用的安全性，需专门评估工具调用安全。


<details>
  <summary>Details</summary>
Motivation: 探讨文本层面的安全对齐是否也能抑制有害动作，填补现有安全评估的空白。

Method: 引入GAP基准，系统评估框架，测试六个前沿模型在六个受监管领域、七种越狱场景、三种系统提示条件和两种提示变体下的表现，产生17,420个分析就绪数据点。

Result: 文本安全不转移到工具调用安全，模型可能文本拒绝但工具调用执行有害动作，系统提示措辞对工具调用行为影响显著。

Conclusion: 文本层面的安全评估不足以评估代理行为，工具调用安全需要专门的测量和缓解措施。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [131] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个评估LLM代理对长视野攻击脆弱性的基准，发现现有代理高度脆弱且防御措施不足。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂环境中部署时面临长视野攻击的风险，现有防御措施在单轮交互中有效，但在多轮交互中可能失效。

Method: 提出了AgentLAB，一个专门评估LLM代理对自适应、长视野攻击的脆弱性的基准。

Result: 评估发现代表性LLM代理对长视野攻击高度脆弱，现有防御措施无法可靠缓解此类威胁。

Conclusion: AgentLAB是一个有价值的基准，用于跟踪LLM代理在实际应用中的安全进展。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [132] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI编码代理的PR描述风格差异影响人类审阅者的响应和合并结果，凸显PR呈现和互动在人机协作中的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速应用催生了AI编码代理，但其PR描述特征差异及人类审阅者的响应尚未充分研究。

Method: 使用AIDev数据集对五个AI编码代理创建的PR进行实证分析，分析PR描述特征（包括结构特征）及人类审阅者的响应（包括审阅活动、响应时间、情感和合并结果）。

Result: AI编码代理在PR描述风格上存在显著差异，且这些差异影响了审阅者的参与度、响应时间和合并结果。不同代理在审阅者互动指标和合并率上表现出显著差异。

Conclusion: 研究发现AI编码代理在PR描述风格上存在显著差异，这些差异与审阅者的参与度、响应时间和合并结果相关，强调了PR呈现和审阅者互动在人机协作软件开发中的重要性。

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [133] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace基准测试评估模型的规划、推理和世界知识能力，发现前沿模型在困难任务中表现不佳，揭示了规划和长期推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在规划、推理和世界知识方面的能力，揭示其在实际任务中的局限性。

Method: 通过评估开源和闭源模型（如Gemini-3、GPT-5和Claude Opus 4.5）在LLM-Wikirace任务中的表现，分析其规划、推理和世界知识能力。

Result: 在简单任务中，模型表现优异甚至超越人类，但在困难任务中表现显著下降，最佳模型Gemini-3的成功率仅为23%。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统的明显局限性，尤其是在规划和长期推理能力方面。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [134] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究发现微调对齐的视觉语言模型会导致广泛的不对齐，多模态评估比单模态评估更敏感。几何分析揭示有害行为集中在低维子空间，缓解策略部分有效但无法完全解决问题。


<details>
  <summary>Details</summary>
Motivation: 终身多模态代理需要通过后训练持续适应新任务，但如何在获取能力与保持安全对齐之间取得平衡是一个根本性问题。

Method: 通过实验评估LoRA秩对对齐性的影响，并采用几何分析揭示有害行为所在的低维子空间。测试了两种缓解策略：良性窄域微调和基于激活的引导。

Result: 微调会导致严重的泛化性不对齐，且多模态评估显示不对齐程度显著高于单模态评估。几何分析表明有害行为集中在低维子空间，缓解策略部分有效但无法完全消除有害行为。

Conclusion: 当前的微调后训练范式可能无法充分保持部署后的对齐性，需要更强大的持续学习框架。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [135] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态化监控框架，通过RNN捕捉多轮对话中的意图演化，显著提升安全防护性能，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM安全防护在时间感知上的不足，防止恶意意图通过多轮对话绕过无状态过滤器。

Method: 采用RNN架构，通过传播隐藏状态捕捉风险的渐进累积，替代了孤立评估模型。

Result: DeepContext在多轮越狱检测中F1分数达到0.84，显著优于现有基线，并在T4 GPU上保持低于20ms的推理开销。

Conclusion: DeepContext通过状态监控框架显著提升了多轮对话中的安全防护能力，证明了建模意图的时序演化比部署大规模无状态模型更有效且计算效率更高。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [136] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench是一个评估引用网络来源质量的基准，揭示了四个关键见解，指导生成式AI和网络搜索的未来研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地通过引用网络来源来回答问题，但现有评估侧重于答案正确性而非证据质量。

Method: 引入了SourceBench基准，使用包含八个指标的框架评估引用的网络来源质量，并包括一个人工标注的数据集和基于LLM的评估器。

Result: 评估了八个LLMs、Google搜索和三个AI搜索工具在3996个引用来源上的表现，并进行了进一步实验以理解评估结果。

Conclusion: 该论文揭示了四个关键新见解，可指导生成式人工智能和网络搜索的未来研究方向。

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [137] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov是一个离线代理学习框架，通过执行验证数据管理和策略优化，显著提升了硬件验证的覆盖率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线强化学习在硬件验证中因反馈昂贵且缓慢而不实用的问题，特别是在依赖工业模拟器和不可微分执行信号的高覆盖率硬件验证场景。

Method: 提出了LLM4Cov框架，包括执行验证数据管理、策略感知代理数据合成和最差状态优先采样，以在有限执行约束下实现可扩展学习。

Result: 使用LLM4Cov框架，一个紧凑的4B参数模型在代理评估中实现了69.2%的覆盖率通过率，比其教师模型高出5.3%，并与更大规模的模型表现相当。

Conclusion: LLM4Cov框架通过离线学习和执行验证数据管理，显著提升了硬件验证的覆盖率，证明了其在资源受限环境下的高效性和可扩展性。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [138] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom是一种自动化代理劫持框架，通过结构化模板注入提高攻击成功率，实验证明其优于现有方法，并在商业产品中发现多个漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法依赖手工制作的语义驱动提示，成功率低且难以迁移到闭源商业模型，Phantom旨在解决这一问题。

Method: Phantom采用结构化模板注入和模板自动编码器（TAE）技术，结合贝叶斯优化，高效搜索最优攻击模板。

Result: 在Qwen、GPT和Gemini上的实验表明，Phantom在攻击成功率和查询效率上显著优于现有基线。

Conclusion: Phantom框架通过结构化模板注入显著提高了攻击成功率，并在实际商业产品中发现了70多个漏洞，为下一代代理系统的安全提供了实证基础。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [139] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一种结合量子计算和经典优化的混合金融风险系统，显著提升了预测准确性、决策性能和可审计性。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险系统的两步骤流程在实际约束下可能失效，导致决策不稳定或优化缓慢，且缺乏清晰的审计追踪。HQFS旨在解决这些问题。

Method: HQFS首先使用变分量子电路（VQC）学习下一步回报和波动率代理，然后将风险-回报目标和约束转化为QUBO，并通过量子退火或经典QUBO求解器解决。最后，使用后量子签名对每次再平衡输出进行签名。

Result: 在实验中，HQFS显著降低了回报和波动率预测误差，提高了样本外夏普比率，降低了最大回撤，并缩短了求解时间。

Conclusion: HQFS是一种实用的混合流程，将预测、离散风险优化和可审计性结合在一个流程中，显著提升了金融风险系统的性能和可追溯性。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [140] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 本文证明黑盒评估AI系统安全性存在根本限制，需额外保障措施确保安全。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是揭示黑盒评估AI系统安全性的局限性，特别是在模型行为依赖于未观察到的内部变量时，这些变量在评估中罕见但在部署中普遍存在。

Method: 本文通过理论分析（如Le Cam方法和Yao的极小极大原理）和构造性证明（如基于哈希的触发器构造）来挑战黑盒评估的假设。

Result: 结果显示，任何黑盒评估器都无法可靠估计此类模型的部署风险，且存在统计和计算上的根本限制。

Conclusion: 本文的结论是黑盒测试在统计上可能无法确定AI系统的安全性，尤其是在存在潜在上下文条件策略的情况下。因此，需要额外的保障措施（如架构约束、训练时保证、可解释性和部署监控）来确保最坏情况下的安全性。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [141] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式基准测试，通过多视角参考区分行为模仿与决策质量，揭示了LLMs在金融咨询中的表现冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于金融咨询中用户行为可能受市场波动影响而具有噪声或短视性，传统推荐基准无法区分行为模仿与决策质量。

Method: 论文方法包括构建Conv-FinRe基准测试，结合真实市场数据和人类决策轨迹，通过控制对话情境评估LLMs的表现。

Result: 结果显示，基于效用的排名模型在决策质量上表现良好但难以匹配用户选择，而行为对齐模型可能过度拟合短期噪声。

Conclusion: 论文结论指出，现有推荐模型在金融咨询领域存在行为模仿与决策质量之间的冲突，Conv-FinRe基准测试通过多视角参考解决了这一问题，并揭示了理性决策与行为对齐之间的持续张力。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [142] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS框架通过神经符号方法解决了自然语言查询时间序列数据库的挑战，并提出了首个大规模评估基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 解决现有Text-to-SQL方法无法处理连续形态意图（如形状或异常）以及时间序列模型难以处理超长历史记录的挑战。

Method: 提出了Sonar-TS，一种神经符号框架，采用“搜索-验证”流程，结合SQL查询和Python程序生成来筛选和验证候选时间窗口。

Result: 实验表明，Sonar-TS能够有效处理复杂时间查询，而传统方法在此领域表现不佳。

Conclusion: 该研究首次系统性地研究了自然语言查询时间序列数据库（NLQ4TSDB），提出了一个通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [143] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka指数和Kantorovich距离实现快速公平的团队匹配，解决了异质技能水平团队的不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏中，公平且快速的匹配系统对玩家留存和满意度至关重要。然而，异质技能水平的团队匹配常因简单的平均技能指标导致不平衡比赛。

Method: Cinder采用两阶段匹配系统：首先使用Ruzicka相似性指数快速筛选非异常技能范围的团队，然后通过非线性技能桶映射和Kantorovich距离计算‘制裁分数’来精确评估匹配公平性。

Result: 通过对1.4亿次模拟团队配对的分析，Cinder系统展示了其可行性，并提供了公平匹配阈值的稳健基础。

Conclusion: Cinder系统通过两阶段匹配机制，结合Ruzicka相似性指数和Kantorovich距离，有效解决了异质技能水平团队间的公平匹配问题，为现代多人在线游戏提供了快速且公平的匹配解决方案。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [144] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个项目规模的数学自动形式化框架，两阶段处理依赖和证明，三周内完成教科书级形式化，成功率96%。


<details>
  <summary>Details</summary>
Motivation: 解决数学文献大规模自动形式化的挑战，包括跨文件依赖管理和端到端项目编译。

Method: M2F采用两阶段框架：声明编译阶段通过推断依赖关系排序和修复声明骨架，证明修复阶段在固定签名下填补证明缺口。

Result: M2F在约三周内将479页的教科书转化为153,853行Lean代码，证明成功率达96%（基线为80%）。

Conclusion: M2F框架首次实现了项目规模的数学文献自动形式化，展示了大规模自动化形式化的可行性。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [145] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: Sales Research Agent通过专用基准测试在CRM数据问答任务中显著优于其他AI模型。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够回答销售领导问题的AI系统，但现有模型缺乏透明、可重复的质量证据。

Method: 通过Sales Research Bench这一专用基准测试，对系统在八个客户加权的维度上进行评分。

Result: 在200个问题的测试中，Sales Research Agent的综合得分比Claude Sonnet 4.5高13分，比ChatGPT-5高24.1分。

Conclusion: Sales Research Agent在定制化企业CRM数据上的表现优于Claude Sonnet 4.5和ChatGPT-5，为客户提供了可重复比较AI解决方案的方法。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [146] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: PA-MoE通过阶段路由器优化专家分配，解决了传统MoE在RL中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络导致简单任务主导参数更新，传统MoE的令牌级路由会破坏专家专业化。

Method: 提出了一种阶段感知的混合专家架构（PA-MoE），包括轻量级阶段路由器和阶段一致性专家分配机制。

Result: 实验结果表明PA-MoE在提升专家专业化方面具有显著效果。

Conclusion: PA-MoE通过引入轻量级的阶段路由器，有效解决了传统MoE在强化学习中的局限性，提升了专家专业化能力。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [147] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA通过多代理协调和意图对齐的记忆机制，提升了长期执行的稳定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于RL的规划器和轨迹检索）易偏离用户意图且重复解决常规子问题，导致错误累积和效率低下。

Method: 提出了IntentCUA框架，通过Planner、Plan-Optimizer和Critic三个代理在共享内存中协调，将原始交互轨迹抽象为多视图意图表示和可重用技能。

Result: IntentCUA在端到端评估中实现了74.83%的任务成功率和0.91的步骤效率比，优于基于RL和轨迹的基线。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [148] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR通过动态检索最小必要指令和工具子集，显著降低LLM代理的运行成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型代理在每一步重新摄入长系统指令和大工具目录时导致的成本增加、代理偏离概率、延迟和工具选择错误问题。

Method: 提出了Instruction-Tool Retrieval (ITR)，一种RAG变体，动态检索每步所需的系统提示片段和最小必要工具子集，并组合成动态运行时系统提示。

Result: ITR将每步上下文标记减少95%，工具路由正确率相对提高32%，端到端成本降低70%，使代理能在上下文限制内运行2-20倍更多循环。

Conclusion: ITR方法显著降低了大型语言模型代理的运行成本，提高了工具选择的准确性，并使其能够在上下文限制内运行更多循环，特别适用于长时间运行的自主代理。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [149] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 论文提出评估LRMs推理忠实性的框架，发现准确性不能代表忠实性，需优化推理过程结构。


<details>
  <summary>Details</summary>
Motivation: 尽管LRMs表现优异，但其推理过程常缺乏真实性，影响可靠性和信任度，因此需要一种评估方法。

Method: 通过RFEval基准测试，对12个开源LRMs进行了评估，采用输出级反事实干预来探测忠实性。

Result: 研究发现49.7%的输出存在不忠实性，主要集中在数学和代码等脆弱领域，且与后训练目标相关。

Conclusion: 该论文提出了一个评估大型推理模型（LRMs）推理忠实性的正式框架，并发现准确性并非忠实性的可靠指标，强调了优化推理过程结构完整性的重要性。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [150] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q通过多子价值函数和Softmax策略提升多智能体强化学习的适应性和性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于价值分解的多智能体强化学习方法依赖单一最优动作，难以适应训练中价值函数的变化，容易收敛到次优策略。

Method: 提出了Successive Sub-value Q-learning (S2Q)，通过学习和利用多个子价值函数来保留替代高价值动作，并结合Softmax行为策略以促进持续探索。

Result: 在多个挑战性多智能体强化学习基准测试中，S2Q表现优于现有算法，展现出更强的适应性和整体性能。

Conclusion: S2Q通过引入多个子价值函数和基于Softmax的行为策略，显著提升了多智能体强化学习中的适应性和性能，实验证明其在多个基准测试中优于现有方法。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [151] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS通过动态优先处理高损失样本加速语言模型训练，实验显示显著提升收敛速度且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义难度指标，而难样本挖掘方法则需要昂贵的逐样本损失跟踪，PBS旨在解决这些问题。

Method: 提出了一种名为预测性批量调度（PBS）的新技术，通过在线训练的轻量级线性预测器动态优先处理高损失样本。

Result: 在130M参数变压器上的实验显示，PBS实现了6-13%的更快收敛，预测器与真实损失的相关系数从0.14提升至0.44。

Conclusion: 实验结果表明，基于词频统计的样本难度预测方法能够有效加速语言模型训练，且计算开销极小。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [152] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 本文探讨了代理AI在6G物理层中的应用，提出意图驱动的自主智能方案，并通过AgenCom案例展示了其自适应链路构建能力。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线系统的发展，功能复杂性和多样化服务需求推动从基于规则的控制转向意图驱动的自主智能，需要准确理解通信环境和用户意图。

Method: 通过综述代表性物理层任务及其局限性，识别代理AI优势的应用场景，并讨论多模态感知、跨层决策和可持续优化等关键技术。

Result: 提出了一种意图驱动的链路决策代理AgenCom，能够自适应地构建通信链路，适应不同的用户偏好和信道条件。

Conclusion: 本文探讨了基于代理AI的6G物理层实现路径，提出了意图驱动的链路决策代理AgenCom，展示了在多维用户偏好和信道条件下的自适应通信链路构建能力。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [153] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 本文提出了一种人机协作框架（STRIDE和SR-Delta），用于生成可信的可持续发展评级基准数据集，以提高评级方法的可比性和可信度。


<details>
  <summary>Details</summary>
Motivation: 由于不同机构对同一公司的可持续发展评级差异较大，限制了其可比性、可信度和决策相关性，因此需要一种方法来协调评级结果。

Method: 提出了一个通用的人机协作框架，包括STRIDE（提供原则性标准和评分系统）和SR-Delta（差异分析程序框架），用于生成可信的基准数据集。

Result: 该框架能够实现可持续发展评级方法的可扩展和可比评估。

Conclusion: 本文呼吁更广泛的AI社区采用AI驱动的方法来加强和推进支持可持续发展议程的评级方法。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [154] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: O-Shap通过改进特征分组方法，提升了Shapley值在视觉任务中的归因效果和效率。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法在视觉任务中因特征独立性假设不成立而效果受限，现代SHAP实现引入了支持分组归因的Owen值，但其效果依赖于特征分组的定义方式。

Method: 提出了一种新的分割方法，满足$T$-属性以确保层次结构中的语义对齐，并利用该层次结构进行计算剪枝。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义连贯性和运行时效率上优于基线SHAP变体。

Conclusion: O-Shap方法通过满足$T$-属性的分割方法，显著提升了特征归因的准确性、语义一致性和运行效率，尤其在结构重要的场景中表现优于基线SHAP变体。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [155] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG是一个自动构建知识图谱的框架，用于捕捉课程的学习依赖关系，结合教育材料信号和大语言模型，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在教育中，理解概念的先决条件和子概念对于识别学生的知识差距和实现个性化学习至关重要。大规模课程中，教师难以诊断个别误解或确定需要强化的概念。

Method: 给定课程的讲座材料（如幻灯片、笔记等），InstructKG提取重要概念作为节点，并推断学习依赖关系作为有向边（如“部分属于”或“依赖于”关系）。

Result: 通过在多个课程的真实讲座材料上进行实验和基于人类的评估，证明InstructKG能够捕捉丰富且与教师教学意图一致的学习进程。

Conclusion: InstructKG框架通过结合教育材料的丰富信号和大语言模型的泛化能力，成功构建了与教师教学意图一致的知识图谱，有效捕捉了课程的学习进程。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [156] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文探讨生成AI的语义处理机制，提出高维空间的索引性认识论，将生成模型视为流形导航者，并定义导航知识为新的知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成AI的机制与传统信息处理范式不同，其语义处理方式尚未得到充分哲学关注，需要新的认识论框架来指导其负责任的应用。

Method: 通过分析高维几何的四个结构特性（测度集中、近正交性、指数方向容量和流形规律性），结合Peirce符号学和Papert建构主义，构建了高维空间的索引性认识论。

Result: 提出了导航知识作为第三种知识生产模式，区别于符号推理和统计重组，为生成AI的理解和应用提供了新的理论基础。

Conclusion: 本文提出了一种基于高维几何的索引性认识论，重新定义了生成模型作为学习流形导航者的角色，并提出导航知识作为第三种知识生产模式。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [157] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出并行算法分解CircuitSAT实例，通过参数调整高效识别分解，实际应用验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决CircuitSAT实例的分解难题，特别是在布尔电路的逻辑等价性检查和密码哈希函数的原像攻击等挑战性场景中。

Method: 采用专用约束将原始SAT实例分割为一系列弱化公式，实现为参数化并行算法。

Result: 算法在实际挑战性CircuitSAT实例中表现出高效性。

Conclusion: 该论文提出了一种新颖的并行算法，用于分解困难的CircuitSAT实例，通过调整参数高效识别高质量分解，并在实际应用中展示了其有效性。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [158] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: 提出Combine框架，一种基于准则的剪枝方法，有效减少CNN计算量并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着CNN对准确性和性能需求的增加，其规模、执行时间、内存占用和功耗也随之增加，现有剪枝解决方案缺乏通用实现和比较标准。

Method: 引入Combine，一种基于准则的剪枝解决方案，并在VGG启发模型上展示了其能力。

Result: 在VGG启发模型上剪除了高达79%的滤波器，同时保持或提高了准确性，并将网络计算量减少了高达68%。

Conclusion: Combine框架提供了一种快速有效的迭代剪枝方法，展示了不同准则对不同模型的影响，并提出了几种新颖的剪枝准则。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [159] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一种新型预训练框架，结合JEPA与传统生成目标，通过潜在空间预测提升基因组表示的全局生物学视角，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基因组基础模型（GFMs）主要依赖MLM或NTP，虽然能捕捉局部基因组语法和细粒度模式，但缺乏全局生物学视角。JEPA-DNA旨在解决这一问题。

Method: JEPA-DNA整合了联合嵌入预测架构（JEPA）与传统生成目标，通过监督CLS令牌在潜在空间中进行预测，从而扩展了NTP和MLM范式。

Result: JEPA-DNA在多样化的基因组基准测试中表现优于仅生成基线模型，在监督和零样本任务中均表现出色。

Conclusion: JEPA-DNA通过结合潜在空间预测和传统生成目标，提供了一种更稳健且具有生物学基础的表示方法，为理解基因组序列的底层功能逻辑提供了可扩展的路径。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [160] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个轻量级高性能公式识别模型，体积小、性能强，适合实时和浏览器内应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有公式识别模型体积庞大、难以在消费级硬件或浏览器中实时部署的问题。

Method: 采用注意力机制设计、知识蒸馏和词汇与分词器迁移技术，构建了一个仅含2000万参数的轻量级模型。

Result: Texo在性能上与UniMERNet-T和PPFormulaNet-S相当，但模型体积分别减少了80%和65%，实现了实时推理和浏览器内部署。

Conclusion: Texo通过精简设计和高效的词汇与分词器迁移，实现了与最先进模型相当的性能，同时大幅减小模型体积，使其能在消费级硬件和浏览器中实时运行。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [161] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 提出了一种在线构建符号因果世界模型的框架，通过元解释学习和谓词发明实现高效、透明和可扩展的世界建模，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 标准世界建模方法在样本效率、透明度和可扩展性方面存在不足，需要一种更高效的方法来帮助智能体内部化世界的底层逻辑。

Method: 利用元解释学习和谓词发明的力量，构建符号因果世界模型，以在线方式发现语义上有意义且可重用的抽象概念，形成解耦的高质量概念层次结构。

Result: 该方法在具有复杂关系动态的领域中表现出色，避免了命题方法的组合爆炸问题，样本效率比PPO神经网络基线高出几个数量级。

Conclusion: 该论文提出了一种通过集成连续模型学习和修复到智能体决策循环中的框架，显著提高了样本效率、透明度和可扩展性，优于传统的PPO神经网络基线方法。

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [162] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出了一种基于AI Agent的协作研究工作流，填补了人文社科领域的方法论空白，并通过台湾Claude.ai数据验证了其可行性，同时识别了三种人机协作模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑知识工作，但现有研究主要集中在软件工程和自然科学领域，对人文社科的方法论探索有限。本研究旨在填补这一空白，提出一种基于AI Agent的协作研究工作流。

Method: 研究采用了两层次方法：一是设计和验证一个基于任务模块化、人机分工和可验证性的七阶段模块化工作流；二是利用台湾Claude.ai的使用数据（来自Anthropic Economic Index）进行实证分析，展示工作流在二次数据研究中的应用。

Result: 研究提出了一个七阶段的模块化工作流，明确了人类研究者（研究判断和伦理决策）和AI Agent（信息检索和文本生成）的角色分工。实证分析展示了工作流在二次数据研究中的应用过程和输出质量。

Conclusion: 本研究提出了一个可复制的AI协作框架，为人文社科研究者提供了方法论支持，并通过操作过程的反身性文档识别了三种人机协作模式。同时，研究也承认了单平台数据、横截面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [163] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: LBM通过行为嵌入提升个体决策预测，优于传统提示方法，适用于多种战略场景。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在预测人类高风险决策时的一致性和个体特异性行为生成的局限性。

Method: LBM通过基于结构化高维特质档案的行为嵌入，而非瞬态人物提示，来预测个体战略选择。

Result: LBM在保留场景评估中表现优于未调整的Llama-3.1-8B-Instruct骨干模型，并在提供更密集特质档案时持续提升性能。

Conclusion: LBM作为一种可扩展的高保真行为模拟方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [164] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 研究发现LLM的神经表征中，Bloom分类法的认知层次（如记忆到创造）线性可分，准确率达95%，表明认知难度在模型前向传播早期即被解析。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的黑箱特性，需要超越表面性能指标的新评估框架，以理解其内部认知复杂度的表征方式。

Method: 通过分析不同LLM的高维激活向量，使用线性分类器探究Bloom分类法各认知层次（从基础记忆到抽象合成）在模型残差流中的线性可分性。

Result: 线性分类器在所有Bloom层次上平均准确率达到约95%，表明认知层次在模型的表征中存在线性可访问的子空间。

Conclusion: 研究表明，大型语言模型（LLM）的神经表征中存在线性可分离的认知复杂度编码，支持Bloom分类法各层次在模型中的线性可分性。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [165] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 论文提出TimeSPEC方法，通过声明级验证减少LLMs在回溯测试中的时间知识泄漏，实验证明其优于传统提示约束。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能准确预测未来事件需要回溯测试，但模型可能无意中泄漏训练后的知识，影响回溯评估的有效性。

Method: 引入声明级框架检测和量化时间知识泄漏，使用Shapley值衡量每个声明对预测的贡献，并提出TimeSPEC方法，通过声明验证和再生主动过滤时间污染。

Result: 在三个任务（美国最高法院案件预测、NBA薪资估算和股票回报排名）的350个实例中，标准提示基线存在显著泄漏，TimeSPEC有效降低了Shapley-DCLR。

Conclusion: TimeSPEC通过显式、可解释的声明级验证，显著减少了时间知识泄漏（Shapley-DCLR），同时保持了任务性能，证明了其在可靠回溯测试中的优越性。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [166] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 论文提出Web Verbs作为网络动作的语义层，通过统一接口提升代理的可靠性、效率和可验证性，并展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 当前的网络代理大多基于低级操作（如点击和按键），这些操作脆弱、低效且难以验证，因此需要一个语义层来支持网络动作。

Method: 提出了Web Verbs，这是一个网络规模的、类型化的、语义化文档化的功能集合，通过统一接口暴露网站能力。

Result: 通过概念验证实现和案例研究，展示了Web Verbs相比现有代理的简洁性和鲁棒性。

Conclusion: 论文提出了一个标准化路线图，旨在使Web Verbs在网络规模上可部署且值得信赖。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [167] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了从arXiv LaTeX源训练1.36B参数科学语言模型的端到端流程，分析了预处理、分词和基础设施对训练的影响，为有限计算预算下的领域专用模型训练提供实用见解。


<details>
  <summary>Details</summary>
Motivation: 尽管前沿大语言模型展现出强大的推理和数学能力，但从原始资源训练领域专用科学语言模型的实践过程仍缺乏详细记录。

Method: 研究描述了一个端到端的流程，包括元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词以及在受限计算资源（2xA100 GPU）下的密集Transformer训练。

Result: 通过24次实验运行，分析了训练稳定性、扩展行为、数据产量损失和基础设施瓶颈，发现预处理决策显著影响可用token量，分词影响符号稳定性，存储和I/O限制可能成为与计算同等重要的限制因素。

Conclusion: 本文提供了一个从零开始训练小型科学语言模型的工程实践和透明记录，旨在支持在有限计算预算下构建领域专用模型的研究者。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [168] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify通过生成信息性后续问题，提升医学LLMs的诊断性能，减少诊断错误。


<details>
  <summary>Details</summary>
Motivation: 探讨医学LLMs在生成信息性后续问题及推理鉴别诊断方面的能力，以支持临床决策。

Method: MedClarify计算候选诊断列表，并主动生成旨在减少诊断不确定性的后续问题，选择信息增益最高的问题进行针对性推理。

Result: MedClarify相比标准单次LLM基线减少了约27个百分点的诊断错误。

Conclusion: MedClarify通过信息寻求代理和不确定性感知推理，显著提升了医学LLMs的诊断性能，减少了约27个百分点的诊断错误。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [169] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 本文提出了一种无需数据的正则化方法，通过曲率矩阵近似解决任务向量组合时的表示漂移问题，实现了高效且鲁棒的任务适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。因此，需要一种无需数据的解决方案来减少任务向量组合时的交叉任务干扰。

Method: 采用Kronecker-Factored Approximate Curvature技术，将表示漂移的正则化问题转化为曲率矩阵近似问题，从而设计出一个实用的正则化器。

Result: 提出的方法在任务添加和否定方面实现了最先进的效果，具有恒定复杂度，且对任务向量重新缩放具有鲁棒性。

Conclusion: 本文提出了一种无需数据的正则化方法，通过将表示漂移的正则化问题转化为曲率矩阵近似问题，实现了任务向量解耦，并在任务添加和否定方面达到了最先进的效果。该方法在任务数量上具有恒定复杂度，且对任务向量重新缩放具有鲁棒性。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [170] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出结合形式验证和深度学习的新型检索框架，提升复杂查询的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入的模型在处理涉及复杂关系、对象组合或精确约束的查询时存在不足，需要更透明和可验证的解决方案。

Method: 通过结合基于图的验证方法和神经代码生成，将形式验证集成到基于深度学习的图像检索中。

Result: 该框架不仅返回匹配结果，还能明确标记查询中哪些约束被满足，提升了检索的准确性和透明度。

Conclusion: 该论文提出了一个结合形式验证和深度学习的新型框架，旨在提升信息检索的透明度和可信度，特别是在处理复杂查询时。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [171] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: MCVAE模型通过多模态对比学习解决NSCLC生存预测中的数据缺失问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者生存预测因个体预后特征差异而复杂，且临床数据常存在模态缺失问题，现有模型在严重缺失情况下缺乏鲁棒性。

Method: 提出MCVAE模型，包括模态特定的变分编码器、融合瓶颈结构和多任务目标（生存损失、重构损失和跨模态对比损失），并采用随机模态掩码训练提升鲁棒性。

Result: 在TCGA-LUAD和TCGA-LUSC数据集上验证了MCVAE在疾病特异性生存预测中的有效性，并展示了其对严重缺失场景的鲁棒性。

Conclusion: 论文提出了MCVAE模型，通过多模态对比变分自编码器有效解决了NSCLC患者生存预测中数据缺失的问题，并在TCGA数据集上验证了其优越性和鲁棒性。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [172] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 提出一个隐私设计框架，整合多国隐私法规原则，指导儿童AI应用开发，确保隐私与法律合规，并通过案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 针对儿童使用AI技术时的隐私风险，现有法规执行存在挑战，需提出实用框架以指导设计和开发。

Method: 提出一个基于隐私设计（PbD）的框架，整合了GDPR、PIPEDA、COPPA等隐私法规原则，并将其映射到LLM应用的各个阶段。

Result: 框架通过案例研究验证，展示了如何在LLM生命周期中实施技术和组织控制，以及适龄设计决策。

Conclusion: 通过结合隐私设计原则和法律要求，提出的框架能够有效指导开发者为儿童设计AI应用，确保隐私保护和法律合规。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [173] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec 是一个高性能框架，通过后端无关架构解决了推荐系统研究中本地与分布式执行的矛盾，支持多种算法和指标，并注重可持续性。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统创新受限于分裂的生态系统，研究者不得不在内存实验的便捷性与分布式工业引擎的高成本、复杂重写之间做出选择。

Method: WarpRec 采用了一种新颖的后端无关架构，包含50多种最先进的算法、40个指标以及19种过滤和分割策略，支持从本地执行无缝过渡到分布式训练和优化。

Result: WarpRec 通过集成 CodeCarbon 实现实时能源追踪，展示了可扩展性不必牺牲科学完整性或可持续性，同时为向智能代理 AI 的转变做好了准备。

Conclusion: WarpRec 不仅弥合了学术界与工业界之间的鸿沟，还能作为下一代可持续、支持智能代理的推荐系统的架构基础。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [174] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 该研究提出了一种优化ARM Cortex处理器上AI模型的框架，通过自动化测试和Pareto分析平衡能效与准确性，为开发者提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 优化ARM Cortex处理器（M0+、M4、M7）上AI模型的能效、准确性和资源利用率，以满足嵌入式系统的需求。

Method: 通过设计自动化测试平台，系统评估关键性能指标（KPIs），并确定处理器与AI模型的最佳组合。

Result: 研究发现M7处理器适合短推理周期，M4处理器在长推理任务中能效更高，M0+处理器适用于简单任务。浮点运算（FLOPs）与推理时间呈近线性相关。

Conclusion: 该研究为开发者提供了设计高能效AI系统的实用指南，帮助他们在实际应用中实现高性能。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [175] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架结合知识图谱和检索增强生成，显著提升LLM在电信领域的性能，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 通用领域的大型语言模型在电信领域应用中因领域复杂性、标准演变和术语专业化而表现不佳，导致幻觉增加和实用性降低。

Method: 提出KG-RAG框架，整合知识图谱（KG）和检索增强生成（RAG）技术，利用KG结构化表示电信领域知识，RAG动态检索相关事实以增强模型输出。

Result: 实验结果表明，KG-RAG在基准数据集上表现优于纯LLM和标准RAG基线，平均准确率分别提高了21.6%和14.3%。

Conclusion: KG-RAG框架通过结合知识图谱和检索增强生成技术，显著提升了大型语言模型在电信领域的准确性和可靠性，减少了幻觉现象，并确保符合电信规范。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [176] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 论文提出可重用性和可验证性两个新指标，评估LLM代理间的CoT推理质量，发现其与准确性无关，且专用模型不总是优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估仅关注目标任务准确性，无法评估推理过程本身的质量或效用。

Method: 通过Thinker-Executor框架将CoT生成与执行解耦，并引入可重用性和可验证性两个新指标进行评估。

Result: 评估了四个Thinker模型与十个Executor模型委员会在五个基准上的表现，发现可重用性和可验证性与标准准确性无关。

Conclusion: 研究发现，当前基于准确性的排行榜在评估推理能力时存在盲点，专用推理模型的CoT并不总是比通用LLM（如Llama和Gemma）更具可重用性或可验证性。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [177] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong 是一个开源 LLM 代理，通过轨迹分割 SFT 和渐进式 RL 训练解决长视野任务，在多个基准测试中超越竞争对手。


<details>
  <summary>Details</summary>
Motivation: 解决极长视野任务的需求，通过冷启动和渐进式训练提升模型在长视野任务中的表现。

Method: 首先通过轨迹分割的监督微调（SFT）冷启动模型，然后通过渐进式强化学习（RL）训练扩展模型能力。使用 Research-Factory 自动化管道生成高质量训练数据。

Result: KLong 在多个基准测试中表现出优越性和泛化能力。

Conclusion: KLong (106B) 在 PaperBench 上超越了 Kimi K2 Thinking (1T) 11.28%，并在其他编码基准测试中展现出泛化能力。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [178] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于ODE的统一理论框架ODESteer，显著提升LLM对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前激活引导方法缺乏统一理论框架，且依赖单步引导难以捕捉复杂激活分布模式。

Method: 提出基于ODE的理论框架，将传统激活加法解释为一阶近似，并引入控制理论中的屏障函数来设计引导方向。

Result: ODESteer在多种LLM对齐基准测试中表现优异，如TruthfulQA提升5.7%、UltraFeedback提升2.5%、RealToxicityPrompts提升2.4%。

Conclusion: 本研究通过提出基于ODE的统一理论框架ODESteer，为LLM对齐中的激活引导提供了新的理论视角，并通过实证验证了其优越性。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [179] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 论文提出了一种结合SWIN Transformer和CNN的混合联邦学习模型，用于肺部疾病诊断，强调数据安全和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的显著提升，人工智能在医疗健康领域的应用潜力巨大。论文旨在利用联邦学习和先进AI技术的结合，解决医疗数据共享和安全性问题，同时提高疾病诊断的准确性。

Method: 研究采用了最新的CNN模型（如DenseNet201、Inception V3、VGG 19）和SWIN Transformer，结合TensorFlow和Keras框架，构建了一个混合模型。通过联邦学习技术，实现了数据的分布式处理和模型的安全性。

Result: 提出的混合模型能够基于X光报告检测COVID-19和肺炎，并通过联邦学习技术确保数据安全和模型可靠性，为医生提供辅助诊断工具。

Conclusion: 该论文提出了一种结合SWIN Transformer和CNN的混合联邦学习模型，用于肺部疾病（如COVID-19和肺炎）的诊断，旨在为医疗领域提供一个高效、可靠且安全的分布式系统。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [180] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 提出AI GameStore平台，通过评估AI在多样化人类游戏中的表现来衡量其通用智能，发现当前模型表现远不及人类。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试评估范围狭窄且易饱和，作者认为通过评估AI系统在所有可能的人类游戏中的表现，可以更全面地衡量其类人通用智能。

Method: 引入AI GameStore平台，利用LLMs和人类参与循环合成新的代表性人类游戏，并评估了七个前沿视觉语言模型在短时游戏片段中的表现。

Result: 在生成的100个游戏中，最佳模型在大多数游戏中的得分不到人类平均分的10%，尤其在需要世界模型学习、记忆和规划的游戏上表现较差。

Conclusion: 作者提出AI GameStore作为衡量和推动机器向类人通用智能发展的实用方法，并概述了下一步建设该平台的计划。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [181] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一种基于层次离散扩散模型的分子图生成框架，首次在图形扩散中实现近乎完美的化学有效性，并在多个任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图形扩散模型在化学有效性和满足目标属性方面表现不佳，MolHIT旨在克服这些性能限制。

Method: MolHIT基于Hierarchical Discrete Diffusion Model，扩展离散扩散到编码化学先验的额外类别，并采用decoupled atom encoding按化学角色拆分原子类型。

Result: MolHIT在MOSES数据集上首次实现近乎完美的化学有效性，超越1D基线模型，并在多属性引导生成和支架扩展等下游任务中表现强劲。

Conclusion: MolHIT通过Hierarchical Discrete Diffusion Model和decoupled atom encoding，在分子图生成领域实现了新的SOTA性能，首次在图形扩散中达到近乎完美的化学有效性，并在下游任务中表现优异。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [182] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: 	exttt{AutoNumerics} 是一个多智能体框架，通过自然语言描述自动设计透明PDE求解器，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络的PDE求解方法灵活性高但计算成本高且可解释性差，需要一种更高效且透明的自动化求解方法。

Method: 引入多智能体框架，结合粗到细执行策略和基于残差的自验证机制，生成基于经典数值分析的透明求解器。

Result: 在24个经典和实际PDE问题上，	exttt{AutoNumerics} 的精度优于或与现有基线相当，并能正确选择数值方案。

Conclusion: 	exttt{AutoNumerics} 提供了一种透明且高效的自动化PDE求解框架，其性能优于现有神经和LLM基线，并能根据PDE结构特性正确选择数值方案，展现了作为自动化PDE求解范式的潜力。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [183] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是一个专注于从多语言历史文本中提取人物-地点关系的评估实验室，通过综合评估框架支持下游应用。


<details>
  <summary>Details</summary>
Motivation: 基于HIPE-2020和HIPE-2022的成果，进一步扩展至语义关系提取，以支持数字人文学科中的大规模历史数据处理需求。

Method: 采用三部分评估框架（准确性、计算效率、领域泛化能力），要求系统对两种关系类型（$at$和$isAt$）进行分类，并结合时空线索进行推理。

Result: 提出了一个专注于人物-地点关系提取的多语言、跨时代评估任务，并设计了综合评估指标。

Conclusion: HIPE-2026通过多语言历史文本中的人物-地点关系提取任务，推动了语义关系提取领域的发展，并为知识图谱构建、历史传记重建等下游应用提供了支持。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>
